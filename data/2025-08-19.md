<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 81]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.LG](#cs.LG) [Total: 9]
- [cs.AI](#cs.AI) [Total: 7]
- [cs.CR](#cs.CR) [Total: 4]
- [cs.RO](#cs.RO) [Total: 1]
- [cs.CV](#cs.CV) [Total: 8]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Deep Language Geometry: Constructing a Metric Space from LLM Weights](https://arxiv.org/abs/2508.11676)
*Maksym Shamrai,Vladyslav Hamolia*

Main category: cs.CL

TL;DR: 提出基于LLM内部权重激活构建语言度量空间的新框架，通过剪枝算法自动生成高维向量表示，无需手工特征工程，在106种语言上验证了与传统语系分类的一致性并发现新的语言关联


<details>
  <summary>Details</summary>
Motivation: 传统语言分类方法依赖手工设计的语言学特征，需要领域专业知识且难以捕捉深层次的语言内在特性。本文旨在利用LLM的内部权重激活自动发现语言间的度量关系

Method: 采用改进的剪枝算法计算权重重要性分数，从多语言LLM中自动推导高维向量表示，构建语言度量空间来捕捉语言内在特征

Result: 在106种语言上验证，结果与传统语系分类高度一致，同时揭示了意想不到的语言间联系，可能反映历史接触或语言演化关系

Conclusion: 该方法能够自动发现语言间的深层关系，为语言分类和演化研究提供了新的数据驱动方法，代码和语言潜在向量已开源

Abstract: We introduce a novel framework that utilizes the internal weight activations
of modern Large Language Models (LLMs) to construct a metric space of
languages. Unlike traditional approaches based on hand-crafted linguistic
features, our method automatically derives high-dimensional vector
representations by computing weight importance scores via an adapted pruning
algorithm. Our approach captures intrinsic language characteristics that
reflect linguistic phenomena. We validate our approach across diverse datasets
and multilingual LLMs, covering 106 languages. The results align well with
established linguistic families while also revealing unexpected inter-language
connections that may indicate historical contact or language evolution. The
source code, computed language latent vectors, and visualization tool are made
publicly available at https://github.com/mshamrai/deep-language-geometry.

</details>


### [2] [Can we Evaluate RAGs with Synthetic Data?](https://arxiv.org/abs/2508.11758)
*Jonas van Elburg,Peter van der Putten,Maarten Marx*

Main category: cs.CL

TL;DR: 研究发现LLM生成的合成QA数据在评估不同检索器配置的RAG系统时能可靠替代人工标注基准，但在比较不同生成器架构时无法产生一致的排名结果。


<details>
  <summary>Details</summary>
Motivation: 探索当人工标注基准不可用时，大语言模型生成的合成问答数据是否能有效替代人工标注数据来评估检索增强生成(RAG)系统。

Method: 通过两个实验进行评估：1）固定生成器，变化检索器参数；2）固定检索器参数，变化生成器架构。在四个数据集（两个开放域、两个专有）上进行测试。

Result: 合成基准在评估不同检索器配置的RAG系统时表现可靠，与人工标注基准结果一致；但在比较不同生成器架构时无法产生一致的排名，可能由于任务不匹配和风格偏好偏差导致。

Conclusion: 合成QA数据可作为评估检索器配置的有效代理，但不适用于比较不同生成器架构的性能评估。

Abstract: We investigate whether synthetic question-answer (QA) data generated by large
language models (LLMs) can serve as an effective proxy for human-labeled
benchmarks when such data is unavailable. We assess the reliability of
synthetic benchmarks across two experiments: one varying retriever parameters
while keeping the generator fixed, and another varying the generator with fixed
retriever parameters. Across four datasets, of which two open-domain and two
proprietary, we find that synthetic benchmarks reliably rank the RAGs varying
in terms of retriever configuration, aligning well with human-labeled benchmark
baselines. However, they fail to produce consistent RAG rankings when comparing
generator architectures. The breakdown possibly arises from a combination of
task mismatch between the synthetic and human benchmarks, and stylistic bias
favoring certain generators.

</details>


### [3] [Limitation Learning: Catching Adverse Dialog with GAIL](https://arxiv.org/abs/2508.11767)
*Noah Kasmanoff,Rahul Zalkikar*

Main category: cs.CL

TL;DR: 应用模仿学习到对话任务中，通过专家演示训练策略和判别器，发现对话模型的局限性，并提出该方法可用于识别对话导向任务中数据模型的不良行为


<details>
  <summary>Details</summary>
Motivation: 在没有奖励的情况下，利用专家演示通过模仿学习创建对话策略，同时训练判别器来区分专家对话和合成对话

Method: 应用模仿学习技术，从专家对话演示中学习策略（能够根据提示与用户对话），并训练判别器来分类专家对话和合成对话

Result: 策略表现有效，但判别器结果显示对话模型存在局限性，该方法能够识别对话导向任务中常见数据模型的不良行为

Conclusion: 模仿学习技术可用于有效识别对话模型中存在的不良行为，为对话系统的质量评估提供了新方法

Abstract: Imitation learning is a proven method for creating a policy in the absence of
rewards, by leveraging expert demonstrations. In this work, we apply imitation
learning to conversation. In doing so, we recover a policy capable of talking
to a user given a prompt (input state), and a discriminator capable of
classifying between expert and synthetic conversation. While our policy is
effective, we recover results from our discriminator that indicate the
limitations of dialog models. We argue that this technique can be used to
identify adverse behavior of arbitrary data models common for dialog oriented
tasks.

</details>


### [4] [Investigating Transcription Normalization in the Faetar ASR Benchmark](https://arxiv.org/abs/2508.11771)
*Leo Peckham,Michael Ong,Naomi Nagy,Ewan Dunbar*

Main category: cs.CL

TL;DR: 研究发现Faetar ASR基准中的转录不一致性问题存在但不是主要挑战，有限词典约束解码有益，但任务仍然极其困难


<details>
  <summary>Details</summary>
Motivation: 检验低资源自动语音识别基准中转录不一致性的影响，确定其是否是任务的主要挑战

Method: 使用手工构建的小型词典分析转录不一致性，测试bigram词级语言模型和有限词典约束解码的效果

Result: 转录不一致确实存在但不是主要问题，bigram语言模型无额外益处，有限词典约束解码有积极效果

Conclusion: Faetar ASR任务极其困难，转录不一致性不是主要障碍，词典约束是有效的改进方向

Abstract: We examine the role of transcription inconsistencies in the Faetar Automatic
Speech Recognition benchmark, a challenging low-resource ASR benchmark. With
the help of a small, hand-constructed lexicon, we conclude that find that,
while inconsistencies do exist in the transcriptions, they are not the main
challenge in the task. We also demonstrate that bigram word-based language
modelling is of no added benefit, but that constraining decoding to a finite
lexicon can be beneficial. The task remains extremely difficult.

</details>


### [5] [A Multi-Task Evaluation of LLMs' Processing of Academic Text Input](https://arxiv.org/abs/2508.11779)
*Tianyi Li,Yu Qin,Olivia R. Liu Sheng*

Main category: cs.CL

TL;DR: 本文评估了大型语言模型（特别是Google Gemini）在学术文本处理方面的能力，通过四个任务测试发现其在学术同行评审中的表现有限，不建议无限制使用


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型在辅助学术同行评审和科学发现方面的实际应用潜力，评估其在学术文本处理中的真实能力

Method: 采用四个任务评估框架：内容复现/比较/评分/反思，使用顶级信息系统期刊文章作为输入文本，结合多种文本指标进行严格性能评估

Result: Gemini在学术文本摘要和转述方面表现可接受，但在文本排序、评分和深度反思方面表现不佳，缺乏区分度和洞察力

Conclusion: LLMs在学术文本处理能力方面存在明显局限，不建议在构建同行评审中无限制使用，需要更谨慎的应用评估

Abstract: How much large language models (LLMs) can aid scientific discovery, notably
in assisting academic peer review, is in heated debate. Between a literature
digest and a human-comparable research assistant lies their practical
application potential. We organize individual tasks that computer science
studies employ in separate terms into a guided and robust workflow to evaluate
LLMs' processing of academic text input. We employ four tasks in the
assessment: content reproduction/comparison/scoring/reflection, each demanding
a specific role of the LLM (oracle/judgmental arbiter/knowledgeable
arbiter/collaborator) in assisting scholarly works, and altogether testing LLMs
with questions that increasingly require intellectual capabilities towards a
solid understanding of scientific texts to yield desirable solutions. We
exemplify a rigorous performance evaluation with detailed instructions on the
prompts. Adopting first-rate Information Systems articles at three top journals
as the input texts and an abundant set of text metrics, we record a compromised
performance of the leading LLM - Google's Gemini: its summary and paraphrase of
academic text is acceptably reliable; using it to rank texts through pairwise
text comparison is faintly scalable; asking it to grade academic texts is prone
to poor discrimination; its qualitative reflection on the text is
self-consistent yet hardly insightful to inspire meaningful research. This
evidence against an endorsement of LLMs' text-processing capabilities is
consistent across metric-based internal (linguistic assessment), external
(comparing to the ground truth), and human evaluation, and is robust to the
variations of the prompt. Overall, we do not recommend an unchecked use of LLMs
in constructing peer reviews.

</details>


### [6] [LLM-Guided Planning and Summary-Based Scientific Text Simplification: DS@GT at CLEF 2025 SimpleText](https://arxiv.org/abs/2508.11816)
*Krishna Chaitanya Marturi,Heba H. Elwazzan*

Main category: cs.CL

TL;DR: 提出了一种基于大语言模型的两阶段科学文本简化方法，在句子级别使用结构化规划指导简化，在文档级别通过摘要引导简化过程


<details>
  <summary>Details</summary>
Motivation: 解决科学文本简化任务，需要在保持内容准确性的同时提高可读性，传统方法难以同时处理句子级别和文档级别的连贯性

Method: 使用大语言模型进行两阶段处理：句子级别先生成结构化规划再按规划简化；文档级别先生成摘要再用摘要指导简化

Result: 该方法能够产生更加连贯且上下文忠实度更高的科学文本简化结果

Conclusion: 基于大语言模型的两阶段规划驱动方法在科学文本简化任务中表现有效，特别是在保持内容准确性和连贯性方面

Abstract: In this paper, we present our approach for the CLEF 2025 SimpleText Task 1,
which addresses both sentence-level and document-level scientific text
simplification. For sentence-level simplification, our methodology employs
large language models (LLMs) to first generate a structured plan, followed by
plan-driven simplification of individual sentences. At the document level, we
leverage LLMs to produce concise summaries and subsequently guide the
simplification process using these summaries. This two-stage, LLM-based
framework enables more coherent and contextually faithful simplifications of
scientific text.

</details>


### [7] [Hallucination Detection and Mitigation in Scientific Text Simplification using Ensemble Approaches: DS@GT at CLEF 2025 SimpleText](https://arxiv.org/abs/2508.11823)
*Krishna Chaitanya Marturi,Heba H. Elwazzan*

Main category: cs.CL

TL;DR: 本文提出了一个集成框架，结合BERT分类器、语义相似度、自然语言推理和LLM推理来检测科学文本简化中的创造性生成和信息失真，并使用LLM后编辑系统进行基于原文的修正。


<details>
  <summary>Details</summary>
Motivation: 解决科学文本简化任务中的创造性生成和信息失真检测问题，提高检测的鲁棒性和准确性。

Method: 构建集成框架，结合BERT分类器、语义相似度测量、自然语言推理模型和LLM推理，使用元分类器整合多种信号；采用LLM后编辑系统基于原文修订简化文本。

Result: 开发了一个综合性的检测和修正系统，能够有效识别科学文本简化过程中的创造性生成和信息失真问题。

Conclusion: 该集成方法通过多策略融合和LLM后编辑，为科学文本简化的质量控制和失真检测提供了有效的解决方案。

Abstract: In this paper, we describe our methodology for the CLEF 2025 SimpleText Task
2, which focuses on detecting and evaluating creative generation and
information distortion in scientific text simplification. Our solution
integrates multiple strategies: we construct an ensemble framework that
leverages BERT-based classifier, semantic similarity measure, natural language
inference model, and large language model (LLM) reasoning. These diverse
signals are combined using meta-classifiers to enhance the robustness of
spurious and distortion detection. Additionally, for grounded generation, we
employ an LLM-based post-editing system that revises simplifications based on
the original input texts.

</details>


### [8] [A Survey of Idiom Datasets for Psycholinguistic and Computational Research](https://arxiv.org/abs/2508.11828)
*Michael Flor,Xinyi Liu,Anna Feldman*

Main category: cs.CL

TL;DR: 这篇调查论文综述了语言学和计算语言学中用于研究习语的53个数据集，分析了它们的内容、形式和用途。


<details>
  <summary>Details</summary>
Motivation: 习语作为图式表达式，其含义无法从单词推断，这给计算处理和人类实验研究带来挑战。需要系统性地评估现有的习语研究数据资源。

Method: 调查分析了53个习语数据集，包括心理语言学资源（包含熟悉度、透明度、组合性等评分）和计算语言学数据集（支持习语检测/分类、改写、跨语言建模等任务）。

Result: 展示了注释实践、覆盖范围和任务构建方面的趋势。近期研究扩大了语言覆盖范围和任务多样性，但心理语言学和计算语言学在习语研究方面仍然缺乏联系。

Conclusion: 虽然习语研究数据集在语言覆盖和任务多样性方面取得进展，但两个学科领域之间的研究仍存在脱节，需要更多跨学科合作来推动习语理解的深入研究。

Abstract: Idioms are figurative expressions whose meanings often cannot be inferred
from their individual words, making them difficult to process computationally
and posing challenges for human experimental studies. This survey reviews
datasets developed in psycholinguistics and computational linguistics for
studying idioms, focusing on their content, form, and intended use.
Psycholinguistic resources typically contain normed ratings along dimensions
such as familiarity, transparency, and compositionality, while computational
datasets support tasks like idiomaticity detection/classification,
paraphrasing, and cross-lingual modeling. We present trends in annotation
practices, coverage, and task framing across 53 datasets. Although recent
efforts expanded language coverage and task diversity, there seems to be no
relation yet between psycholinguistic and computational research on idioms.

</details>


### [9] [Every 28 Days the AI Dreams of Soft Skin and Burning Stars: Scaffolding AI Agents with Hormones and Emotions](https://arxiv.org/abs/2508.11829)
*Leigh Levinson,Christopher J. Agostino*

Main category: cs.CL

TL;DR: 该论文提出通过模拟月经和昼夜节律等生物节律来增强AI系统的上下文相关性过滤能力，在多个基准测试中观察到与生物预期一致的性能变化


<details>
  <summary>Details</summary>
Motivation: 解决AI系统面临的框架问题——从指数级大的可能性空间中确定上下文相关信息，受生物节律作为自然相关性过滤器的启发

Method: 开发了一个框架，通过周期性函数模拟关键激素（雌激素、睾酮、皮质醇）生成系统提示，将模拟的生物节律嵌入大型语言模型

Result: 语言分析显示情感和风格变化跟踪生物阶段：经期悲伤情绪达到峰值，排卵期以快乐为主；昼夜模式显示早晨乐观转向夜间内省。在SQuAD、MMLU等基准测试中表现出与生物预期一致的微妙但一致的性能变化

Conclusion: 该方法为上下文AI提供了新颖途径，同时揭示了语言模型中嵌入的关于性别和生物学的社会偏见

Abstract: Despite significant advances, AI systems struggle with the frame problem:
determining what information is contextually relevant from an exponentially
large possibility space. We hypothesize that biological rhythms, particularly
hormonal cycles, serve as natural relevance filters that could address this
fundamental challenge. We develop a framework that embeds simulated menstrual
and circadian cycles into Large Language Models through system prompts
generated from periodic functions modeling key hormones including estrogen,
testosterone, and cortisol. Across multiple state-of-the-art models, linguistic
analysis reveals emotional and stylistic variations that track biological
phases; sadness peaks during menstruation while happiness dominates ovulation
and circadian patterns show morning optimism transitioning to nocturnal
introspection. Benchmarking on SQuAD, MMLU, Hellaswag, and AI2-ARC demonstrates
subtle but consistent performance variations aligning with biological
expectations, including optimal function in moderate rather than extreme
hormonal ranges. This methodology provides a novel approach to contextual AI
while revealing how societal biases regarding gender and biology are embedded
within language models.

</details>


### [10] [When Does Language Transfer Help? Sequential Fine-Tuning for Cross-Lingual Euphemism Detection](https://arxiv.org/abs/2508.11831)
*Julia Sammartino,Libby Barak,Jing Peng,Anna Feldman*

Main category: cs.CL

TL;DR: 通过跨语言转移的序列精调提高隐喻语检测性能，尤其是对资源较少的语言如约鲁巴语和土耳其语


<details>
  <summary>Details</summary>
Motivation: 隐喻语具有文化变异性和模糊性，给语言模型带来挑战，特别是在低资源环境中

Method: 使用XLM-R和mBERT模型，比较序列精调、单语言精调和同时精调在5种语言（英语、西班牙语、中文、土耳其语、约鲁巴语）上的隐喻语检测性能

Result: 使甩高资源语言进行序列精调显著提高低资源语言的性能，XLM-R获得更大收益但更敏感，mBERT更稳定但性能较低

Conclusion: 序列精调是一种简单但有效的策略，特别适用于改善多语言模型在低资源语言上的隐喻语检测能力

Abstract: Euphemisms are culturally variable and often ambiguous, posing challenges for
language models, especially in low-resource settings. This paper investigates
how cross-lingual transfer via sequential fine-tuning affects euphemism
detection across five languages: English, Spanish, Chinese, Turkish, and
Yoruba. We compare sequential fine-tuning with monolingual and simultaneous
fine-tuning using XLM-R and mBERT, analyzing how performance is shaped by
language pairings, typological features, and pretraining coverage. Results show
that sequential fine-tuning with a high-resource L1 improves L2 performance,
especially for low-resource languages like Yoruba and Turkish. XLM-R achieves
larger gains but is more sensitive to pretraining gaps and catastrophic
forgetting, while mBERT yields more stable, though lower, results. These
findings highlight sequential fine-tuning as a simple yet effective strategy
for improving euphemism detection in multilingual models, particularly when
low-resource languages are involved.

</details>


### [11] [SupraTok: Cross-Boundary Tokenization for Enhanced Language Model Performance](https://arxiv.org/abs/2508.11857)
*Andrei-Valentin Tănase,Elena Pelican*

Main category: cs.CL

TL;DR: SupraTok是一种新颖的分词架构，通过跨边界模式学习、熵驱动数据筛选和多阶段课程学习，实现了比OpenAI和Google分词器高30%以上的效率提升，同时保持多语言竞争力，在GPT-2规模模型上带来8-9%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 分词作为NLP中的基础但未被充分探索的瓶颈，现有策略在模型架构取得显著进展的情况下仍然相对静态，需要重新思考子词分割方法。

Method: 提出SupraTok架构，包含三个创新：跨边界模式学习发现多词语义单元、熵驱动数据优化训练语料质量、多阶段课程学习确保稳定收敛。该方法扩展了BPE算法，学习保持语义完整性的"超词"标记。

Result: 英语分词效率提升31%（5.91 vs 4.51字符/词元），优于OpenAI o200k和Google Gemma 3分词器，在38种语言中保持竞争力。集成到GPT-2模型（124M参数）后，在HellaSWAG和MMLU基准上分别提升8.4%和9.5%。

Conclusion: 高效分词可以作为架构创新的补充路径来提升语言模型性能，虽然小规模结果有希望，但需要在大规模模型上进一步验证。

Abstract: Tokenization remains a fundamental yet underexplored bottleneck in natural
language processing, with strategies largely static despite remarkable progress
in model architectures. We present SupraTok, a novel tokenization architecture
that reimagines subword segmentation through three innovations: cross-boundary
pattern learning that discovers multi-word semantic units, entropy-driven data
curation that optimizes training corpus quality, and multi-phase curriculum
learning for stable convergence. Our approach extends Byte-Pair Encoding by
learning "superword" tokens, coherent multi-word expressions that preserve
semantic unity while maximizing compression efficiency. SupraTok achieves 31%
improvement in English tokenization efficiency (5.91 versus 4.51 characters per
token) compared to OpenAI's o200k tokenizer and 30% improvement over Google's
Gemma 3 tokenizer (256k vocabulary), while maintaining competitive performance
across 38 languages. When integrated with a GPT-2 scale model (124M parameters)
trained on 10 billion tokens from the FineWeb-Edu dataset, SupraTok yields 8.4%
improvement on HellaSWAG and 9.5% on MMLU benchmarks without architectural
modifications. While these results are promising at this scale, further
validation at larger model scales is needed. These findings suggest that
efficient tokenization can complement architectural innovations as a path to
improved language model performance.

</details>


### [12] [In-Context Examples Matter: Improving Emotion Recognition in Conversation with Instruction Tuning](https://arxiv.org/abs/2508.11889)
*Hui Ma,Bo Zhang,Jinpeng Hu,Zenglin Shi*

Main category: cs.CL

TL;DR: 提出InitERC，一个简单有效的一阶段上下文指令调优框架，用于对话情感识别，通过上下文学习实现说话人-上下文-情感的对齐。


<details>
  <summary>Details</summary>
Motivation: 现有的多阶段指令调优方法无法联合捕捉说话人特征和对话上下文之间的动态交互，导致在统一框架内说话人身份、上下文线索和情感状态之间的对齐效果不佳。

Method: 提出InitERC框架，包含四个组件：演示池构建、上下文示例选择、提示模板设计和上下文指令调优。通过单阶段上下文指令调优让大语言模型从上下文示例中学习说话人-上下文-情感对齐。

Result: 在三个广泛使用的数据集上进行大量实验，证明InitERC相比最先进的基线方法取得了显著改进。

Conclusion: InitERC通过单阶段上下文指令调优有效解决了说话人特征和对话上下文的联合建模问题，在情感识别任务上表现出优越性能。

Abstract: Emotion recognition in conversation (ERC) aims to identify the emotion of
each utterance in a conversation, playing a vital role in empathetic artificial
intelligence. With the growing of large language models (LLMs), instruction
tuning has emerged as a critical paradigm for ERC. Existing studies mainly
focus on multi-stage instruction tuning, which first endows LLMs with speaker
characteristics, and then conducts context-aware instruction tuning to
comprehend emotional states. However, these methods inherently constrains the
capacity to jointly capture the dynamic interaction between speaker
characteristics and conversational context, resulting in weak alignment among
speaker identity, contextual cues, and emotion states within a unified
framework. In this paper, we propose InitERC, a simple yet effective one-stage
in-context instruction tuning framework for ERC. InitERC adapts LLMs to learn
speaker-context-emotion alignment from context examples via in-context
instruction tuning. Specifically, InitERC comprises four components, i.e.,
demonstration pool construction, in-context example selection, prompt template
design, and in-context instruction tuning. To explore the impact of in-context
examples, we conduct a comprehensive study on three key factors: retrieval
strategy, example ordering, and the number of examples. Extensive experiments
on three widely used datasets demonstrate that our proposed InitERC achieves
substantial improvements over the state-of-the-art baselines.

</details>


### [13] [CORE: Measuring Multi-Agent LLM Interaction Quality under Game-Theoretic Pressures](https://arxiv.org/abs/2508.11915)
*Punya Syon Pandey,Yongjin Yang,Jiarui Liu,Zhijing Jin*

Main category: cs.CL

TL;DR: 提出CORE指标来量化多游戏互动中LLM语言使用效果，发现合作环境下语言重复性更高但词汇扩展更快，竞争环境则相反


<details>
  <summary>Details</summary>
Motivation: 当前对LLM多游戏互动中语言多样性的量化不足，需要一个综合指标来评估对话质量

Method: 提出CORE指标，结合聚类熵、词汇重复和语义相似性，并基于Zipf和Heaps定律分析不同游戏环境下的词频分布和词汇增长

Result: 合作环境呈现更深的Zipf分布和更高的Heaps指数（更多重复但词汇扩展更快），竞争环境则相反（更少重复但词汇更受限制）

Conclusion: 社会激励影响语言适应，CORE指标可作为多游戏LLM系统语言稳健性的综合评估工具

Abstract: Game-theoretic interactions between agents with Large Language Models (LLMs)
have revealed many emergent capabilities, yet the linguistic diversity of these
interactions has not been sufficiently quantified. In this paper, we present
the Conversational Robustness Evaluation Score: CORE, a metric to quantify the
effectiveness of language use within multi-agent systems across different
game-theoretic interactions. CORE integrates measures of cluster entropy,
lexical repetition, and semantic similarity, providing a direct lens of dialog
quality. We apply CORE to pairwise LLM dialogs across competitive, cooperative,
and neutral settings, further grounding our analysis in Zipf's and Heaps' Laws
to characterize word frequency distributions and vocabulary growth. Our
findings show that cooperative settings exhibit both steeper Zipf distributions
and higher Heap exponents, indicating more repetition alongside greater
vocabulary expansion. In contrast, competitive interactions display lower Zipf
and Heaps exponents, reflecting less repetition and more constrained
vocabularies. These results provide new insights into how social incentives
influence language adaptation, and highlight CORE as a robust diagnostic for
measuring linguistic robustness in multi-agent LLM systems. Our code is
available at https://github.com/psyonp/core.

</details>


### [14] [LLMs Struggle with NLI for Perfect Aspect: A Cross-Linguistic Study in Chinese and Japanese](https://arxiv.org/abs/2508.11927)
*Jie Lu,Du Jin,Hitomi Yanaka*

Main category: cs.CL

TL;DR: 中日语言缺乏完美体的明确时态标记，增加了自然语言推理难度。研究构建了语言学动机模板数据集，发现高级LLM在时态推理上表现较差，尤其在细微时态变化检测方面。


<details>
  <summary>Details</summary>
Motivation: 中文和日语不像英语那样有明确的完美体时态语法形式，这给自然语言推理(NLI)带来了特别挑战。研究者想要探索语言模型在这些语言中的时态语义理解能力。

Method: 构建了一个语言学动机的模板基础NLI数据集，每种语言包含1,350对语料。重点关注完美体在中文和日语中的表达。

Result: 实验结果显示，即使是先进的大语言模型(LLM)也在时态推理任务上表现困难，尤其在检测细微的时态和参考时间变化方面。

Conclusion: 这些发现显示了模型在时态语义理解方面的限制，并强调了跨语言评估在时态语义研究中的重要性。研究数据集已开源提供。

Abstract: Unlike English, which uses distinct forms (e.g., had, has, will have) to mark
the perfect aspect across tenses, Chinese and Japanese lack separate
grammatical forms for tense within the perfect aspect, which complicates
Natural Language Inference (NLI). Focusing on the perfect aspect in these
languages, we construct a linguistically motivated, template-based NLI dataset
(1,350 pairs per language). Experiments reveal that even advanced LLMs struggle
with temporal inference, particularly in detecting subtle tense and
reference-time shifts. These findings highlight model limitations and
underscore the need for cross-linguistic evaluation in temporal semantics. Our
dataset is available at https://github.com/Lujie2001/CrossNLI.

</details>


### [15] [CAMF: Collaborative Adversarial Multi-agent Framework for Machine Generated Text Detection](https://arxiv.org/abs/2508.11933)
*Yue Wang,Liesheng Wei,Yuxiang Wang*

Main category: cs.CL

TL;DR: 基于多治理协作对抗框架CAMF，通过多维语言特征提取、对抗性一致性探测和综合判断聚合的三阶段过程，显著提升了零样本机器生成文本检测的性能


<details>
  <summary>Details</summary>
Motivation: 当前零样本检测方法存在两大问题：(1)对文本属性的分析浅层有限，(2)缺乏对语言多维度（风格、语义、逻辑）一致性的研究

Method: 提出CAMF框架，使用多个LLM基于的治理在三个协同阶段中工作：多维语言特征提取、对抗性一致性探测、综合判断聚合

Result: 经验评估证明CAMF在零样本MGT检测方面显著优于当前最先进技术

Conclusion: CAMF通过结构化的协作-对抗过程，能够深入分析文本中细微的跨维度不协调性，为检测非人类生成文本提供了有效方法

Abstract: Detecting machine-generated text (MGT) from contemporary Large Language
Models (LLMs) is increasingly crucial amid risks like disinformation and
threats to academic integrity. Existing zero-shot detection paradigms, despite
their practicality, often exhibit significant deficiencies. Key challenges
include: (1) superficial analyses focused on limited textual attributes, and
(2) a lack of investigation into consistency across linguistic dimensions such
as style, semantics, and logic. To address these challenges, we introduce the
\textbf{C}ollaborative \textbf{A}dversarial \textbf{M}ulti-agent
\textbf{F}ramework (\textbf{CAMF}), a novel architecture using multiple
LLM-based agents. CAMF employs specialized agents in a synergistic three-phase
process: \emph{Multi-dimensional Linguistic Feature Extraction},
\emph{Adversarial Consistency Probing}, and \emph{Synthesized Judgment
Aggregation}. This structured collaborative-adversarial process enables a deep
analysis of subtle, cross-dimensional textual incongruities indicative of
non-human origin. Empirical evaluations demonstrate CAMF's significant
superiority over state-of-the-art zero-shot MGT detection techniques.

</details>


### [16] [Learning Wisdom from Errors: Promoting LLM's Continual Relation Learning through Exploiting Error Cases](https://arxiv.org/abs/2508.12031)
*Shaozhe Yin,Jinyu Guo,Kai Shuang,Xia Liu,Ruize Ou*

Main category: cs.CL

TL;DR: 通过指令基对比微调方法，专门利用错误案例来缓解大语言模型在持续关系提取中的忘危问题


<details>
  <summary>Details</summary>
Motivation: 现有持续关系提取方法没有充分利用能更有效反映模型认知偏差的错误案例

Method: 将每个任务的训练和记忆数据按初始响应正确性分为两部分，通过双任务微调匹配处理，使用指令基对比微调策略持续缓解认知偏差

Result: 在TACRED和FewRel数据集上达到新的最高水平，显著提升了持续关系提取性能

Conclusion: 专门利用错误案例在缓解大语言模型忘危问题中具有重要价值

Abstract: Continual Relation Extraction (CRE) aims to continually learn new emerging
relations while avoiding catastrophic forgetting. Existing CRE methods mainly
use memory replay and contrastive learning to mitigate catastrophic forgetting.
However, these methods do not attach importance to the error cases that can
reveal the model's cognitive biases more effectively. To address this issue, we
propose an instruction-based continual contrastive tuning approach for Large
Language Models (LLMs) in CRE. Different from existing CRE methods that
typically handle the training and memory data in a unified manner, this
approach splits the training and memory data of each task into two parts
respectively based on the correctness of the initial responses and treats them
differently through dual-task fine-tuning. In addition, leveraging the
advantages of LLM's instruction-following ability, we propose a novel
instruction-based contrastive tuning strategy for LLM to continuously correct
current cognitive biases with the guidance of previous data in an
instruction-tuning manner, which mitigates the gap between old and new
relations in a more suitable way for LLMs. We experimentally evaluate our model
on TACRED and FewRel, and the results show that our model achieves new
state-of-the-art CRE performance with significant improvements, demonstrating
the importance of specializing in exploiting error cases.

</details>


### [17] [Mind the Generation Process: Fine-Grained Confidence Estimation During LLM Generation](https://arxiv.org/abs/2508.12040)
*Jinyi Han,Tingyun Li,Shisong Chen,Jie Shi,Xinyi Wang,Guanglei Yue,Jiaqing Liang,Xin Lin,Liqian Wen,Zulong Chen,Yanghua Xiao*

Main category: cs.CL

TL;DR: FineCE是一种新颖的细粒度置信度估计方法，通过监督学习和后向置信度集成策略，在文本生成过程中提供准确的连续置信度分数，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型缺乏自我意识，经常对错误预测表现出过度自信，现有置信度估计方法只能提供粗粒度的评分，无法在生成过程中提供细粒度的连续置信度估计。

Method: 首先构建训练数据管道捕获LLM响应的概率分布，然后以监督方式训练模型预测任意文本序列的置信度分数。提出后向置信度集成(BCI)策略利用后续文本信息增强当前序列的置信度估计，并引入三种策略确定生成过程中的最佳置信度估计位置。

Result: 在多个基准数据集上的广泛实验表明，FineCE始终优于现有的经典置信度估计方法。

Conclusion: FineCE通过细粒度的置信度估计显著提高了LLM生成输出的可信度和可靠性，为解决LLM过度自信问题提供了有效解决方案。

Abstract: While large language models (LLMs) have demonstrated remarkable performance
across diverse tasks, they fundamentally lack self-awareness and frequently
exhibit overconfidence, assigning high confidence scores to incorrect
predictions. Accurate confidence estimation is therefore critical for enhancing
the trustworthiness and reliability of LLM-generated outputs. However, existing
approaches suffer from coarse-grained scoring mechanisms that fail to provide
fine-grained, continuous confidence estimates throughout the generation
process. To address these limitations, we introduce FineCE, a novel confidence
estimation method that delivers accurate, fine-grained confidence scores during
text generation. Specifically, we first develop a comprehensive pipeline for
constructing training data that effectively captures the underlying
probabilistic distribution of LLM responses, and then train a model to predict
confidence scores for arbitrary text sequences in a supervised manner.
Furthermore, we propose a Backward Confidence Integration (BCI) strategy that
leverages information from the subsequent text to enhance confidence estimation
for the current sequence during inference. We also introduce three strategies
for identifying optimal positions to perform confidence estimation within the
generation process. Extensive experiments on multiple benchmark datasets
demonstrate that FineCE consistently outperforms existing classical confidence
estimation methods. Our code and all baselines used in the paper are available
on GitHub.

</details>


### [18] [J6: Jacobian-Driven Role Attribution for Multi-Objective Prompt Optimization in LLMs](https://arxiv.org/abs/2508.12086)
*Yao Wu*

Main category: cs.CL

TL;DR: J6方法通过雅可比矩阵分解为六个可解释组件，解决LLM多目标优化中的冲突问题，提供硬决策和软策略的动态更新框架


<details>
  <summary>Details</summary>
Motivation: 现有LLM多目标优化方法依赖标量梯度聚合，忽略了目标与参数间的几何结构，难以平衡事实性提升和置信度增加等冲突目标

Method: 提出结构化雅可比方法J6，将梯度交互矩阵分解为六个可解释组件，支持argmax硬决策和softmax软策略的动态更新框架

Result: J6方法提供了参数归因、任务干扰和几何对齐适应的可解释性洞察，能够根据局部冲突和协同关系自适应调整

Conclusion: J6为冲突感知的提示优化提供了原则性和可扩展机制，开辟了将结构化雅可比推理融入多目标神经调优的新途径

Abstract: In large language model (LLM) adaptation, balancing multiple optimization
objectives such as improving factuality (heat) and increasing confidence (via
low entropy) poses a fundamental challenge, especially when prompt parameters
(e.g., hidden-layer insertions h and embedding modifications w) interact in
non-trivial ways. Existing multi-objective optimization strategies often rely
on scalar gradient aggregation, ignoring the deeper geometric structure between
objectives and parameters. We propose J6, a structured Jacobian-based method
that decomposes the gradient interaction matrix into six interpretable
components. This decomposition enables both hard decision-making (e.g.,
choosing the dominant update direction via argmax) and soft strategies (e.g.,
attention-style weighting via softmax over J6), forming a dynamic update
framework that adapts to local conflict and synergy. Moreover, the
interpretable structure of J6 provides insight into parameter attribution, task
interference, and geometry-aligned adaptation. Our work introduces a principled
and extensible mechanism for conflict-aware prompt optimization, and opens a
new avenue for incorporating structured Jacobian reasoning into multi-objective
neural tuning.

</details>


### [19] [STEM: Efficient Relative Capability Evaluation of LLMs through Structured Transition Samples](https://arxiv.org/abs/2508.12096)
*Haiquan Hu,Jiazhi Jiang,Shiyou Xu,Ruhan Zeng,Tian Wang*

Main category: cs.CL

TL;DR: STEM是一个轻量级、可解释的LLM评估框架，通过分析同架构不同参数规模模型间的性能转变来识别关键样本，从而高效估计未知模型的能力位置。


<details>
  <summary>Details</summary>
Motivation: 传统基准测试存在过拟合问题，计算成本高，难以有效区分模型间的真实能力差异，需要更高效、可扩展的评估方法。

Method: 提出STEM框架，通过识别显著转变样本(STS)来分析同架构不同参数规模LLM间的性能一致性转变，构建STS池来估计未知模型能力。

Result: 在六个多样化基准测试上应用Qwen3模型族构建STS池，实验表明STEM能可靠捕捉性能趋势，与真实模型能力排名一致。

Conclusion: STEM是一种实用且可扩展的细粒度、架构无关的LLM评估方法，能有效解决传统评估的局限性。

Abstract: Evaluating large language models (LLMs) has become increasingly challenging
as model capabilities advance rapidly. While recent models often achieve higher
scores on standard benchmarks, these improvements do not consistently reflect
enhanced real-world reasoning capabilities. Moreover, widespread overfitting to
public benchmarks and the high computational cost of full evaluations have made
it both expensive and less effective to distinguish meaningful differences
between models. To address these challenges, we propose the \textbf{S}tructured
\textbf{T}ransition \textbf{E}valuation \textbf{M}ethod (STEM), a lightweight
and interpretable evaluation framework for efficiently estimating the relative
capabilities of LLMs. STEM identifies \textit{significant transition samples}
(STS) by analyzing consistent performance transitions among LLMs of the same
architecture but varying parameter scales. These samples enable STEM to
effectively estimate the capability position of an unknown model. Qwen3 model
family is applied to construct the STS pool on six diverse and representative
benchmarks. To assess generalizability. Experimental results indicate that STEM
reliably captures performance trends, aligns with ground-truth rankings of
model capability. These findings highlight STEM as a practical and scalable
method for fine-grained, architecture-agnostic evaluation of LLMs.

</details>


### [20] [Exploring Efficiency Frontiers of Thinking Budget in Medical Reasoning: Scaling Laws between Computational Resources and Reasoning Quality](https://arxiv.org/abs/2508.12140)
*Ziqian Bi,Lu Chen,Junhao Song,Hongying Luo,Enze Ge,Junmin Huang,Tianyang Wang,Keyu Chen,Chia Xin Liang,Zihan Wei,Huafeng Liu,Chunjie Tian,Jibin Guan,Joe Yeong,Yongzhi Xu,Peng Wang,Junfeng Hao*

Main category: cs.CL

TL;DR: 这研究首次系统评估医疗理由任务中的思维预算机制，发现计算资源与理由质量存在对数缩放关系，并确定了三种效率模式适用于不同临床场景。


<details>
  <summary>Details</summary>
Motivation: 理解医疗AI系统中思维预算机制对理由质量的影响，以优化资源分配并提高临床部署效率。

Method: 系统性评估Qwen3和DeepSeek-R1两大模型家族，涉及1.5B到235B参数范围，在15个医疗数据集上进行思维预算实验（从零到无限令牌）。

Result: 发现准确率改善与思维预算和模型大小呈对数关系；小模型从扩展思维中获益更大（收益比15-20%）；不同医学专科需要不同深度的理由过程。

Conclusion: 思维预算控制是优化医疗AI系统的关键机制，能够根据临床需求动态分配资源，同时保持适合医疗部署的透明性。

Abstract: This study presents the first comprehensive evaluation of thinking budget
mechanisms in medical reasoning tasks, revealing fundamental scaling laws
between computational resources and reasoning quality. We systematically
evaluated two major model families, Qwen3 (1.7B to 235B parameters) and
DeepSeek-R1 (1.5B to 70B parameters), across 15 medical datasets spanning
diverse specialties and difficulty levels. Through controlled experiments with
thinking budgets ranging from zero to unlimited tokens, we establish
logarithmic scaling relationships where accuracy improvements follow a
predictable pattern with both thinking budget and model size. Our findings
identify three distinct efficiency regimes: high-efficiency (0 to 256 tokens)
suitable for real-time applications, balanced (256 to 512 tokens) offering
optimal cost-performance tradeoffs for routine clinical support, and
high-accuracy (above 512 tokens) justified only for critical diagnostic tasks.
Notably, smaller models demonstrate disproportionately larger benefits from
extended thinking, with 15 to 20% improvements compared to 5 to 10% for larger
models, suggesting a complementary relationship where thinking budget provides
greater relative benefits for capacity-constrained models. Domain-specific
patterns emerge clearly, with neurology and gastroenterology requiring
significantly deeper reasoning processes than cardiovascular or respiratory
medicine. The consistency between Qwen3 native thinking budget API and our
proposed truncation method for DeepSeek-R1 validates the generalizability of
thinking budget concepts across architectures. These results establish thinking
budget control as a critical mechanism for optimizing medical AI systems,
enabling dynamic resource allocation aligned with clinical needs while
maintaining the transparency essential for healthcare deployment.

</details>


### [21] [LLM-as-a-Judge for Privacy Evaluation? Exploring the Alignment of Human and LLM Perceptions of Privacy in Textual Data](https://arxiv.org/abs/2508.12158)
*Stephen Meisenbacher,Alexandra Klymenko,Florian Matthes*

Main category: cs.CL

TL;DR: 使用LLM作为隐私评估器的可行性研究，LLM能够模拟全局人类隐私观点，但隐私本身评估存在困难


<details>
  <summary>Details</summary>
Motivation: 解决隐私保护NLP领域中隐私评估的挑战，测试LLM-as-a-Judge模式在隐私敏感性评估中的效果

Method: 使用10个数据集、13个LLM模型和677名人类参与者进行研究，比较LLM与人类在文本隐私评估上的一致性

Result: 人类之间隐私评估一致性较低，但LLM能够准确模拟全局人类隐私观点，展现了LLM-as-a-Judge在隐私评估中的潜力

Conclusion: LLM作为隐私评估器具有可行性，为解决隐私问题提供了创新技术解决方案的基础

Abstract: Despite advances in the field of privacy-preserving Natural Language
Processing (NLP), a significant challenge remains the accurate evaluation of
privacy. As a potential solution, using LLMs as a privacy evaluator presents a
promising approach $\unicode{x2013}$ a strategy inspired by its success in
other subfields of NLP. In particular, the so-called $\textit{LLM-as-a-Judge}$
paradigm has achieved impressive results on a variety of natural language
evaluation tasks, demonstrating high agreement rates with human annotators.
Recognizing that privacy is both subjective and difficult to define, we
investigate whether LLM-as-a-Judge can also be leveraged to evaluate the
privacy sensitivity of textual data. Furthermore, we measure how closely LLM
evaluations align with human perceptions of privacy in text. Resulting from a
study involving 10 datasets, 13 LLMs, and 677 human survey participants, we
confirm that privacy is indeed a difficult concept to measure empirically,
exhibited by generally low inter-human agreement rates. Nevertheless, we find
that LLMs can accurately model a global human privacy perspective, and through
an analysis of human and LLM reasoning patterns, we discuss the merits and
limitations of LLM-as-a-Judge for privacy evaluation in textual data. Our
findings pave the way for exploring the feasibility of LLMs as privacy
evaluators, addressing a core challenge in solving pressing privacy issues with
innovative technical solutions.

</details>


### [22] [Arabic Multimodal Machine Learning: Datasets, Applications, Approaches, and Challenges](https://arxiv.org/abs/2508.12227)
*Abdelhamid Haouhat,Slimane Bellaouar,Attia Nehar,Hadda Cherroun,Ahmed Abdelali*

Main category: cs.CL

TL;DR: 阿拉伯多模态机器学习综述文章，通过新的分类法对该领域进行了结构化分析和总结


<details>
  <summary>Details</summary>
Motivation: 阿拉伯多模态机器学习已经发展到一定成熟阶段，需要进行全面的综述性研究来总结当前状况和指明未来方向

Method: 使用新的分类法将阿拉伯MML研究分为四个主要话题：数据集、应用场景、方法接口和挑战难题，并对现有研究进行深入分析

Result: 提供了阿拉伯MML领域的结构化概览，识别出了尚未被涉及的研究区域和重要的研究空白

Conclusion: 该综述文章为研究人员提供了建议性的研究框架，帮助他们基于识别的机遇和挑战来推动阿拉伯多模态机器学习领域的发展

Abstract: Multimodal Machine Learning (MML) aims to integrate and analyze information
from diverse modalities, such as text, audio, and visuals, enabling machines to
address complex tasks like sentiment analysis, emotion recognition, and
multimedia retrieval. Recently, Arabic MML has reached a certain level of
maturity in its foundational development, making it time to conduct a
comprehensive survey. This paper explores Arabic MML by categorizing efforts
through a novel taxonomy and analyzing existing research. Our taxonomy
organizes these efforts into four key topics: datasets, applications,
approaches, and challenges. By providing a structured overview, this survey
offers insights into the current state of Arabic MML, highlighting areas that
have not been investigated and critical research gaps. Researchers will be
empowered to build upon the identified opportunities and address challenges to
advance the field.

</details>


### [23] [SEA-BED: Southeast Asia Embedding Benchmark](https://arxiv.org/abs/2508.12243)
*Wuttikorn Ponwitayarat,Raymond Ng,Jann Railey Montalan,Thura Aung,Jian Gang Ngui,Yosephine Susanto,William Tjhi,Panuthep Tasawong,Erik Cambria,Ekapol Chuangsuwanich,Sarana Nutanong,Peerat Limkonchotiwat*

Main category: cs.CL

TL;DR: 首个专门针对东南亚语言的大规模嵌入模型评测标准SEA-BED，包含169个数据集、9项任务和10种语言，71%人工构建，发现模型在东南亚语言上表现不稳定且需要人工精检数据集来提升低资源语言性能。


<details>
  <summary>Details</summary>
Motivation: 东南亚地区近7亿人口但缺乏专门的嵌入模型评测标准，现有多语言标准如MMTEB中SEA数据集稀缺且多为机器翻译，失去了本土语言特性。

Method: 构建SEA-BED标准，包含169个数据集、9项任务和10种东南亚语言，其71%人工构建。对17个嵌入模型进行六项研究分析，涉及任务难度、语言问题、跨标准对比和翻译方式影响。

Result: 发现模型在东南亚语言上排名显著变化，表现不一致，尤其是对例如缅甸语等低资源语言，人工精检数据集对性能提升至关重要。

Conclusion: SEA-BED填补了东南亚嵌入模型评测的空白，证明了语言特定标准的必要性，并强调人工构建本土化数据集对低资源语言NLP任务的关键作用。

Abstract: Sentence embeddings are essential for NLP tasks such as semantic search,
re-ranking, and textual similarity. Although multilingual benchmarks like MMTEB
broaden coverage, Southeast Asia (SEA) datasets are scarce and often
machine-translated, missing native linguistic properties. With nearly 700
million speakers, the SEA region lacks a region-specific embedding benchmark.
We introduce SEA-BED, the first large-scale SEA embedding benchmark with 169
datasets across 9 tasks and 10 languages, where 71% are formulated by humans,
not machine generation or translation. We address three research questions: (1)
which SEA languages and tasks are challenging, (2) whether SEA languages show
unique performance gaps globally, and (3) how human vs. machine translations
affect evaluation. We evaluate 17 embedding models across six studies,
analyzing task and language challenges, cross-benchmark comparisons, and
translation trade-offs. Results show sharp ranking shifts, inconsistent model
performance among SEA languages, and the importance of human-curated datasets
for low-resource languages like Burmese.

</details>


### [24] [What do Speech Foundation Models Learn? Analysis and Applications](https://arxiv.org/abs/2508.12255)
*Ankita Pasad*

Main category: cs.CL

TL;DR: 这篇论文提出了一种轻量级的分析框架，用统计工具和无需训练的任务来研究语音基础模型（SFMs）中编码的音响和语言知识，并为口语语言理解任务提供了新的数据集和方法。


<details>
  <summary>Details</summary>
Motivation: 虽然语音基础模型在各种下游任务上表现出艰，但对它们所获得知识的理解远远迟后。同时，对于需要深层理解的口语语言理解任务，相关数据集的缺乏限制了研究。

Method: 使用统计工具和无需训练的任务来分析SFM各层编码的知识；为口语语言理解评测标准提供了口语命名实体识别和命名实体定位任务；开发了基于SFM的端到端模型方法。

Result: 研究发现分析见解对下游任务性能有具体影响；端到端模型能够超越传统的流水线方法（语音识别+文本模型）。

Conclusion: 这份论文解决了关于SFM的之前未解决问题，提供了工具和数据集来深化我们的理解，并为未来模型开发和采用提供了有信息支撑的设计选择。

Abstract: Speech foundation models (SFMs) are designed to serve as general-purpose
representations for a wide range of speech-processing tasks. The last five
years have seen an influx of increasingly successful self-supervised and
supervised pre-trained models with impressive performance on various downstream
tasks.
  Although the zoo of SFMs continues to grow, our understanding of the
knowledge they acquire lags behind. This thesis presents a lightweight analysis
framework using statistical tools and training-free tasks to investigate the
acoustic and linguistic knowledge encoded in SFM layers. We conduct a
comparative study across multiple SFMs and statistical tools. Our study also
shows that the analytical insights have concrete implications for downstream
task performance.
  The effectiveness of an SFM is ultimately determined by its performance on
speech applications. Yet it remains unclear whether the benefits extend to
spoken language understanding (SLU) tasks that require a deeper understanding
than widely studied ones, such as speech recognition. The limited exploration
of SLU is primarily due to a lack of relevant datasets. To alleviate that, this
thesis contributes tasks, specifically spoken named entity recognition (NER)
and named entity localization (NEL), to the Spoken Language Understanding
Evaluation benchmark. We develop SFM-based approaches for NER and NEL, and find
that end-to-end (E2E) models leveraging SFMs can surpass traditional cascaded
(speech recognition followed by a text model) approaches. Further, we evaluate
E2E SLU models across SFMs and adaptation strategies to assess the impact on
task performance.
  Collectively, this thesis tackles previously unanswered questions about SFMs,
providing tools and datasets to further our understanding and to enable the
community to make informed design choices for future model development and
adoption.

</details>


### [25] [Structuring the Unstructured: A Systematic Review of Text-to-Structure Generation for Agentic AI with a Universal Evaluation Framework](https://arxiv.org/abs/2508.12257)
*Zheye Deng,Chunkit Chan,Tianshi Zheng,Wei Fan,Weiqi Wang,Yangqiu Song*

Main category: cs.CL

TL;DR: 本文对文本到结构化转换技术进行了系统性综述，分析了方法、数据集和评估指标，并提出了通用评估框架。


<details>
  <summary>Details</summary>
Motivation: AI系统向代理操作和上下文感知检索发展需要将非结构化文本转换为表格、知识图谱和图表等结构化格式，但目前缺乏对方法、数据集和指标的综合分析。

Method: 采用系统性综述方法，分析文本到结构转换技术、现有数据集和评估标准，并引入通用评估框架。

Result: 建立了文本到结构转换作为下一代AI系统基础架构的地位，系统梳理了该领域的技术挑战和发展现状。

Conclusion: 文本到结构转换是AI系统发展的关键技术，需要建立统一的评估标准和框架来推动该领域的进一步发展。

Abstract: The evolution of AI systems toward agentic operation and context-aware
retrieval necessitates transforming unstructured text into structured formats
like tables, knowledge graphs, and charts. While such conversions enable
critical applications from summarization to data mining, current research lacks
a comprehensive synthesis of methodologies, datasets, and metrics. This
systematic review examines text-to-structure techniques and the encountered
challenges, evaluates current datasets and assessment criteria, and outlines
potential directions for future research. We also introduce a universal
evaluation framework for structured outputs, establishing text-to-structure as
foundational infrastructure for next-generation AI systems.

</details>


### [26] [Fast, Slow, and Tool-augmented Thinking for LLMs: A Review](https://arxiv.org/abs/2508.12265)
*Xinda Jia,Jinpeng Li,Zezhong Wang,Jingjing Li,Xingshan Zeng,Yasheng Wang,Weinan Zhang,Yong Yu,Weiwen Liu*

Main category: cs.CL

TL;DR: 该论文提出了一个基于认知心理学的新颖LLM推理策略分类法，包含快/慢边界和内部/外部边界两个维度，系统性地调研了自适应推理方法并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 现实世界任务需要LLMs能够根据问题需求自适应地选择推理策略，从快速直观响应到深思熟虑的分步推理和工具增强思考，但目前缺乏系统性的分类框架。

Method: 基于认知心理学提出双维度分类法：快/慢边界（直觉vs审慎过程）和内部/外部边界（参数内推理vs外部工具增强推理），并系统性地调研和分类现有自适应推理方法。

Result: 建立了一个全面的LLM推理策略分类框架，为理解不同推理方法的特性和适用场景提供了系统性的理论基础。

Conclusion: 该分类法为开发更自适应、高效和可靠的LLMs提供了重要指导，同时指出了该领域面临的开放挑战和未来研究方向。

Abstract: Large Language Models (LLMs) have demonstrated remarkable progress in
reasoning across diverse domains. However, effective reasoning in real-world
tasks requires adapting the reasoning strategy to the demands of the problem,
ranging from fast, intuitive responses to deliberate, step-by-step reasoning
and tool-augmented thinking. Drawing inspiration from cognitive psychology, we
propose a novel taxonomy of LLM reasoning strategies along two knowledge
boundaries: a fast/slow boundary separating intuitive from deliberative
processes, and an internal/external boundary distinguishing reasoning grounded
in the model's parameters from reasoning augmented by external tools. We
systematically survey recent work on adaptive reasoning in LLMs and categorize
methods based on key decision factors. We conclude by highlighting open
challenges and future directions toward more adaptive, efficient, and reliable
LLMs.

</details>


### [27] [The Self-Execution Benchmark: Measuring LLMs' Attempts to Overcome Their Lack of Self-Execution](https://arxiv.org/abs/2508.12277)
*Elon Ezra,Ariel Weizman,Amos Azaria*

Main category: cs.CL

TL;DR: 本文提出了自我执行基准测试，评估大语言模型预测自身响应特性的能力，发现模型普遍表现不佳且模型规模增大不一定提升性能，揭示了LLM在自我行为认知方面的根本局限性。


<details>
  <summary>Details</summary>
Motivation: 传统评估主要测试LLM的知识和推理能力，本文探索不同的评估维度：LLM是否能预测自身响应的特性，以了解模型对自身行为的认知能力。

Method: 引入自我执行基准测试，通过设计实验让模型预测其输出的各种特性，包括问题难度预测、拒绝回答可能性预测、以及可能产生的关联类型预测等。

Result: 实验结果显示模型在这一基准测试上普遍表现不佳，模型规模或能力的增加并不总是带来性能提升，表明LLM在自我行为表征和推理方面存在根本性局限。

Conclusion: 大语言模型在预测自身响应特性方面存在显著局限性，这种自我认知能力的缺失揭示了当前LLM架构的内在缺陷，对模型自我意识和元认知能力的发展提出了重要挑战。

Abstract: Large language models (LLMs) are commonly evaluated on tasks that test their
knowledge or reasoning abilities. In this paper, we explore a different type of
evaluation: whether an LLM can predict aspects of its own responses. Since LLMs
lack the ability to execute themselves, we introduce the Self-Execution
Benchmark, which measures a model's ability to anticipate properties of its
output, such as whether a question will be difficult for it, whether it will
refuse to answer, or what kinds of associations it is likely to produce. Our
experiments show that models generally perform poorly on this benchmark, and
that increased model size or capability does not consistently lead to better
performance. These results suggest a fundamental limitation in how LLMs
represent and reason about their own behavior.

</details>


### [28] [Legal$Δ$: Enhancing Legal Reasoning in LLMs via Reinforcement Learning with Chain-of-Thought Guided Information Gain](https://arxiv.org/abs/2508.12281)
*Xin Dai,Buqiang Xu,Zhenghao Liu,Yukun Yan,Huiyuan Xie,Xiaoyuan Yi,Shuo Wang,Ge Yu*

Main category: cs.CL

TL;DR: LegalΔ是一个强化学习框架，通过思维链引导的信息增益来增强法律推理能力，在多个法律推理任务上实现了更高的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有法律大语言模型在生成可靠和可解释的推理过程方面存在困难，往往直接给出答案而缺乏多步推理，限制了在复杂法律场景中的有效性。

Method: 采用双模式输入（直接答案模式和推理增强模式），最大化两者间的信息增益；两阶段方法：从DeepSeek-R1提炼潜在推理能力，通过差分比较和多维奖励机制精炼推理质量。

Result: 在多个法律推理任务上优于强基线模型，在准确性和可解释性方面均有提升，能够产生更稳健和可信的法律判断，且无需依赖标注的偏好数据。

Conclusion: LegalΔ框架成功解决了法律AI中推理过程不可靠的问题，通过强化学习和信息增益机制有效提升了法律推理的质量和可解释性。

Abstract: Legal Artificial Intelligence (LegalAI) has achieved notable advances in
automating judicial decision-making with the support of Large Language Models
(LLMs). However, existing legal LLMs still struggle to generate reliable and
interpretable reasoning processes. They often default to fast-thinking behavior
by producing direct answers without explicit multi-step reasoning, limiting
their effectiveness in complex legal scenarios that demand rigorous
justification. To address this challenge, we propose Legal$\Delta$, a
reinforcement learning framework designed to enhance legal reasoning through
chain-of-thought guided information gain. During training, Legal$\Delta$
employs a dual-mode input setup-comprising direct answer and
reasoning-augmented modes-and maximizes the information gain between them. This
encourages the model to acquire meaningful reasoning patterns rather than
generating superficial or redundant explanations. Legal$\Delta$ follows a
two-stage approach: (1) distilling latent reasoning capabilities from a
powerful Large Reasoning Model (LRM), DeepSeek-R1, and (2) refining reasoning
quality via differential comparisons, combined with a multidimensional reward
mechanism that assesses both structural coherence and legal-domain specificity.
Experimental results on multiple legal reasoning tasks demonstrate that
Legal$\Delta$ outperforms strong baselines in both accuracy and
interpretability. It consistently produces more robust and trustworthy legal
judgments without relying on labeled preference data. All code and data will be
released at https://github.com/NEUIR/LegalDelta.

</details>


### [29] [A Question Answering Dataset for Temporal-Sensitive Retrieval-Augmented Generation](https://arxiv.org/abs/2508.12282)
*Ziyang Chen,Erxue Min,Xiang Zhao,Yunxin Li,Xin Jia,Jinzhi Liao,Jichao Li,Shuaiqiang Wang,Baotian Hu,Dawei Yin*

Main category: cs.CL

TL;DR: ChronoQA是一个专门用于评估检索增强生成系统时间推理能力的大规模中文问答基准数据集，包含5,176个高质量问题，覆盖多种时间类型和表达方式。


<details>
  <summary>Details</summary>
Motivation: 现有的问答系统在时间推理方面存在不足，特别是在处理时间对齐和逻辑一致性方面。需要构建一个专门针对中文时间敏感问答的可靠基准数据集来推动相关研究发展。

Method: 基于2019-2024年间超过30万篇新闻文章构建数据集，包含绝对时间、聚合时间和相对时间三种类型的问题，支持单文档和多文档场景，并通过规则、大语言模型和人工验证确保数据质量。

Result: 成功构建了一个包含5,176个高质量问题的中文时间问答基准数据集，具有全面的结构标注和多阶段验证机制，为时间敏感的检索增强问答系统提供了动态、可靠和可扩展的评估资源。

Conclusion: ChronoQA作为一个专门针对中文时间推理的基准数据集，能够有效评估检索增强生成系统的时间敏感能力，为推进时间敏感问答系统的发展提供了重要基础。

Abstract: We introduce ChronoQA, a large-scale benchmark dataset for Chinese question
answering, specifically designed to evaluate temporal reasoning in
Retrieval-Augmented Generation (RAG) systems. ChronoQA is constructed from over
300,000 news articles published between 2019 and 2024, and contains 5,176
high-quality questions covering absolute, aggregate, and relative temporal
types with both explicit and implicit time expressions. The dataset supports
both single- and multi-document scenarios, reflecting the real-world
requirements for temporal alignment and logical consistency. ChronoQA features
comprehensive structural annotations and has undergone multi-stage validation,
including rule-based, LLM-based, and human evaluation, to ensure data quality.
By providing a dynamic, reliable, and scalable resource, ChronoQA enables
structured evaluation across a wide range of temporal tasks, and serves as a
robust benchmark for advancing time-sensitive retrieval-augmented question
answering systems.

</details>


### [30] [Incorporating Legal Logic into Deep Learning: An Intelligent Approach to Probation Prediction](https://arxiv.org/abs/2508.12286)
*Qinghua Wang,Xu Zhang,Lingyan Yang,Rui Shao,Bonan Wang,Fang Wang,Cunquan Qu*

Main category: cs.CL

TL;DR: 通过将法律逻辑与深度学习相结合，提出了一种新的罚金预测模型MT-DT，解决了现有智能司法辅助系统中缺乏专门罚金预测方法的问题


<details>
  <summary>Details</summary>
Motivation: 现有智能司法辅助系统缺乏专门的罚金预测方法，而且大部分现有研究仅依赖数据驱动方法，忽视了司法决策的法律逻辑基础

Method: 构建专门的罚金数据集（包含事实描述和罚金法律要素），设计基于罚金法律逻辑和"双轨刑罚理论"的多任务双理论罚金预测模型（MT-DT）

Result: 实验结果显示MT-DT模型在罚金数据集上表现超过基线模型，并通过法律逻辑分析验证了方法的有效性

Conclusion: 整合法律逻辑的深度学习模型能够更有效地预测罚金，为智能司法辅助系统提供了更符合司法逻辑的解决方案

Abstract: Probation is a crucial institution in modern criminal law, embodying the
principles of fairness and justice while contributing to the harmonious
development of society. Despite its importance, the current Intelligent
Judicial Assistant System (IJAS) lacks dedicated methods for probation
prediction, and research on the underlying factors influencing probation
eligibility remains limited. In addition, probation eligibility requires a
comprehensive analysis of both criminal circumstances and remorse. Much of the
existing research in IJAS relies primarily on data-driven methodologies, which
often overlooks the legal logic underpinning judicial decision-making. To
address this gap, we propose a novel approach that integrates legal logic into
deep learning models for probation prediction, implemented in three distinct
stages. First, we construct a specialized probation dataset that includes fact
descriptions and probation legal elements (PLEs). Second, we design a distinct
probation prediction model named the Multi-Task Dual-Theory Probation
Prediction Model (MT-DT), which is grounded in the legal logic of probation and
the \textit{Dual-Track Theory of Punishment}. Finally, our experiments on the
probation dataset demonstrate that the MT-DT model outperforms baseline models,
and an analysis of the underlying legal logic further validates the
effectiveness of the proposed approach.

</details>


### [31] [CarelessWhisper: Turning Whisper into a Causal Streaming Model](https://arxiv.org/abs/2508.12301)
*Tomer Krichli,Bhiksha Raj,Joseph Keshet*

Main category: cs.CL

TL;DR: 通过LoRA细调和困静数据集将Transformer编码器-解码器ASR模型转换为低延迟流式模型，在小于300毫秒的块大小下超越现有非细调流式方法


<details>
  <summary>Details</summary>
Motivation: 虽然OpenAI Whisper等ASR模型在离线识别中表现突出，但其架构和训练方法不适合流式（实时）识别，需要一种方法将这些模型转换为低延迟流式模型

Method: 修改非困果性编码器为困果性编码器，使用低秩适配（LoRA）和弱对齐数据集同时细调编码器和解码器，提出更新的推理机制支持贪婪和材析解码

Result: 在小于300毫秒的低延迟块大小下，细调模型在大多数情况下超过现有非细调流式方法，且复杂度更低，同时训练过程产生了更好的对齐效果，支持单词级时间戳提取

Conclusion: 通过LoRA细调和困静性改造，可以有效将Transformer编码器-解码器ASR模型转换为低延迟流式模型，为流式ASR研究提供了有效解决方案

Abstract: Automatic Speech Recognition (ASR) has seen remarkable progress, with models
like OpenAI Whisper and NVIDIA Canary achieving state-of-the-art (SOTA)
performance in offline transcription. However, these models are not designed
for streaming (online or real-time) transcription, due to limitations in their
architecture and training methodology. We propose a method to turn the
transformer encoder-decoder model into a low-latency streaming model that is
careless about future context. We present an analysis explaining why it is not
straightforward to convert an encoder-decoder transformer to a low-latency
streaming model. Our proposed method modifies the existing (non-causal) encoder
to a causal encoder by fine-tuning both the encoder and decoder using Low-Rank
Adaptation (LoRA) and a weakly aligned dataset. We then propose an updated
inference mechanism that utilizes the fine-tune causal encoder and decoder to
yield greedy and beam-search decoding, and is shown to be locally optimal.
Experiments on low-latency chunk sizes (less than 300 msec) show that our
fine-tuned model outperforms existing non-fine-tuned streaming approaches in
most cases, while using a lower complexity. Additionally, we observe that our
training process yields better alignment, enabling a simple method for
extracting word-level timestamps. We release our training and inference code,
along with the fine-tuned models, to support further research and development
in streaming ASR.

</details>


### [32] [Consensus or Conflict? Fine-Grained Evaluation of Conflicting Answers in Question-Answering](https://arxiv.org/abs/2508.12355)
*Eviatar Nachshoni,Arie Cattan,Shmuel Amar,Ori Shapira,Ido Dagan*

Main category: cs.CL

TL;DR: 提出了NATCONFQA基准测试，用于评估LLM在多答案问答任务中处理冲突答案的能力，发现现有模型在冲突处理方面表现脆弱


<details>
  <summary>Details</summary>
Motivation: 多答案问答任务中可能存在冲突答案，现有数据集构建成本高且多为合成数据，需要更现实的冲突感知评估基准

Method: 利用事实核查数据集构建NATCONFQA基准，要求模型不仅识别所有有效答案，还要检测特定冲突答案对

Result: 评估了8个高端LLM，发现它们在处理各种冲突类型时表现脆弱，并采用了有缺陷的解决策略

Conclusion: 需要开发更强大的冲突处理能力，NATCONFQA为多答案问答研究提供了现实的评估基准

Abstract: Large Language Models (LLMs) have demonstrated strong performance in question
answering (QA) tasks. However, Multi-Answer Question Answering (MAQA), where a
question may have several valid answers, remains challenging. Traditional QA
settings often assume consistency across evidences, but MAQA can involve
conflicting answers. Constructing datasets that reflect such conflicts is
costly and labor-intensive, while existing benchmarks often rely on synthetic
data, restrict the task to yes/no questions, or apply unverified automated
annotation. To advance research in this area, we extend the conflict-aware MAQA
setting to require models not only to identify all valid answers, but also to
detect specific conflicting answer pairs, if any. To support this task, we
introduce a novel cost-effective methodology for leveraging fact-checking
datasets to construct NATCONFQA, a new benchmark for realistic, conflict-aware
MAQA, enriched with detailed conflict labels, for all answer pairs. We evaluate
eight high-end LLMs on NATCONFQA, revealing their fragility in handling various
types of conflicts and the flawed strategies they employ to resolve them.

</details>


### [33] [ReaLM: Reflection-Enhanced Autonomous Reasoning with Small Language Models](https://arxiv.org/abs/2508.12387)
*Yuanfeng Xu,Zehui Dai,Jian Liang,Jiapeng Guan,Guangrun Wang,Liang Lin,Xiaohui Lv*

Main category: cs.CL

TL;DR: ReaLM是一个强化学习框架，通过多路径过程验证、渐进式自主诱导和引导式思维链蒸馏，提升小语言模型在垂直领域的推理能力、自主性和泛化性。


<details>
  <summary>Details</summary>
Motivation: 小语言模型(SLMs)在复杂推理中存在能力有限、容易出错的问题，现有方法往往在推理能力、自主性或泛化性方面存在妥协。

Method: 提出ReaLM框架：1) MRPV对比正负推理路径提取关键模式；2) EAAI通过渐进减少外部信号提升自主性；3) 引导式思维链蒸馏编码领域知识。

Result: 在垂直领域和通用推理任务上的大量实验表明，ReaLM显著提升了SLM在推理能力、自主性和泛化性三个方面的性能。

Conclusion: ReaLM为小语言模型提供了有效的强化学习框架，解决了现有方法在推理能力、自主性和泛化性方面的局限性，在垂直领域表现出色。

Abstract: Small Language Models (SLMs) are a cost-effective alternative to Large
Language Models (LLMs), but often struggle with complex reasoning due to their
limited capacity and a tendency to produce mistakes or inconsistent answers
during multi-step reasoning. Existing efforts have improved SLM performance,
but typically at the cost of one or more of three key aspects: (1) reasoning
capability, due to biased supervision that filters out negative reasoning paths
and limits learning from errors; (2) autonomy, due to over-reliance on
externally generated reasoning signals; and (3) generalization, which suffers
when models overfit to teacher-specific patterns. In this paper, we introduce
ReaLM, a reinforcement learning framework for robust and self-sufficient
reasoning in vertical domains. To enhance reasoning capability, we propose
Multi-Route Process Verification (MRPV), which contrasts both positive and
negative reasoning paths to extract decisive patterns. To reduce reliance on
external guidance and improve autonomy, we introduce Enabling Autonomy via
Asymptotic Induction (EAAI), a training strategy that gradually fades external
signals. To improve generalization, we apply guided chain-of-thought
distillation to encode domain-specific rules and expert knowledge into SLM
parameters, making them part of what the model has learned. Extensive
experiments on both vertical and general reasoning tasks demonstrate that ReaLM
significantly improves SLM performance across aspects (1)-(3) above.

</details>


### [34] [MedKGent: A Large Language Model Agent Framework for Constructing Temporally Evolving Medical Knowledge Graph](https://arxiv.org/abs/2508.12393)
*Duzhen Zhang,Zixiao Wang,Zhong-Zhi Li,Yahan Yu,Shuncheng Jia,Jiahua Dong,Haotian Xu,Xing Wu,Yingying Zhang,Tielin Zhang,Jie Yang,Xiuying Chen,Le Song*

Main category: cs.CL

TL;DR: MedKGent是一个基于LLM代理的框架，用于构建时间演化的医学知识图谱，通过提取和构建代理处理PubMed文献，生成高质量、时间感知的医学知识图谱，在多个下游任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前知识图谱构建方法存在泛化性有限、忽视知识时间动态性和上下文不确定性的问题，需要能够处理生物医学知识演化的解决方案。

Method: 使用Qwen2.5-32B-Instruct模型驱动的两个专业代理：提取代理识别知识三元组并分配置信度，构建代理基于置信度和时间戳增量整合三元组到时间演化图谱中。

Result: 构建了包含156,275个实体和2,971,384个关系三元组的KG，准确率接近90%，在7个医学问答基准测试中显著优于非增强基线。

Conclusion: MedKGent框架成功解决了医学知识图谱构建中的时间动态性问题，为生物医学知识管理和应用提供了有效的解决方案。

Abstract: The rapid expansion of medical literature presents growing challenges for
structuring and integrating domain knowledge at scale. Knowledge Graphs (KGs)
offer a promising solution by enabling efficient retrieval, automated
reasoning, and knowledge discovery. However, current KG construction methods
often rely on supervised pipelines with limited generalizability or naively
aggregate outputs from Large Language Models (LLMs), treating biomedical
corpora as static and ignoring the temporal dynamics and contextual uncertainty
of evolving knowledge. To address these limitations, we introduce MedKGent, a
LLM agent framework for constructing temporally evolving medical KGs.
Leveraging over 10 million PubMed abstracts published between 1975 and 2023, we
simulate the emergence of biomedical knowledge via a fine-grained daily time
series. MedKGent incrementally builds the KG in a day-by-day manner using two
specialized agents powered by the Qwen2.5-32B-Instruct model. The Extractor
Agent identifies knowledge triples and assigns confidence scores via
sampling-based estimation, which are used to filter low-confidence extractions
and inform downstream processing. The Constructor Agent incrementally
integrates the retained triples into a temporally evolving graph, guided by
confidence scores and timestamps to reinforce recurring knowledge and resolve
conflicts. The resulting KG contains 156,275 entities and 2,971,384 relational
triples. Quality assessments by two SOTA LLMs and three domain experts
demonstrate an accuracy approaching 90%, with strong inter-rater agreement. To
evaluate downstream utility, we conduct RAG across seven medical question
answering benchmarks using five leading LLMs, consistently observing
significant improvements over non-augmented baselines. Case studies further
demonstrate the KG's value in literature-based drug repurposing via
confidence-aware causal inference.

</details>


### [35] [Extracting Post-Acute Sequelae of SARS-CoV-2 Infection Symptoms from Clinical Notes via Hybrid Natural Language Processing](https://arxiv.org/abs/2508.12405)
*Zilong Bai,Zihan Xu,Cong Sun,Chengxi Zang,H. Timothy Bunnell,Catherine Sinfield,Jacqueline Rutter,Aaron Thomas Martinez,L. Charles Bailey,Mark Weiner,Thomas R. Campion,Thomas Carton,Christopher B. Forrest,Rainu Kaushal,Fei Wang,Yifan Peng*

Main category: cs.CL

TL;DR: 开发了一种混合自然语言处理流水线，通过规则基命名实体识别和BERT基于断言检测模块，准确高效地从临床笔记中提取COVID-19后遗症状。


<details>
  <summary>Details</summary>
Motivation: 解决PASC诊断困难，因为其症状多样且随时间变化，需要准确高效的自动化检测方法。

Method: 集成规则基命名实体识别和BERT基于断言检测模块，开发了综合性PASC词典，使用160份进展笔记进行模型开发和评估。

Result: 在内部验证中获得平均F1分0.82，外部验证中获得0.76，每份笔记处理时间仅需2.448秒，相关性检验显示高度显著相关。

Conclusion: 该模型显示出高效性和准确性，有力支持PASC诊断的改进。

Abstract: Accurately and efficiently diagnosing Post-Acute Sequelae of COVID-19 (PASC)
remains challenging due to its myriad symptoms that evolve over long- and
variable-time intervals. To address this issue, we developed a hybrid natural
language processing pipeline that integrates rule-based named entity
recognition with BERT-based assertion detection modules for PASC-symptom
extraction and assertion detection from clinical notes. We developed a
comprehensive PASC lexicon with clinical specialists. From 11 health systems of
the RECOVER initiative network across the U.S., we curated 160 intake progress
notes for model development and evaluation, and collected 47,654 progress notes
for a population-level prevalence study. We achieved an average F1 score of
0.82 in one-site internal validation and 0.76 in 10-site external validation
for assertion detection. Our pipeline processed each note at $2.448\pm 0.812$
seconds on average. Spearman correlation tests showed $\rho >0.83$ for positive
mentions and $\rho >0.72$ for negative ones, both with $P <0.0001$. These
demonstrate the effectiveness and efficiency of our models and their potential
for improving PASC diagnosis.

</details>


### [36] [ZigzagAttention: Efficient Long-Context Inference with Exclusive Retrieval and Streaming Heads](https://arxiv.org/abs/2508.12407)
*Zhuorui Liu,Chen Zhang,Dawei Song*

Main category: cs.CL

TL;DR: ZigzagAttention方法通过将检索头和流式头分离到不同层来优化LLM的长上下文处理，减少KV缓存内存占用和延迟，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型处理长上下文时KV缓存消耗巨大，现有方法虽然能减少内存占用但会带来额外的张量访问和索引延迟。

Method: 设计新的标准，强制将检索头和流式头分别聚集在不同的层中，避免混合计算带来的额外延迟。

Result: 该方法显著减少了延迟，同时只带来可忽略的性能下降，在基准测试中表现竞争力。

Conclusion: ZigzagAttention通过头类型分层策略有效解决了长上下文处理中的内存和延迟权衡问题。

Abstract: With the rapid development of large language models (LLMs), handling long
context has become one of the vital abilities in LLMs. Such long-context
ability is accompanied by difficulties in deployment, especially due to the
increased consumption of KV cache. There is certain work aiming to optimize the
memory footprint of KV cache, inspired by the observation that attention heads
can be categorized into retrieval heads that are of great significance and
streaming heads that are of less significance. Typically, identifying the
streaming heads and and waiving the KV cache in the streaming heads would
largely reduce the overhead without hurting the performance that much. However,
since employing both retrieval and streaming heads in one layer decomposes one
large round of attention computation into two small ones, it may unexpectedly
bring extra latency on accessing and indexing tensors. Based on this intuition,
we impose an important improvement to the identification process of retrieval
and streaming heads, in which we design a criterion that enforces exclusively
retrieval or streaming heads gathered in one unique layer. In this way, we
further eliminate the extra latency and only incur negligible performance
degradation. Our method named \textsc{ZigzagAttention} is competitive among
considered baselines owing to reduced latency and comparable performance.

</details>


### [37] [The Cultural Gene of Large Language Models: A Study on the Impact of Cross-Corpus Training on Model Values and Biases](https://arxiv.org/abs/2508.12411)
*Emanuel Z. Fenech-Borg,Tilen P. Meznaric-Kos,Milica D. Lekovic-Bojovic,Arni J. Hentze-Djurhuus*

Main category: cs.CL

TL;DR: 这篇论文探索了大语言模型的文化偏向，通过文化挖掘数据集和文化对齐指数强化了GPT-4的西方个人主义特征和ERNIE Bot的东方集体主义特征，告诉了避免算法文化霸权的重要性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在全球部署，但其基础文化和伦理偏向很少被探索。研究者想知道这些模型是否继承了训练语料的系统性价值观式。

Method: 提出"文化基因"概念，构建包含200个提示的文化挖掘数据集(CPD)，重点考察个人主义-集体主义(IDV)和权力距离(PDI)两个经典跨文化维度。使用标准化零样本提示比较GPT-4和ERNIE Bot模型。

Result: 人工标注显示两个模型在两个维度上存在显著且一致的分异。GPT-4呈现个人主义和低权力距离倾向，ERNIE Bot呈现集体主义和较高权力距离倾向，差异统计显著(p < 0.001)。文化对齐指数显示GPT-4更接近美国，ERNIE Bot更接近中国。

Conclusion: 研究结果支持大语言模型作为其文化语料库统计镜像的观点，并鼓励采用文化意识评估和部署方案，以避免算法文化霸权。

Abstract: Large language models (LLMs) are deployed globally, yet their underlying
cultural and ethical assumptions remain underexplored. We propose the notion of
a "cultural gene" -- a systematic value orientation that LLMs inherit from
their training corpora -- and introduce a Cultural Probe Dataset (CPD) of 200
prompts targeting two classic cross-cultural dimensions:
Individualism-Collectivism (IDV) and Power Distance (PDI). Using standardized
zero-shot prompts, we compare a Western-centric model (GPT-4) and an
Eastern-centric model (ERNIE Bot). Human annotation shows significant and
consistent divergence across both dimensions. GPT-4 exhibits individualistic
and low-power-distance tendencies (IDV score approx 1.21; PDI score approx
-1.05), while ERNIE Bot shows collectivistic and higher-power-distance
tendencies (IDV approx -0.89; PDI approx 0.76); differences are statistically
significant (p < 0.001). We further compute a Cultural Alignment Index (CAI)
against Hofstede's national scores and find GPT-4 aligns more closely with the
USA (e.g., IDV CAI approx 0.91; PDI CAI approx 0.88) whereas ERNIE Bot aligns
more closely with China (IDV CAI approx 0.85; PDI CAI approx 0.81). Qualitative
analyses of dilemma resolution and authority-related judgments illustrate how
these orientations surface in reasoning. Our results support the view that LLMs
function as statistical mirrors of their cultural corpora and motivate
culturally aware evaluation and deployment to avoid algorithmic cultural
hegemony.

</details>


### [38] [Uncovering Emergent Physics Representations Learned In-Context by Large Language Models](https://arxiv.org/abs/2508.12448)
*Yeongwoo Song,Jaeyong Bae,Dong-Kyum Kim,Hawoong Jeong*

Main category: cs.CL

TL;DR: 这篇论文通过物理学任务探索大语言模型的上下文学习能力，发现模型能够在上下文中编码关键物理变量如能量，揭示了LLM上下文学习的机制。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在上下文学习方面表现突出，但其内部机制仍不明确。物理学任务提供了一个理想的研究平台，因为物理系统基于基础原理且可实验控制。

Method: 使用物理系统动力学预测任务作为代理，分析LLM在长上下文中的表现。通过稀疏自编码器(SAEs)分析模型激活值，识别与物理变量相关的特征。

Result: 实验显示动力学预测性能随着输入上下文长度增加而提升。SAEs捕获的特征与关键物理变量（如能量）呈现显著相关性。

Conclusion: 这个研究提供了新的案例研究，扩展了我们对LLM上下文学习机制的理解，证明在上下文学习过程中有意义的物理概念被编码到模型中。

Abstract: Large language models (LLMs) exhibit impressive in-context learning (ICL)
abilities, enabling them to solve wide range of tasks via textual prompts
alone. As these capabilities advance, the range of applicable domains continues
to expand significantly. However, identifying the precise mechanisms or
internal structures within LLMs that allow successful ICL across diverse,
distinct classes of tasks remains elusive. Physics-based tasks offer a
promising testbed for probing this challenge. Unlike synthetic sequences such
as basic arithmetic or symbolic equations, physical systems provide
experimentally controllable, real-world data based on structured dynamics
grounded in fundamental principles. This makes them particularly suitable for
studying the emergent reasoning behaviors of LLMs in a realistic yet tractable
setting. Here, we mechanistically investigate the ICL ability of LLMs,
especially focusing on their ability to reason about physics. Using a dynamics
forecasting task in physical systems as a proxy, we evaluate whether LLMs can
learn physics in context. We first show that the performance of dynamics
forecasting in context improves with longer input contexts. To uncover how such
capability emerges in LLMs, we analyze the model's residual stream activations
using sparse autoencoders (SAEs). Our experiments reveal that the features
captured by SAEs correlate with key physical variables, such as energy. These
findings demonstrate that meaningful physical concepts are encoded within LLMs
during in-context learning. In sum, our work provides a novel case study that
broadens our understanding of how LLMs learn in context.

</details>


### [39] [M3PO: Multimodal-Model-Guided Preference Optimization for Visual Instruction Following](https://arxiv.org/abs/2508.12458)
*Ruirui Gao,Emily Johnson,Bowen Tan,Yanfei Qian*

Main category: cs.CL

TL;DR: M3PO是一种新颖的多模态模型引导偏好优化方法，通过智能选择LVLM生成的高价值偏好样本对，结合多模态对齐分数和模型自一致性评分来提升视觉指令跟随能力。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型的发展受限于人工标注的高成本和一致性差的问题，传统监督微调和偏好优化方法难以有效利用模型自身生成空间来识别高价值的困难负样本。

Method: 提出M3P-Score机制，整合多模态对齐分数(MAS)和模型自置信度(log概率)，从LVLM生成的候选样本中选择最具学习价值的偏好样本对，然后使用LoRA进行直接偏好优化(DPO)微调。

Result: 在多个多模态指令跟随基准测试(MME-Bench、POPE、IFT、Human Pref. Score)上，M3PO consistently outperforms strong baselines，包括SFT、模拟RLHF、普通DPO和RM-DPO。

Conclusion: M3PO提供了一种数据高效的方法来增强LVLMs的视觉指令跟随能力，通过智能样本选择机制解决了传统方法在识别困难负样本方面的局限性。

Abstract: Large Vision-Language Models (LVLMs) hold immense potential for complex
multimodal instruction following, yet their development is often hindered by
the high cost and inconsistency of human annotation required for effective
fine-tuning and preference alignment. Traditional supervised fine-tuning (SFT)
and existing preference optimization methods like RLHF and DPO frequently
struggle to efficiently leverage the model's own generation space to identify
highly informative "hard negative" samples. To address these challenges, we
propose Multimodal-Model-Guided Preference Optimization (M3PO), a novel and
data-efficient method designed to enhance LVLMs' capabilities in visual
instruction following. M3PO intelligently selects the most "learning-valuable"
preference sample pairs from a diverse pool of LVLM-generated candidates. This
selection is driven by a sophisticated mechanism that integrates two crucial
signals: a Multimodal Alignment Score (MAS) to assess external quality and the
model's Self-Consistency / Confidence (log-probability) to gauge internal
belief. These are combined into a novel M3P-Score, which specifically
identifies preferred responses and challenging dispreferred responses that the
model might confidently generate despite being incorrect. These high-quality
preference pairs are then used for efficient Direct Preference Optimization
(DPO) fine-tuning on base LVLMs like LLaVA-1.5 (7B/13B) using LoRA. Our
extensive experiments demonstrate that M3PO consistently outperforms strong
baselines, including SFT, simulated RLHF, vanilla DPO, and RM-DPO, across a
comprehensive suite of multimodal instruction following benchmarks (MME-Bench,
POPE, IFT, Human Pref. Score).

</details>


### [40] [LoraxBench: A Multitask, Multilingual Benchmark Suite for 20 Indonesian Languages](https://arxiv.org/abs/2508.12459)
*Alham Fikri Aji,Trevor Cohn*

Main category: cs.CL

TL;DR: LoraxBench是一个针对印尼低资源语言的基准测试，涵盖6个任务和20种语言，发现多语言模型在印尼语言上表现存在明显差距，且语域变化显著影响模型性能。


<details>
  <summary>Details</summary>
Motivation: 印尼作为人口大国拥有700种语言，但在NLP发展方面相对落后，需要专门的基准测试来评估低资源语言处理能力。

Method: 构建LoraxBench基准，包含阅读理解、开放域QA、语言推理、因果推理、翻译和文化QA等6个任务，覆盖20种语言和不同语域变体。

Result: 基准测试具有挑战性，印尼语与其他低资源语言性能存在明显差距，区域特定模型与通用多语言模型无显著优势，语域变化（如高级礼貌语）显著影响模型表现。

Conclusion: 印尼低资源语言NLP仍面临重大挑战，需要更多针对性的研究和模型优化，特别是在处理不同语域和文化语境方面。

Abstract: As one of the world's most populous countries, with 700 languages spoken,
Indonesia is behind in terms of NLP progress. We introduce LoraxBench, a
benchmark that focuses on low-resource languages of Indonesia and covers 6
diverse tasks: reading comprehension, open-domain QA, language inference,
causal reasoning, translation, and cultural QA. Our dataset covers 20
languages, with the addition of two formality registers for three languages. We
evaluate a diverse set of multilingual and region-focused LLMs and found that
this benchmark is challenging. We note a visible discrepancy between
performance in Indonesian and other languages, especially the low-resource
ones. There is no clear lead when using a region-specific model as opposed to
the general multilingual model. Lastly, we show that a change in register
affects model performance, especially with registers not commonly found in
social media, such as high-level politeness `Krama' Javanese.

</details>


### [41] [Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI's Latest Open Source Models](https://arxiv.org/abs/2508.12461)
*Ziqian Bi,Keyu Chen,Chiung-Yi Tseng,Danyang Zhang,Tianyang Wang,Hongying Luo,Lu Chen,Junming Huang,Jibin Guan,Junfeng Hao,Junhao Song*

Main category: cs.CL

TL;DR: OpenAI发布GPT-OSS开源模型（20B和120B参数），评估显示20B模型在多个基准测试中优于120B模型，表明稀疏架构的规模扩展不一定带来性能提升


<details>
  <summary>Details</summary>
Motivation: 评估OpenAI首个开源大语言模型GPT-OSS的性能，比较不同参数规模的稀疏架构模型在各项任务中的表现，为开源模型部署提供实证依据

Method: 在10个基准测试上比较6个开源大语言模型（14.7B-235B参数），使用标准化推理设置，采用McNemar检验和效应量分析进行统计验证

Result: GPT-OSS-20B在HumanEval和MMLU等多个基准上持续优于GPT-OSS-120B，且内存和能耗更低；两个模型在开源模型中处于中游水平，代码生成较强但多语言能力较弱

Conclusion: 稀疏架构的规模扩展可能不会带来相应的性能增益，需要进一步研究优化策略，为未来开源部署提供更高效的模型选择指导

Abstract: In August 2025, OpenAI released GPT-OSS models, its first open weight large
language models since GPT-2 in 2019, comprising two mixture of experts
architectures with 120B and 20B parameters. We evaluated both variants against
six contemporary open source large language models ranging from 14.7B to 235B
parameters, representing both dense and sparse designs, across ten benchmarks
covering general knowledge, mathematical reasoning, code generation,
multilingual understanding, and conversational ability. All models were tested
in unquantised form under standardised inference settings, with statistical
validation using McNemars test and effect size analysis. Results show that
gpt-oss-20B consistently outperforms gpt-oss-120B on several benchmarks, such
as HumanEval and MMLU, despite requiring substantially less memory and energy
per response. Both models demonstrate mid-tier overall performance within the
current open source landscape, with relative strength in code generation and
notable weaknesses in multilingual tasks. These findings provide empirical
evidence that scaling in sparse architectures may not yield proportional
performance gains, underscoring the need for further investigation into
optimisation strategies and informing more efficient model selection for future
open source deployments.

</details>


### [42] [The Structural Sources of Verb Meaning Revisited: Large Language Models Display Syntactic Bootstrapping](https://arxiv.org/abs/2508.12482)
*Xiaomeng Zhu,R. Thomas McCoy,Robert Frank*

Main category: cs.CL

TL;DR: 语法启动学习在大语言模型中的表现：移除语法信息比移除共现信息更严重影响动词表征，心理动词受影响更大


<details>
  <summary>Details</summary>
Motivation: 检验大语言模型是否也像儿童一样通过语法环境学习动词含义，即语法启动学习假设false在AI模型中的适用性

Method: 对RoBERTa和GPT-2进行训练，使用经过批变的数据集（移除语法信息或共现信息），比较模型在不同训练情况下的表现

Result: 移除语法线索导致动词表征更大的退化，尤其是心理动词；而名词表征更受共现信息影响；证实语法启动学习在模型中的重要性

Conclusion: 大语言模型通过操纵学习环境可以大规模测试发育假设false，证明语法启动学习在动词学习中具有重要作用

Abstract: Syntactic bootstrapping (Gleitman, 1990) is the hypothesis that children use
the syntactic environments in which a verb occurs to learn its meaning. In this
paper, we examine whether large language models exhibit a similar behavior. We
do this by training RoBERTa and GPT-2 on perturbed datasets where syntactic
information is ablated. Our results show that models' verb representation
degrades more when syntactic cues are removed than when co-occurrence
information is removed. Furthermore, the representation of mental verbs, for
which syntactic bootstrapping has been shown to be particularly crucial in
human verb learning, is more negatively impacted in such training regimes than
physical verbs. In contrast, models' representation of nouns is affected more
when co-occurrences are distorted than when syntax is distorted. In addition to
reinforcing the important role of syntactic bootstrapping in verb learning, our
results demonstrated the viability of testing developmental hypotheses on a
larger scale through manipulating the learning environments of large language
models.

</details>


### [43] [Mitigating Hallucinations in Large Language Models via Causal Reasoning](https://arxiv.org/abs/2508.12495)
*Yuangang Li,Yiqing Shen,Yi Nian,Jiechao Gao,Ziyi Wang,Chenxiao Yu,Shawn Li,Jie Wang,Xiyang Hu,Yue Zhao*

Main category: cs.CL

TL;DR: 通过直接建模变量级因果图进行理论，CDCR-SFT框架显著提升了大语言模型的因果推理能力并降低幻觉现象


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在逻辑不一致性幻觉方面的问题，现有的链式思维等方法在语言标记层面运作，缺乏对基础因果关系的建模能力

Method: 提出CDCR-SFT监督微调框架，训练LLMs明确构建变量级有向无环图(DAG)并在其上进行推理，同时创建了包含25,368个样本的CausalDR数据集

Result: 在4个LLMs和8个任务上的实验显示，CDCR-SFT在CLADDER任务上达到了195.33%的最高精度(首次超越人类表现94.8%)，并在HaluEval上将幻觉降低了10%

Conclusion: 明确的因果结构建模能够有效减少LLM输出中的逻辑不一致性，为提升模型理性推理能力提供了新方向

Abstract: Large language models (LLMs) exhibit logically inconsistent hallucinations
that appear coherent yet violate reasoning principles, with recent research
suggesting an inverse relationship between causal reasoning capabilities and
such hallucinations. However, existing reasoning approaches in LLMs, such as
Chain-of-Thought (CoT) and its graph-based variants, operate at the linguistic
token level rather than modeling the underlying causal relationships between
variables, lacking the ability to represent conditional independencies or
satisfy causal identification assumptions. To bridge this gap, we introduce
causal-DAG construction and reasoning (CDCR-SFT), a supervised fine-tuning
framework that trains LLMs to explicitly construct variable-level directed
acyclic graph (DAG) and then perform reasoning over it. Moreover, we present a
dataset comprising 25,368 samples (CausalDR), where each sample includes an
input question, explicit causal DAG, graph-based reasoning trace, and validated
answer. Experiments on four LLMs across eight tasks show that CDCR-SFT improves
the causal reasoning capability with the state-of-the-art 95.33% accuracy on
CLADDER (surpassing human performance of 94.8% for the first time) and reduces
the hallucination on HaluEval with 10% improvements. It demonstrates that
explicit causal structure modeling in LLMs can effectively mitigate logical
inconsistencies in LLM outputs. Code is available at
https://github.com/MrLYG/CDCR-SFT.

</details>


### [44] [CorrSteer: Steering Improves Task Performance and Safety in LLMs through Correlation-based Sparse Autoencoder Feature Selection](https://arxiv.org/abs/2508.12535)
*Seonglae Cho,Zekun Wu,Adriano Koshiyama*

Main category: cs.CL

TL;DR: CorrSteer是一种基于相关性选择稀疏自编码器特征的新方法，仅使用推理时激活来自动提取相关特征，在多项任务上显著提升性能


<details>
  <summary>Details</summary>
Motivation: 解决传统稀疏自编码器在下游任务中需要对比数据集或大量激活存储的限制，寻求更高效的特征选择和引导方法

Method: 通过将样本正确性与推理时生成的token的SAE激活进行相关性分析来选择特征，使用平均激活获得引导系数，实现全自动化流程

Result: 在Gemma 2 2B和LLaMA 3.1 8B模型上，MMLU性能提升4.1%，HarmBench提升22.9%，仅需4000个样本

Conclusion: 基于相关性选择的方法为自动化SAE引导提供了有效且可扩展的解决方案，揭示了驱动性能的底层能力模式

Abstract: Sparse Autoencoders (SAEs) can extract interpretable features from large
language models (LLMs) without supervision. However, their effectiveness in
downstream steering tasks is limited by the requirement for contrastive
datasets or large activation storage. To address these limitations, we propose
CorrSteer, which selects features by correlating sample correctness with SAE
activations from generated tokens at inference time. This approach uses only
inference-time activations to extract more relevant features, thereby avoiding
spurious correlations. It also obtains steering coefficients from average
activations, automating the entire pipeline. Our method shows improved task
performance on QA, bias mitigation, jailbreaking prevention, and reasoning
benchmarks on Gemma 2 2B and LLaMA 3.1 8B, notably achieving a +4.1%
improvement in MMLU performance and a +22.9% improvement in HarmBench with only
4000 samples. Selected features demonstrate semantically meaningful patterns
aligned with each task's requirements, revealing the underlying capabilities
that drive performance. Our work establishes correlationbased selection as an
effective and scalable approach for automated SAE steering across language
model applications.

</details>


### [45] [Beyond Modality Limitations: A Unified MLLM Approach to Automated Speaking Assessment with Effective Curriculum Learning](https://arxiv.org/abs/2508.12591)
*Yu-Hsuan Fang,Tien-Hong Lo,Yao-Ting Sung,Berlin Chen*

Main category: cs.CL

TL;DR: 本文首次系统性研究多模态大语言模型在自动说话评测中的应用，提出了专门的语音优先多模态训练方法，显著提升了评测性能。


<details>
  <summary>Details</summary>
Motivation: 传统自动说话评测系统存在模态限制：文本方法缺乏声学信息，音频方法缺少语义上下文。多模态大语言模型为全面评测提供了新机遇。

Method: 提出Speech-First Multimodal Training (SFMT)方法，利用课程学习原理，先建立健壮的语音建模基础，再进行跨模态协同融合。

Result: 在标准数据集上将整体评测性能从PCC 0.783提升到0.846。在表达方面的评估中，SFMT方法比传统训练方法绝对准确率提高4%。

Conclusion: 多模态大语言模型能够显著提升自动说话评测的性能，而专门的语音优先训练策略能够有效解决表达方面的评测挑战，为该领域开启了新路径。

Abstract: Traditional Automated Speaking Assessment (ASA) systems exhibit inherent
modality limitations: text-based approaches lack acoustic information while
audio-based methods miss semantic context. Multimodal Large Language Models
(MLLM) offer unprecedented opportunities for comprehensive ASA by
simultaneously processing audio and text within unified frameworks. This paper
presents a very first systematic study of MLLM for comprehensive ASA,
demonstrating the superior performance of MLLM across the aspects of content
and language use . However, assessment on the delivery aspect reveals unique
challenges, which is deemed to require specialized training strategies. We thus
propose Speech-First Multimodal Training (SFMT), leveraging a curriculum
learning principle to establish more robust modeling foundations of speech
before cross-modal synergetic fusion. A series of experiments on a benchmark
dataset show MLLM-based systems can elevate the holistic assessment performance
from a PCC value of 0.783 to 0.846. In particular, SFMT excels in the
evaluation of the delivery aspect, achieving an absolute accuracy improvement
of 4% over conventional training approaches, which also paves a new avenue for
ASA.

</details>


### [46] [Semantic Anchoring in Agentic Memory: Leveraging Linguistic Structures for Persistent Conversational Context](https://arxiv.org/abs/2508.12630)
*Maitreyi Chatterjee,Devansh Agarwal*

Main category: cs.CL

TL;DR: 语义锚定技术通过结合语言结构细节来改善LLM在长期交互中的记忆持续性，在事实回忆和语经连贯性方面超过基线RAG系统18%


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统仅使用浓度向量存储对话历史，忽视了语法依存关系、语经关系和指示关系等细微语言结构，导致长期交互效果受限

Method: 提出混合式代理记忆架构，结合依存解析、语经关系标注和指示消解技术，在向量存储基础上添加显式语言线索来创建结构化记忆项

Result: 在适配的长期对话数据集上实验，语义锚定技术在事实回忆和语经连贯性方面超过强劲RAG基线达18%，进行了消融研究、人工评估和错误分析以评估稳健性和可解释性

Conclusion: 通过将显式语言线索与向量表征相结合，语义锚定技术能够有效提升LLM在多次会话和长期交互中的记忆持续性和上下文一致性

Abstract: Large Language Models (LLMs) have demonstrated impressive fluency and task
competence in conversational settings. However, their effectiveness in
multi-session and long-term interactions is hindered by limited memory
persistence. Typical retrieval-augmented generation (RAG) systems store
dialogue history as dense vectors, which capture semantic similarity but
neglect finer linguistic structures such as syntactic dependencies, discourse
relations, and coreference links. We propose Semantic Anchoring, a hybrid
agentic memory architecture that enriches vector-based storage with explicit
linguistic cues to improve recall of nuanced, context-rich exchanges. Our
approach combines dependency parsing, discourse relation tagging, and
coreference resolution to create structured memory entries. Experiments on
adapted long-term dialogue datasets show that semantic anchoring improves
factual recall and discourse coherence by up to 18% over strong RAG baselines.
We further conduct ablation studies, human evaluations, and error analysis to
assess robustness and interpretability.

</details>


### [47] [Beyond GPT-5: Making LLMs Cheaper and Better via Performance-Efficiency Optimized Routing](https://arxiv.org/abs/2508.12631)
*Yiqun Zhang,Hao Li,Jianhao Chen,Hangfan Zhang,Peng Ye,Lei Bai,Shuyue Hu*

Main category: cs.CL

TL;DR: Avengers-Pro是一个测试时路由框架，通过集成不同容量和效率的LLM，动态分配查询到最适合的模型，在性能-效率权衡方面实现了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在性能和效率之间的平衡挑战，GPT-5虽然提出了测试时路由，但需要一个统一的解决方案来处理所有性能-效率权衡。

Method: 嵌入和聚类传入查询，然后基于性能-效率分数将每个查询路由到最合适的模型，集成不同容量和效率的LLM。

Result: 在6个基准测试和8个领先模型上，Avengers-Pro超越最强单模型(GPT-5-medium)平均准确率+7%，以27%更低的成本达到相同准确率，以63%更低的成本达到90%性能，实现了帕累托最优。

Conclusion: Avengers-Pro提供了一个统一的测试时路由框架，能够动态优化性能-效率权衡，在保持高性能的同时显著降低成本，是大语言模型推理的有效解决方案。

Abstract: Balancing performance and efficiency is a central challenge in large language
model (LLM) advancement. GPT-5 addresses this with test-time routing,
dynamically assigning queries to either an efficient or a high-capacity model
during inference. In this work, we present Avengers-Pro, a test-time routing
framework that ensembles LLMs of varying capacities and efficiencies, providing
a unified solution for all performance-efficiency tradeoffs. The Avengers-Pro
embeds and clusters incoming queries, then routes each to the most suitable
model based on a performance-efficiency score. Across 6 challenging benchmarks
and 8 leading models -- including GPT-5-medium, Gemini-2.5-pro, and
Claude-opus-4.1 -- Avengers-Pro achieves state-of-the-art results: by varying a
performance-efficiency trade-off parameter, it can surpass the strongest single
model (GPT-5-medium) by +7% in average accuracy. Moreover, it can match the
average accuracy of the strongest single model at 27% lower cost, and reach
~90% of that performance at 63% lower cost. Last but not least, it achieves a
Pareto frontier, consistently yielding the highest accuracy for any given cost,
and the lowest cost for any given accuracy, among all single models. Code is
available at https://github.com/ZhangYiqun018/AvengersPro.

</details>


### [48] [Prompt-Induced Linguistic Fingerprints for LLM-Generated Fake News Detection](https://arxiv.org/abs/2508.12632)
*Chi Wang,Min Gao,Zongwei Wang,Junwei Yin,Kai Shu,Chenghua Lin*

Main category: cs.CL

TL;DR: 提出LIFE方法，通过分析恶意提示下LLM生成真假新闻的概率分布差异来检测AI生成的假新闻，在LLM生成和人工撰写假新闻检测上都达到最先进性能


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型发展，假新闻生成变得容易且难以检测，现有方法主要关注文本内容本身，但虚假痕迹往往很隐蔽，需要新的检测方法

Method: 通过分布差异分析发现提示诱导的语言指纹，提出LIFE方法重构词级概率分布来寻找判别模式，并利用关键片段技术放大语言差异

Result: LIFE在LLM生成假新闻检测上达到最先进性能，同时在人工撰写假新闻检测上也保持高性能

Conclusion: 基于概率分布的语言指纹分析是检测LLM生成假新闻的有效方法，LIFE方法具有很好的检测效果和泛化能力

Abstract: With the rapid development of large language models, the generation of fake
news has become increasingly effortless, posing a growing societal threat and
underscoring the urgent need for reliable detection methods. Early efforts to
identify LLM-generated fake news have predominantly focused on the textual
content itself; however, because much of that content may appear coherent and
factually consistent, the subtle traces of falsification are often difficult to
uncover. Through distributional divergence analysis, we uncover prompt-induced
linguistic fingerprints: statistically distinct probability shifts between
LLM-generated real and fake news when maliciously prompted. Based on this
insight, we propose a novel method named Linguistic Fingerprints Extraction
(LIFE). By reconstructing word-level probability distributions, LIFE can find
discriminative patterns that facilitate the detection of LLM-generated fake
news. To further amplify these fingerprint patterns, we also leverage
key-fragment techniques that accentuate subtle linguistic differences, thereby
improving detection reliability. Our experiments show that LIFE achieves
state-of-the-art performance in LLM-generated fake news and maintains high
performance in human-written fake news. The code and data are available at
https://anonymous.4open.science/r/LIFE-E86A.

</details>


### [49] [Breaking Language Barriers: Equitable Performance in Multilingual Language Models](https://arxiv.org/abs/2508.12662)
*Tanay Nagar,Grigorii Khvatskii,Anna Sokol,Nitesh V. Chawla*

Main category: cs.CL

TL;DR: 通过代码转换精确控制的语言混合方法，生成合成的代码转换文本对LLM进行微调，以提升低资源语言在常识推理任务中的性能，同时保持或提升高资源语言的表现


<details>
  <summary>Details</summary>
Motivation: 虽然先进的大语言模型在多语言通信和理解方面表现强劲，但在低资源语言（如印地语、斯瓦希里语）上的常识推理任务表现较差，远不如高资源语言（如英语）。平衡这种性能差异对于确保不同语言社区的公平性至关重要

Method: 使用控制语言混合方法生成合成的代码转换文本，对大语言模型进行微调训练。研究还提供了一个基于CommonSenseQA数据集的新的合成代码转换文本数据集，包含三种不同的语言比例配置

Result: 经验证明，在合成代码转换数据集上微调大语言模型能够实现低资源语言模型性能的显著提升，同时保持或提升高资源语言的表现

Conclusion: 通过控制语言混合方法生成合成代码转换文本对LLM进行微调，是一种有效的方法，可以缩小不同语言在常识推理任务上的性能差距，为低资源语言用户提供更公平的AI访问机会

Abstract: Cutting-edge LLMs have emerged as powerful tools for multilingual
communication and understanding. However, LLMs perform worse in Common Sense
Reasoning (CSR) tasks when prompted in low-resource languages (LRLs) like Hindi
or Swahili compared to high-resource languages (HRLs) like English. Equalizing
this inconsistent access to quality LLM outputs is crucial to ensure fairness
for speakers of LRLs and across diverse linguistic communities. In this paper,
we propose an approach to bridge this gap in LLM performance. Our approach
involves fine-tuning an LLM on synthetic code-switched text generated using
controlled language-mixing methods. We empirically demonstrate that fine-tuning
LLMs on synthetic code-switched datasets leads to substantial improvements in
LRL model performance while preserving or enhancing performance in HRLs.
Additionally, we present a new dataset of synthetic code-switched text derived
from the CommonSenseQA dataset, featuring three distinct language ratio
configurations.

</details>


### [50] [Leveraging Large Language Models for Predictive Analysis of Human Misery](https://arxiv.org/abs/2508.12669)
*Bishanka Seal,Rahul Seetharaman,Aman Bansal,Abhilash Nandy*

Main category: cs.CL

TL;DR: 使用大语言模型预测人类感知的痛苦分数，通过多种提示策略和创新的游戏化评估框架来测试模型的情感推理能力


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型在情感预测任务中的表现，特别是如何从自然语言描述中预测人类感知的痛苦程度，探索超越传统静态评估的动态情感推理能力

Method: 采用回归问题框架，评估零样本、固定上下文少样本和基于BERT句子嵌入的检索式提示策略。创新性地设计了"痛苦游戏秀"游戏化评估框架，包含序数比较、二元分类、标量估计和反馈驱动推理等多个环节

Result: 少样本方法持续优于零样本基线，证明了上下文示例在情感预测中的价值。游戏化评估显示大语言模型在动态情感推理任务中具有超越标准回归的潜力

Conclusion: 大语言模型能够有效预测人类感知的痛苦分数，游戏化评估框架为测试模型的情感推理和适应能力提供了新途径，展示了在动态情感任务中的广泛应用前景

Abstract: This study investigates the use of Large Language Models (LLMs) for
predicting human-perceived misery scores from natural language descriptions of
real-world scenarios. The task is framed as a regression problem, where the
model assigns a scalar value from 0 to 100 to each input statement. We evaluate
multiple prompting strategies, including zero-shot, fixed-context few-shot, and
retrieval-based prompting using BERT sentence embeddings. Few-shot approaches
consistently outperform zero-shot baselines, underscoring the value of
contextual examples in affective prediction. To move beyond static evaluation,
we introduce the "Misery Game Show", a novel gamified framework inspired by a
television format. It tests LLMs through structured rounds involving ordinal
comparison, binary classification, scalar estimation, and feedback-driven
reasoning. This setup enables us to assess not only predictive accuracy but
also the model's ability to adapt based on corrective feedback. The gamified
evaluation highlights the broader potential of LLMs in dynamic emotional
reasoning tasks beyond standard regression. Code and data link:
https://github.com/abhi1nandy2/Misery_Data_Exps_GitHub

</details>


### [51] [ToolACE-MT: Non-Autoregressive Generation for Agentic Multi-Turn Interaction](https://arxiv.org/abs/2508.12685)
*Xingshan Zeng,Weiwen Liu,Lingzhi Wang,Liangyou Li,Fei Mi,Yasheng Wang,Lifeng Shang,Xin Jiang,Qun Liu*

Main category: cs.CL

TL;DR: 提出ToolACE-MT框架，通过非自回归迭代生成高质量多轮机器人对话数据，避免了传统自动回归方法的高成本问题


<details>
  <summary>Details</summary>
Motivation: 现有的多轮机器人任务数据生成方法依赖于多个LLM机器人之间费用昂贵的自动回归交互，限制了机器人任务的实际性能

Method: 三阶段框架：粗粒度初始化构建对话骨架、通过mask-and-fill操作进行迭代精细化、离线验证保证正确性和一致性

Result: 实验证明ToolACE-MT能够实现高效、高效能和可通用的机器人数据生成

Conclusion: 为工具增强型LLM场景提供了高质量数据构建的新范式

Abstract: Agentic task-solving with Large Language Models (LLMs) requires multi-turn,
multi-step interactions, often involving complex function calls and dynamic
user-agent exchanges. Existing simulation-based data generation methods for
such scenarios rely heavily on costly autoregressive interactions between
multiple LLM agents, thereby limiting real-world performance of agentic tasks.
In this paper, we propose a novel Non-Autoregressive Iterative Generation
framework, called ToolACE-MT, for constructing high-quality multi-turn agentic
dialogues. ToolACE-MT generates full conversational trajectories through three
stages: coarse-grained initialization, iterative refinement, and offline
verification. The initialization phase builds a structurally complete yet
semantically coarse dialogue skeleton; the iterative refinement phase
introduces realistic complexities and continued refinement via mask-and-fill
operations; and the offline verification phase ensures correctness and
coherence via rule- and model-based checks. Experiments demonstrate that
ToolACE-MT enables efficient, effective and generalizable agentic data
generation, offering a new paradigm for high-quality data construction in
tool-augmented LLM scenarios.

</details>


### [52] [DESIGNER: Design-Logic-Guided Multidisciplinary Data Synthesis for LLM Reasoning](https://arxiv.org/abs/2508.12726)
*Weize Liu,Yongchi Zhao,Yijia Luo,Mingyu Xu,Jiaheng Liu,Yanan Li,Xiguo Hu,Yuchi Xu,Wenbo Su,Bo Zheng*

Main category: cs.CL

TL;DR: DESIGNER是一个基于设计逻辑的多学科推理数据合成框架，通过从现有问题中提取12万+设计逻辑，结合书籍和网络语料生成大规模、高难度的推理问题数据集，显著提升了LLM的跨学科推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有推理数据集在学科广度和结构深度上不足，难以激发强大的推理能力。需要创建更具挑战性和多样性的多学科推理数据集来提升LLM的复杂推理性能。

Method: 提出设计逻辑概念，模仿人类教育者的出题过程。使用LLM从现有问题中逆向工程提取12万+设计逻辑，结合书籍和网络语料生成推理问题，构建了两个大规模数据集DLR-Book(304万问题)和DLR-Web(166万问题)。

Result: 合成的问题在难度和多样性上远超基线数据集。在Qwen3模型上的SFT实验显示，该数据集显著优于同等规模的多学科数据集，完整训练后模型推理性能甚至超过官方Qwen3模型。

Conclusion: DESIGNER框架成功创建了大规模、高质量的多学科推理数据集，有效提升了LLM的复杂推理能力，证明了设计逻辑引导的数据合成方法的有效性。

Abstract: Large language models (LLMs) have achieved remarkable success in many natural
language tasks but still struggle with complex, multi-step reasoning,
particularly across diverse disciplines. Existing reasoning datasets often
either lack disciplinary breadth or the structural depth necessary to elicit
robust reasoning behaviors. We propose DESIGNER: a DESIGN-logic-guidEd
Reasoning data synthesis pipeline that leverages naturally available, extensive
raw documents (book corpus and web corpus) to generate multidisciplinary
challenging questions. A core innovation of our approach is the introduction of
a Design Logic concept, which mimics the question-creation process of human
educators. We use LLMs to reverse-engineer and abstract over 120,000 design
logics from existing questions across various disciplines. By matching these
design logics with disciplinary source materials, we are able to create
reasoning questions that far surpass the difficulty and diversity of existing
datasets. Based on this pipeline, we synthesized two large-scale reasoning
datasets that span 75 disciplines: Design-Logic-Reasoning-Book (DLR-Book),
containing 3.04 million challenging questions synthesized from the book corpus,
and Design-Logic-Reasoning-Web (DLR-Web), with 1.66 million challenging
questions from the web corpus. Our data analysis demonstrates that the
questions synthesized by our method exhibit substantially greater difficulty
and diversity than those in the baseline datasets. We validate the
effectiveness of these datasets by conducting SFT experiments on the
Qwen3-8B-Base and Qwen3-4B-Base models. The results show that our dataset
significantly outperforms existing multidisciplinary datasets of the same
volume. Training with the full datasets further enables the models to surpass
the multidisciplinary reasoning performance of the official Qwen3-8B and
Qwen3-4B models.

</details>


### [53] [LinguaSafe: A Comprehensive Multilingual Safety Benchmark for Large Language Models](https://arxiv.org/abs/2508.12733)
*Zhiyuan Ning,Tianle Gu,Jiaxin Song,Shixin Hong,Lingyu Li,Huacan Liu,Jie Li,Yixu Wang,Meng Lingyu,Yan Teng,Yingchun Wang*

Main category: cs.CL

TL;DR: LinguaSafe是一个包含12种语言、45k条目的多语言安全基准测试，通过翻译、转创和本地数据构建，填补了LLM在多语言安全评估方面的空白。


<details>
  <summary>Details</summary>
Motivation: 现有LLM多语言安全评估缺乏全面性和多样性数据，限制了多语言安全对齐的发展，需要更全面的多语言安全基准测试。

Method: 结合翻译、转创和本地来源数据构建包含12种语言的45k条目数据集，采用多维度和细粒度的评估框架，包括直接/间接安全评估和过度敏感性评估。

Result: 安全和有用性评估结果在不同领域和语言间差异显著，即使在资源水平相似的语言中也存在明显差异。

Conclusion: LinguaSafe基准测试强调了全面评估LLM多语言安全的重要性，为实现更平衡的安全对齐提供了重要工具，数据集和代码已公开以促进进一步研究。

Abstract: The widespread adoption and increasing prominence of large language models
(LLMs) in global technologies necessitate a rigorous focus on ensuring their
safety across a diverse range of linguistic and cultural contexts. The lack of
a comprehensive evaluation and diverse data in existing multilingual safety
evaluations for LLMs limits their effectiveness, hindering the development of
robust multilingual safety alignment. To address this critical gap, we
introduce LinguaSafe, a comprehensive multilingual safety benchmark crafted
with meticulous attention to linguistic authenticity. The LinguaSafe dataset
comprises 45k entries in 12 languages, ranging from Hungarian to Malay. Curated
using a combination of translated, transcreated, and natively-sourced data, our
dataset addresses the critical need for multilingual safety evaluations of
LLMs, filling the void in the safety evaluation of LLMs across diverse
under-represented languages from Hungarian to Malay. LinguaSafe presents a
multidimensional and fine-grained evaluation framework, with direct and
indirect safety assessments, including further evaluations for oversensitivity.
The results of safety and helpfulness evaluations vary significantly across
different domains and different languages, even in languages with similar
resource levels. Our benchmark provides a comprehensive suite of metrics for
in-depth safety evaluation, underscoring the critical importance of thoroughly
assessing multilingual safety in LLMs to achieve more balanced safety
alignment. Our dataset and code are released to the public to facilitate
further research in the field of multilingual LLM safety.

</details>


### [54] [CRED-SQL: Enhancing Real-world Large Scale Database Text-to-SQL Parsing through Cluster Retrieval and Execution Description](https://arxiv.org/abs/2508.12769)
*Shaoming Duan,Zirui Wang,Chuanyi Liu,Zhibin Zhu,Yuhao Zhang,Peiyi Han,Liang Yan,Zewu Penge*

Main category: cs.CL

TL;DR: CRED-SQL是一个针对大规模数据库的Text-to-SQL框架，通过集群检索和执行描述语言解决语义不匹配问题，在两个大型跨域基准测试中达到了新的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然提升了Text-to-SQL系统的准确性，但在大规模数据库中，自然语言问题与SQL查询之间的语义不匹配问题仍然严重，特别是在语义相似的属性导致模式链接困难和语义漂移的情况下。

Method: 提出CRED-SQL框架，包含两个核心组件：1）基于集群的大规模模式检索，精确定位与NLQ最相关的表和列；2）引入中间自然语言表示-执行描述语言(EDL)，将任务分解为Text-to-EDL和EDL-to-SQL两个阶段，利用LLM的强大推理能力同时减少语义偏差。

Result: 在两个大规模跨域基准测试SpiderUnion和BirdUnion上的广泛实验表明，CRED-SQL达到了新的最先进(SOTA)性能。

Conclusion: CRED-SQL通过创新的集群检索和EDL中间表示，有效解决了大规模数据库中的语义不匹配问题，验证了其有效性和可扩展性。

Abstract: Recent advances in large language models (LLMs) have significantly improved
the accuracy of Text-to-SQL systems. However, a critical challenge remains: the
semantic mismatch between natural language questions (NLQs) and their
corresponding SQL queries. This issue is exacerbated in large-scale databases,
where semantically similar attributes hinder schema linking and semantic drift
during SQL generation, ultimately reducing model accuracy. To address these
challenges, we introduce CRED-SQL, a framework designed for large-scale
databases that integrates Cluster Retrieval and Execution Description. CRED-SQL
first performs cluster-based large-scale schema retrieval to pinpoint the
tables and columns most relevant to a given NLQ, alleviating schema mismatch.
It then introduces an intermediate natural language representation-Execution
Description Language (EDL)-to bridge the gap between NLQs and SQL. This
reformulation decomposes the task into two stages: Text-to-EDL and EDL-to-SQL,
leveraging LLMs' strong general reasoning capabilities while reducing semantic
deviation. Extensive experiments on two large-scale, cross-domain
benchmarks-SpiderUnion and BirdUnion-demonstrate that CRED-SQL achieves new
state-of-the-art (SOTA) performance, validating its effectiveness and
scalability. Our code is available at https://github.com/smduan/CRED-SQL.git

</details>


### [55] [From SALAMANDRA to SALAMANDRATA: BSC Submission for WMT25 General Machine Translation Shared Task](https://arxiv.org/abs/2508.12774)
*Javier Garcia Gilabert,Xixian Liao,Severino Da Dalt,Ella Bohman,Audrey Mash,Francesca De Luca Fornaciari,Irene Baucells,Joan Llop,Miguel Claramunt Argote,Carlos Escolano,Maite Melero*

Main category: cs.CL

TL;DR: SALAMANDRATA模型家族是SALAMANDRA LLMs的改进版本，专门针对38种欧洲语言的翻译任务进行训练，提供2B和7B两个规模版本，采用持续预训练和监督微调的两阶段训练方法，并在WMT25机器翻译任务中应用了质量感知的解码策略。


<details>
  <summary>Details</summary>
Motivation: 开发专门针对多语言翻译任务的高性能模型，特别是针对欧洲语言的翻译需求，提升机器翻译的质量和效率。

Method: 采用两阶段训练方法：首先在平行数据上进行持续预训练，然后在高质量指令上进行监督微调。针对WMT25任务，还进行了词汇表扩展和额外的训练阶段。解码时使用最小贝叶斯风险解码和基于COMET的调优重排序策略。

Result: 开发了SALAMANDRATA模型家族的2B和7B版本，以及更新的SALAMANDRATA-V2模型，这些模型已在Hugging Face平台上公开发布。

Conclusion: SALAMANDRATA模型家族通过专门的训练方法和质量感知解码策略，为多语言机器翻译任务提供了有效的解决方案，特别是在欧洲语言翻译方面表现出色。

Abstract: In this paper, we present the SALAMANDRATA family of models, an improved
iteration of SALAMANDRA LLMs (Gonzalez-Agirre et al., 2025) specifically
trained to achieve strong performance in translation-related tasks for 38
European languages. SALAMANDRATA comes in two scales: 2B and 7B parameters. For
both versions, we applied the same training recipe with a first step of
continual pre-training on parallel data, and a second step of supervised
fine-tuning on high-quality instructions. The BSC submission to the WMT25
General Machine Translation shared task is based on the 7B variant of
SALAMANDRATA. We first adapted the model vocabulary to support the additional
non-European languages included in the task. This was followed by a second
phase of continual pre-training and supervised fine-tuning, carefully designed
to optimize performance across all translation directions for this year's
shared task. For decoding, we employed two quality-aware strategies: Minimum
Bayes Risk Decoding and Tuned Re-ranking using COMET and COMET-KIWI
respectively. We publicly release both the 2B and 7B versions of SALAMANDRATA,
along with the newer SALAMANDRATA-V2 model, on Hugging Face1

</details>


### [56] [HeteroRAG: A Heterogeneous Retrieval-Augmented Generation Framework for Medical Vision Language Tasks](https://arxiv.org/abs/2508.12778)
*Zhe Chen,Yusheng Liao,Shuyang Jiang,Zhiyuan Zhu,Haolin Li,Yanfeng Wang,Yu Wang*

Main category: cs.CL

TL;DR: 医疗多模态语言模型存在事实性问题，这篇文章提出HeteroRAG框架，通过异构知识源增强来提高诊断准确性和可靠性


<details>
  <summary>Details</summary>
Motivation: 医疗大视觉-语言模型(Med-LVLMs)在临床应用中显示潜力，但存在事实性不准确和不可靠输出的问题，在真实诊断中带来风险。当前的多模态RAG系统无法在异构来源中进行有效检索

Method: 构建MedAtlas多模态报告仓库，提出HeteroRAG框架，包括模态特异性CLIPs用于报告检索，多语料库查询生成器动态构建查询，以及异构知识偏好循环训练方法

Result: 在12个数据集和3种模态上进行广泛实验，HeteroRAG在大多数医学视觉语言测试中达到最优性能，显著提高了Med-LVLMs的事实准确性和可靠性

Conclusion: HeteroRAG框架通过异构知识源的有效整合，成功提升了医疗多模态语言模型的诊断性能，为临床应用提供了更可靠的解决方案

Abstract: Medical large vision-language Models (Med-LVLMs) have shown promise in
clinical applications but suffer from factual inaccuracies and unreliable
outputs, posing risks in real-world diagnostics. While retrieval-augmented
generation has emerged as a potential solution, current medical multimodal RAG
systems are unable to perform effective retrieval across heterogeneous sources.
The irrelevance of retrieved reports affects the factuality of analysis, while
insufficient knowledge affects the credibility of clinical decision-making. To
bridge the gap, we construct MedAtlas, which includes extensive multimodal
report repositories and diverse text corpora. Based on it, we present
HeteroRAG, a novel framework that enhances Med-LVLMs through heterogeneous
knowledge sources. The framework introduces Modality-specific CLIPs for
effective report retrieval and a Multi-corpora Query Generator for dynamically
constructing queries for diverse corpora. Incorporating knowledge from such
multifaceted sources, Med-LVLM is then trained with Heterogeneous Knowledge
Preference Tuning to achieve cross-modality and multi-source knowledge
alignment. Extensive experiments across 12 datasets and 3 modalities
demonstrate that the proposed HeteroRAG achieves state-of-the-art performance
in most medical vision language benchmarks, significantly improving factual
accuracy and reliability of Med-LVLMs.

</details>


### [57] [Atom-Searcher: Enhancing Agentic Deep Research via Fine-Grained Atomic Thought Reward](https://arxiv.org/abs/2508.12800)
*Yong Deng,Guoqing Wang,Zhenzhe Ying,Xiaofeng Wu,Jinzhen Lin,Wenwen Xiong,Yuqin Dai,Shuo Yang,Zhanwei Zhang,Qiwen Wang,Yang Qin,Changhua Meng*

Main category: cs.CL

TL;DR: 提出了Atomic Thought思维范式和Atom-Searcher强化学习框架，通过细粒度推理单元和过程奖励解决传统RAG和基于结果的RL在深度研究任务中的局限性


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在复杂任务中静态知识限制、RAG在多层次推理和策略搜索中的刚性工作流问题，以及基于结果的强化学习面临的梯度冲突和奖励稀疏性问题

Method: 1. Atomic Thought：将推理分解为细粒度功能单元，由推理奖励模型提供原子思维奖励 2. Atom-Searcher：集成Atomic Thought和ATR的RL框架，采用课程式奖励调度，早期优先过程级奖励，后期转向结果奖励

Result: 在七个基准测试中均表现出优于现有技术的持续改进，具有可扩展计算、更好的可解释性和更接近人类推理模式等优势

Conclusion: Atomic Thought和Atom-Searcher框架有效解决了深度研究任务中的推理和奖励问题，提供了更高效、可解释的解决方案

Abstract: Large language models (LLMs) exhibit remarkable problem-solving abilities,
but struggle with complex tasks due to static internal knowledge.
Retrieval-Augmented Generation (RAG) enhances access to external information,
yet remains limited in multi-hop reasoning and strategic search due to rigid
workflows. Recent advancements in agentic deep research empower LLMs to
autonomously reason, search, and synthesize information. However, current
approaches relying on outcome-based reinforcement learning (RL) face critical
issues such as conflicting gradients and reward sparsity, limiting performance
gains and training efficiency. To address these, we first propose Atomic
Thought, a novel LLM thinking paradigm that decomposes reasoning into
fine-grained functional units. These units are supervised by Reasoning Reward
Models (RRMs), which provide Atomic Thought Rewards (ATR) for fine-grained
guidance. Building on this, we propose Atom-Searcher, a novel RL framework for
agentic deep research that integrates Atomic Thought and ATR. Atom-Searcher
uses a curriculum-inspired reward schedule, prioritizing process-level ATR
early and transitioning to outcome rewards, accelerating convergence on
effective reasoning paths. Experiments on seven benchmarks show consistent
improvements over the state-of-the-art. Key advantages include: (1)
Atom-Searcher scales computation at test-time. (2) Atomic Thought provides
supervision anchors for RRMs, bridging deep research tasks and RRMs. (3)
Atom-Searcher exhibits more interpretable, human-like reasoning patterns.

</details>


### [58] [When Alignment Hurts: Decoupling Representational Spaces in Multilingual Models](https://arxiv.org/abs/2508.12803)
*Ahmed Elshabrawy,Hour Kaing,Haiyue Song,Alham Fikri Aji,Hideki Tanaka,Masao Utiyama,Raj Dabre*

Main category: cs.CL

TL;DR: 这篇论文通过在阿拉伯语言家族的研究，证明了高资源标准语言的表征空间占据会对相关低资源方言的生成模型产生负面影响，并提出了在线变分探针框架来解耦这种表征缘绕。


<details>
  <summary>Details</summary>
Motivation: 挖掘高资源标准语言与相关低资源方言之间表征关系的洞察，挖掘过度表征缘绕对生成模型的影响，以及提供方法论上的探索，为多语言多域模型的表征分配控制提供实用工具。

Method: 使用在线变分探针框架持续估计标准语言子空间，通过投影基础的解耦技术实现与标准语言表征空间的分离，并在阿拉伯语25种方言上进行实验验证。

Result: 在25种方言上，干预控制措施使生成质量提升最高+4.9 chrF++，平均+2.0，虽然对标准语言性能有所交换。

Conclusion: 高资源语言的子空间占据会限制相关语言的生成能力，给出了因果性证据，并统一了几何与信息论探针技术，为控制多语言多域LLM的表征分配提供了实用工具。

Abstract: Alignment with high-resource standard languages is often assumed to aid the
modeling of related low-resource varieties. We challenge this assumption by
demonstrating that excessive representational entanglement with a dominant
variety, such as Modern Standard Arabic (MSA) in relation to Arabic dialects,
can actively hinder generative modeling. We present the first comprehensive
causal study of this phenomenon by analyzing and directly intervening in the
internal representation geometry of large language models (LLMs). Our key
contribution is an online variational probing framework that continuously
estimates the subspace of the standard variety during fine-tuning, enabling
projection-based decoupling from this space. While our study uses Arabic as a
case due to its unusually rich parallel resources across 25 dialects, the
broader motivation is methodological: dialectal MT serves as a controlled proxy
for generative tasks where comparable multi-variety corpora are unavailable.
Across 25 dialects, our intervention improves generation quality by up to +4.9
chrF++ and +2.0 on average compared to standard fine-tuning, despite a measured
tradeoff in standard-language performance. These results provide causal
evidence that subspace dominance by high-resource varieties can restrict
generative capacity for related varieties. More generally, we unify geometric
and information-theoretic probing with subspace-level causal interventions,
offering practical tools for improving generative modeling in closely related
language families and, more broadly, for controlling representational
allocation in multilingual and multi-domain LLMs. Code will be released.

</details>


### [59] [ding-01 :ARG0: An AMR Corpus for Spontaneous French Dialogue](https://arxiv.org/abs/2508.12819)
*Jeongwoo Kang,Maria Boritchev,Maximin Coavoux*

Main category: cs.CL

TL;DR: 构建法语对话语义语料库，扩展AMR框架适配自发语言特征，并训练AMR解析器作为注解工具


<details>
  <summary>Details</summary>
Motivation: 为法语对话发展语义资源，解决AMR在自发语言动态特征覆盖不足的问题

Method: 注解法语Catan框架游戏对话语料，扩展AMR框架适配自发语言和法语特有句法结构，制定注解指南

Result: 发布免费CC-SA-BY许可的语料库，训练的AMR解析器可作为协助注解工具

Conclusion: 该工作为法语对话语义资源开发做出贡献，提供了扩展的AMR框架和实用的注解工具

Abstract: We present our work to build a French semantic corpus by annotating French
dialogue in Abstract Meaning Representation (AMR). Specifically, we annotate
the DinG corpus, consisting of transcripts of spontaneous French dialogues
recorded during the board game Catan. As AMR has insufficient coverage of the
dynamics of spontaneous speech, we extend the framework to better represent
spontaneous speech and sentence structures specific to French. Additionally, to
support consistent annotation, we provide an annotation guideline detailing
these extensions. We publish our corpus under a free license (CC-SA-BY). We
also train and evaluate an AMR parser on our data. This model can be used as an
assistance annotation tool to provide initial annotations that can be refined
by human annotators. Our work contributes to the development of semantic
resources for French dialogue.

</details>


### [60] [Context Matters: Incorporating Target Awareness in Conversational Abusive Language Detection](https://arxiv.org/abs/2508.12828)
*Raneem Alharthi,Rajwa Alharthi,Aiqi Jiang,Arkaitz Zubiaga*

Main category: cs.CL

TL;DR: 本研究探讨了在社交媒体滥用语言检测中利用上下文信息（父推文）的重要性，发现结合上下文特征能显著提升检测性能，其中基于内容的特征比基于账户的特征贡献更大。


<details>
  <summary>Details</summary>
Motivation: 现有的滥用语言检测研究主要关注单个社交媒体帖子，忽略了可以从周围帖子中获得的额外上下文信息。本研究旨在探索利用父推文的上下文信息是否有助于判断回复帖子是否为滥用内容。

Method: 研究使用了包含对话交换（父推文-回复推文对）的数据集，测试了四种不同的分类模型，比较了仅使用回复推文特征与结合上下文特征的效果，分析了基于内容和基于账户的特征贡献。

Result: 实验表明，结合上下文特征相比仅使用回复推文特征有显著改进。基于内容的特征对分类性能贡献最大，而基于账户的特征贡献较小。使用多种内容特征组合效果最佳。

Conclusion: 上下文信息在滥用语言检测中至关重要，特别是基于内容的特征。研究结果为在涉及对话的现实场景中开发情境化滥用语言检测模型提供了重要见解。

Abstract: Abusive language detection has become an increasingly important task as a
means to tackle this type of harmful content in social media. There has been a
substantial body of research developing models for determining if a social
media post is abusive or not; however, this research has primarily focused on
exploiting social media posts individually, overlooking additional context that
can be derived from surrounding posts. In this study, we look at conversational
exchanges, where a user replies to an earlier post by another user (the parent
tweet). We ask: does leveraging context from the parent tweet help determine if
a reply post is abusive or not, and what are the features that contribute the
most? We study a range of content-based and account-based features derived from
the context, and compare this to the more widely studied approach of only
looking at the features from the reply tweet. For a more generalizable study,
we test four different classification models on a dataset made of
conversational exchanges (parent-reply tweet pairs) with replies labeled as
abusive or not. Our experiments show that incorporating contextual features
leads to substantial improvements compared to the use of features derived from
the reply tweet only, confirming the importance of leveraging context. We
observe that, among the features under study, it is especially the
content-based features (what is being posted) that contribute to the
classification performance rather than account-based features (who is posting
it). While using content-based features, it is best to combine a range of
different features to ensure improved performance over being more selective and
using fewer features. Our study provides insights into the development of
contextualized abusive language detection models in realistic settings
involving conversations.

</details>


### [61] [It takes a village to write a book: Mapping anonymous contributions in Stephen Langton's Quaestiones Theologiae](https://arxiv.org/abs/2508.12830)
*Jan Maliszewski*

Main category: cs.CL

TL;DR: 这篇论文提出了一种采用风格计量技术分析中世纪教育口讲记录文本的方法，以验证关于Stephen Langton神学问题集编诙过程的假设。


<details>
  <summary>Details</summary>
Motivation: 虽然间接证据显示早期经院哲学时代就存在基于口讲教学记录的文学创作，但很少有源泄这种实践。论文强调需要验证关于这些文本集编诙过程的假设。

Method: 采用风格计量作者归属技术，基于最常用词、词性标注和伪后缀进行分析。实施HTR（手写文本识别）流程和风格计量分析。

Result: 这项研究将提供两个方法论政益：直接比较手工编写和自动提取数据的性能，测试基于transformer的OCR和自动转录对齐在经院拓片拓幕中的有效性。

Conclusion: 如果成功，这项研究将为中世纪大学协作文学创作的探索性分析提供一个容易重用的模板，并推进计算教育学传统研究。

Abstract: While the indirect evidence suggests that already in the early scholastic
period the literary production based on records of oral teaching (so-called
reportationes) was not uncommon, there are very few sources commenting on the
practice. This paper details the design of a study applying stylometric
techniques of authorship attribution to a collection developed from
reportationes -- Stephen Langton's Quaestiones Theologiae -- aiming to uncover
layers of editorial work and thus validate some hypotheses regarding the
collection's formation. Following Camps, Cl\'erice, and Pinche (2021), I
discuss the implementation of an HTR pipeline and stylometric analysis based on
the most frequent words, POS tags, and pseudo-affixes. The proposed study will
offer two methodological gains relevant to computational research on the
scholastic tradition: it will directly compare performance on manually composed
and automatically extracted data, and it will test the validity of
transformer-based OCR and automated transcription alignment for workflows
applied to scholastic Latin corpora. If successful, this study will provide an
easily reusable template for the exploratory analysis of collaborative literary
production stemming from medieval universities.

</details>


### [62] [Word Meanings in Transformer Language Models](https://arxiv.org/abs/2508.12863)
*Jumbly Grindrod,Peter Grindrod*

Main category: cs.CL

TL;DR: 这篇论文通过对RoBERTa-base模型的分析，证明了转换器语言模型在词汇嵌入空间中编码了丰富的语义信息，反驳了某些语义消过论假设。


<details>
  <summary>Details</summary>
Motivation: 探索转换器语言模型如何表示词义，特别是是否存在类似于词汇库的语义信息存储机制。

Method: 提取RoBERTa-base模型的token嵌入空间，使用k-means聚类算法将其分为200个聚类，然后通过人工检查和五种心理语言学指标（情感价值、具体性、象征性、禁忌性、获得年龄）进行分析。

Result: 发现词汇嵌入空间中编码了广泛的语义信息，聚类结果显示出对语义信息的敏感性。

Conclusion: 转换器语言模型通过其嵌入空间有效地编码语义信息，这一发现反驳了语义消过论者对模型处理语义方式的假设。

Abstract: We investigate how word meanings are represented in the transformer language
models. Specifically, we focus on whether transformer models employ something
analogous to a lexical store - where each word has an entry that contains
semantic information. To do this, we extracted the token embedding space of
RoBERTa-base and k-means clustered it into 200 clusters. In our first study, we
then manually inspected the resultant clusters to consider whether they are
sensitive to semantic information. In our second study, we tested whether the
clusters are sensitive to five psycholinguistic measures: valence,
concreteness, iconicity, taboo, and age of acquisition. Overall, our findings
were very positive - there is a wide variety of semantic information encoded
within the token embedding space. This serves to rule out certain "meaning
eliminativist" hypotheses about how transformer LLMs process semantic
information.

</details>


### [63] [An LLM Agent-Based Complex Semantic Table Annotation Approach](https://arxiv.org/abs/2508.12868)
*Yilin Geng,Shujing Wang,Chuan Wang,Keqing He,Yanfei Lv,Ying Wang,Zaiwen Feng,Xiaoying Bai*

Main category: cs.CL

TL;DR: 本文提出基于LLM的智能体方法解决语义表格标注(STA)任务，通过ReAct框架设计五个外部工具，在Tough Tables和BiodivTab数据集上表现优于现有方法，并通过Levenshtein距离减少冗余标注，显著降低时间和计算成本。


<details>
  <summary>Details</summary>
Motivation: 复杂表格存在列名/单元格值语义丢失、严格本体层次要求、同义词、拼写错误和缩写等挑战，影响标注准确性，需要新的解决方案。

Method: 基于ReAct框架设计五个外部工具，使用LLM智能体根据表格特征动态选择合适的标注策略，并利用Levenshtein距离减少冗余标注。

Result: 在SemTab挑战的Tough Tables和BiodivTab数据集上，该方法在各种指标上优于现有方法，时间成本降低70%，LLM token使用减少60%。

Conclusion: 该方法为STA任务提供了高效且成本效益高的解决方案，能够有效处理复杂表格的各种挑战。

Abstract: The Semantic Table Annotation (STA) task, which includes Column Type
Annotation (CTA) and Cell Entity Annotation (CEA), maps table contents to
ontology entities and plays important roles in various semantic applications.
However, complex tables often pose challenges such as semantic loss of column
names or cell values, strict ontological hierarchy requirements, homonyms,
spelling errors, and abbreviations, which hinder annotation accuracy. To
address these issues, this paper proposes an LLM-based agent approach for CTA
and CEA. We design and implement five external tools with tailored prompts
based on the ReAct framework, enabling the STA agent to dynamically select
suitable annotation strategies depending on table characteristics. Experiments
are conducted on the Tough Tables and BiodivTab datasets from the SemTab
challenge, which contain the aforementioned challenges. Our method outperforms
existing approaches across various metrics. Furthermore, by leveraging
Levenshtein distance to reduce redundant annotations, we achieve a 70%
reduction in time costs and a 60% reduction in LLM token usage, providing an
efficient and cost-effective solution for STA.

</details>


### [64] [A Stitch in Time Saves Nine: Proactive Self-Refinement for Language Models](https://arxiv.org/abs/2508.12903)
*Jinyi Han,Xinyi Wang,Haiquan Zhao,Tingyun li,Zishang Jiang,Sihang Jiang,Jiaqing Liang,Xin Lin,Weikang Zhou,Zeye Sun,Fei Yu,Yanghua Xiao*

Main category: cs.CL

TL;DR: PASR是一种主动式自优化方法，让大语言模型在生成过程中动态决定是否、何时以及如何优化输出，相比固定迭代次数的传统方法，显著降低了token消耗并提高了准确性。


<details>
  <summary>Details</summary>
Motivation: 现有自优化方法主要依赖固定迭代次数的被动过程，难以根据生成上下文动态确定最佳优化时机和内容，而人类在思考过程中会动态优化自己的想法。

Method: 提出ProActive Self-Refinement (PASR)方法，基于模型内部状态和演化上下文，主动决定是否、何时以及如何优化输出，而不是重新生成整个响应。

Result: 在10个多样化任务上的实验表明，PASR显著提升了问题解决性能。在Qwen3-8B上，相比标准生成平均减少41.6%的token消耗，同时准确率提升8.2%。

Conclusion: PASR通过主动式自优化方法，有效解决了传统固定迭代方法的局限性，在降低计算成本的同时提高了模型性能，为大语言模型的实时优化提供了新思路。

Abstract: Recent advances in self-refinement have demonstrated significant potential
for improving the outputs of large language models (LLMs) through iterative
refinement. However, most existing self-refinement methods rely on a reactive
process with a fixed number of iterations, making it difficult to determine the
optimal timing and content of refinement based on the evolving generation
context. Inspired by the way humans dynamically refine their thoughts during
execution, we propose ProActive Self-Refinement (PASR), a novel method that
enables LLMs to refine their outputs during the generation process. Unlike
methods that regenerate entire responses, PASR proactively decides whether,
when, and how to refine based on the model's internal state and evolving
context. We conduct extensive experiments on a diverse set of 10 tasks to
evaluate the effectiveness of PASR. Experimental results show that PASR
significantly enhances problem-solving performance. In particular, on Qwen3-8B,
PASR reduces average token consumption by 41.6 percent compared to standard
generation, while also achieving an 8.2 percent improvement in accuracy. Our
code and all baselines used in the paper are available in the GitHub.

</details>


### [65] [Analyzing Information Sharing and Coordination in Multi-Agent Planning](https://arxiv.org/abs/2508.12981)
*Tianyue Ou,Saujas Vaduguru,Daniel Fried*

Main category: cs.CL

TL;DR: 多游客规划任务中，通过笔记本共享和组织者协调机制，多游客系统在长期谜划划任务中显著提升性能，最终通过率提高25%


<details>
  <summary>Details</summary>
Motivation: 解决长期谜划划任务中的复杂约束条件和详细信息处理挑战，提升大语言模型多游客系统的规划能力

Method: 构建基于LLM的多游客系统，采用笔记本机制促进信息共享，以及组织者游客来改善自由对话中的协调

Result: 笔记本减少幽灵细节错误18%，组织者在重点子领域进一步减少错误13.5%。组合机制在TravelPlanner净龄上达到25%通过率，相比单游客基准提升17.5%

Conclusion: 结构化信息共享和反思性组织是长期谜划划任务中多游客系统的关键组成部分，显著提升了LLM游客的规划性能

Abstract: Multi-agent systems (MASs) have pushed the boundaries of large language model
(LLM) agents in domains such as web research and software engineering. However,
long-horizon, multi-constraint planning tasks involve conditioning on detailed
information and satisfying complex interdependent constraints, which can pose a
challenge for these systems. In this study, we construct an LLM-based MAS for a
travel planning task which is representative of these challenges. We evaluate
the impact of a notebook to facilitate information sharing, and evaluate an
orchestrator agent to improve coordination in free form conversation between
agents. We find that the notebook reduces errors due to hallucinated details by
18%, while an orchestrator directs the MAS to focus on and further reduce
errors by up to 13.5% within focused sub-areas. Combining both mechanisms
achieves a 25% final pass rate on the TravelPlanner benchmark, a 17.5% absolute
improvement over the single-agent baseline's 7.5% pass rate. These results
highlight the potential of structured information sharing and reflective
orchestration as key components in MASs for long horizon planning with LLMs.

</details>


### [66] [WebMall -- A Multi-Shop Benchmark for Evaluating Web Agents](https://arxiv.org/abs/2508.13024)
*Ralph Peeters,Aaron Steiner,Luca Schwarz,Julian Yuya Caspary,Christian Bizer*

Main category: cs.CL

TL;DR: WebMall是一个多商店在线购物基准测试，用于评估Web代理在比较购物中的效果和效率，包含4个模拟商店和91个跨商店任务，比现有基准更复杂和真实。


<details>
  <summary>Details</summary>
Motivation: 现有的电子商务基准测试如WebShop或ShoppingBench主要关注单一商店内的任务，缺乏跨多个商店的比较购物任务评估，无法充分测试Web代理在真实复杂购物场景中的能力。

Method: 构建包含4个模拟在线商店的基准测试，产品数据来自Common Crawl的真实商品信息，包含91个跨商店任务（基础任务和高级任务），并评估了8个不同配置的基线代理。

Result: 最佳配置在基础任务集上达到75%完成率和87% F1分数，在高级任务集上达到53%完成率和63% F1分数，显示了当前Web代理在复杂购物任务中的性能水平。

Conclusion: WebMall基准测试为Web代理研究提供了更真实、更复杂的评估环境，有助于推动电子商务场景中的导航、推理和效率方面的技术进步，并已公开发布以促进相关研究。

Abstract: LLM-based web agents have the potential to automate long-running web tasks,
such as finding offers for specific products in multiple online shops and
subsequently ordering the cheapest products that meet the users needs. This
paper introduces WebMall, a multi-shop online shopping benchmark for evaluating
the effectiveness and efficiency of web agents for comparison-shopping. WebMall
consists of four simulated online shops populated with authentic product offers
sourced from the Common Crawl, alongside a suite of 91 cross-shop tasks. These
tasks include basic tasks such as finding specific products in multiple shops,
performing price comparisons, adding items to the shopping cart, and completing
checkout. Advanced tasks involve searching for products based on vague
requirements, identifying suitable substitutes, and finding compatible
products. Compared to existing e-commerce benchmarks, such as WebShop or
ShoppingBench, WebMall introduces comparison-shopping tasks across multiple
shops. Furthermore, the product offers are more heterogeneous, as they
originate from hundreds of distinct real-world shops. The tasks in WebMall
require longer interaction trajectories than those in WebShop, while remaining
representative of real-world shopping behaviors. We evaluate eight baseline
agents on WebMall, varying in observation modality, memory utilization, and
underlying large language model (GPT 4.1 and Claude Sonnet 4). The
best-performing configurations achieve completion rates of 75% and 53%, and F1
scores of 87% and 63%, on the basic and advanced task sets, respectively.
WebMall is publicly released to facilitate research on web agents and to
promote advancements in navigation, reasoning, and efficiency within e-commerce
scenarios.

</details>


### [67] [Integrating Feedback Loss from Bi-modal Sarcasm Detector for Sarcastic Speech Synthesis](https://arxiv.org/abs/2508.13028)
*Zhu Li,Yuqing Zhang,Xiyuan Gao,Devraj Raghuvanshi,Nagendra Kumar,Shekhar Nayak,Matt Coler*

Main category: cs.CL

TL;DR: 提出了一种结合双模态讽刺检测模型反馈损失和两阶段微调策略的讽刺语音合成方法，通过迁移学习和多阶段训练提升合成语音的讽刺表达质量


<details>
  <summary>Details</summary>
Motivation: 讽刺语音合成对于增强娱乐和人机交互应用的自然性至关重要，但由于讽刺的微妙韵律特征和标注数据有限，现有方法面临挑战

Method: 1) 将双模态讽刺检测模型的反馈损失整合到TTS训练过程中；2) 采用两阶段微调：先在包含多种语音风格的数据集上微调，再在专门的讽刺语音数据集上进一步优化

Result: 主客观评估表明，所提方法显著提高了合成语音的质量、自然度和讽刺感知能力

Conclusion: 该方法有效解决了讽刺语音合成的挑战，为生成具有丰富情感表达的语音提供了可行方案

Abstract: Sarcastic speech synthesis, which involves generating speech that effectively
conveys sarcasm, is essential for enhancing natural interactions in
applications such as entertainment and human-computer interaction. However,
synthesizing sarcastic speech remains a challenge due to the nuanced prosody
that characterizes sarcasm, as well as the limited availability of annotated
sarcastic speech data. To address these challenges, this study introduces a
novel approach that integrates feedback loss from a bi-modal sarcasm detection
model into the TTS training process, enhancing the model's ability to capture
and convey sarcasm. In addition, by leveraging transfer learning, a speech
synthesis model pre-trained on read speech undergoes a two-stage fine-tuning
process. First, it is fine-tuned on a diverse dataset encompassing various
speech styles, including sarcastic speech. In the second stage, the model is
further refined using a dataset focused specifically on sarcastic speech,
enhancing its ability to generate sarcasm-aware speech. Objective and
subjective evaluations demonstrate that our proposed methods improve the
quality, naturalness, and sarcasm-awareness of synthesized speech.

</details>


### [68] [Can Large Models Teach Student Models to Solve Mathematical Problems Like Human Beings? A Reasoning Distillation Method via Multi-LoRA Interaction](https://arxiv.org/abs/2508.13037)
*Xinhe Li,Jiajun Liu,Peng Wang*

Main category: cs.CL

TL;DR: LoRID是一种基于多LoRA交互的数学推理蒸馏方法，通过模拟人类System 1和System 2两种思维模式，提升小语言模型的数学推理能力，在GSM8K等数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 解决小语言模型(SLMs)数学推理能力差的问题。现有方法主要依赖大语言模型生成大量数据进行填鸭式训练，这类似于心理学中的System 1思维，但人类学习还需要System 2思维（先获取知识再通过实践强化）。

Method: 提出LoRID方法：1）用LLM创建知识增强数据集；2）训练Intuitive Reasoner（IR）直接生成推理链；3）训练Knowledge Generator（KG）输出知识；4）训练Deep Reasoner（DR）利用知识进行推理；5）通过IR和DR输出一致性检查进行迭代推理。

Result: 在GSM8K数据集上，LoRID在五个基础模型上分别比第二好方法提升了2.3%、16.1%、2.4%、12.3%和1.8%的准确率，达到了最先进的性能。

Conclusion: LoRID通过模拟人类双系统思维模式，有效提升了小语言模型的数学推理能力，证明了多LoRA交互蒸馏方法的有效性。

Abstract: Recent studies have demonstrated that Large Language Models (LLMs) have
strong mathematical reasoning abilities but rely on hundreds of billions of
parameters. To tackle the challenge of poor reasoning in Small Language Models
(SLMs), existing methods typically leverage LLMs to generate massive amounts of
data for cramming training. In psychology, they are akin to System 1 thinking,
which resolves reasoning problems rapidly based on experience and intuition.
However, human learning also requires System 2 thinking, where knowledge is
first acquired and then reinforced through practice. Inspired by such two
distinct modes of thinking, we propose a novel method based on the multi-LoRA
Interaction for mathematical reasoning Distillation (LoRID). First, we input
the question and reasoning of each sample into an LLM to create
knowledge-enhanced datasets. Subsequently, we train a LoRA block on the student
model as an Intuitive Reasoner (IR), which directly generates Chain-of-Thoughts
for problem-solving. Then, to imitate System 2 thinking, we train the Knowledge
Generator (KG) and Deep Reasoner (DR), respectively. The former outputs only
knowledge after receiving problems, while the latter uses that knowledge to
perform reasoning. Finally, to address the randomness in the generation of IR
and DR, we evaluate whether their outputs are consistent, and the inference
process needs to be iterated if not. This step can enhance the mathematical
reasoning ability of SLMs through mutual feedback. Experimental results show
that LoRID achieves state-of-the-art performance, especially on the GSM8K
dataset, where it outperforms the second-best method by 2.3%, 16.1%, 2.4%,
12.3%, and 1.8% accuracy across the five base models, respectively.

</details>


### [69] [Büyük Dil Modelleri için TR-MMLU Benchmarkı: Performans Değerlendirmesi, Zorluklar ve İyileştirme Fırsatları](https://arxiv.org/abs/2508.13044)
*M. Ali Bayram,Ali Arda Fincan,Ahmet Semih Gümüş,Banu Diri,Savaş Yıldırım,Öner Aytaş*

Main category: cs.CL

TL;DR: 土耳其语言模型评测标准缺乏，研究人员创建了TR-MMLU评测框架，包含6,200道多选题，用于评估大语言模型在土耳其语上的语言和概念理解能力。


<details>
  <summary>Details</summary>
Motivation: 虽然语言模型取得了重大进展，但对于资源有限的语言（如土耳其语）的评估仍面临挑战。当前缺乏专门为土耳其语言模型设计的维并评测标准。

Method: 基于土耳其教育体系中62个学科的6,200道多选题，细心编译和维护了一个维并的评测数据集。这个标准框架允许对大语言模型在土耳其语文本处理能力进行详细分析。

Result: 研究人员在TR-MMLU上评估了当前最先进的大语言模型，并指出了模型设计中需要改进的领域。

Conclusion: TR-MMLU为推动土耳其语自然语言处理研究设立了新标准，并将激励未来的创新。

Abstract: Language models have made significant advancements in understanding and
generating human language, achieving remarkable success in various
applications. However, evaluating these models remains a challenge,
particularly for resource-limited languages like Turkish. To address this
issue, we introduce the Turkish MMLU (TR-MMLU) benchmark, a comprehensive
evaluation framework designed to assess the linguistic and conceptual
capabilities of large language models (LLMs) in Turkish. TR-MMLU is based on a
meticulously curated dataset comprising 6,200 multiple-choice questions across
62 sections within the Turkish education system. This benchmark provides a
standard framework for Turkish NLP research, enabling detailed analyses of
LLMs' capabilities in processing Turkish text. In this study, we evaluated
state-of-the-art LLMs on TR-MMLU, highlighting areas for improvement in model
design. TR-MMLU sets a new standard for advancing Turkish NLP research and
inspiring future innovations.

</details>


### [70] [Doğal Dil İşlemede Tokenizasyon Standartları ve Ölçümü: Türkçe Üzerinden Büyük Dil Modellerinin Karşılaştırmalı Analizi](https://arxiv.org/abs/2508.13058)
*M. Ali Bayram,Ali Arda Fincan,Ahmet Semih Gümüş,Sercan Karakaş,Banu Diri,Savaş Yıldırım*

Main category: cs.CL

TL;DR: 这研究提出了一个新的评估框架，用于分析对于形态丰富语言（如土耳其语）的分词器性能，发现语言特定分词百分比比分词纯度更能预测下游任务表现，并强调了语言特定分词方法的重要性。


<details>
  <summary>Details</summary>
Motivation: 分词是NLP中的基础预处理步骤，对大语言模型的语言学习能力有重要影响。当前的分词方法在形态丰富和低资源语言（如土耳其语）中面临特殊挑战，需要专门的评估标准来评估其效果。

Method: 使用土耳其教育系统的TR-MMLU数据集（6,200道选择题），提出了一系列新的评估指标：词汇量大小、分词数量、处理时间、语言特定分词百分比（%TR）和分词纯度（%Pure），用于评估分词器保留语言结构的能力。

Result: 分析显示，语言特定分词百分比（%TR）与下游任务表现（如MMLU分数）的相关性更强，而仅仅增加模型参数并不能必然提升语言性能力。

Conclusion: 该研究建立了一个健壮而实用的分词标准框架，特别适用于形态复杂语言。结果强调了采用语言特定的、细致调整的分词方法对于提升语言模型性能的关键作用。

Abstract: Tokenization is a fundamental preprocessing step in Natural Language
Processing (NLP), significantly impacting the capability of large language
models (LLMs) to capture linguistic and semantic nuances. This study introduces
a novel evaluation framework addressing tokenization challenges specific to
morphologically-rich and low-resource languages such as Turkish. Utilizing the
Turkish MMLU (TR-MMLU) dataset, comprising 6,200 multiple-choice questions from
the Turkish education system, we assessed tokenizers based on vocabulary size,
token count, processing time, language-specific token percentages (\%TR), and
token purity (\%Pure). These newly proposed metrics measure how effectively
tokenizers preserve linguistic structures. Our analysis reveals that
language-specific token percentages exhibit a stronger correlation with
downstream performance (e.g., MMLU scores) than token purity. Furthermore,
increasing model parameters alone does not necessarily enhance linguistic
performance, underscoring the importance of tailored, language-specific
tokenization methods. The proposed framework establishes robust and practical
tokenization standards for morphologically complex languages.

</details>


### [71] [Evaluating ASR robustness to spontaneous speech errors: A study of WhisperX using a Speech Error Database](https://arxiv.org/abs/2508.13060)
*John Alderete,Macarious Kin Fung Hui,Aanchan Mohan*

Main category: cs.CL

TL;DR: SFUSED语音错误数据库可用于评估语音识别模型性能，通过分析WhisperX在5300个标注错误上的转录准确率，证明了该数据库作为ASR系统诊断工具的有效性。


<details>
  <summary>Details</summary>
Motivation: 开发一个公开的语音错误数据库，用于语言学和心理语言学研究，并展示其如何用于测试和评估语音识别模型。

Method: 使用SFUSED数据库中的5300个标注的词汇和语音错误，评估WhisperX语音识别模型的转录准确性。数据库包含系统标注的自发英语语音错误，每个错误都标注了预期和实际产出。

Result: 分析证明了SFUSED数据库作为ASR系统性能诊断工具的有效性，能够评估模型在不同分类维度（如语言层级、上下文敏感性等）上的表现。

Conclusion: SFUSED语音错误数据库是一个有价值的工具，可用于全面评估语音识别模型的性能，特别是在处理各种类型语音错误方面的能力。

Abstract: The Simon Fraser University Speech Error Database (SFUSED) is a public data
collection developed for linguistic and psycholinguistic research. Here we
demonstrate how its design and annotations can be used to test and evaluate
speech recognition models. The database comprises systematically annotated
speech errors from spontaneous English speech, with each error tagged for
intended and actual error productions. The annotation schema incorporates
multiple classificatory dimensions that are of some value to model assessment,
including linguistic hierarchical level, contextual sensitivity, degraded
words, word corrections, and both word-level and syllable-level error
positioning. To assess the value of these classificatory variables, we
evaluated the transcription accuracy of WhisperX across 5,300 documented word
and phonological errors. This analysis demonstrates the atabase's effectiveness
as a diagnostic tool for ASR system performance.

</details>


### [72] [Reinforced Context Order Recovery for Adaptive Reasoning and Planning](https://arxiv.org/abs/2508.13070)
*Long Ma,Fangwei Zhong,Yizhou Wang*

Main category: cs.CL

TL;DR: ReCOR是一个基于强化学习的框架，通过自适应选择token生成顺序来提升语言模型在复杂推理任务上的性能，无需标注数据即可学习最优生成顺序。


<details>
  <summary>Details</summary>
Motivation: 当前因果语言模型和扩散模型使用固定或随机的token生成顺序，这与原始逻辑顺序不符，导致在需要自适应生成顺序的复杂推理问题上表现不佳。

Method: 提出ReCOR强化学习框架，通过token预测统计进行自监督，估计每个未填充token的预测难度，在训练和推理过程中自适应选择下一个要生成的token。

Result: 在具有挑战性的推理和规划数据集上，ReCOR表现出优于基线模型的性能，有时甚至超过使用真实顺序监督的oracle模型。

Conclusion: 自适应token生成顺序对于复杂推理任务至关重要，ReCOR框架能够有效学习数据依赖的生成顺序，显著提升模型性能。

Abstract: Modern causal language models, followed by rapid developments in discrete
diffusion models, can now produce a wide variety of interesting and useful
content. However, these families of models are predominantly trained to output
tokens with a fixed (left-to-right) or random order, which may deviate from the
logical order in which tokens are generated originally. In this paper, we
observe that current causal and diffusion models encounter difficulties in
problems that require adaptive token generation orders to solve tractably,
which we characterize with the $\mathcal{V}$-information framework. Motivated
by this, we propose Reinforced Context Order Recovery (ReCOR), a
reinforcement-learning-based framework to extract adaptive, data-dependent
token generation orders from text data without annotations. Self-supervised by
token prediction statistics, ReCOR estimates the hardness of predicting every
unfilled token and adaptively selects the next token during both training and
inference. Experiments on challenging reasoning and planning datasets
demonstrate the superior performance of ReCOR compared with baselines,
sometimes outperforming oracle models supervised with the ground-truth order.

</details>


### [73] [DocHPLT: A Massively Multilingual Document-Level Translation Dataset](https://arxiv.org/abs/2508.13079)
*Dayyán O'Brien,Bhavitvya Malik,Ona de Gibert,Pinzhen Chen,Barry Haddow,Jörg Tiedemann*

Main category: cs.CL

TL;DR: DocHPLT是迄今为止最大的公开文档级翻译数据集，包含1.24亿个文档对，涵盖50种语言与英语的配对，共42.6亿个句子，为多语言文档级翻译提供重要基础设施。


<details>
  <summary>Details</summary>
Motivation: 现有文档级机器翻译资源仅适用于少数高资源语言，为了促进文档级翻译训练和评估，以及更广泛的全球社区长上下文建模，需要创建大规模多语言文档级翻译数据集。

Method: 修改现有的网页提取流程，从源头保留完整的文档完整性（包括未对齐部分），而不是基于句子级数据重建文档。通过初步实验确定最佳训练上下文策略。

Result: 在DocHPLT上微调的大型语言模型显著优于现成的指令调优基线模型，特别是对资源匮乏语言的改进尤为显著。

Conclusion: DocHPLT数据集在宽松许可下开源，为推进多语言文档级翻译提供了必要的基础设施，特别有助于资源匮乏语言的发展。

Abstract: Existing document-level machine translation resources are only available for
a handful of languages, mostly high-resourced ones. To facilitate the training
and evaluation of document-level translation and, more broadly, long-context
modeling for global communities, we create DocHPLT, the largest publicly
available document-level translation dataset to date. It contains 124 million
aligned document pairs across 50 languages paired with English, comprising 4.26
billion sentences, with further possibility to provide 2500 bonus pairs not
involving English. Unlike previous reconstruction-based approaches that piece
together documents from sentence-level data, we modify an existing web
extraction pipeline to preserve complete document integrity from the source,
retaining all content including unaligned portions. After our preliminary
experiments identify the optimal training context strategy for document-level
translation, we demonstrate that LLMs fine-tuned on DocHPLT substantially
outperform off-the-shelf instruction-tuned baselines, with particularly
dramatic improvements for under-resourced languages. We open-source the dataset
under a permissive license, providing essential infrastructure for advancing
multilingual document-level translation.

</details>


### [74] [All for law and law for all: Adaptive RAG Pipeline for Legal Research](https://arxiv.org/abs/2508.13107)
*Figarri Keisha,Prince Singh,Pallavi,Dion Fernandes,Aravindh Manivannan,Ilham Wicaksono,Faisal Ahmad*

Main category: cs.CL

TL;DR: 通过上下文感知查询翻译器、开源提取策略和综合评估框架三项改进，构建了一个效果更好、成本更低的法律领域RAG系统，在查询质量和回答忠实性方面超过了专有方案。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在法律领域中的幻觉问题，通过基于来源的RAG技术提高输出的可靠性和准确性，适合法律研究的高要求。

Method: 建立了一个结合上下文感知查询翻译器的端到端RAG流水线，使用SBERT和GTE引擎进行开源提取，并使用RAGAS、BERTScore-F1和ROUGE-Recall进行综合评估。

Result: 提取策略在Recall@K上提升30-95%，在Precision@K上提升约2.5倍（K>4时），开源流水线在提取质量上可与专有方案相比或更优，自定义法律提示能产生更忠实、上下文相关的答案。

Conclusion: 通过任务感知的组件级调整，可以建立基于法律基础、可复现且成本效益高的RAG系统，为法律研究提供有效支持。

Abstract: Retrieval-Augmented Generation (RAG) mitigates hallucinations by grounding
large language model outputs in cited sources, a capability that is especially
critical in the legal domain. We present an end-to-end RAG pipeline that
revisits and extends the LegalBenchRAG baseline with three targeted
enhancements: (i) a context-aware query translator that disentangles document
references from natural-language questions and adapts retrieval depth and
response style based on expertise and specificity, (ii) open-source retrieval
strategies using SBERT and GTE embeddings that achieve substantial performance
gains (improving Recall@K by 30-95\% and Precision@K by $\sim$2.5$\times$ for
$K>4$) while remaining cost-efficient, and (iii) a comprehensive evaluation and
generation framework that combines RAGAS, BERTScore-F1, and ROUGE-Recall to
assess semantic alignment and faithfulness across models and prompt designs.
Our results show that carefully designed open-source pipelines can rival or
outperform proprietary approaches in retrieval quality, while a custom
legal-grounded prompt consistently produces more faithful and contextually
relevant answers than baseline prompting. Taken together, these contributions
demonstrate the potential of task-aware, component-level tuning to deliver
legally grounded, reproducible, and cost-effective RAG systems for legal
research assistance.

</details>


### [75] [AutoBnB-RAG: Enhancing Multi-Agent Incident Response with Retrieval-Augmented Generation](https://arxiv.org/abs/2508.13118)
*Zefang Liu,Arman Anwar*

Main category: cs.CL

TL;DR: AutoBnB-RAG是一个基于检索增强生成的多代理事件响应框架，通过在AutoBnB框架中集成RAG技术，使LLM代理能够在网络安全事件调查中访问外部知识，提高决策质量。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在模拟事件响应中的推理能力受限于缺乏外部知识访问，需要增强其检索能力来支持更明智的网络安全决策。

Method: 在Backdoors & Breaches桌面游戏环境中构建AutoBnB-RAG框架，引入两种检索设置：基于技术文档的RAG-Wiki和基于事件报告的RAG-News，评估8种团队结构。

Result: 检索增强显著提高了决策质量和成功率，能够重建复杂的多阶段网络攻击，在不同组织模型中均表现良好。

Conclusion: 将检索机制集成到基于LLM的多代理系统中对网络安全决策具有重要价值，AutoBnB-RAG框架展示了这种集成在实际网络事件响应中的有效性。

Abstract: Incident response (IR) requires fast, coordinated, and well-informed
decision-making to contain and mitigate cyber threats. While large language
models (LLMs) have shown promise as autonomous agents in simulated IR settings,
their reasoning is often limited by a lack of access to external knowledge. In
this work, we present AutoBnB-RAG, an extension of the AutoBnB framework that
incorporates retrieval-augmented generation (RAG) into multi-agent incident
response simulations. Built on the Backdoors & Breaches (B&B) tabletop game
environment, AutoBnB-RAG enables agents to issue retrieval queries and
incorporate external evidence during collaborative investigations. We introduce
two retrieval settings: one grounded in curated technical documentation
(RAG-Wiki), and another using narrative-style incident reports (RAG-News). We
evaluate performance across eight team structures, including newly introduced
argumentative configurations designed to promote critical reasoning. To
validate practical utility, we also simulate real-world cyber incidents based
on public breach reports, demonstrating AutoBnB-RAG's ability to reconstruct
complex multi-stage attacks. Our results show that retrieval augmentation
improves decision quality and success rates across diverse organizational
models. This work demonstrates the value of integrating retrieval mechanisms
into LLM-based multi-agent systems for cybersecurity decision-making.

</details>


### [76] [Spot the BlindSpots: Systematic Identification and Quantification of Fine-Grained LLM Biases in Contact Center Summaries](https://arxiv.org/abs/2508.13124)
*Kawin Mayilvaghanan,Siddhant Gupta,Ayush Kumar*

Main category: cs.CL

TL;DR: 提出了BlindSpot框架来检测和量化LLM在客服中心摘要生成中的操作偏见，发现所有模型都存在系统性偏见


<details>
  <summary>Details</summary>
Motivation: LLM在客服中心生成大量通话摘要，但存在潜在的操作偏见（如口吃、说话者、话题等维度），需要系统性的检测方法

Method: 构建15个操作偏见维度的分类体系，使用LLM作为零样本分类器计算转录本和摘要的分布差异，用Fidelity Gap和Coverage两个指标量化偏见

Result: 对2500个真实通话和20个不同规模LLM生成的摘要进行分析，发现所有模型无论大小或家族都存在系统性偏见

Conclusion: 操作偏见在LLM摘要生成中普遍存在，需要专门的检测框架来识别和量化这些偏见

Abstract: Abstractive summarization is a core application in contact centers, where
Large Language Models (LLMs) generate millions of summaries of call transcripts
daily. Despite their apparent quality, it remains unclear whether LLMs
systematically under- or over-attend to specific aspects of the transcript,
potentially introducing biases in the generated summary. While prior work has
examined social and positional biases, the specific forms of bias pertinent to
contact center operations - which we term Operational Bias - have remained
unexplored. To address this gap, we introduce BlindSpot, a framework built upon
a taxonomy of 15 operational bias dimensions (e.g., disfluency, speaker, topic)
for the identification and quantification of these biases. BlindSpot leverages
an LLM as a zero-shot classifier to derive categorical distributions for each
bias dimension in a pair of transcript and its summary. The bias is then
quantified using two metrics: Fidelity Gap (the JS Divergence between
distributions) and Coverage (the percentage of source labels omitted). Using
BlindSpot, we conducted an empirical study with 2500 real call transcripts and
their summaries generated by 20 LLMs of varying scales and families (e.g., GPT,
Llama, Claude). Our analysis reveals that biases are systemic and present
across all evaluated models, regardless of size or family.

</details>


### [77] [MuDRiC: Multi-Dialect Reasoning for Arabic Commonsense Validation](https://arxiv.org/abs/2508.13130)
*Kareem Elozeiri,Mervat Abassy,Preslav Nakov,Yuxia Wang*

Main category: cs.CL

TL;DR: 这篇论文提出了MuDRiC数据集和一种基于图印象印印印网络的新方法，用于阿拉伯语多方言的常识验证任务，补充了现有资源主要集中在现代标准阿拉伯语的空白。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语常识验证任务在英语上取得了显著进步，但在阿拉伯语中研究较少，尤其是考虑到阿拉伯语丰富的语言多样性。现有资源主要集中在现代标准阿拉伯语(MSA)，而地方方言在口语中广泛使用却被忽视。

Method: 提出了一种新的方法，适配图印象印印印网络(GCNs)用于阿拉伯语常识推理，通过增强语义关系建模来改善常识验证的性能。

Result: 实验结果表明，该方法在阿拉伯语常识验证任务上取得了优异的性能。

Conclusion: 这项工作通过提供基础数据集和新的方法，增强了阿拉伯语自然语言理解能力，能够处理其复杂的语言变体。这是第一个阿拉语多方言常识推理数据集。

Abstract: Commonsense validation evaluates whether a sentence aligns with everyday
human understanding, a critical capability for developing robust natural
language understanding systems. While substantial progress has been made in
English, the task remains underexplored in Arabic, particularly given its rich
linguistic diversity. Existing Arabic resources have primarily focused on
Modern Standard Arabic (MSA), leaving regional dialects underrepresented
despite their prevalence in spoken contexts. To bridge this gap, we present two
key contributions: (i) we introduce MuDRiC, an extended Arabic commonsense
dataset incorporating multiple dialects, and (ii) a novel method adapting Graph
Convolutional Networks (GCNs) to Arabic commonsense reasoning, which enhances
semantic relationship modeling for improved commonsense validation. Our
experimental results demonstrate that this approach achieves superior
performance in Arabic commonsense validation. Our work enhances Arabic natural
language understanding by providing both a foundational dataset and a novel
method for handling its complex variations. To the best of our knowledge, we
release the first Arabic multi-dialect commonsense reasoning dataset.

</details>


### [78] [Improving Detection of Watermarked Language Models](https://arxiv.org/abs/2508.13131)
*Dara Bahri,John Wieting*

Main category: cs.CL

TL;DR: 水印检测在语言模型产出检测中效果受到模型熵的限制，本文探索通过结合水印检测器和非水印检测器的混合方案来提高检测性能。


<details>
  <summary>Details</summary>
Motivation: 由于指令循环调整或RLHF等后训练模型的熵较低，单纯依靠水印检测效果有限，需要提高检测能力。

Method: 探索多种混合检测方案，将水印检测器与非水印检测器结合使用。

Result: 在广泛的实验条件下，混合检测方案在性能上超过了任何单一类型检测器。

Conclusion: 通过结合不同检测方法，可以有效提高对语言模型生成内容的检测效果。

Abstract: Watermarking has recently emerged as an effective strategy for detecting the
generations of large language models (LLMs). The strength of a watermark
typically depends strongly on the entropy afforded by the language model and
the set of input prompts. However, entropy can be quite limited in practice,
especially for models that are post-trained, for example via instruction tuning
or reinforcement learning from human feedback (RLHF), which makes detection
based on watermarking alone challenging. In this work, we investigate whether
detection can be improved by combining watermark detectors with non-watermark
ones. We explore a number of hybrid schemes that combine the two, observing
performance gains over either class of detector under a wide range of
experimental conditions.

</details>


### [79] [OptimalThinkingBench: Evaluating Over and Underthinking in LLMs](https://arxiv.org/abs/2508.13141)
*Pranjal Aggarwal,Seungone Kim,Jack Lanchantin,Sean Welleck,Jason Weston,Ilia Kulikov,Swarnadeep Saha*

Main category: cs.CL

TL;DR: OptimalThinkingBench是一个统一基准，用于评估LLMs的过度思考（overthinking）和思考不足（underthinking）问题，包含72个简单查询领域的OverthinkingBench和11个挑战性推理任务的UnderthinkingBench两个子基准。


<details>
  <summary>Details</summary>
Motivation: 现有的思考型LLMs在复杂任务上表现更好但计算成本高且对简单问题过度思考，而非思考型LLMs虽然更快更便宜但在困难推理问题上思考不足，需要用户自行选择合适模型。

Method: 开发了包含两个子基准的统一评测框架，使用新颖的思考调整准确率指标，对33种不同思考和非思考模型进行了广泛评估。

Result: 没有模型能在该基准上实现最优思考。思考型模型在简单查询上过度思考数百个token但性能没有提升，大型非思考型模型思考不足，表现不如更小的思考型模型。

Conclusion: 需要开发更好的统一最优模型，当前方法往往在一个子基准上改进却牺牲另一个子基准的性能。

Abstract: Thinking LLMs solve complex tasks at the expense of increased compute and
overthinking on simpler problems, while non-thinking LLMs are faster and
cheaper but underthink on harder reasoning problems. This has led to the
development of separate thinking and non-thinking LLM variants, leaving the
onus of selecting the optimal model for each query on the end user. In this
work, we introduce OptimalThinkingBench, a unified benchmark that jointly
evaluates overthinking and underthinking in LLMs and also encourages the
development of optimally-thinking models that balance performance and
efficiency. Our benchmark comprises two sub-benchmarks: OverthinkingBench,
featuring simple queries in 72 domains, and UnderthinkingBench, containing 11
challenging reasoning tasks. Using novel thinking-adjusted accuracy metrics, we
perform extensive evaluation of 33 different thinking and non-thinking models
and show that no model is able to optimally think on our benchmark. Thinking
models often overthink for hundreds of tokens on the simplest user queries
without improving performance. In contrast, large non-thinking models
underthink, often falling short of much smaller thinking models. We further
explore several methods to encourage optimal thinking, but find that these
approaches often improve on one sub-benchmark at the expense of the other,
highlighting the need for better unified and optimal models in the future.

</details>


### [80] [Signal and Noise: A Framework for Reducing Uncertainty in Language Model Evaluation](https://arxiv.org/abs/2508.13144)
*David Heineman,Valentin Hofmann,Ian Magnusson,Yuling Gu,Noah A. Smith,Hannaneh Hajishirzi,Kyle Lo,Jesse Dodge*

Main category: cs.CL

TL;DR: 这篇论文分析了评估基准的信号和噪音特性，提出了三种提高基准质量的干预方法，并建议选择高信号低噪音的评估基准来提高模型决策的可靠性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型开发成本昂贵，需要通过小规模实验做出决策。但当前的多任务评估基准在可靠性方面存在问题，影响了小规模实验的决策质量。

Method: 引入了两个关键指标：信号（benchmark区分模型好坏的能力）和噪音（benchmark对训练步驿随机变化的敏感性）。提出了三种干预方法：改用更好的指标（如用perplexity替代accuracy）、过滤噪音子任务、平均中间checkpoint输出来降噪。基于30个基准和375个模型的900K评估结果进行实验分析。

Result: 证明了信号-噪音比更好的benchmark在小规模实验中更可靠，且噪音更低的benchmark有更低的缩放律预测错误。三种干预方法都能有效提高评估的可靠性和准确性。

Conclusion: 建议在创建新基准或选择现有基准时，应该选择具有高信号和低噪音特性的benchmark，这样可以提高小规模实验的决策质量和可靠性。

Abstract: Developing large language models is expensive and involves making decisions
with small experiments, typically by evaluating on large, multi-task evaluation
suites. In this work, we analyze specific properties which make a benchmark
more reliable for such decisions, and interventions to design higher-quality
evaluation benchmarks. We introduce two key metrics that show differences in
current benchmarks: signal, a benchmark's ability to separate better models
from worse models, and noise, a benchmark's sensitivity to random variability
between training steps. We demonstrate that benchmarks with a better
signal-to-noise ratio are more reliable when making decisions at small scale,
and those with less noise have lower scaling law prediction error. These
results suggest that improving signal or noise will lead to more useful
benchmarks, so we introduce three interventions designed to directly affect
signal or noise. For example, we propose that switching to a metric that has
better signal and noise (e.g., perplexity rather than accuracy) leads to better
reliability and improved scaling law error. We also find that filtering noisy
subtasks, to improve an aggregate signal-to-noise ratio, leads to more reliable
multi-task evaluations. We also find that averaging the output of a model's
intermediate checkpoints to reduce noise leads to consistent improvements. We
conclude by recommending that those creating new benchmarks, or selecting which
existing benchmarks to use, aim for high signal and low noise. We use 30
benchmarks for these experiments, and 375 open-weight language models from 60M
to 32B parameters, resulting in a new, publicly available dataset of 900K
evaluation benchmark results, totaling 200M instances.

</details>


### [81] [RepreGuard: Detecting LLM-Generated Text by Revealing Hidden Representation Patterns](https://arxiv.org/abs/2508.13152)
*Xin Chen,Junchao Wu,Shu Yang,Runzhe Zhan,Zeyu Wu,Ziyang Luo,Di Wang,Min Yang,Lidia S. Chao,Derek F. Wong*

Main category: cs.CL

TL;DR: RepreGuard是一种基于LLM内部表示统计特征的检测方法，在分布内外场景下都能有效区分AI生成文本和人类写作文本，平均AUROC达到94.92%


<details>
  <summary>Details</summary>
Motivation: 现有检测方法在分布外场景下的鲁棒性不足，LLM内部表示包含更全面和原始的特征，能更好地区分AI生成文本和人类写作文本的统计模式差异

Method: 使用代理模型收集两种文本的表示，提取能更好识别AI生成文本的激活特征，通过计算文本表示在该特征方向上的投影分数并与预计算阈值比较进行分类

Result: 在分布内外场景下平均AUROC达到94.92%，优于所有基线方法，对不同文本长度和主流攻击具有鲁棒性

Conclusion: LLM内部表示确实包含更有效的区分特征，RepreGuard方法在检测AI生成文本方面表现出色且鲁棒

Abstract: Detecting content generated by large language models (LLMs) is crucial for
preventing misuse and building trustworthy AI systems. Although existing
detection methods perform well, their robustness in out-of-distribution (OOD)
scenarios is still lacking. In this paper, we hypothesize that, compared to
features used by existing detection methods, the internal representations of
LLMs contain more comprehensive and raw features that can more effectively
capture and distinguish the statistical pattern differences between
LLM-generated texts (LGT) and human-written texts (HWT). We validated this
hypothesis across different LLMs and observed significant differences in neural
activation patterns when processing these two types of texts. Based on this, we
propose RepreGuard, an efficient statistics-based detection method.
Specifically, we first employ a surrogate model to collect representation of
LGT and HWT, and extract the distinct activation feature that can better
identify LGT. We can classify the text by calculating the projection score of
the text representations along this feature direction and comparing with a
precomputed threshold. Experimental results show that RepreGuard outperforms
all baselines with average 94.92% AUROC on both in-distribution (ID) and OOD
scenarios, while also demonstrating robust resilience to various text sizes and
mainstream attacks. Data and code are publicly available at:
https://github.com/NLP2CT/RepreGuard

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [82] [Insight Rumors: A Novel Textual Rumor Locating and Marking Model Leveraging Att_BiMamba2 Network](https://arxiv.org/abs/2508.12574)
*Bin Ma,Yifei Zhang,Yongjin Xian,Qi Li,Linna Zhou,Gongxun Miao*

Main category: cs.SI

TL;DR: 本文提出了一种新的谣言检测模型Insight Rumors，能够不仅识别谣言还能准确定位和标记谣言内容，超越了以往只能大致判断谣言的方案。


<details>
  <summary>Details</summary>
Motivation: 现有谣言检测模型主要关注文本是否为谣言，缺乏对具体谣言内容的定位和标记能力，需要解决这一限制。

Method: 提出了具有点积注意力的双向Mamba2网络(Att_BiMamba2)，通过跨连接网络将高维谣言特征投影到低维标签特征，并使用条件随机场(CRF)对输出标签进行约束以确保准确定位。

Result: 综合实验表明，该方案不仅能够准确检测谣言，还能在文本中精确定位和标记谣言内容，表现超过了只能大致识别谣言的最新方案。

Conclusion: 该研究成功开发了一种能够同时进行谣言检测和精确定位的模型，为社交媒体谣言监测提供了更加完整的解决方案，具有重要的应用价值。

Abstract: With the development of social media networks, rumor detection models have
attracted more and more attention. Whereas, these models primarily focus on
classifying contexts as rumors or not, lacking the capability to locate and
mark specific rumor content. To address this limitation, this paper proposes a
novel rumor detection model named Insight Rumors to locate and mark rumor
content within textual data. Specifically, we propose the Bidirectional Mamba2
Network with Dot-Product Attention (Att_BiMamba2), a network that constructs a
bidirectional Mamba2 model and applies dot-product attention to weight and
combine the outputs from both directions, thereby enhancing the representation
of high-dimensional rumor features. Simultaneously, a Rumor Locating and
Marking module is designed to locate and mark rumors. The module constructs a
skip-connection network to project high-dimensional rumor features onto
low-dimensional label features. Moreover, Conditional Random Fields (CRF) is
employed to impose strong constraints on the output label features, ensuring
accurate rumor content location. Additionally, a labeled dataset for rumor
locating and marking is constructed, with the effectiveness of the proposed
model is evaluated through comprehensive experiments. Extensive experiments
indicate that the proposed scheme not only detects rumors accurately but also
locates and marks them in context precisely, outperforming state-of-the-art
schemes that can only discriminate rumors roughly.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [83] [Sparse Attention across Multiple-context KV Cache](https://arxiv.org/abs/2508.11661)
*Ziyi Cao,Qingyi Si,Jingbin Zhang,Bingquan Liu*

Main category: cs.LG

TL;DR: SamKV是首个针对多上下文KV缓存进行注意力稀疏化的方法，通过考虑其他上下文的互补信息进行稀疏化并局部重计算，在RAG场景中将序列长度压缩至15%且不损失精度


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在长序列推理中面临显著成本挑战，现有稀疏注意力方法仅限于单上下文场景，无法处理RAG中多上下文KV缓存缺乏交叉注意力的问题

Method: SamKV方法在稀疏化一个上下文时考虑其他上下文的互补信息，然后局部重计算被稀疏化的信息

Result: 实验表明该方法将序列长度压缩到15%，相比完全重计算基线没有精度损失，显著提升了多上下文RAG场景的吞吐量

Conclusion: SamKV成功解决了多上下文KV缓存的高效压缩问题，为RAG场景中的长序列推理提供了有效的解决方案

Abstract: Large language models face significant cost challenges in long-sequence
inference. To address this, reusing historical Key-Value (KV) Cache for
improved inference efficiency has become a mainstream approach. Recent advances
further enhance throughput by sparse attention mechanisms to select the most
relevant KV Cache, thereby reducing sequence length. However, such techniques
are limited to single-context scenarios, where historical KV Cache is computed
sequentially with causal-attention dependencies. In retrieval-augmented
generation (RAG) scenarios, where retrieved documents as context are unknown
beforehand, each document's KV Cache is computed and stored independently
(termed multiple-context KV Cache), lacking cross-attention between contexts.
This renders existing methods ineffective. Although prior work partially
recomputes multiple-context KV Cache to mitigate accuracy loss from missing
cross-attention, it requires retaining all KV Cache throughout, failing to
reduce memory overhead. This paper presents SamKV, the first exploration of
attention sparsification for multiple-context KV Cache. Specifically, SamKV
takes into account the complementary information of other contexts when
sparsifying one context, and then locally recomputes the sparsified
information. Experiments demonstrate that our method compresses sequence length
to 15% without accuracy degradation compared with full-recompuation baselines,
significantly boosting throughput in multi-context RAG scenarios.

</details>


### [84] [Assessing Representation Stability for Transformer Models](https://arxiv.org/abs/2508.11667)
*Bryan E. Tuck,Rakesh M. Verma*

Main category: cs.LG

TL;DR: 提出了Representation Stability (RS)框架，通过测量掩码重要词时嵌入表示的变化来检测对抗文本攻击，无需重新训练模型，在多个数据集和攻击类型上达到88%以上的检测准确率。


<details>
  <summary>Details</summary>
Motivation: 对抗文本攻击对transformer模型构成持续威胁，现有防御方法要么针对特定攻击，要么需要昂贵的模型重新训练成本。

Method: RS框架首先使用重要性启发式对单词进行排序，然后测量掩码前k个关键词时的嵌入敏感性，最后使用BiLSTM检测器处理得到的模式。

Result: 在三个数据集、三种攻击类型和两个受害模型上，RS达到超过88%的检测准确率，计算成本较低，且能很好地泛化到未见过的数据集、攻击和模型。

Conclusion: RS提供了一个实用的对抗文本检测解决方案，无需重新训练即可有效检测多种对抗攻击，梯度基排名方法在扰动识别质量上表现最佳。

Abstract: Adversarial text attacks remain a persistent threat to transformer models,
yet existing defenses are typically attack-specific or require costly model
retraining. We introduce Representation Stability (RS), a model-agnostic
detection framework that identifies adversarial examples by measuring how
embedding representations change when important words are masked. RS first
ranks words using importance heuristics, then measures embedding sensitivity to
masking top-k critical words, and processes the resulting patterns with a
BiLSTM detector. Experiments show that adversarially perturbed words exhibit
disproportionately high masking sensitivity compared to naturally important
words. Across three datasets, three attack types, and two victim models, RS
achieves over 88% detection accuracy and demonstrates competitive performance
compared to existing state-of-the-art methods, often at lower computational
cost. Using Normalized Discounted Cumulative Gain (NDCG) to measure
perturbation identification quality, we reveal that gradient-based ranking
outperforms attention and random selection approaches, with identification
quality correlating with detection performance for word-level attacks. RS also
generalizes well to unseen datasets, attacks, and models without retraining,
providing a practical solution for adversarial text detection.

</details>


### [85] [Generative Medical Event Models Improve with Scale](https://arxiv.org/abs/2508.12104)
*Shane Waxler,Paul Blazek,Davis White,Daniel Sneider,Kevin Chung,Mani Nagarathnam,Patrick Williams,Hank Voeller,Karen Wong,Matthew Swanhorst,Sheng Zhang,Naoto Usuyama,Cliff Wong,Tristan Naumann,Hoifung Poon,Andrew Loza,Daniella Meeker,Seth Hain,Rahul Shah*

Main category: cs.LG

TL;DR: CoMET是基于163亿次医疗事件训练的大型医疗基础模型，通过自回归生成模拟患者健康时间线，在78个医疗任务中无需微调即可达到或超越专用模型性能


<details>
  <summary>Details</summary>
Motivation: 实现规模化个性化医疗需要从纵向患者旅程中提取洞察，基础模型预训练在大规模医疗事件数据上代表了扩展真实世界证据生成和泛化到多样化下游任务的有前景方向

Method: 使用Epic Cosmos数据集（163亿次医疗事件、3亿患者记录），训练解码器Transformer模型CoMET，进行自回归医疗事件生成，建立了医疗数据的缩放定律研究

Result: CoMET在诊断预测、疾病预后和医疗运营等78个任务中，无需任务特定微调或few-shot示例，普遍优于或匹配专用监督模型，预测能力随模型规模和预训练规模持续提升

Conclusion: CoMET作为生成式医疗事件基础模型，能有效捕捉复杂临床动态，为支持临床决策、简化医疗运营和改善患者结局提供了可扩展和可泛化的框架

Abstract: Realizing personalized medicine at scale calls for methods that distill
insights from longitudinal patient journeys, which can be viewed as a sequence
of medical events. Foundation models pretrained on large-scale medical event
data represent a promising direction for scaling real-world evidence generation
and generalizing to diverse downstream tasks. Using Epic Cosmos, a dataset with
medical events from de-identified longitudinal health records for 16.3 billion
encounters over 300 million unique patient records from 310 health systems, we
introduce the Cosmos Medical Event Transformer ( CoMET) models, a family of
decoder-only transformer models pretrained on 118 million patients representing
115 billion discrete medical events (151 billion tokens). We present the
largest scaling-law study for medical event data, establishing a methodology
for pretraining and revealing power-law scaling relationships for compute,
tokens, and model size. Based on this, we pretrained a series of
compute-optimal models with up to 1 billion parameters. Conditioned on a
patient's real-world history, CoMET autoregressively generates the next medical
event, simulating patient health timelines. We studied 78 real-world tasks,
including diagnosis prediction, disease prognosis, and healthcare operations.
Remarkably for a foundation model with generic pretraining and simulation-based
inference, CoMET generally outperformed or matched task-specific supervised
models on these tasks, without requiring task-specific fine-tuning or few-shot
examples. CoMET's predictive power consistently improves as the model and
pretraining scale. Our results show that CoMET, a generative medical event
foundation model, can effectively capture complex clinical dynamics, providing
an extensible and generalizable framework to support clinical decision-making,
streamline healthcare operations, and improve patient outcomes.

</details>


### [86] [DynamixSFT: Dynamic Mixture Optimization of Instruction Tuning Collections](https://arxiv.org/abs/2508.12116)
*Haebin Shin,Lei Ji,Xiao Liu,Zhiwei Yu,Qi Chen,Yeyun Gong*

Main category: cs.LG

TL;DR: DynamixSFT是一种动态自动化的指令调优数据集混合优化方法，通过多臂老虎机框架和先验缩放玻尔兹曼探索来动态平衡数据集采样概率，在Tulu-v2数据集集合上实现了2.2%的性能提升


<details>
  <summary>Details</summary>
Motivation: 随着指令调优数据集不断涌现，如何在训练过程中动态平衡和优化这些数据集的混合比例成为一个关键挑战

Method: 将问题建模为多臂老虎机框架，提出先验缩放玻尔兹曼探索方法，使用轻量级1步前瞻奖励来更新采样概率，保持原始数据集比例的软锚定

Result: 在包含16个指令调优数据集的Tulu-v2集合上，DynamixSFT在10个基准测试中实现了最高2.2%的性能提升

Conclusion: 该方法能够有效动态优化数据集混合比例，同时保持数据集的固有多样性和覆盖范围，为指令调优提供了有效的自动化解决方案

Abstract: As numerous instruction-tuning datasets continue to emerge during the
post-training stage, dynamically balancing and optimizing their mixtures has
become a critical challenge. To address this, we propose DynamixSFT, a dynamic
and automated method for instruction-tuning dataset mixture optimization. We
formulate the problem as a multi-armed bandit setup and introduce a
Prior-scaled Boltzmann Exploration that softly anchors the updated sampling
distribution to the original dataset proportions, thereby preserving the
inherent diversity and coverage of the collection. Sampling probabilities are
updated using a lightweight 1-Step Look-ahead Reward, reflecting how much the
dataset contributes to improving the model's performance at its current state.
When applied to the Tulu-v2-mixture collection comprising 16 instruction-tuning
datasets, DynamixSFT achieves up to a 2.2% performance improvement across 10
benchmarks. Furthermore, we provide a comprehensive analysis and visualizations
to offer deeper insights into the adaptive dynamics of our method.

</details>


### [87] [Bridging Human and LLM Judgments: Understanding and Narrowing the Gap](https://arxiv.org/abs/2508.12792)
*Felipe Maia Polo,Xinhe Wang,Mikhail Yurochkin,Gongjun Xu,Moulinath Banerjee,Yuekai Sun*

Main category: cs.LG

TL;DR: Bridge是一个统一的统计框架，通过建模人类偏好和LLM评估之间的系统性差异，提高LLM作为评判者的准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型被广泛用作评判者来评估模型输出，但其评估结果与人类判断存在系统性差异，需要一种方法来弥合这种差距。

Method: 提出了Bridge统计框架，假设每个提示-响应对存在潜在人类偏好分数，将LLM偏差建模为捕获差异来源的协变量的线性变换。提供了具有渐近保证的高效拟合算法。

Result: 在六个LLM评判者和两个基准测试(BigGen Bench和Chatbot Arena)上，Bridge实现了与人类评分更高的一致性(准确性、校准和KL散度)，并揭示了系统性的人-LLM差距。

Conclusion: Bridge提供了一个简单而有原则的框架，用于改进LLM评分并表征人类与LLM之间的系统性差异，为解决LLM评估偏差问题提供了有效解决方案。

Abstract: Large language models are increasingly used as judges (LLM-as-a-judge) to
evaluate model outputs at scale, but their assessments often diverge
systematically from human judgments. We present Bridge, a unified statistical
framework that explicitly bridges human and LLM evaluations under both absolute
scoring and pairwise comparison paradigms. Bridge posits a latent human
preference score for each prompt-response pair and models LLM deviations as
linear transformations of covariates that capture sources of discrepancies.
This offers a simple and principled framework for refining LLM ratings and
characterizing systematic discrepancies between humans and LLMs. We provide an
efficient fitting algorithm with asymptotic guarantees for statistical
inference. Using six LLM judges and two benchmarks (BigGen Bench and Chatbot
Arena), Bridge achieves higher agreement with human ratings (accuracy,
calibration, and KL divergence) and exposes systematic human-LLM gaps.

</details>


### [88] [Maximum Score Routing For Mixture-of-Experts](https://arxiv.org/abs/2508.12801)
*Bowen Dong,Yilong Fan,Yutao Sun,Zhenyu Li,Tengyu Pan,Xun Zhou,Jianyong Wang*

Main category: cs.LG

TL;DR: MaxScore是一种新的MoE路由方法，通过最小成本最大流问题和SoftTopk算子解决传统MoE网络中的令牌丢弃和硬件效率问题，在相同FLOPs下获得更好的性能。


<details>
  <summary>Details</summary>
Motivation: 传统MoE网络存在专家容量约束导致令牌丢弃和硬件效率低下，而无约束方法又会损害负载平衡和计算效率，需要新的路由范式来解决这些根本问题。

Method: 提出Maximum Score Routing (MaxScore)，将路由建模为最小成本最大流问题，并集成SoftTopk算子，避免了迭代重路由和最优传输公式的局限性。

Result: 在相同FLOPs下，相比有约束和无约束的基线方法，MaxScore实现了更低的训练损失和更高的评估分数。

Conclusion: MaxScore通过创新的路由范式有效解决了MoE网络中的容量约束问题，在保持计算效率的同时提升了模型性能。

Abstract: Routing networks in sparsely activated mixture-of-experts (MoE) dynamically
allocate input tokens to top-k experts through differentiable sparse
transformations, enabling scalable model capacity while preserving
computational efficiency. Traditional MoE networks impose an expert capacity
constraint to ensure GPU-friendly computation. However, this leads to token
dropping when capacity is saturated and results in low hardware efficiency due
to padding in underutilized experts. Removing the capacity constraint, in turn,
compromises load balancing and computational efficiency. To address these
issues, we propose Maximum Score Routing ($\mathbf{MaxScore}$), a novel MoE
routing paradigm that models routing as a minimum-cost maximum-flow problem and
integrates a SoftTopk operator. MaxScore resolves the fundamental limitations
of iterative rerouting and optimal transport formulations, achieving lower
training losses and higher evaluation scores at equivalent FLOPs compared to
both constrained and unconstrained baselines. Implementation details and
experimental configurations can be obtained from
$\href{https://github.com/dongbw18/MaxScore.git}{MaxScore}$.

</details>


### [89] [Learning to Steer: Input-dependent Steering for Multimodal LLMs](https://arxiv.org/abs/2508.12815)
*Jayneel Parekh,Pegah Khayatan,Mustafa Shukor,Arnaud Dapogny,Alasdair Newson,Matthieu Cord*

Main category: cs.LG

TL;DR: 这篇论文提出了L2S方法，通过训练辅助模块预测输入特异的控制向量，实现多模态大语言模型的细粒度控制，在减少幻觉和提高安全性方面超越了静态方法。


<details>
  <summary>Details</summary>
Motivation: 现有的控制技术依赖单一静态向量，无法处理不同输入查询的特异性需求，比如医疗建议需要指向外部资源，而非法活动需要拒绝回答。

Method: 提出L2S方法，使用对比语提生成输入特异的线性偏移，并训练一个小型辅助模块来预测这些控制向量，以消除测试时知识缺失的问题。

Result: L2S方法在多模态大语言模型中显著减少了幻觉现象，同时有效提升了模型的安全性能力，表现超过了其他静态基线方法。

Conclusion: 输入特异的细粒度控制方法能够更有效地导向多模态大语言模型的行为，L2S通过学习预测控制向量的方式实现了这一目标，为控制技术在多模态领域的应用提供了新的视角。

Abstract: Steering has emerged as a practical approach to enable post-hoc guidance of
LLMs towards enforcing a specific behavior. However, it remains largely
underexplored for multimodal LLMs (MLLMs); furthermore, existing steering
techniques, such as mean steering, rely on a single steering vector, applied
independently of the input query. This paradigm faces limitations when the
desired behavior is dependent on the example at hand. For example, a safe
answer may consist in abstaining from answering when asked for an illegal
activity, or may point to external resources or consultation with an expert
when asked about medical advice. In this paper, we investigate a fine-grained
steering that uses an input-specific linear shift. This shift is computed using
contrastive input-specific prompting. However, the input-specific prompts
required for this approach are not known at test time. Therefore, we propose to
train a small auxiliary module to predict the input-specific steering vector.
Our approach, dubbed as L2S (Learn-to-Steer), demonstrates that it reduces
hallucinations and enforces safety in MLLMs, outperforming other static
baselines.

</details>


### [90] [TCUQ: Single-Pass Uncertainty Quantification from Temporal Consistency with Streaming Conformal Calibration for TinyML](https://arxiv.org/abs/2508.12905)
*Ismail Lamaakal,Chaymae Yahyati,Khalid El Makkaoui,Ibrahim Ouahbi,Yassine Maleh*

Main category: cs.LG

TL;DR: TCUQ是一种用于TinyML流式处理的单次通过、无标签不确定性监测器，通过轻量级信号将短期时间一致性转换为校准风险分数，具有O(W)环形缓冲区和O(1)每步更新。


<details>
  <summary>Details</summary>
Motivation: 为资源受限的微控制器设备提供高效的不确定性监测解决方案，避免传统方法如早期退出和深度集成的高内存占用和延迟问题。

Method: 使用流式保形校准层将时间一致性分数转换为预算化的接受/弃权规则，无需在线标签或额外前向传播。

Result: 在微控制器上，TCUQ比早期退出和深度集成减少约50-60%的占用空间和30-45%的延迟，在损坏分布流中提高3-7个AUPRC点的精度下降检测，达到0.86 AUPRC，故障检测达到0.92 AUROC。

Conclusion: 时间一致性结合流式保形校准为TinyML设备监测提供了实用且资源高效的基础。

Abstract: We introduce TCUQ, a single pass, label free uncertainty monitor for
streaming TinyML that converts short horizon temporal consistency captured via
lightweight signals on posteriors and features into a calibrated risk score
with an O(W ) ring buffer and O(1) per step updates. A streaming conformal
layer turns this score into a budgeted accept/abstain rule, yielding calibrated
behavior without online labels or extra forward passes. On microcontrollers,
TCUQ fits comfortably on kilobyte scale devices and reduces footprint and
latency versus early exit and deep ensembles (typically about 50 to 60% smaller
and about 30 to 45% faster), while methods of similar accuracy often run out of
memory. Under corrupted in distribution streams, TCUQ improves accuracy drop
detection by 3 to 7 AUPRC points and reaches up to 0.86 AUPRC at high
severities; for failure detection it attains up to 0.92 AUROC. These results
show that temporal consistency, coupled with streaming conformal calibration,
provides a practical and resource efficient foundation for on device monitoring
in TinyML.

</details>


### [91] [SNAP-UQ: Self-supervised Next-Activation Prediction for Single-Pass Uncertainty in TinyML](https://arxiv.org/abs/2508.12907)
*Ismail Lamaakal,Chaymae Yahyati,Khalid El Makkaoui,Ibrahim Ouahbi,Yassine Maleh*

Main category: cs.LG

TL;DR: SNAP-UQ是一种面向TinyML的单次前向传播、无需标签的不确定性量化方法，通过深度激活预测来估计风险，具有资源高效的特点。


<details>
  <summary>Details</summary>
Motivation: 在TinyML设备上部署不确定性量化方法时，传统方法如早退机制和深度集成需要额外的计算开销、内存缓冲或多轮前向传播，这在资源受限的微控制器单元(MCU)上不可行。需要一种轻量级、单次前向传播的不确定性估计方法。

Method: 使用int8精度的微小预测头来从压缩的前一层视图预测下一层的统计信息，然后通过轻量级单调映射器将预测的惊讶度转换为可操作的分数。无需时间缓冲、辅助退出或重复前向传播。

Result: 在视觉和音频骨干网络上，SNAP-UQ相比早退机制和深度集成减少了约40-60%的存储空间和25-35%的延迟，具有相似精度的竞争方法往往超出内存限制。在损坏数据流中，AUPRC提高了几个点，单次前向传播下AUROC≈0.9的强故障检测能力。

Conclusion: 基于层间动态的不确定性估计为TinyML设备上的实时监控提供了一个实用且资源高效的基础，能够在单次前向传播中实现有效的风险估计。

Abstract: We introduce \textbf{SNAP-UQ}, a single-pass, label-free uncertainty method
for TinyML that estimates risk from \emph{depth-wise next-activation
prediction}: tiny int8 heads forecast the statistics of the next layer from a
compressed view of the previous one, and a lightweight monotone mapper turns
the resulting surprisal into an actionable score. The design requires no
temporal buffers, auxiliary exits, or repeated forward passes, and adds only a
few tens of kilobytes to MCU deployments. Across vision and audio backbones,
SNAP-UQ consistently reduces flash and latency relative to early-exit and deep
ensembles (typically $\sim$40--60\% smaller and $\sim$25--35\% faster), with
competing methods of similar accuracy often exceeding memory limits. In
corrupted streams it improves accuracy-drop detection by several AUPRC points
and maintains strong failure detection (AUROC $\approx$0.9) in a single pass.
Grounding uncertainty in layer-to-layer dynamics yields a practical,
resource-efficient basis for on-device monitoring in TinyML.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [92] [LARC: Towards Human-level Constrained Retrosynthesis Planning through an Agentic Framework](https://arxiv.org/abs/2508.11860)
*Frazier N. Baker,Daniel Adu-Ampratwum,Reza Averly,Botao Yu,Huan Sun,Xia Ning*

Main category: cs.AI

TL;DR: LARC是首个基于LLM的约束逆合成规划代理框架，通过工具化推理和代理评估机制，在化学合成路线规划中实现了72.9%的成功率，接近专家水平但耗时更少。


<details>
  <summary>Details</summary>
Motivation: 化学中的约束逆合成规划是一个关键但具有挑战性的过程，需要从商业可用起始材料到目标分子的合成路线识别，同时满足实际约束条件。现有方法在处理复杂约束方面存在局限。

Method: LARC框架采用基于LLM的代理评估机制（Agent-as-a-Judge），将工具化推理的代理反馈直接整合到逆合成规划过程中，通过代理化约束评估来指导和约束路线生成。

Result: 在精心策划的48个约束逆合成规划任务（涵盖3种约束类型）上，LARC实现了72.9%的成功率，显著优于LLM基线方法，并且在远少于人类专家所需时间的情况下接近专家级成功率。

Conclusion: LARC框架具有可扩展性，是朝着为人类专家开发有效代理工具或协作科学家的第一步，为约束逆合成规划提供了新的解决方案。

Abstract: Large language model (LLM) agent evaluators leverage specialized tools to
ground the rational decision-making of LLMs, making them well-suited to aid in
scientific discoveries, such as constrained retrosynthesis planning.
Constrained retrosynthesis planning is an essential, yet challenging, process
within chemistry for identifying synthetic routes from commercially available
starting materials to desired target molecules, subject to practical
constraints. Here, we present LARC, the first LLM-based Agentic framework for
Retrosynthesis planning under Constraints. LARC incorporates agentic constraint
evaluation, through an Agent-as-a-Judge, directly into the retrosynthesis
planning process, using agentic feedback grounded in tool-based reasoning to
guide and constrain route generation. We rigorously evaluate LARC on a
carefully curated set of 48 constrained retrosynthesis planning tasks across 3
constraint types. LARC achieves a 72.9% success rate on these tasks, vastly
outperforming LLM baselines and approaching human expert-level success in
substantially less time. The LARC framework is extensible, and serves as a
first step towards an effective agentic tool or a co-scientist to human experts
for constrained retrosynthesis.

</details>


### [93] [CHBench: A Cognitive Hierarchy Benchmark for Evaluating Strategic Reasoning Capability of LLMs](https://arxiv.org/abs/2508.11944)
*Hongtao Liu,Zhicheng Du,Zihe Wang,Weiran Shen*

Main category: cs.AI

TL;DR: 提出了CHBench评估框架，基于认知层次模型来评估大语言模型的战略推理能力，发现聊天机制会降低战略推理而记忆机制会增强它。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要依赖效用性能指标来评估LLMs的游戏能力，但这些指标不够稳健，受对手行为和游戏结构变化影响较大。

Method: 采用认知层次模型的三阶段系统框架，在15个精选的正规形式游戏中收集6个最先进LLMs的行为数据进行分析。

Result: 实验显示LLMs在不同对手间表现出一致的战略推理水平，聊天机制显著降低战略推理能力，而记忆机制则增强该能力。

Conclusion: CHBench是一个有前景的评估工具，对LLMs能力评估具有重要研究价值和应用潜力。

Abstract: Game-playing ability serves as an indicator for evaluating the strategic
reasoning capability of large language models (LLMs). While most existing
studies rely on utility performance metrics, which are not robust enough due to
variations in opponent behavior and game structure. To address this limitation,
we propose \textbf{Cognitive Hierarchy Benchmark (CHBench)}, a novel evaluation
framework inspired by the cognitive hierarchy models from behavioral economics.
We hypothesize that agents have bounded rationality -- different agents behave
at varying reasoning depths/levels. We evaluate LLMs' strategic reasoning
through a three-phase systematic framework, utilizing behavioral data from six
state-of-the-art LLMs across fifteen carefully selected normal-form games.
Experiments show that LLMs exhibit consistent strategic reasoning levels across
diverse opponents, confirming the framework's robustness and generalization
capability. We also analyze the effects of two key mechanisms (Chat Mechanism
and Memory Mechanism) on strategic reasoning performance. Results indicate that
the Chat Mechanism significantly degrades strategic reasoning, whereas the
Memory Mechanism enhances it. These insights position CHBench as a promising
tool for evaluating LLM capabilities, with significant potential for future
research and practical applications.

</details>


### [94] [Non-Iterative Symbolic-Aided Chain-of-Thought for Logical Reasoning](https://arxiv.org/abs/2508.12425)
*Phuong Minh Nguyen,Tien Huu Dang,Naoya Inoue*

Main category: cs.AI

TL;DR: Symbolic-Aided CoT通过整合轻量级符号表示到少样本提示中，改进标准CoT方法，提升LLM在逻辑推理中的透明度、可解释性和分析能力，在多个基准测试中显著优于传统CoT。


<details>
  <summary>Details</summary>
Motivation: 标准CoT方法在逻辑推理中缺乏透明度和可解释性，需要一种能保持通用性同时增强推理过程清晰度的方法。

Method: 在少样本提示中集成轻量级符号表示，使用一致的策略构建推理步骤，使推理模式在非迭代推理过程中更加明确。

Result: 在ProofWriter、FOLIO、ProntoQA和LogicalDeduction四个基准测试中表现优异，特别是在需要处理多重约束的复杂推理任务中，在三个数据集上显著超越传统CoT。

Conclusion: Symbolic-Aided CoT有效提升了LLM的逻辑推理能力，增强了推理过程的透明度和可分析性，在不同规模的模型上都表现出稳定的改进效果。

Abstract: This work introduces Symbolic-Aided Chain-of-Thought (CoT), an improved
approach to standard CoT, for logical reasoning in large language models
(LLMs). The key idea is to integrate lightweight symbolic representations into
few-shot prompts, structuring the inference steps with a consistent strategy to
make reasoning patterns more explicit within a non-iterative reasoning process.
By incorporating these symbolic structures, our method preserves the
generalizability of standard prompting techniques while enhancing the
transparency, interpretability, and analyzability of LLM logical reasoning.
Extensive experiments on four well-known logical reasoning benchmarks --
ProofWriter, FOLIO, ProntoQA, and LogicalDeduction, which cover diverse
reasoning scenarios -- demonstrate the effectiveness of the proposed approach,
particularly in complex reasoning tasks that require navigating multiple
constraints or rules. Notably, Symbolic-Aided CoT consistently improves LLMs'
reasoning capabilities across various model sizes and significantly outperforms
conventional CoT on three out of four datasets, ProofWriter, ProntoQA, and
LogicalDeduction.

</details>


### [95] [An LLM + ASP Workflow for Joint Entity-Relation Extraction](https://arxiv.org/abs/2508.12611)
*Trang Tran,Trung Hoang Le,Huiping Cao,Tran Cao Son*

Main category: cs.AI

TL;DR: 使用大语言模型和答案集编程结合的新方法，在少量训练数据下实现了更好的聚合实体-关系提取效果


<details>
  <summary>Details</summary>
Motivation: 传统的聚合实体-关系提取方法需要大量标注数据，劳动密集且无法轻松融入领域知识

Method: 结合生成式预训练大语言模型(LLMs)的自然语言理解能力和答案集编程(ASP)的知识表示与推理能力，提出通用工作流

Result: 在三个标准数据集上进行实验，仅需10%训练数据情况下就超过现有最优方法，在SciERC数据集上关系提取任务的性能提升15%到35%

Conclusion: LLM + ASP结合的方法为聚合实体-关系提取提供了一种通用、效果好且需要少量训练数据的解决方案

Abstract: Joint entity-relation extraction (JERE) identifies both entities and their
relationships simultaneously. Traditional machine-learning based approaches to
performing this task require a large corpus of annotated data and lack the
ability to easily incorporate domain specific information in the construction
of the model. Therefore, creating a model for JERE is often labor intensive,
time consuming, and elaboration intolerant. In this paper, we propose
harnessing the capabilities of generative pretrained large language models
(LLMs) and the knowledge representation and reasoning capabilities of Answer
Set Programming (ASP) to perform JERE. We present a generic workflow for JERE
using LLMs and ASP. The workflow is generic in the sense that it can be applied
for JERE in any domain. It takes advantage of LLM's capability in natural
language understanding in that it works directly with unannotated text. It
exploits the elaboration tolerant feature of ASP in that no modification of its
core program is required when additional domain specific knowledge, in the form
of type specifications, is found and needs to be used. We demonstrate the
usefulness of the proposed workflow through experiments with limited training
data on three well-known benchmarks for JERE. The results of our experiments
show that the LLM + ASP workflow is better than state-of-the-art JERE systems
in several categories with only 10\% of training data. It is able to achieve a
2.5 times (35\% over 15\%) improvement in the Relation Extraction task for the
SciERC corpus, one of the most difficult benchmarks.

</details>


### [96] [Reinforcement Learning with Rubric Anchors](https://arxiv.org/abs/2508.12790)
*Zenan Huang,Yihong Zhuang,Guoshan Lu,Zeyu Qin,Haokai Xu,Tianyu Zhao,Ru Peng,Jiaqi Hu,Zhanming Shen,Xiaomeng Hu,Xijun Gu,Peiyi Tu,Jiaxin Liu,Wenyu Chen,Yuzhuo Fu,Zhiting Fan,Yanmei Gu,Yuanyuan Wang,Zhengkai Yang,Jianguo Li,Junbo Zhao*

Main category: cs.AI

TL;DR: 本文提出了一种基于量规奖励的RLVR方法，将可验证奖励学习扩展到开放式任务，通过结构化评分标准实现主观输出的自动评分，在仅使用5K+样本的情况下在开放式基准上提升5.2%


<details>
  <summary>Details</summary>
Motivation: 传统RLVR方法主要局限于可自动检查结果的领域，无法有效处理开放式主观任务，需要扩展RLVR范式到开放式任务领域

Method: 构建了包含10,000+量规的奖励系统（人工、LLM生成或人机协作），通过精心设计的量规作为结构化、模型可解释的评分标准，实现主观输出的自动评分

Result: 1) 在开放式基准（特别是人文学科）上提升5.2%，超越671B DeepSeek-V3模型2.4%；2) 提供细粒度风格控制，减轻"AI腔调"，产生更人性化、富有表现力的回答

Conclusion: 基于量规的RL方法为开放式任务提供了有效的训练框架，在保持通用和推理能力的同时显著提升主观任务表现，并提供了风格控制的新途径

Abstract: Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a
powerful paradigm for enhancing Large Language Models (LLMs), exemplified by
the success of OpenAI's o-series. In RLVR, rewards are derived from verifiable
signals-such as passing unit tests in code generation or matching correct
answers in mathematical reasoning. While effective, this requirement largely
confines RLVR to domains with automatically checkable outcomes. To overcome
this, we extend the RLVR paradigm to open-ended tasks by integrating
rubric-based rewards, where carefully designed rubrics serve as structured,
model-interpretable criteria for automatic scoring of subjective outputs. We
construct, to our knowledge, the largest rubric reward system to date, with
over 10,000 rubrics from humans, LLMs, or a hybrid human-LLM collaboration.
Implementing rubric-based RL is challenging; we tackle these issues with a
clear framework and present an open-sourced Qwen-30B-A3B model with notable
gains: 1) With only 5K+ samples, our system improves by +5.2% on open-ended
benchmarks (especially humanities), outperforming a 671B DeepSeek-V3 model by
+2.4%, while preserving general and reasoning abilities. 2) Our method provides
fine-grained stylistic control, using rubrics as anchors to mitigate the
"AI-like" tone and produce more human-like, expressive responses. We share key
lessons in rubric construction, data selection, and training, and discuss
limitations and future releases.

</details>


### [97] [E3RG: Building Explicit Emotion-driven Empathetic Response Generation System with Multimodal Large Language Model](https://arxiv.org/abs/2508.12854)
*Ronghao Lin,Shuai Shen,Weipeng Hu,Qiaolin He,Aolin Xiong,Li Huang,Haifeng Hu,Yap-peng Tan*

Main category: cs.AI

TL;DR: E3RG是一个基于多模态大语言模型的显式情感驱动共情响应生成系统，通过将多模态共情响应生成任务分解为三个部分来实现自然、情感丰富且身份一致的响应，无需额外训练。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型改进了基于文本的共情响应生成，但在处理多模态情感内容和保持身份一致性方面仍存在挑战，因此需要开发能够处理多模态情感内容并保持身份一致性的系统。

Method: 将多模态共情响应生成任务分解为三个部分：多模态共情理解、共情记忆检索和多模态响应生成，并整合先进的表达性语音和视频生成模型。

Result: 实验验证了系统在零样本和少样本设置下的优越性，在ACM MM 25的基于Avatar的多模态共情挑战中获得Top-1位置。

Conclusion: E3RG系统能够生成自然、情感丰富且身份一致的多模态共情响应，无需额外训练，在多模态共情响应生成任务中表现出色。

Abstract: Multimodal Empathetic Response Generation (MERG) is crucial for building
emotionally intelligent human-computer interactions. Although large language
models (LLMs) have improved text-based ERG, challenges remain in handling
multimodal emotional content and maintaining identity consistency. Thus, we
propose E3RG, an Explicit Emotion-driven Empathetic Response Generation System
based on multimodal LLMs which decomposes MERG task into three parts:
multimodal empathy understanding, empathy memory retrieval, and multimodal
response generation. By integrating advanced expressive speech and video
generative models, E3RG delivers natural, emotionally rich, and
identity-consistent responses without extra training. Experiments validate the
superiority of our system on both zero-shot and few-shot settings, securing
Top-1 position in the Avatar-based Multimodal Empathy Challenge on ACM MM 25.
Our code is available at https://github.com/RH-Lin/E3RG.

</details>


### [98] [PC-Sampler: Position-Aware Calibration of Decoding Bias in Masked Diffusion Models](https://arxiv.org/abs/2508.13021)
*Pengcheng Huang,Shuhao Liu,Zhenghao Liu,Yukun Yan,Shuo Wang,Zulong Chen,Tong Xiao*

Main category: cs.AI

TL;DR: PC-Sampler是一种新的解码策略，通过位置感知权重和置信度校准来解决掩码扩散模型解码中的全局轨迹控制和早期平凡token偏向问题，在多个基准测试中平均提升10%以上性能。


<details>
  <summary>Details</summary>
Motivation: 现有掩码扩散模型(MDMs)的解码策略存在两个关键限制：缺乏全局轨迹控制和解码早期对平凡token的明显偏向，这限制了MDMs的潜力。

Method: 提出位置感知置信度校准采样(PC-Sampler)，结合位置感知权重机制来调节解码路径，并使用校准置信度分数来抑制早期选择平凡token。

Result: 在7个具有挑战性的基准测试（包括逻辑推理和规划任务）上，PC-Sampler平均比现有MDM解码策略提升超过10%，显著缩小了与最先进自回归模型的性能差距。

Conclusion: PC-Sampler通过统一的全局轨迹规划和内容感知信息最大化，有效解决了MDMs解码策略的关键限制，显著提升了生成质量。

Abstract: Recent advances in masked diffusion models (MDMs) have established them as
powerful non-autoregressive alternatives for sequence generation. Nevertheless,
our preliminary experiments reveal that the generation quality of MDMs is still
highly sensitive to the choice of decoding strategy. In particular, widely
adopted uncertainty-based samplers suffer from two key limitations: a lack of
global trajectory control and a pronounced bias toward trivial tokens in the
early stages of decoding. These shortcomings restrict the full potential of
MDMs. In this work, we introduce Position-Aware Confidence-Calibrated Sampling
(PC-Sampler), a novel decoding strategy that unifies global trajectory planning
with content-aware informativeness maximization. PC-Sampler incorporates a
position-aware weighting mechanism to regulate the decoding path and a
calibrated confidence score to suppress the premature selection of trivial
tokens. Extensive experiments on three advanced MDMs across seven challenging
benchmarks-including logical reasoning and planning tasks-demonstrate that
PC-Sampler consistently outperforms existing MDM decoding strategies by more
than 10% on average, significantly narrowing the performance gap with
state-of-the-art autoregressive models. All codes are available at
https://github.com/NEUIR/PC-Sampler.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [99] [Code Vulnerability Detection Across Different Programming Languages with AI Models](https://arxiv.org/abs/2508.11710)
*Hael Abdulhakim Ali Humran,Ferdi Sonmez*

Main category: cs.CR

TL;DR: 这篇论文研究了使用转换器模型（CodeBERT和CodeLlama）进行代码漏洞检测的方法，通过细调模型在存在漏洞和安全代码片段上实现了超过97%的准确率，显示了AI在提升漏洞检测可靠性和可扩展性方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统的基于规则的静态分析工具在检测上下文依赖性错误时效果不佳且导致高假正率，需要更有效的AI方法来解决这些问题。

Method: 方法包括数据集收集、语言标准化、模型细调，以及集成学习和可解释性AI技术的结合。使用CodeBERT和CodeLlama模型在多种代码漏洞数据集上进行实验。

Result: 实验结果显示细调后的CodeBERT模型准确率超过97%，甚至超过某些现有静态分析器。模型能够完美识别漏洞（接近完美的召回率），但精确度可能会下降。混合模型和验证流程能有效减少假正率。

Conclusion: AI基于漏洞检测方案能够良好地渐进到不同编程语言和漏洞类型，显示了在提升漏洞检测的可靠性和可扩展性方面的潜力。但在稳健性、可解释性和部署准备度方面仍需进一步研究。

Abstract: Security vulnerabilities present in a code that has been written in diverse
programming languages are among the most critical yet complicated aspects of
source code to detect. Static analysis tools based on rule-based patterns
usually do not work well at detecting the context-dependent bugs and lead to
high false positive rates. Recent developments in artificial intelligence,
specifically the use of transformer-based models like CodeBERT and CodeLlama,
provide light to this problem, as they show potential in finding such flaws
better. This paper presents the implementations of these models on various
datasets of code vulnerability, showing how off-the-shelf models can
successfully produce predictive capacity in models through dynamic fine-tuning
of the models on vulnerable and safe code fragments. The methodology comprises
the gathering of the dataset, normalization of the language, fine-tuning of the
model, and incorporation of ensemble learning and explainable AI. Experiments
show that a well-trained CodeBERT can be as good as or even better than some
existing static analyzers in terms of accuracy greater than 97%. Further study
has indicated that although language models can achieve close-to-perfect
recall, the precision can decrease. A solution to this is given by hybrid
models and validation procedures, which will reduce false positives. According
to the results, the AI-based solutions generalize to different programming
languages and classes of vulnerability. Nevertheless, robustness,
interpretability, and deployment readiness are still being developed. The
results illustrate the probabilities that AI will enhance the trustworthiness
in the usability and scalability of machine-learning-based detectors of
vulnerabilities.

</details>


### [100] [Optimizing Token Choice for Code Watermarking: A RL Approach](https://arxiv.org/abs/2508.11925)
*Zhimeng Guo,Huaisheng Zhu,Siyuan Xu,Hangfan Zhang,Teng Xiao,Minhao Cheng*

Main category: cs.CR

TL;DR: CodeTracer是一个基于强化学习的自适应代码水印框架，通过策略驱动的参数化模型在代码生成中嵌入可检测的水印，同时保持代码功能完整性。


<details>
  <summary>Details</summary>
Motivation: 检测LLM生成的代码需要能够在高度结构化、语法约束的环境中工作的水印系统。

Method: 采用强化学习训练范式，使用策略驱动的方法通过参数化模型在下一个token预测时智能偏置token选择，结合Gumbel Top-k重参数化实现离散水印决策的梯度优化。

Result: 广泛的比较评估显示CodeTracer在水印可检测性和生成代码功能保持方面显著优于最先进的基线方法。

Conclusion: CodeTracer提供了一个有效的解决方案，能够在保持代码功能的同时嵌入统计可检测的水印，为LLM生成代码的检测提供了可靠的技术手段。

Abstract: The need for detecting LLM-generated code necessitates watermarking systems
capable of operating within its highly structured and syntactically constrained
environment. To address this, we introduce CodeTracer, an innovative adaptive
code watermarking framework underpinned by a novel reinforcement learning
training paradigm. At its core, CodeTracer features a policy-driven approach
that utilizes a parameterized model to intelligently bias token choices during
next-token prediction. This strategy ensures that embedded watermarks maintain
code functionality while exhibiting subtle yet statistically detectable
deviations from typical token distributions. To facilitate policy learning, we
devise a comprehensive reward system that seamlessly integrates execution
feedback with watermark embedding signals, balancing process-level and
outcome-level rewards. Additionally, we employ Gumbel Top-k reparameterization
to enable gradient-based optimization of discrete watermarking decisions.
Extensive comparative evaluations demonstrate CodeTracer's significant
superiority over state-of-the-art baselines in both watermark detectability and
the preservation of generated code's functionality.

</details>


### [101] [Mitigating Jailbreaks with Intent-Aware LLMs](https://arxiv.org/abs/2508.12072)
*Wei Jie Yeo,Ranjan Satapathy,Erik Cambria*

Main category: cs.CR

TL;DR: Intent-FT是一种简单轻量的微调方法，通过训练LLMs在响应前推断指令的潜在意图，显著提高模型对越狱攻击的鲁棒性，同时保持通用能力。


<details>
  <summary>Details</summary>
Motivation: 尽管进行了广泛的安全调优，大语言模型仍然容易受到通过对抗性指令进行的越狱攻击，这反映了安全性和任务性能之间的持续权衡。

Method: 提出Intent-FT方法，在针对性的对抗指令集上进行微调，使LLMs能够将意图推断泛化到未见过的攻击，包括参数化和非参数化攻击的全面评估。

Result: Intent-FT持续缓解所有评估的攻击类别，没有单一攻击成功率超过50%，而现有防御仅部分有效。方法保持模型通用能力，减少对包含表面有害关键词的良性指令的过度拒绝。

Conclusion: 该方法使模型能够准确识别对抗攻击中的隐藏有害意图，并且这些学习到的意图可以有效地转移到增强普通模型防御中。

Abstract: Despite extensive safety-tuning, large language models (LLMs) remain
vulnerable to jailbreak attacks via adversarially crafted instructions,
reflecting a persistent trade-off between safety and task performance. In this
work, we propose Intent-FT, a simple and lightweight fine-tuning approach that
explicitly trains LLMs to infer the underlying intent of an instruction before
responding. By fine-tuning on a targeted set of adversarial instructions,
Intent-FT enables LLMs to generalize intent deduction to unseen attacks,
thereby substantially improving their robustness. We comprehensively evaluate
both parametric and non-parametric attacks across open-source and proprietary
models, considering harmfulness from attacks, utility, over-refusal, and impact
against white-box threats. Empirically, Intent-FT consistently mitigates all
evaluated attack categories, with no single attack exceeding a 50\% success
rate -- whereas existing defenses remain only partially effective. Importantly,
our method preserves the model's general capabilities and reduces excessive
refusals on benign instructions containing superficially harmful keywords.
Furthermore, models trained with Intent-FT accurately identify hidden harmful
intent in adversarial attacks, and these learned intentions can be effectively
transferred to enhance vanilla model defenses.

</details>


### [102] [Where to Start Alignment? Diffusion Large Language Model May Demand a Distinct Position](https://arxiv.org/abs/2508.12398)
*Zhixin Xie,Xurui Song,Jun Luo*

Main category: cs.CR

TL;DR: 本文首次分析了扩散大语言模型(dLLMs)的安全性，发现响应中间token对安全性更关键，提出了一种针对中间token的安全对齐方法MOSA，在安全性和实用性方面都表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型(dLLMs)作为一种新兴的非自回归范式，目前缺乏对其安全性的研究，需要探索适合其独特生成特性的安全对齐方法。

Method: 提出Middle-tOken Safety Alignment (MOSA)方法，通过强化学习直接对齐模型的中间生成过程与安全拒绝响应，利用dLLMs中间token对安全性更关键的特性。

Result: 在8种攻击方法和2个基准测试中，MOSA表现出优越的安全性能，同时在编程、数学和通用推理任务上保持了良好的实用性。

Conclusion: 研究揭示了dLLMs安全性的关键不对称性，提出的MOSA方法有效提升了模型安全性，为扩散大语言模型的安全对齐提供了新思路。

Abstract: Diffusion Large Language Models (dLLMs) have recently emerged as a
competitive non-autoregressive paradigm due to their unique training and
inference approach. However, there is currently a lack of safety study on this
novel architecture. In this paper, we present the first analysis of dLLMs'
safety performance and propose a novel safety alignment method tailored to
their unique generation characteristics. Specifically, we identify a critical
asymmetry between the defender and attacker in terms of security. For the
defender, we reveal that the middle tokens of the response, rather than the
initial ones, are more critical to the overall safety of dLLM outputs; this
seems to suggest that aligning middle tokens can be more beneficial to the
defender. The attacker, on the contrary, may have limited power to manipulate
middle tokens, as we find dLLMs have a strong tendency towards a sequential
generation order in practice, forcing the attack to meet this distribution and
diverting it from influencing the critical middle tokens. Building on this
asymmetry, we introduce Middle-tOken Safety Alignment (MOSA), a novel method
that directly aligns the model's middle generation with safe refusals
exploiting reinforcement learning. We implement MOSA and compare its security
performance against eight attack methods on two benchmarks. We also test the
utility of MOSA-aligned dLLM on coding, math, and general reasoning. The
results strongly prove the superiority of MOSA.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [103] [Using Natural Language for Human-Robot Collaboration in the Real World](https://arxiv.org/abs/2508.11759)
*Peter Lindes,Kaoutar Skiker*

Main category: cs.RO

TL;DR: 本文探讨了如何通过大语言模型提升自主机器人的语言理解能力，以实现人机自然语言协作的远景。


<details>
  <summary>Details</summary>
Motivation: 实现自主机器人与人类在物理世界中进行复杂任务协作，需要机器人具备自然语言沟通能力。传统交互式任务学习系统的语言理解能力有限，而大语言模型为提升机器人语言理解提供了新机遇。

Method: 提出了一种以认知代理为核心的AI系统架构，该代理控制物理机器人、与人类和LLM交互、通过经验积累情境知识。使用ChatGPT进行了三个理解自然语言挑战的概念验证实验。

Result: 通过概念验证实验证明了LLM在提升机器人语言理解方面的潜力，为建立集成LLM语言理解能力的机器人协助系统奠定了基础。

Conclusion: 将LLM集成到机器人系统中可以大大提升人机协作能力，但需要进一步研究如何将这些概念验证转化为可操作的集成系统，实现自然语言驱动的机器人协助器。

Abstract: We have a vision of a day when autonomous robots can collaborate with humans
as assistants in performing complex tasks in the physical world. This vision
includes that the robots will have the ability to communicate with their human
collaborators using language that is natural to the humans. Traditional
Interactive Task Learning (ITL) systems have some of this ability, but the
language they can understand is very limited. The advent of large language
models (LLMs) provides an opportunity to greatly improve the language
understanding of robots, yet integrating the language abilities of LLMs with
robots that operate in the real physical world is a challenging problem.
  In this chapter we first review briefly a few commercial robot products that
work closely with humans, and discuss how they could be much better
collaborators with robust language abilities. We then explore how an AI system
with a cognitive agent that controls a physical robot at its core, interacts
with both a human and an LLM, and accumulates situational knowledge through its
experiences, can be a possible approach to reach that vision. We focus on three
specific challenges of having the robot understand natural language, and
present a simple proof-of-concept experiment using ChatGPT for each. Finally,
we discuss what it will take to turn these simple experiments into an
operational system where LLM-assisted language understanding is a part of an
integrated robotic assistant that uses language to collaborate with humans.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [104] [Ovis2.5 Technical Report](https://arxiv.org/abs/2508.11737)
*Shiyin Lu,Yang Li,Yu Xia,Yuwei Hu,Shanshan Zhao,Yanqing Ma,Zhichao Wei,Yinglun Li,Lunhao Duan,Jianshan Zhao,Yuxuan Han,Haijun Li,Wanying Chen,Junke Tang,Chengkun Hou,Zhixing Du,Tianli Zhou,Wenjie Zhang,Huping Ding,Jiahe Li,Wen Li,Gui Hu,Yiliang Gu,Siran Yang,Jiamang Wang,Hailong Sun,Yibo Wang,Hui Sun,Jinlong Huang,Yuping He,Shengze Shi,Weihong Zhang,Guodong Zheng,Junpeng Jiang,Sensen Gao,Yi-Feng Wu,Sijia Chen,Yuhui Chen,Qing-Guo Chen,Zhao Xu,Weihua Luo,Kaifu Zhang*

Main category: cs.CV

TL;DR: Ovis2.5是一个多模态大语言模型，通过原生分辨率视觉处理和反思推理能力，在OpenCompass排行榜上取得78.3分，在40B参数以下开源模型中达到SOTA水平。


<details>
  <summary>Details</summary>
Motivation: 为了解决固定分辨率图像处理导致的细节丢失问题，并增强模型的多模态推理能力，特别是对于视觉密集内容（如复杂图表）的分析。

Method: 采用原生分辨率视觉Transformer处理可变分辨率图像，引入反思推理机制（包括自检和修订），通过五阶段课程学习（基础预训练、指令微调、DPO/GRPO对齐）进行训练，使用多模态数据打包和混合并行技术提升效率。

Result: Ovis2.5-9B在OpenCompass上平均得分78.3，相比前代Ovis2-8B有显著提升；Ovis2.5-2B得分73.9，在同等规模模型中达到SOTA。在STEM基准测试、接地任务、视频任务和复杂图表分析方面均取得领先结果。

Conclusion: Ovis2.5通过原生分辨率处理和反思推理机制，在多模态理解任务上实现了显著的性能提升，特别是在视觉密集内容分析方面表现出色，为资源受限设备提供了高性能的小型模型选择。

Abstract: We present Ovis2.5, a successor to Ovis2 designed for native-resolution
visual perception and strong multimodal reasoning. Ovis2.5 integrates a
native-resolution vision transformer that processes images at their native,
variable resolutions, avoiding the degradation from fixed-resolution tiling and
preserving both fine detail and global layout -- crucial for visually dense
content like complex charts. To strengthen reasoning, we train the model to
move beyond linear chain-of-thought and perform reflection -- including
self-checking and revision. This advanced capability is exposed as an optional
"thinking mode" at inference time, allowing users to trade latency for enhanced
accuracy on difficult inputs. The model is trained via a comprehensive
five-phase curriculum that progressively builds its skills. The process begins
with foundational visual and multimodal pretraining, advances through
large-scale instruction tuning, and culminates in alignment and reasoning
enhancement using DPO and GRPO. To scale these upgrades efficiently, we employ
multimodal data packing and hybrid parallelism, yielding a significant
end-to-end speedup. We release two open-source models: Ovis2.5-9B and
Ovis2.5-2B. The latter continues the "small model, big performance" philosophy
of Ovis2, making it ideal for resource-constrained, on-device scenarios. On the
OpenCompass multimodal leaderboard, Ovis2.5-9B averages 78.3, marking a
substantial improvement over its predecessor, Ovis2-8B, and achieving
state-of-the-art results among open-source MLLMs in the sub-40B parameter
range; Ovis2.5-2B scores 73.9, establishing SOTA for its size. Beyond aggregate
scores, Ovis2.5 achieves leading results on STEM benchmarks, exhibits strong
capabilities on grounding and video tasks, and achieves open-source SOTA at its
scale for complex chart analysis.

</details>


### [105] [VideoAVE: A Multi-Attribute Video-to-Text Attribute Value Extraction Dataset and Benchmark Models](https://arxiv.org/abs/2508.11801)
*Ming Cheng,Tong Wu,Jiazhen Hu,Jiaying Gong,Hoda Eldardiry*

Main category: cs.CV

TL;DR: VideoAVE是首个公开的视频到文本电商属性值提取数据集，覆盖14个领域和172个属性，包含224k训练数据和25k评估数据，通过CLIP-MoE过滤系统确保数据质量，并建立了全面的基准测试。


<details>
  <summary>Details</summary>
Motivation: 解决现有AVE数据集仅限于文本到文本或图像到文本设置，缺乏对产品视频支持、多样化属性覆盖和公开可用性的问题。

Method: 提出基于CLIP的专家混合过滤系统(CLIP-MoE)来移除不匹配的视频-产品对，确保数据质量；建立包含属性条件值预测和开放属性值对提取任务的综合基准。

Result: 视频到文本AVE仍然是一个具有挑战性的问题，特别是在开放设置中，现有视频视觉语言模型在利用有效时序信息方面仍有改进空间。

Conclusion: VideoAVE数据集填补了视频到文本AVE领域的空白，为开发更先进的视觉语言模型提供了重要资源，展示了该领域仍需进一步研究和发展。

Abstract: Attribute Value Extraction (AVE) is important for structuring product
information in e-commerce. However, existing AVE datasets are primarily limited
to text-to-text or image-to-text settings, lacking support for product videos,
diverse attribute coverage, and public availability. To address these gaps, we
introduce VideoAVE, the first publicly available video-to-text e-commerce AVE
dataset across 14 different domains and covering 172 unique attributes. To
ensure data quality, we propose a post-hoc CLIP-based Mixture of Experts
filtering system (CLIP-MoE) to remove the mismatched video-product pairs,
resulting in a refined dataset of 224k training data and 25k evaluation data.
In order to evaluate the usability of the dataset, we further establish a
comprehensive benchmark by evaluating several state-of-the-art video vision
language models (VLMs) under both attribute-conditioned value prediction and
open attribute-value pair extraction tasks. Our results analysis reveals that
video-to-text AVE remains a challenging problem, particularly in open settings,
and there is still room for developing more advanced VLMs capable of leveraging
effective temporal information. The dataset and benchmark code for VideoAVE are
available at: https://github.com/gjiaying/VideoAVE

</details>


### [106] [Labels or Input? Rethinking Augmentation in Multimodal Hate Detection](https://arxiv.org/abs/2508.11808)
*Sahajpreet Singh,Rongxin Ouyang,Subhayan Mukerjee,Kokil Jaidka*

Main category: cs.CV

TL;DR: 该论文提出了一种双重措施来改善多模态恨恼图片检测：通过提示优化框架和多模态数据增强管道，提高了模型的稳健性和通用性。


<details>
  <summary>Details</summary>
Motivation: 现代网络中多模态恨恼内容流行，常以幽默或讽刺的方式传播。虽然视觉-语言模型(VLMs)有一定潜力，但缺乏细粒度监督支持，容易受隐式恨恼语言影响。

Method: 1) 提出提示优化框架，系统性变化提示结构、监督细度和训练模态。
2) 引入多模态数据增强管道，通过多代理LLM-VLM设置生成2,479个反事实中立图片，通过隔离和重写恨恼模态来减少偏偏相关性。

Result: 结构化提示提高了小模型的稳健性，InternVL2在二值和细粒度设置中获得最佳F1分数。数据增强管道成功减少了偏偏相关性，提高了分类器的通用性。

Conclusion: 提示结构和数据组成与模型大小同样关键，目标导向的数据增强能够支持更可信豖和上下文敏感的恨恼检测。这些方法为构建合成数据来训练稳健和公平的视觉-语言模型开启了新方向。

Abstract: The modern web is saturated with multimodal content, intensifying the
challenge of detecting hateful memes, where harmful intent is often conveyed
through subtle interactions between text and image under the guise of humor or
satire. While recent advances in Vision-Language Models (VLMs) show promise,
these models lack support for fine-grained supervision and remain susceptible
to implicit hate speech. In this paper, we present a dual-pronged approach to
improve multimodal hate detection. First, we propose a prompt optimization
framework that systematically varies prompt structure, supervision granularity,
and training modality. We show that prompt design and label scaling both
influence performance, with structured prompts improving robustness even in
small models, and InternVL2 achieving the best F1-scores across binary and
scaled settings. Second, we introduce a multimodal data augmentation pipeline
that generates 2,479 counterfactually neutral memes by isolating and rewriting
the hateful modality. This pipeline, powered by a multi-agent LLM-VLM setup,
successfully reduces spurious correlations and improves classifier
generalization. Our approaches inspire new directions for building synthetic
data to train robust and fair vision-language models. Our findings demonstrate
that prompt structure and data composition are as critical as model size, and
that targeted augmentation can support more trustworthy and context-sensitive
hate detection.

</details>


### [107] [EVTP-IVS: Effective Visual Token Pruning For Unifying Instruction Visual Segmentation In Multi-Modal Large Language Models](https://arxiv.org/abs/2508.11886)
*Wenhui Zhu,Xiwen Chen,Zhipeng Wang,Shao Tang,Sayan Ghosh,Xuanzhao Dong,Rajat Koner,Yalin Wang*

Main category: cs.CV

TL;DR: 通过空间信息集成的k-center视觉令牌剪枝方法EVTP-IV，在指令视觉分割任务中实现5倍视频速度提升和3.5倍图像速度提升，仅使用20%令牌保持相当的准确性。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在指令视觉分割任务中的推理成本过高，尤其是在视频处理时，需要批量处理大量视觉令牌导致性能瓶颈。

Method: 提出EVTP-IV方法，基于k-center算法集成空间信息，选择空间上具有代表性的简约令牌子集来加速推理，并通过信息论分析验证设计。

Result: 在标准IVS测试集上实现了最高5倍视频速度提升和3.5倍图像速度提升，仅使用20%令牌却保持了相当的分割准确性，且在不同剪枝比下均超过现有最优方法。

Conclusion: EVTP-IV通过简单有效的视觉令牌剪枝方法，显著提升了多模态大模型在指令视觉分割任务中的推理效率，为高效视觉理解提供了可行解决方案。

Abstract: Instructed Visual Segmentation (IVS) tasks require segmenting objects in
images or videos based on natural language instructions. While recent
multimodal large language models (MLLMs) have achieved strong performance on
IVS, their inference cost remains a major bottleneck, particularly in video. We
empirically analyze visual token sampling in MLLMs and observe a strong
correlation between subset token coverage and segmentation performance. This
motivates our design of a simple and effective token pruning method that
selects a compact yet spatially representative subset of tokens to accelerate
inference. In this paper, we introduce a novel visual token pruning method for
IVS, called EVTP-IV, which builds upon the k-center by integrating spatial
information to ensure better coverage. We further provide an
information-theoretic analysis to support our design. Experiments on standard
IVS benchmarks show that our method achieves up to 5X speed-up on video tasks
and 3.5X on image tasks, while maintaining comparable accuracy using only 20%
of the tokens. Our method also consistently outperforms state-of-the-art
pruning baselines under varying pruning ratios.

</details>


### [108] [VimoRAG: Video-based Retrieval-augmented 3D Motion Generation for Motion Language Models](https://arxiv.org/abs/2508.12081)
*Haidong Xu,Guangwei Xu,Zhedong Zheng,Xiatian Zhu,Wei Ji,Xiangtai Li,Ruijie Guo,Meishan Zhang,Min zhang,Hao Fei*

Main category: cs.CV

TL;DR: VimoRAG是一个基于视频检索增强的运动生成框架，通过从大规模视频数据库中检索相关的2D人体运动信号来解决运动大语言模型的数据稀缺问题，显著提升了仅使用文本输入的运动生成性能。


<details>
  <summary>Details</summary>
Motivation: 运动大语言模型由于标注数据有限而面临严重的域外/词汇外问题，需要利用大规模野外视频数据库来增强3D运动生成能力。

Method: 设计了Gemini Motion Video Retriever机制和Motion-centric Dual-alignment DPO Trainer，解决视频检索中的两个关键瓶颈：有效的人体姿态和动作区分，以及次优检索结果的误差传播问题。

Result: 实验结果表明，VimoRAG显著提升了仅使用文本输入的运动大语言模型的性能。

Conclusion: VimoRAG框架通过视频检索增强的方式有效解决了运动生成中的数据稀缺问题，为运动大语言模型提供了重要的性能提升。

Abstract: This paper introduces VimoRAG, a novel video-based retrieval-augmented motion
generation framework for motion large language models (LLMs). As motion LLMs
face severe out-of-domain/out-of-vocabulary issues due to limited annotated
data, VimoRAG leverages large-scale in-the-wild video databases to enhance 3D
motion generation by retrieving relevant 2D human motion signals. While
video-based motion RAG is nontrivial, we address two key bottlenecks: (1)
developing an effective motion-centered video retrieval model that
distinguishes human poses and actions, and (2) mitigating the issue of error
propagation caused by suboptimal retrieval results. We design the Gemini Motion
Video Retriever mechanism and the Motion-centric Dual-alignment DPO Trainer,
enabling effective retrieval and generation processes. Experimental results
show that VimoRAG significantly boosts the performance of motion LLMs
constrained to text-only input.

</details>


### [109] [Adversarial Attacks on VQA-NLE: Exposing and Alleviating Inconsistencies in Visual Question Answering Explanations](https://arxiv.org/abs/2508.12430)
*Yahsin Yeh,Yilun Wu,Bokai Ruan,Honghan Shuai*

Main category: cs.CV

TL;DR: 这篇论文探索了视觉问题回答中自然语言解释系统的脏点，通过攻击和缓觡方法揭示了当前系统的安全性和可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 现有VQA-NLE系统存在解释不一致和缺乏真正理解的问题，需要揭示这些脏点并提出改善方案。

Method: 继承现有的问题干扰策略，提出新的图像最小改动攻击策略，并使用外部知识来缓解不一致性。

Result: 在两个标准测试集和两个常用VQA-NLE模型上证明了攻击的有效性和知识基防御的潜力。

Conclusion: 当前VQA-NLE系统存在严重的安全性和可靠性问题，知识基防御方案展示了提升模型健壮性的潜力。

Abstract: Natural language explanations in visual question answering (VQA-NLE) aim to
make black-box models more transparent by elucidating their decision-making
processes. However, we find that existing VQA-NLE systems can produce
inconsistent explanations and reach conclusions without genuinely understanding
the underlying context, exposing weaknesses in either their inference pipeline
or explanation-generation mechanism. To highlight these vulnerabilities, we not
only leverage an existing adversarial strategy to perturb questions but also
propose a novel strategy that minimally alters images to induce contradictory
or spurious outputs. We further introduce a mitigation method that leverages
external knowledge to alleviate these inconsistencies, thereby bolstering model
robustness. Extensive evaluations on two standard benchmarks and two widely
used VQA-NLE models underscore the effectiveness of our attacks and the
potential of knowledge-based defenses, ultimately revealing pressing security
and reliability concerns in current VQA-NLE systems.

</details>


### [110] [Vision-G1: Towards General Vision Language Reasoning with Multi-Domain Data Curation](https://arxiv.org/abs/2508.12680)
*Yuheng Zha,Kun Zhou,Yujia Wu,Yushu Wang,Jie Feng,Zhi Xu,Shibo Hao,Zhengzhong Liu,Eric P. Xing,Zhiting Hu*

Main category: cs.CV

TL;DR: 构建了涵盖8个维度46个数据源的视觉推理数据集，提出基于影响力函数的数据选择和难度过滤策略，通过多轮RL训练Vision-G1模型，在多个基准测试中达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型的训练主要局限于数学和逻辑推理任务，缺乏跨领域的泛化能力，且多领域数据整合困难

Method: 构建大规模多领域视觉推理数据集，采用影响力函数数据选择和难度过滤策略，使用多轮强化学习配合数据课程进行模型训练

Result: Vision-G1模型在多个视觉推理基准测试中表现优异，超越同类规模VLMs甚至GPT-4o和Gemini-1.5 Flash等专有模型

Conclusion: 提出的多领域数据集构建方法和训练策略有效提升了视觉语言模型的推理泛化能力，为跨领域视觉推理提供了新的解决方案

Abstract: Despite their success, current training pipelines for reasoning VLMs focus on
a limited range of tasks, such as mathematical and logical reasoning. As a
result, these models face difficulties in generalizing their reasoning
capabilities to a wide range of domains, primarily due to the scarcity of
readily available and verifiable reward data beyond these narrowly defined
areas. Moreover, integrating data from multiple domains is challenging, as the
compatibility between domain-specific datasets remains uncertain. To address
these limitations, we build a comprehensive RL-ready visual reasoning dataset
from 46 data sources across 8 dimensions, covering a wide range of tasks such
as infographic, mathematical, spatial, cross-image, graphic user interface,
medical, common sense and general science. We propose an influence function
based data selection and difficulty based filtering strategy to identify
high-quality training samples from this dataset. Subsequently, we train the
VLM, referred to as Vision-G1, using multi-round RL with a data curriculum to
iteratively improve its visual reasoning capabilities. Our model achieves
state-of-the-art performance across various visual reasoning benchmarks,
outperforming similar-sized VLMs and even proprietary models like GPT-4o and
Gemini-1.5 Flash. The model, code and dataset are publicly available at
https://github.com/yuh-zha/Vision-G1.

</details>


### [111] [Has GPT-5 Achieved Spatial Intelligence? An Empirical Study](https://arxiv.org/abs/2508.13142)
*Zhongang Cai,Yubo Wang,Qingping Sun,Ruisi Wang,Chenyang Gu,Wanqi Yin,Zhiqian Lin,Zhitao Yang,Chen Wei,Xuanke Shi,Kewang Deng,Xiaoyang Han,Zukai Chen,Jiaqi Li,Xiangyu Fan,Hanming Deng,Lewei Lu,Bo Li,Ziwei Liu,Quan Wang,Dahua Lin,Lei Yang*

Main category: cs.CV

TL;DR: GPT-5在多模态空间智能方面取得显著进展但仍落后于人类水平，研究发现专有模型在最困难问题上并无决定性优势


<details>
  <summary>Details</summary>
Motivation: 评估当前最先进多模态模型在空间理解和推理能力方面的表现，特别是新发布的GPT-5模型，以了解AI在空间智能方面的发展现状

Method: 提出统一的空间任务分类法，在8个关键基准测试上评估最先进的专有和开源模型，消耗超过10亿个token，并进行定性评估

Result: GPT-5展现出前所未有的空间智能能力，但在广泛任务中仍不及人类表现；专有模型在最困难问题上没有明显优势；识别出多模态模型更具挑战性的空间智能问题

Conclusion: 多模态模型在空间智能方面虽有进步但仍存在显著局限，需要进一步研究来弥合与人类能力的差距

Abstract: Multi-modal models have achieved remarkable progress in recent years.
Nevertheless, they continue to exhibit notable limitations in spatial
understanding and reasoning, which are fundamental capabilities to achieving
artificial general intelligence. With the recent release of GPT-5, allegedly
the most powerful AI model to date, it is timely to examine where the leading
models stand on the path toward spatial intelligence. First, we propose a
comprehensive taxonomy of spatial tasks that unifies existing benchmarks and
discuss the challenges in ensuring fair evaluation. We then evaluate
state-of-the-art proprietary and open-source models on eight key benchmarks, at
a cost exceeding one billion total tokens. Our empirical study reveals that (1)
GPT-5 demonstrates unprecedented strength in spatial intelligence, yet (2)
still falls short of human performance across a broad spectrum of tasks.
Moreover, we (3) identify the more challenging spatial intelligence problems
for multi-modal models, and (4) proprietary models do not exhibit a decisive
advantage when facing the most difficult problems. In addition, we conduct a
qualitative evaluation across a diverse set of scenarios that are intuitive for
humans yet fail even the most advanced multi-modal models.

</details>
