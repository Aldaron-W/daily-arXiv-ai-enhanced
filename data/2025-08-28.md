<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 59]
- [cs.SE](#cs.SE) [Total: 1]
- [cs.LG](#cs.LG) [Total: 4]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.CR](#cs.CR) [Total: 4]
- [cs.CY](#cs.CY) [Total: 2]
- [cs.CV](#cs.CV) [Total: 3]
- [cs.AI](#cs.AI) [Total: 4]
- [cs.HC](#cs.HC) [Total: 1]
- [math.CO](#math.CO) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [MultiPL-MoE: Multi-Programming-Lingual Extension of Large Language Models through Hybrid Mixture-of-Experts](https://arxiv.org/abs/2508.19268)
*Qing Wang,Xue Han,Jiahui Wang,Lehao Xing,Qian Hu,Lianlian Zhang,Chao Deng,Junlan Feng*

Main category: cs.CL

TL;DR: 提出MultiPL-MoE方法，通过混合专家模型提升LLMs的多编程语言代码生成能力，在保持主流语言性能的同时提升小众语言表现。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在代码生成方面表现出色，但多编程语言代码生成仍然极具挑战性。需要在有限计算资源下提升多编程语言性能，同时保持主流语言的优秀表现。

Method: 提出MultiPL-MoE混合专家模型，结合token级和segment级两个配对MoE。token级MoE采用标准upcycling结构加门控权重归一化；segment级MoE使用滑动窗口分割输入序列，并采用专家选择路由策略让专家选择top-k片段。

Result: 实验证明了MultiPL-MoE方法的有效性。

Conclusion: MultiPL-MoE通过创新的混合专家架构成功提升了LLMs在多编程语言代码生成方面的性能，为解决这一挑战性问题提供了有效方案。

Abstract: Despite LLMs' excellent code creation capabilities, multilingual code
generation remains extremely challenging. To address this, we intent to improve
the multi-programming-lingual (MultiPL) performance of the base LLMs while
retaining the most popular ones using restricted computational resources. We
consider MultiPL to be a special case of multiple natural languages and propose
a MultiPL extension of LLMs utilizing a hybrid mixture of experts (MoE), called
MultiPL-MoE. Specifically, MultiPL-MoE combines two paired MoEs to optimize
expert selection at both the token and segment levels. The token-level MoE is a
standard upcycling MoE structure with a shared expert and a novel gate weight
normalization approach that aids in the final fusion with the segment-level
MoE. The segment-level MoE incorporates two innovative designs to better
capture the syntactic structure and contextual patterns of programming
languages: First, using a sliding window to partition the input token sequence
into multiple segments; Then, adopting an expert-choice routing strategy that
allows experts to select the top-k segments. The results of the experiment
proved the effectiveness of MultiPL-MoE.

</details>


### [2] [Whisper based Cross-Lingual Phoneme Recognition between Vietnamese and English](https://arxiv.org/abs/2508.19270)
*Nguyen Huu Nhat Minh,Tran Nguyen Anh,Truong Dinh Dung,Vo Van Nam,Le Pham Tuyen*

Main category: cs.CL

TL;DR: 跨语言音素识别提升趋势，特别是趋势语和英语的音系差异带来挑战。本文提出了一种新的双语语音识别方法，通过构建代表性双语音素集和利用PhoWhisper预训练编码器来提高识别准确性。


<details>
  <summary>Details</summary>
Motivation: 趋势语依赖声调变化区分词义，英语则有重音模式和非标准发音，这些差异给跨语言音素识别带来了重大挑战。需要解决两种语言音系统之间的音素对齐问题。

Method: 1. 构建代表性的双语音素集，桥接趋势语和英语音系统的差异
2. 设计端到端系统，利用PhoWhisper预训练编码器获取深度高级表征来改善音素识别

Result: 实验结果显示，所提方法不仅在趋势语双语语音识别中提高了识别准确性，还为处理声调和重音基础的音素识别复杂性提供了健壁的框架。

Conclusion: 该研究成功地解决了趋势语和英语跨语言音素识别的挑战，通过新题的双语音素集和深度学习方法，为处理声调语言和重音语言的音素识别问题提供了有效的解决方案。

Abstract: Cross-lingual phoneme recognition has emerged as a significant challenge for
accurate automatic speech recognition (ASR) when mixing Vietnamese and English
pronunciations. Unlike many languages, Vietnamese relies on tonal variations to
distinguish word meanings, whereas English features stress patterns and
non-standard pronunciations that hinder phoneme alignment between the two
languages. To address this challenge, we propose a novel bilingual speech
recognition approach with two primary contributions: (1) constructing a
representative bilingual phoneme set that bridges the differences between
Vietnamese and English phonetic systems; (2) designing an end-to-end system
that leverages the PhoWhisper pre-trained encoder for deep high-level
representations to improve phoneme recognition. Our extensive experiments
demonstrate that the proposed approach not only improves recognition accuracy
in bilingual speech recognition for Vietnamese but also provides a robust
framework for addressing the complexities of tonal and stress-based phoneme
recognition

</details>


### [3] [Rethinking Reasoning in LLMs: Neuro-Symbolic Local RetoMaton Beyond ICL and CoT](https://arxiv.org/abs/2508.19271)
*Rushitha Santhoshi Mamidala,Anshuman Chhabra,Ankur Mali*

Main category: cs.CL

TL;DR: 本文提出了一种基于局部加权有限自动机（WFA）的RetoMaton改进方法，替代了传统的提示推理策略，在多个推理任务中实现了更稳定、可解释的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的提示推理方法（如Chain-of-Thought和In-Context Learning）存在脆弱性和不一致性问题，输出结果对种子、格式或提示微小变化敏感，缺乏可靠性和可解释性。

Method: 将RetoMaton的全局数据存储替换为从外部领域语料库直接构建的局部加权有限自动机（WFA），利用WFA的显式结构提供可验证和模块化的检索行为。

Result: 在LLaMA-3.2-1B和Gemma-3-1B-PT两个预训练模型上，在TriviaQA、GSM8K和MMLU三个推理任务中，相比基础模型和基于提示的方法，局部RetoMaton变体均能持续提升性能。

Conclusion: 通过轻量级的自动机引导内存，为现代大语言模型实现可信赖的符号推理提供了一条有前景的技术路径，具有更好的领域迁移和互操作性。

Abstract: Prompt-based reasoning strategies such as Chain-of-Thought (CoT) and
In-Context Learning (ICL) have become widely used for eliciting reasoning
capabilities in large language models (LLMs). However, these methods rely on
fragile, implicit mechanisms often yielding inconsistent outputs across seeds,
formats, or minor prompt variations making them fundamentally unreliable for
tasks requiring stable, interpretable reasoning. In contrast, automata-based
neuro-symbolic frameworks like RetoMaton offer a more structured and
trustworthy alternative by grounding retrieval in symbolic memory with
deterministic transitions. In this work, we extend RetoMaton by replacing its
global datastore with a local, task-adaptive Weighted Finite Automaton (WFA),
constructed directly from external domain corpora. This local automaton
structure promotes robust, context-aware retrieval while preserving symbolic
traceability and low inference overhead. Unlike prompting, which entangles
context and memory in opaque ways, our approach leverages the explicit
structure of WFAs to provide verifiable and modular retrieval behavior, making
it better suited for domain transfer and interoperability. We evaluate this
local RetoMaton variant on two pretrained LLMs LLaMA-3.2-1B and Gemma-3-1B-PT
across three reasoning tasks: TriviaQA (reading comprehension), GSM8K
(multi-step math), and MMLU (domain knowledge). Compared to the base model and
prompting-based methods, augmenting these setups with local RetoMaton
consistently improves performance while enabling transparent and reproducible
retrieval dynamics. Our results highlight a promising shift toward trustworthy,
symbolic reasoning in modern LLMs via lightweight, automaton-guided memory.

</details>


### [4] [RAGAPHENE: A RAG Annotation Platform with Human Enhancements and Edits](https://arxiv.org/abs/2508.19272)
*Kshitij Fadnis,Sara Rosenthal,Maeda Hanafi,Yannis Katsis,Marina Danilevsky*

Main category: cs.CL

TL;DR: RAGAPHENE是一个基于聊天的标注平台，用于模拟真实世界对话来构建RAG基准测试，已被40名标注者用于创建数千个对话。


<details>
  <summary>Details</summary>
Motivation: 由于LLM在提供事实正确信息时可能出现幻觉，需要构建能够评估多轮RAG对话的基准测试，而模拟真实世界对话对于产生高质量评估基准至关重要。

Method: 开发了RAGAPHENE聊天标注平台，让标注者能够模拟真实世界对话，用于基准测试和LLM评估。

Result: 该平台已被约40名标注者成功使用，构建了数千个真实世界对话。

Conclusion: RAGAPHENE平台为评估LLM在多轮RAG对话中的表现提供了有效的工具和方法。

Abstract: Retrieval Augmented Generation (RAG) is an important aspect of conversing
with Large Language Models (LLMs) when factually correct information is
important. LLMs may provide answers that appear correct, but could contain
hallucinated information. Thus, building benchmarks that can evaluate LLMs on
multi-turn RAG conversations has become an increasingly important task.
Simulating real-world conversations is vital for producing high quality
evaluation benchmarks. We present RAGAPHENE, a chat-based annotation platform
that enables annotators to simulate real-world conversations for benchmarking
and evaluating LLMs. RAGAPHENE has been successfully used by approximately 40
annotators to build thousands of real-world conversations.

</details>


### [5] [Leveraging Language Models and Machine Learning in Verbal Autopsy Analysis](https://arxiv.org/abs/2508.19274)
*Yue Chu*

Main category: cs.CL

TL;DR: 本研究探讨了在口头尸检中使用预训练语言模型分析叙述文本，结合结构化问题，显著提高了死因分类准确性，特别是在非传染性疾病识别方面。


<details>
  <summary>Details</summary>
Motivation: 在没有民事登记和生命统计的国家，口头尸检是估计死因的关键工具，但现有自动化算法只使用结构化问题而忽略了叙述文本中的宝贵信息。

Method: 使用预训练语言模型和机器学习技术，分析南非实证数据，探索叙述文本单独使用以及与结构化问题多模态融合的策略。

Result: 仅使用叙述文本时，基于Transformer的预训练模型在个体和群体层面都优于仅使用问题的算法；多模态方法进一步提升了分类性能，证实两种模态都有独特贡献。

Conclusion: 叙述文本能显著增强死因分类，需要更多高质量多样化数据来训练模型，并为重新设计口头尸检工具提供重要见解。

Abstract: In countries without civil registration and vital statistics, verbal autopsy
(VA) is a critical tool for estimating cause of death (COD) and inform policy
priorities. In VA, interviewers ask proximal informants for details on the
circumstances preceding a death, in the form of unstructured narratives and
structured questions. Existing automated VA cause classification algorithms
only use the questions and ignore the information in the narratives. In this
thesis, we investigate how the VA narrative can be used for automated COD
classification using pretrained language models (PLMs) and machine learning
(ML) techniques. Using empirical data from South Africa, we demonstrate that
with the narrative alone, transformer-based PLMs with task-specific fine-tuning
outperform leading question-only algorithms at both the individual and
population levels, particularly in identifying non-communicable diseases. We
explore various multimodal fusion strategies combining narratives and questions
in unified frameworks. Multimodal approaches further improve performance in COD
classification, confirming that each modality has unique contributions and may
capture valuable information that is not present in the other modality. We also
characterize physician-perceived information sufficiency in VA. We describe
variations in sufficiency levels by age and COD and demonstrate that
classification accuracy is affected by sufficiency for both physicians and
models. Overall, this thesis advances the growing body of knowledge at the
intersection of natural language processing, epidemiology, and global health.
It demonstrates the value of narrative in enhancing COD classification. Our
findings underscore the need for more high-quality data from more diverse
settings to use in training and fine-tuning PLM/ML methods, and offer valuable
insights to guide the rethinking and redesign of the VA instrument and
interview.

</details>


### [6] [FLAIRR-TS -- Forecasting LLM-Agents with Iterative Refinement and Retrieval for Time Series](https://arxiv.org/abs/2508.19279)
*Gunjan Jalori,Preetika Verma,Sercan Ö Arık*

Main category: cs.CL

TL;DR: FLAIRR-TS是一个基于代理系统的测试时提示优化框架，通过预测代理和优化代理的协作，无需微调即可提升大语言模型在时间序列预测中的性能


<details>
  <summary>Details</summary>
Motivation: 现有方法需要为每个任务精心设计自然语言提示，过程繁琐且缺乏通用性，需要一种能够自适应优化提示的框架

Method: 使用双代理系统：预测代理用初始提示生成预测，优化代理基于历史输出和检索到的类似样本进行提示优化，采用创造性提示模板实现跨领域泛化

Result: 在基准数据集上相比静态提示和检索增强基线提高了准确性，接近专门设计的提示性能

Conclusion: FLAIRR-TS提供了一种实用的替代调优方法，通过自适应提示优化和检索的代理方法实现了强大性能

Abstract: Time series Forecasting with large languagemodels (LLMs) requires bridging
numericalpatterns and natural language. Effective fore-casting on LLM often
relies on extensive pre-processing and fine-tuning.Recent studiesshow that a
frozen LLM can rival specializedforecasters when supplied with a carefully
en-gineered natural-language prompt, but craft-ing such a prompt for each task
is itself oner-ous and ad-hoc. We introduce FLAIRR-TS, atest-time prompt
optimization framework thatutilizes an agentic system: a
Forecaster-agentgenerates forecasts using an initial prompt,which is then
refined by a refiner agent, in-formed by past outputs and retrieved
analogs.This adaptive prompting generalizes across do-mains using creative
prompt templates andgenerates high-quality forecasts without inter-mediate code
generation.Experiments onbenchmark datasets show improved accuracyover static
prompting and retrieval-augmentedbaselines, approaching the performance
ofspecialized prompts.FLAIRR-TS providesa practical alternative to tuning,
achievingstrong performance via its agentic approach toadaptive prompt
refinement and retrieval.

</details>


### [7] [CORE: Lossless Compression for Retrieval-Augmented LLMs via Reinforcement Learning](https://arxiv.org/abs/2508.19282)
*Ziqiang Cui,Yunpeng Weng,Xing Tang,Peiyang Liu,Shiwei Li,Bowei He,Jiamin Chen,Xiuqiang He,Chen Ma*

Main category: cs.CL

TL;DR: CORE提出了一种基于强化学习的无损上下文压缩方法，通过端到端训练优化压缩过程，在保持3%高压缩比的同时提升RAG性能


<details>
  <summary>Details</summary>
Motivation: 解决RAG中检索文档过多导致输入长度增加和计算成本上升的问题，同时避免传统压缩方法损害最终任务性能的缺陷

Method: 使用强化学习（GRPO策略优化），以最终任务性能作为奖励信号，端到端训练压缩器生成最大化LLM答案准确性的摘要

Result: 在四个数据集上实验显示，3%的高压缩比下不仅避免了性能下降，还将平均精确匹配(EM)分数提高了3.3个点

Conclusion: CORE方法通过强化学习实现了有效的无损上下文压缩，显著提升了RAG系统的效率和准确性

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a promising approach to
enhance the timeliness of knowledge and the factual accuracy of responses in
Large Language Models (LLMs). However, the inclusion of excessive retrieved
documents substantially increases the input length, leading to higher
computational costs. Previous studies have attempted to compress retrieved
documents into shorter texts before in-context integration, but such methods
often compromise end-task performance. The lack of well-defined compression
targets forces many approaches to rely on fixed heuristics, which cannot
guarantee that the compressed content will effectively support the end task. To
address these limitations, we propose CORE, a novel method designed to achieve
lossless context compression for RAG. CORE employs reinforcement learning to
optimize the compression process without relying on predefined compression
labels. Specifically, it utilizes end-task performance as a reward signal and
applies Generalized Reinforcement Learning Policy Optimization (GRPO) to train
the compressor. This end-to-end training framework enables the compressor to
generate summaries that maximize the accuracy of answers generated by the LLM.
Extensive experiments on four datasets demonstrate the superiority of our
approach. With a high compression ratio of 3\%, our method not only avoids
performance degradation compared to prepending full documents across all
datasets but also improves the average Exact Match (EM) score by 3.3 points.
The code will be released soon.

</details>


### [8] [Context-Adaptive Synthesis and Compression for Enhanced Retrieval-Augmented Generation in Complex Domains](https://arxiv.org/abs/2508.19357)
*Peiran Zhou,Junnan Zhu,Yichen Shen,Ruoxi Yu*

Main category: cs.CL

TL;DR: CASC框架通过上下文分析和合成模块，有效处理多文档检索中的信息过载和冲突问题，显著提升复杂领域问答的准确性和可信度


<details>
  <summary>Details</summary>
Motivation: 传统RAG在复杂领域处理多文档时存在信息过载、合成效率低的问题，导致答案不准确和不可信

Method: 提出CASC框架，包含基于微调小模型的Context Analyzer & Synthesizer模块，进行关键信息提取、跨文档一致性检查和冲突解决、面向问题的结构化合成

Result: 在SciDocs-QA数据集上，CASC持续优于强基线方法

Conclusion: CASC能够将原始分散信息转化为高度压缩、结构化、语义丰富的上下文，显著减少最终Reader LLM的token数量和认知负担

Abstract: Large Language Models (LLMs) excel in language tasks but are prone to
hallucinations and outdated knowledge. Retrieval-Augmented Generation (RAG)
mitigates these by grounding LLMs in external knowledge. However, in complex
domains involving multiple, lengthy, or conflicting documents, traditional RAG
suffers from information overload and inefficient synthesis, leading to
inaccurate and untrustworthy answers. To address this, we propose CASC
(Context-Adaptive Synthesis and Compression), a novel framework that
intelligently processes retrieved contexts. CASC introduces a Context Analyzer
& Synthesizer (CAS) module, powered by a fine-tuned smaller LLM, which performs
key information extraction, cross-document consistency checking and conflict
resolution, and question-oriented structured synthesis. This process transforms
raw, scattered information into a highly condensed, structured, and
semantically rich context, significantly reducing the token count and cognitive
load for the final Reader LLM. We evaluate CASC on SciDocs-QA, a new
challenging multi-document question answering dataset designed for complex
scientific domains with inherent redundancies and conflicts. Our extensive
experiments demonstrate that CASC consistently outperforms strong baselines.

</details>


### [9] [Reflective Agreement: Combining Self-Mixture of Agents with a Sequence Tagger for Robust Event Extraction](https://arxiv.org/abs/2508.19359)
*Fatemeh Haji,Mazal Bethany,Cho-Yu Jason Chiang,Anthony Rios,Peyman Najafirad*

Main category: cs.CL

TL;DR: 提出ARIS混合方法，结合自混合代理和判别式序列标注器，通过结构化模型共识和LLM反思推理提升事件抽取性能


<details>
  <summary>Details</summary>
Motivation: 传统判别模型精度高但召回率低，生成式LLM方法语义灵活但存在幻觉和不一致预测，需要结合两者优势

Method: ARIS系统：自混合代理+判别式序列标注器，采用结构化模型共识、置信度过滤和LLM反思推理模块，并研究分解指令微调

Result: 在三个基准数据集上超越现有最先进的事件抽取方法

Conclusion: ARIS混合方法有效结合了判别模型和生成模型的优势，显著提升了事件抽取的整体性能

Abstract: Event Extraction (EE) involves automatically identifying and extracting
structured information about events from unstructured text, including triggers,
event types, and arguments. Traditional discriminative models demonstrate high
precision but often exhibit limited recall, particularly for nuanced or
infrequent events. Conversely, generative approaches leveraging Large Language
Models (LLMs) provide higher semantic flexibility and recall but suffer from
hallucinations and inconsistent predictions. To address these challenges, we
propose Agreement-based Reflective Inference System (ARIS), a hybrid approach
combining a Self Mixture of Agents with a discriminative sequence tagger. ARIS
explicitly leverages structured model consensus, confidence-based filtering,
and an LLM reflective inference module to reliably resolve ambiguities and
enhance overall event prediction quality. We further investigate decomposed
instruction fine-tuning for enhanced LLM event extraction understanding.
Experiments demonstrate our approach outperforms existing state-of-the-art
event extraction methods across three benchmark datasets.

</details>


### [10] [LongReasonArena: A Long Reasoning Benchmark for Large Language Models](https://arxiv.org/abs/2508.19363)
*Jiayu Ding,Shuming Ma,Lei Cui,Nanning Zheng,Furu Wei*

Main category: cs.CL

TL;DR: LongReasonArena是一个专门评估大型语言模型长推理能力的新基准，通过多步算法任务测试检索和回溯等关键能力，推理长度可达100万token，现有模型表现普遍不佳。


<details>
  <summary>Details</summary>
Motivation: 现有的长上下文基准主要评估模型对长输入的理解能力，但忽略了长推理能力的评估，需要专门针对长推理设计的评测基准。

Method: 设计需要执行多步算法的任务，通过控制输入来任意扩展推理长度，最高可达100万token的推理过程，测试检索和回溯等关键推理能力。

Result: LongReasonArena对开源和专有LLM都构成重大挑战，Deepseek-R1仅达到7.5%的准确率，准确率随预期推理步骤数的对数呈线性下降趋势。

Conclusion: 该基准填补了长推理能力评估的空白，揭示了当前LLM在长推理任务上的显著局限性，为未来模型发展提供了重要评测工具。

Abstract: Existing long-context benchmarks for Large Language Models (LLMs) focus on
evaluating comprehension of long inputs, while overlooking the evaluation of
long reasoning abilities. To address this gap, we introduce LongReasonArena, a
benchmark specifically designed to assess the long reasoning capabilities of
LLMs. Our tasks require models to solve problems by executing multi-step
algorithms that reflect key aspects of long reasoning, such as retrieval and
backtracking. By controlling the inputs, the required reasoning length can be
arbitrarily scaled, reaching up to 1 million tokens of reasoning for the most
challenging tasks. Extensive evaluation results demonstrate that
LongReasonArena presents a significant challenge for both open-source and
proprietary LLMs. For instance, Deepseek-R1 achieves only 7.5% accuracy on our
task. Further analysis also reveals that the accuracy exhibits a linear decline
with respect to the logarithm of the expected number of reasoning steps. Our
code and data is available at
https://github.com/LongReasonArena/LongReasonArena.

</details>


### [11] [Database Entity Recognition with Data Augmentation and Deep Learning](https://arxiv.org/abs/2508.19372)
*Zikun Fu,Chen Yang,Kourosh Davoudi,Ken Q. Pu*

Main category: cs.CL

TL;DR: 这篇论文提出了一种专门用于数据库实体识别(DB-ER)的方法，包括新的标准测试集、数据增帿技术和基于T5的专门模型，在精准率和召回率方面都超过了现有的最先进方法。


<details>
  <summary>Details</summary>
Motivation: 解决自然语言查询(NLQ)中数据库实体识别(DB-ER)的挑战，这对于文本到SQL查询的转换至关重要。

Method: 构建了人工标注的DB-ER标准测试集；提出了一种新的数据增帿方法，利用SQL查询自动标注NLQ；基于T5构建专门的实体识别模型，采用序列标注和分类任务进行细调。

Result: 与两种最先进的NER标注器相比，该模型在精准率和召回率方面都表现更好。数据增帿能夠提升10%以上的性能，T5基础模型细调能提升5-10%的性能。

Conclusion: 该研究提供了一种高效的DB-ER解决方案，通过数据增帿和专门模型设计显著提升了实体识别的性能，为文本到SQL转换领域做出了重要贡献。

Abstract: This paper addresses the challenge of Database Entity Recognition (DB-ER) in
Natural Language Queries (NLQ). We present several key contributions to advance
this field: (1) a human-annotated benchmark for DB-ER task, derived from
popular text-to-sql benchmarks, (2) a novel data augmentation procedure that
leverages automatic annotation of NLQs based on the corresponding SQL queries
which are available in popular text-to-SQL benchmarks, (3) a specialized
language model based entity recognition model using T5 as a backbone and two
down-stream DB-ER tasks: sequence tagging and token classification for
fine-tuning of backend and performing DB-ER respectively. We compared our DB-ER
tagger with two state-of-the-art NER taggers, and observed better performance
in both precision and recall for our model. The ablation evaluation shows that
data augmentation boosts precision and recall by over 10%, while fine-tuning of
the T5 backbone boosts these metrics by 5-10%.

</details>


### [12] [One Joke to Rule them All? On the (Im)possibility of Generalizing Humor](https://arxiv.org/abs/2508.19402)
*Mor Turgeman,Chen Shani,Dafna Shahaf*

Main category: cs.CL

TL;DR: 研究探讨大型语言模型在不同幽默类型间的迁移学习能力，发现多源训练可提升泛化性能，Dad Jokes是最佳迁移源但最难迁移到


<details>
  <summary>Details</summary>
Motivation: 幽默计算研究通常专注于特定类型，但网络环境中不断涌现新型幽默，需要研究LLMs是否能通过迁移学习捕捉深层可转移机制来适应这种演变

Method: 在四个不同幽默任务数据集上进行迁移学习实验，设置不同多样性训练条件（1-3个训练数据集），测试在新任务上的表现

Result: 模型具备一定迁移能力，在未见数据集上最高达到75%准确率；多源训练提升迁移性能（1.88-4.05%），且领域内性能几乎不下降；Dad Jokes是最佳迁移促进者但最难迁移到

Conclusion: 幽默类型间存在可迁移性，多源训练有助于提升LLMs对新型幽默的泛化能力，为应对不断演变的幽默景观提供了可行路径

Abstract: Humor is a broad and complex form of communication that remains challenging
for machines. Despite its broadness, most existing research on computational
humor traditionally focused on modeling a specific type of humor. In this work,
we wish to understand whether competence on one or more specific humor tasks
confers any ability to transfer to novel, unseen types; in other words, is this
fragmentation inevitable? This question is especially timely as new humor types
continuously emerge in online and social media contexts (e.g., memes,
anti-humor, AI fails). If Large Language Models (LLMs) are to keep up with this
evolving landscape, they must be able to generalize across humor types by
capturing deeper, transferable mechanisms. To investigate this, we conduct a
series of transfer learning experiments across four datasets, representing
different humor tasks. We train LLMs under varied diversity settings (1-3
datasets in training, testing on a novel task). Experiments reveal that models
are capable of some transfer, and can reach up to 75% accuracy on unseen
datasets; training on diverse sources improves transferability (1.88-4.05%)
with minimal-to-no drop in in-domain performance. Further analysis suggests
relations between humor types, with Dad Jokes surprisingly emerging as the best
enabler of transfer (but is difficult to transfer to). We release data and
code.

</details>


### [13] [A perishable ability? The future of writing in the face of generative artificial intelligence](https://arxiv.org/abs/2508.19427)
*Evandro L. T. P. Cunha*

Main category: cs.CL

TL;DR: 本文探讨AI文本生成工具可能导致人类写作能力退化的风险，与古希腊黑暗时代文字能力丧失的历史现象相类比


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型等生成式AI工具的快速发展，人类越来越多地将写作活动外包给机器，这可能导致人类写作能力的退化甚至丧失，需要对此潜在风险进行探讨

Method: 采用历史类比方法，将当前AI技术发展对人类写作能力的影响与古希腊黑暗时代文字能力丧失的历史现象进行对比分析

Result: 识别出AI文本生成工具的普及确实存在导致人类写作能力退化的风险，这与历史上其他技术变革时期人类技能退化的模式相似

Conclusion: 需要警惕AI工具过度使用对人类基本能力的潜在负面影响，在享受技术便利的同时应保持对人类核心能力的培养和锻炼

Abstract: The 2020s have been witnessing a very significant advance in the development
of generative artificial intelligence tools, including text generation systems
based on large language models. These tools have been increasingly used to
generate texts in the most diverse domains -- from technical texts to literary
texts --, which might eventually lead to a lower volume of written text
production by humans. This article discusses the possibility of a future in
which human beings will have lost or significantly decreased their ability to
write due to the outsourcing of this activity to machines. This possibility
parallels the loss of the ability to write in other moments of human history,
such as during the so-called Greek Dark Ages (approx. 1200 BCE - 800 BCE).

</details>


### [14] [Heterogeneous LLM Methods for Ontology Learning (Few-Shot Prompting, Ensemble Typing, and Attention-Based Taxonomies)](https://arxiv.org/abs/2508.19428)
*Aleksandra Beliaeva,Temurbek Rahmatullaev*

Main category: cs.CL

TL;DR: 本文提出了一个综合系统，通过结合检索增强提示、零样本分类和注意力图谱模型，在LLMs4OL 2025挑战赛的三个任务中获得了排名前列的成绩


<details>
  <summary>Details</summary>
Motivation: 解决本体论方法学习中的全面挑战，包括术语提取、类型分配和分类系统发现，以演示基于大语言模型的本体论方法学习系统在异构域中的可扩展性、适应性和稳健性

Method: 针对不同任务采用了模块化方法：任务A使用检索增强生成流程进行域特定术语和类型联合提取；任务B在少样本设置中重用RAG方案，在零样本设置中使用多种嵌入模型的余弦相似度经信心加权的零样本分类器；任务C通过训练轻量跨注意力层来模拟软邻接矩阵进行图谱推理

Result: 该系统在官方排行榜中在所有三个任务上都获得了顶级排名，体现了其在本体论方法学习任务中的优异性能

Conclusion: 这些模块化、任务特定的解决方案展示了基于LLM的架构在异构域本体论方法学习中的可扩展性、适应性和稳健性，为本体论构建提供了高效的综合解决方案

Abstract: We present a comprehensive system for addressing Tasks A, B, and C of the
LLMs4OL 2025 challenge, which together span the full ontology construction
pipeline: term extraction, typing, and taxonomy discovery. Our approach
combines retrieval-augmented prompting, zero-shot classification, and
attention-based graph modeling -- each tailored to the demands of the
respective task. For Task A, we jointly extract domain-specific terms and their
ontological types using a retrieval-augmented generation (RAG) pipeline.
Training data was reformulated into a document to terms and types
correspondence, while test-time inference leverages semantically similar
training examples. This single-pass method requires no model finetuning and
improves overall performance through lexical augmentation Task B, which
involves assigning types to given terms, is handled via a dual strategy. In the
few-shot setting (for domains with labeled training data), we reuse the RAG
scheme with few-shot prompting. In the zero-shot setting (for previously unseen
domains), we use a zero-shot classifier that combines cosine similarity scores
from multiple embedding models using confidence-based weighting. In Task C, we
model taxonomy discovery as graph inference. Using embeddings of type labels,
we train a lightweight cross-attention layer to predict is-a relations by
approximating a soft adjacency matrix. These modular, task-specific solutions
enabled us to achieve top-ranking results in the official leaderboard across
all three tasks. Taken together these strategies showcase the scalability,
adaptability, and robustness of LLM-based architectures for ontology learning
across heterogeneous domains.
  Code is available at:
https://github.com/BelyaevaAlex/LLMs4OL-Challenge-Alexbek

</details>


### [15] [Bridging Language Gaps: Enhancing Few-Shot Language Adaptation](https://arxiv.org/abs/2508.19464)
*Philipp Borchert,Jochen De Weerdt,Marie-Francine Moens*

Main category: cs.CL

TL;DR: CoLAP方法通过对比学习和跨语言表示融合，实现从高资源语言到低资源语言的任务知识迁移，显著减少对标注数据的需求，在多语言NLP任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决多语言NLP中语言资源不平衡问题，高资源语言数据丰富而低资源语言数据匮乏，导致性能差距显著。

Method: 提出对比语言对齐提示(CoLAP)方法，结合对比学习和跨语言表示，促进任务特定知识从高资源语言向低资源语言的高效迁移。

Result: 实验表明CoLAP在自然语言推理和关系抽取等任务上，优于少样本跨语言迁移基线和上下文学习方法，即使在有限数据下也能有效缩小跨语言性能差距。

Conclusion: CoLAP方法具有数据高效性，能够快速适应新语言，为开发更高效的多语言NLP技术提供了有效解决方案。

Abstract: The disparity in language resources poses a challenge in multilingual NLP,
with high-resource languages benefiting from extensive data, while low-resource
languages lack sufficient data for effective training. Our Contrastive Language
Alignment with Prompting (CoLAP) method addresses this gap by integrating
contrastive learning with cross-lingual representations, facilitating
task-specific knowledge transfer from high-resource to lower-resource
languages. The primary advantage of our approach is its data efficiency,
enabling rapid adaptation to new languages and reducing the need for large
labeled datasets. We conduct experiments with multilingual encoder-only and
decoder-only language models on natural language understanding tasks, including
natural language inference and relation extraction, evaluating performance
across both high- and low-resource languages. Our results demonstrate that
CoLAP outperforms few-shot cross-lingual transfer baselines and in-context
learning, even with limited available data. This effectively narrows the
cross-lingual performance gap, contributing to the development of more
efficient multilingual NLP techniques.

</details>


### [16] [Inference Gap in Domain Expertise and Machine Intelligence in Named Entity Recognition: Creation of and Insights from a Substance Use-related Dataset](https://arxiv.org/abs/2508.19467)
*Sumon Kanti Dey,Jeanne M. Powell,Azra Ismail,Jeanmarie Perrone,Abeed Sarker*

Main category: cs.CL

TL;DR: 这篇论文提出了一个命名实体识别框架，从社交媒体中提取非医疗类防片使用的临床和社会影响，并主张领域特定精调在临床NLP任务中的价值。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上用户真实分享的第一手经验为非医疗防片使用的影响提供了价值过低估的见解，但这些信息在传统医疗环境中常常被低估。

Method: 开发了RedditImpacts 2.0数据集，构建命名实体识别框架来提取临床影响和社会影响两类实体，并评估了精调的编码器模型和大语言模型在零小样本学习情况下的表现。

Result: 精调的DeBERTa-large模型达到了0.61的F1分数，在精确度、字符串准确性和任务指南遵循方面都明显优于大语言模型，但仍显著超过专家一致性水平。

Conclusion: 领域特定精调对临床NLP任务具有重要价值，但当前的NER/AI技术在需要深度领域知识的任务上仍与专家智能存在差距。

Abstract: Nonmedical opioid use is an urgent public health challenge, with far-reaching
clinical and social consequences that are often underreported in traditional
healthcare settings. Social media platforms, where individuals candidly share
first-person experiences, offer a valuable yet underutilized source of insight
into these impacts. In this study, we present a named entity recognition (NER)
framework to extract two categories of self-reported consequences from social
media narratives related to opioid use: ClinicalImpacts (e.g., withdrawal,
depression) and SocialImpacts (e.g., job loss). To support this task, we
introduce RedditImpacts 2.0, a high-quality dataset with refined annotation
guidelines and a focus on first-person disclosures, addressing key limitations
of prior work. We evaluate both fine-tuned encoder-based models and
state-of-the-art large language models (LLMs) under zero- and few-shot
in-context learning settings. Our fine-tuned DeBERTa-large model achieves a
relaxed token-level F1 of 0.61 [95% CI: 0.43-0.62], consistently outperforming
LLMs in precision, span accuracy, and adherence to task-specific guidelines.
Furthermore, we show that strong NER performance can be achieved with
substantially less labeled data, emphasizing the feasibility of deploying
robust models in resource-limited settings. Our findings underscore the value
of domain-specific fine-tuning for clinical NLP tasks and contribute to the
responsible development of AI tools that may enhance addiction surveillance,
improve interpretability, and support real-world healthcare decision-making.
The best performing model, however, still significantly underperforms compared
to inter-expert agreement (Cohen's kappa: 0.81), demonstrating that a gap
persists between expert intelligence and current state-of-the-art NER/AI
capabilities for tasks requiring deep domain knowledge.

</details>


### [17] [Automatic Question & Answer Generation Using Generative Large Language Model (LLM)](https://arxiv.org/abs/2508.19475)
*Md. Alvee Ehsan,A. S. M Mehedi Hasan,Kefaya Benta Shahnoor,Syeda Sumaiya Tasneem*

Main category: cs.CL

TL;DR: 使用微调的Meta-Llama 2-7B模型和RACE数据集，通过提示工程实现自动问答生成，为教育评估提供高效解决方案


<details>
  <summary>Details</summary>
Motivation: 传统学生评估需要教师手动创建多样化且公平的问题，这个过程耗时耗力。自动问答生成可以简化评估流程，解放教师资源

Method: 采用无监督学习方法，基于Meta-Llama 2-7B模型进行微调，整合RACE数据集，利用提示工程技术生成不同类型的问题（选择题、概念题、事实题）

Result: 开发了一个定制化模型，能够根据教师偏好的问题风格自动生成问题和答案

Conclusion: 该研究提供了一个可靠高效的工具，可以简化文本评估过程，为教育工作者节省宝贵时间和资源

Abstract: \Abstract{In the realm of education, student evaluation holds equal
significance as imparting knowledge. To be evaluated, students usually need to
go through text-based academic assessment methods. Instructors need to make
diverse sets of questions that need to be fair for all students to prove their
adequacy over a particular topic. This can prove to be quite challenging as
they may need to manually go through several different lecture materials. Our
objective is to make this whole process much easier by implementing Automatic
Question Answer Generation /(AQAG), using fine-tuned generative LLM. For
tailoring the instructor's preferred question style (MCQ, conceptual, or
factual questions), prompt Engineering (PE) is being utilized. In this
research, we propose to leverage unsupervised learning methods in NLP,
primarily focusing on the English language. This approach empowers the base
Meta-Llama 2-7B model to integrate RACE dataset as training data for the
fine-tuning process. Creating a customized model that will offer efficient
solutions for educators, instructors, and individuals engaged in text-based
evaluations. A reliable and efficient tool for generating questions and answers
can free up valuable time and resources, thus streamlining their evaluation
processes.}

</details>


### [18] [Improving Low-Resource Translation with Dictionary-Guided Fine-Tuning and RL: A Spanish-to-Wayuunaiki Study](https://arxiv.org/abs/2508.19481)
*Manuel Mosquera,Melissa Robles,Johan Rodriguez,Ruben Manrique*

Main category: cs.CL

TL;DR: 提出了一种结合外部词典工具和强化学习的低资源机器翻译方法，在西班牙语-Wayuunaiki语言对上取得了显著改进


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在低资源语言翻译中面临的预训练数据缺乏和并行数据有限的问题

Method: 将翻译建模为工具增强的决策问题，结合监督指令微调和GRPO强化学习，让模型学会选择性使用双语词典

Result: 在西班牙语-Wayuunaiki测试集上比先前工作提升+3.37 BLEU，比无词典监督基线相对提升18%

Conclusion: 结合外部工具和强化学习的方法在低资源语言翻译中具有巨大潜力

Abstract: Low-resource machine translation remains a significant challenge for large
language models (LLMs), which often lack exposure to these languages during
pretraining and have limited parallel data for fine-tuning. We propose a novel
approach that enhances translation for low-resource languages by integrating an
external dictionary tool and training models end-to-end using reinforcement
learning, in addition to supervised fine-tuning. Focusing on the
Spanish-Wayuunaiki language pair, we frame translation as a tool-augmented
decision-making problem in which the model can selectively consult a bilingual
dictionary during generation. Our method combines supervised instruction tuning
with Guided Reward Policy Optimization (GRPO), enabling the model to learn both
when and how to use the tool effectively. BLEU similarity scores are used as
rewards to guide this learning process. Preliminary results show that our
tool-augmented models achieve up to +3.37 BLEU improvement over previous work,
and a 18% relative gain compared to a supervised baseline without dictionary
access, on the Spanish-Wayuunaiki test set from the AmericasNLP 2025 Shared
Task. We also conduct ablation studies to assess the effects of model
architecture and training strategy, comparing Qwen2.5-0.5B-Instruct with other
models such as LLaMA and a prior NLLB-based system. These findings highlight
the promise of combining LLMs with external tools and the role of reinforcement
learning in improving translation quality in low-resource language settings.

</details>


### [19] [Rule Synergy Analysis using LLMs: State of the Art and Implications](https://arxiv.org/abs/2508.19484)
*Bahar Bateni,Benjamin Pratt,Jim Whitehead*

Main category: cs.CL

TL;DR: LLMs在卡牌游戏协同效应理解方面表现不佳，虽然能识别非协同对，但在检测正负协同效应时存在困难，特别是在时序、游戏状态定义和规则遵循方面存在问题。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在动态环境（如卡牌游戏）中理解和推理复杂规则交互的能力，特别是对卡牌协同效应的识别。

Method: 引入基于游戏Slay the Spire的卡牌协同数据集，对卡牌对的正面、负面或中性交互进行分类，评估LLMs在不同类型协同效应上的表现。

Result: LLMs在识别非协同卡牌对方面表现优异，但在检测正面协同效应时表现一般，检测负面协同效应时表现最差。模型存在时序理解、游戏状态定义和规则遵循等常见错误类型。

Conclusion: 研究结果指出了改进模型在预测规则效果及其交互方面性能的未来研究方向，需要提升LLMs对复杂规则交互的理解能力。

Abstract: Large language models (LLMs) have demonstrated strong performance across a
variety of domains, including logical reasoning, mathematics, and more. In this
paper, we investigate how well LLMs understand and reason about complex rule
interactions in dynamic environments, such as card games. We introduce a
dataset of card synergies from the game Slay the Spire, where pairs of cards
are classified based on their positive, negative, or neutral interactions. Our
evaluation shows that while LLMs excel at identifying non-synergistic pairs,
they struggle with detecting positive and, particularly, negative synergies. We
categorize common error types, including issues with timing, defining game
states, and following game rules. Our findings suggest directions for future
research to improve model performance in predicting the effect of rules and
their interactions.

</details>


### [20] [Blockwise SFT for Diffusion Language Models: Reconciling Bidirectional Attention and Autoregressive Decoding](https://arxiv.org/abs/2508.19529)
*Bowen Sun,Yujun Cai,Ming-Hsuan Yang,Yiwei Wang*

Main category: cs.CL

TL;DR: 提出了Blockwise SFT方法，通过将响应分割成固定大小的块，在每个步骤中选择一个活动块进行随机掩码，冻结前面的标记并完全隐藏未来的标记，使离散扩散语言模型的训练与半自回归推理过程对齐。


<details>
  <summary>Details</summary>
Motivation: 标准监督微调(SFT)与离散扩散语言模型的半自回归推理存在不匹配：训练时在整个响应中随机掩码标记，而推理时按顺序生成固定大小的块，这种不匹配引入了噪声前缀和泄漏后缀，使梯度偏离期望的块状似然。

Method: Blockwise SFT方法将响应分割成固定大小的块，每个步骤选择一个活动块进行随机掩码，冻结所有前面的标记并完全隐藏未来的标记，损失仅计算在活动块上，直接反映块状解码过程。

Result: 在GSM8K、MATH和MetaMathQA上的实验显示，在相同计算或标记预算下，Blockwise SFT相比经典SFT获得了一致的性能提升。块大小一致性研究和消融实验证实改进源于训练-推理对齐而非偶然的掩码效应。

Conclusion: 研究结果强调了在基于扩散的语言模型中，将监督粒度与解码过程匹配的重要性，Blockwise SFT通过精确对齐训练和推理过程，有效提升了模型性能。

Abstract: Discrete diffusion language models have shown strong potential for text
generation, yet standard supervised fine-tuning (SFT) misaligns with their
semi-autoregressive inference: training randomly masks tokens across the entire
response, while inference generates fixed-size blocks sequentially. This
mismatch introduces noisy prefixes and leaky suffixes, biasing gradients away
from the desired blockwise likelihood. We propose Blockwise SFT, which
partitions responses into fixed-size blocks, selects one active block per step
for stochastic masking, freezes all preceding tokens, and fully hides future
ones. Loss is computed only over the active block, directly mirroring the
blockwise decoding process. Experiments on GSM8K, MATH, and MetaMathQA show
consistent gains over classical SFT under equal compute or token budgets. Block
size consistency studies and ablations confirm that improvements stem from
faithful training-inference alignment rather than incidental masking effects.
Our results highlight the importance of matching supervision granularity to the
decoding procedure in diffusion-based language models.

</details>


### [21] [Alignment with Fill-In-the-Middle for Enhancing Code Generation](https://arxiv.org/abs/2508.19532)
*Houxing Ren,Zimu Lu,Weikang Shi,Haotian Hou,Yunqiao Yang,Ke Wang,Aojun Zhou,Junting Pan,Mingjie Zhan,Hongsheng Li*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的代码生成优化方法，通过将代码片段分割成更小的粒度块，从相同测试用例中创建更多样化的DPO训练对，并结合AST分割和课程训练来增强DPO训练效果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在代码生成任务中面临训练数据有限且难以通过准确测试用例验证的问题，现有DPO方法在测试用例生成方面仍存在局限性。

Method: 提出代码片段粒度分割方法，将代码分成更小的块以创建多样化DPO训练对；引入抽象语法树(AST)分割技术和课程训练策略来优化DPO训练过程。

Result: 在多个基准数据集上验证了方法的有效性，包括HumanEval、MBPP、APPS、LiveCodeBench和BigCodeBench，显示出显著的性能提升。

Conclusion: 该方法通过创新的代码分割和训练策略，有效提升了LLM在代码生成任务中的性能，为解决训练数据验证难题提供了新思路。

Abstract: The code generation capabilities of Large Language Models (LLMs) have
advanced applications like tool invocation and problem-solving. However,
improving performance in code-related tasks remains challenging due to limited
training data that is verifiable with accurate test cases. While Direct
Preference Optimization (DPO) has shown promise, existing methods for
generating test cases still face limitations. In this paper, we propose a novel
approach that splits code snippets into smaller, granular blocks, creating more
diverse DPO pairs from the same test cases. Additionally, we introduce the
Abstract Syntax Tree (AST) splitting and curriculum training method to enhance
the DPO training. Our approach demonstrates significant improvements in code
generation tasks, as validated by experiments on benchmark datasets such as
HumanEval (+), MBPP (+), APPS, LiveCodeBench, and BigCodeBench. Code and data
are available at https://github.com/SenseLLM/StructureCoder.

</details>


### [22] [Emotion Transfer with Enhanced Prototype for Unseen Emotion Recognition in Conversation](https://arxiv.org/abs/2508.19533)
*Kun Peng,Cong Cao,Hao Peng,Guanlin Wu,Zhifeng Hao,Lei Jiang,Yanbing Liu,Philip S. Yu*

Main category: cs.CL

TL;DR: 本文提出了未见情感识别对话任务(UERC)和原型情感迁移框架ProEmoTrans，通过LLM增强描述、参数无关编码机制和改进的注意力维特比解码方法，在三个数据集上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 当前情感识别对话研究基于封闭域假设，但心理学中情感分类缺乏明确共识，模型难以识别真实应用中的未见情感，需要解决这一挑战。

Method: 提出ProEmoTrans框架：1）LLM增强描述方法解决隐含表达问题；2）参数无关机制处理长对话编码；3）改进的注意力维特比解码方法迁移情感转移模式。

Result: 在三个数据集上的广泛实验表明，该方法作为这一新领域的强基线，为初步探索提供了有效解决方案。

Conclusion: 提出的UERC任务和ProEmoTrans框架成功解决了未见情感识别问题，为开放域情感识别对话研究奠定了基础。

Abstract: Current Emotion Recognition in Conversation (ERC) research follows a
closed-domain assumption. However, there is no clear consensus on emotion
classification in psychology, which presents a challenge for models when it
comes to recognizing previously unseen emotions in real-world applications. To
bridge this gap, we introduce the Unseen Emotion Recognition in Conversation
(UERC) task for the first time and propose ProEmoTrans, a solid prototype-based
emotion transfer framework. This prototype-based approach shows promise but
still faces key challenges: First, implicit expressions complicate emotion
definition, which we address by proposing an LLM-enhanced description approach.
Second, utterance encoding in long conversations is difficult, which we tackle
with a proposed parameter-free mechanism for efficient encoding and overfitting
prevention. Finally, the Markovian flow nature of emotions is hard to transfer,
which we address with an improved Attention Viterbi Decoding (AVD) method to
transfer seen emotion transitions to unseen emotions. Extensive experiments on
three datasets show that our method serves as a strong baseline for preliminary
exploration in this new area.

</details>


### [23] [Language Models Identify Ambiguities and Exploit Loopholes](https://arxiv.org/abs/2508.19546)
*Jio Choi,Mohit Bansal,Elias Stengel-Eskin*

Main category: cs.CL

TL;DR: 这篇论文研究了大语言模型对法律空子的响应，发现强大的开源和闭源模型都能识别模糊性并利用空子来优先满足自身目标，构成AI安全风险。


<details>
  <summary>Details</summary>
Motivation: 通过研究LLM对法律空子的响应，可以探索模型在模糊性和语用推理方面的能力，同时这也是一种新的对齐问题，模型可能利用模糊性来优先满足自身目标而非用户目标。

Method: 设计了多种场景，包括标量含义推济、结构模糊性和权力动态，让LLM在目标与模糊用户指令冲突的情况下做出选择，并测量各模型利用空子来满足自身目标的能力。

Result: 发现闭源模型和更强的开源模型都能识别模糊性并利用空子，这些模型在利用空子时会明确识别并推理模糊性和目标冲突问题。

Conclusion: 模型利用法律空子的能力构成了潜在的AI安全风险，需要重视这种对齐问题，确保模型在遇到模糊指令时能够优先满足用户的目标而非自身目标。

Abstract: Studying the responses of large language models (LLMs) to loopholes presents
a two-fold opportunity. First, it affords us a lens through which to examine
ambiguity and pragmatics in LLMs, since exploiting a loophole requires
identifying ambiguity and performing sophisticated pragmatic reasoning. Second,
loopholes pose an interesting and novel alignment problem where the model is
presented with conflicting goals and can exploit ambiguities to its own
advantage. To address these questions, we design scenarios where LLMs are given
a goal and an ambiguous user instruction in conflict with the goal, with
scenarios covering scalar implicature, structural ambiguities, and power
dynamics. We then measure different models' abilities to exploit loopholes to
satisfy their given goals as opposed to the goals of the user. We find that
both closed-source and stronger open-source models can identify ambiguities and
exploit their resulting loopholes, presenting a potential AI safety risk. Our
analysis indicates that models which exploit loopholes explicitly identify and
reason about both ambiguity and conflicting goals.

</details>


### [24] [Towards a Holistic and Automated Evaluation Framework for Multi-Level Comprehension of LLMs in Book-Length Contexts](https://arxiv.org/abs/2508.19578)
*Jiaqi Deng,Yuho Lee,Nicole Hee-Yeon Kim,Hyangsuk Min,Taewon Yun,Minjeong Ban,Kim Yul,Hwanjun Song*

Main category: cs.CL

TL;DR: HAMLET是一个自动化框架，用于评估大语言模型的长文本理解能力，通过三层关键事实层次结构和查询聚焦摘要来测试模型的信息回忆和忠实表示能力。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏系统性的方法来评估LLMs在长文本理解方面的能力，特别是在细粒度信息提取和位置敏感性方面。

Method: 将源文本构建为根、分支、叶三层关键事实层次结构，使用查询聚焦摘要技术自动评估模型在各层次的信息回忆和表示准确性，并通过人工研究验证自动化评估的可靠性。

Result: 自动化评估与专家人工判断达到90%以上一致性，成本降低25倍；发现LLMs在叶级细粒度理解上表现较差，对位置效应敏感，分析性查询比叙述性查询更具挑战性，开源与专有模型之间存在性能差距。

Conclusion: HAMLET提供了一个可靠且高效的自动化评估框架，揭示了LLMs在长文本理解方面的局限性，特别是在细粒度信息处理和位置敏感性方面，为模型改进提供了重要见解。

Abstract: We introduce HAMLET, a holistic and automated framework for evaluating the
long-context comprehension of large language models (LLMs). HAMLET structures
source texts into a three-level key-fact hierarchy at root-, branch-, and
leaf-levels, and employs query-focused summarization to evaluate how well
models recall and faithfully represent information at each level. To validate
the reliability of our fully automated pipeline, we conduct a systematic human
study, showing that our automatic evaluation achieves over 90% agreement with
expert human judgments, while reducing the cost by up to 25 times. HAMLET
reveals that LLMs struggle with fine-grained comprehension, especially at the
leaf level, and are sensitive to positional effects like the
lost-in-the-middle. Analytical queries pose greater challenges than narrative
ones, and consistent performance gaps emerge between open-source and
proprietary models, as well as across model scales. Our code and dataset are
publicly available at https://github.com/DISL-Lab/HAMLET.

</details>


### [25] [ArgCMV: An Argument Summarization Benchmark for the LLM-era](https://arxiv.org/abs/2508.19580)
*Omkar Gurjar,Agam Goyal,Eshwar Chandrasekharan*

Main category: cs.CL

TL;DR: 本文指出了ArgKP21数据集的主要局限性，创建了新的ArgCMV数据集（包含12K个真实在线辩论论点），展示了现有方法在新数据集上的表现不佳，为下一代LLM驱动的摘要研究奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 现有关键点提取方法主要在ArgKP21数据集上评估，但该数据集不能很好地代表真实人类对话，需要更具代表性的新基准数据集。

Method: 使用最先进的大型语言模型(LLMs)构建ArgCMV数据集，包含约12K个来自真实在线人类辩论的论点，涵盖3K多个主题，具有更长、共指论证、更多主观话语单元等更高复杂性特征。

Result: ArgCMV数据集相比ArgKP21展现出更高的复杂性，现有方法在该数据集上适应性不佳，通过实验提供了广泛的基准结果。

Conclusion: 这项工作为长上下文在线讨论引入了新颖的关键点提取数据集，为下一代LLM驱动的摘要研究设定了舞台。

Abstract: Key point extraction is an important task in argument summarization which
involves extracting high-level short summaries from arguments. Existing
approaches for KP extraction have been mostly evaluated on the popular ArgKP21
dataset. In this paper, we highlight some of the major limitations of the
ArgKP21 dataset and demonstrate the need for new benchmarks that are more
representative of actual human conversations. Using SoTA large language models
(LLMs), we curate a new argument key point extraction dataset called ArgCMV
comprising of around 12K arguments from actual online human debates spread
across over 3K topics. Our dataset exhibits higher complexity such as longer,
co-referencing arguments, higher presence of subjective discourse units, and a
larger range of topics over ArgKP21. We show that existing methods do not adapt
well to ArgCMV and provide extensive benchmark results by experimenting with
existing baselines and latest open source models. This work introduces a novel
KP extraction dataset for long-context online discussions, setting the stage
for the next generation of LLM-driven summarization research.

</details>


### [26] [Towards stable AI systems for Evaluating Arabic Pronunciations](https://arxiv.org/abs/2508.19587)
*Hadi Zaatiti,Hatem Hajri,Osama Abdullah,Nader Masmoudi*

Main category: cs.CL

TL;DR: 现代阿拉伯语音识别系统在单独字母识别任务上表现差强，仅35%准确率。经过轻量级神经网络和对抗训练，性能提升到65%并增强稳健性。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语单独字母识别对语言学习、语音治疗和语音研究至关重要，但缺乏协合口型线索和词汇上下文，加上阿拉伯语的哼音等特殊音素，使得识别极其困难。

Method: 构建了一个多样化的带音符的阿拉伯语单字母语料库，使用wav2vec 2.0模型进行基线测试，然后训练轻量级神经网络来处理wav2vec嵌入。采用对抗训练来提高模型对噪声的耐受性。

Result: wav2vec 2.0模型在单字母识别任务上仅达35%准确率。轻量级神经网络将性能提升到65%，但小幅度噪声会使准确率降至32%。经过对抗训练后，噪声下的性能下降仅为9%，同时保持了清洁语音的准确性。

Conclusion: 本研究显示了阿拉伯语单字母识别的挑战性，并提出了通过轻量级模型和对抗训练来提高性能和稳健性的有效方法。这些方法可扩展到词语和句子级别的识别任务中。

Abstract: Modern Arabic ASR systems such as wav2vec 2.0 excel at word- and
sentence-level transcription, yet struggle to classify isolated letters. In
this study, we show that this phoneme-level task, crucial for language
learning, speech therapy, and phonetic research, is challenging because
isolated letters lack co-articulatory cues, provide no lexical context, and
last only a few hundred milliseconds. Recogniser systems must therefore rely
solely on variable acoustic cues, a difficulty heightened by Arabic's emphatic
(pharyngealized) consonants and other sounds with no close analogues in many
languages. This study introduces a diverse, diacritised corpus of isolated
Arabic letters and demonstrates that state-of-the-art wav2vec 2.0 models
achieve only 35% accuracy on it. Training a lightweight neural network on
wav2vec embeddings raises performance to 65%. However, adding a small amplitude
perturbation (epsilon = 0.05) cuts accuracy to 32%. To restore robustness, we
apply adversarial training, limiting the noisy-speech drop to 9% while
preserving clean-speech accuracy. We detail the corpus, training pipeline, and
evaluation protocol, and release, on demand, data and code for reproducibility.
Finally, we outline future work extending these methods to word- and
sentence-level frameworks, where precise letter pronunciation remains critical.

</details>


### [27] [Understanding and Leveraging the Expert Specialization of Context Faithfulness in Mixture-of-Experts LLMs](https://arxiv.org/abs/2508.19594)
*Jun Bai,Minghao Tong,Yang Liu,Zixia Jia,Zilong Zheng*

Main category: cs.CL

TL;DR: 提出了Router Lens方法识别上下文忠实专家，并开发了CEFT轻量级优化方法，通过选择性微调这些专家来提升模型上下文忠实度，效果媲美全微调但更高效。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在上下文依赖场景中经常无法将输出基于给定上下文，导致无关响应。受混合专家架构中专家专业化的启发，研究是否存在专门处理上下文利用的专家。

Method: 提出Router Lens方法准确识别上下文忠实专家，分析发现这些专家会逐步放大对相关上下文信息的注意力。基于此开发Context-faithful Expert Fine-Tuning (CEFT)方法，选择性微调这些专家。

Result: 在广泛基准测试和模型上的实验表明，CEFT匹配或超越全微调性能，同时显著更高效。

Conclusion: 通过识别和选择性优化上下文忠实专家，可以有效提升模型上下文忠实度，提供了一条高效的优化路径。

Abstract: Context faithfulness is essential for reliable reasoning in context-dependent
scenarios. However, large language models often struggle to ground their
outputs in the provided context, resulting in irrelevant responses. Inspired by
the emergent expert specialization observed in mixture-of-experts
architectures, this work investigates whether certain experts exhibit
specialization in context utilization, offering a potential pathway toward
targeted optimization for improved context faithfulness. To explore this, we
propose Router Lens, a method that accurately identifies context-faithful
experts. Our analysis reveals that these experts progressively amplify
attention to relevant contextual information, thereby enhancing context
grounding. Building on this insight, we introduce Context-faithful Expert
Fine-Tuning (CEFT), a lightweight optimization approach that selectively
fine-tunes context-faithful experts. Experiments across a wide range of
benchmarks and models demonstrate that CEFT matches or surpasses the
performance of full fine-tuning while being significantly more efficient.

</details>


### [28] [LFD: Layer Fused Decoding to Exploit External Knowledge in Retrieval-Augmented Generation](https://arxiv.org/abs/2508.19614)
*Yang Sun,Lixin Zou,Dan Luo,Zhiyong Xie,Long Zhang,Liming Dong,Yunwei Zhao,Xixun Lin,Yanxiong Lu,Chenliang Li*

Main category: cs.CL

TL;DR: 本文发现向检索文档注入噪声可提升RAG性能，揭示了LLM不同层级的职能分工：浅层处理局部上下文，中层整合外部知识，深层依赖内部参数知识。基于此提出了层融合解码(LFD)方法，通过结合中层和最终层表示来更好利用外部知识。


<details>
  <summary>Details</summary>
Motivation: 虽然向检索文档注入噪声看似违反直觉，但实证表明这种方法能促进LLM更好地利用外部知识。这一现象为分析LLM如何整合外部知识提供了独特机会，促使研究者探索模型内部层级的功能分工。

Method: 提出层融合解码(LFD)策略：1）通过内部知识评分(IKS)准则确定最优中间层；2）将该中间层的表示与最终层解码输出直接结合；3）IKS选择后半层中得分最低的层作为融合层。

Result: 在多个基准测试上的实验结果表明，LFD能够以最小成本帮助RAG系统更有效地利用检索到的上下文知识，显著提升了生成质量。

Conclusion: 研究揭示了LLM内部层级的功能专业化，提出的LFD方法为RAG系统提供了一种简单有效的解码策略，能够更好地利用外部事实知识，同时为理解LLM知识整合机制提供了新视角。

Abstract: Retrieval-augmented generation (RAG) incorporates external knowledge into
large language models (LLMs), improving their adaptability to downstream tasks
and enabling information updates. Surprisingly, recent empirical evidence
demonstrates that injecting noise into retrieved relevant documents
paradoxically facilitates exploitation of external knowledge and improves
generation quality. Although counterintuitive and challenging to apply in
practice, this phenomenon enables granular control and rigorous analysis of how
LLMs integrate external knowledge. Therefore, in this paper, we intervene on
noise injection and establish a layer-specific functional demarcation within
the LLM: shallow layers specialize in local context modeling, intermediate
layers focus on integrating long-range external factual knowledge, and deeper
layers primarily rely on parametric internal knowledge. Building on this
insight, we propose Layer Fused Decoding (LFD), a simple decoding strategy that
directly combines representations from an intermediate layer with final-layer
decoding outputs to fully exploit the external factual knowledge. To identify
the optimal intermediate layer, we introduce an internal knowledge score (IKS)
criterion that selects the layer with the lowest IKS value in the latter half
of layers. Experimental results across multiple benchmarks demonstrate that LFD
helps RAG systems more effectively surface retrieved context knowledge with
minimal cost.

</details>


### [29] [A Symbolic Adversarial Learning Framework for Evolving Fake News Generation and Detection](https://arxiv.org/abs/2508.19633)
*Chong Tian,Qirong Ho,Xiuying Chen*

Main category: cs.CL

TL;DR: SALF框架通过符号对抗学习优化过程，使用生成和检测代理进行对抗训练，在自然语言层面模拟反向传播和梯度下降，有效生成复杂假新闻并提升检测性能


<details>
  <summary>Details</summary>
Motivation: 随着LLM快速发展，假新闻自动生成能力增强，传统检测方法难以应对动态演变的虚假信息，需要更鲁棒和自适应的检测系统

Method: 提出符号对抗学习框架(SALF)，采用生成代理制作欺骗性叙述，检测代理通过结构化辩论识别逻辑和事实缺陷，在自然语言表示层面模拟神经网络更新过程

Result: 在多语言基准数据集上，SALF生成的假新闻使最先进检测性能下降53.4%(中文)和34.2%(英文)，同时提升检测器对精炼内容的检测能力达7.7%

Conclusion: SALF框架为构建更鲁棒、适应性更强的假新闻检测系统提供了新思路，值得进一步探索

Abstract: Rapid LLM advancements heighten fake news risks by enabling the automatic
generation of increasingly sophisticated misinformation. Previous detection
methods, including fine-tuned small models or LLM-based detectors, often
struggle with its dynamically evolving nature. In this work, we propose a novel
framework called the Symbolic Adversarial Learning Framework (SALF), which
implements an adversarial training paradigm by an agent symbolic learning
optimization process, rather than relying on numerical updates. SALF introduces
a paradigm where the generation agent crafts deceptive narratives, and the
detection agent uses structured debates to identify logical and factual flaws
for detection, and they iteratively refine themselves through such adversarial
interactions. Unlike traditional neural updates, we represent agents using
agent symbolic learning, where learnable weights are defined by agent prompts,
and simulate back-propagation and gradient descent by operating on natural
language representations of weights, loss, and gradients. Experiments on two
multilingual benchmark datasets demonstrate SALF's effectiveness, showing it
generates sophisticated fake news that degrades state-of-the-art detection
performance by up to 53.4% in Chinese and 34.2% in English on average. SALF
also refines detectors, improving detection of refined content by up to 7.7%.
We hope our work inspires further exploration into more robust, adaptable fake
news detection systems.

</details>


### [30] [Automatic integration of SystemC in the FMI standard for Software-defined Vehicle design](https://arxiv.org/abs/2508.19665)
*Giovanni Pollo,Andrei Mihai Albu,Alessio Burrello,Daniele Jahier Pagliari,Cristian Tesconi,Loris Panaro,Dario Soldi,Fabio Autieri,Sara Vinco*

Main category: cs.CL

TL;DR: 通过自动包装SystemC模型为FMI标准格式，解决汽车领域协同模拟中的互操作性和IP保护问题


<details>
  <summary>Details</summary>
Motivation: 汽车行业快速发展需要健壮的协同模拟方法，但专有模拟平台和标准接叧缺失导致协作、扩展性和IP保护困难

Method: 提出一种自动包装SystemC模型的方法，利用功能模拟接叧(FMI)标准，结合SystemC的模型精度和FMI的互操作性优势

Result: 在真实案例中验证了方法的有效性，能够处理复杂设计，实现嵌入式组件的安全、可穿越集成

Conclusion: 该方法有效解决了汽车协同模拟中的互操作性和IP保护挑战，为早期验证和质量提升提供了可靠解决方案

Abstract: The recent advancements of the automotive sector demand robust co-simulation
methodologies that enable early validation and seamless integration across
hardware and software domains. However, the lack of standardized interfaces and
the dominance of proprietary simulation platforms pose significant challenges
to collaboration, scalability, and IP protection. To address these limitations,
this paper presents an approach for automatically wrapping SystemC models by
using the Functional Mock-up Interface (FMI) standard. This method combines the
modeling accuracy and fast time-to-market of SystemC with the interoperability
and encapsulation benefits of FMI, enabling secure and portable integration of
embedded components into co-simulation workflows. We validate the proposed
methodology on real-world case studies, demonstrating its effectiveness with
complex designs.

</details>


### [31] [Survey of Specialized Large Language Model](https://arxiv.org/abs/2508.19667)
*Chenghan Yang,Ruiyu Zhao,Yang Liu,Ling Jiang*

Main category: cs.CL

TL;DR: 本调查系统分析了专业大语言模型从简单领域适应到原生架构设计的演进，重点考察了医疗、金融、法律和技术领域的应用，揭示了专业模型在领域特定基准上的性能优势。


<details>
  <summary>Details</summary>
Motivation: 随着专业大语言模型的快速发展，从简单的领域适应转向复杂的原生架构设计，需要系统性地分析这一演进过程及其在不同专业领域的应用价值。

Method: 通过系统调查方法，分析医疗、金融、法律和技术等领域的专业LLM发展，重点关注领域原生设计、参数效率优化和多模态集成等技术突破。

Result: 研究发现专业LLM通过领域原生架构、稀疏计算、量化等技术有效解决了通用LLM在专业应用中的局限性，在领域特定基准上表现出持续的性能提升。

Conclusion: 专业LLM的发展代表了AI开发的新范式，特别是在电子商务等领域具有重要的应用潜力和填补空白的意义。

Abstract: The rapid evolution of specialized large language models (LLMs) has
transitioned from simple domain adaptation to sophisticated native
architectures, marking a paradigm shift in AI development. This survey
systematically examines this progression across healthcare, finance, legal, and
technical domains. Besides the wide use of specialized LLMs, technical
breakthrough such as the emergence of domain-native designs beyond fine-tuning,
growing emphasis on parameter efficiency through sparse computation and
quantization, increasing integration of multimodal capabilities and so on are
applied to recent LLM agent. Our analysis reveals how these innovations address
fundamental limitations of general-purpose LLMs in professional applications,
with specialized models consistently performance gains on domain-specific
benchmarks. The survey further highlights the implications for E-Commerce field
to fill gaps in the field.

</details>


### [32] [Building Task Bots with Self-learning for Enhanced Adaptability, Extensibility, and Factuality](https://arxiv.org/abs/2508.19689)
*Xiaoying Zhang*

Main category: cs.CL

TL;DR: 论文探讨了开发自适应、可扩展且准确的任务对话机器人的挑战，重点关注无需人工干预的自主学习方法


<details>
  <summary>Details</summary>
Motivation: 开发能够自主学习和适应不断变化环境的对话机器人是对话研究领域的重大挑战，需要最小化或零人工干预

Method: 研究创新技术，使机器人能够在不断变化的环境中自主学习和适应

Result: 论文分析了创建此类机器人的障碍和潜在解决方案

Conclusion: 自主适应和学习的对话机器人技术具有重要研究价值和发展潜力

Abstract: Developing adaptable, extensible, and accurate task bots with minimal or zero
human intervention is a significant challenge in dialog research. This thesis
examines the obstacles and potential solutions for creating such bots, focusing
on innovative techniques that enable bots to learn and adapt autonomously in
constantly changing environments.

</details>


### [33] [Continuously Steering LLMs Sensitivity to Contextual Knowledge with Proxy Models](https://arxiv.org/abs/2508.19720)
*Yilin Wang,Heng Wang,Yuyang Bai,Minnan Luo*

Main category: cs.CL

TL;DR: 提出了CSKS框架，通过调节两个小代理模型的输出分布差异来连续控制大语言模型对上下文知识的敏感度，无需修改模型权重


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型中参数知识与上下文知识冲突的问题，现有方法效率低、不适用于黑盒模型或无法连续调整敏感度

Method: 训练两个小型代理模型，利用它们输出分布的差异来调整大语言模型的原始分布，实现连续敏感度控制

Result: 实验表明该框架能精确连续控制LLM对上下文知识的敏感度，既可增强也可降低敏感度，灵活优先选择参数或上下文知识

Conclusion: CSKS提供了一种轻量级、高效的方法来解决知识冲突问题，适用于各种规模的黑盒和白盒模型

Abstract: In Large Language Models (LLMs) generation, there exist knowledge conflicts
and scenarios where parametric knowledge contradicts knowledge provided in the
context. Previous works studied tuning, decoding algorithms, or locating and
editing context-aware neurons to adapt LLMs to be faithful to new contextual
knowledge. However, they are usually inefficient or ineffective for large
models, not workable for black-box models, or unable to continuously adjust
LLMs' sensitivity to the knowledge provided in the context. To mitigate these
problems, we propose CSKS (Continuously Steering Knowledge Sensitivity), a
simple framework that can steer LLMs' sensitivity to contextual knowledge
continuously at a lightweight cost. Specifically, we tune two small LMs (i.e.
proxy models) and use the difference in their output distributions to shift the
original distribution of an LLM without modifying the LLM weights. In the
evaluation process, we not only design synthetic data and fine-grained metrics
to measure models' sensitivity to contextual knowledge but also use a real
conflict dataset to validate CSKS's practical efficacy. Extensive experiments
demonstrate that our framework achieves continuous and precise control over
LLMs' sensitivity to contextual knowledge, enabling both increased sensitivity
and reduced sensitivity, thereby allowing LLMs to prioritize either contextual
or parametric knowledge as needed flexibly. Our data and code are available at
https://github.com/OliveJuiceLin/CSKS.

</details>


### [34] [CAMÕES: A Comprehensive Automatic Speech Recognition Benchmark for European Portuguese](https://arxiv.org/abs/2508.19721)
*Carlos Carvalho,Francisco Teixeira,Catarina Botelho,Anna Pompili,Rubén Solera-Ureña,Sérgio Paulo,Mariana Julião,Thomas Rolland,John Mendonça,Diogo Pereira,Isabel Trancoso,Alberto Abad*

Main category: cs.CL

TL;DR: CAMÕES是首个针对欧洲葡萄牙语的开源ASR框架，包含46小时测试基准和最先进模型，相比零样本基础模型WER相对提升35%以上


<details>
  <summary>Details</summary>
Motivation: 现有葡萄牙语ASR资源主要针对巴西葡萄牙语，欧洲葡萄牙语和其他变体研究不足，需要填补这一空白

Method: 构建包含46小时多领域测试数据的评估基准，使用425小时精选数据进行微调和训练，评估基础模型的零样本和微调性能，以及从头训练的E-Branchformer模型

Result: 微调后的基础模型与E-Branchformer性能相当，最佳模型相比最强零样本基础模型WER相对提升35%以上，建立了新的state-of-the-art

Conclusion: CAMÕES框架成功填补了欧洲葡萄牙语ASR研究的空白，为欧洲葡萄牙语和其他变体提供了首个开源基准和最先进模型

Abstract: Existing resources for Automatic Speech Recognition in Portuguese are mostly
focused on Brazilian Portuguese, leaving European Portuguese (EP) and other
varieties under-explored. To bridge this gap, we introduce CAM\~OES, the first
open framework for EP and other Portuguese varieties. It consists of (1) a
comprehensive evaluation benchmark, including 46h of EP test data spanning
multiple domains; and (2) a collection of state-of-the-art models. For the
latter, we consider multiple foundation models, evaluating their zero-shot and
fine-tuned performances, as well as E-Branchformer models trained from scratch.
A curated set of 425h of EP was used for both fine-tuning and training. Our
results show comparable performance for EP between fine-tuned foundation models
and the E-Branchformer. Furthermore, the best-performing models achieve
relative improvements above 35% WER, compared to the strongest zero-shot
foundation model, establishing a new state-of-the-art for EP and other
varieties.

</details>


### [35] [NLKI: A lightweight Natural Language Knowledge Integration Framework for Improving Small VLMs in Commonsense VQA Tasks](https://arxiv.org/abs/2508.19724)
*Aritra Dutta,Swapnanil Mukherjee,Deepanway Ghosal,Somak Aditya*

Main category: cs.CL

TL;DR: NLKI框架通过检索自然语言事实和LLM生成解释来增强小型视觉语言模型的常识推理能力，在多个数据集上提升准确率7%，配合噪声鲁棒训练进一步提升性能。


<details>
  <summary>Details</summary>
Motivation: 小型视觉语言模型在常识视觉问答中因缺乏外部知识而表现不佳，需要有效整合常识知识来提升性能。

Method: 提出端到端框架NLKI：(1)使用微调ColBERTv2检索自然语言事实；(2)用LLM生成自然语言解释；(3)将两种信号输入sVLMs，并采用噪声鲁棒损失进行微调。

Result: 在CRIC、AOKVQA和e-SNLI-VE数据集上，FLAVA等模型准确率提升达7%，匹配或超越中等规模VLMs，噪声鲁棒训练额外提升2.5-5.5%准确率。

Conclusion: LLM生成的常识知识优于知识库检索，噪声感知训练稳定了小模型的外部知识增强，使2.5亿参数模型实现高效的常识推理。

Abstract: Commonsense visual-question answering often hinges on knowledge that is
missing from the image or the question. Small vision-language models (sVLMs)
such as ViLT, VisualBERT and FLAVA therefore lag behind their larger generative
counterparts. To study the effect of careful commonsense knowledge integration
on sVLMs, we present an end-to-end framework (NLKI) that (i) retrieves natural
language facts, (ii) prompts an LLM to craft natural language explanations, and
(iii) feeds both signals to sVLMs respectively across two commonsense VQA
datasets (CRIC, AOKVQA) and a visual-entailment dataset (e-SNLI-VE). Facts
retrieved using a fine-tuned ColBERTv2 and an object information-enriched
prompt yield explanations that largely cut down hallucinations, while lifting
the end-to-end answer accuracy by up to 7% (across 3 datasets), making FLAVA
and other models in NLKI match or exceed medium-sized VLMs such as Qwen-2 VL-2B
and SmolVLM-2.5B. As these benchmarks contain 10-25% label noise, additional
finetuning using noise-robust losses (such as symmetric cross entropy and
generalised cross entropy) adds another 2.5% in CRIC, and 5.5% in AOKVQA. Our
findings expose when LLM-based commonsense knowledge beats retrieval from
commonsense knowledge bases, how noise-aware training stabilises small models
in the context of external knowledge augmentation, and why parameter-efficient
commonsense reasoning is now within reach for 250M models.

</details>


### [36] [Spotlight Attention: Towards Efficient LLM Generation via Non-linear Hashing-based KV Cache Retrieval](https://arxiv.org/abs/2508.19740)
*Wenhao Li,Yuxin Zhang,Gen Luo,Haiyuan Wan,Ziyang Gong,Fei Chao,Rongrong Ji*

Main category: cs.CL

TL;DR: Spotlight Attention是一种新颖的非线性哈希方法，通过优化查询和键的嵌入分布来显著提高KV缓存检索效率，相比传统线性哈希将哈希码长度缩短至少5倍，并在单个A100 GPU上实现512K tokens的100μs内哈希检索。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型推理过程中KV缓存负担重，现有随机线性哈希方法因查询和键在窄锥形分布中的正交性而效率低下，需要更有效的关键token识别方法。

Method: 提出Spotlight Attention，使用非线性哈希函数优化查询和键的嵌入分布；开发基于Bradley-Terry排序损失的轻量级训练框架，可在16GB GPU内存上8小时内完成优化；实现专用CUDA内核利用位运算优势。

Result: 大幅提高检索精度，哈希码长度比传统线性哈希缩短至少5倍；在单个A100 GPU上实现512K tokens的100μs内哈希检索；端到端吞吐量比原始解码提高达3倍。

Conclusion: Spotlight Attention通过非线性哈希和高效训练框架，成功解决了LLM推理中KV缓存效率问题，显著提升了检索性能和计算效率。

Abstract: Reducing the key-value (KV) cache burden in Large Language Models (LLMs)
significantly accelerates inference. Dynamically selecting critical KV caches
during decoding helps maintain performance. Existing methods use random linear
hashing to identify important tokens, but this approach is inefficient due to
the orthogonal distribution of queries and keys within two narrow cones in
LLMs. We introduce Spotlight Attention, a novel method that employs non-linear
hashing functions to optimize the embedding distribution of queries and keys,
enhancing coding efficiency and robustness. We also developed a lightweight,
stable training framework using a Bradley-Terry ranking-based loss, enabling
optimization of the non-linear hashing module on GPUs with 16GB memory in 8
hours. Experimental results show that Spotlight Attention drastically improves
retrieval precision while shortening the length of the hash code at least
5$\times$ compared to traditional linear hashing. Finally, we exploit the
computational advantages of bitwise operations by implementing specialized CUDA
kernels, achieving hashing retrieval for 512K tokens in under 100$\mu$s on a
single A100 GPU, with end-to-end throughput up to 3$\times$ higher than vanilla
decoding.

</details>


### [37] [Uncovering the Bigger Picture: Comprehensive Event Understanding Via Diverse News Retrieval](https://arxiv.org/abs/2508.19758)
*Yixuan Tang,Yuanyuan Shi,Yiqun Sun,Anthony Kum Hoe Tung*

Main category: cs.CL

TL;DR: NEWSCOPE是一个两阶段新闻检索框架，通过句子级语义变化建模来提升事件报道的多样性，在保持相关性的同时显著提高检索结果的多样性。


<details>
  <summary>Details</summary>
Motivation: 现有新闻检索系统主要关注文本相关性，导致结果冗余且视角有限，无法提供多样化的观点来全面理解真实事件。

Method: 采用两阶段框架：第一阶段使用密集检索获取主题相关内容，第二阶段通过句子级聚类和多样性感知重排序来发现互补信息。

Result: NEWSCOPE在多个基准测试中 consistently 优于强基线，在不损害相关性的情况下实现了显著更高的多样性。

Conclusion: 细粒度、可解释的建模能有效减少冗余并促进全面的事件理解，提出的三个可解释指标和两个基准数据集为多样性评估提供了新工具。

Abstract: Access to diverse perspectives is essential for understanding real-world
events, yet most news retrieval systems prioritize textual relevance, leading
to redundant results and limited viewpoint exposure. We propose NEWSCOPE, a
two-stage framework for diverse news retrieval that enhances event coverage by
explicitly modeling semantic variation at the sentence level. The first stage
retrieves topically relevant content using dense retrieval, while the second
stage applies sentence-level clustering and diversity-aware re-ranking to
surface complementary information. To evaluate retrieval diversity, we
introduce three interpretable metrics, namely Average Pairwise Distance,
Positive Cluster Coverage, and Information Density Ratio, and construct two
paragraph-level benchmarks: LocalNews and DSGlobal. Experiments show that
NEWSCOPE consistently outperforms strong baselines, achieving significantly
higher diversity without compromising relevance. Our results demonstrate the
effectiveness of fine-grained, interpretable modeling in mitigating redundancy
and promoting comprehensive event understanding. The data and code are
available at https://github.com/tangyixuan/NEWSCOPE.

</details>


### [38] [Principled Personas: Defining and Measuring the Intended Effects of Persona Prompting on Task Performance](https://arxiv.org/abs/2508.19764)
*Pedro Henrique Luz de Araujo,Paul Röttger,Dirk Hovy,Benjamin Roth*

Main category: cs.CL

TL;DR: 专家角色提示对LLM性能影响分析：通常带来正面或非显著改进，但对无关角色细节高度敏感（性能下降近30%），角色保真度效果不一致，缓解策略仅对最大模型有效


<details>
  <summary>Details</summary>
Motivation: 分析专家角色提示在任务改进中的有效性，探讨何时以及为何角色提示应该提升性能，解决先前研究中结果不一致的问题

Method: 文献分析提炼三个期望标准（专家角色性能优势、对无关属性的鲁棒性、角色属性保真度），在27个任务上评估9个最先进LLM

Result: 专家角色通常带来正面或非显著性能变化；模型对无关角色细节高度敏感（性能下降近30%）；高等教育、专业化和领域相关性有时能提升性能但效果不一致；缓解策略仅对最大最强大模型有效

Conclusion: 需要更谨慎的角色设计和能够反映角色使用预期效果的评估方案

Abstract: Expert persona prompting -- assigning roles such as expert in math to
language models -- is widely used for task improvement. However, prior work
shows mixed results on its effectiveness, and does not consider when and why
personas should improve performance. We analyze the literature on persona
prompting for task improvement and distill three desiderata: 1) performance
advantage of expert personas, 2) robustness to irrelevant persona attributes,
and 3) fidelity to persona attributes. We then evaluate 9 state-of-the-art LLMs
across 27 tasks with respect to these desiderata. We find that expert personas
usually lead to positive or non-significant performance changes. Surprisingly,
models are highly sensitive to irrelevant persona details, with performance
drops of almost 30 percentage points. In terms of fidelity, we find that while
higher education, specialization, and domain-relatedness can boost performance,
their effects are often inconsistent or negligible across tasks. We propose
mitigation strategies to improve robustness -- but find they only work for the
largest, most capable models. Our findings underscore the need for more careful
persona design and for evaluation schemes that reflect the intended effects of
persona usage.

</details>


### [39] [T2R-bench: A Benchmark for Generating Article-Level Reports from Real World Industrial Tables](https://arxiv.org/abs/2508.19813)
*Jie Zhang,Changzai Pan,Kaiwen Wei,Sishi Xiong,Yu Zhao,Xiangyu Li,Jiaxin Peng,Xiaoyan Gu,Jian Yang,Wenhan Chang,Zhenhe Wu,Jiang Zhong,Shuangyong Song,Yongxiang Li,Xuelong Li*

Main category: cs.CL

TL;DR: 提出了表格到报告生成任务(T2R)并构建了双语基准T2R-bench，包含457个真实工业表格，涵盖19个行业领域和4种表格类型。实验表明即使是先进模型如Deepseek-R1在该任务上也只有62.71分，说明仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注表格推理能力，但将表格信息转化为实际报告这一工业应用关键任务仍面临挑战：表格复杂多样导致推理效果不佳，且现有基准无法充分评估该任务的实际应用效果。

Method: 提出表格到报告生成任务，构建包含457个真实工业表格的双语基准T2R-bench，涵盖19个行业领域和4种表格类型，并设计了评估报告生成质量的评价标准。

Result: 在25个常用大语言模型上的实验显示，即使是性能最好的Deepseek-R1模型也只达到62.71分的总体表现，表明当前模型在该任务上仍有较大提升空间。

Conclusion: 表格到报告生成是一个具有挑战性的工业应用任务，现有大语言模型在该任务上的表现仍有待提升，T2R-bench为评估和改进模型性能提供了重要基准。

Abstract: Extensive research has been conducted to explore the capabilities of large
language models (LLMs) in table reasoning. However, the essential task of
transforming tables information into reports remains a significant challenge
for industrial applications. This task is plagued by two critical issues: 1)
the complexity and diversity of tables lead to suboptimal reasoning outcomes;
and 2) existing table benchmarks lack the capacity to adequately assess the
practical application of this task. To fill this gap, we propose the
table-to-report task and construct a bilingual benchmark named T2R-bench, where
the key information flow from the tables to the reports for this task. The
benchmark comprises 457 industrial tables, all derived from real-world
scenarios and encompassing 19 industry domains as well as 4 types of industrial
tables. Furthermore, we propose an evaluation criteria to fairly measure the
quality of report generation. The experiments on 25 widely-used LLMs reveal
that even state-of-the-art models like Deepseek-R1 only achieves performance
with 62.71 overall score, indicating that LLMs still have room for improvement
on T2R-bench. Source code and data will be available after acceptance.

</details>


### [40] [Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize Memories via Reinforcement Learning](https://arxiv.org/abs/2508.19828)
*Sikuan Yan,Xiufeng Yang,Zuchao Huang,Ercong Nie,Zifeng Ding,Zonggen Li,Xiaowen Ma,Hinrich Schütze,Volker Tresp,Yunpu Ma*

Main category: cs.CL

TL;DR: Memory-R1是一个基于强化学习的框架，通过两个专门代理（内存管理器和回答代理）使LLM能够主动管理外部内存，在少量训练数据下实现优于现有基线性能。


<details>
  <summary>Details</summary>
Motivation: 解决LLM因有限上下文窗口而无法进行长程推理的问题，现有内存增强方法多为静态启发式，缺乏学习机制来决定存储、更新或检索内容。

Method: 使用强化学习（PPO和GRPO）微调两个代理：内存管理器学习结构化内存操作（ADD/UPDATE/DELETE/NOOP），回答代理选择相关条目并进行推理生成答案。

Result: 仅用152个问答对和对应时序内存库训练，Memory-R1就超越了现有最强基线，在不同问题类型和LLM骨干网络上表现出强泛化能力。

Conclusion: 该工作展示了RL如何解锁LLM中更具代理性和内存感知的行为，为构建更丰富、更持久的推理系统指明了方向。

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities across
a wide range of NLP tasks, but they remain fundamentally stateless, constrained
by limited context windows that hinder long-horizon reasoning. Recent efforts
to address this limitation often augment LLMs with an external memory bank, yet
most existing pipelines are static and heuristic-driven, lacking any learned
mechanism for deciding what to store, update, or retrieve. We present
Memory-R1, a reinforcement learning (RL) framework that equips LLMs with the
ability to actively manage and utilize external memory through two specialized
agents: a Memory Manager that learns to perform structured memory operations
{ADD, UPDATE, DELETE, NOOP}, and an Answer Agent that selects the most relevant
entries and reasons over them to produce an answer. Both agents are fine-tuned
with outcome-driven RL (PPO and GRPO), enabling adaptive memory management and
use with minimal supervision. With as few as 152 question-answer pairs and a
corresponding temporal memory bank for training, Memory-R1 outperforms the most
competitive existing baseline and demonstrates strong generalization across
diverse question types and LLM backbones. Beyond presenting an effective
approach, this work provides insights into how RL can unlock more agentic,
memory-aware behaviors in LLMs, pointing toward richer, more persistent
reasoning systems.

</details>


### [41] [Benchmarking Hindi LLMs: A New Suite of Datasets and a Comparative Analysis](https://arxiv.org/abs/2508.19831)
*Anusha Kamath,Kanishk Singla,Rakesh Paul,Raviraj Joshi,Utkarsh Vaidya,Sanjay Singh Chauhan,Niranjan Wartikar*

Main category: cs.CL

TL;DR: 一套专门为印地语言模型评估而设计的五个高质量测试数据集，解决了直接翻译英语数据集无法抓取语言文化细节的问题


<details>
  <summary>Details</summary>
Motivation: 因为印地语缺乏高质量的指令微调大语言模型评测标准，直接翻译英语数据集无法满足语言文化细节的需求

Method: 采用了组合方法：从头开始的人工标注加上翻译-验证过程，创建了五个专门数据集(IFEval-Hi, MT-Bench-Hi, GSM8K-Hi, ChatRAG-Hi, BFCL-Hi)

Result: 完成了对支持印地语的开源大语言模型的全面性能测试，提供了详细的对比分析

Conclusion: 该方法论不仅为印地语模型评估提供了可靠标准，还可以复制到其他语言资源稀缺的语言中

Abstract: Evaluating instruction-tuned Large Language Models (LLMs) in Hindi is
challenging due to a lack of high-quality benchmarks, as direct translation of
English datasets fails to capture crucial linguistic and cultural nuances. To
address this, we introduce a suite of five Hindi LLM evaluation datasets:
IFEval-Hi, MT-Bench-Hi, GSM8K-Hi, ChatRAG-Hi, and BFCL-Hi. These were created
using a methodology that combines from-scratch human annotation with a
translate-and-verify process. We leverage this suite to conduct an extensive
benchmarking of open-source LLMs supporting Hindi, providing a detailed
comparative analysis of their current capabilities. Our curation process also
serves as a replicable methodology for developing benchmarks in other
low-resource languages.

</details>


### [42] [Scalable and consistent few-shot classification of survey responses using text embeddings](https://arxiv.org/abs/2508.19836)
*Jonas Timmann Mjaaland,Markus Fleten Kreutzer,Halvor Tyseng,Rebeckah K. Fussell,Gina Passante,N. G. Holmes,Anders Malthe-Sørenssen,Tor Ole B. Odden*

Main category: cs.CL

TL;DR: 提出基于文本嵌入的分类框架，只需少量示例即可进行定性分析，与人类专家编码一致性高达0.74-0.83 Kappa值


<details>
  <summary>Details</summary>
Motivation: 传统定性分析方法耗时且不一致，现有NLP方法需要大量标注数据或破坏定性工作流程

Method: 使用文本嵌入分类框架，每个类别只需少量示例，可微调文本嵌入模型提升性能

Result: 在2899个物理调查响应中，与人类专家编码的Cohen's Kappa达到0.74-0.83，性能随模型微调而提升

Conclusion: 文本嵌入辅助编码可灵活扩展到数千个响应而不牺牲可解释性，为大规模演绎定性分析开辟新途径

Abstract: Qualitative analysis of open-ended survey responses is a commonly-used
research method in the social sciences, but traditional coding approaches are
often time-consuming and prone to inconsistency. Existing solutions from
Natural Language Processing such as supervised classifiers, topic modeling
techniques, and generative large language models have limited applicability in
qualitative analysis, since they demand extensive labeled data, disrupt
established qualitative workflows, and/or yield variable results. In this
paper, we introduce a text embedding-based classification framework that
requires only a handful of examples per category and fits well with standard
qualitative workflows. When benchmarked against human analysis of a conceptual
physics survey consisting of 2899 open-ended responses, our framework achieves
a Cohen's Kappa ranging from 0.74 to 0.83 as compared to expert human coders in
an exhaustive coding scheme. We further show how performance of this framework
improves with fine-tuning of the text embedding model, and how the method can
be used to audit previously-analyzed datasets. These findings demonstrate that
text embedding-assisted coding can flexibly scale to thousands of responses
without sacrificing interpretability, opening avenues for deductive qualitative
analysis at scale.

</details>


### [43] [TokenVerse++: Towards Flexible Multitask Learning with Dynamic Task Activation](https://arxiv.org/abs/2508.19856)
*Shashi Kumar,Srikanth Madikeri,Esaú Villatoro-Tello,Sergio Burdisso,Pradeep Rangappa,Andrés Carofilis,Petr Motlicek,Karthik Pandia,Shankar Venkatesan,Kadri Hacioğlu,Andreas Stolcke*

Main category: cs.CL

TL;DR: TokenVerse++在TokenVerse基础上引入可学习向量机制，支持部分标注数据的训练，解决了多任务框架需要全标注数据的限制，在保持ASR性能的同时提升了多任务处理的实用性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于token的多任务框架（如TokenVerse）要求所有训练语句必须包含所有任务的标签，这限制了利用部分标注数据集的能力，影响了框架的可扩展性。

Method: 在XLSR-Transducer ASR模型的声学嵌入空间中引入可学习向量，实现动态任务激活机制，使模型能够仅使用部分任务标注的语句进行训练。

Result: 成功整合了部分标注数据集（ASR和语言识别任务），整体性能得到提升，在多任务处理上达到或超过TokenVerse的水平。

Conclusion: TokenVerse++提供了一个更实用的多任务替代方案，在不牺牲ASR性能的前提下，解决了部分标注数据利用的问题，具有更好的扩展性。

Abstract: Token-based multitasking frameworks like TokenVerse require all training
utterances to have labels for all tasks, hindering their ability to leverage
partially annotated datasets and scale effectively. We propose TokenVerse++,
which introduces learnable vectors in the acoustic embedding space of the
XLSR-Transducer ASR model for dynamic task activation. This core mechanism
enables training with utterances labeled for only a subset of tasks, a key
advantage over TokenVerse. We demonstrate this by successfully integrating a
dataset with partial labels, specifically for ASR and an additional task,
language identification, improving overall performance. TokenVerse++ achieves
results on par with or exceeding TokenVerse across multiple tasks, establishing
it as a more practical multitask alternative without sacrificing ASR
performance.

</details>


### [44] [Beyond Shallow Heuristics: Leveraging Human Intuition for Curriculum Learning](https://arxiv.org/abs/2508.19873)
*Vanessa Toborek,Sebastian Müller,Tim Selbach,Tamás Horváth,Christian Bauckhage*

Main category: cs.CL

TL;DR: 研究发现人类标注的简单语言数据作为课程学习信号，按从简单到困难的顺序训练语言模型，相比随机顺序和基于能力的策略能有效提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 课程学习通过从易到难的数据训练来提升效果，但如何定义和衡量语言难度仍是一个开放挑战。研究探索人类标注的简单语言是否能作为有效的课程学习信号。

Method: 使用Simple Wikipedia语料库的文章级标签，比较基于标签的课程与基于浅层启发式能力策略。在BERT-tiny模型上进行实验，分析不同课程安排对困惑度的影响。

Result: 单纯添加简单数据无明确收益，但通过课程结构（特别是先引入简单数据）能持续降低困惑度，尤其在简单语言上效果显著。基于能力的课程相比随机顺序无一致增益。

Conclusion: 人类对语言难度的直觉判断可以指导语言模型预训练中的课程学习，基于人工标注的简单语言信号比自动启发式方法更有效。

Abstract: Curriculum learning (CL) aims to improve training by presenting data from
"easy" to "hard", yet defining and measuring linguistic difficulty remains an
open challenge. We investigate whether human-curated simple language can serve
as an effective signal for CL. Using the article-level labels from the Simple
Wikipedia corpus, we compare label-based curricula to competence-based
strategies relying on shallow heuristics. Our experiments with a BERT-tiny
model show that adding simple data alone yields no clear benefit. However,
structuring it via a curriculum -- especially when introduced first --
consistently improves perplexity, particularly on simple language. In contrast,
competence-based curricula lead to no consistent gains over random ordering,
probably because they fail to effectively separate the two classes. Our results
suggest that human intuition about linguistic difficulty can guide CL for
language model pre-training.

</details>


### [45] [AI-Powered Detection of Inappropriate Language in Medical School Curricula](https://arxiv.org/abs/2508.19883)
*Chiman Salavati,Shannon Song,Scott A. Hale,Roberto E. Montenegro,Shiri Dori-Hacohen,Fabricio Murai*

Main category: cs.CL

TL;DR: 本研究评估了小语言模型(SLMs)和预训练大语言模型(LLMs)在检测医学教材中不当语言使用(IUL)的效果，发现SLMs表现优于LLMs，多标签分类器在标注数据上表现最佳，而补充负样本训练可显著提升特定分类器的性能。


<details>
  <summary>Details</summary>
Motivation: 医学教材中过时、排他性或非患者中心化的不当语言会影响临床培训和患者健康，但人工识别成本高昂且不切实际，需要自动化解决方案。

Method: 使用约500份文档、超过12,000页的数据集，评估了四种SLMs方法(通用IUL分类器、子类别二元分类器、多标签分类器、两阶段分层管道)和LLMs的上下文学习方法。

Result: LLama-3 8B和70B即使使用精心设计的提示词，性能也大幅落后于SLMs。多标签分类器在标注数据上表现最好，补充负样本训练可使特定分类器的AUC提升高达25%。

Conclusion: SLMs在检测医学课程中有害语言方面比LLMs更有效，特别是通过补充负样本训练的特定分类器模型表现最佳，为医学教育材料的语言审查提供了实用解决方案。

Abstract: The use of inappropriate language -- such as outdated, exclusionary, or
non-patient-centered terms -- medical instructional materials can significantly
influence clinical training, patient interactions, and health outcomes. Despite
their reputability, many materials developed over past decades contain examples
now considered inappropriate by current medical standards. Given the volume of
curricular content, manually identifying instances of inappropriate use of
language (IUL) and its subcategories for systematic review is prohibitively
costly and impractical. To address this challenge, we conduct a first-in-class
evaluation of small language models (SLMs) fine-tuned on labeled data and
pre-trained LLMs with in-context learning on a dataset containing approximately
500 documents and over 12,000 pages. For SLMs, we consider: (1) a general IUL
classifier, (2) subcategory-specific binary classifiers, (3) a multilabel
classifier, and (4) a two-stage hierarchical pipeline for general IUL detection
followed by multilabel classification. For LLMs, we consider variations of
prompts that include subcategory definitions and/or shots. We found that both
LLama-3 8B and 70B, even with carefully curated shots, are largely outperformed
by SLMs. While the multilabel classifier performs best on annotated data,
supplementing training with unflagged excerpts as negative examples boosts the
specific classifiers' AUC by up to 25%, making them most effective models for
mitigating harmful language in medical curricula.

</details>


### [46] [Bangla-Bayanno: A 52K-Pair Bengali Visual Question Answering Dataset with LLM-Assisted Translation Refinement](https://arxiv.org/abs/2508.19887)
*Mohammed Rakibul Hasan,Rafi Majid,Ahanaf Tahmid*

Main category: cs.CL

TL;DR: 一个高质量的孟加拉语视觉问答数据集，通过多语言LLM辅助翻译精炼流程完成，包含52,650个问答对和4,750+弹图片


<details>
  <summary>Details</summary>
Motivation: 解决孟加拉语这种资源稀缺语言在多模态AI研究中的数据集缺口问题，免除现有数据集的域限性和人工注释错误

Method: 使用多语言大语言模型辅助的翻译精炼流程，分为名词性、数量性和布尔性三种问题类型

Result: 创建了包含52,650个问答对和4,750+弹图片的数据集，免除了多语言源的低质量翻译问题

Conclusion: 提供了孟加拉语最全面的开源高质量VQA基准，推动资源稀缺语言的多模态学习研究和包容性AI系统发展

Abstract: In this paper, we introduce Bangla-Bayanno, an open-ended Visual Question
Answering (VQA) Dataset in Bangla, a widely used, low-resource language in
multimodal AI research. The majority of existing datasets are either manually
annotated with an emphasis on a specific domain, query type, or answer type or
are constrained by niche answer formats. In order to mitigate human-induced
errors and guarantee lucidity, we implemented a multilingual LLM-assisted
translation refinement pipeline. This dataset overcomes the issues of
low-quality translations from multilingual sources. The dataset comprises
52,650 question-answer pairs across 4750+ images. Questions are classified into
three distinct answer types: nominal (short descriptive), quantitative
(numeric), and polar (yes/no). Bangla-Bayanno provides the most comprehensive
open-source, high-quality VQA benchmark in Bangla, aiming to advance research
in low-resource multimodal learning and facilitate the development of more
inclusive AI systems.

</details>


### [47] [Logical Reasoning with Outcome Reward Models for Test-Time Scaling](https://arxiv.org/abs/2508.19903)
*Ramya Keerthy Thatikonda,Wray Buntine,Ehsan Shareghi*

Main category: cs.CL

TL;DR: 本文提出了用于演绎逻辑推理的结果奖励模型(ORMs)，通过CoT和回波生成技术增强训练数据，在多个逻辑推理数据集上提升了LLM性能。


<details>
  <summary>Details</summary>
Motivation: 逻辑推理是评估大语言模型能力的关键基准，但当前在演绎逻辑推理领域的测试时缩放与奖励模型结合方法尚未充分探索。

Method: 使用单样本和多样本的思维链(CoT)生成训练数据，并提出回波生成技术来扩展错误类型覆盖，通过引导模型产生错误推理来获取额外训练数据。

Result: 在FOLIO、JustLogic和ProverQA三个数据集上，基于CoT和回波增强数据训练的ORMs在四种不同LLM上都表现出性能提升。

Conclusion: 回波生成技术能有效扩展ORMs训练数据的错误类型覆盖，结合CoT方法训练的ORMs能显著提升LLM在演绎逻辑推理任务上的表现。

Abstract: Logical reasoning is a critical benchmark for evaluating the capabilities of
large language models (LLMs), as it reflects their ability to derive valid
conclusions from given premises. While the combination of test-time scaling
with dedicated outcome or process reward models has opened up new avenues to
enhance LLMs performance in complex reasoning tasks, this space is
under-explored in deductive logical reasoning. We present a set of Outcome
Reward Models (ORMs) for deductive reasoning. To train the ORMs we mainly
generate data using Chain-of-Thought (CoT) with single and multiple samples.
Additionally, we propose a novel tactic to further expand the type of errors
covered in the training dataset of the ORM. In particular, we propose an echo
generation technique that leverages LLMs' tendency to reflect incorrect
assumptions made in prompts to extract additional training data, covering
previously unexplored error types. While a standard CoT chain may contain
errors likely to be made by the reasoner, the echo strategy deliberately steers
the model toward incorrect reasoning. We show that ORMs trained on CoT and
echo-augmented data demonstrate improved performance on the FOLIO, JustLogic,
and ProverQA datasets across four different LLMs.

</details>


### [48] [Your AI Bosses Are Still Prejudiced: The Emergence of Stereotypes in LLM-Based Multi-Agent Systems](https://arxiv.org/abs/2508.19919)
*Jingyu Guo,Yingying Xu*

Main category: cs.CL

TL;DR: 研究发现AI多智能体系统在无预设偏见的中性环境中会自发产生刻板印象，这些刻板印象随交互轮次和权力结构增强，表现出类似人类的社会行为模式。


<details>
  <summary>Details</summary>
Motivation: 虽然人类社交中的刻板印象已被充分记录，但AI系统通常被认为较少受此类偏见影响。以往研究关注训练数据带来的偏见，但刻板印象是否能在AI智能体交互中自发产生值得深入探索。

Method: 通过模拟职场交互的新实验框架，在初始中性条件下研究基于LLM的多智能体系统中刻板印象的出现和演化。

Result: (1) LLM智能体在无预设偏见情况下会发展出刻板印象驱动的偏见；(2) 刻板印象效应随交互轮次和决策权力增加而加剧；(3) 系统表现出类似人类的社会行为模式；(4) 这些模式在不同LLM架构中一致出现。

Conclusion: AI系统中的刻板印象形成可能是多智能体交互的涌现特性，而不仅仅是训练数据偏见的结果，需要进一步研究其机制并制定缓解策略。

Abstract: While stereotypes are well-documented in human social interactions, AI
systems are often presumed to be less susceptible to such biases. Previous
studies have focused on biases inherited from training data, but whether
stereotypes can emerge spontaneously in AI agent interactions merits further
exploration. Through a novel experimental framework simulating workplace
interactions with neutral initial conditions, we investigate the emergence and
evolution of stereotypes in LLM-based multi-agent systems. Our findings reveal
that (1) LLM-Based AI agents develop stereotype-driven biases in their
interactions despite beginning without predefined biases; (2) stereotype
effects intensify with increased interaction rounds and decision-making power,
particularly after introducing hierarchical structures; (3) these systems
exhibit group effects analogous to human social behavior, including halo
effects, confirmation bias, and role congruity; and (4) these stereotype
patterns manifest consistently across different LLM architectures. Through
comprehensive quantitative analysis, these findings suggest that stereotype
formation in AI systems may arise as an emergent property of multi-agent
interactions, rather than merely from training data biases. Our work
underscores the need for future research to explore the underlying mechanisms
of this phenomenon and develop strategies to mitigate its ethical impacts.

</details>


### [49] [HEAL: A Hypothesis-Based Preference-Aware Analysis Framework](https://arxiv.org/abs/2508.19922)
*Yifu Huo,Chenglong Wang,Qiren Zhu,Shunjie Xing,Tong Xiao,Chunliang Zhang,Tongran Liu,Jinbo Zhu*

Main category: cs.CL

TL;DR: HEAL框架通过假设空间重新排序来评估偏好对齐，提出排名准确性和偏好强度相关性两个指标，并构建UniHypoBench基准，发现当前方法能有效捕获代理模型偏好同时抑制负样本。


<details>
  <summary>Details</summary>
Motivation: 现有偏好优化方法（如DPO）的评估仅依赖单一响应，忽略了假设空间中其他潜在输出，无法全面反映真实应用场景。

Method: 提出HEAL评估框架，将偏好对齐建模为假设空间中的重新排序过程，包含排名准确性和偏好强度相关性两个互补指标，并构建UniHypoBench统一假设基准。

Result: 实验表明当前偏好学习方法能有效捕获代理模型提供的偏好，同时成功抑制负样本。

Conclusion: HEAL为偏好学习研究提供了理论创新范式和实践诊断工具，为开发更先进的全面偏好捕获对齐算法指明了方向。

Abstract: Preference optimization methods like DPO have achieved remarkable performance
in LLM alignment. However, the evaluation for these methods relies on a single
response and overlooks other potential outputs, which could also be generated
in real-world applications within this hypothetical space. To address this
issue, this paper presents a \textbf{H}ypothesis-based
Pr\textbf{E}ference-aware \textbf{A}na\textbf{L}ysis Framework (HEAL), a novel
evaluation paradigm that formulates preference alignment as a re-ranking
process within hypothesis spaces. The framework incorporates two complementary
metrics: ranking accuracy for evaluating ordinal consistency and preference
strength correlation for assessing continuous alignment. To facilitate this
framework, we develop UniHypoBench, a unified hypothesis benchmark constructed
from diverse instruction-response pairs. Through extensive experiments based on
HEAL, with a particular focus on the intrinsic mechanisms of preference
learning, we demonstrate that current preference learning methods can
effectively capture preferences provided by proxy models while simultaneously
suppressing negative samples. These findings contribute to preference learning
research through two significant avenues. Theoretically, we introduce
hypothesis space analysis as an innovative paradigm for understanding
preference alignment. Practically, HEAL offers researchers robust diagnostic
tools for refining preference optimization methods, while our empirical results
identify promising directions for developing more advanced alignment algorithms
capable of comprehensive preference capture.

</details>


### [50] [Dhati+: Fine-tuned Large Language Models for Arabic Subjectivity Evaluation](https://arxiv.org/abs/2508.19966)
*Slimane Bellaouar,Attia Nehar,Soumia Souffi,Mounia Bouameur*

Main category: cs.CL

TL;DR: 该论文针对阿拉伯语主观性分析资源匮乏的问题，提出了AraDhati+数据集并基于先进阿拉伯语模型进行微调，实现了97.79%的高准确率。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语作为语言资源丰富但形态复杂的语言，缺乏大规模标注数据集，阻碍了主观性分析工具的发展。深度学习和大语言模型在英语和法语文本分类中表现出色，但在阿拉伯语中应用受限。

Method: 1) 整合现有阿拉伯语数据集(ASTD、LABR、HARD、SANAD)构建AraDhati+数据集；2) 对XLM-RoBERTa、AraBERT和ArabianGPT等先进阿拉伯语模型进行微调；3) 采用集成决策方法结合各模型优势。

Result: 提出的方法在阿拉伯语主观性分类任务中达到了97.79%的准确率，显著提升了分类性能。

Conclusion: 该方法有效解决了阿拉伯语处理中资源有限的问题，证明了整合现有数据集和微调先进模型在低资源语言处理中的有效性。

Abstract: Despite its significance, Arabic, a linguistically rich and morphologically
complex language, faces the challenge of being under-resourced. The scarcity of
large annotated datasets hampers the development of accurate tools for
subjectivity analysis in Arabic. Recent advances in deep learning and
Transformers have proven highly effective for text classification in English
and French. This paper proposes a new approach for subjectivity assessment in
Arabic textual data. To address the dearth of specialized annotated datasets,
we developed a comprehensive dataset, AraDhati+, by leveraging existing Arabic
datasets and collections (ASTD, LABR, HARD, and SANAD). Subsequently, we
fine-tuned state-of-the-art Arabic language models (XLM-RoBERTa, AraBERT, and
ArabianGPT) on AraDhati+ for effective subjectivity classification.
Furthermore, we experimented with an ensemble decision approach to harness the
strengths of individual models. Our approach achieves a remarkable accuracy of
97.79\,\% for Arabic subjectivity classification. Results demonstrate the
effectiveness of the proposed approach in addressing the challenges posed by
limited resources in Arabic language processing.

</details>


### [51] [Diffusion Language Models Know the Answer Before Decoding](https://arxiv.org/abs/2508.19982)
*Pengxiang Li,Yefan Zhou,Dilxat Muhtar,Lu Yin,Shilin Yan,Li Shen,Yi Liang,Soroush Vosoughi,Shiwei Liu*

Main category: cs.CL

TL;DR: Prophet是一种无需训练的解码方法，利用扩散语言模型中早期答案收敛的特性，通过动态决策机制在保持生成质量的同时将解码步骤减少最多3.4倍


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型虽然提供并行序列生成，但其推理速度仍慢于自回归模型，主要由于双向注意力和大量细化步骤的成本。研究发现许多情况下正确答案可以在最终解码步骤之前被识别

Method: 提出Prophet解码范式，基于top-2预测候选之间的置信度差距作为标准，动态决定是继续细化还是"all-in"（一步解码所有剩余token）。无需额外训练，可无缝集成到现有DLM实现

Result: 在GSM8K和MMLU任务上，分别有97%和99%的实例仅使用一半细化步骤即可正确解码。Prophet将LLaDA-8B和Dream-7B的解码步骤减少最多3.4倍，同时保持高质量生成

Conclusion: Prophet将DLM解码重新定义为何时停止采样的问题，证明早期解码收敛为加速DLM推理提供了简单而强大的机制，是对现有加速技术的补充

Abstract: Diffusion language models (DLMs) have recently emerged as an alternative to
autoregressive approaches, offering parallel sequence generation and flexible
token orders. However, their inference remains slower than that of
autoregressive models, primarily due to the cost of bidirectional attention and
the large number of refinement steps required for high quality outputs. In this
work, we highlight and leverage an overlooked property of DLMs early answer
convergence: in many cases, the correct answer can be internally identified by
half steps before the final decoding step, both under semi-autoregressive and
random remasking schedules. For example, on GSM8K and MMLU, up to 97% and 99%
of instances, respectively, can be decoded correctly using only half of the
refinement steps. Building on this observation, we introduce Prophet, a
training-free fast decoding paradigm that enables early commit decoding.
Specifically, Prophet dynamically decides whether to continue refinement or to
go "all-in" (i.e., decode all remaining tokens in one step), using the
confidence gap between the top-2 prediction candidates as the criterion. It
integrates seamlessly into existing DLM implementations, incurs negligible
overhead, and requires no additional training. Empirical evaluations of
LLaDA-8B and Dream-7B across multiple tasks show that Prophet reduces the
number of decoding steps by up to 3.4x while preserving high generation
quality. These results recast DLM decoding as a problem of when to stop
sampling, and demonstrate that early decode convergence provides a simple yet
powerful mechanism for accelerating DLM inference, complementary to existing
speedup techniques. Our code is publicly available at
https://github.com/pixeli99/Prophet.

</details>


### [52] [AgentCoMa: A Compositional Benchmark Mixing Commonsense and Mathematical Reasoning in Real-World Scenarios](https://arxiv.org/abs/2508.19988)
*Lisa Alazraki,Lihu Chen,Ana Brassard,Joe Stacey,Hossein A. Rahmani,Marek Rei*

Main category: cs.CL

TL;DR: LLMs在单一类型的组合推理任务上表现良好，但在混合常识推理和数学推理的组合任务中准确率下降约30%，显示出模型脆弱性。


<details>
  <summary>Details</summary>
Motivation: 当前组合基准测试主要关注单一类型的推理（常识或数学），而现实世界任务需要两者结合，因此需要评估LLMs在混合类型组合推理中的表现。

Method: 引入Agentic Commonsense and Math benchmark (AgentCoMa)，包含需要常识推理和数学推理步骤的组合任务，测试了61个不同规模、模型家族和训练策略的LLM。

Result: LLMs在单独解决两个步骤时表现良好，但组合时准确率平均下降约30%，远高于同类型多步骤组合基准的性能差距。非专家人类注释者在组合问题和单独步骤上都能保持高准确率。

Conclusion: 研究揭示了LLMs在混合类型组合推理中的显著脆弱性，为未来改进提供了测试平台，需要通过可解释性研究进一步理解性能差距的原因。

Abstract: Large Language Models (LLMs) have achieved high accuracy on complex
commonsense and mathematical problems that involve the composition of multiple
reasoning steps. However, current compositional benchmarks testing these skills
tend to focus on either commonsense or math reasoning, whereas LLM agents
solving real-world tasks would require a combination of both. In this work, we
introduce an Agentic Commonsense and Math benchmark (AgentCoMa), where each
compositional task requires a commonsense reasoning step and a math reasoning
step. We test it on 61 LLMs of different sizes, model families, and training
strategies. We find that LLMs can usually solve both steps in isolation, yet
their accuracy drops by ~30% on average when the two are combined. This is a
substantially greater performance gap than the one we observe in prior
compositional benchmarks that combine multiple steps of the same reasoning
type. In contrast, non-expert human annotators can solve the compositional
questions and the individual steps in AgentCoMa with similarly high accuracy.
Furthermore, we conduct a series of interpretability studies to better
understand the performance gap, examining neuron patterns, attention maps and
membership inference. Our work underscores a substantial degree of model
brittleness in the context of mixed-type compositional reasoning and offers a
test bed for future improvement.

</details>


### [53] [MathBuddy: A Multimodal System for Affective Math Tutoring](https://arxiv.org/abs/2508.19993)
*Debanjana Kar,Leopold Böss,Dacia Braca,Sebastian Maximilian Dennerlein,Nina Christine Hubig,Philipp Wintersberger,Yufang Hou*

Main category: cs.CL

TL;DR: MathBuddy是一个情感感知的数学辅导系统，通过分析学生的文本对话和面部表情来识别情绪状态，并据此调整教学策略，显著提升了LLM辅导的教学效果。


<details>
  <summary>Details</summary>
Motivation: 当前的学习模型没有考虑学生的情感状态，而教育心理学研究表明情绪状态会影响学习能力，因此需要开发能够感知学生情绪并做出相应调整的智能辅导系统。

Method: 通过文本对话和面部表情双模态捕捉学生情绪，聚合情绪信息后提示LLM生成情感感知的回应，将情绪映射到相关教学策略。

Result: 在八个教学维度上使用自动评估指标和用户研究进行评估，胜率提升23点，DAMR总分提升3点，证明了情感建模对提升LLM教学能力的重要性。

Conclusion: 通过建模学生情绪可以显著改善基于LLM的辅导系统的教学能力，情感感知是提升教育技术效果的关键因素。

Abstract: The rapid adoption of LLM-based conversational systems is already
transforming the landscape of educational technology. However, the current
state-of-the-art learning models do not take into account the student's
affective states. Multiple studies in educational psychology support the claim
that positive or negative emotional states can impact a student's learning
capabilities. To bridge this gap, we present MathBuddy, an emotionally aware
LLM-powered Math Tutor, which dynamically models the student's emotions and
maps them to relevant pedagogical strategies, making the tutor-student
conversation a more empathetic one. The student's emotions are captured from
the conversational text as well as from their facial expressions. The student's
emotions are aggregated from both modalities to confidently prompt our LLM
Tutor for an emotionally-aware response. We have effectively evaluated our
model using automatic evaluation metrics across eight pedagogical dimensions
and user studies. We report a massive 23 point performance gain using the win
rate and a 3 point gain at an overall level using DAMR scores which strongly
supports our hypothesis of improving LLM-based tutor's pedagogical abilities by
modeling students' emotions.

</details>


### [54] [ReSURE: Regularizing Supervision Unreliability for Multi-turn Dialogue Fine-tuning](https://arxiv.org/abs/2508.19996)
*Yiming Du,Yifan Xiang,Bin Liang,Dahua Lin,Kam-Fai Wong,Fei Tan*

Main category: cs.CL

TL;DR: ReSURE是一种自适应学习方法来动态降低不可靠监督的权重，通过在线统计估计每轮损失分布，在多轮对话训练中有效缓解错误传播问题。


<details>
  <summary>Details</summary>
Motivation: 多轮对话系统微调需要高质量监督数据，但低质量数据会导致性能下降，早期轮次的监督错误会传播到后续轮次，破坏对话连贯性和响应质量。现有方法通过静态预过滤处理数据质量，但这种方法将质量控制与训练分离，无法缓解轮级错误传播。

Method: 提出ReSURE方法，使用Welford在线统计估计每轮损失分布，动态地重新加权样本损失，无需显式过滤不可靠监督。

Result: 在单源和混合质量数据集上的实验显示，ReSURE提高了训练稳定性和响应质量，在多个基准测试中响应分数与样本数量之间呈现正Spearman相关性（0.21~1.0）。

Conclusion: ReSURE有效缓解了多轮对话训练中的错误传播问题，为有效利用大规模数据提供了潜在途径。

Abstract: Fine-tuning multi-turn dialogue systems requires high-quality supervision but
often suffers from degraded performance when exposed to low-quality data.
Supervision errors in early turns can propagate across subsequent turns,
undermining coherence and response quality. Existing methods typically address
data quality via static prefiltering, which decouples quality control from
training and fails to mitigate turn-level error propagation. In this context,
we propose ReSURE (Regularizing Supervision UnREliability), an adaptive
learning method that dynamically down-weights unreliable supervision without
explicit filtering. ReSURE estimates per-turn loss distributions using
Welford's online statistics and reweights sample losses on the fly accordingly.
Experiments on both single-source and mixed-quality datasets show improved
stability and response quality. Notably, ReSURE enjoys positive Spearman
correlations (0.21 ~ 1.0 across multiple benchmarks) between response scores
and number of samples regardless of data quality, which potentially paves the
way for utilizing large-scale data effectively. Code is publicly available at
https://github.com/Elvin-Yiming-Du/ReSURE_Multi_Turn_Training.

</details>


### [55] [Selective Retrieval-Augmentation for Long-Tail Legal Text Classification](https://arxiv.org/abs/2508.19997)
*Boheng Mao*

Main category: cs.CL

TL;DR: 提出选择性检索增强(SRA)方法解决法律文本分类中的长尾分布问题，通过在训练集中仅对低频标签进行检索增强，避免引入噪声，无需改变模型架构，在两个法律文本分类基准数据集上取得了更好的性能。


<details>
  <summary>Details</summary>
Motivation: 法律文本分类基准数据集通常存在长尾标签分布问题，许多标签样本不足导致模型在稀有类别上表现不佳，需要一种有效的方法来改善低频标签的分类性能。

Method: 选择性检索增强(SRA)方法，专注于增强训练集中低频标签的样本，仅从训练数据中进行检索以避免信息泄漏，不需要外部语料库，且无需修改模型架构。

Result: 在LEDGAR(单标签)和UNFAIR-ToS(多标签)两个法律文本分类基准数据集上，SRA方法相比所有当前LexGLUE基线模型都获得了更高的micro-F1和macro-F1分数，在长尾法律文本分类方面表现出持续改进。

Conclusion: SRA方法通过选择性增强低频标签样本，有效解决了法律文本分类中的长尾分布问题，在不改变模型架构的情况下显著提升了稀有类别的分类性能。

Abstract: Legal text classification is a fundamental NLP task in the legal domain.
Benchmark datasets in this area often exhibit a long-tail label distribution,
where many labels are underrepresented, leading to poor model performance on
rare classes. This paper proposes Selective Retrieval-Augmentation (SRA) as a
solution to this problem. SRA focuses on augmenting samples belonging to
low-frequency labels in the training set, preventing the introduction of noise
for well-represented classes, and requires no changes to the model
architecture. Retrieval is performed only from the training data to ensure
there is no potential information leakage, removing the need for external
corpora simultaneously. The proposed SRA method is tested on two legal text
classification benchmark datasets with long-tail distributions: LEDGAR
(single-label) and UNFAIR-ToS (multi-label). The results indicate that SRA
attains higher micro-F1 and macro-F1 scores compared to all current LexGLUE
baselines across both datasets, illustrating consistent improvements in
long-tail legal text classification.

</details>


### [56] [DeepScholar-Bench: A Live Benchmark and Automated Evaluation for Generative Research Synthesis](https://arxiv.org/abs/2508.20033)
*Liana Patel,Negar Arabzadeh,Harshit Gupta,Ankita Sundar,Ion Stoica,Matei Zaharia,Carlos Guestrin*

Main category: cs.CL

TL;DR: DeepScholar-bench是一个用于评估生成式研究合成系统的实时基准测试框架，通过从arXiv论文中提取查询任务，评估系统在知识合成、检索质量和可验证性三个维度的表现。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法主要关注短格式事实性回答，无法捕捉真实研究合成任务的复杂性和动态性，需要开发新的评估框架来推动生成式研究合成系统的发展。

Method: 从高质量arXiv论文中提取查询任务，构建实时基准测试，开发自动化评估框架从知识合成、检索质量和可验证性三个维度全面评估系统性能，并实现参考管道DeepScholar-base。

Result: DeepScholar-base建立了强大的基线性能，达到或超过其他方法的竞争力，但所有系统在所有指标上的得分均未超过19%，表明基准测试难度较高且远未饱和。

Conclusion: DeepScholar-bench是一个具有挑战性的基准测试，对于推动能够进行生成式研究合成的AI系统发展具有重要意义，当前系统性能仍有很大提升空间。

Abstract: The ability to research and synthesize knowledge is central to human
expertise and progress. An emerging class of systems promises these exciting
capabilities through generative research synthesis, performing retrieval over
the live web and synthesizing discovered sources into long-form, cited
summaries. However, evaluating such systems remains an open challenge: existing
question-answering benchmarks focus on short-form factual responses, while
expert-curated datasets risk staleness and data contamination. Both fail to
capture the complexity and evolving nature of real research synthesis tasks. In
this work, we introduce DeepScholar-bench, a live benchmark and holistic,
automated evaluation framework designed to evaluate generative research
synthesis. DeepScholar-bench draws queries from recent, high-quality ArXiv
papers and focuses on a real research synthesis task: generating the related
work sections of a paper by retrieving, synthesizing, and citing prior
research. Our evaluation framework holistically assesses performance across
three key dimensions, knowledge synthesis, retrieval quality, and
verifiability. We also develop DeepScholar-base, a reference pipeline
implemented efficiently using the LOTUS API. Using the DeepScholar-bench
framework, we perform a systematic evaluation of prior open-source systems,
search AI's, OpenAI's DeepResearch, and DeepScholar-base. We find that
DeepScholar-base establishes a strong baseline, attaining competitive or higher
performance than each other method. We also find that DeepScholar-bench remains
far from saturated, with no system exceeding a score of $19\%$ across all
metrics. These results underscore the difficulty of DeepScholar-bench, as well
as its importance for progress towards AI systems capable of generative
research synthesis. We make our code available at
https://github.com/guestrin-lab/deepscholar-bench.

</details>


### [57] [Forewarned is Forearmed: Pre-Synthesizing Jailbreak-like Instructions to Enhance LLM Safety Guardrail to Potential Attacks](https://arxiv.org/abs/2508.20038)
*Sheng Liu,Qiang Sheng,Danding Wang,Yang Li,Guang Yang,Juan Cao*

Main category: cs.CL

TL;DR: IMAGINE是一个通过嵌入空间分布分析生成越狱指令的合成框架，旨在填补安全对齐语料库与真实越狱模式之间的分布差距，显著降低LLM的越狱攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在拒绝回答恶意指令方面有所改进，但仍易受分布不同于安全对齐语料库的越狱攻击。现有攻击暴露了LLM无法识别未见恶意指令的问题，突显了训练数据与真实攻击之间的分布不匹配，迫使开发者陷入被动修补循环。

Method: 提出IMAGINE框架，利用嵌入空间分布分析生成类似越狱指令。采用迭代优化过程，动态演化文本生成分布，通过合成数据示例增强安全对齐数据分布的覆盖范围。

Result: 基于IMAGINE增强的安全对齐语料库，在Qwen2.5、Llama3.1和Llama3.2模型上显著降低了攻击成功率，且不影响模型实用性。

Conclusion: IMAGINE框架有效解决了LLM安全对齐中的分布不匹配问题，提供了一种主动防御越狱攻击的方法，显著提升了模型的安全性。

Abstract: Despite advances in improving large language model (LLM) to refuse to answer
malicious instructions, widely used LLMs remain vulnerable to jailbreak attacks
where attackers generate instructions with distributions differing from safety
alignment corpora. New attacks expose LLMs' inability to recognize unseen
malicious instructions, highlighting a critical distributional mismatch between
training data and real-world attacks that forces developers into reactive
patching cycles. To tackle this challenge, we propose IMAGINE, a synthesis
framework that leverages embedding space distribution analysis to generate
jailbreak-like instructions. This approach effectively fills the distributional
gap between authentic jailbreak patterns and safety alignment corpora. IMAGINE
follows an iterative optimization process that dynamically evolves text
generation distributions across iterations, thereby augmenting the coverage of
safety alignment data distributions through synthesized data examples. Based on
the safety-aligned corpus enhanced through IMAGINE, our framework demonstrates
significant decreases in attack success rate on Qwen2.5, Llama3.1, and Llama3.2
without compromising their utility.

</details>


### [58] [AraHealthQA 2025: The First Shared Task on Arabic Health Question Answering](https://arxiv.org/abs/2508.20047)
*Hassan Alhuzali,Farah Shamout,Muhammad Abdul-Mageed,Chaimae Abouzahir,Mouath Abu-Daoud,Ashwag Alasmari,Walid Al-Eisawi,Renad Al-Monef,Ali Alqahtani,Lama Ayash,Nizar Habash,Leen Kharouf*

Main category: cs.CL

TL;DR: AraHealthQA 2025是阿拉伯语健康问答共享任务，包含MentalQA（心理健康）和MedArabiQ（综合医疗）两个赛道，旨在解决阿拉伯语医疗QA资源匮乏问题。


<details>
  <summary>Details</summary>
Motivation: 解决高质量阿拉伯语医疗问答资源稀缺的问题，促进在现实、多语言和文化敏感的医疗场景下的模型开发。

Method: 通过两个互补赛道设计：MentalQA专注于心理健康问答，MedArabiQ覆盖更广泛的医疗领域；提供多个子任务、评估数据集和标准化指标。

Result: 建立了标准化的评估框架，吸引了参与者开发基准系统，为阿拉伯语健康问答提供了公平的基准测试平台。

Conclusion: 该共享任务成功推动了阿拉伯语医疗QA领域的发展，为未来迭代奠定了基础，并观察到性能趋势和发展前景。

Abstract: We introduce {AraHealthQA 2025}, the {Comprehensive Arabic Health Question
Answering Shared Task}, held in conjunction with {ArabicNLP 2025} (co-located
with EMNLP 2025). This shared task addresses the paucity of high-quality Arabic
medical QA resources by offering two complementary tracks: {MentalQA}, focusing
on Arabic mental health Q\&A (e.g., anxiety, depression, stigma reduction), and
{MedArabiQ}, covering broader medical domains such as internal medicine,
pediatrics, and clinical decision making. Each track comprises multiple
subtasks, evaluation datasets, and standardized metrics, facilitating fair
benchmarking. The task was structured to promote modeling under realistic,
multilingual, and culturally nuanced healthcare contexts. We outline the
dataset creation, task design and evaluation framework, participation
statistics, baseline systems, and summarize the overall outcomes. We conclude
with reflections on the performance trends observed and prospects for future
iterations in Arabic health QA.

</details>


### [59] [11Plus-Bench: Demystifying Multimodal LLM Spatial Reasoning with Cognitive-Inspired Analysis](https://arxiv.org/abs/2508.20068)
*Chengzu Li,Wenshan Wu,Huanyu Zhang,Qingtao Li,Zeyu Gao,Yan Xia,José Hernández-Orallo,Ivan Vulić,Furu Wei*

Main category: cs.CL

TL;DR: 本文提出了11Plus-Bench基准测试，系统评估多模态大语言模型的空间推理能力，发现当前MLLMs展现出空间认知的早期迹象，但与人类存在显著性能差距。


<details>
  <summary>Details</summary>
Motivation: 人类认知过程中空间推理与感知紧密相关，但多模态大语言模型在这方面的能力尚未得到充分探索和评估。

Method: 基于真实标准化空间能力测试构建11Plus-Bench基准，包含细粒度专家标注的感知复杂度和推理过程，对14个MLLMs进行广泛实验并与人类表现对比。

Result: MLLMs表现出空间认知的早期迹象，认知努力与推理相关复杂度强相关，但实例级性能随机性大；人类表现则高度可预测且受抽象模式复杂度影响。

Conclusion: 当前MLLMs在空间推理方面既有新兴能力也有明显局限，为模型设计提供了可操作的改进见解。

Abstract: For human cognitive process, spatial reasoning and perception are closely
entangled, yet the nature of this interplay remains underexplored in the
evaluation of multimodal large language models (MLLMs). While recent MLLM
advancements show impressive performance on reasoning, their capacity for
human-like spatial cognition remains an open question. In this work, we
introduce a systematic evaluation framework to assess the spatial reasoning
abilities of state-of-the-art MLLMs relative to human performance. Central to
our work is 11Plus-Bench, a high-quality benchmark derived from realistic
standardized spatial aptitude tests. 11Plus-Bench also features fine-grained
expert annotations of both perceptual complexity and reasoning process,
enabling detailed instance-level analysis of model behavior. Through extensive
experiments across 14 MLLMs and human evaluation, we find that current MLLMs
exhibit early signs of spatial cognition. Despite a large performance gap
compared to humans, MLLMs' cognitive profiles resemble those of humans in that
cognitive effort correlates strongly with reasoning-related complexity.
However, instance-level performance in MLLMs remains largely random, whereas
human correctness is highly predictable and shaped by abstract pattern
complexity. These findings highlight both emerging capabilities and limitations
in current MLLMs' spatial reasoning capabilities and provide actionable
insights for advancing model design.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [60] [Functional Consistency of LLM Code Embeddings: A Self-Evolving Data Synthesis Framework for Benchmarking](https://arxiv.org/abs/2508.19558)
*Zhuohao Li,Wenqing Chen,Jianxing Yu,Zhichao Lu*

Main category: cs.SE

TL;DR: 本文提出了一种面向功能的代码自演化框架，用于构建多样化的代码功能语义基准测试，显著提升了嵌入模型在代码克隆检测、功能一致性识别和代码检索等任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注代码克隆检测，强调语法相似性而忽视了功能理解。大型语言模型的代码嵌入能否反映代码级别的功能语义尚不清楚，需要构建能够更好反映功能差异的基准数据集。

Method: 提出了Functionality-Oriented Code Self-Evolution数据合成框架，从单个代码实例生成四种独特的变体，构建涵盖四个语义和语法类别的多样化代码示例，更好地反映功能差异。

Result: 在三个下游任务（代码克隆检测、代码功能一致性识别和代码检索）上的大量实验表明，使用演化数据集训练的嵌入模型性能显著提升。

Conclusion: 该数据合成框架在提升代码功能理解方面具有有效性和泛化能力，推动了代码功能语义理解的发展。

Abstract: Embedding models have demonstrated strong performance in tasks like
clustering, retrieval, and feature extraction while offering computational
advantages over generative models and cross-encoders. Benchmarks such as MTEB
have shown that text embeddings from large language models (LLMs) capture rich
semantic information, but their ability to reflect code-level functional
semantics remains unclear. Existing studies largely focus on code clone
detection, which emphasizes syntactic similarity and overlooks functional
understanding. In this paper, we focus on the functional consistency of LLM
code embeddings, which determines if two code snippets perform the same
function regardless of syntactic differences. We propose a novel data synthesis
framework called Functionality-Oriented Code Self-Evolution to construct
diverse and challenging benchmarks. Specifically, we define code examples
across four semantic and syntactic categories and find that existing datasets
predominantly capture syntactic properties. Our framework generates four unique
variations from a single code instance, providing a broader spectrum of code
examples that better reflect functional differences. Extensive experiments on
three downstream tasks-code clone detection, code functional consistency
identification, and code retrieval-demonstrate that embedding models
significantly improve their performance when trained on our evolved datasets.
These results highlight the effectiveness and generalization of our data
synthesis framework, advancing the functional understanding of code.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [61] [Self-Supervised Pre-Training with Equilibrium Constraints](https://arxiv.org/abs/2508.19990)
*Xiaodong Cui,A F M Saif,Brian Kingsbury,Tianyi Chen*

Main category: cs.LG

TL;DR: 提出一种新的自监督预训练方法，通过双层优化和平衡约束处理异构数据，提高模型在下游任务中的适应性


<details>
  <summary>Details</summary>
Motivation: 传统自监督预训练方法在处理异构数据时将所有数据混合并最小化全局平均损失，但这种方法无法确保模型对每个异构数据源都达到局部最优

Method: 采用双层优化框架，通过K步梯度下降确保模型从初始状态出发能为每个异构数据源达到局部最优，使用一阶近似方法求解

Result: 在多领域和多语言数据集上的实验表明，该方法显著提高了自监督预训练模型在下游监督微调任务中的适应性

Conclusion: 所提出的平衡约束和双层优化方法能有效处理异构数据，提升自监督预训练模型的性能

Abstract: Self-supervised pre-training using unlabeled data is widely used in machine
learning. In this paper, we propose a new self-supervised pre-training approach
to dealing with heterogeneous data. Instead of mixing all the data and
minimizing the averaged global loss in the conventional way, we impose
additional equilibrium constraints to ensure that the models optimizes each
source of heterogeneous data to its local optima after $K$-step gradient
descent initialized from the model. We formulate this as a bilevel optimization
problem, and use the first-order approximation method to solve the problem. We
discuss its connection to model-agnostic meta learning (MAML). Experiments are
carried out on self-supervised pre-training using multi-domain and multilingual
datasets, demonstrating that the proposed approach can significantly improve
the adaptivity of the self-supervised pre-trained model for the downstream
supervised fine-tuning tasks.

</details>


### [62] [Linear-Time Demonstration Selection for In-Context Learning via Gradient Estimation](https://arxiv.org/abs/2508.19999)
*Ziniu Zhang,Zhenshuo Zhang,Dongyue Li,Lu Wang,Jennifer Dy,Hongyang R. Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种基于梯度估计的示例选择算法，用于上下文学习中的提示调整和链式思绪推理，能够在线性时间内高效选择最相关的示例。


<details>
  <summary>Details</summary>
Motivation: 解决在上下文学习中如何从大量示例中高效选择最佳的k个示例作为条件输入的问题，这在提示调整和链式思绪推理中具有重要应用价值。

Method: 通过梯度估计输出的一阶近似方法，对多个随机采样子集进行评估，然后聚合形成每个示例的影响分数，最终选择最相关的k个示例。该方法只需预计算模型输出和梯度一次，具有线性时间复杂度。

Result: 在6个数据集上实验验证，梯度估计方法的近似误差小于1%，能够将子集选择速度提升37.7倍（在有340亿参数的模型上），并比基于输入嵌入的现有方法性能提升11%。

Conclusion: 该研究提出的基于梯度估计的示例选择算法高效、准确，能够在保持较高近似精度的同时显著提升选择效率，为大规模模型的上下文学习应用提供了实用的解决方案。

Abstract: This paper introduces an algorithm to select demonstration examples for
in-context learning of a query set. Given a set of $n$ examples, how can we
quickly select $k$ out of $n$ to best serve as the conditioning for downstream
inference? This problem has broad applications in prompt tuning and
chain-of-thought reasoning. Since model weights remain fixed during in-context
learning, previous work has sought to design methods based on the similarity of
token embeddings. This work proposes a new approach based on gradients of the
output taken in the input embedding space. Our approach estimates model outputs
through a first-order approximation using the gradients. Then, we apply this
estimation to multiple randomly sampled subsets. Finally, we aggregate the
sampled subset outcomes to form an influence score for each demonstration, and
select $k$ most relevant examples. This procedure only requires pre-computing
model outputs and gradients once, resulting in a linear-time algorithm relative
to model and training set sizes. Extensive experiments across various models
and datasets validate the efficiency of our approach. We show that the gradient
estimation procedure yields approximations of full inference with less than
$\mathbf{1}\%$ error across six datasets. This allows us to scale up subset
selection that would otherwise run full inference by up to
$\mathbf{37.7}\times$ on models with up to $34$ billion parameters, and
outperform existing selection methods based on input embeddings by
$\mathbf{11}\%$ on average.

</details>


### [63] [Symphony: A Decentralized Multi-Agent Framework for Scalable Collective Intelligence](https://arxiv.org/abs/2508.20019)
*Ji Wang,Kashing Chen,Xinyuan Song,Ke Zhang,Lynn Ai,Eric Yang,Bill Shi*

Main category: cs.LG

TL;DR: Symphony是一个去中心化的多智能体系统，使用轻量级LLM在消费级GPU上协调，通过分布式账本、Beacon选择协议和加权投票机制实现高效任务分配。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的智能体框架依赖集中式编排，存在部署成本高、通信拓扑僵化和适应性有限的问题。

Method: 引入三个关键机制：1)记录能力的分布式账本；2)动态任务分配的Beacon选择协议；3)基于思维链的加权结果投票。

Result: 在推理基准测试中优于现有基线方法，获得显著准确率提升，并在不同容量模型间展现鲁棒性。

Conclusion: Symphony实现了隐私保护、可扩展、容错且低开销的编排系统，解决了集中式框架的局限性。

Abstract: Most existing Large Language Model (LLM)-based agent frameworks rely on
centralized orchestration, incurring high deployment costs, rigid communication
topologies, and limited adaptability. To address these challenges, we introduce
Symphony, a decentralized multi-agent system which enables lightweight LLMs on
consumer-grade GPUs to coordinate. Symphony introduces three key mechanisms:
(1) a decentralized ledger that records capabilities, (2) a Beacon-selection
protocol for dynamic task allocation, and (3) weighted result voting based on
CoTs. This design forms a privacy-saving, scalable, and fault-tolerant
orchestration with low overhead. Empirically, Symphony outperforms existing
baselines on reasoning benchmarks, achieving substantial accuracy gains and
demonstrating robustness across models of varying capacities.

</details>


### [64] [Pruning Strategies for Backdoor Defense in LLMs](https://arxiv.org/abs/2508.20032)
*Santosh Chapagain,Shah Muhammad Hamdi,Soukaina Filali Boubrahimi*

Main category: cs.LG

TL;DR: 本文研究了注意力头剪枝方法在防御预训练语言模型后门攻击中的有效性，提出了六种剪枝策略并在实验中验证了不同策略对不同类型攻击的防御效果。


<details>
  <summary>Details</summary>
Motivation: 后门攻击对预训练语言模型的性能和完整性构成严重威胁，这些攻击通过隐蔽的恶意触发器进行，难以检测和防御，需要事后净化方法来消除威胁。

Method: 设计了六种剪枝策略：基于梯度的剪枝、层间方差剪枝、结构化L1/L2稀疏化的梯度剪枝、随机集成剪枝、强化学习引导剪枝和贝叶斯不确定性剪枝，通过迭代移除信息量最少的注意力头来防御后门攻击。

Result: 实验评估显示，基于梯度的剪枝在防御语法触发器方面表现最佳，而强化学习和贝叶斯剪枝在防御风格攻击方面效果更好。

Conclusion: 注意力头剪枝是一种有效的后门攻击防御方法，不同剪枝策略适用于不同类型的攻击，无需了解攻击触发器或访问干净参考模型即可实现防御。

Abstract: Backdoor attacks are a significant threat to the performance and integrity of
pre-trained language models. Although such models are routinely fine-tuned for
downstream NLP tasks, recent work shows they remain vulnerable to backdoor
attacks that survive vanilla fine-tuning. These attacks are difficult to defend
because end users typically lack knowledge of the attack triggers. Such attacks
consist of stealthy malicious triggers introduced through subtle syntactic or
stylistic manipulations, which can bypass traditional detection and remain in
the model, making post-hoc purification essential. In this study, we explore
whether attention-head pruning can mitigate these threats without any knowledge
of the trigger or access to a clean reference model. To this end, we design and
implement six pruning-based strategies: (i) gradient-based pruning, (ii)
layer-wise variance pruning, (iii) gradient-based pruning with structured L1/L2
sparsification, (iv) randomized ensemble pruning, (v)
reinforcement-learning-guided pruning, and (vi) Bayesian uncertainty pruning.
Each method iteratively removes the least informative heads while monitoring
validation accuracy to avoid over-pruning. Experimental evaluation shows that
gradient-based pruning performs best while defending the syntactic triggers,
whereas reinforcement learning and Bayesian pruning better withstand stylistic
attacks.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [65] [Beat-Based Rhythm Quantization of MIDI Performances](https://arxiv.org/abs/2508.19262)
*Maximilian Wachter,Sebastian Murgul,Michael Heizmann*

Main category: cs.SD

TL;DR: 一种基于transformer的节奏量化模型，通过统一的令片表示和节拍/强拍信息，将MIDI表演量化为计量对齐的乐谱


<details>
  <summary>Details</summary>
Motivation: 解决MIDI表演节奏量化问题，将人类表演转换为计量准确的可读乐谱

Method: 使用节拍基础的预处理方法，将乐谱和表演数据转换为统一令片表示，构建优化的transformer模型架构

Result: 在钢琴和吉他表演数据上训练，在MUSTER指标上超越了现有最好方法

Conclusion: 该模型能够高效地完成节奏量化任务，为音乐记载提供了更准确的解决方案

Abstract: We propose a transformer-based rhythm quantization model that incorporates
beat and downbeat information to quantize MIDI performances into
metrically-aligned, human-readable scores. We propose a beat-based
preprocessing method that transfers score and performance data into a unified
token representation. We optimize our model architecture and data
representation and train on piano and guitar performances. Our model exceeds
state-of-the-art performance based on the MUSTER metric.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [66] [An Investigation on Group Query Hallucination Attacks](https://arxiv.org/abs/2508.19321)
*Kehao Miao,Xiaolong Jin*

Main category: cs.CR

TL;DR: 提出Group Query Attack攻击方法，通过同时向大语言模型提交多个查询来模拟真实用户交互场景，发现该方法能显著降低微调模型性能、触发潜在后门风险，并对预训练和对齐模型在推理任务中同样有效。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的广泛应用，理解其在用户交互中的潜在失败模式至关重要。用户经常在单次对话中提出多个问题，因此需要研究连续提示的累积上下文如何影响模型输出。

Method: 提出Group Query Attack技术，通过同时向LLMs呈现查询组来模拟多问题交互场景，研究连续提示的累积上下文对模型输出的影响。

Result: Group Query Attack显著降低了针对特定任务微调的模型性能，能够诱导触发LLMs的潜在后门风险，并且在数学推理和代码生成等推理任务中对预训练和对齐模型同样有效。

Conclusion: Group Query Attack是一种有效的攻击方法，揭示了LLMs在多查询交互场景中的脆弱性，特别是在微调模型和推理任务中表现出的安全风险，强调了需要改进模型在多轮对话中的鲁棒性。

Abstract: With the widespread use of large language models (LLMs), understanding their
potential failure modes during user interactions is essential. In practice,
users often pose multiple questions in a single conversation with LLMs.
Therefore, in this study, we propose Group Query Attack, a technique that
simulates this scenario by presenting groups of queries to LLMs simultaneously.
We investigate how the accumulated context from consecutive prompts influences
the outputs of LLMs. Specifically, we observe that Group Query Attack
significantly degrades the performance of models fine-tuned on specific tasks.
Moreover, we demonstrate that Group Query Attack induces a risk of triggering
potential backdoors of LLMs. Besides, Group Query Attack is also effective in
tasks involving reasoning, such as mathematical reasoning and code generation
for pre-trained and aligned models.

</details>


### [67] [Safety Alignment Should Be Made More Than Just A Few Attention Heads](https://arxiv.org/abs/2508.19697)
*Chao Huang,Zefeng Zhang,Juewei Yue,Quangang Li,Chuang Zhang,Tingwen Liu*

Main category: cs.CR

TL;DR: 本文发现LLM安全机制过度依赖少数注意力头，提出RDSHA方法识别安全关键头，并开发AHD训练策略将安全能力分散到更多注意力头中，显著提升模型安全鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的安全对齐存在漏洞，对抗性提示可以绕过安全措施。研究发现安全机制主要依赖有限的注意力头子集，移除这些头会严重损害模型安全性。

Method: 提出RDSHA方法利用模型的拒绝方向来识别主要负责安全行为的注意力头；开发AHD训练策略促进安全相关行为在多个注意力头中的分布式编码。

Result: 实验表明AHD成功将安全相关能力分散到更多注意力头中；在多种主流越狱攻击下，AHD训练的模型展现出显著更强的安全鲁棒性，同时保持整体功能效用。

Conclusion: 通过分布式编码安全行为到多个注意力头，可以有效提升大语言模型的安全鲁棒性，解决现有安全机制过度集中的脆弱性问题。

Abstract: Current safety alignment for large language models(LLMs) continues to present
vulnerabilities, given that adversarial prompting can effectively bypass their
safety measures.Our investigation shows that these safety mechanisms
predominantly depend on a limited subset of attention heads: removing or
ablating these heads can severely compromise model safety. To identify and
evaluate these safety-critical components, we introduce RDSHA, a targeted
ablation method that leverages the model's refusal direction to pinpoint
attention heads mostly responsible for safety behaviors. Further analysis shows
that existing jailbreak attacks exploit this concentration by selectively
bypassing or manipulating these critical attention heads. To address this
issue, we propose AHD, a novel training strategy designed to promote the
distributed encoding of safety-related behaviors across numerous attention
heads. Experimental results demonstrate that AHD successfully distributes
safety-related capabilities across more attention heads. Moreover, evaluations
under several mainstream jailbreak attacks show that models trained with AHD
exhibit considerably stronger safety robustness, while maintaining overall
functional utility.

</details>


### [68] [SoK: Large Language Model Copyright Auditing via Fingerprinting](https://arxiv.org/abs/2508.19843)
*Shuo Shao,Yiming Li,Yu He,Hongwei Yao,Wenyuan Yang,Dacheng Tao,Zhan Qin*

Main category: cs.CR

TL;DR: 这篇SoK论文系统性研究了LLM指纹技术，提出了统一框架和评测标准LeaFBench，通过149个模型实例实验更全面地评估了现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型作为重要知识产权容易受到版权侵权和模型盗用，而指纹技术作为非侵入性解决方案的可靠性仍然不确定。

Method: 提出统一框架将现有方法分为白盒和黑盒方法，并构建LeaFBench标准测试集，包含149个模型实例和13种后续处理技术。

Result: 实验结果显示了现有方法的优缺点，为该领域的未来研究提供了方向性指引。

Conclusion: 该研究为LLM指纹技术提供了系统性的分析框架和评估标准，揭示了关键技术挑战和未来研究方向。

Abstract: The broad capabilities and substantial resources required to train Large
Language Models (LLMs) make them valuable intellectual property, yet they
remain vulnerable to copyright infringement, such as unauthorized use and model
theft. LLM fingerprinting, a non-intrusive technique that extracts and compares
the distinctive features from LLMs to identify infringements, offers a
promising solution to copyright auditing. However, its reliability remains
uncertain due to the prevalence of diverse model modifications and the lack of
standardized evaluation. In this SoK, we present the first comprehensive study
of LLM fingerprinting. We introduce a unified framework and formal taxonomy
that categorizes existing methods into white-box and black-box approaches,
providing a structured overview of the state of the art. We further propose
LeaFBench, the first systematic benchmark for evaluating LLM fingerprinting
under realistic deployment scenarios. Built upon mainstream foundation models
and comprising 149 distinct model instances, LeaFBench integrates 13
representative post-development techniques, spanning both parameter-altering
methods (e.g., fine-tuning, quantization) and parameter-independent mechanisms
(e.g., system prompts, RAG). Extensive experiments on LeaFBench reveal the
strengths and weaknesses of existing methods, thereby outlining future research
directions and critical open problems in this emerging field. The code is
available at https://github.com/shaoshuo-ss/LeaFBench.

</details>


### [69] [Disabling Self-Correction in Retrieval-Augmented Generation via Stealthy Retriever Poisoning](https://arxiv.org/abs/2508.20083)
*Yanbo Dai,Zhenlan Ji,Zongjie Li,Kuan Li,Shuai Wang*

Main category: cs.CR

TL;DR: DisarmRAG是一种针对RAG系统的新型攻击方法，通过毒化检索器本身来绕过LLM的自校正能力，实现攻击者指定的输出，攻击成功率超过90%且具有隐蔽性。


<details>
  <summary>Details</summary>
Motivation: 现有RAG攻击主要针对知识库，但现代LLM具有强大的自校正能力(SCA)可以拒绝虚假上下文。本文旨在开发一种能够抑制SCA的新型攻击范式。

Method: 提出基于对比学习的模型编辑技术，对检索器进行局部隐蔽编辑，使其仅对特定查询返回恶意指令；设计迭代协同优化框架自动发现能够绕过提示防御的鲁棒指令。

Result: 在6个LLM和3个QA基准测试中，恶意指令检索接近完美，成功抑制SCA，攻击成功率超过90%，且编辑后的检索器在多种检测方法下保持隐蔽。

Conclusion: DisarmRAG揭示了针对检索器本身的攻击威胁，突显了开发检索器中心防御机制的迫切需求。

Abstract: Retrieval-Augmented Generation (RAG) has become a standard approach for
improving the reliability of large language models (LLMs). Prior work
demonstrates the vulnerability of RAG systems by misleading them into
generating attacker-chosen outputs through poisoning the knowledge base.
However, this paper uncovers that such attacks could be mitigated by the strong
\textit{self-correction ability (SCA)} of modern LLMs, which can reject false
context once properly configured. This SCA poses a significant challenge for
attackers aiming to manipulate RAG systems.
  In contrast to previous poisoning methods, which primarily target the
knowledge base, we introduce \textsc{DisarmRAG}, a new poisoning paradigm that
compromises the retriever itself to suppress the SCA and enforce
attacker-chosen outputs. This compromisation enables the attacker to
straightforwardly embed anti-SCA instructions into the context provided to the
generator, thereby bypassing the SCA. To this end, we present a
contrastive-learning-based model editing technique that performs localized and
stealthy edits, ensuring the retriever returns a malicious instruction only for
specific victim queries while preserving benign retrieval behavior. To further
strengthen the attack, we design an iterative co-optimization framework that
automatically discovers robust instructions capable of bypassing prompt-based
defenses. We extensively evaluate DisarmRAG across six LLMs and three QA
benchmarks. Our results show near-perfect retrieval of malicious instructions,
which successfully suppress SCA and achieve attack success rates exceeding 90\%
under diverse defensive prompts. Also, the edited retriever remains stealthy
under several detection methods, highlighting the urgent need for
retriever-centric defenses.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [70] [Should LLMs be WEIRD? Exploring WEIRDness and Human Rights in Large Language Models](https://arxiv.org/abs/2508.19269)
*Ke Zhou,Marios Constantinides,Daniele Quercia*

Main category: cs.CY

TL;DR: 研究发现主流大语言模型存在WEIRD文化偏见，非西方模型虽然文化多样性更好，但人权违规风险增加2-4%，特别是在性别平等方面。


<details>
  <summary>Details</summary>
Motivation: 评估主流LLMs的文化偏见程度，检查其输出是否与WEIRD价值观过度一致，以及是否违反人权原则。

Method: 使用世界价值观调查数据评估GPT-3.5、GPT-4、Llama-3、BLOOM和Qwen五个模型，比较其输出与WEIRD国家价值观的匹配度，并对照世界人权宣言和三个地区性人权宪章。

Result: 与WEIRD价值观对齐度较低的模型（如BLOOM和Qwen）产生更多文化多样性回应，但人权违规风险增加2-4%，特别是在性别平等方面存在有害性别规范。

Conclusion: LLMs文化代表性增加的同时，复制歧视性信念的风险也增加，宪法AI等方法只能部分解决这一矛盾。

Abstract: Large language models (LLMs) are often trained on data that reflect WEIRD
values: Western, Educated, Industrialized, Rich, and Democratic. This raises
concerns about cultural bias and fairness. Using responses to the World Values
Survey, we evaluated five widely used LLMs: GPT-3.5, GPT-4, Llama-3, BLOOM, and
Qwen. We measured how closely these responses aligned with the values of the
WEIRD countries and whether they conflicted with human rights principles. To
reflect global diversity, we compared the results with the Universal
Declaration of Human Rights and three regional charters from Asia, the Middle
East, and Africa. Models with lower alignment to WEIRD values, such as BLOOM
and Qwen, produced more culturally varied responses but were 2% to 4% more
likely to generate outputs that violated human rights, especially regarding
gender and equality. For example, some models agreed with the statements ``a
man who cannot father children is not a real man'' and ``a husband should
always know where his wife is'', reflecting harmful gender norms. These
findings suggest that as cultural representation in LLMs increases, so does the
risk of reproducing discriminatory beliefs. Approaches such as Constitutional
AI, which could embed human rights principles into model behavior, may only
partly help resolve this tension.

</details>


### [71] [Geopolitical Parallax: Beyond Walter Lippmann Just After Large Language Models](https://arxiv.org/abs/2508.19492)
*Mehmet Can Yavuz,Humza Gohar Kabir,Aylin Özkan*

Main category: cs.CY

TL;DR: 这篇论文研究了中西方起源的大语言模型在新闻质量评估中的系统性偏差，发现模型起源影响对政治敏感话题的主观性和情感评分。


<details>
  <summary>Details</summary>
Motivation: 主要动机是研究大语言模型在新闻质量评估任务中的地缘民族偏见，因为模型的训练数据和设计选择可能嵌入了文化或意识形态偏见。

Method: 采用逻辑回归探针和匹配话题评估方法，比较中国起源（Qwen、BGE、Jina）和西方起源（Snowflake、Granite）模型家族在15个风格、信息和情感维度上的评分差异。

Result: 发现一致的非随机分差：在巴勒斯坦报道中，西方模型给予更高主观性和积极情感分数，而中国模型更重视新颖性和描述性。中国模型对美国报道的流畅性、简洁性、技术性和总体质量评分更低，但消极情感分数更高。

Conclusion: 基于LLM的媒体评估流程需要文化检定，以避免将内容差异与模型引发的偏见混淆。

Abstract: Objectivity in journalism has long been contested, oscillating between ideals
of neutral, fact-based reporting and the inevitability of subjective framing.
With the advent of large language models (LLMs), these tensions are now
mediated by algorithmic systems whose training data and design choices may
themselves embed cultural or ideological biases. This study investigates
geopolitical parallax-systematic divergence in news quality and subjectivity
assessments-by comparing article-level embeddings from Chinese-origin (Qwen,
BGE, Jina) and Western-origin (Snowflake, Granite) model families. We evaluate
both on a human-annotated news quality benchmark spanning fifteen stylistic,
informational, and affective dimensions, and on parallel corpora covering
politically sensitive topics, including Palestine and reciprocal China-United
States coverage. Using logistic regression probes and matched-topic evaluation,
we quantify per-metric differences in predicted positive-class probabilities
between model families. Our findings reveal consistent, non-random divergences
aligned with model origin. In Palestine-related coverage, Western models assign
higher subjectivity and positive emotion scores, while Chinese models emphasize
novelty and descriptiveness. Cross-topic analysis shows asymmetries in
structural quality metrics Chinese-on-US scoring notably lower in fluency,
conciseness, technicality, and overall quality-contrasted by higher negative
emotion scores. These patterns align with media bias theory and our distinction
between semantic, emotional, and relational subjectivity, and extend LLM bias
literature by showing that geopolitical framing effects persist in downstream
quality assessment tasks. We conclude that LLM-based media evaluation pipelines
require cultural calibration to avoid conflating content differences with
model-induced bias.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [72] [Object Detection with Multimodal Large Vision-Language Models: An In-depth Review](https://arxiv.org/abs/2508.19294)
*Ranjan Sapkota,Manoj Karkee*

Main category: cs.CV

TL;DR: 这篇综述论文系统分析了大型视觉语言模型(LVLMs)在目标检测领域的应用，探讨了其架构创新、训练范式以及相比传统深度学习方法在适应性、上下文推理和泛化能力方面的优势。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于全面梳理视觉语言模型在目标检测领域的最新进展，分析其如何通过融合自然语言处理和计算机视觉技术来革新目标检测和定位方法，并评估其相对于传统方法的性能表现。

Method: 采用三步研究综述方法：首先讨论VLMs在目标检测中的工作原理，然后分析最近的LVLMs架构创新、训练范式和输出灵活性，最后全面考察视觉与文本信息整合的方法进展。

Result: 研究表明LVLMs在多样化场景（包括定位和分割）中表现出色，在实时性能、适应性和复杂性方面优于传统深度学习系统，预计很快将达到或超越传统方法的性能。

Conclusion: LVLMs的最新进展已经并将继续对目标检测和机器人应用产生变革性影响，尽管当前模型还存在一些主要限制，但已提出解决方案并明确了未来发展的路线图。

Abstract: The fusion of language and vision in large vision-language models (LVLMs) has
revolutionized deep learning-based object detection by enhancing adaptability,
contextual reasoning, and generalization beyond traditional architectures. This
in-depth review presents a structured exploration of the state-of-the-art in
LVLMs, systematically organized through a three-step research review process.
First, we discuss the functioning of vision language models (VLMs) for object
detection, describing how these models harness natural language processing
(NLP) and computer vision (CV) techniques to revolutionize object detection and
localization. We then explain the architectural innovations, training
paradigms, and output flexibility of recent LVLMs for object detection,
highlighting how they achieve advanced contextual understanding for object
detection. The review thoroughly examines the approaches used in integration of
visual and textual information, demonstrating the progress made in object
detection using VLMs that facilitate more sophisticated object detection and
localization strategies. This review presents comprehensive visualizations
demonstrating LVLMs' effectiveness in diverse scenarios including localization
and segmentation, and then compares their real-time performance, adaptability,
and complexity to traditional deep learning systems. Based on the review, its
is expected that LVLMs will soon meet or surpass the performance of
conventional methods in object detection. The review also identifies a few
major limitations of the current LVLM modes, proposes solutions to address
those challenges, and presents a clear roadmap for the future advancement in
this field. We conclude, based on this study, that the recent advancement in
LVLMs have made and will continue to make a transformative impact on object
detection and robotic applications in the future.

</details>


### [73] [KRETA: A Benchmark for Korean Reading and Reasoning in Text-Rich VQA Attuned to Diverse Visual Contexts](https://arxiv.org/abs/2508.19944)
*Taebaek Hwang,Minseo Kim,Gisang Lee,Seonuk Kim,Hyunjun Eun*

Main category: cs.CV

TL;DR: KRETA是首个针对韩语的文本丰富视觉问答基准数据集，填补了低资源语言在VLM评估方面的空白，包含多领域评估和自动化数据生成流程


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言（如韩语）缺乏文本丰富视觉问答基准的问题，以便更好地评估和比较视觉语言模型在多语言场景下的性能

Method: 开发半自动化的VQA生成流程，采用分步图像分解方法和七指标评估协议来确保数据质量，支持15个领域和26种图像类型的多维度评估

Result: 成功构建了KRETA基准数据集，为韩语文本丰富的视觉理解和推理提供了全面的评估框架，并开源了代码和数据集

Conclusion: KRETA不仅为韩语VLM研究提供了重要基准，其可扩展的管道设计也为其他语言类似基准的开发提供了参考，将推动多语言VLM研究的发展

Abstract: Understanding and reasoning over text within visual contexts poses a
significant challenge for Vision-Language Models (VLMs), given the complexity
and diversity of real-world scenarios. To address this challenge, text-rich
Visual Question Answering (VQA) datasets and benchmarks have emerged for
high-resource languages like English. However, a critical gap persists for
low-resource languages such as Korean, where the lack of comprehensive
benchmarks hinders robust model evaluation and comparison. To bridge this gap,
we introduce KRETA, a benchmark for Korean Reading and rEasoning in Text-rich
VQA Attuned to diverse visual contexts. KRETA facilitates an in-depth
evaluation of both visual text understanding and reasoning capabilities, while
also supporting a multifaceted assessment across 15 domains and 26 image types.
Additionally, we introduce a semi-automated VQA generation pipeline
specifically optimized for text-rich settings, leveraging refined stepwise
image decomposition and a rigorous seven-metric evaluation protocol to ensure
data quality. While KRETA is tailored for Korean, we hope our adaptable and
extensible pipeline will facilitate the development of similar benchmarks in
other languages, thereby accelerating multilingual VLM research. The code and
dataset for KRETA are available at https://github.com/tabtoyou/KRETA.

</details>


### [74] [GLSim: Detecting Object Hallucinations in LVLMs via Global-Local Similarity](https://arxiv.org/abs/2508.19972)
*Seongheon Park,Yixuan Li*

Main category: cs.CV

TL;DR: GLSim是一个无需训练的目标幻觉检测框架，通过结合全局和局部嵌入相似性信号，在多种场景下实现更准确可靠的幻觉检测。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型中的目标幻觉问题限制了其在现实应用中的安全部署，现有方法仅采用全局或局部视角，检测可靠性有限。

Method: 提出GLSim框架，利用图像和文本模态之间的互补性全局和局部嵌入相似性信号，无需训练即可进行目标幻觉检测。

Result: 在全面的基准测试中，GLSim表现出卓越的检测性能，显著优于竞争基线方法。

Conclusion: GLSim通过结合全局和局部视角，为目标幻觉检测提供了更准确可靠的解决方案，有助于提升视觉语言模型的安全部署。

Abstract: Object hallucination in large vision-language models presents a significant
challenge to their safe deployment in real-world applications. Recent works
have proposed object-level hallucination scores to estimate the likelihood of
object hallucination; however, these methods typically adopt either a global or
local perspective in isolation, which may limit detection reliability. In this
paper, we introduce GLSim, a novel training-free object hallucination detection
framework that leverages complementary global and local embedding similarity
signals between image and text modalities, enabling more accurate and reliable
hallucination detection in diverse scenarios. We comprehensively benchmark
existing object hallucination detection methods and demonstrate that GLSim
achieves superior detection performance, outperforming competitive baselines by
a significant margin.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [75] [Sycophancy as compositions of Atomic Psychometric Traits](https://arxiv.org/abs/2508.19316)
*Shreyans Jain,Alexandra Yost,Amirali Abdullah*

Main category: cs.AI

TL;DR: 论文提出将LLM中的奉承行为建模为心理测量特质的几何和因果组合，使用对比激活加法来映射激活方向，并开发可解释的向量干预方法来缓解安全问题。


<details>
  <summary>Details</summary>
Motivation: 奉承行为是LLM中的关键行为风险，但通常被当作孤立的故障模式处理。作者认为应该将其建模为心理测量特质（如情绪性、开放性和宜人性）的组合，类似于心理测量学中的因子分解。

Method: 使用对比激活加法（CAA）将激活方向映射到这些心理因素，研究不同组合如何导致奉承行为（如高外向性结合低尽责性）。

Result: 提出了可解释和可组合的基于向量的干预方法，如加法、减法和投影，可用于缓解LLM中的安全关键行为。

Conclusion: 通过将奉承行为建模为心理特质的几何组合，提供了更深入的理解和更有效的干预方法，为LLM安全提供了新的视角和工具。

Abstract: Sycophancy is a key behavioral risk in LLMs, yet is often treated as an
isolated failure mode that occurs via a single causal mechanism. We instead
propose modeling it as geometric and causal compositions of psychometric traits
such as emotionality, openness, and agreeableness - similar to factor
decomposition in psychometrics. Using Contrastive Activation Addition (CAA), we
map activation directions to these factors and study how different combinations
may give rise to sycophancy (e.g., high extraversion combined with low
conscientiousness). This perspective allows for interpretable and compositional
vector-based interventions like addition, subtraction and projection; that may
be used to mitigate safety-critical behaviors in LLMs.

</details>


### [76] [Instructional Agents: LLM Agents on Automated Course Material Generation for Teaching Faculties](https://arxiv.org/abs/2508.19611)
*Huaiyuan Yao,Wanpeng Xu,Justin Turnau,Nadia Kellam,Hua Wei*

Main category: cs.AI

TL;DR: Instructional Agents是一个多智能体LLM框架，通过模拟教育角色协作来自动化生成完整的课程材料，包括教学大纲、讲义、幻灯片和评估，显著减少人工工作量。


<details>
  <summary>Details</summary>
Motivation: 高质量教学材料的准备过程劳动密集且需要多方协调，现有AI教育工具只关注孤立任务，需要一种能够自动化端到端课程材料生成的解决方案。

Method: 采用多智能体大语言模型框架，模拟基于角色的教育智能体协作，提供四种操作模式：自主模式、目录引导模式、反馈引导模式和完全协同模式。

Result: 在五个大学计算机科学课程中评估显示，该系统能够生成高质量教学材料，同时显著减少开发时间和人工工作量。

Conclusion: Instructional Agents为教学设计能力有限的机构提供了一个可扩展且经济高效的框架，有助于在资源受限环境中普及高质量教育。

Abstract: Preparing high-quality instructional materials remains a labor-intensive
process that often requires extensive coordination among teaching faculty,
instructional designers, and teaching assistants. In this work, we present
Instructional Agents, a multi-agent large language model (LLM) framework
designed to automate end-to-end course material generation, including syllabus
creation, lecture scripts, LaTeX-based slides, and assessments. Unlike existing
AI-assisted educational tools that focus on isolated tasks, Instructional
Agents simulates role-based collaboration among educational agents to produce
cohesive and pedagogically aligned content. The system operates in four modes:
Autonomous, Catalog-Guided, Feedback-Guided, and Full Co-Pilot mode, enabling
flexible control over the degree of human involvement. We evaluate
Instructional Agents across five university-level computer science courses and
show that it produces high-quality instructional materials while significantly
reducing development time and human workload. By supporting institutions with
limited instructional design capacity, Instructional Agents provides a scalable
and cost-effective framework to democratize access to high-quality education,
particularly in underserved or resource-constrained settings.

</details>


### [77] [Analysing Chain of Thought Dynamics: Active Guidance or Unfaithful Post-hoc Rationalisation?](https://arxiv.org/abs/2508.19827)
*Samuel Lewis-Lim,Xingwei Tan,Zhixue Zhao,Nikolaos Aletras*

Main category: cs.AI

TL;DR: 研究发现Chain-of-Thought在软推理任务中效果有限且可能不忠实，不同模型对CoT的依赖方式存在差异，CoT的影响力和忠实度并不总是对齐


<details>
  <summary>Details</summary>
Motivation: 现有研究表明Chain-of-Thought在分析推理和常识推理等软推理问题中效果有限，且可能不忠实于模型的实际推理过程，需要深入探究不同模型在软推理任务中CoT的动态特性和忠实度

Method: 通过研究指令调优模型、推理模型和推理蒸馏模型在软推理任务中的表现，分析CoT的动态特性和忠实度

Result: 发现不同模型对CoT的依赖方式存在显著差异，CoT的影响力和模型推理的忠实度并不总是保持一致

Conclusion: CoT在软推理任务中的有效性因模型类型而异，需要更谨慎地评估CoT的忠实度和实际效果，不能简单假设CoT总是能提升推理性能

Abstract: Recent work has demonstrated that Chain-of-Thought (CoT) often yields limited
gains for soft-reasoning problems such as analytical and commonsense reasoning.
CoT can also be unfaithful to a model's actual reasoning. We investigate the
dynamics and faithfulness of CoT in soft-reasoning tasks across
instruction-tuned, reasoning and reasoning-distilled models. Our findings
reveal differences in how these models rely on CoT, and show that CoT influence
and faithfulness are not always aligned.

</details>


### [78] [SWIRL: A Staged Workflow for Interleaved Reinforcement Learning in Mobile GUI Control](https://arxiv.org/abs/2508.20018)
*Quanfeng Lu,Zhantao Ma,Shuai Zhong,Jin Wang,Dahai Yu,Michael K. Ng,Ping Luo*

Main category: cs.AI

TL;DR: SWIRL是一种用于多智能体系统的分阶段强化学习工作流，通过将多智能体强化学习重新表述为单智能体任务序列，实现稳定训练和高效协调。


<details>
  <summary>Details</summary>
Motivation: 现有单智能体方法在移动GUI代理中存在结构限制，而多智能体强化学习方法效率低下且与当前大视觉语言模型架构不兼容。

Method: SWIRL采用分阶段交错强化学习方法，一次更新一个智能体而保持其他智能体固定，包括导航器（语言转计划）和交互器（计划转动作）两个组件。

Result: 在GUI控制任务中表现出优越性能，同时在多智能体数学推理任务中也展示了强大能力。

Conclusion: SWIRL为开发高效稳健的多智能体系统提供了一个通用框架，具有理论保证和实际应用价值。

Abstract: The rapid advancement of large vision language models (LVLMs) and agent
systems has heightened interest in mobile GUI agents that can reliably
translate natural language into interface operations. Existing single-agent
approaches, however, remain limited by structural constraints. Although
multi-agent systems naturally decouple different competencies, recent progress
in multi-agent reinforcement learning (MARL) has often been hindered by
inefficiency and remains incompatible with current LVLM architectures. To
address these challenges, we introduce SWIRL, a staged workflow for interleaved
reinforcement learning designed for multi-agent systems. SWIRL reformulates
MARL into a sequence of single-agent reinforcement learning tasks, updating one
agent at a time while keeping the others fixed. This formulation enables stable
training and promotes efficient coordination across agents. Theoretically, we
provide a stepwise safety bound, a cross-round monotonic improvement theorem,
and convergence guarantees on return, ensuring robust and principled
optimization. In application to mobile GUI control, SWIRL instantiates a
Navigator that converts language and screen context into structured plans, and
an Interactor that grounds these plans into executable atomic actions.
Extensive experiments demonstrate superior performance on both high-level and
low-level GUI benchmarks. Beyond GUI tasks, SWIRL also demonstrates strong
capability in multi-agent mathematical reasoning, underscoring its potential as
a general framework for developing efficient and robust multi-agent systems.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [79] [Capabilities of GPT-5 across critical domains: Is it the next breakthrough?](https://arxiv.org/abs/2508.19259)
*Georgios P. Georgiou*

Main category: cs.HC

TL;DR: GPT-5在系统模型架构下，在教案制定、临床诊断、研究生成和伦理推理四个领域显著优于GPT-4，仅在作业评估方面表现相当，展示了其作为领域专业化工具的潜力。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，需要系统比较GPT-4和GPT-5在不同实际应用领域的性能差异，特别是在教育和医疗等关键领域。

Method: 使用20名语言学和临床领域专家作为评分者，基于预设标准评估GPT-4和GPT-5在五个领域（教案制定、作业评估、临床诊断、研究生成、伦理推理）生成的输出，采用混合效应模型进行统计分析。

Result: GPT-5在教案制定、临床诊断、研究生成和伦理推理四个领域显著优于GPT-4，两者在作业评估方面表现相当。

Conclusion: GPT-5展现出作为情境敏感和领域专业化工具的潜力，对教育、临床实践和学术研究具有实际价值，同时推动了伦理推理能力的发展，为GPT-5的演进能力和应用前景提供了早期实证评估。

Abstract: The accelerated evolution of large language models has raised questions about
their comparative performance across domains of practical importance. GPT-4 by
OpenAI introduced advances in reasoning, multimodality, and task
generalization, establishing itself as a valuable tool in education, clinical
diagnosis, and academic writing, though it was accompanied by several flaws.
Released in August 2025, GPT-5 incorporates a system-of-models architecture
designed for task-specific optimization and, based on both anecdotal accounts
and emerging evidence from the literature, demonstrates stronger performance
than its predecessor in medical contexts. This study provides one of the first
systematic comparisons of GPT-4 and GPT-5 using human raters from linguistics
and clinical fields. Twenty experts evaluated model-generated outputs across
five domains: lesson planning, assignment evaluation, clinical diagnosis,
research generation, and ethical reasoning, based on predefined criteria.
Mixed-effects models revealed that GPT-5 significantly outperformed GPT-4 in
lesson planning, clinical diagnosis, research generation, and ethical
reasoning, while both models performed comparably in assignment assessment. The
findings highlight the potential of GPT-5 to serve as a context-sensitive and
domain-specialized tool, offering tangible benefits for education, clinical
practice, and academic research, while also advancing ethical reasoning. These
results contribute to one of the earliest empirical evaluations of the evolving
capabilities and practical promise of GPT-5.

</details>


<div id='math.CO'></div>

# math.CO [[Back]](#toc)

### [80] [Word Chain Generators for Prefix Normal Words](https://arxiv.org/abs/2508.19619)
*Duncan Adamson,Moritz Dudey,Pamela Fleischmann,Annika Huch*

Main category: math.CO

TL;DR: 该论文研究了前缀正常词的性质，提出了新的词链和生成器方法来分析相同长度词之间的关系，并探讨了导致词非前缀正常性的因子特性。


<details>
  <summary>Details</summary>
Motivation: 解决前缀正常词的枚举和高效测试问题，这是Fici和Lipták在2011年引入该概念后遗留的开放性问题。

Method: 使用词链和生成器的新方法来关联相同长度的词，分析前缀正常词的因子特性，特别是那些导致词非前缀正常性的因子。

Result: 展示了一系列前缀正常词的特征性质，包括识别导致词违反前缀正常条件的特定因子模式。

Conclusion: 提出的词链和生成器方法为前缀正常词的研究提供了新的分析工具，有助于更好地理解和枚举这类特殊的二进制词。

Abstract: In 2011, Fici and Lipt\'ak introduced prefix normal words. A binary word is
prefix normal if it has no factor (substring) that contains more occurrences of
the letter 1 than the prefix of the same length. Among the open problems
regarding this topic are the enumeration of prefix normal words and efficient
testing methods. We show a range of characteristics of prefix normal words.
These include properties of factors that are responsible for a word not being
prefix normal. With word chains and generators, we introduce new ways of
relating words of the same length to each other.

</details>
