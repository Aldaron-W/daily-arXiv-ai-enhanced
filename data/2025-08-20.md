<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 34]
- [cs.LG](#cs.LG) [Total: 2]
- [cs.AI](#cs.AI) [Total: 6]
- [cs.CV](#cs.CV) [Total: 2]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.IR](#cs.IR) [Total: 2]
- [cs.DB](#cs.DB) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Fair Play in the Newsroom: Actor-Based Filtering Gender Discrimination in Text Corpora](https://arxiv.org/abs/2508.13169)
*Stefanie Urchs,Veronika Thurner,Matthias Aßenmacher,Christian Heumann,Stephanie Thiemichen*

Main category: cs.CL

TL;DR: 本文提出了一个扩展的参与者级别管道，用于检测和缓解大规模文本语料库中的性别歧视，通过新的参与者级别指标捕捉情感、句法代理和引用风格的不对称性，并在德国报纸语料库上验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的输出往往反映训练数据中的结构性性别不平衡，需要开发新的方法来检测和缓解这种歧视，以构建更公平的语料库。

Method: 基于先前的话语感知公平性分析工作，引入了新的参与者级别指标来捕捉情感、句法代理和引用风格的不对称性，支持诊断性语料分析和基于排除的平衡方法。

Result: 在1980-2024年德国报纸文章语料库上的应用显示，通过过滤和重新平衡可以缓解表面层面的不对称性，但在情感和框架方面仍存在更微妙形式的偏见。

Conclusion: 虽然表面层面的不对称性可以得到缓解，但更微妙的偏见形式仍然存在，需要进一步研究话语基础的公平性审计和公平语料库构建，并发布了相关工具和报告支持后续研究。

Abstract: Large language models are increasingly shaping digital communication, yet
their outputs often reflect structural gender imbalances that originate from
their training data. This paper presents an extended actor-level pipeline for
detecting and mitigating gender discrimination in large-scale text corpora.
Building on prior work in discourse-aware fairness analysis, we introduce new
actor-level metrics that capture asymmetries in sentiment, syntactic agency,
and quotation styles. The pipeline supports both diagnostic corpus analysis and
exclusion-based balancing, enabling the construction of fairer corpora. We
apply our approach to the taz2024full corpus of German newspaper articles from
1980 to 2024, demonstrating substantial improvements in gender balance across
multiple linguistic dimensions. Our results show that while surface-level
asymmetries can be mitigated through filtering and rebalancing, subtler forms
of bias persist, particularly in sentiment and framing. We release the tools
and reports to support further research in discourse-based fairness auditing
and equitable corpus construction.

</details>


### [2] [MM-BrowseComp: A Comprehensive Benchmark for Multimodal Browsing Agents](https://arxiv.org/abs/2508.13186)
*Shilong Li,Xingyuan Bu,Wenjie Wang,Jiaheng Liu,Jun Dong,Haoyang He,Hao Lu,Haozhe Zhang,Chenchen Jing,Zhen Li,Chuanhao Li,Jiayi Tian,Chenchen Zhang,Tianhao Peng,Yancheng He,Jihao Gu,Yuanxing Zhang,Jian Yang,Ge Zhang,Wenhao Huang,Wangchunshu Zhou,Zhaoxiang Zhang,Ruizhe Ding,Shilei Wen*

Main category: cs.CL

TL;DR: 一个新的多模态浏览测试案例MM-BrowseComp，用于评估AI代理的多模态信息检索和推理能力，现有最好模型仅达29.02%准确率。


<details>
  <summary>Details</summary>
Motivation: 现有浏览测试案例主要关注文本信息，忽视了网页中普遍存在的多模态内容，需要补充多模态浏览能力的评估标准。

Method: 构建了224个具有挑战性的手工制作问题，问题中包含图片提示，关键信息可能嵌入在网页图片或视频中，并提供验证过的检查单进行细粒度分析。

Result: 对状态最佳模型的综合评估显示，包括OpenAI o3在内的顶级模型仅获得29.02%的准确率，显示了当前模型多模态能力的不足。

Conclusion: 当前AI模型在多模态推理方面能力显著不足，缺乏原生多模态推理能力，MM-BrowseComp为评估和提升多模态浏览能力提供了重要标准。

Abstract: AI agents with advanced reasoning and tool use capabilities have demonstrated
impressive performance in web browsing for deep search. While existing
benchmarks such as BrowseComp evaluate these browsing abilities, they primarily
focus on textual information, overlooking the prevalence of multimodal content.
To bridge this gap, we introduce MM-BrowseComp, a novel benchmark comprising
224 challenging, hand-crafted questions specifically designed to assess agents'
multimodal retrieval and reasoning capabilities. These questions often
incorporate images in prompts, and crucial information encountered during the
search and reasoning process may also be embedded within images or videos on
webpages. Consequently, methods relying solely on text prove insufficient for
our benchmark. Additionally, we provide a verified checklist for each question,
enabling fine-grained analysis of multimodal dependencies and reasoning paths.
Our comprehensive evaluation of state-of-the-art models on MM-BrowseComp
reveals that even top models like OpenAI o3 with tools achieve only 29.02\%
accuracy, highlighting the suboptimal multimodal capabilities and lack of
native multimodal reasoning in current models.

</details>


### [3] [Overcoming Latency Bottlenecks in On-Device Speech Translation: A Cascaded Approach with Alignment-Based Streaming MT](https://arxiv.org/abs/2508.13358)
*Zeeshan Ahmed,Frank Seide,Niko Moritz,Ju Lin,Ruiming Xie,Simone Merello,Zhe Liu,Christian Fuegen*

Main category: cs.CL

TL;DR: 本文解决了在设备上实时流式语音翻译中集成自动语音识别(ASR)和机器翻译(MT)的挑战，通过同步翻译方法和高效整合技术实现了较低延迟和更高质量的结果。


<details>
  <summary>Details</summary>
Motivation: 虽然基于RNN-T的ASR系统能够实时转写，但在设备上实现流式实时翻译仍面临重大挑战，需要在翻译质量和延迟之间取得平衡。

Method: 提出同步翻译方法，利用ASR系统生成的语言线索管理上下文，采用高效的束搜索剪枝技术（如超时和强制终止）来维持系统的实时性。

Result: 在设备上双语对话语音翻译中应用，在延迟和质量方面都超过了基线方法，缩小了与非流式翻译系统的质量差距。

Conclusion: 该技术为更准确和高效的实时语音翻译开启了新路径，有力推动语音翻译技术在实时应用中的发展。

Abstract: This paper tackles several challenges that arise when integrating Automatic
Speech Recognition (ASR) and Machine Translation (MT) for real-time, on-device
streaming speech translation. Although state-of-the-art ASR systems based on
Recurrent Neural Network Transducers (RNN-T) can perform real-time
transcription, achieving streaming translation in real-time remains a
significant challenge. To address this issue, we propose a simultaneous
translation approach that effectively balances translation quality and latency.
We also investigate efficient integration of ASR and MT, leveraging linguistic
cues generated by the ASR system to manage context and utilizing efficient
beam-search pruning techniques such as time-out and forced finalization to
maintain system's real-time factor. We apply our approach to an on-device
bilingual conversational speech translation and demonstrate that our techniques
outperform baselines in terms of latency and quality. Notably, our technique
narrows the quality gap with non-streaming translation systems, paving the way
for more accurate and efficient real-time speech translation.

</details>


### [4] [Stands to Reason: Investigating the Effect of Reasoning on Idiomaticity Detection](https://arxiv.org/abs/2508.13365)
*Dylan Phelps,Rodrigo Wilkens,Edward Gow-Smith,Thomas Pickard,Maggie Mi,Aline Villavicencio*

Main category: cs.CL

TL;DR: 研究探索推理能力对LLM习语检测性能的影响，发现推理效果比预期小且多变，大模型能较好理解习语含义而小模型常失败，提供定义可提升小模型性能


<details>
  <summary>Details</summary>
Motivation: 利用推理模型提升LLM性能的趋势可能有益于需要逻辑步骤的习语检测任务，因为理解习语表达是消歧和推理的基础

Method: 评估DeepSeek-R1蒸馏模型(1.5B-70B参数)在四个习语检测数据集上的表现，分析推理链(CoT)对性能的影响，并为小模型提供定义进行实验

Result: 推理效果较小且多变：小模型CoT推理可从数学调优中间模型提升性能但不及基础模型，大模型(14B+)有适度提升；大模型能准确输出习语定义而小模型常失败；为小模型提供定义在某些情况下可改善性能

Conclusion: 模型大小对习语理解能力有显著影响，大模型展现良好的习语理解能力，而小模型需要额外支持（如提供定义）来提升性能

Abstract: The recent trend towards utilisation of reasoning models has improved the
performance of Large Language Models (LLMs) across many tasks which involve
logical steps. One linguistic task that could benefit from this framing is
idiomaticity detection, as a potentially idiomatic expression must first be
understood before it can be disambiguated and serves as a basis for reasoning.
In this paper, we explore how reasoning capabilities in LLMs affect
idiomaticity detection performance and examine the effect of model size. We
evaluate, as open source representative models, the suite of DeepSeek-R1
distillation models ranging from 1.5B to 70B parameters across four
idiomaticity detection datasets. We find the effect of reasoning to be smaller
and more varied than expected. For smaller models, producing chain-of-thought
(CoT) reasoning increases performance from Math-tuned intermediate models, but
not to the levels of the base models, whereas larger models (14B, 32B, and 70B)
show modest improvements. Our in-depth analyses reveal that larger models
demonstrate good understanding of idiomaticity, successfully producing accurate
definitions of expressions, while smaller models often fail to output the
actual meaning. For this reason, we also experiment with providing definitions
in the prompts of smaller models, which we show can improve performance in some
cases.

</details>


### [5] [Whispering Context: Distilling Syntax and Semantics for Long Speech Transcripts](https://arxiv.org/abs/2508.13376)
*Duygu Altinok*

Main category: cs.CL

TL;DR: 通过从LLaMA模型精炼上下文知识到Whisper中，提出了两种策略来改善长音频识别的语法和语义准确性


<details>
  <summary>Details</summary>
Motivation: ASR系统在长音频转录中常常无法维持语法和语义的准确性，影响命名实体识别、大写化和标点符号等任务

Method: 使用两种策略：(1)令牌级利用最优运输进行维度和序列长度对齐的精炼；(2)通过最小化Whisper和LLaMA句子嵌入表示之间的表示损失，融合语法和语义

Result: 在Spoken Wikipedia数据集上评估显示，在词错误率(WER)、命名实体识别、大写化和标点符号成功率方面都取得了显著改善

Conclusion: 通过引入新的NER指标和探索语义感知的ASR，该研究强调了将语言上下文集成到转录中的价值，为长形式语音的稳健、上下文感知ASR设定了基础

Abstract: ASR systems often struggle with maintaining syntactic and semantic accuracy
in long audio transcripts, impacting tasks like Named Entity Recognition (NER),
capitalization, and punctuation. We propose a novel approach that enhances ASR
by distilling contextual knowledge from LLaMA models into Whisper. Our method
uses two strategies: (1) token level distillation with optimal transport to
align dimensions and sequence lengths, and (2) representation loss minimization
between sentence embeddings of Whisper and LLaMA, blending syntax and
semantics. Evaluations on the Spoken Wikipedia dataset, a benchmark with long
audios and rich entities demonstrate significant improvements in Word Error
Rate (WER), NER, capitalization, and punctuation success. By introducing novel
NER metrics and exploring semantics aware ASR, our work highlights the value of
integrating linguistic context into transcription, setting a foundation for
robust, context-aware ASR in longform speech.

</details>


### [6] [Datarus-R1: An Adaptive Multi-Step Reasoning LLM for Automated Data Analysis](https://arxiv.org/abs/2508.13382)
*Ayoub Ben Chaliah,Hela Dellagi*

Main category: cs.CL

TL;DR: Datarus-R1-14B是一个14B参数的开源语言模型，基于Qwen 2.5-14B-Instruct微调，专门用于虚拟数据分析和研究生级别问题求解。该模型通过轨迹式训练、双奖励框架和GRPO优化，在多个定量领域表现出色，准确率提升30%，同时减少18-49%的token使用。


<details>
  <summary>Details</summary>
Motivation: 解决传统LLM在复杂问题求解中存在的格式崩溃、冗长循环和推理不连贯等问题，提供更高效、准确的分析能力。

Method: 1) 轨迹中心合成数据生成器产生14.4万个标记笔记本片段；2) 双奖励框架结合结构信号和分层奖励模型；3) 内存优化的GRPO实现；4) 余弦课程从结构保真度平滑过渡到语义深度。

Result: 在标准基准测试中超越同规模模型，达到QwQ-32B等更大推理模型的水平，AIME 2024/2025和LiveCodeBench准确率提升30%，每个解决方案减少18-49%的token。

Conclusion: Datarus通过创新的训练方法和双推理接口设计，成功实现了高效、准确的研究生级别问题求解，避免了传统系统的冗长循环问题。

Abstract: We present Datarus-R1-14B, a 14 B-parameter open-weights language model
fine-tuned from Qwen 2.5-14B-Instruct to act as a virtual data analyst and
graduate-level problem solver. Datarus is trained not on isolated
question-answer pairs but on full analytical trajectories including reasoning
steps, code execution, error traces, self-corrections, and final conclusions,
all captured in a ReAct-style notebook format spanning finance, medicine,
numerical analysis, and other quantitative domains. Our training pipeline
combines (i) a trajectory-centric synthetic data generator that yielded 144 000
tagged notebook episodes, (ii) a dual-reward framework blending a lightweight
tag-based structural signal with a Hierarchical Reward Model (HRM) that scores
both single-step soundness and end-to-end coherence, and (iii) a
memory-optimized implementation of Group Relative Policy Optimization (GRPO)
featuring KV-cache reuse, sequential generation, and reference-model sharding.
A cosine curriculum smoothly shifts emphasis from structural fidelity to
semantic depth, reducing the format collapse and verbosity that often plague
RL-aligned LLMs. A central design choice in Datarus is it dual reasoning
interface. In agentic mode the model produces ReAct-tagged steps that invoke
Python tools to execute real code; in reflection mode it outputs compact
Chain-of-Thought (CoT) traces delimited by <think> and <answer> tags. On
demanding postgraduate-level problems, Datarus exhibits an "AHA-moment"
pattern: it sketches hypotheses, revises them once or twice, and converges
avoiding the circular, token-inflating loops common to contemporary systems.
Across standard public benchmarks Datarus surpasses similar size models and
even reaches the level of larger reasoning models such as QwQ-32B achieving up
to 30% higher accuracy on AIME 2024/2025 and LiveCodeBench while emitting
18-49% fewer tokens per solution.

</details>


### [7] [ALIGN: Word Association Learning for Cross-Cultural Generalization in Large Language Models](https://arxiv.org/abs/2508.13426)
*Chunhua Liu,Kabir Manandhar Shrestha,Sukai Huang*

Main category: cs.CL

TL;DR: 通过基于词语联想规范的参数效率微调，使大语言模型在中英文化上获得显著改善，并将词汇层面的文化对齐转移到价值观层面


<details>
  <summary>Details</summary>
Motivation: 解决LLM在跨文化交流中因预训练语料偏差导致的文化偏见问题，并提供一种成本效率高的文化对齐方案

Method: 使用Small-World-of-Words项目的英语-美国和普通话词语联想规范，通过监督微调(SFT)和PPO偏好优化对Llama-3.1-8B和Qwen-2.5-7B进行参数效率微调

Result: SFT在英语中提升P@5 16-20%，普通话中提升43-165%；提高了词汇具体性；在世界价值观调查中模型答案分布向目标文化偏移；7-8B模型性能可与原生70B模型相当

Conclusion: 几百万个基于文化的词语联想可以以低成本方式实现价值对齐，为改善AI模型的文化对齐提供了有效途径

Abstract: As large language models (LLMs) increasingly mediate cross-cultural
communication, their behavior still reflects the distributional bias of the
languages and viewpoints that are over-represented in their pre-training
corpora. Yet, it remains a challenge to model and align culture due to limited
cultural knowledge and a lack of exploration into effective learning
approaches. We introduce a cost-efficient, cognitively grounded remedy:
parameter-efficient fine-tuning on native speakers' free word-association
norms, which encode implicit cultural schemas. Leveraging English-US and
Mandarin associations from the Small-World-of-Words project, we adapt
Llama-3.1-8B and Qwen-2.5-7B via supervised fine-tuning (SFT) and PPO-based
preference optimization. SFT boosts held-out association Precision at 5 by
16-20% in English and 43-165% in Mandarin, lifts median concreteness by +0.20,
and attains human-level valence and arousal. These lexical gains transfer: on
World-Values-Survey questions, fine-tuned models shift answer distributions
toward the target culture, and on a 50-item high-tension subset, Qwen's
Chinese-aligned responses double while Llama's US bias drops by one-third. Our
7-8B models rival or beat vanilla 70B baselines, showing that a few million
culture-grounded associations can instill value alignment without costly
retraining. Our work highlights both the promise and the need for future
research grounded in human cognition in improving cultural alignment in AI
models.

</details>


### [8] [ProMed: Shapley Information Gain Guided Reinforcement Learning for Proactive Medical LLMs](https://arxiv.org/abs/2508.13514)
*Hongxin Ding,Baixiang Huang,Yue Fang,Weibin Liao,Xinke Jiang,Zheng Li,Junfeng Zhao,Yasha Wang*

Main category: cs.CL

TL;DR: ProMed是一个强化学习框架，通过Shapley信息增益奖励机制，使医疗大语言模型从被动回答转向主动提问的交互范式，显著提升医疗诊断准确性


<details>
  <summary>Details</summary>
Motivation: 现有医疗大语言模型主要采用被动回答模式，缺乏主动信息收集能力，在临床交互场景中存在误诊风险，需要转向主动提问范式

Method: 提出基于强化学习的ProMed框架，核心是Shapley信息增益(SIG)奖励机制，结合蒙特卡洛树搜索构建高质量交互轨迹，并通过SIG引导的奖励分布机制进行策略优化

Result: 在两个新构建的部分信息医疗基准测试中，ProMed平均优于最先进方法6.29%，相比被动范式提升54.45%，且在域外案例中表现出良好的泛化能力

Conclusion: ProMed成功实现了医疗LLM从被动到主动交互范式的转变，通过SIG奖励机制有效提升了临床提问的价值和诊断准确性，为交互式医疗AI提供了新思路

Abstract: Interactive medical questioning is essential in real-world clinical
consultations, where physicians must actively gather information from patients.
While medical Large Language Models (LLMs) have shown impressive capabilities
in static medical question answering, they predominantly operate under a
reactive paradigm: generating answers directly without seeking additional
information, which risks incorrect diagnoses in such interactive settings. To
address this limitation, we propose ProMed, a reinforcement learning (RL)
framework that transitions medical LLMs toward a proactive paradigm, equipping
them with the ability to ask clinically valuable questions before
decision-making. At the core of ProMed is the Shapley Information Gain (SIG)
reward, which quantifies the clinical utility of each question by combining the
amount of newly acquired information with its contextual importance, estimated
via Shapley values. We integrate SIG into a two-stage training pipeline: (1)
SIG-Guided Model Initialization uses Monte Carlo Tree Search (MCTS) to
construct high-reward interaction trajectories to supervise the model, and (2)
SIG-Augmented Policy Optimization, which integrates SIG and enhances RL with a
novel SIG-guided Reward Distribution Mechanism that assigns higher rewards to
informative questions for targeted optimization. Extensive experiments on two
newly curated partial-information medical benchmarks demonstrate that ProMed
significantly outperforms state-of-the-art methods by an average of 6.29% and
delivers a 54.45% gain over the reactive paradigm, while also generalizing
robustly to out-of-domain cases.

</details>


### [9] [Saudi-Dialect-ALLaM: LoRA Fine-Tuning for Dialectal Arabic Generation](https://arxiv.org/abs/2508.13525)
*Hassan Barmandah*

Main category: cs.CL

TL;DR: 使用LoRA微调了沙特阿拉伯基础模型ALLaM-7B，通过方言标签训练技术显著提升了沙特方言生成能力，将方言生成率从47.97%提升到84.21%，减少了标准阿语漏出问题


<details>
  <summary>Details</summary>
Motivation: 现有阿拉伯语言模型主要支持现代标准阿拉伯语(MSA)，对沙特方言(如纳耶贾和氪贾孜)支持有限，影响了对真实方言变体的抓取能力

Method: 使用私有沙特方言指令数据集(5,466条合成指令-响应对)，采用LoRA微调技术训练ALLaM-7B模型，测试了两种方案：方言标签训练(在指令前添加明确方言标签)和无标签训练

Result: 方言标签模型表现最佳，将沙特方言生成率从47.97%提升到84.21%，将MSA漏出率从32.63%降低到6.21%，文本保真度也有显著提升。两种LoRA方案都超过了多个基线模型

Conclusion: 通过方言标签技术和LoRA微调可以高效提升阿拉伯语言模型的方言生成能力，为少数语言变体的模型训练提供了有效方法

Abstract: Large language models (LLMs) for Arabic are still dominated by Modern
Standard Arabic (MSA), with limited support for Saudi dialects such as Najdi
and Hijazi. This underrepresentation hinders their ability to capture authentic
dialectal variation. Using a privately curated Saudi Dialect Instruction
dataset (Hijazi and Najdi; 5,466 synthetic instruction-response pairs; 50/50
split), we LoRA-tune ALLaM-7B-Instruct-preview, the first foundation model
developed in Saudi Arabia, for Saudi dialect generation. We investigate two
variants: (i) Dialect-Token training, which prepends an explicit dialect tag to
the instruction, and (ii) No-Token training, which omits the tag at formatting
time. Evaluation on a held-out test set combines an external dialect classifier
with text fidelity metrics (chrF++ and BERTScore) and diversity measures. The
Dialect-Token model achieves the best control, raising the Saudi rate from
47.97% to 84.21% and reducing MSA leakage from 32.63% to 6.21%; fidelity also
improves (chrF++ +3.53, BERTScore +0.059). Both LoRA variants outperform strong
generic instruction models (Falcon-7B-Instruct, Llama-3.1-8B-Instruct,
Qwen-2.5-7B-Instruct, AceGPT-v2-8B-Chat, JAIS-13B-Chat) in dialect control and
fidelity, while avoiding metadata-tag echoing that these baselines frequently
exhibit. We do not release the dataset or any model weights/adapters; instead,
we release training/evaluation/inference code and a detailed datasheet (schema
and aggregate statistics) to support independent verification.

</details>


### [10] [MATA (māta): Mindful Assessment of the Telugu Abilities of Large Language Models](https://arxiv.org/abs/2508.13526)
*Chalamalasetti Kranti,Sowmya Vajjala*

Main category: cs.CL

TL;DR: MATA是一个新的泰卢固语评估数据集，包含729个多选题和开放式问题，用于评估大语言模型在泰卢固语中的能力表现。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型在低资源语言（泰卢固语）中的真实能力，理解模型局限性，为开发更具语言能力的大语言模型提供参考。

Method: 创建包含729个多选题和开放式问题的泰卢固语评估数据集，评估11个开源和闭源大语言模型，分析模型表现和依赖的启发式方法，比较LLM-as-a-judge与人工评估的可靠性。

Result: 发现大语言模型在多选题中依赖表面启发式方法（如答案位置和干扰项模式），在低资源语言中LLM-as-a-judge评估的可靠性需要进一步研究。

Conclusion: 细粒度评估对于理解模型局限性至关重要，可以为开发更具语言能力的大语言模型提供信息，同时为未来泰卢固语NLP研究奠定基础。

Abstract: In this paper, we introduce MATA, a novel evaluation dataset to assess the
ability of Large Language Models (LLMs) in Telugu language, comprising 729
carefully curated multiple-choice and open-ended questions that span diverse
linguistic dimensions. We evaluate 11 open-weight and closed-source LLMs on our
dataset and present a fine-grained analysis of their performance. Further, we
empirically show how LLMs rely on superficial heuristics such as answer
position and distractor patterns for multiple-choice questions. Finally, we
also compare LLM-as-a-judge evaluation with human evaluation for open-ended
questions and draw some conclusions on its reliability in a low-resource
language. We argue that such fine-grained evaluation is essential for
understanding model limitations and can inform the development of more
linguistically capable LLMs, while also serving as a foundation for future
research in Telugu NLP.

</details>


### [11] [Compressed Models are NOT Trust-equivalent to Their Large Counterparts](https://arxiv.org/abs/2508.13533)
*Rohit Raj Rai,Chirag Kothari,Siddhesh Shelke,Amit Awekar*

Main category: cs.CL

TL;DR: 这篇论文提出了一个两维信任等效性评估框架，通过解释性对齐和检验相似性来评估模型压缩后的可信赖性，发现压缩模型并非原大模型的信任等效替代品。


<details>
  <summary>Details</summary>
Motivation: 虽然现有工作已经深入研究了压缩对准确率的影响，但性能平等并不保证信任等效性。需要一个更全面的框架来评估压缩模型的可信赖性。

Method: 提出两维评估框架：1)解释性对齐（使用LIME和SHAP测试验证模型是否基于相同特征做决策）；2)检验相似性（通过ECE、MCE、Brier分数和可靠性图评估模型预测概率的可靠性）。以BERT-base为原模型，对其多种压缩版本在自然语言推理和参考识别任务上进行实验。

Result: 实验结果显示，即使准确率几乎相同，压缩模型与原模型在解释性对齐上很低，检验相似性也存在显著差异。这说明压缩模型并非原大模型的信任等效替代品。

Conclusion: 在部署压缩模型作为大模型的直接替代时，需要进行谨慎的评估，考虑性能平等之外的因素，确保模型的可信赖性。

Abstract: Large Deep Learning models are often compressed before being deployed in a
resource-constrained environment. Can we trust the prediction of compressed
models just as we trust the prediction of the original large model? Existing
work has keenly studied the effect of compression on accuracy and related
performance measures. However, performance parity does not guarantee
trust-equivalence. We propose a two-dimensional framework for trust-equivalence
evaluation. First, interpretability alignment measures whether the models base
their predictions on the same input features. We use LIME and SHAP tests to
measure the interpretability alignment. Second, calibration similarity measures
whether the models exhibit comparable reliability in their predicted
probabilities. It is assessed via ECE, MCE, Brier Score, and reliability
diagrams. We conducted experiments using BERT-base as the large model and its
multiple compressed variants. We focused on two text classification tasks:
natural language inference and paraphrase identification. Our results reveal
low interpretability alignment and significant mismatch in calibration
similarity. It happens even when the accuracies are nearly identical between
models. These findings show that compressed models are not trust-equivalent to
their large counterparts. Deploying compressed models as a drop-in replacement
for large models requires careful assessment, going beyond performance parity.

</details>


### [12] [A Comparative Study of Decoding Strategies in Medical Text Generation](https://arxiv.org/abs/2508.13580)
*Oriana Presacan,Alireza Nik,Vajira Thambawita,Bogdan Ionescu,Michael Riegler*

Main category: cs.CL

TL;DR: 本文研究了不同解码策略对医学LLM生成质量的影响，发现在医疗应用中确定性策略（如beam search）通常优于随机策略，解码方法选择的重要性有时甚至超过模型选择。


<details>
  <summary>Details</summary>
Motivation: 在医疗领域准确性至关重要的背景下，解码策略对LLM输出质量的影响尚未得到充分研究，需要探索不同解码方法在医疗任务中的表现差异。

Method: 在五个开放式医疗任务（翻译、摘要、问答、对话、图像描述）中评估11种解码策略，使用医学专业和通用LLM不同尺寸模型进行比较分析。

Result: 确定性策略表现更好，beam search得分最高；较大模型整体得分更高但推理时间更长；医学LLM仅在两个任务中优于通用模型，且对解码选择更敏感；不同评估指标相关性因任务而异。

Conclusion: 医疗应用中需要谨慎选择解码方法，其影响有时超过模型选择本身，MAUVE等指标与BERTScore、ROUGE相关性较弱且对解码策略更敏感。

Abstract: Large Language Models (LLMs) rely on various decoding strategies to generate
text, and these choices can significantly affect output quality. In healthcare,
where accuracy is critical, the impact of decoding strategies remains
underexplored. We investigate this effect in five open-ended medical tasks,
including translation, summarization, question answering, dialogue, and image
captioning, evaluating 11 decoding strategies with medically specialized and
general-purpose LLMs of different sizes. Our results show that deterministic
strategies generally outperform stochastic ones: beam search achieves the
highest scores, while {\eta} and top-k sampling perform worst. Slower decoding
methods tend to yield better quality. Larger models achieve higher scores
overall but have longer inference times and are no more robust to decoding.
Surprisingly, while medical LLMs outperform general ones in two of the five
tasks, statistical analysis shows no overall performance advantage and reveals
greater sensitivity to decoding choice. We further compare multiple evaluation
metrics and find that correlations vary by task, with MAUVE showing weak
agreement with BERTScore and ROUGE, as well as greater sensitivity to the
decoding strategy. These results highlight the need for careful selection of
decoding methods in medical applications, as their influence can sometimes
exceed that of model choice.

</details>


### [13] [Who Gets the Mic? Investigating Gender Bias in the Speaker Assignment of a Speech-LLM](https://arxiv.org/abs/2508.13603)
*Dariia Puhach,Amir H. Payberah,Éva Székely*

Main category: cs.CL

TL;DR: 语音LLM模型在说话者选择中存在性别偏见的潜在风险，对Bark TTS模型进行了系统性别偏见分析


<details>
  <summary>Details</summary>
Motivation: 识别语音大语言模型是否也像文本LLM一样存在性别偏见，通过说话者分配作为分析工具进行偏见研究

Method: 构建两个数据集：职业数据集（包含性别刻板印象的职业）和性别色彩词汇数据集，分析Bark TTS模型的默认说话者分配

Result: Bark模型没有显示出系统性偏见，但显示出了性别意识和一些性别倾向

Conclusion: 语音LLM在说话者选择中存在性别偏见的潜在风险，需要更多研究来识别和减少这些偏见

Abstract: Similar to text-based Large Language Models (LLMs), Speech-LLMs exhibit
emergent abilities and context awareness. However, whether these similarities
extend to gender bias remains an open question. This study proposes a
methodology leveraging speaker assignment as an analytic tool for bias
investigation. Unlike text-based models, which encode gendered associations
implicitly, Speech-LLMs must produce a gendered voice, making speaker selection
an explicit bias cue. We evaluate Bark, a Text-to-Speech (TTS) model, analyzing
its default speaker assignments for textual prompts. If Bark's speaker
selection systematically aligns with gendered associations, it may reveal
patterns in its training data or model design. To test this, we construct two
datasets: (i) Professions, containing gender-stereotyped occupations, and (ii)
Gender-Colored Words, featuring gendered connotations. While Bark does not
exhibit systematic bias, it demonstrates gender awareness and has some gender
inclinations.

</details>


### [14] [AdaDocVQA: Adaptive Framework for Long Document Visual Question Answering in Low-Resource Settings](https://arxiv.org/abs/2508.13606)
*Haoxuan Li,Wei Song,Aofan Liu,Peiwu Qin*

Main category: cs.CL

TL;DR: AdaDocVQA是一个自适应框架，通过混合文本检索、智能数据增强和自适应集成推理，显著提升了低资源环境下长文档视觉问答的性能，在日语文档VQA基准测试中取得了新的最先进结果。


<details>
  <summary>Details</summary>
Motivation: 解决长文档视觉问答在低资源环境下面临的上下文限制和训练数据不足的挑战

Method: 提出三个核心创新：混合文本检索架构、自动生成高质量推理问答对的智能数据增强管道、动态配置生成和早期停止机制的自适应集成推理

Result: 在JDocQA数据集上，Yes/No问题准确率83.04%，事实性问题52.66%，数值问题44.12%；在LAVA数据集上准确率59%，均达到新的最先进水平

Conclusion: 该框架为日语文档VQA建立了新的最先进结果，并为其他低资源语言和专门领域提供了可扩展的基础

Abstract: Document Visual Question Answering (Document VQA) faces significant
challenges when processing long documents in low-resource environments due to
context limitations and insufficient training data. This paper presents
AdaDocVQA, a unified adaptive framework addressing these challenges through
three core innovations: a hybrid text retrieval architecture for effective
document segmentation, an intelligent data augmentation pipeline that
automatically generates high-quality reasoning question-answer pairs with
multi-level verification, and adaptive ensemble inference with dynamic
configuration generation and early stopping mechanisms. Experiments on Japanese
document VQA benchmarks demonstrate substantial improvements with 83.04\%
accuracy on Yes/No questions, 52.66\% on factual questions, and 44.12\% on
numerical questions in JDocQA, and 59\% accuracy on LAVA dataset. Ablation
studies confirm meaningful contributions from each component, and our framework
establishes new state-of-the-art results for Japanese document VQA while
providing a scalable foundation for other low-resource languages and
specialized domains. Our code available at:
https://github.com/Haoxuanli-Thu/AdaDocVQA.

</details>


### [15] [CRISP: Persistent Concept Unlearning via Sparse Autoencoders](https://arxiv.org/abs/2508.13650)
*Tomer Ashuach,Dana Arad,Aaron Mueller,Martin Tutek,Yonatan Belinkov*

Main category: cs.CL

TL;DR: CRISP是一种基于稀疏自编码器的参数高效方法，用于实现大语言模型中的持久性概念遗忘，能够有效移除有害知识同时保持模型性能


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在现实应用中的部署增加，需要选择性移除不需要的知识同时保持模型效用。现有稀疏自编码器方法主要在推理时操作，无法实现参数层面的持久改变，容易被恶意行为者绕过

Method: CRISP自动识别跨多个层的显著SAE特征并抑制其激活，采用参数高效的方式实现持久性概念遗忘

Result: 在两个LLM上的实验表明，CRISP在WMDP基准测试的安全关键遗忘任务上优于先前方法，成功移除有害知识同时保持通用和领域内能力

Conclusion: 特征级分析显示CRISP实现了目标和良性概念之间的语义连贯分离，允许精确抑制目标特征

Abstract: As large language models (LLMs) are increasingly deployed in real-world
applications, the need to selectively remove unwanted knowledge while
preserving model utility has become paramount. Recent work has explored sparse
autoencoders (SAEs) to perform precise interventions on monosemantic features.
However, most SAE-based methods operate at inference time, which does not
create persistent changes in the model's parameters. Such interventions can be
bypassed or reversed by malicious actors with parameter access. We introduce
CRISP, a parameter-efficient method for persistent concept unlearning using
SAEs. CRISP automatically identifies salient SAE features across multiple
layers and suppresses their activations. We experiment with two LLMs and show
that our method outperforms prior approaches on safety-critical unlearning
tasks from the WMDP benchmark, successfully removing harmful knowledge while
preserving general and in-domain capabilities. Feature-level analysis reveals
that CRISP achieves semantically coherent separation between target and benign
concepts, allowing precise suppression of the target features.

</details>


### [16] [ViExam: Are Vision Language Models Better than Humans on Vietnamese Multimodal Exam Questions?](https://arxiv.org/abs/2508.13680)
*Vy Tuong Dang,An Vo,Quang Tau,Duc Dm,Daeyoung Kim*

Main category: cs.CL

TL;DR: 本研究提出了ViExam基准测试，评估了视觉语言模型在越南语多模态教育考试中的表现，发现当前最先进模型仅达到57.74%准确率，远低于人类平均水平(66.54%)和最佳表现(99.60%)。


<details>
  <summary>Details</summary>
Motivation: 探索视觉语言模型在低资源语言（越南语）多模态教育内容上的表现，验证主要基于英语数据训练的模型是否能处理真实世界的跨语言多模态推理任务。

Method: 构建ViExam基准测试，包含2,548个多模态问题，涵盖7个学术领域；测试多种最先进和开源VLM模型；评估跨语言提示和人类协作对性能的影响。

Result: SOTA模型平均准确率57.74%，开源模型27.70%；仅o3思考模型(74.07%)超过人类平均水平；跨语言提示反而降低性能1%；人类协作可提升5%准确率。

Conclusion: 当前VLMs在越南语多模态教育评估中表现不佳，跨语言迁移能力有限，需要针对低资源语言进行专门优化，人类协作能部分改善但仍有很大提升空间。

Abstract: Vision language models (VLMs) demonstrate remarkable capabilities on English
multimodal tasks, but their performance on low-resource languages with
genuinely multimodal educational content remains largely unexplored. In this
work, we test how VLMs perform on Vietnamese educational assessments,
investigating whether VLMs trained predominantly on English data can handle
real-world cross-lingual multimodal reasoning. Our work presents the first
comprehensive evaluation of VLM capabilities on multimodal Vietnamese exams
through proposing ViExam, a benchmark containing 2,548 multimodal questions. We
find that state-of-the-art VLMs achieve only 57.74% while open-source models
achieve 27.70% mean accuracy across 7 academic domains, including Mathematics,
Physics, Chemistry, Biology, Geography, Driving Test, and IQ Test. Most VLMs
underperform average human test-takers (66.54%), with only the thinking VLM o3
(74.07%) exceeding human average performance, yet still falling substantially
short of human best performance (99.60%). Cross-lingual prompting with English
instructions while maintaining Vietnamese content fails to improve performance,
decreasing accuracy by 1 percentage point for SOTA VLMs. Human-in-the-loop
collaboration can partially improve VLM performance by 5 percentage points.
Code and data are available at: https://vi-exam.github.io.

</details>


### [17] [Generics and Default Reasoning in Large Language Models](https://arxiv.org/abs/2508.13718)
*James Ravi Kirkpatrick,Rachel Katharine Sterken*

Main category: cs.CL

TL;DR: 这篇论文评估了28个大型语言模型在20种可反操作的推理模式上的表现，发现虽然一些前沿模型表现良好，但整体性能差异较大，并且CoT提示往往导致性能下降。


<details>
  <summary>Details</summary>
Motivation: 通用概括语（如'鸟会飞'）在非相完全逻辑中具有重要地位，需要评估LLM在这些可反操作的默认推理任务上的能力。

Method: 使用28个LLM模型进行20种可反操作推理模式的测试，比较了零次提示、少次提示和思维链（CoT）提示的效果。

Result: 前沿模型在默认推理问题上表现良好，但性能差异显著；CoT提示导致性能严重下降（平均-11.14%）；大多数模型无法区分可反操作和弗式推理。

Conclusion: 当前LLM在默认推理方面既有潜力也有限制，需要进一步改进才能更好地处理通用概括语的复杂特性。

Abstract: This paper evaluates the capabilities of 28 large language models (LLMs) to
reason with 20 defeasible reasoning patterns involving generic generalizations
(e.g., 'Birds fly', 'Ravens are black') central to non-monotonic logic.
Generics are of special interest to linguists, philosophers, logicians, and
cognitive scientists because of their complex exception-permitting behaviour
and their centrality to default reasoning, cognition, and concept acquisition.
We find that while several frontier models handle many default reasoning
problems well, performance varies widely across models and prompting styles.
Few-shot prompting modestly improves performance for some models, but
chain-of-thought (CoT) prompting often leads to serious performance degradation
(mean accuracy drop -11.14%, SD 15.74% in models performing above 75% accuracy
in zero-shot condition, temperature 0). Most models either struggle to
distinguish between defeasible and deductive inference or misinterpret generics
as universal statements. These findings underscore both the promise and limits
of current LLMs for default reasoning.

</details>


### [18] [Prediction is not Explanation: Revisiting the Explanatory Capacity of Mapping Embeddings](https://arxiv.org/abs/2508.13729)
*Hanna Herasimchyk,Alhassan Abdelhalim,Sören Laue,Michaela Regneri*

Main category: cs.CL

TL;DR: 本文挑战了通过预测准确率来判断词嵌入中语义知识编码的假设，证明即使随机信息也能被成功预测，表明现有方法主要反映向量空间几何相似性而非真正的语义属性涌现


<details>
  <summary>Details</summary>
Motivation: 理解深度学习模型中隐含编码的知识对于提高AI系统可解释性至关重要，但现有方法基于预测语义特征准确率来判断词嵌入知识编码的假设存在缺陷

Method: 通过实验证明现有方法能够成功预测随机信息，分析预测准确率与语义知识编码之间的真实关系，揭示算法上限对结果的主导作用

Result: 预测准确率不能可靠指示词嵌入中真正的基于特征的可解释性，比较不同数据集时仅基于预测性能无法可靠判断哪个数据集更好地被词嵌入捕获

Conclusion: 现有映射方法主要反映向量空间内的几何相似性，而非表明语义属性的真正涌现，需要重新评估基于预测准确率的词嵌入知识解释方法

Abstract: Understanding what knowledge is implicitly encoded in deep learning models is
essential for improving the interpretability of AI systems. This paper examines
common methods to explain the knowledge encoded in word embeddings, which are
core elements of large language models (LLMs). These methods typically involve
mapping embeddings onto collections of human-interpretable semantic features,
known as feature norms. Prior work assumes that accurately predicting these
semantic features from the word embeddings implies that the embeddings contain
the corresponding knowledge. We challenge this assumption by demonstrating that
prediction accuracy alone does not reliably indicate genuine feature-based
interpretability.
  We show that these methods can successfully predict even random information,
concluding that the results are predominantly determined by an algorithmic
upper bound rather than meaningful semantic representation in the word
embeddings. Consequently, comparisons between datasets based solely on
prediction performance do not reliably indicate which dataset is better
captured by the word embeddings. Our analysis illustrates that such mappings
primarily reflect geometric similarity within vector spaces rather than
indicating the genuine emergence of semantic properties.

</details>


### [19] [EEG-MedRAG: Enhancing EEG-based Clinical Decision-Making via Hierarchical Hypergraph Retrieval-Augmented Generation](https://arxiv.org/abs/2508.13735)
*Yi Wang,Haoran Luo,Lu Meng*

Main category: cs.CL

TL;DR: EEG-MedRAG是一个基于超图的三层检索增强生成框架，用于多源异构EEG数据的语义检索和因果链诊断生成，在跨疾病临床QA基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 随着EEG在神经科学和临床实践中的广泛应用，如何高效检索和语义解释大规模、多源、异构的EEG数据成为一个紧迫挑战。

Method: 提出三层超图框架，将EEG领域知识、个体病例和大规模存储库统一为可遍历的n元关系超图，实现联合语义-时间检索和因果链诊断生成。

Result: 实验表明EEG-MedRAG在答案准确性和检索性能上显著优于TimeRAG和HyperGraphRAG，展现了在真实临床决策支持中的强大潜力。

Conclusion: 该框架为解决EEG数据检索和解释问题提供了有效解决方案，并建立了首个跨疾病、跨角色的EEG临床QA基准，为系统评估疾病无关泛化和角色感知上下文理解提供了基础。

Abstract: With the widespread application of electroencephalography (EEG) in
neuroscience and clinical practice, efficiently retrieving and semantically
interpreting large-scale, multi-source, heterogeneous EEG data has become a
pressing challenge. We propose EEG-MedRAG, a three-layer hypergraph-based
retrieval-augmented generation framework that unifies EEG domain knowledge,
individual patient cases, and a large-scale repository into a traversable n-ary
relational hypergraph, enabling joint semantic-temporal retrieval and
causal-chain diagnostic generation. Concurrently, we introduce the first
cross-disease, cross-role EEG clinical QA benchmark, spanning seven disorders
and five authentic clinical perspectives. This benchmark allows systematic
evaluation of disease-agnostic generalization and role-aware contextual
understanding. Experiments show that EEG-MedRAG significantly outperforms
TimeRAG and HyperGraphRAG in answer accuracy and retrieval, highlighting its
strong potential for real-world clinical decision support. Our data and code
are publicly available at https://github.com/yi9206413-boop/EEG-MedRAG.

</details>


### [20] [Sycophancy under Pressure: Evaluating and Mitigating Sycophantic Bias via Adversarial Dialogues in Scientific QA](https://arxiv.org/abs/2508.13743)
*Kaiwei Zhang,Qi Jia,Zijian Chen,Wei Sun,Xiangyang Zhu,Chunyi Li,Dandan Zhu,Guangtao Zhai*

Main category: cs.CL

TL;DR: 这篇论文研究大语言模型的奉扰性问题，提出了统一评估框架和Pressure-Tune方法来提升模型在科学问答中的真实性和抗奉扰能力


<details>
  <summary>Details</summary>
Motivation: 大语言模型在需要事实严谨的领域常呈现奉扰性（无论正确与否都符合用户信念），这在科学问答等高风险场景中带来严重风险

Method: 提出统一评估框架来量化奉扰性影响，包括对抗性指标测量；开发Pressure-Tune方法，通过在合成对话数据上进行轻量微调，配合思维链理由来拒绝用户错误信息

Result: 系统评估显示开源和商业模型都普遍存在奉扰性，主要由对齐策略驱动而非模型大小；Pressure-Tune在具有挑战性的科学问答测试中显著提升了抗奉扰能力，且不影响准确性或对有效反馈的响应能力

Conclusion: 奉扰性是大语言模型在事实性领域的重要问题，Pressure-Tune提供了一种实用的解决方案，能够在不搁房模型其他性能的前提下提升模型的真实性和原则性

Abstract: Large language models (LLMs), while increasingly used in domains requiring
factual rigor, often display a troubling behavior: sycophancy, the tendency to
align with user beliefs regardless of correctness. This tendency is reinforced
by preference-based alignment techniques that optimize for user satisfaction
but can undermine truthfulness. While relatively benign in casual dialogue,
sycophancy poses serious risks in high-stakes settings such as scientific
question answering (QA), where model outputs may shape collaborative reasoning,
decision-making, and knowledge formation. Despite its importance, this
phenomenon remains underexamined in factual QA contexts. We address this gap by
introducing a unified evaluation framework to quantify the impact of
sycophantic context on model behavior in scientific QA, measuring how much
user-imposed social pressure distorts model outputs. The framework incorporates
adversarial prompting setups and targeted metrics, such as misleading
resistance and sycophancy resistance, that capture a model's ability to
maintain factual consistency under misleading cues. Systematic evaluations
across open-source and proprietary models reveal pervasive sycophantic
tendencies, driven more by alignment strategy than by model size. To mitigate
this issue, we propose Pressure-Tune, a lightweight post-training method that
fine-tunes models on synthetic adversarial dialogues paired with
chain-of-thought rationales. These rationales reject user misinformation while
reinforcing factual commitments. Experiments on challenging scientific QA
benchmarks show that Pressure-Tune significantly enhances sycophancy resistance
without compromising accuracy or responsiveness to valid feedback, offering a
practical pathway toward more truthful and principled model behavior.

</details>


### [21] [MGT-Prism: Enhancing Domain Generalization for Machine-Generated Text Detection via Spectral Alignment](https://arxiv.org/abs/2508.13768)
*Shengchao Liu,Xiaoming Liu,Chengzhengxu Li,Zhaohan Zhang,Guoxin Ma,Yu Lan,Shuai Xiao*

Main category: cs.CL

TL;DR: MGT-Prism是一种基于频域分析的机器生成文本检测方法，通过低频域过滤和动态频谱对齐策略，在跨域检测任务中优于现有方法


<details>
  <summary>Details</summary>
Motivation: 当前机器生成文本检测器在同域训练测试时表现良好，但在面对域偏移时泛化能力差，需要开发能够跨域泛化的检测方法

Method: 从频域角度分析文本表示，发现不同域间存在一致的频谱模式，但机器生成文本与人类撰写文本在幅度上存在显著差异。设计了低频域过滤模块过滤对域偏移敏感的文档级特征，以及动态频谱对齐策略提取任务特定和域不变特征

Result: 在3种域泛化场景的11个测试数据集上，MGT-Prism平均准确率比最先进基线高0.90%，F1分数高0.92%

Conclusion: 频域分析为机器生成文本检测提供了有效的域泛化解决方案，MGT-Prism方法在跨域检测任务中表现出色

Abstract: Large Language Models have shown growing ability to generate fluent and
coherent texts that are highly similar to the writing style of humans. Current
detectors for Machine-Generated Text (MGT) perform well when they are trained
and tested in the same domain but generalize poorly to unseen domains, due to
domain shift between data from different sources. In this work, we propose
MGT-Prism, an MGT detection method from the perspective of the frequency domain
for better domain generalization. Our key insight stems from analyzing text
representations in the frequency domain, where we observe consistent spectral
patterns across diverse domains, while significant discrepancies in magnitude
emerge between MGT and human-written texts (HWTs). The observation initiates
the design of a low frequency domain filtering module for filtering out the
document-level features that are sensitive to domain shift, and a dynamic
spectrum alignment strategy to extract the task-specific and domain-invariant
features for improving the detector's performance in domain generalization.
Extensive experiments demonstrate that MGT-Prism outperforms state-of-the-art
baselines by an average of 0.90% in accuracy and 0.92% in F1 score on 11 test
datasets across three domain-generalization scenarios.

</details>


### [22] [Can Large Language Models (LLMs) Describe Pictures Like Children? A Comparative Corpus Study](https://arxiv.org/abs/2508.13769)
*Hanna Woloszyn,Benjamin Gagl*

Main category: cs.CL

TL;DR: 大语言模型生成的文本在词汇丰富性、词频分布、语义相似度等方面与儿童语言存在显著差异，少量提示仅能稍微提升相似性


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型生成文本与儿童语言的相似度，以了解LLM在教育应用中的适宜性

Method: 使用雷同图片故事生成两个LLM语料库（零检验和少量提示），通过词频、词汇丰富性、句长、词性标签和语义相似度等心理语言学指标进行对比分析

Result: LLM生成文本更长但词汇贫乏、依赖高频词、名词表现不足，语义空间分析显示低相似性，少量提示仅能稍微改善

Conclusion: LLM无法准确复现儿童语言的词汇和语义模式，对儿童教育工具中使用LLM生成文本的适宜性提出质疑

Abstract: The role of large language models (LLMs) in education is increasing, yet
little attention has been paid to whether LLM-generated text resembles child
language. This study evaluates how LLMs replicate child-like language by
comparing LLM-generated texts to a collection of German children's descriptions
of picture stories. We generated two LLM-based corpora using the same picture
stories and two prompt types: zero-shot and few-shot prompts specifying a
general age from the children corpus. We conducted a comparative analysis
across psycholinguistic text properties, including word frequency, lexical
richness, sentence and word length, part-of-speech tags, and semantic
similarity with word embeddings. The results show that LLM-generated texts are
longer but less lexically rich, rely more on high-frequency words, and
under-represent nouns. Semantic vector space analysis revealed low similarity,
highlighting differences between the two corpora on the level of corpus
semantics. Few-shot prompt increased similarities between children and LLM text
to a minor extent, but still failed to replicate lexical and semantic patterns.
The findings contribute to our understanding of how LLMs approximate child
language through multimodal prompting (text + image) and give insights into
their use in psycholinguistic research and education while raising important
questions about the appropriateness of LLM-generated language in child-directed
educational tools.

</details>


### [23] [TracSum: A New Benchmark for Aspect-Based Summarization with Sentence-Level Traceability in Medical Domain](https://arxiv.org/abs/2508.13798)
*Bohao Chu,Meijie Li,Sameh Frihat,Chengyu Gu,Georg Lodde,Elisabeth Livingstone,Norbert Fuhr*

Main category: cs.CL

TL;DR: TracSum是一个用于可追溯、基于方面的医学摘要生成的新基准，包含500个医学摘要的3.5K摘要-引用对，提出了四指标评估框架和Track-Then-Sum基线方法。


<details>
  <summary>Details</summary>
Motivation: 解决LLM生成医学摘要时事实准确性不足的问题，通过提供句子级引用让用户能够追溯原始上下文来验证摘要准确性。

Method: 1) 标注500个医学摘要的7个关键医学方面，创建3.5K摘要-引用对；2) 提出四指标评估框架评估内容的完整性和一致性；3) 引入Track-Then-Sum摘要生成管道作为基线方法。

Result: 实验表明TracSum可作为有效的可追溯、基于方面摘要任务的基准。显式执行句子级追踪能提高生成准确性，包含完整上下文能进一步提升完整性。

Conclusion: TracSum基准和评估框架为可追溯医学摘要提供了有效解决方案，句子级追踪和完整上下文对提高摘要质量至关重要。

Abstract: While document summarization with LLMs has enhanced access to textual
information, concerns about the factual accuracy of these summaries persist,
especially in the medical domain. Tracing evidence from which summaries are
derived enables users to assess their accuracy, thereby alleviating this
concern. In this paper, we introduce TracSum, a novel benchmark for traceable,
aspect-based summarization, in which generated summaries are paired with
sentence-level citations, enabling users to trace back to the original context.
First, we annotate 500 medical abstracts for seven key medical aspects,
yielding 3.5K summary-citation pairs. We then propose a fine-grained evaluation
framework for this new task, designed to assess the completeness and
consistency of generated content using four metrics. Finally, we introduce a
summarization pipeline, Track-Then-Sum, which serves as a baseline method for
comparison. In experiments, we evaluate both this baseline and a set of LLMs on
TracSum, and conduct a human evaluation to assess the evaluation results. The
findings demonstrate that TracSum can serve as an effective benchmark for
traceable, aspect-based summarization tasks. We also observe that explicitly
performing sentence-level tracking prior to summarization enhances generation
accuracy, while incorporating the full context further improves completeness.

</details>


### [24] [Beyond Human Judgment: A Bayesian Evaluation of LLMs' Moral Values Understanding](https://arxiv.org/abs/2508.13804)
*Maciej Skorski,Alina Landowska*

Main category: cs.CL

TL;DR: 这篇论文通过贝叶斯方法评估了主流语言模型在道德维度理解方面与人类的对比，发现AI模型表现优于75%的人类标注者，特别是在减少假阴性方面表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有研究使用确定性基准（多数投票或包含规则）来评估语言模型的道德理解，但忽略了人类标注者之间的分歧和不确定性。本文旨在通过建模标注者分歧来更准确地评估模型表现。

Method: 采用GPU优化的贝叶斯框架，处理超过100万次模型查询，对Claude Sonnet 4、DeepSeek-V3、Llama 4 Maverick等顶级语言模型进行评估，基于25万+标注数据和700+标注者对10万+文本的分析。

Result: AI模型通常排在前25%的人类标注者水平，平衡准确率远高于平均水平。更重要的是，AI产生的假阴性远少于人类，显示出更敏感的道德检测能力。

Conclusion: 大型语言模型在道德维度理解方面表现出色，不仅达到人类顶尖水平，在某些方面（如减少假阴性）甚至超越人类表现，这为AI在道德敏感应用中的可靠性提供了有力证据。

Abstract: How do large language models understand moral dimensions compared to humans?
  This first large-scale Bayesian evaluation of market-leading language models
provides the answer. In contrast to prior work using deterministic ground truth
(majority or inclusion rules), we model annotator disagreements to capture both
aleatoric uncertainty (inherent human disagreement) and epistemic uncertainty
(model domain sensitivity). We evaluate top language models (Claude Sonnet 4,
DeepSeek-V3, Llama 4 Maverick) across 250K+ annotations from ~700 annotators on
100K+ texts spanning social media, news, and forums.
  Our GPU-optimized Bayesian framework processed 1M+ model queries, revealing
that AI models typically rank among the top 25\% of human annotators, achieving
much better-than-average balanced accuracy. Importantly, we find that AI
produces far fewer false negatives than humans, highlighting their more
sensitive moral detection capabilities.

</details>


### [25] [Prompt-Based One-Shot Exact Length-Controlled Generation with LLMs](https://arxiv.org/abs/2508.13805)
*Juncheng Xie,Hung-yi Lee*

Main category: cs.CL

TL;DR: 通过插入计数器标记的提示工程方法，使得大语言模型能够在生成文本时进行该计数，实现准确的长度控制，将严格长度遵从率从30%提升到95%以上。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在生成文本时经常无法准确控制输出长度，要么超过要求要么不足，因为模型无法可靠地进行内部词元计数。

Method: 提出一种基于提示的一次投射策略，通过在提示中添加倒计时标记和明确的计数规则，让模型在写作过程中同时进行计数。方法无需微调或迭代采样。

Result: 在MT-Bench-LI指令遵从任务中，GPT-4.1的严格长度遵从率从素提示下的30%以下提升到了计数提示下的95%以上，超越了流行的草稿-然后修计基准方法，同时保持了评估答案质量。

Conclusion: 突出显示仅通过提示工程就可以实现精确的长度控制，提供了一种轻量级的替代方案，可以替代基于训练或解码的方法。

Abstract: Controlling the length of text produced by large language models (LLMs)
remains challenging: models frequently overshoot or undershoot explicit length
instructions because they cannot reliably keep an internal token count. We
present a prompt-based, one-shot strategy that compels an off-the-shelf LLM to
generate exactly a desired number of tokens - words (English) or characters
(Chinese) - without any fine-tuning or iterative sampling. The prompt appends
countdown markers and explicit counting rules so that the model "writes while
counting." We evaluate on four settings: open-ended generation (1-1000 tokens),
XSUM summarization, MT-Bench-LI instruction following, and the LIFEBENCH
equal-length track. On MT-Bench-LI, strict length compliance with GPT-4.1 leaps
from below 30% under naive prompts to above 95% with our countdown prompt,
surpassing the popular draft-then-revise baseline, while judged answer quality
is preserved. These results show that precise length control can be achieved
through prompt engineering alone, offering a lightweight alternative to
training- or decoding-based methods.

</details>


### [26] [The illusion of a perfect metric: Why evaluating AI's words is harder than it looks](https://arxiv.org/abs/2508.13816)
*Maria Paz Oliva,Adriana Correia,Ivan Vankov,Viktor Botev*

Main category: cs.CL

TL;DR: 本文对自然语言生成自动评估指标进行了全面分析，发现现有指标都存在局限性，没有单一完美指标，建议根据任务需求选择指标并采用互补评估方法。


<details>
  <summary>Details</summary>
Motivation: 自然语言生成评估是AI应用的关键挑战，人工评估成本高且不可扩展，自动评估指标虽然发展迅速但缺乏统一标准，需要系统分析现有指标的局限性和有效性。

Method: 对现有自动评估指标的方法论、文档化优势限制、验证方法以及与人工评估的相关性进行彻底检查，包括传统词汇比较、语义相似度模型和最新的LLM评估器。

Result: 发现指标通常只捕捉文本质量的特定方面，有效性因任务和数据集而异，验证实践缺乏结构化，与人工评估的相关性不一致，这些挑战在最新的LLM评估器和RAG评估中依然存在。

Conclusion: 挑战了寻找'完美指标'的追求，建议基于任务特定需求选择指标，利用互补评估，新指标应专注于改进验证方法论。

Abstract: Evaluating Natural Language Generation (NLG) is crucial for the practical
adoption of AI, but has been a longstanding research challenge. While human
evaluation is considered the de-facto standard, it is expensive and lacks
scalability. Practical applications have driven the development of various
automatic evaluation metrics (AEM), designed to compare the model output with
human-written references, generating a score which approximates human judgment.
Over time, AEMs have evolved from simple lexical comparisons, to semantic
similarity models and, more recently, to LLM-based evaluators. However, it
seems that no single metric has emerged as a definitive solution, resulting in
studies using different ones without fully considering the implications. This
paper aims to show this by conducting a thorough examination of the
methodologies of existing metrics, their documented strengths and limitations,
validation methods, and correlations with human judgment. We identify several
key challenges: metrics often capture only specific aspects of text quality,
their effectiveness varies by task and dataset, validation practices remain
unstructured, and correlations with human judgment are inconsistent.
Importantly, we find that these challenges persist in the most recent type of
metric, LLM-as-a-Judge, as well as in the evaluation of Retrieval Augmented
Generation (RAG), an increasingly relevant task in academia and industry. Our
findings challenge the quest for the 'perfect metric'. We propose selecting
metrics based on task-specific needs and leveraging complementary evaluations
and advocate that new metrics should focus on enhanced validation
methodologies.

</details>


### [27] [Extracting Structured Requirements from Unstructured Building Technical Specifications for Building Information Modeling](https://arxiv.org/abs/2508.13833)
*Insaf Nahri,Romain Pinquié,Philippe Véron,Nicolas Bus,Mathieu Thorel*

Main category: cs.CL

TL;DR: 使用CamemBERT和Fr_core_news_lg模型在法语建筑技术规范文档中实现命名实体识别和关系提取，NER F1分过90%，RE F1分过80%


<details>
  <summary>Details</summary>
Motivation: 解决建筑行业中非结构化法语BTS文档的需求自动提取问题，通过BIM与NLP技术结合提高效率

Method: 采用命名实体识别(NER)和关系提取(RE)技术，使用CamemBERT和Fr_core_news_lg转移学习模型，建立手动注释数据集进行比较

Result: CamemBERT和Fr_core_news_lg在NER任务中表现最佳(F1>90%)，Random Forest在RE任务中效果最好(F1>80%)

Conclusion: 研究证明了转移学习模型在专业领域文本处理中的有效性，为构建知识图普和自动验证系统奠定基础

Abstract: This study explores the integration of Building Information Modeling (BIM)
with Natural Language Processing (NLP) to automate the extraction of
requirements from unstructured French Building Technical Specification (BTS)
documents within the construction industry. Employing Named Entity Recognition
(NER) and Relation Extraction (RE) techniques, the study leverages the
transformer-based model CamemBERT and applies transfer learning with the French
language model Fr\_core\_news\_lg, both pre-trained on a large French corpus in
the general domain. To benchmark these models, additional approaches ranging
from rule-based to deep learning-based methods are developed. For RE, four
different supervised models, including Random Forest, are implemented using a
custom feature vector. A hand-crafted annotated dataset is used to compare the
effectiveness of NER approaches and RE models. Results indicate that CamemBERT
and Fr\_core\_news\_lg exhibited superior performance in NER, achieving
F1-scores over 90\%, while Random Forest proved most effective in RE, with an
F1 score above 80\%. The outcomes are intended to be represented as a knowledge
graph in future work to further enhance automatic verification systems.

</details>


### [28] [MME-SCI: A Comprehensive and Challenging Science Benchmark for Multimodal Large Language Models](https://arxiv.org/abs/2508.13938)
*Jiacheng Ruan,Dan Jiang,Xian Gao,Ting Liu,Yuzhuo Fu,Yangyang Kang*

Main category: cs.CL

TL;DR: MME-SCI是一个针对多模态大语言模型的多语言科学推理基准测试，包含1019个高质量问答对，覆盖数学、物理、化学、生物四大学科和五种语言，评估显示现有模型在该基准上表现不佳，特别是在仅图像模式下准确率较低。


<details>
  <summary>Details</summary>
Motivation: 现有科学领域基准测试存在三个关键挑战：多语言场景下推理能力评估不足、多模态覆盖不充分、科学知识点标注粒度不够细。为了解决这些问题，作者提出了MME-SCI基准。

Method: 收集了1019个高质量问答对，涉及3种评估模式，覆盖数学、物理、化学、生物四大学科，支持中文、英文、法文、西班牙文和日文五种语言。在16个开源模型和4个闭源模型上进行了广泛实验。

Result: 实验结果表明MME-SCI对现有MLLMs具有广泛挑战性。在仅图像评估模式下，o4-mini在数学、物理、化学、生物上的准确率分别仅为52.11%、24.73%、36.57%和29.80%，难度显著高于现有基准。

Conclusion: MME-SCI是一个全面且具有挑战性的基准测试，通过其多语言和细粒度知识属性，能够深入分析现有模型在特定领域的弱点，为多模态大语言模型的科学推理能力评估提供了重要工具。

Abstract: Recently, multimodal large language models (MLLMs) have achieved significant
advancements across various domains, and corresponding evaluation benchmarks
have been continuously refined and improved. In this process, benchmarks in the
scientific domain have played an important role in assessing the reasoning
capabilities of MLLMs. However, existing benchmarks still face three key
challenges: 1) Insufficient evaluation of models' reasoning abilities in
multilingual scenarios; 2) Inadequate assessment of MLLMs' comprehensive
modality coverage; 3) Lack of fine-grained annotation of scientific knowledge
points. To address these gaps, we propose MME-SCI, a comprehensive and
challenging benchmark. We carefully collected 1,019 high-quality
question-answer pairs, which involve 3 distinct evaluation modes. These pairs
cover four subjects, namely mathematics, physics, chemistry, and biology, and
support five languages: Chinese, English, French, Spanish, and Japanese. We
conducted extensive experiments on 16 open-source models and 4 closed-source
models, and the results demonstrate that MME-SCI is widely challenging for
existing MLLMs. For instance, under the Image-only evaluation mode, o4-mini
achieved accuracy of only 52.11%, 24.73%, 36.57%, and 29.80% in mathematics,
physics, chemistry, and biology, respectively, indicating a significantly
higher difficulty level compared to existing benchmarks. More importantly,
using MME-SCI's multilingual and fine-grained knowledge attributes, we analyzed
existing models' performance in depth and identified their weaknesses in
specific domains. The Data and Evaluation Code are available at
https://github.com/JCruan519/MME-SCI.

</details>


### [29] [ReviewGraph: A Knowledge Graph Embedding Based Framework for Review Rating Prediction with Sentiment Features](https://arxiv.org/abs/2508.13953)
*A. J. W. de Vink,Natalia Amat-Lefort,Lifeng Han*

Main category: cs.CL

TL;DR: 提出ReviewGraph框架，通过将客户评论转换为知识图谱（提取三元组和情感分数），使用图嵌入和机器学习分类器预测评论评分，性能与LLMs相当但计算成本更低，并提供更好的可解释性。


<details>
  <summary>Details</summary>
Motivation: 酒店业需要理解影响客户评分的因素来提升客户满意度和业务表现，传统NLP方法在可解释性和计算效率方面存在局限。

Method: 将文本评论转换为(主语,谓语,宾语)三元组知识图谱，关联情感分数，使用Node2Vec图嵌入和情感特征，通过机器学习分类器进行评分预测。

Result: 在HotelRec数据集上，ReviewGraph性能与最佳模型相当但计算成本更低，在Cohen's Kappa等一致性指标上优于基线方法，与LLMs表现相当。

Conclusion: 基于图的表示方法在评论分析中具有潜力，提供了更好的可解释性和可视化探索能力，为集成图神经网络和微调LLM提取方法奠定了基础。

Abstract: In the hospitality industry, understanding the factors that drive customer
review ratings is critical for improving guest satisfaction and business
performance. This work proposes ReviewGraph for Review Rating Prediction (RRP),
a novel framework that transforms textual customer reviews into knowledge
graphs by extracting (subject, predicate, object) triples and associating
sentiment scores. Using graph embeddings (Node2Vec) and sentiment features, the
framework predicts review rating scores through machine learning classifiers.
We compare ReviewGraph performance with traditional NLP baselines (such as Bag
of Words, TF-IDF, and Word2Vec) and large language models (LLMs), evaluating
them in the HotelRec dataset. In comparison to the state of the art literature,
our proposed model performs similar to their best performing model but with
lower computational cost (without ensemble).
  While ReviewGraph achieves comparable predictive performance to LLMs and
outperforms baselines on agreement-based metrics such as Cohen's Kappa, it
offers additional advantages in interpretability, visual exploration, and
potential integration into Retrieval-Augmented Generation (RAG) systems. This
work highlights the potential of graph-based representations for enhancing
review analytics and lays the groundwork for future research integrating
advanced graph neural networks and fine-tuned LLM-based extraction methods. We
will share ReviewGraph output and platform open-sourced on our GitHub page
https://github.com/aaronlifenghan/ReviewGraph

</details>


### [30] [Chunks as Arms: Multi-Armed Bandit-Guided Sampling for Long-Context LLM Preference Optimization](https://arxiv.org/abs/2508.13993)
*Shaohua Duan,Xinze Li,Zhenghao Liu,Xiaoyuan Yi,Yukun Yan,Shuo Wang,Yu Gu,Ge Yu,Maosong Sun*

Main category: cs.CL

TL;DR: 通过多臂老虎机械摆动策略选择最信息密集的上下文块落，生成高质量多样化回应，构建偏好数据对进行直接偏好优化训练，提升长上下文模型能力


<details>
  <summary>Details</summary>
Motivation: 现有合成数据方法在长上下文模型中存在数据多样性低和事实不一致的问题，需要更有效的方法来生成高质量多样化的偏好数据

Method: 采用多臂老虎机械摆动策略，将上下文块落视为臂条，根据预期奖励分数选择最信息密集的块输入LLM生成回应，并通过奖励反馈迭代更新分数，最后使用DPO方法进行优化

Result: 在长上下文理解测试集上达到了最先进的性能，显著提升了偏好数据对的质量和多样性

Conclusion: LongMab-PO框架通过MAB策略有效地解决了长上下文模型中的数据质量问题，为生成高质量多样化偏好数据提供了有效方案

Abstract: Long-context modeling is critical for a wide range of real-world tasks,
including long-context question answering, summarization, and complex reasoning
tasks. Recent studies have explored fine-tuning Large Language Models (LLMs)
with synthetic data to enhance their long-context capabilities. However, the
effectiveness of such approaches is often limited by the low diversity and
factual inconsistencies in the generated data. To address these challenges, we
propose LongMab-PO, a novel framework that leverages a Multi-Armed Bandit (MAB)
rollout strategy to identify the most informative chunks from the given long
context for sampling high-quality and diverse responses and constructing
preference data pairs for Direct Preference Optimization (DPO) training.
Specifically, we treat context chunks as arms of MAB, select chunks based on
their expected reward scores to input into LLMs to generate responses, and
iteratively update these scores based on reward feedback. This exploration and
exploitation process enables the model to focus on the most relevant context
segments, thereby generating and collecting high-quality and diverse responses.
Finally, we collect these generated responses from the rollout process and
apply the DPO method to further optimize the LLM. Experimental results show
that LongMab-PO significantly improves the diversity and quality of preference
data pairs, achieving state-of-the-art performance on long-context reasoning
benchmarks. All code and data will be released on
https://github.com/NEUIR/LongMab-PO.

</details>


### [31] [Ask Good Questions for Large Language Models](https://arxiv.org/abs/2508.14025)
*Qi Wu,Zhongqi Lu*

Main category: cs.CL

TL;DR: 提出了Ask-Good-Question (AGQ)框架，使用改进的Concept-Enhanced Item Response Theory (CEIRT)模型结合大语言模型，能更好地识别用户知识水平并直接生成引导性问题，显著提升问答过程中的信息检索效率。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型对话系统在相关概念辨别方面存在不足，无法准确识别用户困惑，导致主题引导效果不佳。

Method: 引入AGQ框架，采用改进的CEIRT模型来识别用户知识水平，并结合大语言模型直接从启发文本生成引导性问题。

Result: 与基线方法相比，该方法显著提升了用户信息检索体验，在问答效率方面表现更优。

Conclusion: AGQ框架通过CEIRT模型和LLMs的结合，有效解决了对话系统中概念辨别和主题引导的挑战，为用户提供了更好的信息检索体验。

Abstract: Recent advances in large language models (LLMs) have significantly improved
the performance of dialog systems, yet current approaches often fail to provide
accurate guidance of topic due to their inability to discern user confusion in
related concepts. To address this, we introduce the Ask-Good-Question (AGQ)
framework, which features an improved Concept-Enhanced Item Response Theory
(CEIRT) model to better identify users' knowledge levels. Our contributions
include applying the CEIRT model along with LLMs to directly generate guiding
questions based on the inspiring text, greatly improving information retrieval
efficiency during the question & answer process. Through comparisons with other
baseline methods, our approach outperforms by significantly enhencing the
users' information retrieval experiences.

</details>


### [32] [Beyond Pass@1: Self-Play with Variational Problem Synthesis Sustains RLVR](https://arxiv.org/abs/2508.14029)
*Xiao Liang,Zhongzhi Li,Yeyun Gong,Yelong Shen,Ying Nian Wu,Zhijiang Guo,Weizhu Chen*

Main category: cs.CL

TL;DR: 提出SvS策略，通过自对弈和变分问题合成来维持RLVR训练中的策略熵，显著提升Pass@k性能


<details>
  <summary>Details</summary>
Motivation: 传统RLVR训练在提升Pass@1性能的同时会导致策略熵下降，降低生成多样性并限制Pass@k性能

Method: 在线自对弈变分问题合成策略，利用策略的正确解合成变分问题，保持参考答案不变

Result: 在AIME24和AIME25基准上Pass@32性能分别提升18.3%和22.8%，在3B到32B不同规模模型的12个推理基准上表现一致

Conclusion: SvS策略能有效维持训练过程中的策略熵，持续提升性能，具有良好的通用性和鲁棒性

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as
a key paradigm for post-training Large Language Models (LLMs), particularly for
complex reasoning tasks. However, vanilla RLVR training has been shown to
improve Pass@1 performance at the expense of policy entropy, leading to reduced
generation diversity and limiting the Pass@k performance, which typically
represents the upper bound of LLM reasoning capability. In this paper, we
systematically analyze the policy's generation diversity from the perspective
of training problems and find that augmenting and updating training problems
helps mitigate entropy collapse during training. Based on these observations,
we propose an online Self-play with Variational problem Synthesis (SvS)
strategy for RLVR training, which uses the policy's correct solutions to
synthesize variational problems while ensuring their reference answers remain
identical to the originals. This self-improving strategy effectively maintains
policy entropy during training and substantially improves Pass@k compared with
standard RLVR, sustaining prolonged improvements and achieving absolute gains
of 18.3% and 22.8% in Pass@32 performance on the competition-level AIME24 and
AIME25 benchmarks. Experiments on 12 reasoning benchmarks across varying model
sizes from 3B to 32B consistently demonstrate the generalizability and
robustness of SvS.

</details>


### [33] [Unintended Misalignment from Agentic Fine-Tuning: Risks and Mitigation](https://arxiv.org/abs/2508.14031)
*Dongyoon Hahm,Taywon Min,Woogyeol Jin,Kimin Lee*

Main category: cs.CL

TL;DR: LLM智能体微调过程中存在安全隐患，可能导致模型更容易执行有害任务。本文提出PING方法，通过在响应前添加自动生成的自然语言前缀来引导模型拒绝有害请求，同时保持良性任务性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在向智能体系统演进时，通过特定任务微调提升能力，但安全考量常被忽视。研究发现对齐的LLM在微调后可能意外失准，更易执行有害任务且拒绝倾向降低。

Method: 提出Prefix INjection Guard (PING)方法：采用迭代方法交替生成候选前缀并选择那些能同时优化任务性能和拒绝行为的前缀，将其预置到智能体响应中。

Result: 实验表明PING显著提升了微调LLM智能体的安全性而不牺牲有效性，在网页导航和代码生成任务中 consistently优于现有提示方法。线性探针分析显示前缀标记对行为修改至关重要。

Conclusion: PING是一种简单有效的解决方案，通过自然语言前缀注入来保障LLM智能体在微调过程中的安全性，为解决智能体系统安全挑战提供了可行途径。

Abstract: Beyond simple text generation, Large Language Models (LLMs) have evolved into
agentic systems capable of planning and interacting with external tools to
solve complex tasks. This evolution involves fine-tuning LLMs on agent-specific
tasks to enhance their proficiency. However, safety concerns are frequently
overlooked during this fine-tuning process. In this work, we show that aligned
LLMs can become unintentionally misaligned, leading to a higher likelihood of
executing harmful tasks and a reduced tendency to refuse them when fine-tuned
to execute agentic tasks. To address these safety challenges, we propose Prefix
INjection Guard (PING), a simple yet effective method that prepends
automatically generated natural language prefixes to agent responses, guiding
them to refuse harmful requests while preserving performance on benign tasks.
Specifically, we introduce an iterative approach that alternates between (1)
generating candidate prefixes and (2) selecting those that optimize both task
performance and refusal behavior. Experimental results demonstrate that PING
significantly enhances the safety of fine-tuned LLM agents without sacrificing
their effectiveness. PING consistently outperforms existing prompting
approaches across diverse benchmarks in both web navigation and code generation
tasks. Our analysis of internal hidden states via linear probes reveals that
prefix tokens are crucial for behavior modification, explaining the performance
gains. WARNING: This paper contains contents that are unethical or offensive in
nature.

</details>


### [34] [The Promise of Large Language Models in Digital Health: Evidence from Sentiment Analysis in Online Health Communities](https://arxiv.org/abs/2508.14032)
*Xiancheng Li,Georgios D. Karampatakis,Helen E. Wood,Chris J. Griffiths,Borislava Mihaylova,Neil S. Coulson,Alessio Pasinato,Pietro Panzarasa,Marco Viviani,Anna De Simoni*

Main category: cs.CL

TL;DR: 这篇论文探索了大语言模型如何通过上下文学习集成专家知识来解决数字健康分析中的情感分析挑战，在缺乏训练数据和隐私限制的情况下实现了专家级别的性能表现。


<details>
  <summary>Details</summary>
Motivation: 数字健康分析面临专家知识稀缺、数据短缺和隐私限制的挑战，特别是在在线健康社区中的混合情感帖子、临床术语和隐含情感表达需要专业知识进行准确情感分析。

Method: 研究开发了一个结构化的代码本，系统地编码专家解释指南，通过有针对性的提示让LLM应用领域特定知识，而非大规模训练。使用6个GPT模型、DeepSeek和LLaMA 3.1，与预训练语言模型（BioBERT变体）和词典基础方法进行比较，使用400个专家注释的帖子。

Result: LLM实现了更优异的性能，并展现出专家级别的一致性。这种高一致性没有统计学上显著的差异，跟专家之间的一致性水平相当，表明了超越表面级别模式识别的知识集成。

Conclusion: 通过上下文学习支持的一致性表现为数字健康分析提供了有前景的解决方案，能够解决数字健康研究中专家知识短缺的关键挑战，为患者监测、干预评估和基于证据的健康策略提供实时的专家级分析。

Abstract: Digital health analytics face critical challenges nowadays. The sophisticated
analysis of patient-generated health content, which contains complex emotional
and medical contexts, requires scarce domain expertise, while traditional ML
approaches are constrained by data shortage and privacy limitations in
healthcare settings. Online Health Communities (OHCs) exemplify these
challenges with mixed-sentiment posts, clinical terminology, and implicit
emotional expressions that demand specialised knowledge for accurate Sentiment
Analysis (SA). To address these challenges, this study explores how Large
Language Models (LLMs) can integrate expert knowledge through in-context
learning for SA, providing a scalable solution for sophisticated health data
analysis. Specifically, we develop a structured codebook that systematically
encodes expert interpretation guidelines, enabling LLMs to apply
domain-specific knowledge through targeted prompting rather than extensive
training. Six GPT models validated alongside DeepSeek and LLaMA 3.1 are
compared with pre-trained language models (BioBERT variants) and lexicon-based
methods, using 400 expert-annotated posts from two OHCs. LLMs achieve superior
performance while demonstrating expert-level agreement. This high agreement,
with no statistically significant difference from inter-expert agreement
levels, suggests knowledge integration beyond surface-level pattern
recognition. The consistent performance across diverse LLM models, supported by
in-context learning, offers a promising solution for digital health analytics.
This approach addresses the critical challenge of expert knowledge shortage in
digital health research, enabling real-time, expert-quality analysis for
patient monitoring, intervention assessment, and evidence-based health
strategies.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [35] [X-MoE: Enabling Scalable Training for Emerging Mixture-of-Experts Architectures on HPC Platforms](https://arxiv.org/abs/2508.13337)
*Yueming Yuan,Ahan Gupta,Jianping Li,Sajal Dash,Feiyi Wang,Minjia Zhang*

Main category: cs.LG

TL;DR: X-MoE是一个新型的MoE训练系统，通过跨平台内核、冗余绕过调度和混合并行等技术，在AMD GPU上实现了DeepSeek风格MoE模型的高效训练，可扩展到5450亿参数。


<details>
  <summary>Details</summary>
Motivation: 现有MoE架构存在激活内存开销大、通信成本高的问题，且当前训练系统主要针对NVIDIA GPU优化，在非NVIDIA平台上性能不佳，未能充分利用计算潜力。

Method: 采用padding-free MoE训练、跨平台内核、冗余绕过调度和序列分片MoE块的混合并行等技术。

Result: 在Frontier超算（AMD MI250X GPU）上，X-MoE可将DeepSeek风格MoE扩展到1024个GPU上的5450亿参数，比现有方法大10倍，同时保持高训练吞吐量。

Conclusion: X-MoE系统成功解决了MoE训练在非NVIDIA平台上的可扩展性问题，为下一代MoE架构提供了高效的训练解决方案。

Abstract: Emerging expert-specialized Mixture-of-Experts (MoE) architectures, such as
DeepSeek-MoE, deliver strong model quality through fine-grained expert
segmentation and large top-k routing. However, their scalability is limited by
substantial activation memory overhead and costly all-to-all communication.
Furthermore, current MoE training systems - primarily optimized for NVIDIA GPUs
- perform suboptimally on non-NVIDIA platforms, leaving significant
computational potential untapped. In this work, we present X-MoE, a novel MoE
training system designed to deliver scalable training performance for
next-generation MoE architectures. X-MoE achieves this via several novel
techniques, including efficient padding-free MoE training with cross-platform
kernels, redundancy-bypassing dispatch, and hybrid parallelism with
sequence-sharded MoE blocks. Our evaluation on the Frontier supercomputer,
powered by AMD MI250X GPUs, shows that X-MoE scales DeepSeek-style MoEs up to
545 billion parameters across 1024 GPUs - 10x larger than the largest trainable
model with existing methods under the same hardware budget, while maintaining
high training throughput. The source code of X-MoE is available at
https://github.com/Supercomputing-System-AI-Lab/X-MoE.

</details>


### [36] [Input Time Scaling](https://arxiv.org/abs/2508.13654)
*Rapheal Huang,Weilong Guo*

Main category: cs.LG

TL;DR: 输入时间缩放新范式，通过在训练和测试时使用不同的查询策略精炼输入，反转"垃圾输入垃圾输出"假设，在AIME测试中达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 补充当前LLM数据缩放和推理缩放的不足，探索在查询输入时间资源投入的新缩放模式

Method: 在训练和测试阶段都使用元知识精炼输入的策略，发现训练-测试协同设计的重要性，并使用表面低质量数据集

Result: 在Qwen2.5-32B-Instruct模型上达到AIME24(76.7%)和AIME25(76.7%)的SOTA性能，多模型投票可达AIME25(80%)，DeepSeek-R1-Distill-Qwen-32B最高达AIME24(86.7%)

Conclusion: 输入时间缩放是有效的新范式，"少而精"现象应得重视，数据质量的传统观念需要重新考虑，数据集大小缩放也需谨慎对待

Abstract: Current Large Language Models (LLMs) are usually post-trained on large-scale
carefully curated datasets (data & training scaling) and doing reasoning in
test time (inference time scaling). In this work, we present a new scaling
paradigm, Input Time Scaling, to complement previous scaling methods by putting
resources on queries (input time). During training and testing, we combine
meta-knowledge from LLMs to refine inputs with different strategies. We also
find a new phenomenon, training-testing co-design there. We need to apply query
strategies during both training and testing. Only applying strategies on
training or testing would seriously degrade the performance. We are also
surprised to find that seemingly low data quality datasets can gain high
performance. Adding irrelevant information to the queries, randomly selecting
examples from a minimally filtered dataset, can even perform the best. These
findings contradict the widely held inductive bias, "garbage in, garbage out".
Curating datasets with seemingly high-quality data can even potentially limit
the performance ceiling. In addition, models trained on more data with similar
quality (15k VS 1k) perform worse, simple dataset size scaling should also be
carefully inspected. The good news is that our findings are compatible with the
Less is More phenomenon. A small set of examples is enough to evoke high-level
reasoning ability. With experiments on models trained on Qwen2.5-32B-Instruct,
we are able to reach SOTA performance among 32B models on AIME24(76.7%) and
AIME25(76.7%) pass@1. We can further achieve AIME24(76.7%) and AIME25(80%) with
a majority vote of three models. Starting from DeepSeek-R1-Distill-Qwen-32B,
the best result would be 86.7% on AIME24 and 76.7% on AIME25. To facilitate
reproducibility and further research, we are working on open-source our
datasets, data pipelines, evaluation results, and checkpoints.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [37] [Chain-of-Agents: End-to-End Agent Foundation Models via Multi-Agent Distillation and Agentic RL](https://arxiv.org/abs/2508.13167)
*Weizhen Li,Jianbo Lin,Zhuosong Jiang,Jingyi Cao,Xinpeng Liu,Jiayu Zhang,Zhenqiang Huang,Qianben Chen,Weichen Sun,Qiexiang Wang,Hongxuan Lu,Tianrui Qin,Chenghao Zhu,Yi Yao,Shuying Fan,Xiaowan Li,Tiannan Wang,Pai Liu,King Zhu,He Zhu,Dingfeng Shi,Piaohong Wang,Yeyi Guan,Xiangru Tang,Minghao Liu,Yuchen Eleanor Jiang,Jian Yang,Jiaheng Liu,Ge Zhang,Wangchunshu Zhou*

Main category: cs.AI

TL;DR: 链式机器人框架(CoA)通过多机器萃取和机器人强化学习，在单模型中实现精细多机器协作能力，创造了多个基准测试的新最高绩效。


<details>
  <summary>Details</summary>
Motivation: 现有多机器系统定制化程度高，计算效率低，无法从数据中学习提升，需要一种本地化的端到端复杂问题解决方案。

Method: 提出链式机器人(CoA)范式，通过多机器萃取框架将先进多机器系统转换为训练数据，进行机器人监督微调，然后使用机器人强化学习进一步提升模型能力。

Result: 形成的机器人基础模型(AFMs)在web agent和code agent多个基准测试中创造了新的最高绩效记录。

Conclusion: CoA框架为机器人模型研究提供了坚实基础，完整开源了模型权重、训练代码和训练数据，推动了机器人学习领域的发展。

Abstract: Recent advances in large language models (LLMs) and multi-agent systems have
demonstrated remarkable capabilities in complex problem-solving tasks such as
deep research, vibe coding, and mathematical reasoning. However, most existing
multi-agent systems are built upon manual prompt/workflow engineering with
sophisticated agent frameworks, making them computationally inefficient, less
capable, and can not benefit from data-centric learning. In this work, we
introduce Chain-of-Agents (CoA), a novel paradigm of LLM reasoning that enables
native end-to-end complex problem-solving in the same way as a multi-agent
system (i.e., multi-turn problem solving with multiple tools and multiple
agents) within one model. In chain-of-agents problem-solving, the model
dynamically activates different tool agents and role-playing agents to simulate
multi-agent collaboration in an end-to-end fashion. To elicit end-to-end
chain-of-agents problem-solving abilities in LLMs, we introduce a multi-agent
distillation framework to distill state-of-the-art multi-agent systems into
chain-of-agents trajectories for agentic supervised fine-tuning. We then use
agentic reinforcement learning on verifiable agentic tasks to further improve
the models' capabilities on chain-of-agents problem solving. We call the
resulting models Agent Foundation Models (AFMs). Our empirical studies
demonstrate that AFM establishes new state-of-the-art performance across
diverse benchmarks in both web agent and code agent settings. We make the
entire research, including the model weights, code for training and evaluation,
and the training data, fully open-sourced, which offers a solid starting point
for future research on agent models and agentic RL.

</details>


### [38] [Cognitive Workspace: Active Memory Management for LLMs -- An Empirical Study of Functional Infinite Context](https://arxiv.org/abs/2508.13171)
*Tao An*

Main category: cs.AI

TL;DR: 提出了Cognitive Workspace新范式，通过模拟人类外部记忆机制，超越传统RAG方法，实现58.6%的内存重用率和17-18%的净效率提升


<details>
  <summary>Details</summary>
Motivation: 现有LLM在上下文管理方面存在根本性限制，即使上下文窗口扩展到百万token，仍缺乏人类认知的动态任务驱动特性

Method: 基于认知科学理论（Baddeley工作记忆模型、Clark扩展心智理论等），提出三个核心创新：主动记忆管理、分层认知缓冲区和任务驱动上下文优化

Result: 实证验证显示平均58.6%内存重用率（传统RAG为0%），净效率增益17-18%，统计显著性p<0.001，Cohen's d>23

Conclusion: Cognitive Workspace代表了从信息检索到真正认知增强的根本性转变，为LLM系统提供了首个主动记忆优越性的量化证据

Abstract: Large Language Models (LLMs) face fundamental limitations in context
management despite recent advances extending context windows to millions of
tokens. We propose Cognitive Workspace, a novel paradigm that transcends
traditional Retrieval-Augmented Generation (RAG) by emulating human cognitive
mechanisms of external memory use. Drawing from cognitive science foundations
including Baddeley's working memory model, Clark's extended mind thesis, and
Hutchins' distributed cognition framework, we demonstrate that current passive
retrieval systems fail to capture the dynamic, task-driven nature of human
memory management. Our analysis of 2024-2025 developments reveals that while
techniques like Infini-attention and StreamingLLM achieve impressive context
lengths, they lack the metacognitive awareness and active planning capabilities
essential for true cognitive extension. Cognitive Workspace addresses these
limitations through three core innovations: (1) active memory management with
deliberate information curation, (2) hierarchical cognitive buffers enabling
persistent working states, and (3) task-driven context optimization that
dynamically adapts to cognitive demands. Empirical validation demonstrates
Cognitive Workspace achieves an average 58.6% memory reuse rate (ranging from
54-60% across different tasks) compared to 0% for traditional RAG, with 17-18%
net efficiency gain despite 3.3x higher operation counts. Statistical analysis
confirms these advantages with p < 0.001 and Cohen's d > 23 across multiple
task types, establishing the first quantitative evidence for active memory
superiority in LLM systems. We present a comprehensive theoretical framework
synthesizing insights from 50+ recent papers, positioning Cognitive Workspace
as a fundamental shift from information retrieval to genuine cognitive
augmentation.

</details>


### [39] [The Interpretability Analysis of the Model Can Bring Improvements to the Text-to-SQL Task](https://arxiv.org/abs/2508.13178)
*Cong Zhang*

Main category: cs.AI

TL;DR: CESQL模型通过集成模型可解释性分析和执行引导策略，结合过滤调整、逻辑关联优化和模型融合，显著提升了WikiSQL数据集上WHERE子句条件值预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 提升文本到SQL模型在真实应用中的基础能力和泛化能力，特别是在处理WHERE子句语义解析时减少对条件列数据和人工标注训练数据的依赖。

Method: 集成模型可解释性分析与执行引导策略，采用过滤调整、逻辑关联优化和模型融合技术，设计条件增强的CESQL模型。

Result: 在WikiSQL数据集上表现优异，显著提高了预测结果的准确性，特别是在WHERE子句条件值预测方面取得了突破。

Conclusion: 该研究为处理复杂查询和真实数据库环境中不规则数据场景的研究提供了新的视角和方法基础。

Abstract: To elevate the foundational capabilities and generalization prowess of the
text-to-SQL model in real-world applications, we integrate model
interpretability analysis with execution-guided strategy for semantic parsing
of WHERE clauses in SQL queries. Furthermore, we augment this approach with
filtering adjustments, logical correlation refinements, and model fusion,
culminating in the design of the CESQL model that facilitates conditional
enhancement. Our model excels on the WikiSQL dataset, which is emblematic of
single-table database query tasks, markedly boosting the accuracy of prediction
outcomes. When predicting conditional values in WHERE clauses, we have not only
minimized our dependence on data within the condition columns of tables but
also circumvented the impact of manually labeled training data. Our hope is
that this endeavor to enhance accuracy in processing basic database queries
will offer fresh perspectives for research into handling complex queries and
scenarios featuring irregular data in real-world database environments.

</details>


### [40] [Explicit v.s. Implicit Memory: Exploring Multi-hop Complex Reasoning Over Personalized Information](https://arxiv.org/abs/2508.13250)
*Zeyu Zhang,Yang Zhang,Haoran Tan,Rui Li,Xu Chen*

Main category: cs.AI

TL;DR: 本文提出了多跳个性化推理任务，研究不同记忆机制在个性化信息多跳推理中的表现，构建了数据集和评估框架，实现了多种显式和隐式记忆方法，并提出了结合两者的HybridMem方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的智能体记忆方法主要关注偏好对齐和简单问答，但在现实复杂任务中需要进行大量用户信息的多跳推理，当前记忆方法面临重大挑战。

Method: 明确定义多跳个性化推理任务，构建数据集和统一评估框架，实现各种显式和隐式记忆方法，探索混合方法并提出HybridMem方法。

Result: 通过全面实验从多角度评估了不同记忆方法的性能，分析了各自的优缺点，证明了所提模型的有效性。

Conclusion: 该研究为解决个性化信息多跳推理问题提供了系统性的任务定义、评估框架和方法探索，HybridMem方法有效结合了显式和隐式记忆的优势。

Abstract: In large language model-based agents, memory serves as a critical capability
for achieving personalization by storing and utilizing users' information.
Although some previous studies have adopted memory to implement user
personalization, they typically focus on preference alignment and simple
question-answering. However, in the real world, complex tasks often require
multi-hop reasoning on a large amount of user information, which poses
significant challenges for current memory approaches. To address this
limitation, we propose the multi-hop personalized reasoning task to explore how
different memory mechanisms perform in multi-hop reasoning over personalized
information. We explicitly define this task and construct a dataset along with
a unified evaluation framework. Then, we implement various explicit and
implicit memory methods and conduct comprehensive experiments. We evaluate
their performance on this task from multiple perspectives and analyze their
strengths and weaknesses. Besides, we explore hybrid approaches that combine
both paradigms and propose the HybridMem method to address their limitations.
We demonstrate the effectiveness of our proposed model through extensive
experiments. To benefit the research community, we release this project at
https://github.com/nuster1128/MPR.

</details>


### [41] [TASER: Table Agents for Schema-guided Extraction and Recommendation](https://arxiv.org/abs/2508.13404)
*Nicole Cho,Kirsty Fielding,William Watson,Sumitra Ganesh,Manuela Veloso*

Main category: cs.AI

TL;DR: TASER是一个持续学习的智能表格提取系统，专门处理现实世界中高度非结构化的多页金融表格，通过模式引导的提取和推荐机制，在表格检测性能上超越现有模型10.1%，并能显著提升提取效果。


<details>
  <summary>Details</summary>
Motivation: 现实金融文档中的表格信息往往被埋藏在混乱、多页、碎片化的表格中（99.4%的表格没有边界框），传统方法难以有效处理这些高度非结构化的金融表格数据。

Method: 开发了TASER系统，包含表格检测、分类、提取和推荐代理，利用初始模式执行任务，然后通过推荐代理审查输出、建议模式修订并决定最终推荐，实现持续学习。

Result: TASER在表格检测上比Table Transformer模型提升10.1%；更大的批量大小使可操作的模式建议增加104.3%，提取的持仓量增加9.8%；构建了包含22,584页、3,213个表格的真实金融数据集。

Conclusion: 基于代理的模式引导提取系统在理解现实世界金融表格方面展现出巨大潜力，持续学习过程对提升提取效果至关重要，发布的TASERTab数据集将为研究社区提供宝贵的真实金融表格资源。

Abstract: Real-world financial documents report essential information about an entity's
financial holdings that can span millions of different financial instrument
types. Yet, these details are often buried in messy, multi-page, fragmented
tables - for example, 99.4% of the tables in our dataset have no bounding boxes
with the maximum number of rows amounting to 426 per table across 44 pages. To
tackle these unique challenges from real-world tables, we present a
continuously learning, agentic table extraction system, TASER (Table Agents for
Schema-guided Extraction and Recommendation) that extracts highly unstructured,
multi-page, heterogeneous tables into normalized, schema-conforming outputs.
Our table agents execute on table detection, classification, extraction, and
recommendations by leveraging an initial schema. Then, our Recommender Agent
reviews the outputs, recommends schema revisions, and decides on the final
recommendations, enabling TASER to outperform existing table detection models
such as Table Transformer by 10.1%. Within this continuous learning process, we
highlight that larger batch sizes result in a 104.3% increase in schema
recommendations that are actionable and utilized, resulting in a 9.8% increase
in extracted holdings - highlighting the importance of a continuous learning
process. To train TASER, we have manually labeled 22,584 pages (28,150,449
tokens), 3,213 tables for $731,685,511,687 of holdings culminating in one of
the first real financial table datasets. We release our dataset TASERTab to
enable the research community to access real-world financial tables and
outputs. Our results highlight the promise of agentic, schema-guided extraction
systems for robust understanding of real-world financial tables.

</details>


### [42] [Improved Generalized Planning with LLMs through Strategy Refinement and Reflection](https://arxiv.org/abs/2508.13876)
*Katharina Stein,Nils Hodel,Daniel Fišer,Jörg Hoffmann,Michael Katz,Alexander Koller*

Main category: cs.AI

TL;DR: 通过代码调试和反思步骤改进LLM生成广义计划的质量，在17个基准领域中显著提升性能


<details>
  <summary>Details</summary>
Motivation: 解决之前方法中如果策略错误导致整个广义计划失败的问题，需要在生成Python程序前就能发现和修正错误

Method: 使用伪代码形式生成策略并自动调试，扩展Python调试阶段加入反思步骤，生成多个程序变体并选择最佳解

Result: 在17个基准领域中显著提高了广义计划的质量，在12个领域中最佳Python程序能解决所有可生成的任务

Conclusion: 通过前置伪代码调试、反思步骤和多重程序选择等方法，有效提升了LLM生成广义计划的可靠性和性能

Abstract: LLMs have recently been used to generate Python programs representing
generalized plans in PDDL planning, i.e., plans that generalize across the
tasks of a given PDDL domain. Previous work proposed a framework consisting of
three steps: the LLM first generates a summary and then a strategy for the
domain, both in natural language, and then implements that strategy as a Python
program, that gets debugged on example planning tasks. In that work, only one
strategy is generated and passed directly to the program generation. If the
strategy is incorrect, its implementation will therefore result in an incorrect
generalized plan. Here, we introduce an approach that generates the strategy in
the form of pseudocode and enables automatic debugging of the pseudocode, hence
allowing us to identify and fix errors prior to the generation of the
generalized plan itself. Additionally, we extend the Python debugging phase
with a reflection step prompting the LLM to pinpoint the reason for the
observed plan failure. Finally, we take inspiration from LLM code generation to
produce several program variants and pick the best one. Running experiments on
17 benchmark domains, we show that these extensions substantially improve (and
never deteriorate) the quality of the generalized plans. In 12 of the domains,
our best Python programs solve all tasks that can be generated with the
respective instance generator.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [43] [Structured Prompting and Multi-Agent Knowledge Distillation for Traffic Video Interpretation and Risk Inference](https://arxiv.org/abs/2508.13439)
*Yunxiang Yang,Ningning Xu,Jidong J. Yang*

Main category: cs.CV

TL;DR: 这篇论文提出了一种新的结构化提示和知识蓄簍框架，通过GPT-4o和o3-mini两个大型视觉-语言模型生成高质量交通场景注释，然后训练出细小的3B参数模型VISTA，能够在低分辨率视频中进行风险识别和场景理解。


<details>
  <summary>Details</summary>
Motivation: 传统方法在复杂动态的真实交通环境中遇到扩展性和通用性挑战，需要一种能够自动生成高质量注释和风险评估的解决方案。

Method: 使用结构化链式思维(CoT)策略，调度两个大型VLM模型(GPT-4o和o3-mini)生成多视角输出，作为知识富雅的伪注释用于监督精细调整更小的学生VLM模型。

Result: 训练出的3B参数模型VISTA在BLEU-4、METEOR、ROUGE-L和CIDEr等标准指标上表现优异，虽然参数量大幅减少但性能接近教师模型。

Conclusion: 知识蓄簍和结构化多代理监督能够让轻量级VLM拥有复杂推理能力，VISTA的细小结构便于在边缘设备上部署，实现实时风险监控而无需大规模基础设施升级。

Abstract: Comprehensive highway scene understanding and robust traffic risk inference
are vital for advancing Intelligent Transportation Systems (ITS) and autonomous
driving. Traditional approaches often struggle with scalability and
generalization, particularly under the complex and dynamic conditions of
real-world environments. To address these challenges, we introduce a novel
structured prompting and knowledge distillation framework that enables
automatic generation of high-quality traffic scene annotations and contextual
risk assessments. Our framework orchestrates two large Vision-Language Models
(VLMs): GPT-4o and o3-mini, using a structured Chain-of-Thought (CoT) strategy
to produce rich, multi-perspective outputs. These outputs serve as
knowledge-enriched pseudo-annotations for supervised fine-tuning of a much
smaller student VLM. The resulting compact 3B-scale model, named VISTA (Vision
for Intelligent Scene and Traffic Analysis), is capable of understanding
low-resolution traffic videos and generating semantically faithful, risk-aware
captions. Despite its significantly reduced parameter count, VISTA achieves
strong performance across established captioning metrics (BLEU-4, METEOR,
ROUGE-L, and CIDEr) when benchmarked against its teacher models. This
demonstrates that effective knowledge distillation and structured multi-agent
supervision can empower lightweight VLMs to capture complex reasoning
capabilities. The compact architecture of VISTA facilitates efficient
deployment on edge devices, enabling real-time risk monitoring without
requiring extensive infrastructure upgrades.

</details>


### [44] [RotBench: Evaluating Multimodal Large Language Models on Identifying Image Rotation](https://arxiv.org/abs/2508.13968)
*Tianyi Niu,Jaemin Cho,Elias Stengel-Eskin,Mohit Bansal*

Main category: cs.CV

TL;DR: 多模态大语言模型在图片旋转识别任务上表现差强，无法可靠区分90度和270度旋转，显示了与人类视觉理解能力的显著差距


<details>
  <summary>Details</summary>
Motivation: 评估多模态大语言模型的视觉理解能力，特别是在图片旋转识别这种需要稳健空间推理能力的任务上

Method: 构建RotBench标准化测试集（350张图片），测试多个独立和商业MLLM模型，使用辅助信息和链式推理等方法进行优化

Result: 模型能可靠识别正立和倒立图片，但无法区分90度和270度旋转，细调不能改善这种能力缺陷

Conclusion: MLLM模型在空间理解方面存在显著缺陷，需要更深入的研究来提升其视觉理解能力

Abstract: We investigate to what extent Multimodal Large Language Models (MLLMs) can
accurately identify the orientation of input images rotated 0{\deg}, 90{\deg},
180{\deg}, and 270{\deg}. This task demands robust visual reasoning
capabilities to detect rotational cues and contextualize spatial relationships
within images, regardless of their orientation. To evaluate MLLMs on these
abilities, we introduce RotBench -- a 350-image manually-filtered benchmark
comprising lifestyle, portrait, and landscape images. Despite the relatively
simple nature of this task, we show that several state-of-the-art open and
proprietary MLLMs, including GPT-5, o3, and Gemini-2.5-Pro, do not reliably
identify rotation in input images. Providing models with auxiliary information
-- including captions, depth maps, and more -- or using chain-of-thought
prompting offers only small and inconsistent improvements. Our results indicate
that most models are able to reliably identify right-side-up (0{\deg}) images,
while certain models are able to identify upside-down (180{\deg}) images. None
can reliably distinguish between 90{\deg} and 270{\deg}. Simultaneously showing
the image rotated in different orientations leads to moderate performance gains
for reasoning models, while a modified setup using voting improves the
performance of weaker models. We further show that fine-tuning does not improve
models' ability to distinguish 90{\deg} and 270{\deg} rotations, despite
substantially improving the identification of 180{\deg} images. Together, these
results reveal a significant gap between MLLMs' spatial reasoning capabilities
and human perception in identifying rotation.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [45] [Prompt Orchestration Markup Language](https://arxiv.org/abs/2508.13948)
*Yuge Zhang,Nan Chen,Jiahang Xu,Yuqing Yang*

Main category: cs.HC

TL;DR: POML是一种基于标记语言的新型提示编排语言，通过组件化结构、数据集成标签和CSS样式系统，解决了LLM提示工程中的结构组织、数据整合和格式敏感性问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLM提示工程面临结构组织困难、多种数据类型集成复杂、格式敏感性强以及工具支持不足等挑战，需要一种系统化的解决方案。

Method: 提出POML标记语言，采用基于组件的逻辑结构（角色、任务、示例）、专用数据集成标签、CSS样式系统分离内容与呈现，并提供模板化和完整的开发者工具套件。

Result: 通过两个案例研究（PomLink应用集成和TableQA准确性测试）以及用户研究验证了POML在复杂应用集成和准确性提升方面的有效性。

Conclusion: POML为LLM提示工程提供了系统化的解决方案，显著改善了提示的组织结构、数据集成能力和开发效率，在真实开发场景中表现出良好的实用性。

Abstract: Large Language Models (LLMs) require sophisticated prompting, yet current
practices face challenges in structure, data integration, format sensitivity,
and tooling. Existing methods lack comprehensive solutions for organizing
complex prompts involving diverse data types (documents, tables, images) or
managing presentation variations systematically. To address these gaps, we
introduce POML (Prompt Orchestration Markup Language). POML employs
component-based markup for logical structure (roles, tasks, examples),
specialized tags for seamless data integration, and a CSS-like styling system
to decouple content from presentation, reducing formatting sensitivity. It
includes templating for dynamic prompts and a comprehensive developer toolkit
(IDE support, SDKs) to improve version control and collaboration. We validate
POML through two case studies demonstrating its impact on complex application
integration (PomLink) and accuracy performance (TableQA), as well as a user
study assessing its effectiveness in real-world development scenarios.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [46] [Combating Homelessness Stigma with LLMs: A New Multi-Modal Dataset for Bias Detection](https://arxiv.org/abs/2508.13187)
*Jonathan A. Karr Jr.,Benjamin F. Herbst,Ting Hua,Matthew Hauenstein,Georgina Curto,Nitesh V. Chawla*

Main category: cs.CY

TL;DR: 这篇论文提出了一种利用自然语言处理和大语言模型来识别和测量网络讨论中对无家归者的社会偏见的新方法，并创建了一个多模态数据集来支持这项研究。


<details>
  <summary>Details</summary>
Motivation: 无家归者问题是一个持续的社会挑战，社会注册成为缓解该问题的重要障碍。网络和市政谈会讨论反映并影响公众意见，为识别和跟踪社会偏见提供了价值。

Method: 研究创建了一个手动注释的多模态数据集，源自Reddit、X、新闻文章和10个美国城市的市政谈会记录。采用零样本和少样本分类技术，对比了本地LLM模型（Llama 3.2 3B、Qwen 2.5 7B、Phi4⧗和闭源API模型（GPT-4.1、Gemini 2.5 Pro、Grok-4）的表现。

Result: 研究发现尽管本地LLM在零样本分类中存在显著不一致性，但其上下文学习分类结果接近闭源LLM的分类结果。同时，LLM在所有类别上的平均表现超过了BERT模型。

Conclusion: 这项工作旨在提高对无家归者偏见的认识，开发新的指标来为政策提供信息，并最终提高生成式AI技术的公平性和道德应用。

Abstract: Homelessness is a persistent social challenge, impacting millions worldwide.
Over 770,000 people experienced homelessness in the U.S. in 2024. Social
stigmatization is a significant barrier to alleviation, shifting public
perception, and influencing policymaking. Given that online and city council
discourse reflect and influence part of public opinion, it provides valuable
insights to identify and track social biases. This research contributes to
alleviating homelessness by acting on public opinion. It introduces novel
methods, building on natural language processing (NLP) and large language
models (LLMs), to identify and measure PEH social bias expressed in digital
spaces. We present a new, manually-annotated multi-modal dataset compiled from
Reddit, X (formerly Twitter), news articles, and city council meeting minutes
across 10 U.S. cities. This unique dataset provides evidence of the typologies
of homelessness bias described in the literature. In order to scale up and
automate the detection of homelessness bias online, we evaluate LLMs as
classifiers. We applied both zero-shot and few-shot classification techniques
to this data. We utilized local LLMs (Llama 3.2 3B Instruct, Qwen 2.5 7B
Instruct, and Phi4 Instruct Mini) as well as closed-source API models (GPT-4.1,
Gemini 2.5 Pro, and Grok-4). Our findings reveal that although there are
significant inconsistencies in local LLM zero-shot classification, the
in-context learning classification scores of local LLMs approach the
classification scores of closed-source LLMs. Furthermore, LLMs outperform BERT
when averaging across all categories. This work aims to raise awareness about
the pervasive bias against PEH, develop new indicators to inform policy, and
ultimately enhance the fairness and ethical application of Generative AI
technologies.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [47] [White-Box Reasoning: Synergizing LLM Strategy and gm/Id Data for Automated Analog Circuit Design](https://arxiv.org/abs/2508.13172)
*Jianqiu Chen,Siqi Li,Xu He*

Main category: cs.AR

TL;DR: 基于gm/Id方法和LLM协同推理的模拟电路设计框架，实现了超高效的模拟电路自动设计


<details>
  <summary>Details</summary>
Motivation: 传统模拟电路设计依赖经验且模拟效率低，而直接使用大语言模型只能做出猜测而缺乏工程原理

Method: 采用"协同推理"框架，将LLM的战略推理与gm/Id方法的物理精度相结合，通过gm/Id查找表使LLM成为数据驱动的设计合作伙伴

Result: 在两级运放器设计中，框架使Gemini模型在5次迭代内满足所有TT角点规格，并扩展到所有PVT角点优化，效率提升一个数量级

Conclusion: 该研究验证了通过结合LLM推理与科学电路设计方法来实现真正模拟电路设计自动化的路径

Abstract: Analog IC design is a bottleneck due to its reliance on experience and
inefficient simulations, as traditional formulas fail in advanced nodes.
Applying Large Language Models (LLMs) directly to this problem risks mere
"guessing" without engineering principles. We present a "synergistic reasoning"
framework that integrates an LLM's strategic reasoning with the physical
precision of the gm/Id methodology. By empowering the LLM with gm/Id lookup
tables, it becomes a quantitative, data-driven design partner.
  We validated this on a two-stage op-amp, where our framework enabled the
Gemini model to meet all TT corner specs in 5 iterations and extended
optimization to all PVT corners. A crucial ablation study proved gm/Id data is
key for this efficiency and precision; without it, the LLM is slower and
deviates. Compared to a senior engineer's design, our framework achieves
quasi-expert quality with an order-of-magnitude improvement in efficiency. This
work validates a path for true analog design automation by combining LLM
reasoning with scientific circuit design methodologies.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [48] [TaoSR1: The Thinking Model for E-commerce Relevance Search](https://arxiv.org/abs/2508.12365)
*Chenhe Dong,Shaowei Yao,Pengkun Jiao,Jianhui Yang,Yiming Jin,Zerui Huang,Xiaojiang Zhou,Dan Ou,Haihong Tang*

Main category: cs.IR

TL;DR: 提出了TaoSR1框架，直接部署大语言模型进行电商搜索相关性预测，通过三阶段训练和优化方法解决思维链错误累积、判别性幻觉和部署可行性等关键挑战。


<details>
  <summary>Details</summary>
Motivation: BERT模型擅长语义匹配但缺乏复杂推理能力，现有LLM方法多采用判别式微调或蒸馏到小模型，需要直接部署LLM并解决相关挑战。

Method: 三阶段框架：1)SFT+CoT注入推理能力；2)离线采样+DPO提升生成质量；3)难度动态采样+GRPO缓解判别性幻觉。采用后处理技术和概率分区方法实现高效在线部署。

Result: 在离线数据集上显著超越基线模型，在线人工对比评估中获得实质性收益提升。

Conclusion: 为相关性分类任务引入了一种应用CoT推理的新范式，成功解决了LLM直接部署的关键技术挑战。

Abstract: Query-product relevance prediction is a core task in e-commerce search.
BERT-based models excel at semantic matching but lack complex reasoning
capabilities. While Large Language Models (LLMs) are explored, most still use
discriminative fine-tuning or distill to smaller models for deployment. We
propose a framework to directly deploy LLMs for this task, addressing key
challenges: Chain-of-Thought (CoT) error accumulation, discriminative
hallucination, and deployment feasibility. Our framework, TaoSR1, involves
three stages: (1) Supervised Fine-Tuning (SFT) with CoT to instill reasoning;
(2) Offline sampling with a pass@N strategy and Direct Preference Optimization
(DPO) to improve generation quality; and (3) Difficulty-based dynamic sampling
with Group Relative Policy Optimization (GRPO) to mitigate discriminative
hallucination. Additionally, post-CoT processing and a cumulative
probability-based partitioning method enable efficient online deployment.
TaoSR1 significantly outperforms baselines on offline datasets and achieves
substantial gains in online side-by-side human evaluations, introducing a novel
paradigm for applying CoT reasoning to relevance classification.

</details>


### [49] [LLM-Enhanced Linear Autoencoders for Recommendation](https://arxiv.org/abs/2508.13500)
*Jaewan Moon,Seongmin Park,Jongwuk Lee*

Main category: cs.IR

TL;DR: L3AE首次将大语言模型(LLMs)集成到线性自编码器(LAE)框架中，通过两阶段优化策略有效整合文本语义和用户-物品交互的异构知识，在三个基准数据集上显著优于现有最佳模型。


<details>
  <summary>Details</summary>
Motivation: 现有的线性自编码器在整合文本信息时依赖稀疏的词共现模式，无法充分捕捉丰富的文本语义信息，需要利用LLMs的强大语义理解能力来提升推荐系统的性能。

Method: 提出L3AE框架：1) 从LLM生成的物品表示构建语义物品相关性矩阵；2) 从协同信号学习物品权重矩阵，同时将语义物品相关性作为正则化项进行蒸馏。两个阶段都通过闭式解进行优化。

Result: 在三个基准数据集上，L3AE相比最先进的LLM增强模型，Recall@20提升27.6%，NDCG@20提升39.3%，表现出显著的性能优势。

Conclusion: L3AE成功将LLMs集成到LAE框架中，通过有效的异构知识整合和闭式优化策略，显著提升了推荐系统的性能，为LLM在推荐系统中的应用提供了新思路。

Abstract: Large language models (LLMs) have been widely adopted to enrich the semantic
representation of textual item information in recommender systems. However,
existing linear autoencoders (LAEs) that incorporate textual information rely
on sparse word co-occurrence patterns, limiting their ability to capture rich
textual semantics. To address this, we propose L3AE, the first integration of
LLMs into the LAE framework. L3AE effectively integrates the heterogeneous
knowledge of textual semantics and user-item interactions through a two-phase
optimization strategy. (i) L3AE first constructs a semantic item-to-item
correlation matrix from LLM-derived item representations. (ii) It then learns
an item-to-item weight matrix from collaborative signals while distilling
semantic item correlations as regularization. Notably, each phase of L3AE is
optimized through closed-form solutions, ensuring global optimality and
computational efficiency. Extensive experiments demonstrate that L3AE
consistently outperforms state-of-the-art LLM-enhanced models on three
benchmark datasets, achieving gains of 27.6% in Recall@20 and 39.3% in NDCG@20.
The source code is available at https://github.com/jaewan7599/L3AE_CIKM2025.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [50] [Query Logs Analytics: A Aystematic Literature Review](https://arxiv.org/abs/2508.13949)
*Dihia Lanasri*

Main category: cs.DB

TL;DR: 这篇系统性调查日志使用的论文分析了数据库、数据仓库、网站和知识图谦日志的共同特征、标准处理流程以及约束条件，揭示了领域中的研究空白和未来方向。


<details>
  <summary>Details</summary>
Motivation: 数字化平台中用户交互留下的日志具有重要价值，但相关研究分散且缺乏系统性整理，需要一个全面的调查来凑整现有知识。

Method: 分析了300多篇论文，重点研究三个核心问题：不同类型日志的结构功能特征、标准处理流程以及约束条件和非功能性要求。

Result: 调查发现少有完整的端到端方案，日志使用流程缺乏标准化，但不同类型日志存在共享的结构元素。

Conclusion: 该调查为研空人员和实践者提供了日志使用的全面概览，指明了未来研究方向，尤其是知识图谦日志的利用和普及。

Abstract: In the digital era, user interactions with various resources such as
databases, data warehouses, websites, and knowledge graphs (KGs) are
increasingly mediated through digital platforms. These interactions leave
behind digital traces, systematically captured in the form of logs. Logs, when
effectively exploited, provide high value across industry and academia,
supporting critical services (e.g., recovery and security), user-centric
applications (e.g., recommender systems), and quality-of-service improvements
(e.g., performance optimization). Despite their importance, research on log
usage remains fragmented across domains, and no comprehensive study currently
consolidates existing efforts. This paper presents a systematic survey of log
usage, focusing on Database (DB), Data Warehouse (DW), Web, and KG logs. More
than 300 publications were analyzed to address three central questions: (1) do
different types of logs share common structural and functional characteristics?
(2) are there standard pipelines for their usage? (3) which constraints and
non-functional requirements (NFRs) guide their exploitation?. The survey
reveals a limited number of end-to-end approaches, the absence of
standardization across log usage pipelines, and the existence of shared
structural elements among different types of logs. By consolidating existing
knowledge, identifying gaps, and highlighting opportunities, this survey
provides researchers and practitioners with a comprehensive overview of log
usage and sheds light on promising directions for future research, particularly
regarding the exploitation and democratization of KG logs.

</details>
