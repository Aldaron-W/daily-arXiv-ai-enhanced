<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 77]
- [cs.CV](#cs.CV) [Total: 5]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.LG](#cs.LG) [Total: 7]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.SE](#cs.SE) [Total: 1]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.AI](#cs.AI) [Total: 3]
- [cs.CR](#cs.CR) [Total: 1]
- [q-fin.CP](#q-fin.CP) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Contextual Augmentation for Entity Linking using Large Language Models](https://arxiv.org/abs/2510.18888)
*Daniel Vollmers,Hamada M. Zahera,Diego Moussallem,Axel-Cyrille Ngonga Ngomo*

Main category: cs.CL

TL;DR: 提出一种联合实体识别与消歧的统一框架，利用大语言模型丰富实体上下文，在跨领域数据集上达到最优性能。


<details>
  <summary>Details</summary>
Motivation: 传统实体链接方法采用分离的两步流程（实体识别和消歧），计算量大且效果有限，需要更高效的统一方法。

Method: 构建联合实体识别与消歧的统一框架，通过微调模型和大语言模型增强实体上下文表示。

Result: 在基准数据集上评估，与多个基线方法比较，在跨领域数据集上达到最优性能。

Conclusion: 提出的联合框架有效提升了实体链接性能，特别是在跨领域场景下表现优异。

Abstract: Entity Linking involves detecting and linking entity mentions in natural
language texts to a knowledge graph. Traditional methods use a two-step process
with separate models for entity recognition and disambiguation, which can be
computationally intensive and less effective. We propose a fine-tuned model
that jointly integrates entity recognition and disambiguation in a unified
framework. Furthermore, our approach leverages large language models to enrich
the context of entity mentions, yielding better performance in entity
disambiguation. We evaluated our approach on benchmark datasets and compared
with several baselines. The evaluation results show that our approach achieves
state-of-the-art performance on out-of-domain datasets.

</details>


### [2] [Small Language Models Offer Significant Potential for Science Community](https://arxiv.org/abs/2510.18890)
*Jian Zhang*

Main category: cs.CL

TL;DR: 开发了一个使用小型语言模型(MiniLMs)从地球科学文献中精确、快速、低成本检索信息的框架，相比大型语言模型具有计算效率高、能识别专家验证信息等优势。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型在自然语言处理方面有进展，但存在信息偏见和计算成本高的担忧，需要开发更精确、经济的信息检索方法。

Method: 构建了包含约7700万高质量句子的地球科学文献语料库，使用MiniLMs通过语义搜索和句子级索引技术进行高效信息提取，并结合情感分析和无监督聚类分析情感倾向和主题演变。

Result: MiniLMs能够有效识别大量经过专家验证的多学科信息，特别擅长处理定量研究结果，并能追踪地球科学界结论演变、研究重点和新兴问题。

Conclusion: MiniLMs在地球科学社区具有重要应用潜力，可用于事实检索、图像检索、趋势分析、矛盾分析和教育目的。

Abstract: Recent advancements in natural language processing, particularly with large
language models (LLMs), are transforming how scientists engage with the
literature. While the adoption of LLMs is increasing, concerns remain regarding
potential information biases and computational costs. Rather than LLMs, I
developed a framework to evaluate the feasibility of precise, rapid, and
cost-effective information retrieval from extensive geoscience literature using
freely available small language models (MiniLMs). A curated corpus of
approximately 77 million high-quality sentences, extracted from 95 leading
peer-reviewed geoscience journals such as Geophysical Research Letters and
Earth and Planetary Science Letters published during years 2000 to 2024, was
constructed. MiniLMs enable a computationally efficient approach for extracting
relevant domain-specific information from these corpora through semantic search
techniques and sentence-level indexing. This approach, unlike LLMs such as
ChatGPT-4 that often produces generalized responses, excels at identifying
substantial amounts of expert-verified information with established,
multi-disciplinary sources, especially for information with quantitative
findings. Furthermore, by analyzing emotional tone via sentiment analysis and
topical clusters through unsupervised clustering within sentences, MiniLM
provides a powerful tool for tracking the evolution of conclusions, research
priorities, advancements, and emerging questions within geoscience communities.
Overall, MiniLM holds significant potential within the geoscience community for
applications such as fact and image retrievals, trend analyses, contradiction
analyses, and educational purposes.

</details>


### [3] [When Models Can't Follow: Testing Instruction Adherence Across 256 LLMs](https://arxiv.org/abs/2510.18892)
*Richard J. Young,Brandon Gillins,Alice M. Matthews*

Main category: cs.CL

TL;DR: 提出了一个简化的评估框架，使用20个精心设计的提示来评估LLM的指令遵循能力，并通过大规模实证研究测试了256个模型。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模基准测试可能被模型记忆，需要新的评估方法来真实评估指令遵循能力，而不是记忆性能。

Method: 使用20个精心设计的提示构建紧凑测试套件，评估指令遵循的不同方面，包括格式合规性、内容约束、逻辑排序和多步骤任务执行。

Result: 发现了持续存在的失败模式，识别了特定指令类型带来的挑战，并提供了主要提供商和新兴实现模型的比较性能分析。

Conclusion: 这项工作既提供了一个实用的评估工具，又对当代LLM的指令遵循能力进行了最全面的实证分析之一。

Abstract: Despite widespread deployment of Large Language Models, systematic evaluation
of instruction-following capabilities remains challenging. While comprehensive
benchmarks exist, focused assessments that quickly diagnose specific
instruction adherence patterns are valuable. As newer models may be trained on
existing benchmarks, novel evaluation approaches are needed to assess genuine
capabilities rather than memorized performance. This paper presents a
streamlined evaluation framework using twenty carefully designed prompts to
assess LLM instruction-following across diverse task categories. We demonstrate
this framework through a large-scale empirical study conducted on October 14,
2025, testing 256 verified working models from 331 available via OpenRouter. To
ensure methodological rigor and prevent selection bias, we first verified each
model's basic functionality before inclusion. Unlike large-scale benchmarks
requiring extensive computational resources, our approach offers a practical
diagnostic tool researchers and practitioners can readily apply. Our
methodology builds upon verifiable instructions while introducing a compact
test suite balancing comprehensiveness with efficiency. Each prompt targets
distinct aspects of instruction following, including format compliance, content
constraints, logical sequencing, and multi-step task execution. We evaluate
models from major providers (OpenAI, Anthropic, Google, Meta, Mistral) and
emerging implementations (Qwen, DeepSeek, community models), providing
comparative performance analysis. Our findings reveal consistent failure modes
and identify specific instruction types posing particular challenges. This work
contributes both a practical evaluation tool and one of the most comprehensive
empirical analyses of instruction-following capabilities across the
contemporary LLM landscape.

</details>


### [4] [Transformer-Based Low-Resource Language Translation: A Study on Standard Bengali to Sylheti](https://arxiv.org/abs/2510.18898)
*Mangsura Kabir Oni,Tabia Tanzin Prama*

Main category: cs.CL

TL;DR: 该论文研究了孟加拉语到锡尔赫特语的机器翻译，通过微调多语言Transformer模型并与零样本大语言模型进行比较，发现微调模型显著优于LLMs。


<details>
  <summary>Details</summary>
Motivation: 虽然机器翻译在高资源语言上取得了显著进展，但像锡尔赫特语这样的低资源语言仍然研究不足，需要针对性地开发包容性语言技术。

Method: 通过微调多语言Transformer模型（如mBART-50和MarianMT），并与零样本大语言模型进行比较，评估孟加拉语到锡尔赫特语的翻译性能。

Result: 微调模型显著优于大语言模型，其中mBART-50在翻译充分性方面表现最佳，MarianMT在字符级保真度方面最强。

Conclusion: 针对代表性不足的语言，任务特定的适应性调整至关重要，这有助于推动包容性语言技术的发展。

Abstract: Machine Translation (MT) has advanced from rule-based and statistical methods
to neural approaches based on the Transformer architecture. While these methods
have achieved impressive results for high-resource languages, low-resource
varieties such as Sylheti remain underexplored. In this work, we investigate
Bengali-to-Sylheti translation by fine-tuning multilingual Transformer models
and comparing them with zero-shot large language models (LLMs). Experimental
results demonstrate that fine-tuned models significantly outperform LLMs, with
mBART-50 achieving the highest translation adequacy and MarianMT showing the
strongest character-level fidelity. These findings highlight the importance of
task-specific adaptation for underrepresented languages and contribute to
ongoing efforts toward inclusive language technologies.

</details>


### [5] [DuoLens: A Framework for Robust Detection of Machine-Generated Multilingual Text and Code](https://arxiv.org/abs/2510.18904)
*Shriyansh Agrawal,Aidan Lau,Sanyam Shah,Ahan M R,Kevin Zhu,Sunishchal Dev,Vasu Sharma*

Main category: cs.CL

TL;DR: 本文提出通过微调编码器专用的小型语言模型（RoBERTa和CodeBERTa）来检测机器生成内容，相比大型语言模型在计算效率和准确性方面都有显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前基于零样本方法的机器生成内容检测器存在计算成本高或准确性不足的问题，需要在准确性和效率之间做出权衡，因此需要改进方案。

Method: 微调预训练的编码器专用小型语言模型（RoBERTa和CodeBERTa），使用专门的源代码和自然语言数据集进行二元分类任务训练。

Result: 提出的编码器在AUROC达到0.97-0.99，宏F1分数达到0.89-0.94，同时将延迟降低8-12倍，峰值VRAM使用减少3-5倍。在跨生成器转移和对抗性转换下，性能保持≥92%的干净AUROC。

Conclusion: 对于二元分类任务，小型语言模型在计算效率远高于大型语言模型的同时，性能表现也大幅超越，为机器生成内容检测提供了更优的解决方案。

Abstract: The prevalence of Large Language Models (LLMs) for generating multilingual
text and source code has only increased the imperative for machine-generated
content detectors to be accurate and efficient across domains. Current
detectors, predominantly utilizing zero-shot methods, such as Fast DetectGPT or
GPTZero, either incur high computational cost or lack sufficient accuracy,
often with a trade-off between the two, leaving room for further improvement.
To address these gaps, we propose the fine-tuning of encoder-only Small
Language Models (SLMs), in particular, the pre-trained models of RoBERTA and
CodeBERTa using specialized datasets on source code and other natural language
to prove that for the task of binary classification, SLMs outperform LLMs by a
huge margin whilst using a fraction of compute. Our encoders achieve AUROC $=
0.97$ to $0.99$ and macro-F1 $0.89$ to $0.94$ while reducing latency by
$8$-$12\times$ and peak VRAM by $3$-$5\times$ at $512$-token inputs. Under
cross-generator shifts and adversarial transformations (paraphrase,
back-translation; code formatting/renaming), performance retains $\geq 92%$ of
clean AUROC. We release training and evaluation scripts with seeds and configs;
a reproducibility checklist is also included.

</details>


### [6] [Improving Topic Modeling of Social Media Short Texts with Rephrasing: A Case Study of COVID-19 Related Tweets](https://arxiv.org/abs/2510.18908)
*Wangjiaxuan Xin,Shuhua Yin,Shi Chen,Yaorong Ge*

Main category: cs.CL

TL;DR: TM-Rephrase是一个模型无关的框架，利用大语言模型将原始推文重写为更标准化的语言，以提高主题建模在社交媒体短文本分析中的效果。


<details>
  <summary>Details</summary>
Motivation: 社交媒体短文本（如推文）的简洁性、非正式性和噪声限制了传统主题建模的效果，导致产生难以解释的冗余或不连贯主题。

Method: 开发TM-Rephrase框架，使用LLMs对原始推文进行重写，研究两种重写策略（通用重写和口语到正式重写）对多种主题建模方法的影响。

Result: TM-Rephrase提高了主题连贯性、独特性和多样性三个指标，减少了大多数主题建模算法的主题冗余，其中口语到正式重写策略对LDA算法的提升效果最显著。

Conclusion: 该研究为公共卫生相关社交媒体分析提供了一种模型无关的方法来增强主题建模，对改善健康危机及其他重要领域公共话语的理解具有广泛意义。

Abstract: Social media platforms such as Twitter (now X) provide rich data for
analyzing public discourse, especially during crises such as the COVID-19
pandemic. However, the brevity, informality, and noise of social media short
texts often hinder the effectiveness of traditional topic modeling, producing
incoherent or redundant topics that are often difficult to interpret. To
address these challenges, we have developed \emph{TM-Rephrase}, a
model-agnostic framework that leverages large language models (LLMs) to
rephrase raw tweets into more standardized and formal language prior to topic
modeling. Using a dataset of 25,027 COVID-19-related Twitter posts, we
investigate the effects of two rephrasing strategies, general- and
colloquial-to-formal-rephrasing, on multiple topic modeling methods. Results
demonstrate that \emph{TM-Rephrase} improves three metrics measuring topic
modeling performance (i.e., topic coherence, topic uniqueness, and topic
diversity) while reducing topic redundancy of most topic modeling algorithms,
with the colloquial-to-formal strategy yielding the greatest performance gains
and especially for the Latent Dirichlet Allocation (LDA) algorithm. This study
contributes to a model-agnostic approach to enhancing topic modeling in public
health related social media analysis, with broad implications for improved
understanding of public discourse in health crisis as well as other important
domains.

</details>


### [7] [Learning from the Best, Differently: A Diversity-Driven Rethinking on Data Selection](https://arxiv.org/abs/2510.18909)
*Hongyi He,Xiao Liu,Zhenghao Lin,Mingni Tang,Yi Cheng,Jintao Wang,Wenjie Li,Peng Cheng,Yeyun Gong*

Main category: cs.CL

TL;DR: 提出了正交多样性感知选择(ODiS)算法，通过主成分分析将多维评分解相关为正交维度，从每个维度选择高质量数据，确保预训练数据同时具备高质量和多样性。


<details>
  <summary>Details</summary>
Motivation: 现有基于评分的数据选择方法存在维度相关性偏差，导致选择的数据虽然评分高但缺乏多样性，直接选择高分数据反而会降低模型性能。

Method: 1. 从语言质量、知识质量和理解难度等多维度评估数据；2. 使用PCA对多维评分进行解相关得到正交维度；3. 训练Roberta评分器进行大规模推理；4. 从每个正交维度选择高分数据构建训练集。

Result: ODiS选择的数据维度间重叠度小于2%，证实了维度的正交性。使用ODiS选择数据训练的模型在下游基准测试中显著优于其他基线方法。

Conclusion: 正交、多样性感知的数据选择对LLM预训练至关重要，ODiS算法能有效平衡数据质量和多样性。

Abstract: High-quality pre-training data is crutial for large language models, where
quality captures factual reliability and semantic value, and diversity ensures
broad coverage and distributional heterogeneity. Existing approaches typically
rely on single or multiple-dimensional score-based selection. However, directly
selecting top-scored data often degrades performance, and sampling from a
broader range is required to recover results. The above non-monotonicity
between dataset scores and downstream benchmark results reveals a fundamental
bias: score-based methods collapse correlated dimensions, causing top-scored
data to appear high-quality while systematically overlooking diversity. We
argue that ensuring diversity requires decomposing correlated metrics into
orthogonal feature dimensions, from which the top-scored data can be directly
selected. Therefore, we proposed the Orthogonal Diversity-Aware Selection
(ODiS) algorithm, which preserves both quality and diversity during data
selection. First, ODiS evaluates data from multiple dimensions, covering
language quality, knowledge quality, and comprehension difficulty. The
multi-dimensional scores are then decorrelated via Principal Component Analysis
(PCA), yielding orthogonal evaluation dimensions. For each dimension, a
Roberta-based scorer is trained to regress the data onto PCA-projected scores,
enabling scalable inference on large corpora. Finally, ODiS constructs the
training dataset by selecting top-scored data within each orthogonal dimension,
thereby ensuring both quality and diversity. Empirical results show that
ODiS-selected data exhibit less than 2\% inter-dimension overlap, confirming
orthogonality between dimensions. More importantly, models trained with
ODiS-selected data significantly outperform other baselines on downstream
benchmarks, highlighting the necessity of orthogonal, diversity-aware data
selection for LLMs.

</details>


### [8] [Context-aware Fairness Evaluation and Mitigation in LLMs](https://arxiv.org/abs/2510.18914)
*Afrozah Nadeem,Mark Dras,Usman Naseem*

Main category: cs.CL

TL;DR: 提出了一种动态可逆的剪枝框架，通过检测上下文感知的神经元激活并应用自适应掩码来调节生成过程中的影响，实现推理时的细粒度公平性控制。


<details>
  <summary>Details</summary>
Motivation: 大语言模型内部表示中嵌入的不良行为会损害公平性、导致不一致性漂移、放大有害内容以及在长对话中传播不良模式。现有方法计算成本高、不可逆且难以适应新对话上下文。

Method: 动态可逆剪枝框架，检测上下文感知的神经元激活，应用自适应掩码来调节生成过程中的影响，实现推理时的细粒度控制。

Result: 在多语言单轮和多轮对话中保持知识完整性，产生更一致的行为，实现动态公平性控制。

Conclusion: 该方法为现实世界对话AI提供了灵活、透明且可适应的公平性控制解决方案。

Abstract: Large language models often display undesirable behaviors embedded in their
internal representations, undermining fairness, inconsistency drift,
amplification of harmful content, and the propagation of unwanted patterns
during extended dialogue and conversations. Although training-time or
data-centric methods attempt to reduce these effects, they are computationally
expensive, irreversible once deployed, and slow to adapt to new conversational
contexts. Pruning-based methods provide a flexible and transparent way to
reduce bias by adjusting the neurons responsible for certain behaviors.
However, most existing approaches are static; once a neuron is removed, the
model loses the ability to adapt when the conversation or context changes. To
address this, we propose a dynamic, reversible, pruning-based framework that
detects context-aware neuron activations and applies adaptive masking to
modulate their influence during generation. Our inference-time solution
provides fine-grained, memory-aware mitigation with knowledge-preserved, more
coherent behavior across multilingual single- and multi-turn dialogues,
enabling dynamic fairness control in real-world conversational AI.

</details>


### [9] [MMAO-Bench: MultiModal All in One Benchmark Reveals Compositional Law between Uni-modal and Omni-modal in OmniModels](https://arxiv.org/abs/2510.18915)
*Chen Chen,ZeYang Hu,Fengjiao Chen,Liya Ma,Jiaxing Liu,Xiaoyu Li,Xuezhi Cao*

Main category: cs.CL

TL;DR: 提出了一个新颖的多模态全能模型基准MMAO-Bench，用于评估单模态和全模态理解能力，包含1880个人工标注样本，涵盖44种任务类型。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型正从单模态理解向统一视觉、音频和语言模态的全能模型发展，但单模态与全模态之间的相关性尚不明确，需要全面评估来推动全能模型的智能演进。

Method: 构建了包含1880个人工标注样本、44种任务类型的基准，并创新性地引入了多步骤开放式问题类型来更好地评估复杂推理任务。

Result: 实验结果显示跨模态和单模态性能之间存在组合规律，全模态能力在弱模型上表现为瓶颈效应，而在强模型上表现出协同促进作用。

Conclusion: MMAO-Bench是一个高质量、多样化的全能模型基准，能够有效评估单模态和全模态理解能力，揭示了全模态能力在不同模型强度下的不同表现模式。

Abstract: Multimodal Large Languages models have been progressing from uni-modal
understanding toward unifying visual, audio and language modalities,
collectively termed omni models. However, the correlation between uni-modal and
omni-modal remains unclear, which requires comprehensive evaluation to drive
omni model's intelligence evolution. In this work, we propose a novel, high
quality and diversity omni model benchmark, MultiModal All in One Benchmark
(MMAO-Bench), which effectively assesses both uni-modal and omni-modal
understanding capabilities. The benchmark consists of 1880 human curated
samples, across 44 task types, and a innovative multi-step open-ended question
type that better assess complex reasoning tasks. Experimental result shows the
compositional law between cross-modal and uni-modal performance and the
omni-modal capability manifests as a bottleneck effect on weak models, while
exhibiting synergistic promotion on strong models.

</details>


### [10] [Misinformation Detection using Large Language Models with Explainability](https://arxiv.org/abs/2510.18918)
*Jainee Patel,Chintan Bhatt,Himani Trivedi,Thanh Thi Nguyen*

Main category: cs.CL

TL;DR: 提出基于transformer预训练语言模型的可解释性假新闻检测方法，通过两阶段微调策略优化RoBERTa和DistilBERT，在保持性能的同时显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 在线平台上的虚假信息传播破坏了用户信任并阻碍明智决策，需要开发高效且可解释的检测方法。

Method: 采用两阶段微调策略：先冻结主干网络训练分类头，然后逐步解冻主干层并应用分层学习率衰减。集成LIME和SHAP提供局部和全局解释。

Result: 在COVID Fake News和FakeNewsNet GossipCop数据集上，DistilBERT达到与RoBERTa相当的准确率，同时显著减少计算资源需求。

Conclusion: 预训练语言模型结合原则性微调和可解释性方法，可构建可扩展、可信赖的虚假信息检测框架。

Abstract: The rapid spread of misinformation on online platforms undermines trust among
individuals and hinders informed decision making. This paper shows an
explainable and computationally efficient pipeline to detect misinformation
using transformer-based pretrained language models (PLMs). We optimize both
RoBERTa and DistilBERT using a two-step strategy: first, we freeze the backbone
and train only the classification head; then, we progressively unfreeze the
backbone layers while applying layer-wise learning rate decay. On two
real-world benchmark datasets, COVID Fake News and FakeNewsNet GossipCop, we
test the proposed approach with a unified protocol of preprocessing and
stratified splits. To ensure transparency, we integrate the Local Interpretable
Model-Agnostic Explanations (LIME) at the token level to present token-level
rationales and SHapley Additive exPlanations (SHAP) at the global feature
attribution level. It demonstrates that DistilBERT achieves accuracy comparable
to RoBERTa while requiring significantly less computational resources. This
work makes two key contributions: (1) it quantitatively shows that a
lightweight PLM can maintain task performance while substantially reducing
computational cost, and (2) it presents an explainable pipeline that retrieves
faithful local and global justifications without compromising performance. The
results suggest that PLMs combined with principled fine-tuning and
interpretability can be an effective framework for scalable, trustworthy
misinformation detection.

</details>


### [11] [Evaluating LLM Story Generation through Large-scale Network Analysis of Social Structures](https://arxiv.org/abs/2510.18932)
*Hiroshi Nonaka,K. E. Perry*

Main category: cs.CL

TL;DR: 提出了一种通过分析叙事中的符号化角色网络来评估LLM故事生成能力的新方法，发现LLM生成的故事倾向于紧密的积极关系网络


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在复杂任务中的创造力通常需要难以扩展的人工评估，需要开发可扩展的评估方法

Method: 通过分析叙事中的符号化角色网络（带正负权重的边），比较了4个主流LLM生成的1200多个故事和人类写作语料库的网络属性

Result: LLM生成的故事在密度、聚类和符号边权重等网络属性上表现出对紧密积极关系的强烈偏好，这与之前人工评估的研究结果一致

Conclusion: 该方法为评估当前和未来LLM在创意故事讲述中的局限性和倾向提供了有价值的工具

Abstract: Evaluating the creative capabilities of large language models (LLMs) in
complex tasks often requires human assessments that are difficult to scale. We
introduce a novel, scalable methodology for evaluating LLM story generation by
analyzing underlying social structures in narratives as signed character
networks. To demonstrate its effectiveness, we conduct a large-scale
comparative analysis using networks from over 1,200 stories, generated by four
leading LLMs (GPT-4o, GPT-4o mini, Gemini 1.5 Pro, and Gemini 1.5 Flash) and a
human-written corpus. Our findings, based on network properties like density,
clustering, and signed edge weights, show that LLM-generated stories
consistently exhibit a strong bias toward tightly-knit, positive relationships,
which aligns with findings from prior research using human assessment. Our
proposed approach provides a valuable tool for evaluating limitations and
tendencies in the creative storytelling of current and future LLMs.

</details>


### [12] [Lost in the Maze: Overcoming Context Limitations in Long-Horizon Agentic Search](https://arxiv.org/abs/2510.18939)
*Howard Yen,Ashwin Paranjape,Mengzhou Xia,Thejas Venkatesh,Jack Hessel,Danqi Chen,Yuhao Zhang*

Main category: cs.CL

TL;DR: SLIM框架通过分离搜索和浏览工具，并定期总结轨迹，解决了长轨迹代理搜索中的上下文限制问题，在保持性能的同时显著降低了成本和工具调用次数。


<details>
  <summary>Details</summary>
Motivation: 现有的代理搜索框架在长轨迹任务中面临上下文限制问题，包括积累大量噪声内容、超出上下文窗口和工具预算、过早停止等，无法有效扩展到长轨迹搜索。

Method: 提出了SLIM框架，将检索分离为独立的搜索和浏览工具，并定期总结搜索轨迹，保持上下文简洁的同时支持更长、更专注的搜索。

Result: 在长视野任务中，SLIM以显著更低的成本和更少的工具调用实现了与强开源基线相当的性能。使用o3作为基础模型时，在BrowseComp上达到56%，在HLE上达到31%，分别比所有开源框架高出8和4个绝对百分点，同时工具调用次数减少4-6倍。

Conclusion: SLIM框架在长轨迹代理搜索中表现出更少的幻觉，其简单的工具设计和分析框架为未来长视野代理的发展提供了参考。

Abstract: Long-horizon agentic search requires iteratively exploring the web over long
trajectories and synthesizing information across many sources, and is the
foundation for enabling powerful applications like deep research systems. In
this work, we show that popular agentic search frameworks struggle to scale to
long trajectories primarily due to context limitations-they accumulate long,
noisy content, hit context window and tool budgets, or stop early. Then, we
introduce SLIM (Simple Lightweight Information Management), a simple framework
that separates retrieval into distinct search and browse tools, and
periodically summarizes the trajectory, keeping context concise while enabling
longer, more focused searches. On long-horizon tasks, SLIM achieves comparable
performance at substantially lower cost and with far fewer tool calls than
strong open-source baselines across multiple base models. Specifically, with o3
as the base model, SLIM achieves 56% on BrowseComp and 31% on HLE,
outperforming all open-source frameworks by 8 and 4 absolute points,
respectively, while incurring 4-6x fewer tool calls. Finally, we release an
automated fine-grained trajectory analysis pipeline and error taxonomy for
characterizing long-horizon agentic search frameworks; SLIM exhibits fewer
hallucinations than prior systems. We hope our analysis framework and simple
tool design inform future long-horizon agents.

</details>


### [13] [ProfBench: Multi-Domain Rubrics requiring Professional Knowledge to Answer and Judge](https://arxiv.org/abs/2510.18941)
*Zhilin Wang,Jaehun Jung,Ximing Lu,Shizhe Diao,Ellie Evans,Jiaqi Zeng,Pavlo Molchanov,Yejin Choi,Jan Kautz,Yi Dong*

Main category: cs.CL

TL;DR: ProfBench是一个包含7000多个专家评估响应的专业领域基准，用于评估LLM在处理专业文档、信息合成和报告生成方面的能力，通过构建经济高效的LLM-Judges使评估更公平和可访问。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估主要局限于数学、编程等易于验证的任务，而现实应用需要评估LLM在处理专业文档和生成综合报告方面的能力。

Method: 构建ProfBench基准，包含物理学博士、化学博士、金融MBA和咨询MBA等专业领域的7000多个响应-标准对，通过人类专家评估，并开发LLM-Judges来降低评估成本。

Result: ProfBench对最先进的LLM构成显著挑战，表现最佳的GPT-5-high模型仅达到65.9%的总体性能，发现专有模型和开源模型之间存在明显性能差距。

Conclusion: ProfBench为专业领域LLM评估提供了重要基准，揭示了扩展思维在处理复杂专业任务中的作用，使评估更加公平和可访问。

Abstract: Evaluating progress in large language models (LLMs) is often constrained by
the challenge of verifying responses, limiting assessments to tasks like
mathematics, programming, and short-form question-answering. However, many
real-world applications require evaluating LLMs in processing professional
documents, synthesizing information, and generating comprehensive reports in
response to user queries. We introduce ProfBench: a set of over 7000
response-criterion pairs as evaluated by human-experts with professional
knowledge across Physics PhD, Chemistry PhD, Finance MBA and Consulting MBA. We
build robust and affordable LLM-Judges to evaluate ProfBench rubrics, by
mitigating self-enhancement bias and reducing the cost of evaluation by 2-3
orders of magnitude, to make it fair and accessible to the broader community.
Our findings reveal that ProfBench poses significant challenges even for
state-of-the-art LLMs, with top-performing models like GPT-5-high achieving
only 65.9\% overall performance. Furthermore, we identify notable performance
disparities between proprietary and open-weight models and provide insights
into the role that extended thinking plays in addressing complex,
professional-domain tasks. Data:
https://huggingface.co/datasets/nvidia/ProfBench and Code:
https://github.com/NVlabs/ProfBench

</details>


### [14] [Dynamic Evaluation for Oversensitivity in LLMs](https://arxiv.org/abs/2510.19005)
*Sophia Xiao Pu,Sitao Cheng,Xin Eric Wang,William Yang Wang*

Main category: cs.CL

TL;DR: 开发了一个动态生成模型特定挑战数据集框架，构建了OVERBENCH基准，包含45万个样本，用于持续监测语言模型的过度敏感性。


<details>
  <summary>Details</summary>
Motivation: 解决语言模型对良性提示的防御性拒绝问题，以及静态数据集因模型演化而退化导致的评估能力下降问题。

Method: 开发动态生成模型特定挑战数据集的框架，构建OVERBENCH基准，聚合来自25个模型的450,000个样本。

Result: OVERBENCH提供了动态演化的过度敏感性视角，能够持续监测防御性触发因素，发现静态数据集忽略的漏洞。

Conclusion: 该框架能够有效应对模型演化带来的挑战，为语言模型过度敏感性提供持续、准确的评估工具。

Abstract: Oversensitivity occurs when language models defensively reject prompts that
are actually benign. This behavior not only disrupts user interactions but also
obscures the boundary between harmful and harmless content. Existing benchmarks
rely on static datasets that degrade overtime as models evolve, leading to data
contamination and diminished evaluative power. To address this, we develop a
framework that dynamically generates model-specific challenging datasets,
capturing emerging defensive patterns and aligning with each model's unique
behavior. Building on this approach, we construct OVERBENCH, a benchmark that
aggregates these datasets across diverse LLM families, encompassing 450,000
samples from 25 models. OVERBENCH provides a dynamic and evolving perspective
on oversensitivity, allowing for continuous monitoring of defensive triggers as
models advance, highlighting vulnerabilities that static datasets overlook.

</details>


### [15] [Are they lovers or friends? Evaluating LLMs' Social Reasoning in English and Korean Dialogues](https://arxiv.org/abs/2510.19028)
*Eunsu Kim,Junyeong Park,Juhyun Oh,Kiwoong Park,Seyoung Song,A. Seza Dogruoz,Najoung Kim,Alice Oh*

Main category: cs.CL

TL;DR: 提出了SCRIPTS数据集，用于评估LLM在对话中推断人际关系的社交推理能力，发现当前模型在英语上表现尚可（75-80%），但在韩语上表现下降（58-69%），且存在选择不可能关系的倾向。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在人类-AI交互中的广泛应用，其在人际语境中的社交推理能力变得至关重要。

Method: 引入SCRIPTS数据集（包含1k个英语和韩语对话，来自电影剧本），要求模型推断对话者之间的人际关系，并由母语者标注概率关系标签。

Result: 评估9个模型发现：专有LLM在英语数据集上达到75-80%准确率，但在韩语上降至58-69%；模型在10-25%的响应中选择不可能关系；思维模型和思维链提示对社交推理帮助有限，有时会放大社会偏见。

Conclusion: 当前LLM的社交推理能力存在显著局限，需要开发更具社会意识的语言模型。

Abstract: As large language models (LLMs) are increasingly used in human-AI
interactions, their social reasoning capabilities in interpersonal contexts are
critical. We introduce SCRIPTS, a 1k-dialogue dataset in English and Korean,
sourced from movie scripts. The task involves evaluating models' social
reasoning capability to infer the interpersonal relationships (e.g., friends,
sisters, lovers) between speakers in each dialogue. Each dialogue is annotated
with probabilistic relational labels (Highly Likely, Less Likely, Unlikely) by
native (or equivalent) Korean and English speakers from Korea and the U.S.
Evaluating nine models on our task, current proprietary LLMs achieve around
75-80% on the English dataset, whereas their performance on Korean drops to
58-69%. More strikingly, models select Unlikely relationships in 10-25% of
their responses. Furthermore, we find that thinking models and chain-of-thought
prompting, effective for general reasoning, provide minimal benefits for social
reasoning and occasionally amplify social biases. Our findings reveal
significant limitations in current LLMs' social reasoning capabilities,
highlighting the need for efforts to develop socially-aware language models.

</details>


### [16] [Re:Member: Emotional Question Generation from Personal Memories](https://arxiv.org/abs/2510.19030)
*Zackary Rackauckas,Nobuaki Minematsu,Julia Hirschberg*

Main category: cs.CL

TL;DR: Re:Member是一个基于情感表达和记忆的交互系统，通过个人视频和风格化语音问题来增强第二语言学习的情感参与度。


<details>
  <summary>Details</summary>
Motivation: 探索情感表达和记忆基础的交互如何支持更具吸引力的第二语言学习，强调情感回忆和对话参与的重要性。

Method: 使用WhisperX转录对齐、3帧视觉采样和Style-BERT-VITS2情感合成，构建模块化生成管道，将情感语调与视觉上下文对齐。

Result: 系统能够生成风格化的口语问题（如耳语或深夜语调），唤起特定情绪，增强学习体验。

Conclusion: Re:Member作为风格化交互探针，突出了情感和个人媒体在以学习者为中心的教育技术中的重要作用。

Abstract: We present Re:Member, a system that explores how emotionally expressive,
memory-grounded interaction can support more engaging second language (L2)
learning. By drawing on users' personal videos and generating stylized spoken
questions in the target language, Re:Member is designed to encourage affective
recall and conversational engagement. The system aligns emotional tone with
visual context, using expressive speech styles such as whispers or late-night
tones to evoke specific moods. It combines WhisperX-based transcript alignment,
3-frame visual sampling, and Style-BERT-VITS2 for emotional synthesis within a
modular generation pipeline. Designed as a stylized interaction probe,
Re:Member highlights the role of affect and personal media in learner-centered
educational technologies.

</details>


### [17] [When Can We Trust LLMs in Mental Health? Large-Scale Benchmarks for Reliable LLM Evaluation](https://arxiv.org/abs/2510.19032)
*Abeer Badawi,Elahe Rahimi,Md Tahmid Rahman Laskar,Sheri Grach,Lindsay Bertrand,Lames Danok,Jimmy Huang,Frank Rudzicz,Elham Dolatabadi*

Main category: cs.CL

TL;DR: 提出了两个用于心理健康支持LLM评估的基准：MentalBench-100k包含10万对对话响应，MentalAlign-70k比较了LLM评判者与人类专家的7万个评分，使用情感认知一致性框架量化一致性。


<details>
  <summary>Details</summary>
Motivation: 现有评估基准在规模、可靠性方面有限，依赖合成或社交媒体数据，缺乏评估自动评判者可信度的框架。

Method: 构建MentalBench-100k（10万响应对）和MentalAlign-70k（7万评分），使用类内相关系数统计方法量化LLM评判者与人类专家的一致性、一致性和偏差。

Result: LLM评判者存在系统性评分膨胀，认知属性（如指导性和信息性）可靠性强，共情精度降低，安全和相关性存在不可靠性。

Conclusion: 为心理健康领域LLM的可靠、大规模评估建立了新的方法论和实证基础。

Abstract: Evaluating Large Language Models (LLMs) for mental health support is
challenging due to the emotionally and cognitively complex nature of
therapeutic dialogue. Existing benchmarks are limited in scale, reliability,
often relying on synthetic or social media data, and lack frameworks to assess
when automated judges can be trusted. To address the need for large-scale
dialogue datasets and judge reliability assessment, we introduce two benchmarks
that provide a framework for generation and evaluation. MentalBench-100k
consolidates 10,000 one-turn conversations from three real scenarios datasets,
each paired with nine LLM-generated responses, yielding 100,000 response pairs.
MentalAlign-70k}reframes evaluation by comparing four high-performing LLM
judges with human experts across 70,000 ratings on seven attributes, grouped
into Cognitive Support Score (CSS) and Affective Resonance Score (ARS). We then
employ the Affective Cognitive Agreement Framework, a statistical methodology
using intraclass correlation coefficients (ICC) with confidence intervals to
quantify agreement, consistency, and bias between LLM judges and human experts.
Our analysis reveals systematic inflation by LLM judges, strong reliability for
cognitive attributes such as guidance and informativeness, reduced precision
for empathy, and some unreliability in safety and relevance. Our contributions
establish new methodological and empirical foundations for reliable,
large-scale evaluation of LLMs in mental health. We release the benchmarks and
codes at: https://github.com/abeerbadawi/MentalBench/

</details>


### [18] [From Memorization to Generalization: Fine-Tuning Large Language Models for Biomedical Term-to-Identifier Normalization](https://arxiv.org/abs/2510.19036)
*Suswitha Pericharla,Daniel B. Hier,Tayo Obafemi-Ajayi*

Main category: cs.CL

TL;DR: 该研究评估了大型语言模型在生物医学术语标准化任务中的表现，发现微调效果取决于标识符的流行度和词汇化程度。基因标识符表现最佳，而GO和HPO等本体由于标识符的任意性限制了模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 生物医学数据集成依赖于术语标准化，即将自然语言术语映射到标准化标识符。虽然大型语言模型在此任务中显示出潜力，但不同术语系统的表现差异很大，需要系统评估其记忆和泛化能力。

Method: 使用Llama 3.1 8B模型在多个生物医学本体上进行微调，评估记忆能力（训练术语表现）和泛化能力（验证术语表现），并分析嵌入空间中的语义对齐。

Result: 微调效果因术语系统而异：GO映射显示强记忆增益（准确率提升77%），HPO改进最小；只有基因映射显示泛化增益（13.9%）。GPT-4o在所有术语系统中都优于Llama变体。嵌入分析显示基因符号与蛋白质名称语义对齐紧密，但GO和HPO的术语与标识符对齐较弱。

Conclusion: 微调成功取决于两个相互作用因素：标识符流行度和词汇化程度。流行标识符在预训练中更常见，增强记忆；词汇化标识符（如基因符号）支持语义泛化。而GO和HPO的任意标识符限制了模型只能进行机械学习。

Abstract: Effective biomedical data integration depends on automated term
normalization, the mapping of natural language biomedical terms to standardized
identifiers. This linking of terms to identifiers is essential for semantic
interoperability. Large language models (LLMs) show promise for this task but
perform unevenly across terminologies. We evaluated both memorization
(training-term performance) and generalization (validation-term performance)
across multiple biomedical ontologies. Fine-tuning Llama 3.1 8B revealed marked
differences by terminology. GO mappings showed strong memorization gains (up to
77% improvement in term-to-identifier accuracy), whereas HPO showed minimal
improvement. Generalization occurred only for protein-gene (GENE) mappings
(13.9% gain), while fine-tuning for HPO and GO yielded negligible transfer.
Baseline accuracy varied by model scale, with GPT-4o outperforming both Llama
variants for all terminologies. Embedding analyses showed tight semantic
alignment between gene symbols and protein names but weak alignment between
terms and identifiers for GO or HPO, consistent with limited lexicalization.
Fine-tuning success depended on two interacting factors: identifier popularity
and lexicalization. Popular identifiers were more likely encountered during
pretraining, enhancing memorization. Lexicalized identifiers, such as gene
symbols, enabled semantic generalization. By contrast, arbitrary identifiers in
GO and HPO constrained models to rote learning. These findings provide a
predictive framework for when fine-tuning enhances factual recall versus when
it fails due to sparse or non-lexicalized identifiers.

</details>


### [19] [That's Deprecated! Understanding, Detecting, and Steering Knowledge Conflicts in Language Models for Code Generation](https://arxiv.org/abs/2510.19116)
*Jaesung Bae,Cameron Churchwell,Mitchell Hermon,Tsun-An Hsieh,Jocelyn Xu,Yekaterina Yegorova,Mark Hasegawa-Johnson,Heng Ji*

Main category: cs.CL

TL;DR: 该论文研究了大型语言模型在处理参数知识与提示中冲突信息时的行为，特别关注代码生成领域，提出了知识冲突检测框架和评估方法，发现大模型能编码知识冲突概念，并通过激活级引导提升性能。


<details>
  <summary>Details</summary>
Motivation: 基于先前问答研究，将知识冲突研究扩展到代码生成领域，探索LLMs如何应对参数知识与提示信息之间的不一致性。

Method: 提出了领域无关的知识冲突构建和解释框架，开发了针对代码冲突场景的新型评估方法和数据集，使用激活级引导技术进行实验。

Result: 实验表明足够大的LLMs能在参数中编码知识冲突概念，检测准确率达80.65%；激活级引导相比随机基线可提升12.6%的引导成功率，但效果取决于模型大小、任务领域和引导方向的平衡。

Conclusion: LLMs能够识别和处理知识冲突，激活引导技术可有效改善模型行为，但需要综合考虑模型规模、任务特性和引导策略的协调。

Abstract: This paper investigates how large language models (LLMs) behave when faced
with discrepancies between their parametric knowledge and conflicting
information contained in a prompt. Building on prior question-answering (QA)
research, we extend the investigation of knowledge conflicts to the realm of
code generation. We propose a domain-agnostic framework for constructing and
interpreting such conflicts, along with a novel evaluation method and dataset
tailored to code conflict scenarios. Our experiments indicate that sufficiently
large LLMs encode the notion of a knowledge conflict in their parameters,
enabling us to detect knowledge conflicts with up to \textbf{80.65\%} accuracy.
Building on these insights, we show that activation-level steering can achieve
up to a \textbf{12.6\%} improvement in steering success over a random baseline.
However, effectiveness depends critically on balancing model size, task domain,
and steering direction. The experiment code and data will be made publicly
available after acceptance.

</details>


### [20] [A Graph Signal Processing Framework for Hallucination Detection in Large Language Models](https://arxiv.org/abs/2510.19117)
*Valentin Noël*

Main category: cs.CL

TL;DR: 提出了一个基于谱分析的框架，通过将transformer层建模为注意力诱导的动态图，使用图信号处理方法来检测LLM中的幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在取得显著成果的同时，区分事实推理与幻觉仍然具有挑战性，需要新的检测方法。

Method: 将transformer层建模为注意力诱导的动态图，token嵌入作为图上的信号，通过图信号处理定义诊断指标（Dirichlet能量、谱熵、高频能量比），并与计算稳定性建立理论联系。

Result: 实验发现普遍的光谱模式：事实陈述呈现一致的"能量山"行为并低频收敛，而不同类型的幻觉显示不同特征。基于光谱特征的简单检测器达到88.75%准确率，优于基于困惑度的基线方法（75%）。

Conclusion: 光谱几何可能捕捉推理模式和错误行为，为大型语言模型中的幻觉检测提供了一个有前景的框架。

Abstract: Large language models achieve impressive results but distinguishing factual
reasoning from hallucinations remains challenging. We propose a spectral
analysis framework that models transformer layers as dynamic graphs induced by
attention, with token embeddings as signals on these graphs. Through graph
signal processing, we define diagnostics including Dirichlet energy, spectral
entropy, and high-frequency energy ratios, with theoretical connections to
computational stability. Experiments across GPT architectures suggest universal
spectral patterns: factual statements exhibit consistent "energy mountain"
behavior with low-frequency convergence, while different hallucination types
show distinct signatures. Logical contradictions destabilize spectra with large
effect sizes ($g>1.0$), semantic errors remain stable but show connectivity
drift, and substitution hallucinations display intermediate perturbations. A
simple detector using spectral signatures achieves 88.75% accuracy versus 75%
for perplexity-based baselines, demonstrating practical utility. These findings
indicate that spectral geometry may capture reasoning patterns and error
behaviors, potentially offering a framework for hallucination detection in
large language models.

</details>


### [21] [Training-Free Spectral Fingerprints of Voice Processing in Transformers](https://arxiv.org/abs/2510.19131)
*Valentin Noël*

Main category: cs.CL

TL;DR: 通过图信号处理分析transformer架构的注意力图，发现不同模型在语音转换任务中展现出独特的计算指纹，这些指纹与架构设计和训练重点相关。


<details>
  <summary>Details</summary>
Motivation: 研究不同transformer架构如何通过不同的连接模式实现相同的语言计算，并检测这些模式形成的计算指纹。

Method: 使用图信号处理分析注意力诱导的token图，跟踪20种语言和三个模型家族在语音转换过程中代数连通性的变化，重点关注早期层（2-5层）。

Result: 发现清晰的架构特征：Phi-3-Mini在英语中显示显著的早期层中断，而其他19种语言影响最小；Qwen2.5-7B对形态丰富语言显示分布式小变化；LLaMA-3.2-1B显示系统但温和的响应。这些特征与行为差异强相关。

Conclusion: 训练重点会在计算中留下可检测的印记，表现为句法转换期间可测量的连接模式。该框架可作为无训练的诊断工具，揭示架构偏见并支持模型可靠性分析。

Abstract: Different transformer architectures implement identical linguistic
computations via distinct connectivity patterns, yielding model imprinted
``computational fingerprints'' detectable through spectral analysis. Using
graph signal processing on attention induced token graphs, we track changes in
algebraic connectivity (Fiedler value, $\Delta\lambda_2$) under voice
alternation across 20 languages and three model families, with a prespecified
early window (layers 2--5). Our analysis uncovers clear architectural
signatures: Phi-3-Mini shows a dramatic English specific early layer disruption
($\overline{\Delta\lambda_2}_{[2,5]}\!\approx\!-0.446$) while effects in 19
other languages are minimal, consistent with public documentation that
positions the model primarily for English use. Qwen2.5-7B displays small,
distributed shifts that are largest for morphologically rich languages, and
LLaMA-3.2-1B exhibits systematic but muted responses. These spectral signatures
correlate strongly with behavioral differences (Phi-3: $r=-0.976$) and are
modulated by targeted attention head ablations, linking the effect to early
attention structure and confirming functional relevance. Taken together, the
findings are consistent with the view that training emphasis can leave
detectable computational imprints: specialized processing strategies that
manifest as measurable connectivity patterns during syntactic transformations.
Beyond voice alternation, the framework differentiates reasoning modes,
indicating utility as a simple, training free diagnostic for revealing
architectural biases and supporting model reliability analysis.

</details>


### [22] [Tibetan Language and AI: A Comprehensive Survey of Resources, Methods and Challenges](https://arxiv.org/abs/2510.19144)
*Cheng Huang,Nyima Tashi,Fan Gao,Yutong Liu,Jiahao Li,Hao Tian,Siyang Jiang,Thupten Tsering,Ban Ma-bao,Renzeg Duojie,Gadeng Luosang,Rinchen Dongrub,Dorje Tashi,Jin Zhang,Xiao Feng,Hao Wang,Jie Tang,Guojie Tang,Xiangxiang Wang,Jia Zhang,Tsengdar Lee,Yongbin Yu*

Main category: cs.CL

TL;DR: 本文对藏语AI研究现状进行全面调查，涵盖文本和语音数据资源、NLP任务、机器翻译、语音识别及大语言模型发展，旨在为藏语AI研究提供基础参考。


<details>
  <summary>Details</summary>
Motivation: 藏语作为亚洲主要低资源语言，具有独特的语言和社会文化特征，但由于缺乏可访问数据资源、标准化基准和专用工具，在AI研究中关注有限。

Method: 系统分类现有数据集和工具，评估不同任务中使用的方法，并在可能的情况下比较性能表现。

Result: 识别出数据稀疏性、正字法变异和缺乏统一评估指标等持续瓶颈，并讨论了跨语言迁移、多模态学习和社区驱动资源创建的潜力。

Conclusion: 本调查旨在为未来藏语AI研究提供基础参考，并鼓励合作努力，为低资源语言构建包容和可持续的AI生态系统。

Abstract: Tibetan, one of the major low-resource languages in Asia, presents unique
linguistic and sociocultural characteristics that pose both challenges and
opportunities for AI research. Despite increasing interest in developing AI
systems for underrepresented languages, Tibetan has received limited attention
due to a lack of accessible data resources, standardized benchmarks, and
dedicated tools. This paper provides a comprehensive survey of the current
state of Tibetan AI in the AI domain, covering textual and speech data
resources, NLP tasks, machine translation, speech recognition, and recent
developments in LLMs. We systematically categorize existing datasets and tools,
evaluate methods used across different tasks, and compare performance where
possible. We also identify persistent bottlenecks such as data sparsity,
orthographic variation, and the lack of unified evaluation metrics.
Additionally, we discuss the potential of cross-lingual transfer, multi-modal
learning, and community-driven resource creation. This survey aims to serve as
a foundational reference for future work on Tibetan AI research and encourages
collaborative efforts to build an inclusive and sustainable AI ecosystem for
low-resource languages.

</details>


### [23] ["You Are Rejected!": An Empirical Study of Large Language Models Taking Hiring Evaluations](https://arxiv.org/abs/2510.19167)
*Dingjie Fu,Dianxing Shi*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型能否通过科技公司招聘评估，发现所有测试的LLMs都无法通过标准化招聘考试。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术发展，科技公司需要高效筛选大量软件工程师申请人。鉴于LLMs在编程和推理任务中的出色表现，研究其是否能通过专业招聘评估具有重要意义。

Method: 使用最先进的LLMs对广泛使用的专业评估问卷生成回答，并将其与公司参考解决方案进行比较评估。

Result: 分析显示模型生成的答案与公司参考解决方案之间存在显著不一致，所有评估的LLMs都未能通过招聘评估。

Conclusion: 尽管LLMs在编程任务中表现出色，但它们目前无法通过科技公司的标准化招聘评估，表明这些模型与理想工程师标准仍存在差距。

Abstract: With the proliferation of the internet and the rapid advancement of
Artificial Intelligence, leading technology companies face an urgent annual
demand for a considerable number of software and algorithm engineers. To
efficiently and effectively identify high-potential candidates from thousands
of applicants, these firms have established a multi-stage selection process,
which crucially includes a standardized hiring evaluation designed to assess
job-specific competencies. Motivated by the demonstrated prowess of Large
Language Models (LLMs) in coding and reasoning tasks, this paper investigates a
critical question: Can LLMs successfully pass these hiring evaluations? To this
end, we conduct a comprehensive examination of a widely used professional
assessment questionnaire. We employ state-of-the-art LLMs to generate responses
and subsequently evaluate their performance. Contrary to any prior expectation
of LLMs being ideal engineers, our analysis reveals a significant inconsistency
between the model-generated answers and the company-referenced solutions. Our
empirical findings lead to a striking conclusion: All evaluated LLMs fails to
pass the hiring evaluation.

</details>


### [24] [Think Straight, Stop Smart: Structured Reasoning for Efficient Multi-Hop RAG](https://arxiv.org/abs/2510.19171)
*Jihwan Bang,Juntae Lee,Seunghan Yang,Sungha Choi*

Main category: cs.CL

TL;DR: TSSS是一个高效的多跳检索增强生成框架，通过模板化推理和检索器终止器来减少令牌使用并实现稳定推理。


<details>
  <summary>Details</summary>
Motivation: 现有的多跳RAG方法存在效率低下问题，包括重复生成可预测令牌序列和随机停止机制，导致令牌使用过多和终止不稳定。

Method: 提出TSSS框架，包含：(1)基于模板的推理，缓存重复前缀并将子查询锚定到主问题；(2)基于检索器的终止器，在子查询重复时确定性地停止推理。

Result: 在HotpotQA、2WikiMultiHop和MuSiQue数据集上达到最先进的准确率，并在RAG-CoT方法中具有竞争力的效率。

Conclusion: TSSS在效率受限场景（如设备端推理）中表现出色，通过分离结构化推理和终止控制实现更快推理和更可靠答案。

Abstract: Multi-hop retrieval-augmented generation (RAG) is a promising strategy for
complex reasoning, yet existing iterative prompting approaches remain
inefficient. They often regenerate predictable token sequences at every step
and rely on stochastic stopping, leading to excessive token usage and unstable
termination. We propose TSSS (Think Straight, Stop Smart), a structured
multi-hop RAG framework designed for efficiency. TSSS introduces (i) a
template-based reasoning that caches recurring prefixes and anchors sub-queries
to the main question, reducing token generation cost while promoting stable
reasoning, and (ii) a retriever-based terminator, which deterministically halts
reasoning once additional sub-queries collapse into repetition. This separation
of structured reasoning and termination control enables both faster inference
and more reliable answers. On HotpotQA, 2WikiMultiHop, and MuSiQue, TSSS
achieves state-of-the-art accuracy and competitive efficiency among RAG-CoT
approaches, highlighting its effectiveness in efficiency-constrained scenarios
such as on-device inference.

</details>


### [25] [When Facts Change: Probing LLMs on Evolving Knowledge with evolveQA](https://arxiv.org/abs/2510.19172)
*Nishanth Sridhar Nakshatri,Shamik Roy,Manoj Ghuhan Arivazhagan,Hanhan Zhou,Vinayshekhar Bannihatti Kumar,Rashmi Gangadharaiah*

Main category: cs.CL

TL;DR: 提出了evolveQA基准测试，专门评估LLMs处理时间演化知识的能力，发现相比静态知识问题性能下降高达31%


<details>
  <summary>Details</summary>
Motivation: 现有研究使用结构化知识库评估时间知识冲突，但主要关注流行实体且缺乏动态结构来公平评估不同知识截止日期的LLMs

Method: 从AWS更新、Azure变更和WHO疾病爆发报告三个真实世界时间戳语料库构建基准，识别自然发生的知识演化并生成针对不同LLM知识截止日期的问题

Result: 对12个开源和闭源LLMs在3种知识探测格式上的广泛评估显示，在evolveQA上相比静态知识问题性能显著下降达31%

Conclusion: evolveQA基准有效揭示了LLMs处理时间演化知识的局限性，为评估时间知识冲突提供了更公平的框架

Abstract: LLMs often fail to handle temporal knowledge conflicts--contradictions
arising when facts evolve over time within their training data. Existing
studies evaluate this phenomenon through benchmarks built on structured
knowledge bases like Wikidata, but they focus on widely-covered,
easily-memorized popular entities and lack the dynamic structure needed to
fairly evaluate LLMs with different knowledge cut-off dates. We introduce
evolveQA, a benchmark specifically designed to evaluate LLMs on temporally
evolving knowledge, constructed from 3 real-world, time-stamped corpora: AWS
updates, Azure changes, and WHO disease outbreak reports. Our framework
identifies naturally occurring knowledge evolution and generates questions with
gold answers tailored to different LLM knowledge cut-off dates. Through
extensive evaluation of 12 open and closed-source LLMs across 3 knowledge
probing formats, we demonstrate significant performance drops of up to 31% on
evolveQA compared to static knowledge questions.

</details>


### [26] [Interpretable Question Answering with Knowledge Graphs](https://arxiv.org/abs/2510.19181)
*Kartikeya Aneja,Manasvi Srivastava,Subhayan Das,Nagender Aneja*

Main category: cs.CL

TL;DR: 提出一个基于知识图谱检索的问答系统，不使用LLM增强检索，而是用小模型对检索到的知识图谱边进行释义来生成答案。


<details>
  <summary>Details</summary>
Motivation: 探索不依赖大型语言模型增强检索的问答系统，通过知识图谱检索和小模型释义来构建更轻量级的解决方案。

Method: 两阶段流程：1) 预处理文档生成问答对；2) 将问答对转换为知识图谱，使用嵌入和模糊技术进行图检索，然后查询、重排序和释义生成最终答案。

Result: 在CRAG基准测试中使用LLM-as-a-judge评估，LLAMA-3.2和GPT-3.5-Turbo分别达到71.9%和54.4%的准确率。

Conclusion: 证明了基于知识图谱检索和小模型释义的问答系统可行性，提供了一种不依赖LLM增强检索的替代方案。

Abstract: This paper presents a question answering system that operates exclusively on
a knowledge graph retrieval without relying on retrieval augmented generation
(RAG) with large language models (LLMs). Instead, a small paraphraser model is
used to paraphrase the entity relationship edges retrieved from querying the
knowledge graph. The proposed pipeline is divided into two main stages. The
first stage involves pre-processing a document to generate sets of
question-answer (QA) pairs. The second stage converts these QAs into a
knowledge graph from which graph-based retrieval is performed using embeddings
and fuzzy techniques. The graph is queried, re-ranked, and paraphrased to
generate a final answer. This work includes an evaluation using LLM-as-a-judge
on the CRAG benchmark, which resulted in accuracies of 71.9% and 54.4% using
LLAMA-3.2 and GPT-3.5-Turbo, respectively.

</details>


### [27] [Multi-Faceted Evaluation of Tool-Augmented Dialogue Systems](https://arxiv.org/abs/2510.19186)
*Zhaoyi Joey Hou,Tanya Shourya,Yingfan Wang,Shamik Roy,Vinayshekhar Bannihatti Kumar,Rashmi Gangadharaiah*

Main category: cs.CL

TL;DR: 提出了TRACE基准和SCOPE评估框架，用于评估使用外部工具的对话AI系统，特别关注多轮工具增强对话中的错误模式。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法要么评估用户满意度，要么评估代理的工具调用能力，但无法捕捉多轮工具增强对话中的关键错误，比如代理误解工具结果但仍让用户满意的情况。

Method: 引入TRACE基准（系统合成的工具增强对话数据集）和SCOPE评估框架（自动发现多样化错误模式和评估标准）。

Result: 实验表明SCOPE显著优于基线方法，特别是在用户满意度信号具有误导性的挑战性案例上表现突出。

Conclusion: 该研究为评估工具增强对话系统提供了更全面的方法，能够识别传统评估方法可能遗漏的关键错误。

Abstract: Evaluating conversational AI systems that use external tools is challenging,
as errors can arise from complex interactions among user, agent, and tools.
While existing evaluation methods assess either user satisfaction or agents'
tool-calling capabilities, they fail to capture critical errors in multi-turn
tool-augmented dialogues-such as when agents misinterpret tool results yet
appear satisfactory to users. We introduce TRACE, a benchmark of systematically
synthesized tool-augmented conversations covering diverse error cases, and
SCOPE, an evaluation framework that automatically discovers diverse error
patterns and evaluation rubrics in tool-augmented dialogues. Experiments show
SCOPE significantly outperforms the baseline, particularly on challenging cases
where user satisfaction signals are misleading.

</details>


### [28] [DiSRouter: Distributed Self-Routing for LLM Selections](https://arxiv.org/abs/2510.19208)
*Hang Zheng,Hongshen Xu,Yongkai Lin,Shuai Fan,Lu Chen,Kai Yu*

Main category: cs.CL

TL;DR: DiSRouter是一种分布式自路由范式，通过让LLM代理基于自我意识自主决定是否回答或路由查询，取代传统的集中式路由系统，在性能、灵活性和泛化性方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有路由系统依赖固定的集中式外部路由器，无法充分理解不同LLM的知识边界，导致性能不佳且缺乏灵活性。需要一种更有效的查询路由方法来平衡性能与成本。

Method: 提出DiSRouter分布式自路由范式，查询在网络中的LLM代理间传递，每个代理基于自我意识独立决定是否回答或路由。采用两阶段自我意识训练流程增强LLM的自我认知能力。

Result: 实验表明DiSRouter在各种场景下的效用显著优于现有路由方法，能有效区分简单和困难查询，并在领域外任务上表现出强大的泛化能力。

Conclusion: 利用LLM内在的自我意识比外部评估更有效，为更模块化和高效的多智能体系统开辟了新途径。

Abstract: The proliferation of Large Language Models (LLMs) has created a diverse
ecosystem of models with highly varying performance and costs, necessitating
effective query routing to balance performance and expense. Current routing
systems often rely on a centralized external router trained on a fixed set of
LLMs, making them inflexible and prone to poor performance since the small
router can not fully understand the knowledge boundaries of different LLMs. We
introduce DiSRouter (Distributed Self-Router), a novel paradigm that shifts
from centralized control to distributed routing. In DiSRouter, a query
traverses a network of LLM agents, each independently deciding whether to
answer or route to other agents based on its own self-awareness, its ability to
judge its competence. This distributed design offers superior flexibility,
scalability, and generalizability. To enable this, we propose a two-stage
Self-Awareness Training pipeline that enhances each LLM's self-awareness.
Extensive experiments demonstrate that DiSRouter significantly outperforms
existing routing methods in utility across various scenarios, effectively
distinguishes between easy and hard queries, and shows strong generalization to
out-of-domain tasks. Our work validates that leveraging an LLM's intrinsic
self-awareness is more effective than external assessment, paving the way for
more modular and efficient multi-agent systems.

</details>


### [29] [Modality Matching Matters: Calibrating Language Distances for Cross-Lingual Transfer in URIEL+](https://arxiv.org/abs/2510.19217)
*York Hay Ng,Aditya Khan,Xiang Lu,Matteo Salloum,Michael Zhou,Phuong H. Hoang,A. Seza Doğruöz,En-Shiun Annie Lee*

Main category: cs.CL

TL;DR: 提出了类型匹配语言距离框架，通过结构感知表示和统一信号来解决现有语言知识库的局限性，在多语言NLP任务中提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有语言知识库（如URIEL+）存在两个关键限制：一刀切的向量表示不适合语言数据的多样结构，缺乏将多种信号聚合为单一综合得分的原则性方法。

Method: 引入类型匹配语言距离框架，为每种距离类型提出新的结构感知表示：地理距离使用说话者加权分布，谱系距离使用双曲嵌入，类型学距离使用潜变量模型，并将这些信号统一为稳健的任务无关复合距离。

Result: 在迁移语言选择中，所提出的表示和复合距离在广泛的NLP任务中持续提升性能。

Conclusion: 该框架为多语言研究提供了更原则性和有效的工具包。

Abstract: Existing linguistic knowledge bases such as URIEL+ provide valuable
geographic, genetic and typological distances for cross-lingual transfer but
suffer from two key limitations. One, their one-size-fits-all vector
representations are ill-suited to the diverse structures of linguistic data,
and two, they lack a principled method for aggregating these signals into a
single, comprehensive score. In this paper, we address these gaps by
introducing a framework for type-matched language distances. We propose novel,
structure-aware representations for each distance type: speaker-weighted
distributions for geography, hyperbolic embeddings for genealogy, and a latent
variables model for typology. We unify these signals into a robust,
task-agnostic composite distance. In selecting transfer languages, our
representations and composite distances consistently improve performance across
a wide range of NLP tasks, providing a more principled and effective toolkit
for multilingual research.

</details>


### [30] [SheetBrain: A Neuro-Symbolic Agent for Accurate Reasoning over Complex and Large Spreadsheets](https://arxiv.org/abs/2510.19247)
*Ziwei Wang,Jiayuan Su,Mengyu Zhou,Huaxing Zeng,Mengni Jia,Xiao Lv,Haoyu Dong,Xiaojun Ma,Shi Han,Dongmei Zhang*

Main category: cs.CL

TL;DR: SheetBrain是一个神经符号双工作流代理框架，用于在表格数据上进行准确推理，支持电子表格问答和操作任务。它通过理解、执行和验证三个核心模块显著提高了复杂表格处理的准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在理解和推理复杂电子表格方面存在困难，难以准确捕捉表格的复杂结构并确保推理正确性。

Method: SheetBrain包含三个核心模块：理解模块生成电子表格的全面概览；执行模块集成Python沙箱和Excel工具包进行多轮推理；验证模块验证推理和答案的正确性，必要时触发重新执行。

Result: 实验结果表明，SheetBrain在现有基准测试和更具挑战性的SheetBench场景中都显著提高了准确性。

Conclusion: SheetBrain通过神经符号双工作流框架有效解决了LLMs在复杂表格推理中的局限性，为表格数据处理提供了更准确的解决方案。

Abstract: Understanding and reasoning over complex spreadsheets remain fundamental
challenges for large language models (LLMs), which often struggle with
accurately capturing the complex structure of tables and ensuring reasoning
correctness. In this work, we propose SheetBrain, a neuro-symbolic dual
workflow agent framework designed for accurate reasoning over tabular data,
supporting both spreadsheet question answering and manipulation tasks.
SheetBrain comprises three core modules: an understanding module, which
produces a comprehensive overview of the spreadsheet - including sheet summary
and query-based problem insight to guide reasoning; an execution module, which
integrates a Python sandbox with preloaded table-processing libraries and an
Excel helper toolkit for effective multi-turn reasoning; and a validation
module, which verifies the correctness of reasoning and answers, triggering
re-execution when necessary. We evaluate SheetBrain on multiple public tabular
QA and manipulation benchmarks, and introduce SheetBench, a new benchmark
targeting large, multi-table, and structurally complex spreadsheets.
Experimental results show that SheetBrain significantly improves accuracy on
both existing benchmarks and the more challenging scenarios presented in
SheetBench. Our code is publicly available at
https://github.com/microsoft/SheetBrain.

</details>


### [31] [Difficulty-Controllable Multiple-Choice Question Generation Using Large Language Models and Direct Preference Optimization](https://arxiv.org/abs/2510.19265)
*Yuto Tomikawa,Masaki Uto*

Main category: cs.CL

TL;DR: 提出了一种基于大语言模型和直接偏好优化技术的难度可控选择题生成方法，用于阅读理解任务。


<details>
  <summary>Details</summary>
Motivation: 现有难度可控问题生成方法存在两个主要局限：无法直接生成选择题（教育场景中最常用的问题类型），以及没有明确训练来优化难度控制的准确性。

Method: 使用大语言模型，通过直接偏好优化技术进行训练，以提高难度控制的准确性。

Result: 该方法能够直接生成难度可控的多项选择题，改进了传统方法的局限性。

Conclusion: 所提出的方法在难度可控选择题生成方面具有更好的性能，为自适应学习支持提供了更有效的工具。

Abstract: Difficulty-controllable question generation for reading comprehension has
gained significant attention in the field of education as a fundamental tool
for adaptive learning support. Although several neural question generation
methods have recently succeeded in controlling difficulty, conventional
approaches still face two major limitations. First, they cannot directly
generate multiple-choice questions, which are the most widely used question
type in educational contexts. Second, they are not explicitly trained to
optimize the accuracy of difficulty control, leaving room for further
improvement in difficulty controllability. To address these limitations, this
study proposes a novel difficulty-controllable multiple-choice question
generation method for reading comprehension which leverages a large language
model trained using a direct preference optimization technique to improve the
accuracy of difficulty control.

</details>


### [32] [TheMCPCompany: Creating General-purpose Agents with Task-specific Tools](https://arxiv.org/abs/2510.19286)
*Reza Esfandiarpoor,Vishwas Suryanarayanan,Stephen H. Bach,Vishal Chowdhary,Anthony Aue*

Main category: cs.CL

TL;DR: 提出了TheMCPCompany基准，用于评估工具调用代理在真实世界服务任务中的表现，包含超过18,000个工具，实验显示先进模型能有效发现工具，但在复杂企业环境中仍面临挑战。


<details>
  <summary>Details</summary>
Motivation: 当前通用代理主要依赖浏览器与环境交互，而MCP协议提供了大量任务专用工具集，需要评估工具调用代理在实际服务任务中的表现。

Method: 使用真实服务的REST API创建MCP服务器，提供手动标注的真实工具，通过工具检索和真实工具两种方式评估代理性能。

Result: 使用真实工具时，工具调用代理能提升性能并降低成本；通过工具检索时，所有模型表现相似或优于浏览器代理，但小模型无法充分利用可用工具，GPT-5的工具检索性能接近真实工具性能。

Conclusion: 最先进的推理模型在简单环境中能有效发现工具，但在复杂企业环境中导航数万工具并以非平凡方式组合解决复杂问题仍具挑战，需要更好的推理和检索模型。

Abstract: Since the introduction of the Model Context Protocol (MCP), the number of
available tools for Large Language Models (LLMs) has increased significantly.
These task-specific tool sets offer an alternative to general-purpose tools
such as web browsers, while being easier to develop and maintain than GUIs.
However, current general-purpose agents predominantly rely on web browsers for
interacting with the environment. Here, we introduce TheMCPCompany, a benchmark
for evaluating tool-calling agents on tasks that involve interacting with
various real-world services. We use the REST APIs of these services to create
MCP servers, which include over 18,000 tools. We also provide manually
annotated ground-truth tools for each task. In our experiments, we use the
ground truth tools to show the potential of tool-calling agents for both
improving performance and reducing costs assuming perfect tool retrieval. Next,
we explore agent performance using tool retrieval to study the real-world
practicality of tool-based agents. While all models with tool retrieval perform
similarly or better than browser-based agents, smaller models cannot take full
advantage of the available tools through retrieval. On the other hand, GPT-5's
performance with tool retrieval is very close to its performance with
ground-truth tools. Overall, our work shows that the most advanced reasoning
models are effective at discovering tools in simpler environments, but
seriously struggle with navigating complex enterprise environments.
TheMCPCompany reveals that navigating tens of thousands of tools and combining
them in non-trivial ways to solve complex problems is still a challenging task
for current models and requires both better reasoning and better retrieval
models.

</details>


### [33] [JointCQ: Improving Factual Hallucination Detection with Joint Claim and Query Generation](https://arxiv.org/abs/2510.19310)
*Fan Xu,Huixuan Zhang,Zhenliang Zhang,Jiahao Wang,Xiaojun Wan*

Main category: cs.CL

TL;DR: JointCQ是一个联合声明-查询生成框架，通过精心设计的评估标准筛选合成训练数据，微调语言模型进行联合声明提取和查询生成，在多个开放域QA幻觉检测基准上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型存在幻觉问题，现有方法在声明提取阶段存在上下文丢失，在查询生成阶段特异性低，导致整个幻觉检测流程性能下降。

Method: 提出JointCQ框架，设计评估标准筛选训练数据，微调语言模型进行联合声明提取和查询生成，为下游搜索和验证提供可靠输入。

Result: 在多个开放域QA幻觉检测基准上优于现有方法，提高了语言模型系统的可信度和透明度。

Conclusion: JointCQ框架通过联合声明和查询生成，有效解决了现有幻觉检测方法在早期阶段的局限性，推进了更可信赖和透明的语言模型系统的发展。

Abstract: Current large language models (LLMs) often suffer from hallucination issues,
i,e, generating content that appears factual but is actually unreliable. A
typical hallucination detection pipeline involves response decomposition (i.e.,
claim extraction), query generation, evidence collection (i.e., search or
retrieval), and claim verification. However, existing methods exhibit
limitations in the first two stages, such as context loss during claim
extraction and low specificity in query generation, resulting in degraded
performance across the hallucination detection pipeline. In this work, we
introduce JointCQ https://github.com/pku0xff/JointCQ, a joint claim-and-query
generation framework designed to construct an effective and efficient
claim-query generator. Our framework leverages elaborately designed evaluation
criteria to filter synthesized training data, and finetunes a language model
for joint claim extraction and query generation, providing reliable and
informative inputs for downstream search and verification. Experimental results
demonstrate that our method outperforms previous methods on multiple
open-domain QA hallucination detection benchmarks, advancing the goal of more
trustworthy and transparent language model systems.

</details>


### [34] [KORE: Enhancing Knowledge Injection for Large Multimodal Models via Knowledge-Oriented Augmentations and Constraints](https://arxiv.org/abs/2510.19316)
*Kailin Jiang,Hongbo Jiang,Ning Jiang,Zhi Gao,Jinhe Bi,Yuchen Ren,Bin Li,Yuntao Du,Lei Liu,Qing Li*

Main category: cs.CL

TL;DR: KORE是一种协同方法，通过知识导向的增强和约束，在向大型多模态模型注入新知识的同时保留旧知识，解决了知识适应和知识保留的挑战。


<details>
  <summary>Details</summary>
Motivation: 大型多模态模型的知识是静态的，无法跟上现实世界的发展，这阻碍了持续知识获取。现有方法在学习新知识时往往难以避免灾难性遗忘。

Method: KORE自动将单个知识项转换为结构化知识以确保准确学习新知识，同时将先前知识存储在LMM线性层激活的协方差矩阵中，并通过将原始权重投影到矩阵的零空间来初始化适配器。

Result: 在LLaVA-v1.5-7B、LLaVA-v1.5-13B和Qwen2.5-VL-7B等多种LMM上的广泛实验表明，KORE实现了优越的新知识注入性能，并有效缓解了灾难性遗忘。

Conclusion: KORE方法在知识适应和知识保留方面表现出色，为大型多模态模型的持续知识更新提供了有效解决方案。

Abstract: Large Multimodal Models encode extensive factual knowledge in their
pre-trained weights. However, its knowledge remains static and limited, unable
to keep pace with real-world developments, which hinders continuous knowledge
acquisition. Effective knowledge injection thus becomes critical, involving two
goals: knowledge adaptation (injecting new knowledge) and knowledge retention
(preserving old knowledge). Existing methods often struggle to learn new
knowledge and suffer from catastrophic forgetting. To address this, we propose
KORE, a synergistic method of KnOwledge-oRientEd augmentations and constraints
for injecting new knowledge into large multimodal models while preserving old
knowledge. Unlike general text or image data augmentation, KORE automatically
converts individual knowledge items into structured and comprehensive knowledge
to ensure that the model accurately learns new knowledge, enabling accurate
adaptation. Meanwhile, KORE stores previous knowledge in the covariance matrix
of LMM's linear layer activations and initializes the adapter by projecting the
original weights into the matrix's null space, defining a fine-tuning direction
that minimizes interference with previous knowledge, enabling powerful
retention. Extensive experiments on various LMMs, including LLaVA-v1.5-7B,
LLaVA-v1.5-13B, and Qwen2.5-VL-7B, show that KORE achieves superior new
knowledge injection performance and effectively mitigates catastrophic
forgetting.

</details>


### [35] [HAD: HAllucination Detection Language Models Based on a Comprehensive Hallucination Taxonomy](https://arxiv.org/abs/2510.19318)
*Fan Xu,Xinyu Hu,Zhenghan Yu,Li Lin,Xu Zhang,Yang Zhang,Wei Zhou,Jinjie Gu,Xiaojun Wan*

Main category: cs.CL

TL;DR: 提出了一个包含11个类别的幻觉综合分类法，并开发了HAD模型，将幻觉检测、跨度级识别和纠正集成到单一推理过程中。在约90K样本的合成数据集上训练，在多个基准测试中达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 随着自然语言生成模型特别是大语言模型的广泛应用，模型产生看似合理但错误信息的幻觉问题日益突出，幻觉检测成为关键任务。

Method: 引入综合幻觉分类法，提出HAD模型集成检测、识别和纠正功能，使用约90K合成样本训练，并构建包含2,248样本的HADTest测试集。

Result: 在领域内和领域外测试集上，HAD模型普遍优于现有基线，在HaluEval、FactCHD和FaithBench上达到最先进结果，证实了其鲁棒性和通用性。

Conclusion: HAD模型通过集成检测、识别和纠正功能，在多个基准测试中表现出色，为解决NLG模型幻觉问题提供了有效解决方案。

Abstract: The increasing reliance on natural language generation (NLG) models,
particularly large language models, has raised concerns about the reliability
and accuracy of their outputs. A key challenge is hallucination, where models
produce plausible but incorrect information. As a result, hallucination
detection has become a critical task. In this work, we introduce a
comprehensive hallucination taxonomy with 11 categories across various NLG
tasks and propose the HAllucination Detection (HAD) models
https://github.com/pku0xff/HAD, which integrate hallucination detection,
span-level identification, and correction into a single inference process.
Trained on an elaborate synthetic dataset of about 90K samples, our HAD models
are versatile and can be applied to various NLG tasks. We also carefully
annotate a test set for hallucination detection, called HADTest, which contains
2,248 samples. Evaluations on in-domain and out-of-domain test sets show that
our HAD models generally outperform the existing baselines, achieving
state-of-the-art results on HaluEval, FactCHD, and FaithBench, confirming their
robustness and versatility.

</details>


### [36] [Balancing Rewards in Text Summarization: Multi-Objective Reinforcement Learning via HyperVolume Optimization](https://arxiv.org/abs/2510.19325)
*Junjie Song,Yiwen Liu,Dapeng Li,Yin Sun,Shukun Fu,Siqi Chen,Yuji Cao*

Main category: cs.CL

TL;DR: 提出了一种基于超体积优化的多目标强化学习策略HVO，用于优化文本摘要的多个目标（一致性、连贯性、相关性和流畅性），在多个数据集上表现优于GRPO方法，7B模型增强后可媲美GPT-4。


<details>
  <summary>Details</summary>
Motivation: 文本摘要需要同时优化多个目标，但现有基于LLM的强化学习方法很少关注多目标优化问题。

Method: 引入超体积优化(HVO)策略，在RL奖励过程中使用超体积方法动态调整组间分数，引导模型逐步逼近帕累托前沿。

Result: 在多个代表性摘要数据集上，HVO在总体得分上优于GRPO，在不同维度上表现更均衡；7B基础模型增强后与GPT-4性能相当，且生成长度更短。

Conclusion: HVO方法能有效解决文本摘要的多目标优化问题，生成平衡多个目标的摘要，性能优越且高效。

Abstract: Text summarization is a crucial task that requires the simultaneous
optimization of multiple objectives, including consistency, coherence,
relevance, and fluency, which presents considerable challenges. Although large
language models (LLMs) have demonstrated remarkable performance, enhanced by
reinforcement learning (RL), few studies have focused on optimizing the
multi-objective problem of summarization through RL based on LLMs. In this
paper, we introduce hypervolume optimization (HVO), a novel optimization
strategy that dynamically adjusts the scores between groups during the reward
process in RL by using the hypervolume method. This method guides the model's
optimization to progressively approximate the pareto front, thereby generating
balanced summaries across multiple objectives. Experimental results on several
representative summarization datasets demonstrate that our method outperforms
group relative policy optimization (GRPO) in overall scores and shows more
balanced performance across different dimensions. Moreover, a 7B foundation
model enhanced by HVO performs comparably to GPT-4 in the summarization task,
while maintaining a shorter generation length. Our code is publicly available
at https://github.com/ai4business-LiAuto/HVO.git

</details>


### [37] [Slot Filling as a Reasoning Task for SpeechLLMs](https://arxiv.org/abs/2510.19326)
*Kadri Hacioglu,Manjunath K E,Andreas Stolcke*

Main category: cs.CL

TL;DR: 将推理能力集成到语音大语言模型中，用于端到端槽位填充任务，通过思维链框架分解任务步骤，并展示推理步骤带来的性能提升。


<details>
  <summary>Details</summary>
Motivation: 受推理大语言模型发展的启发，希望将推理能力引入语音大语言模型，以提升槽位填充任务的性能。

Method: 使用思维链框架将槽位填充任务分解为多个推理步骤，创建推理数据集，并对语音大语言模型进行监督微调，比较不同类型和大小的文本基础模型。

Result: 引入推理步骤确实提升了性能，但主要为数学、逻辑和编码领域设计的推理文本大语言模型作为语音推理模型的基础模型可能表现较差。混合语音大语言模型（基于混合文本基础模型并保留直接和推理两种操作模式）表现最佳。

Conclusion: 将推理能力集成到语音大语言模型可以提升槽位填充性能，但需要选择合适的文本基础模型，混合模型（同时保留直接和推理模式）表现优于单一模式模型。

Abstract: We propose integration of reasoning into speech large language models
(speechLLMs) for the end-to-end slot-filling task. Inspired by the recent
development of reasoning LLMs, we use a chain-of-thought framework to decompose
the slot-filling task into multiple reasoning steps, create a reasoning dataset
and apply the supervised fine-tuning strategy to a speechLLM. We distinguish
between regular and reasoning speechLLMs and experiment with different types
and sizes of LLMs as their text foundation models. We demonstrate performance
improvements by introducing reasoning (intermediate) steps. However, we show
that a reasoning textual LLM developed mainly for math, logic and coding
domains might be inferior as a foundation model for a reasoning speechLLM. We
further show that hybrid speechLLMs, built on a hybrid text foundation LLM and
fine-tuned to preserve both direct and reasoning modes of operation, have
better performance than those fine-tuned employing only one mode of operation.

</details>


### [38] [Algorithmic Fairness in NLP: Persona-Infused LLMs for Human-Centric Hate Speech Detection](https://arxiv.org/abs/2510.19331)
*Ewelina Gajewska,Arda Derbent,Jaroslaw A Chudziak,Katarzyna Budzynska*

Main category: cs.CL

TL;DR: 研究个性化大语言模型在仇恨言论检测中的偏见问题，通过浅层和深层人物设定提示方法，分析群体身份对模型敏感性的影响。


<details>
  <summary>Details</summary>
Motivation: 探讨在仇恨言论检测中，标注者与目标群体身份关系（内群体vs外群体）如何影响LLM的偏见和公平性，将心理学群体身份理论与NLP技术结合。

Method: 使用Gemini和GPT-4.1-mini模型，采用两种人物设定提示方法：浅层人物提示和基于RAG的深度情境化人物开发，分析内群体和外群体标注者人物设定对模型性能的影响。

Result: 研究表明将社会人口属性融入LLM可以解决自动仇恨言论检测中的偏见，但基于人物设定的方法在减少偏见方面既有潜力也有局限性。

Conclusion: 基于人物设定的方法为开发更公平的仇恨言论检测系统提供了有价值的见解，但需要进一步改进以充分发挥其潜力。

Abstract: In this paper, we investigate how personalising Large Language Models
(Persona-LLMs) with annotator personas affects their sensitivity to hate
speech, particularly regarding biases linked to shared or differing identities
between annotators and targets. To this end, we employ Google's Gemini and
OpenAI's GPT-4.1-mini models and two persona-prompting methods: shallow persona
prompting and a deeply contextualised persona development based on
Retrieval-Augmented Generation (RAG) to incorporate richer persona profiles. We
analyse the impact of using in-group and out-group annotator personas on the
models' detection performance and fairness across diverse social groups. This
work bridges psychological insights on group identity with advanced NLP
techniques, demonstrating that incorporating socio-demographic attributes into
LLMs can address bias in automated hate speech detection. Our results highlight
both the potential and limitations of persona-based approaches in reducing
bias, offering valuable insights for developing more equitable hate speech
detection systems.

</details>


### [39] [Local Obfuscation by GLINER for Impartial Context Aware Lineage: Development and evaluation of PII Removal system](https://arxiv.org/abs/2510.19346)
*Prakrithi Shivaprakash,Lekhansh Shukla,Animesh Mukherjee,Prabhat Chand,Pratima Murthy*

Main category: cs.CL

TL;DR: LOGICAL是一个基于微调GLiNER模型的本地化PII去除系统，在临床笔记中高效去除个人身份信息，性能优于大型语言模型，适合资源受限环境。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在临床笔记PII去除中计算成本高和数据隐私风险的问题，特别是在低资源环境下。

Method: 使用1515份临床文档，定义9个PII类别，微调GLiNER模型，并与Azure NER、Presidio及Gemini-Pro-2.5、Llama-3.3-70B-Instruct进行对比评估。

Result: 微调GLiNER模型达到0.980的F1分数，显著优于Gemini-Pro-2.5的0.845，95%文档完全净化，在标准笔记本上高效运行。

Conclusion: 微调的专业化transformer模型如GLiNER为临床笔记PII去除提供了准确、计算高效且安全的解决方案，是资源密集型LLMs的实用替代方案。

Abstract: Removing Personally Identifiable Information (PII) from clinical notes in
Electronic Health Records (EHRs) is essential for research and AI development.
While Large Language Models (LLMs) are powerful, their high computational costs
and the data privacy risks of API-based services limit their use, especially in
low-resource settings. To address this, we developed LOGICAL (Local Obfuscation
by GLINER for Impartial Context-Aware Lineage), an efficient, locally
deployable PII removal system built on a fine-tuned Generalist and Lightweight
Named Entity Recognition (GLiNER) model. We used 1515 clinical documents from a
psychiatric hospital's EHR system. We defined nine PII categories for removal.
A modern-gliner-bi-large-v1.0 model was fine-tuned on 2849 text instances and
evaluated on a test set of 376 instances using character-level precision,
recall, and F1-score. We compared its performance against Microsoft Azure NER,
Microsoft Presidio, and zero-shot prompting with Gemini-Pro-2.5 and
Llama-3.3-70B-Instruct. The fine-tuned GLiNER model achieved superior
performance, with an overall micro-average F1-score of 0.980, significantly
outperforming Gemini-Pro-2.5 (F1-score: 0.845). LOGICAL correctly sanitised 95%
of documents completely, compared to 64% for the next-best solution. The model
operated efficiently on a standard laptop without a dedicated GPU. However, a
2% entity-level false negative rate underscores the need for human-in-the-loop
validation across all tested systems. Fine-tuned, specialised transformer
models like GLiNER offer an accurate, computationally efficient, and secure
solution for PII removal from clinical notes. This "sanitisation at the source"
approach is a practical alternative to resource-intensive LLMs, enabling the
creation of de-identified datasets for research and AI development while
preserving data privacy, particularly in resource-constrained environments.

</details>


### [40] [Modeling Turn-Taking with Semantically Informed Gestures](https://arxiv.org/abs/2510.19350)
*Varsha Suresh,M. Hamza Mughal,Christian Theobalt,Vera Demberg*

Main category: cs.CL

TL;DR: 本文扩展了DnD Gesture语料库，添加了2,663个语义手势标注，通过混合专家框架整合文本、音频和手势来预测对话轮次转换，实验表明语义手势能持续提升性能。


<details>
  <summary>Details</summary>
Motivation: 研究多模态线索在对话轮次转换中的作用，特别是手势作为语言和声学特征的补充信息。

Method: 使用DnD Gesture++语料库，通过混合专家框架整合文本、音频和手势特征来建模轮次转换预测。

Result: 实验结果显示，加入语义引导的手势特征相比基线模型能获得持续的性能提升。

Conclusion: 手势在多模态轮次转换中扮演补充角色，语义手势能有效提升预测性能。

Abstract: In conversation, humans use multimodal cues, such as speech, gestures, and
gaze, to manage turn-taking. While linguistic and acoustic features are
informative, gestures provide complementary cues for modeling these
transitions. To study this, we introduce DnD Gesture++, an extension of the
multi-party DnD Gesture corpus enriched with 2,663 semantic gesture annotations
spanning iconic, metaphoric, deictic, and discourse types. Using this dataset,
we model turn-taking prediction through a Mixture-of-Experts framework
integrating text, audio, and gestures. Experiments show that incorporating
semantically guided gestures yields consistent performance gains over
baselines, demonstrating their complementary role in multimodal turn-taking.

</details>


### [41] [M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models](https://arxiv.org/abs/2510.19358)
*Yejin Kwon,Taewoo Kang,Hyunsoo Yoon,Changouk Kim*

Main category: cs.CL

TL;DR: M3-SLU是一个新的多模态大语言模型基准，用于评估多说话人、多轮对话的口语理解能力，重点关注说话人归属推理问题。


<details>
  <summary>Details</summary>
Motivation: 现有模型在语音和文本理解方面表现良好，但在理解自然对话中谁在什么时候说了什么（说话人归属推理）方面仍存在困难。

Method: 基于四个开放语料库构建了包含12,000多个验证实例的数据集，包含音频、转录文本和元数据，并设计了两个任务：说话人归属问答和通过话语匹配的说话人归属。

Result: 模型能够捕捉说了什么内容，但经常无法识别是谁说的，揭示了说话人感知对话理解的关键差距。

Conclusion: M3-SLU作为一个具有挑战性的基准，旨在推动说话人感知多模态理解的研究发展。

Abstract: We present M3-SLU, a new multimodal large language model (MLLM) benchmark for
evaluating multi-speaker, multi-turn spoken language understanding. While
recent models show strong performance in speech and text comprehension, they
still struggle with speaker-attributed reasoning, the ability to understand who
said what and when in natural conversations. M3-SLU is built from four open
corpora (CHiME-6, MELD, MultiDialog, and AMI) and comprises over 12,000
validated instances with paired audio, transcripts, and metadata. It includes
two tasks: (1) Speaker-Attributed Question Answering and (2) Speaker
Attribution via Utterance Matching. We provide baseline results for both
cascaded pipelines and end-to-end MLLMs, evaluated using an LLM-as-Judge and
accuracy metrics. Results show that while models can capture what was said,
they often fail to identify who said it, revealing a key gap in speaker-aware
dialogue understanding. M3-SLU offers as a challenging benchmark to advance
research in speaker-aware multimodal understanding.

</details>


### [42] [AgenticMath: Enhancing LLM Reasoning via Agentic-based Math Data Generation](https://arxiv.org/abs/2510.19361)
*Xianyang Liu,Yilin Liu,Shuai Wang,Hao Cheng,Andrew Estornell,Yuzhi Zhao,Jiaheng Wei*

Main category: cs.CL

TL;DR: AgenticMath是一个用于生成高质量数学问答对的智能代理流程，通过四阶段方法提升LLM的数学推理能力，仅需3-6万样本就能在3B-8B参数模型上达到或超越使用更大规模数据的基线性能。


<details>
  <summary>Details</summary>
Motivation: 当前方法在生成高质量数据集以改进LLM推理方面存在挑战，常产生低质量/错误答案且信息丰富度有限，需要更有效的数据生成方法。

Method: 四阶段流程：1)种子问题筛选；2)多代理问题重述；3)思维链答案增强；4)问答对评估。通过智能代理生成多样化、逻辑一致的数学问题，并使用思维链推理增强答案正确性。

Result: 在3B-8B参数LLM上使用AgenticMath生成的数据集（仅3-6万数学样本）进行微调，在多样数学推理基准测试中达到或超越了使用更大规模数据（如40万或230万样本）的基线性能。

Conclusion: 针对性的高质量数据生成比大规模低质量数据更能有效提升LLM的数学推理能力，展示了数据质量优于数量的重要性。

Abstract: The creation of high-quality datasets to improve Large Language Model (LLM)
reasoning remains a significant challenge, as current methods often suffer from
generating low-quality/incorrect answers and limited information richness from
available data sources. To address this, we propose AgenticMath, a novel
agentic pipeline for generating high-quality mathematical question-answer pairs
to enhance the supervised fine-tuning of LLMs. Our method operates through four
stages: (1) Seed Question Filter that selects questions with high information
richness, complexity, and clarity; (2) an Agentic Question Rephrase step that
employs a multi-agent system to generate diverse, logically consistent
paraphrases; (3) an Answer Augment step where rewrite answers using
chain-of-thought reasoning to enhance numerical and logical correctness,
without reliance on human-provided labels; and (4) a final Question and Answer
Evaluation that retains only the most superior pairs. Extensive experiments
demonstrate that, fine-tuning 3B-8B parameter LLMs on AgenticMath generated
datasets (comprising only 30-60K math samples) achieves competitive or superior
performance on diverse in domain and out-of-domain mathematical reasoning
benchmarks compared to baselines trained on much more data (e.g., 400K or 2.3M
samples). Our work demonstrates that targeted, high-quality data generation is
a more efficient path to improving mathematical reasoning in LLMs than
large-scale, low-quality alternatives.

</details>


### [43] [LoongRL:Reinforcement Learning for Advanced Reasoning over Long Contexts](https://arxiv.org/abs/2510.19363)
*Siyuan Wang,Gaokai Zhang,Li Lyna Zhang,Ning Shang,Fan Yang,Dongyao Chen,Mao Yang*

Main category: cs.CL

TL;DR: LoongRL是一种数据驱动的强化学习方法，通过KeyChain技术将短多跳QA转化为高难度长上下文任务，训练模型在16K长度下有效解决128K任务，显著提升长上下文推理能力。


<details>
  <summary>Details</summary>
Motivation: 长上下文推理对大型语言模型至关重要，但现有强化学习方法主要针对短上下文，缺乏对长上下文高级推理模式的探索，且高难度RL数据稀缺。

Method: 提出LoongRL方法，核心是KeyChain合成技术：通过插入UUID链将短多跳QA转化为长上下文任务，模型需要追踪正确链、识别真实问题、检索相关事实并进行推理。

Result: 在Qwen2.5-7B和14B上，LoongRL将长上下文多跳QA准确率分别提升23.5%和21.1%。LoongRL-14B达到74.2分，媲美更大模型，同时通过128K压力测试并保持短上下文推理能力。

Conclusion: LoongRL通过数据驱动方法有效解决了长上下文推理挑战，在有限训练长度下实现泛化，为长上下文推理提供了高效解决方案。

Abstract: Reasoning over long contexts is essential for large language models. While
reinforcement learning (RL) enhances short-context reasoning by inducing "Aha"
moments in chain-of-thought, the advanced thinking patterns required for
long-context reasoning remain largely unexplored, and high-difficulty RL data
are scarce. In this paper, we introduce LoongRL, a data-driven RL method for
advanced long-context reasoning. Central to LoongRL is KeyChain, a synthesis
approach that transforms short multi-hop QA into high-difficulty long-context
tasks by inserting UUID chains that hide the true question among large
collections of distracting documents. Solving these tasks requires the model to
trace the correct chain step-by-step, identify the true question, retrieve
relevant facts and reason over them to answer correctly. RL training on
KeyChain data induces an emergent plan-retrieve-reason-recheck reasoning
pattern that generalizes far beyond training length. Models trained at 16K
effectively solve 128K tasks without prohibitive full-length RL rollout costs.
On Qwen2.5-7B and 14B, LoongRL substantially improves long-context multi-hop QA
accuracy by +23.5% and +21.1% absolute gains. The resulting LoongRL-14B reaches
a score of 74.2, rivaling much larger frontier models such as o3-mini (74.5)
and DeepSeek-R1 (74.9). It also improves long-context retrieval, passes all
128K needle-in-a-haystack stress tests, and preserves short-context reasoning
capabilities.

</details>


### [44] [The Massive Legal Embedding Benchmark (MLEB)](https://arxiv.org/abs/2510.19365)
*Umar Butler,Abdur-Rahman Butler,Adrian Lucas Malec*

Main category: cs.CL

TL;DR: MLEB是迄今为止最大、最多样、最全面的开源法律信息检索基准，包含10个专家标注的数据集，涵盖多个司法管辖区、文档类型和任务类型。


<details>
  <summary>Details</summary>
Motivation: 填补开源法律信息检索领域在司法管辖区和领域方面的空白，为法律信息检索提供更全面和多样化的评估基准。

Method: 构建了10个专家标注的数据集，涵盖美国、英国、欧盟、澳大利亚、爱尔兰和新加坡等多个司法管辖区，包括案件、立法、监管指南、合同和文献等文档类型，以及搜索、零样本分类和问答等任务类型。其中7个数据集是新构建的。

Result: 发布了MLEB基准，包含代码、结果和数据，以支持可重复的评估。

Conclusion: MLEB为法律信息检索领域提供了首个大规模、多样化的开源基准，填补了现有基准在司法管辖区和文档类型覆盖方面的不足。

Abstract: We present the Massive Legal Embedding Benchmark (MLEB), the largest, most
diverse, and most comprehensive open-source benchmark for legal information
retrieval to date. MLEB consists of ten expert-annotated datasets spanning
multiple jurisdictions (the US, UK, EU, Australia, Ireland, and Singapore),
document types (cases, legislation, regulatory guidance, contracts, and
literature), and task types (search, zero-shot classification, and question
answering). Seven of the datasets in MLEB were newly constructed in order to
fill domain and jurisdictional gaps in the open-source legal information
retrieval landscape. We document our methodology in building MLEB and creating
the new constituent datasets, and release our code, results, and data openly to
assist with reproducible evaluations.

</details>


### [45] [MoE-Prism: Disentangling Monolithic Experts for Elastic MoE Services via Model-System Co-Designs](https://arxiv.org/abs/2510.19366)
*Xinfeng Xia,Jiacheng Liu,Xiaofeng Hou,Peng Tang,Mingxuan Zhang,Wenfeng Wang,Chao Li*

Main category: cs.CL

TL;DR: MoE-Prism通过模型-系统协同设计，将僵化的MoE模型转变为弹性服务，提供细粒度控制能力，在保持质量的同时实现资源优化。


<details>
  <summary>Details</summary>
Motivation: 传统MoE模型存在"质量悬崖"问题，只能在少数粗粒度操作点之间切换，无法适应多样化的服务级别目标，导致资源过度配置和成本-质量权衡困难。

Method: 采用两阶段方法：离线重构引擎将单体专家分解为细粒度"子专家"，使用基于元启发式的分区优化算法；在线调度引擎实现QoS感知调度，包含吞吐量最大化和延迟优化卸载策略。

Result: 在三种不同MoE模型上的评估显示，MoE-Prism提供超过4倍的不同稳定操作点，在严格延迟预算下吞吐量提升19.9%，在有限资源下延迟降低10.36%。

Conclusion: MoE-Prism填补了模型与系统之间的鸿沟，为下一代自适应、高效和QoS感知的AI服务提供了关键的"控制旋钮"。

Abstract: Mixture-of-Experts (MoE) models, the state-of-the-art in large-scale AI,
achieve high quality by sparsely activating parameters. However, their reliance
on routing between a few monolithic experts via a top-k mechanism creates a
"quality cliff", offering only a few coarse-grained operating points. This
inflexibility forces a difficult trade-off between cost and quality, preventing
adaptation to diverse Service Level Objectives (SLOs) and leading to
significant resource over-provisioning.
  This paper introduces MoE-Prism, a model-system co-design that transforms
rigid MoE models into elastic services. Our methodology is divided into two
phases. First, an \emph{Offline Refactoring Engine} systematically deconstructs
monolithic experts into fine-grained "sub-experts." This engine employs a
partitioning optimization solver that uses a metaheuristic-based approach to
group neurons, preserving functional locality without requiring retraining.
Second, an \emph{Online Scheduling Engine} leverages this new elasticity
through QoS-aware scheduling. It implements specialized policies to solve
complex system problems, including maximizing throughput in cloud deployments
and managing latency-optimized offloading for memory-constrained devices. Our
evaluation across three different MoE models shows that MoE-Prismprovides over
4 times more distinct, stable operating points than the baseline. This allows
an AI service to dynamically improve throughput by up to 19.9\% under a strict
latency budget or reduce latency by up to 10.36\% under limited resources.
MoE-Prism provides the critical "control knob" to bridge the model-system gap,
enabling the next generation of adaptive, efficient, and QoS-aware AI services.

</details>


### [46] [Sign Language Translation with Sentence Embedding Supervision](https://arxiv.org/abs/2510.19367)
*Yasser Hamidullah,Josef van Genabith,Cristina España-Bonet*

Main category: cs.CL

TL;DR: 提出了一种新颖的手语翻译方法，使用目标句子的句子嵌入替代传统的手语注释，无需手动标注，在德语和美国手语数据集上显著优于其他无注释方法


<details>
  <summary>Details</summary>
Motivation: 现有手语翻译系统依赖手语注释，但这类标注数据通常规模有限且不同数据集间差异很大，需要寻找不依赖手动标注的替代方案

Method: 使用目标句子的句子嵌入作为训练时的监督信号，这种监督不需要手动标注，而是从原始文本数据中学习，支持单语和多语言句子嵌入及翻译系统

Result: 在德语(PHOENIX-2014T)和美国手语(How2Sign)数据集上的评估显示，该方法显著优于其他无注释方法，在没有额外手语翻译数据集预训练的情况下，缩小了无注释与依赖注释系统之间的差距

Conclusion: 该方法为手语注释不可用的数据集设立了新的最先进水平，通过句子嵌入有效替代了传统的手语注释监督

Abstract: State-of-the-art sign language translation (SLT) systems facilitate the
learning process through gloss annotations, either in an end2end manner or by
involving an intermediate step. Unfortunately, gloss labelled sign language
data is usually not available at scale and, when available, gloss annotations
widely differ from dataset to dataset. We present a novel approach using
sentence embeddings of the target sentences at training time that take the role
of glosses. The new kind of supervision does not need any manual annotation but
it is learned on raw textual data. As our approach easily facilitates
multilinguality, we evaluate it on datasets covering German (PHOENIX-2014T) and
American (How2Sign) sign languages and experiment with mono- and multilingual
sentence embeddings and translation systems. Our approach significantly
outperforms other gloss-free approaches, setting the new state-of-the-art for
data sets where glosses are not available and when no additional SLT datasets
are used for pretraining, diminishing the gap between gloss-free and
gloss-dependent systems.

</details>


### [47] [SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision](https://arxiv.org/abs/2510.19398)
*Yasser Hamidullah,Shakib Yazdani,Cennet Oguz,Josef van Genabith,Cristina España-Bonet*

Main category: cs.CL

TL;DR: 使用语言无关的多模态嵌入来监督手语翻译，结合多语言目标增强和视频级扰动，提升模型鲁棒性和跨语言泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统手语翻译通常使用单一语言的文本进行训练，限制了可扩展性和跨语言泛化能力。现有方法虽然用文本句子嵌入替代了gloss监督，但仍局限于特定语言和模态。

Method: 采用在多种语言文本和语音上训练的语言无关多模态嵌入来监督SLT，实现直接多语言翻译。提出耦合增强方法，结合多语言目标增强（翻译成多种语言）和视频级扰动，提高模型鲁棒性。

Result: 实验显示在BLEURT指标上持续优于仅使用文本句子嵌入监督的方法，在低资源设置下改进更显著。

Conclusion: 语言无关嵌入监督结合耦合增强为传统SLT训练提供了可扩展且语义鲁棒的替代方案。

Abstract: Sign language translation (SLT) is typically trained with text in a single
spoken language, which limits scalability and cross-language generalization.
Earlier approaches have replaced gloss supervision with text-based sentence
embeddings, but up to now, these remain tied to a specific language and
modality. In contrast, here we employ language-agnostic, multimodal embeddings
trained on text and speech from multiple languages to supervise SLT, enabling
direct multilingual translation. To address data scarcity, we propose a coupled
augmentation method that combines multilingual target augmentations (i.e.
translations into many languages) with video-level perturbations, improving
model robustness. Experiments show consistent BLEURT gains over text-only
sentence embedding supervision, with larger improvements in low-resource
settings. Our results demonstrate that language-agnostic embedding supervision,
combined with coupled augmentation, provides a scalable and semantically robust
alternative to traditional SLT training.

</details>


### [48] [ToMMeR -- Efficient Entity Mention Detection from Large Language Models](https://arxiv.org/abs/2510.19410)
*Victor Morand,Nadi Tomeh,Josiane Mothe,Benjamin Piwowarski*

Main category: cs.CL

TL;DR: ToMMeR是一个轻量级模型（<30万参数），通过探测早期LLM层实现了高效的实体提及检测，在13个NER基准测试中达到93%的零样本召回率，且很少产生虚假预测。


<details>
  <summary>Details</summary>
Motivation: 实体提及检测是信息提取的基础，但也是性能瓶颈。研究旨在探索早期transformer层中是否存在结构化的实体表示，并能否通过轻量级模型有效恢复。

Method: 开发ToMMeR模型，通过探测早期LLM层来提取实体提及检测能力，使用LLM作为判断器来评估预测质量，并进行跨模型架构分析。

Result: 在13个NER基准测试中实现93%的零样本召回率，90%以上的精确度；跨模型分析显示不同架构（14M-15B参数）在实体边界识别上高度一致（DICE >75%）；扩展分类头后达到接近SOTA的NER性能（80-87% F1）。

Conclusion: 实体提及检测能力自然地从语言建模中涌现，结构化实体表示存在于早期transformer层中，可以通过少量参数高效恢复。

Abstract: Identifying which text spans refer to entities -- mention detection -- is
both foundational for information extraction and a known performance
bottleneck. We introduce ToMMeR, a lightweight model (<300K parameters) probing
mention detection capabilities from early LLM layers. Across 13 NER benchmarks,
ToMMeR achieves 93\% recall zero-shot, with over 90\% precision using an LLM as
a judge showing that ToMMeR rarely produces spurious predictions despite high
recall. Cross-model analysis reveals that diverse architectures (14M-15B
parameters) converge on similar mention boundaries (DICE >75\%), confirming
that mention detection emerges naturally from language modeling. When extended
with span classification heads, ToMMeR achieves near SOTA NER performance
(80-87\% F1 on standard benchmarks). Our work provides evidence that structured
entity representations exist in early transformer layers and can be efficiently
recovered with minimal parameters.

</details>


### [49] [Spatio-temporal Sign Language Representation and Translation](https://arxiv.org/abs/2510.19413)
*Yasser Hamidullah,Josef van Genabith,Cristina España-Bonet*

Main category: cs.CL

TL;DR: DFKI-MLT团队提交了WMT-SLT 2022手语翻译任务系统，从瑞士德语手语视频翻译为德语文本，采用端到端架构学习时空特征表示。


<details>
  <summary>Details</summary>
Motivation: 现有手语翻译系统通常使用通用seq2seq架构，但标准方法往往无法充分利用时间特征，需要改进以更好地泛化到新数据集。

Method: 提出单一模型同时学习时空特征表示和翻译，实现真正的端到端架构，而不是使用预提取的视频帧特征。

Result: 在开发集上达到5±1 BLEU分，但在测试集上性能显著下降至0.11±0.06 BLEU分。

Conclusion: 端到端手语翻译架构在开发集上表现良好，但在测试集上泛化能力有限，需要进一步改进。

Abstract: This paper describes the DFKI-MLT submission to the WMT-SLT 2022 sign
language translation (SLT) task from Swiss German Sign Language (video) into
German (text). State-of-the-art techniques for SLT use a generic seq2seq
architecture with customized input embeddings. Instead of word embeddings as
used in textual machine translation, SLT systems use features extracted from
video frames. Standard approaches often do not benefit from temporal features.
In our participation, we present a system that learns spatio-temporal feature
representations and translation in a single model, resulting in a real
end-to-end architecture expected to better generalize to new data sets. Our
best system achieved $5\pm1$ BLEU points on the development set, but the
performance on the test dropped to $0.11\pm0.06$ BLEU points.

</details>


### [50] [BLiSS 1.0: Evaluating Bilingual Learner Competence in Second Language Small Language Models](https://arxiv.org/abs/2510.19419)
*Yuan Gao,Suchir Salhan,Andrew Caines,Paula Buttery,Weiwei Sun*

Main category: cs.CL

TL;DR: BLiSS 1.0是一个评估认知启发模型的基准，通过选择性容忍度测试模型对自然学习者错误与人工错误的偏好差异。


<details>
  <summary>Details</summary>
Motivation: 弥补性能导向基准与认知启发模型评估之间的差距，验证不同训练目标如何影响模型与人类语言习得模式的契合度。

Method: 基于280万自然学习者句子构建136,867个控制三元组（正确、学习者、人工错误），测试模型对自然学习者错误与匹配人工错误的偏好。

Result: 实验表明选择性容忍度与标准语法性是不同能力，性能按训练范式聚类，验证BLiSS作为评估训练目标影响模型与人类语言习得模式对齐的稳健工具。

Conclusion: BLiSS基准成功验证了选择性容忍度作为评估模型认知对齐度的有效指标，为研究训练目标对语言习得模式的影响提供了可靠工具。

Abstract: To bridge the gap between performance-oriented benchmarks and the evaluation
of cognitively inspired models, we introduce BLiSS 1.0, a Benchmark of Learner
Interlingual Syntactic Structure. Our benchmark operationalizes a new paradigm
of selective tolerance, testing whether a model finds a naturalistic learner
error more plausible than a matched, artificial error within the same sentence.
Constructed from over 2.8 million naturalistic learner sentences, BLiSS
provides 136,867 controlled triplets (corrected, learner, artificial) for this
purpose. Experiments on a diverse suite of models demonstrate that selective
tolerance is a distinct capability from standard grammaticality, with
performance clustering strongly by training paradigm. This validates BLiSS as a
robust tool for measuring how different training objectives impact a model's
alignment with the systematic patterns of human language acquisition.

</details>


### [51] [MINED: Probing and Updating with Multimodal Time-Sensitive Knowledge for Large Multimodal Models](https://arxiv.org/abs/2510.19457)
*Kailin Jiang,Ning Jiang,Yuchen Ren,Yuchen Li,Yifan Gao,Jinhe Bi,Yunpu Ma,Qingqing Liu,Xianhao Wang,Yifan Jia,Hongbo Jiang,Yaocong Hu,Bin Li,Lei Liu,Yuntao Du*

Main category: cs.CL

TL;DR: MINED是一个评估大型多模态模型时间感知能力的综合基准，包含6个维度和11个任务，涵盖2104个时间敏感知识样本。评估显示大多数开源LMM缺乏时间理解能力，但可以通过知识编辑方法有效更新知识。


<details>
  <summary>Details</summary>
Motivation: 现有基准的静态设计无法充分评估LMMs对时间敏感知识的理解能力，需要开发专门的评估框架。

Method: 从维基百科构建包含2104个时间敏感知识样本的MINED基准，评估15个广泛使用的LMMs，并研究通过知识编辑方法更新时间敏感知识的可行性。

Result: Gemini-2.5-Pro获得最高平均CEM分数63.07，大多数开源LMMs缺乏时间理解能力。LMMs在组织知识上表现最好，在体育知识上表现最弱。知识编辑方法在单次编辑场景中能有效更新知识。

Conclusion: MINED基准揭示了LMMs在时间敏感知识理解方面的局限性，但知识编辑方法为更新这类知识提供了可行途径。

Abstract: Large Multimodal Models (LMMs) encode rich factual knowledge via cross-modal
pre-training, yet their static representations struggle to maintain an accurate
understanding of time-sensitive factual knowledge. Existing benchmarks remain
constrained by static designs, inadequately evaluating LMMs' ability to
understand time-sensitive knowledge. To address this gap, we propose MINED, a
comprehensive benchmark that evaluates temporal awareness along 6 key
dimensions and 11 challenging tasks: cognition, awareness, trustworthiness,
understanding, reasoning, and robustness. MINED is constructed from Wikipedia
by two professional annotators, containing 2,104 time-sensitive knowledge
samples spanning six knowledge types. Evaluating 15 widely used LMMs on MINED
shows that Gemini-2.5-Pro achieves the highest average CEM score of 63.07,
while most open-source LMMs still lack time understanding ability. Meanwhile,
LMMs perform best on organization knowledge, whereas their performance is
weakest on sport. To address these challenges, we investigate the feasibility
of updating time-sensitive knowledge in LMMs through knowledge editing methods
and observe that LMMs can effectively update knowledge via knowledge editing
methods in single editing scenarios.

</details>


### [52] [Re-evaluating Minimum Bayes Risk Decoding for Automatic Speech Recognition](https://arxiv.org/abs/2510.19471)
*Yuu Jinnai*

Main category: cs.CL

TL;DR: MBR解码在语音转文本任务（ASR和ST）中表现优于波束搜索，在大多数实验设置中提高了准确性。


<details>
  <summary>Details</summary>
Motivation: 鉴于MBR解码在文本到文本生成任务中有效，期望其在语音到文本任务中也有效，但目前语音转文本任务仍主要使用波束搜索。

Method: 在英语和日语上使用Whisper及其衍生模型评估MBR解码在ASR和ST任务中的表现。

Result: MBR解码在大多数实验设置中的准确性超过了波束搜索。

Conclusion: MBR解码是离线ASR和ST任务中追求高准确性的有前景方法。

Abstract: Recent work has shown that sample-based Minimum Bayes Risk (MBR) decoding
outperforms beam search in text-to-text generation tasks, such as machine
translation, text summarization, and image captioning. On the other hand, beam
search is the current practice for speech-to-text tasks such as automatic
speech recognition (ASR) and Speech Translation (ST). Given that MBR decoding
is effective in text-to-text generation tasks, it is reasonable to expect it to
also be effective for speech-to-text tasks. In this paper, we evaluate MBR
decoding for ASR and ST tasks on English and Japanese using Whisper and its
derivative models. We observe that the accuracy of MBR decoding outperforms
that of beam search in most of the experimental settings we have evaluated. The
results show that MBR decoding is a promising method for offline ASR and ST
tasks that require high accuracy. The code is available at
https://github.com/CyberAgentAILab/mbr-for-asr

</details>


### [53] [VideoAgentTrek: Computer Use Pretraining from Unlabeled Videos](https://arxiv.org/abs/2510.19488)
*Dunjie Lu,Yiheng Xu,Junli Wang,Haoyuan Wu,Xinyuan Wang,Zekun Wang,Junlin Yang,Hongjin Su,Jixuan Chen,Junda Chen,Yuchen Mao,Jingren Zhou,Junyang Lin,Binyuan Hui,Tao Yu*

Main category: cs.CL

TL;DR: VideoAgentTrek是一个从公开屏幕录制视频自动挖掘训练数据的可扩展管道，无需手动标注，显著提升了计算机使用代理的性能。


<details>
  <summary>Details</summary>
Motivation: 训练计算机使用代理需要大量的GUI交互数据，但手动标注成本高昂。本文旨在利用公开的屏幕录制视频自动生成训练数据，解决数据稀缺问题。

Method: 开发了Video2Action逆向动力学模块，包含视频定位模型（检测GUI动作的时间边界和上下文）和动作内容识别器（提取点击坐标和输入文本等结构化参数）。应用该管道处理39,000个YouTube教程视频。

Result: 自动生成了152万交互步骤。在OSWorld-Verified上，任务成功率从9.3%提升至15.8%（相对提升70%）；在AgentNetBench上，步骤准确率从64.1%提升至69.3%。

Conclusion: 被动互联网视频可以转化为高质量的训练监督信号，为计算机使用代理提供了可扩展的替代方案，避免了昂贵的手动标注。

Abstract: Training computer-use agents requires massive amounts of GUI interaction
data, but manually annotating action trajectories at scale is prohibitively
expensive. We present VideoAgentTrek, a scalable pipeline that automatically
mines training data from publicly available screen-recorded videos at web
scale, eliminating the need for manual annotation. Our approach addresses a key
challenge: raw videos contain implicit demonstrations but lack explicit action
labels. To solve this, we develop Video2Action, an inverse dynamics module
(IDM) with two components: (1) a video grounding model that detects and
localizes GUI actions with precise temporal boundaries and context, and (2) an
action-content recognizer that extracts structured parameters like click
coordinates and typed text with high fidelity. Applied to 39,000 YouTube
tutorial videos, our pipeline generates 1.52 million interaction steps
automatically. We leverage this data through continued pretraining followed by
supervised fine-tuning. On OSWorld-Verified, our approach improves task success
rates from 9.3% (SFT-only baseline) to 15.8%, a 70% relative improvement. On
AgentNetBench, step accuracy increases from 64.1% to 69.3%. Our results
demonstrate that passive internet videos can be transformed into high-quality
supervision for computer-use agents, providing a scalable alternative to
expensive manual annotation.

</details>


### [54] [Machine Text Detectors are Membership Inference Attacks](https://arxiv.org/abs/2510.19492)
*Ryuto Koike,Liam Dugan,Masahiro Kaneko,Chris Callison-Burch,Naoaki Okazaki*

Main category: cs.CL

TL;DR: 本文研究了成员推理攻击和机器生成文本检测之间的可迁移性，证明了两种任务的最优性能度量相同，并通过大规模实验验证了跨任务性能的高度相关性。


<details>
  <summary>Details</summary>
Motivation: 尽管成员推理攻击和机器生成文本检测目标不同，但方法都基于语言模型的概率分布信号。这两种任务被独立研究，可能导致忽略另一任务中开发的更强方法和宝贵见解。

Method: 通过理论和实证研究探讨两种任务间的可迁移性。理论上证明两种任务的最优性能度量相同；实证上使用7种最先进的MIA方法和5种机器文本检测器，在13个领域和10个生成器上进行大规模实验。

Result: 实验显示跨任务性能具有非常强的秩相关性（rho > 0.6）。特别发现Binoculars（原为机器文本检测设计）在MIA基准测试中也达到最先进性能。

Conclusion: 研究结果强调两个研究社区需要更多的跨任务意识和合作。为此引入了MINT，一个统一的评估套件，包含来自两个任务的15种最新方法实现。

Abstract: Although membership inference attacks (MIAs) and machine-generated text
detection target different goals, identifying training samples and synthetic
texts, their methods often exploit similar signals based on a language model's
probability distribution. Despite this shared methodological foundation, the
two tasks have been independently studied, which may lead to conclusions that
overlook stronger methods and valuable insights developed in the other task. In
this work, we theoretically and empirically investigate the transferability,
i.e., how well a method originally developed for one task performs on the
other, between MIAs and machine text detection. For our theoretical
contribution, we prove that the metric that achieves the asymptotically highest
performance on both tasks is the same. We unify a large proportion of the
existing literature in the context of this optimal metric and hypothesize that
the accuracy with which a given method approximates this metric is directly
correlated with its transferability. Our large-scale empirical experiments,
including 7 state-of-the-art MIA methods and 5 state-of-the-art machine text
detectors across 13 domains and 10 generators, demonstrate very strong rank
correlation (rho > 0.6) in cross-task performance. We notably find that
Binoculars, originally designed for machine text detection, achieves
state-of-the-art performance on MIA benchmarks as well, demonstrating the
practical impact of the transferability. Our findings highlight the need for
greater cross-task awareness and collaboration between the two research
communities. To facilitate cross-task developments and fair evaluations, we
introduce MINT, a unified evaluation suite for MIAs and machine-generated text
detection, with implementation of 15 recent methods from both tasks.

</details>


### [55] [What is the Best Sequence Length for BABYLM?](https://arxiv.org/abs/2510.19493)
*Suchir Salhan,Richard Diehl Martinez,Zébulon Goriely,Paula Buttery*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Transformer language models typically operate with a fixed-length context
window, which has grown in step with large-scale pretraining datasets. In the
BabyLM Challenge, however, many past submissions have defaulted to using much
shorter sequence lengths. We examine the impact of sequence length on BabyLM
pretraining, to answer the simple question: what sequence length should we be
using when training Baby LMs? Using 100M-word training data and fixed compute
budgets, we compare 125M-parameter Mamba and OPT models, finding that although
longer is often better, the optimal length depends on both task and
architecture. Shorter sequences are sufficient for grammatical generalization
tasks whereas longer contexts benefit morphological analogical reasoning tasks.

</details>


### [56] [Lookahead Routing for Large Language Models](https://arxiv.org/abs/2510.19506)
*Canbin Huang,Tianyuan Shi,Yuhua Zhu,Ruijun Chen,Xiaojun Quan*

Main category: cs.CL

TL;DR: Lookahead是一个LLM路由框架，通过预测潜在输出来指导模型选择，在多个基准测试中平均性能提升7.7%


<details>
  <summary>Details</summary>
Motivation: 现有路由方法仅基于输入查询进行分类，忽略了潜在输出中的有价值信息，无法捕捉隐含意图和上下文细微差别，导致复杂或模糊查询的路由决策不理想

Method: 提出Lookahead框架，通过预测模型输出的潜在表示来"预见"可能的输出，使用这些预测指导模型选择，实现无需完整推理的知情路由。实现了基于因果和掩码语言模型的两种方法

Result: 在七个公共基准测试（指令遵循、数学推理和代码生成）上的实证评估显示，Lookahead始终优于现有路由基线，平均性能提升7.7%

Conclusion: Lookahead通过预见潜在输出改进了LLM路由决策，在保持效率的同时提高了路由质量，为多模型系统提供了更有效的路由解决方案

Abstract: Large language model (LLM) routers improve the efficiency of multi-model
systems by directing each query to the most appropriate model while leveraging
the diverse strengths of heterogeneous LLMs. Most existing approaches frame
routing as a classification problem based solely on the input query. While this
reduces overhead by avoiding inference across all models, it overlooks valuable
information that could be gleaned from potential outputs and fails to capture
implicit intent or contextual nuances that often emerge only during response
generation. These limitations can result in suboptimal routing decisions,
particularly for complex or ambiguous queries that require deeper semantic
understanding. To address this challenge, we propose Lookahead, a routing
framework that "foresees" potential model outputs by predicting their latent
representations and uses these predictions to guide model selection, thus
enabling more informed routing without full inference. Within this framework,
we implement two approaches based on causal and masked language models.
Empirical evaluations across seven public benchmarks - spanning instruction
following, mathematical reasoning, and code generation - show that Lookahead
consistently outperforms existing routing baselines, achieving an average
performance gain of 7.7% over the state-of-the-art. Our code is available at
https://github.com/huangcb01/lookahead-routing.

</details>


### [57] [Which Evaluation for Which Model? A Taxonomy for Speech Model Assessment](https://arxiv.org/abs/2510.19509)
*Maureen de Seyssel,Eeshan Gunesh Dhekane*

Main category: cs.CL

TL;DR: 该论文提出了一个统一分类法来解决语音基础模型的评估问题，定义三个正交轴：评估方面、模型能力和任务要求，为选择合适的评估方法提供框架。


<details>
  <summary>Details</summary>
Motivation: 语音基础模型在不同任务上表现出色，但评估方法分散且不统一，需要系统化的评估框架来指导模型与评估方法的匹配。

Method: 提出三轴分类法：评估方面（测量内容）、模型能力（完成任务所需能力）、任务要求（执行任务所需协议），对现有评估和基准进行分类映射。

Result: 分类法成功映射了广泛的现有评估方法，揭示了系统性差距（如韵律、交互、推理方面的覆盖不足），为未来基准设计指明方向。

Conclusion: 该工作为语音模型评估的选择、解释和扩展提供了概念基础和实用指南，有助于系统化评估实践。

Abstract: Speech foundation models have recently achieved remarkable capabilities
across a wide range of tasks. However, their evaluation remains disjointed
across tasks and model types. Different models excel at distinct aspects of
speech processing and thus require different evaluation protocols. This paper
proposes a unified taxonomy that addresses the question: Which evaluation is
appropriate for which model? The taxonomy defines three orthogonal axes: the
\textbf{evaluation aspect} being measured, the model capabilities required to
attempt the task, and the task or protocol requirements needed to perform it.
We classify a broad set of existing evaluations and benchmarks along these
axes, spanning areas such as representation learning, speech generation, and
interactive dialogue. By mapping each evaluation to the capabilities a model
exposes (e.g., speech generation, real-time processing) and to its
methodological demands (e.g., fine-tuning data, human judgment), the taxonomy
provides a principled framework for aligning models with suitable evaluation
methods. It also reveals systematic gaps, such as limited coverage of prosody,
interaction, or reasoning, that highlight priorities for future benchmark
design. Overall, this work offers a conceptual foundation and practical guide
for selecting, interpreting, and extending evaluations of speech models.

</details>


### [58] [Conditions for Catastrophic Forgetting in Multilingual Translation](https://arxiv.org/abs/2510.19546)
*Danni Liu,Jan Niehues*

Main category: cs.CL

TL;DR: 本文系统研究了多语言模型微调中的灾难性遗忘问题，发现模型与数据规模的相对比例是主要决定因素，指令跟随能力比架构更重要，参数高效微调相比全参数微调没有明显优势，跨语言对齐可以缓解遗忘并促进正向迁移。


<details>
  <summary>Details</summary>
Motivation: 多语言基础模型在特定语言上微调时经常出现灾难性遗忘，降低在未见语言上的性能。现有文献对此现象的结果碎片化，缺乏系统性研究。

Method: 使用机器翻译作为测试平台，在不同模型架构、数据规模和微调方法下进行受控实验，系统分析触发灾难性遗忘的条件。

Result: 发现模型与数据规模的相对比例是遗忘的主要决定因素；指令跟随能力对保留多语言知识比架构更重要；参数高效微调相比全参数微调没有明显优势；跨语言对齐可以缓解遗忘并促进正向迁移。

Conclusion: 多语言微调中的灾难性遗忘主要由模型-数据规模比例决定，指令跟随能力是关键因素，跨语言对齐是有效的缓解策略。

Abstract: Fine-tuning multilingual foundation models on specific languages often
induces catastrophic forgetting, degrading performance on languages unseen in
fine-tuning. While this phenomenon is widely-documented, the literature
presents fragmented results about when forgetting occurs. To address this
ambiguity, we conduct a systematic empirical study using machine translation as
a testbed to identify the conditions that trigger catastrophic forgetting in
multilingual fine-tuning. Through controlled experiments across different model
architectures, data scales, and fine-tuning approaches, we reveal that the
relative scale between model and data size is a primary determinant of
forgetting. Moreover, we demonstrate that a model's instruction-following
ability is more critical for retaining multilingual knowledge than its
architecture. Contrary to assumptions, parameter-efficient fine-tuning offers
no clear advantage over full fine-tuning in mitigating forgetting. Lastly, we
show that cross-lingual alignment can mitigate forgetting while also
facilitating positive transfer to unseen target languages.

</details>


### [59] [Detecting Latin in Historical Books with Large Language Models: A Multimodal Benchmark](https://arxiv.org/abs/2510.19585)
*Yu Wu,Ke Shu,Jonas Fischer,Lidia Pivovarova,David Rosson,Eetu Mäkelä,Mikko Tolonen*

Main category: cs.CL

TL;DR: 本文提出从混合语言历史文档中提取拉丁片段的新任务，评估了大型基础模型在724页多模态数据集上的性能，证明现代模型可实现可靠的拉丁检测。


<details>
  <summary>Details</summary>
Motivation: 解决从布局多样的混合语言历史文档中提取拉丁片段这一新任务，填补该领域的研究空白。

Method: 使用724页带标注的多模态数据集，对大型基础模型进行基准测试和性能评估。

Result: 结果表明当代模型能够实现可靠的拉丁检测，为模型在该任务上的能力提供了首个全面分析。

Conclusion: 研究证明了大型基础模型在混合语言历史文档中拉丁片段提取任务上的可行性，并明确了其能力边界。

Abstract: This paper presents a novel task of extracting Latin fragments from
mixed-language historical documents with varied layouts. We benchmark and
evaluate the performance of large foundation models against a multimodal
dataset of 724 annotated pages. The results demonstrate that reliable Latin
detection with contemporary models is achievable. Our study provides the first
comprehensive analysis of these models' capabilities and limits for this task.

</details>


### [60] [PBBQ: A Persian Bias Benchmark Dataset Curated with Human-AI Collaboration for Large Language Models](https://arxiv.org/abs/2510.19616)
*Farhan Farsi,Shayan Bali,Fatemeh Valeh,Parsa Ghofrani,Alireza Pakniat,Kian Kashfipour,Amir H. Payberah*

Main category: cs.CL

TL;DR: 提出了PBBQ基准数据集，用于评估波斯语大语言模型中的社会偏见，包含16个文化类别和超过37,000个问题。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的广泛应用，确保其与社会规范保持一致变得至关重要。目前缺乏针对波斯文化背景的社会偏见检测资源。

Method: 通过与社会科学专家合作，让250名来自不同人口统计背景的个体填写问卷，开发了包含16个文化类别的PBBQ基准数据集。

Result: 评估发现当前LLMs在波斯文化中表现出显著的社会偏见，且模型输出往往复制人类的偏见模式。

Conclusion: PBBQ数据集为波斯语语言模型的偏见评估和缓解提供了基础，揭示了学习表征与文化刻板印象之间的复杂关系。

Abstract: With the increasing adoption of large language models (LLMs), ensuring their
alignment with social norms has become a critical concern. While prior research
has examined bias detection in various languages, there remains a significant
gap in resources addressing social biases within Persian cultural contexts. In
this work, we introduce PBBQ, a comprehensive benchmark dataset designed to
evaluate social biases in Persian LLMs. Our benchmark, which encompasses 16
cultural categories, was developed through questionnaires completed by 250
diverse individuals across multiple demographics, in close collaboration with
social science experts to ensure its validity. The resulting PBBQ dataset
contains over 37,000 carefully curated questions, providing a foundation for
the evaluation and mitigation of bias in Persian language models. We benchmark
several open-source LLMs, a closed-source model, and Persian-specific
fine-tuned models on PBBQ. Our findings reveal that current LLMs exhibit
significant social biases across Persian culture. Additionally, by comparing
model outputs to human responses, we observe that LLMs often replicate human
bias patterns, highlighting the complex interplay between learned
representations and cultural stereotypes.Upon acceptance of the paper, our PBBQ
dataset will be publicly available for use in future work. Content warning:
This paper contains unsafe content.

</details>


### [61] [CrossNews-UA: A Cross-lingual News Semantic Similarity Benchmark for Ukrainian, Polish, Russian, and English](https://arxiv.org/abs/2510.19628)
*Daryna Dementieva,Evgeniya Sukhodolskaya,Alexander Fraser*

Main category: cs.CL

TL;DR: 提出了一种可扩展、可解释的众包流程，用于跨语言新闻相似性评估，并构建了以乌克兰语为中心的跨语言新闻数据集CrossNews-UA，测试了多种模型在跨语言新闻分析中的表现。


<details>
  <summary>Details</summary>
Motivation: 在社交媒体和虚假信息快速传播的时代，跨语言假新闻检测面临挑战。现有跨语言新闻分析数据集由记者和专家手动整理，限制了可扩展性和对新语言的适应性。

Method: 开发了可扩展、可解释的众包流程来收集跨语言新闻相似性数据，构建了以乌克兰语为中心、包含波兰语、俄语和英语的CrossNews-UA数据集，每个新闻对都基于4W标准（Who、What、Where、When）进行语义相似性标注。

Result: 测试了从传统词袋模型、基于Transformer的架构到大型语言模型（LLMs）的一系列模型，结果突显了多语言新闻分析的挑战，并提供了模型性能的见解。

Conclusion: 这项工作填补了跨语言新闻分析数据集的空白，为多语言新闻相似性评估提供了可扩展的解决方案，并揭示了当前模型在多语言新闻分析中的局限性。

Abstract: In the era of social networks and rapid misinformation spread, news analysis
remains a critical task. Detecting fake news across multiple languages,
particularly beyond English, poses significant challenges. Cross-lingual news
comparison offers a promising approach to verify information by leveraging
external sources in different languages (Chen and Shu, 2024). However, existing
datasets for cross-lingual news analysis (Chen et al., 2022a) were manually
curated by journalists and experts, limiting their scalability and adaptability
to new languages. In this work, we address this gap by introducing a scalable,
explainable crowdsourcing pipeline for cross-lingual news similarity
assessment. Using this pipeline, we collected a novel dataset CrossNews-UA of
news pairs in Ukrainian as a central language with linguistically and
contextually relevant languages-Polish, Russian, and English. Each news pair is
annotated for semantic similarity with detailed justifications based on the 4W
criteria (Who, What, Where, When). We further tested a range of models, from
traditional bag-of-words, Transformer-based architectures to large language
models (LLMs). Our results highlight the challenges in multilingual news
analysis and offer insights into models performance.

</details>


### [62] [Style Attack Disguise: When Fonts Become a Camouflage for Adversarial Intent](https://arxiv.org/abs/2510.19641)
*Yangshijie Zhang,Xinda Wang,Jialin Liu,Wenqiang Wang,Zhicong Ma,Xingxing Jia*

Main category: cs.CL

TL;DR: 提出Style Attack Disguise (SAD)攻击方法，利用风格字体和字体类表情符号在人类可读但模型处理为不同token的特性，对NLP模型进行攻击。


<details>
  <summary>Details</summary>
Motivation: 社交媒体中用户使用风格字体和字体类表情符号表达个性，这些视觉上吸引人的文本对人类可读，但对NLP模型会产生干扰，存在人机感知差距的安全漏洞。

Method: 设计SAD攻击方法，包括轻量版（查询效率高）和强力版（攻击性能优），利用风格字体字符替换原始文本中的字符。

Result: 在情感分类和机器翻译任务上，对传统模型、LLM和商业服务进行实验，SAD表现出强大的攻击性能，并在多模态任务（文本到图像、文本到语音生成）中展示潜在威胁。

Conclusion: 风格字体在NLP系统中存在安全风险，SAD攻击方法有效利用了人机感知差距，对多种NLP和多模态任务构成威胁。

Abstract: With social media growth, users employ stylistic fonts and font-like emoji to
express individuality, creating visually appealing text that remains
human-readable. However, these fonts introduce hidden vulnerabilities in NLP
models: while humans easily read stylistic text, models process these
characters as distinct tokens, causing interference. We identify this
human-model perception gap and propose a style-based attack, Style Attack
Disguise (SAD). We design two sizes: light for query efficiency and strong for
superior attack performance. Experiments on sentiment classification and
machine translation across traditional models, LLMs, and commercial services
demonstrate SAD's strong attack performance. We also show SAD's potential
threats to multimodal tasks including text-to-image and text-to-speech
generation.

</details>


### [63] [LLavaCode: Compressed Code Representations for Retrieval-Augmented Code Generation](https://arxiv.org/abs/2510.19644)
*Daria Cherniuk,Nikita Sukhorukov,Nikita Sushko,Daniil Gusak,Danil Sivtsov,Elena Tutubalina,Evgeny Frolov*

Main category: cs.CL

TL;DR: LlavaCode框架通过将代码压缩为紧凑的语义表示，显著减少检索增强生成中的上下文长度，在保持代码生成质量的同时降低推理延迟。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成在代码补全中很有效，但引入上下文会显著增加序列长度，导致推理变慢，这在IDE等交互式环境中是严重限制。

Method: 使用小型投影器模块将代码压缩为可由代码LLM解释的紧凑语义表示，将检索上下文减少到仅几个压缩的单标记向量。

Result: 压缩上下文使行补全任务的首次标记时间减少20-38%，同时通过可忽略的延迟增加显著提高了代码模型的EM和ES指标。

Conclusion: LlavaCode框架在保持生成质量的同时有效解决了检索增强生成中的延迟问题，为交互式代码补全提供了实用解决方案。

Abstract: Retrieval-augmented generation has emerged as one of the most effective
approaches for code completion, particularly when context from a surrounding
repository is essential. However, incorporating context significantly extends
sequence length, leading to slower inference - a critical limitation for
interactive settings such as IDEs. In this work, we introduce LlavaCode, a
framework that compresses code into compact, semantically rich representations
interpretable by code LLM, enhancing generation quality while reducing the
retrieved context to only a few compressed single-token vectors. Using a small
projector module we can significantly increase the EM and ES metrics of coding
model with negligible latency increase. Our experiments demonstrate that
compressed context enables 20-38% reduction in Time-to-First-Token (TTFT) on
line completion tasks compared to full-RAG pipelines.

</details>


### [64] [Unraveling Emotions with Pre-Trained Models](https://arxiv.org/abs/2510.19668)
*Alejandro Pajón-Sanmartín,Francisco De Arriba-Pérez,Silvia García-Méndez,Fátima Leal,Benedita Malheiro,Juan Carlos Burguillo-Rial*

Main category: cs.CL

TL;DR: 本文比较了微调预训练模型与使用提示工程的通用大语言模型在情感检测中的效果，发现微调模型在情感识别中达到70%以上指标，而LLMs需要结构化提示工程和情感分组来提升性能。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer模型在情感识别方面取得进展，但在开放文本中自动情感分析仍面临上下文歧义、语言变异性和复杂情感表达解释等挑战，使得通用模型直接应用困难。

Method: 比较微调预训练模型与使用提示工程的通用LLMs在三种场景下的表现：(i)微调模型与简单提示LLMs的性能对比；(ii)不同情感提示设计对LLMs的效果；(iii)情感分组技术对这些模型的影响。

Result: 实验测试显示，微调预训练模型在情感识别中达到70%以上指标，而LLMs需要结构化提示工程和情感分组才能提升性能。

Conclusion: 微调预训练模型在情感识别中表现优异，而LLMs需要通过结构化提示工程和情感分组技术来增强其性能，这些进展有助于改进情感分析、人机交互和跨领域用户行为理解。

Abstract: Transformer models have significantly advanced the field of emotion
recognition. However, there are still open challenges when exploring open-ended
queries for Large Language Models (LLMs). Although current models offer good
results, automatic emotion analysis in open texts presents significant
challenges, such as contextual ambiguity, linguistic variability, and
difficulty interpreting complex emotional expressions. These limitations make
the direct application of generalist models difficult. Accordingly, this work
compares the effectiveness of fine-tuning and prompt engineering in emotion
detection in three distinct scenarios: (i) performance of fine-tuned
pre-trained models and general-purpose LLMs using simple prompts; (ii)
effectiveness of different emotion prompt designs with LLMs; and (iii) impact
of emotion grouping techniques on these models. Experimental tests attain
metrics above 70% with a fine-tuned pre-trained model for emotion recognition.
Moreover, the findings highlight that LLMs require structured prompt
engineering and emotion grouping to enhance their performance. These
advancements improve sentiment analysis, human-computer interaction, and
understanding of user behavior across various domains.

</details>


### [65] [DiffAdapt: Difficulty-Adaptive Reasoning for Token-Efficient LLM Inference](https://arxiv.org/abs/2510.19669)
*Xiang Liu,Xuming Hu,Xiaowen Chu,Eunsol Choi*

Main category: cs.CL

TL;DR: DiffAdapt框架通过分析推理轨迹的熵模式来识别LLM的过度思考现象，并根据问题难度自适应选择推理策略，在不微调基础模型的情况下显著减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在推理时经常生成冗长的思考轨迹，但其效用不明确，存在过度思考现象，特别是在简单问题上。

Method: 通过分析推理轨迹中token概率的熵模式，发现U形熵分布；提出DiffAdapt框架，基于问题难度和推理轨迹熵为每个问题选择Easy/Normal/Hard推理策略，每个策略包含固定的提示词、温度和最大token长度。

Result: 在5个模型和8个基准测试上的评估显示，该方法在保持或提高准确率的同时，将token使用量减少了高达22.4%。

Conclusion: DiffAdapt为计算高效的推理提供了一条实用路径，通过轻量级适配实现了显著的效率提升。

Abstract: Recent reasoning Large Language Models (LLMs) demonstrate remarkable
problem-solving abilities but often generate long thinking traces whose utility
is unclear. Our work aims to improve their efficiency, enabling them to reach
high performance without overthinking. First, we analyze the entropy of token
probabilities in reasoning traces. Across three models, we observe a consistent
U-shaped entropy pattern: high entropy on easy problems despite high accuracy,
low entropy on problems with medium difficulty, and high entropy on hard
problems reflecting uncertainty. Specifically, we notice 22--25\% entropy
reduction from easy to medium difficulty regions, suggesting an {overthinking}
phenomenon on easy instances. Building on these insights, we introduce
\textbf{DiffAdapt}, a lightweight framework that selects Easy/Normal/Hard
inference strategies per question based on their difficulty and reasoning trace
entropy. Each inference strategy consists of a fixed prompt, temperature and
maximum token length. In contrast to existing efficiency optimization methods,
our approach does not fine-tune base LLM but a small probe that classifies
LLM's final hidden state, allowing inexpensive adaptation. We comprehensively
evaluate our method on five models and eight benchmarks. Our method achieves
comparable or improved accuracy while reducing token usage by up to 22.4\%,
establishing a practical path toward compute-efficient reasoning.

</details>


### [66] [CoSense-LLM: Semantics at the Edge with Cost- and Uncertainty-Aware Cloud-Edge Cooperation](https://arxiv.org/abs/2510.19670)
*Hasan Akgul,Mari Eplik,Javier Rojas,Aina Binti Abdullah,Pieter van der Merwe*

Main category: cs.CL

TL;DR: CoSense-LLM是一个边缘优先框架，将多模态传感器流转化为紧凑、可验证的语义令牌，在延迟、能耗、带宽和隐私约束下与大型语言模型协调工作。


<details>
  <summary>Details</summary>
Motivation: 解决在干扰环境中部署大型模型时面临的语义理解、隐私保护和延迟预测等挑战，将传感器数据与语言模型有效结合。

Method: 包含四个组件：SenseFusion轻量编码器、Edge-RAG本地检索层、PromptRouter成本感知策略和Secure Execution安全执行机制，支持多种优化技术。

Result: 在家庭、办公室和诊所部署中实现亚秒级延迟，减少带宽成本，提高事实一致性，降低能耗，并保护隐私。

Conclusion: 支持边缘优先设计，将语义、隐私和可预测延迟作为大型模型在干扰环境中部署的同等重要目标。

Abstract: We present CoSense-LLM, an edge-first framework that turns continuous
multimodal sensor streams (for example Wi-Fi CSI, IMU, audio, RFID, and
lightweight vision) into compact, verifiable semantic tokens and coordinates
with large language models under explicit latency, energy, bandwidth, and
privacy constraints. CoSense-LLM has four parts: (i) SenseFusion, a lightweight
encoder that aligns sensor embeddings with language and compresses them into
short discrete code sequences; (ii) Edge-RAG, a local hybrid retrieval layer
that grounds generation in site specific policies and notes; (iii)
PromptRouter, a cost and uncertainty aware policy that selects edge only
generation, edge plus retrieval, or compact cloud escalation; and (iv) Secure
Execution, an auditable redaction path that enforces data minimization so raw
waveforms never leave the device. The system works with modern serving
optimizations, including paged or streaming KV caches, FlashAttention style
kernels, speculative decoding, and quantized LoRA adapters, and supports on
device personalization and federated updates under non IID drift. Across home,
office, and clinic deployments, CoSense-LLM delivers grounded explanations
while meeting tight service level objectives: it sustains sub second (p95) end
to end latency on edge dominant paths, reduces inter tier token and bandwidth
costs by preferring local retrieval grounded responses, and preserves privacy
by transmitting only discrete codes and redacted metadata. Ablations show that
Edge-RAG improves factual consistency and reduces contradictions, calibrated
uncertainty enables selective abstention and controlled escalations, and KV
plus decoding accelerators lower energy per decision. The results support an
edge first design that treats semantics, privacy, and predictable latency as co
equal goals for large model deployments in interference prone environments.

</details>


### [67] [Are Large Language Models Sensitive to the Motives Behind Communication?](https://arxiv.org/abs/2510.19687)
*Addison J. Wu,Ryan Liu,Kerem Oktar,Theodore R. Sumers,Thomas L. Griffiths*

Main category: cs.CL

TL;DR: 该论文研究了大型语言模型是否具备动机警觉能力，即能否像人类一样考虑信息源动机来评估内容可信度。研究发现LLMs在受控实验中表现良好，但在真实广告场景中表现不佳，通过简单的干预可显著提升其表现。


<details>
  <summary>Details</summary>
Motivation: 人类沟通具有动机性，而LLMs在现实世界中需要能够批判性评估内容，考虑信息源的动机和意图来判断可信度。

Method: 使用认知科学中的受控实验验证LLMs行为是否与理性学习模型一致，并扩展到赞助在线广告等更自然的信息环境进行评估。

Result: LLMs在受控实验中能成功折扣有偏见来源的信息，但在真实广告场景中表现不佳；简单的干预措施能显著提升其与理性模型的对应性。

Conclusion: LLMs具备对他人动机的基本敏感性，但要推广到新的现实世界场景需要进一步改进这些模型。

Abstract: Human communication is motivated: people speak, write, and create content
with a particular communicative intent in mind. As a result, information that
large language models (LLMs) and AI agents process is inherently framed by
humans' intentions and incentives. People are adept at navigating such nuanced
information: we routinely identify benevolent or self-serving motives in order
to decide what statements to trust. For LLMs to be effective in the real world,
they too must critically evaluate content by factoring in the motivations of
the source -- for instance, weighing the credibility of claims made in a sales
pitch. In this paper, we undertake a comprehensive study of whether LLMs have
this capacity for motivational vigilance. We first employ controlled
experiments from cognitive science to verify that LLMs' behavior is consistent
with rational models of learning from motivated testimony, and find they
successfully discount information from biased sources in a human-like manner.
We then extend our evaluation to sponsored online adverts, a more naturalistic
reflection of LLM agents' information ecosystems. In these settings, we find
that LLMs' inferences do not track the rational models' predictions nearly as
closely -- partly due to additional information that distracts them from
vigilance-relevant considerations. However, a simple steering intervention that
boosts the salience of intentions and incentives substantially increases the
correspondence between LLMs and the rational model. These results suggest that
LLMs possess a basic sensitivity to the motivations of others, but generalizing
to novel real-world settings will require further improvements to these models.

</details>


### [68] [Do Prompts Reshape Representations? An Empirical Study of Prompting Effects on Embeddings](https://arxiv.org/abs/2510.19694)
*Cesar Gonzalez-Gutierrez,Dirk Hovy*

Main category: cs.CL

TL;DR: 本文通过实验研究发现，提示词虽然影响语言模型表示质量，但这种变化与提示词对目标任务的相关性并不一致相关，挑战了"更相关提示词必然产生更好表示"的假设。


<details>
  <summary>Details</summary>
Motivation: 研究提示词与内部表示质量的关系，以理解预训练嵌入如何支持上下文任务解决，揭示语言模型在零样本设置下执行多样化任务的机制。

Method: 进行一系列探测实验，分析零样本分类中不同提示词模板组合的嵌入表示，研究提示词对表示质量的影响。

Result: 发现提示词确实影响表示质量，但这些变化与提示词对目标任务的相关性没有一致相关性，更相关的提示词不一定产生更好的表示。

Conclusion: 研究结果挑战了提示词相关性与表示质量正相关的假设，表明需要更深入地理解提示词影响语言模型内部表示的机制。

Abstract: Prompting is a common approach for leveraging LMs in zero-shot settings.
However, the underlying mechanisms that enable LMs to perform diverse tasks
without task-specific supervision remain poorly understood. Studying the
relationship between prompting and the quality of internal representations can
shed light on how pre-trained embeddings may support in-context task solving.
In this empirical study, we conduct a series of probing experiments on prompt
embeddings, analyzing various combinations of prompt templates for zero-shot
classification. Our findings show that while prompting affects the quality of
representations, these changes do not consistently correlate with the relevance
of the prompts to the target task. This result challenges the assumption that
more relevant prompts necessarily lead to better representations. We further
analyze potential factors that may contribute to this unexpected behavior.

</details>


### [69] [From Answers to Guidance: A Proactive Dialogue System for Legal Documents](https://arxiv.org/abs/2510.19723)
*Ashish Chouhan,Michael Gertz*

Main category: cs.CL

TL;DR: 提出了EUDial对话数据集和LexGuide框架，用于改善欧盟法律信息的可访问性，通过主动多轮对话帮助非专业人士理解复杂法律文本。


<details>
  <summary>Details</summary>
Motivation: 欧盟虽然提供开放的法律信息访问，但这些资源对普通公民来说仍然难以理解和探索，存在法律信息可用性与公民理解之间的差距。

Method: 构建了包含880个对话轮次的EUDial数据集，并提出LexGuide框架，利用检索增强生成和分层主题组织来结构化对话进程。

Result: 主动结构化导航缩小了法律信息可用性与公民理解之间的差距，EUDial和LexGuide成为推进主动法律对话系统的实用资源。

Conclusion: 该研究为改善法律信息可访问性提供了有效解决方案，通过主动对话系统帮助公民更好地理解和应用复杂法律文本。

Abstract: The accessibility of legal information remains a constant challenge,
particularly for laypersons seeking to understand and apply complex
institutional texts. While the European Union provides open access to
legislation, parliamentary responses, and regulatory documents, these resources
can be challenging for laypeople to explore. In this paper, we introduce
EUDial, a proactive multi-turn dialogue dataset constructed from 204 blogs
curated by the Citizens' Enquiries Unit (AskEP) of the European Parliamentary
Research Service. EUDial contains 880 dialogue turns (averaging 4.3 turns per
dialogue), where each dialogue includes initial questions, structured answers,
and follow-up questions. Beyond dataset construction, we propose the LexGuide
framework that leverages retrieval-augmented generation with hierarchical topic
organization to structure dialogue progression, ensuring both comprehensive
coverage of legal aspects and coherence across conversational turns. The
results demonstrate that proactive, structured navigation closes the gap
between the availability of legal information and citizen comprehension,
establishing EUDial and LexGuide as practical resources for advancing proactive
legal dialogue systems.

</details>


### [70] [Zhyper: Factorized Hypernetworks for Conditioned LLM Fine-Tuning](https://arxiv.org/abs/2510.19733)
*M. H. I. Abdalla,Zhipin Wang,Christian Frey,Steffen Eger,Josif Grabocka*

Main category: cs.CL

TL;DR: Zhyper是一种参数高效的超网络框架，通过文本描述生成上下文感知的LoRA适配器，用于大语言模型的文化对齐，相比现有方法减少26倍参数。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法确保LLM按照特定文化规范生成内容，而直接微调LoRA权重会引入大量参数。

Method: 提出参数高效的因子化超网络框架，从文本描述生成上下文感知的LoRA适配器。

Result: 在多个基准测试中达到竞争性性能，参数比最先进基线减少26倍，在文化对齐任务中展现出更好的泛化能力和细粒度上下文价值捕获。

Conclusion: Zhyper框架在保持性能的同时显著减少参数数量，在文化对齐任务中表现优异。

Abstract: Large Language Model (LLM) conditioning refers to instructing an LLM to
generate content in accordance with the norms and values of a specific culture,
beliefs of a particular political orientation, or any desired text-specified
semantic conditioning. Unfortunately, prompt engineering does not ensure that
LLMs behave in accordance with a desired conditioning due to the inductive bias
of the pre-training and alignment datasets. Prior works have focused on
fine-tuning LLMs by directly conditioning the LoRA weights; however, such
methods introduce a large number of parameters. As a remedy, we propose Zhyper,
a parameter-efficient factorized hypernetwork framework that generates
context-aware LoRA adapters from textual descriptions. Experiments on multiple
benchmarks show that Zhyper achieves competitive performance with up to 26x
fewer parameters than the state-of-the-art baselines. Furthermore, we extend
Zhyper to cultural alignment, demonstrating improved generalization to
out-of-domain settings and a better capturing of fine-grained contextual
values.

</details>


### [71] [SmartSwitch: Advancing LLM Reasoning by Overcoming Underthinking via Promoting Deeper Thought Exploration](https://arxiv.org/abs/2510.19767)
*Xichen Zhang,Sitong Wu,Haoru Tan,Shaozuo Yu,Yinghao Zhu,Ziyi He,Jiaya Jia*

Main category: cs.CL

TL;DR: 提出SmartSwitch推理框架解决LLM在长链推理中的'欠思考'问题，通过监控推理过程检测过早放弃的高潜力思路，引导模型进行更深入探索


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在复杂推理任务中存在的'欠思考'问题，即模型频繁切换思路而不充分探索，限制了性能和token效率

Method: SmartSwitch推理框架包含感知模块和干预模块。感知模块识别思路切换点并用现成过程奖励模型评估前一个思路的潜力；干预模块在发现高潜力思路被过早放弃时中断推理，回溯到切换点前插入'深化提示'鼓励进一步探索

Result: 在具有挑战性的数学推理基准测试上的广泛实验表明，该方法显著提升了不同规模大语言模型的性能

Conclusion: SmartSwitch是一个简单有效的即插即用推理策略，能够有效解决LLM在长链推理中的欠思考问题，提升推理深度和性能

Abstract: The long chain-of-thought (LongCoT) capability is central to the recent
breakthroughs achieved by large language models in complex reasoning tasks.
However, the accompanying issue of ''underthinking'', where models exhibit
shallow reasoning by frequently switching thoughts without sufficient
exploration, limits both performance and token efficiency. To address this
problem, we propose a simple yet effective reasoning strategy: the SmartSwitch
inference framework. This framework can be easily integrated into any large
language model as a plug-and-play solution, continuously monitoring the model's
reasoning process to detect underthinking and guide it toward deeper
exploration of promising but overlooked thoughts. Specifically, the perception
module identifies points where thoughts switch and evaluates the potential of
the preceding thought using an off-the-shelf process reward model (PRM). If a
high-potential thought is found to be prematurely abandoned, the intervention
module interrupts the ongoing inference, backtracks to the point before the
switch, and inserts a "deepening prompt" to encourage further exploration along
that promising path. Extensive experiments on challenging mathematical
reasoning benchmarks demonstrate that our method significantly enhances the
performance of various large language models of different sizes.

</details>


### [72] [AdaSPEC: Selective Knowledge Distillation for Efficient Speculative Decoders](https://arxiv.org/abs/2510.19779)
*Yuezhou Hu,Jiaxin Guo,Xinyu Feng,Tuo Zhao*

Main category: cs.CL

TL;DR: 提出了AdaSPEC方法，通过选择性令牌过滤改进知识蒸馏过程，提高推测解码中的令牌接受率


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏方法最小化所有令牌的KL散度，与推测解码最大化令牌接受率的目标不一致，导致草稿模型性能不佳

Method: 使用参考模型识别并过滤难以拟合的令牌，在更简单的令牌上蒸馏草稿模型，使其与目标模型更好对齐

Result: 在算术推理、指令跟随、编码和摘要等任务上，AdaSPEC在31M/1.4B和350M/2.7B参数配置下，相比最先进的DistillSpec方法实现了更高的接受率（最高提升15%）

Conclusion: AdaSPEC通过选择性令牌过滤改进了知识蒸馏过程，在不影响生成质量的前提下提高了推测解码的令牌接受率

Abstract: Speculative Decoding (SD) accelerates large language model inference by
employing a small draft model to generate predictions, which are then verified
by a larger target model. The effectiveness of SD hinges on the alignment
between these models, which is typically enhanced by Knowledge Distillation
(KD). However, conventional KD methods aim to minimize the KL divergence
between the draft and target models across all tokens, a goal that is
misaligned with the true objective of SD, which is to maximize token acceptance
rate. Therefore, draft models often struggle to fully assimilate the target
model's knowledge due to capacity constraints, leading to suboptimal
performance. To address this challenge, we propose AdaSPEC, a novel method that
incorporates selective token filtering into the KD process. AdaSPEC utilizes a
reference model to identify and filter out difficult-to-fit tokens, enabling
the distillation of a draft model that better aligns with the target model on
simpler tokens. This approach improves the overall token acceptance rate
without compromising generation quality. We evaluate AdaSPEC across diverse
tasks, including arithmetic reasoning, instruction-following, coding, and
summarization, using model configurations of 31M/1.4B and 350M/2.7B parameters.
Our results demonstrate that AdaSPEC consistently outperforms the
state-of-the-art DistillSpec method, achieving higher acceptance rates across
all tasks (up to 15\%). The code is publicly available at
https://github.com/yuezhouhu/adaspec.

</details>


### [73] [Adapting Multilingual Models to Code-Mixed Tasks via Model Merging](https://arxiv.org/abs/2510.19782)
*Prashant Kodali,Vaishnavi Shivkumar,Swarang Joshi,Monojit Choudhary,Ponnurangam Kumaraguru,Manish Shrivastava*

Main category: cs.CL

TL;DR: 该论文提出通过模型合并方法提升代码混合NLP任务性能，相比传统微调方法在英语-印地语和英语-西班牙语情感分类和仇恨言论检测任务上获得2-5个F1分数提升，且跨语言迁移能力更强。


<details>
  <summary>Details</summary>
Motivation: 研究模型合并作为代码混合NLP传统适应策略的实用替代方案，旨在更有效地利用未标注代码混合数据，提升模型在低资源语言对上的性能。

Method: 从多语言基础模型出发：(1)在未标注代码混合文本上进行持续预训练获得适应检查点；(2)将检查点与基础模型合并；(3)在下游任务数据上进行微调。使用XLM-R和Llama-3.2-1B模型在英语-印地语和英语-西班牙语上进行评估。

Result: 合并模型始终优于完全微调和CPT->FT方法，在F1分数上比完全微调高出2-5分，比CPT->FT高出约1-2分。在跨语言对迁移测试中，合并检查点比单语英语基线迁移效果更好（TV/TIES变体达到0.65-0.68 F1 vs 完全微调的0.61-0.63）。

Conclusion: 模型合并比单独使用持续预训练更有效地利用未标注数据，代码混合知识是低资源语言对更可靠的基板。论文提供了针对不同数据场景的适应方案，并讨论了在大模型和更广泛任务上的扩展考虑。

Abstract: We study model merging as a practical alternative to conventional adaptation
strategies for code-mixed NLP. Starting from a multilingual base model, we: (i)
perform continued pre-training (CPT) on unlabeled code-mixed text to obtain an
adapted checkpoint, (ii) merge checkpoint with the base model, and (iii)
fine-tune (FT) on the downstream task data. We evaluate our approach for
sentence classification (sentiment and hate speech) task in English-Hindi
(En-Hi) and English-Spanish (En-Es) using XLM-R and Llama-3.2-1B models. Our
results show that merged models consistently outperform full fine-tuning and
CPT->FT. We observe gains of 2--5 points in F1 over full fine-tuning and ~1-2
points over CPT->FT, indicating that unlabeled data is leveraged more
effectively via merging than via CPT alone. Zero-/few-shot prompting with
larger LLMs (e.g., Llama-3.3-70B) lags behind fine-tuned and merged
checkpoints, underscoring limits of in-context learning for code-mixed inputs.
We further test cross-pair transfer by training on En-Hi and evaluating on
En-Ta and En-Ml: merged checkpoints transfer more strongly than
monolingual-English baselines (e.g., TV/TIES variants reaching 0.65-0.68 F1 vs
0.61-0.63 for full fine-tuning), suggesting that code-mixed knowledge is a more
reliable substrate for low-resource pairs. We conclude with adaptation recipes
matched to common data regimes (labeled only; labeled+unlabeled; transfer-only)
and discuss limitations and scaling considerations for broader tasks and larger
models.

</details>


### [74] [ToolDreamer: Instilling LLM Reasoning Into Tool Retrievers](https://arxiv.org/abs/2510.19791)
*Saptarshi Sengupta,Zhengyu Zhou,Jun Araki,Xingbo Wang,Bingqing Wang,Suhang Wang,Zhe Feng*

Main category: cs.CL

TL;DR: ToolDreamer框架通过使用LLM生成假设性工具描述来改进工具检索，使检索器能更好地理解用户查询与工具之间的语义对齐，从而提升检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有工具检索模型基于用户查询与工具描述的相似度进行排名，但用户请求往往与工具描述语言不匹配，导致检索效果不佳。需要一种方法使检索器能更好地理解查询意图与工具功能之间的对应关系。

Method: 提出ToolDreamer框架，使用LLM生成假设性（合成）工具描述，这些描述是LLM认为可能对查询有用的工具描述。通过这种方式在工具描述的语言空间内实现查询与工具的更自然对齐。

Result: 在ToolRet数据集上应用ToolDreamer，结果显示该方法提升了稀疏和稠密检索器的性能，无论是否进行训练都有效，证明了其灵活性。

Conclusion: ToolDreamer框架通过将部分推理负担转移到检索器上，使LLM能够有效处理大量工具而不会超出上下文窗口限制，实现了更好的工具检索效果。

Abstract: Tool calling has become increasingly popular for Large Language Models
(LLMs). However, for large tool sets, the resulting tokens would exceed the
LLM's context window limit, making it impossible to include every tool. Hence,
an external retriever is used to provide LLMs with the most relevant tools for
a query. Existing retrieval models rank tools based on the similarity between a
user query and a tool description (TD). This leads to suboptimal retrieval as
user requests are often poorly aligned with the language of TD. To remedy the
issue, we propose ToolDreamer, a framework to condition retriever models to
fetch tools based on hypothetical (synthetic) TD generated using an LLM, i.e.,
description of tools that the LLM feels will be potentially useful for the
query. The framework enables a more natural alignment between queries and tools
within the language space of TD's. We apply ToolDreamer on the ToolRet dataset
and show that our method improves the performance of sparse and dense
retrievers with and without training, thus showcasing its flexibility. Through
our proposed framework, our aim is to offload a portion of the reasoning burden
to the retriever so that the LLM may effectively handle a large collection of
tools without inundating its context window.

</details>


### [75] [The Art of Asking: Multilingual Prompt Optimization for Synthetic Data](https://arxiv.org/abs/2510.19806)
*David Mora,Viraat Aryabumi,Wei-Yin Ko,Sara Hooker,Julia Kreutzer,Marzieh Fadaee*

Main category: cs.CL

TL;DR: 提出了一种轻量级的提示空间优化框架，通过自然性、文化适应和难度增强三个维度系统性地改进多语言提示，显著提升了多语言LLM的性能。


<details>
  <summary>Details</summary>
Motivation: 当前多语言合成数据主要依赖基于翻译的提示，这种方法继承了英语中心的框架和风格，忽视了文化维度，限制了模型的泛化能力。

Method: 开发了一个轻量级框架，对翻译后的提示进行系统化转换，包括自然性改进、文化适应和难度增强三个方面的优化。

Result: 在12种语言上的实验显示，该方法相比仅翻译的基线在Global-MMLU准确率上提升4.7%，Flores XCometXL提升2.4%，mArenaHard偏好胜率提升35.3%。

Conclusion: 提示空间优化是一个简单而强大的范式，能够构建更鲁棒、文化基础更扎实、全球能力更强的多语言LLM。

Abstract: Synthetic data has become a cornerstone for scaling large language models,
yet its multilingual use remains bottlenecked by translation-based prompts.
This strategy inherits English-centric framing and style and neglects cultural
dimensions, ultimately constraining model generalization. We argue that the
overlooked prompt space-the very inputs that define training
distributions-offers a more powerful lever for improving multilingual
performance. We introduce a lightweight framework for prompt-space
optimization, where translated prompts are systematically transformed for
Naturalness, Cultural Adaptation, and Difficulty Enhancement. Using an
off-the-shelf multilingual LLM, we apply these transformations to prompts for
12 languages spanning 7 families. Under identical data conditions, our
approaches achieve substantial and consistent downstream improvements over the
translation-only baseline: +4.7% on Global-MMLU accuracy, +2.4% on Flores
XCometXL and +35.3% wins in preferences on mArenaHard. We establish
prompt-space optimization as a simple yet powerful paradigm for building
multilingual LLMs that are more robust, culturally grounded, and globally
capable.

</details>


### [76] [Scaf-GRPO: Scaffolded Group Relative Policy Optimization for Enhancing LLM Reasoning](https://arxiv.org/abs/2510.19807)
*Xichen Zhang,Sitong Wu,Yinghao Zhu,Haoru Tan,Shaozuo Yu,Ziyi He,Jiaya Jia*

Main category: cs.CL

TL;DR: Scaf-GRPO通过渐进式训练框架解决强化学习中的"学习悬崖"问题，当模型学习停滞时提供分层提示，显著提升数学推理能力


<details>
  <summary>Details</summary>
Motivation: 解决强化学习中的"学习悬崖"现象——当面对远超当前能力的问题时，模型持续失败导致零奖励信号，使学习梯度消失，阻碍进步

Method: Scaffolded Group Relative Policy Optimization (Scaf-GRPO)：渐进式训练框架，先诊断学习停滞，然后注入分层提示（从抽象概念到具体步骤），让模型自行构建有效解决方案

Result: 在AIME24基准测试中，将Qwen2.5-Math-7B模型的pass@1分数相对GRPO基线提升了44.3%

Conclusion: 该框架为解锁模型解决超出其能力范围问题的能力提供了稳健有效的方法论，是推进LLM自主推理前沿的关键一步

Abstract: Reinforcement learning from verifiable rewards has emerged as a powerful
technique for enhancing the complex reasoning abilities of Large Language
Models (LLMs). However, these methods are fundamentally constrained by the
''learning cliff'' phenomenon: when faced with problems far beyond their
current capabilities, models consistently fail, yielding a persistent
zero-reward signal. In policy optimization algorithms like GRPO, this collapses
the advantage calculation to zero, rendering these difficult problems invisible
to the learning gradient and stalling progress. To overcome this, we introduce
Scaf-GRPO (Scaffolded Group Relative Policy Optimization), a progressive
training framework that strategically provides minimal guidance only when a
model's independent learning has plateaued. The framework first diagnoses
learning stagnation and then intervenes by injecting tiered in-prompt hints,
ranging from abstract concepts to concrete steps, enabling the model to
construct a valid solution by itself. Extensive experiments on challenging
mathematics benchmarks demonstrate Scaf-GRPO's effectiveness, boosting the
pass@1 score of the Qwen2.5-Math-7B model on the AIME24 benchmark by a relative
44.3% over a vanilla GRPO baseline. This result demonstrates our framework
provides a robust and effective methodology for unlocking a model's ability to
solve problems previously beyond its reach, a critical step towards extending
the frontier of autonomous reasoning in LLM.

</details>


### [77] [Hubble: a Model Suite to Advance the Study of LLM Memorization](https://arxiv.org/abs/2510.19811)
*Johnny Tian-Zheng Wei,Ameya Godbole,Mohammad Aflah Khan,Ryan Wang,Xiaoyuan Zhu,James Flemings,Nitya Kashyap,Krishna P. Gummadi,Willie Neiswanger,Robin Jia*

Main category: cs.CL

TL;DR: Hubble是一套用于研究LLM记忆机制的开源模型套件，包含标准版和扰动版模型，通过控制文本插入来模拟记忆风险，揭示了训练数据频率和训练阶段对记忆的影响。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型的记忆机制和风险，特别是敏感数据如何被记忆以及如何减轻这种风险。

Method: 开发标准模型和扰动模型，通过在不同训练阶段插入特定文本（如书籍段落、传记、测试集）来模拟记忆风险，并分析不同参数规模和训练数据量的影响。

Result: 发现记忆风险取决于敏感数据在训练语料中的相对频率，以及敏感数据在训练过程中是否持续暴露。较小的语料库中单次出现的密码比大语料库中更容易被记忆。

Conclusion: 提出了两个减轻记忆风险的最佳实践：通过增加训练语料规模来稀释敏感数据，以及让敏感数据在训练早期出现。Hubble为记忆研究、成员推理和机器遗忘提供了理想的测试平台。

Abstract: We present Hubble, a suite of fully open-source large language models (LLMs)
for the scientific study of LLM memorization. Hubble models come in standard
and perturbed variants: standard models are pretrained on a large English
corpus, and perturbed models are trained in the same way but with controlled
insertion of text (e.g., book passages, biographies, and test sets) designed to
emulate key memorization risks. Our core release includes 8 models -- standard
and perturbed models with 1B or 8B parameters, pretrained on 100B or 500B
tokens -- establishing that memorization risks are determined by the frequency
of sensitive data relative to size of the training corpus (i.e., a password
appearing once in a smaller corpus is memorized better than the same password
in a larger corpus). Our release also includes 6 perturbed models with text
inserted at different pretraining phases, showing that sensitive data without
continued exposure can be forgotten. These findings suggest two best practices
for addressing memorization risks: to dilute sensitive data by increasing the
size of the training corpus, and to order sensitive data to appear earlier in
training. Beyond these general empirical findings, Hubble enables a broad range
of memorization research; for example, analyzing the biographies reveals how
readily different types of private information are memorized. We also
demonstrate that the randomized insertions in Hubble make it an ideal testbed
for membership inference and machine unlearning, and invite the community to
further explore, benchmark, and build upon our work.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [78] [PoSh: Using Scene Graphs To Guide LLMs-as-a-Judge For Detailed Image Descriptions](https://arxiv.org/abs/2510.19060)
*Amith Ananthram,Elias Stengel-Eskin,Lorena A. Bradford,Julia Demarest,Adam Purvis,Keith Krut,Robert Stein,Rina Elster Pantalony,Mohit Bansal,Kathleen McKeown*

Main category: cs.CV

TL;DR: PoSh是一个用于详细图像描述评估的新指标，它使用场景图作为结构化评分标准来指导LLMs作为评判者，相比现有指标能更好地反映人类评分。同时提出了DOCENT数据集来验证该指标。


<details>
  <summary>Details</summary>
Motivation: 现有的图像描述评估指标（如CIDEr、SPICE）是为短文本设计的，无法有效评估长文本中的属性和关系错误。需要一种能够定位具体文本错误的新评估方法。

Method: PoSh使用场景图作为结构化评分标准，指导LLMs作为评判者进行细粒度错误分析。同时创建了DOCENT数据集，包含艺术作品、专家撰写的参考描述以及艺术史学生的质量评估。

Result: PoSh在DOCENT数据集上与人类评估的相关性比最佳替代方案高出0.05 Spearman ρ，对不同类型的图像都具有鲁棒性，并且能够作为有效的奖励函数，优于标准监督微调。

Conclusion: PoSh和DOCENT为详细图像描述评估提供了新的基准，发现基础模型在描述具有丰富场景动态的图像时难以实现无错误的完整覆盖，这为VLM进展设立了新的挑战性任务。

Abstract: While vision-language models (VLMs) have advanced into detailed image
description, evaluation remains a challenge. Standard metrics (e.g. CIDEr,
SPICE) were designed for short texts and tuned to recognize errors that are now
uncommon, such as object misidentification. In contrast, long texts require
sensitivity to attribute and relation attachments and scores that localize
errors to particular text spans. In this work, we introduce PoSh, a metric for
detailed image description that uses scene graphs as structured rubrics to
guide LLMs-as-a-Judge, producing aggregate scores grounded in fine-grained
errors (e.g. mistakes in compositional understanding). PoSh is replicable,
interpretable and a better proxy for human raters than existing metrics
(including GPT4o-as-a-Judge). To validate PoSh, we introduce a challenging new
dataset, DOCENT. This novel benchmark contains artwork, paired with
expert-written references, and model-generated descriptions, augmented with
granular and coarse judgments of their quality from art history students. Thus,
DOCENT enables evaluating both detailed image description metrics and detailed
image description itself in a challenging new domain. We show that PoSh
achieves stronger correlations (+0.05 Spearman $\rho$) with the human judgments
in DOCENT than the best open-weight alternatives, is robust to image type
(using CapArena, an existing dataset of web imagery) and is a capable reward
function, outperforming standard supervised fine-tuning. Then, using PoSh, we
characterize the performance of open and closed models in describing the
paintings, sketches and statues in DOCENT and find that foundation models
struggle to achieve full, error-free coverage of images with rich scene
dynamics, establishing a demanding new task to gauge VLM progress. Through both
PoSh and DOCENT, we hope to enable advances in important areas such as
assistive text generation.

</details>


### [79] [[De|Re]constructing VLMs' Reasoning in Counting](https://arxiv.org/abs/2510.19555)
*Simone Alghisi,Gabriel Roccabruna,Massimo Rizzoli,Seyed Mahed Mousavi,Giuseppe Riccardi*

Main category: cs.CV

TL;DR: 该论文分析了视觉语言模型在计数任务中的局限性，发现模型对物体数量、类型、空间排列和干扰物高度敏感，通过仅微调输出层可将准确率提升高达21%。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在下游任务中表现优异，但在视觉推理方面仍存在局限，如识别关系、理解时间序列和计数物体等。本文旨在深入分析这些失败的根本原因并针对性提升其推理能力。

Method: 在受控实验条件下研究7个最先进视觉语言模型的计数能力，进行分层分析，发现错误源于最后一层表示到输出空间的映射问题，通过仅微调输出层进行针对性训练。

Result: 实验显示视觉语言模型对物体数量、类型、空间排列和干扰物高度敏感，分层分析确认错误源于输出映射问题，仅微调输出层可将准确率提升高达21%，并在真实数据集上获得一致改进。

Conclusion: 视觉语言模型在计数任务中的失败主要源于输出层映射问题，通过针对性微调输出层可显著提升性能，这为改进视觉推理能力提供了有效途径。

Abstract: Vision-Language Models (VLMs) have recently gained attention due to their
competitive performance on multiple downstream tasks, achieved by following
user-input instructions. However, VLMs still exhibit several limitations in
visual reasoning, such as difficulties in identifying relations (e.g., spatial,
temporal, and among objects), understanding temporal sequences (e.g., frames),
and counting objects. In this work, we go beyond score-level benchmark
evaluations of VLMs by investigating the underlying causes of their failures
and proposing a targeted approach to improve their reasoning capabilities. We
study the reasoning skills of seven state-of-the-art VLMs in the counting task
under controlled experimental conditions. Our experiments show that VLMs are
highly sensitive to the number and type of objects, their spatial arrangement,
and the co-occurrence of distractors. A layer-wise analysis reveals that errors
are due to incorrect mapping of the last-layer representation into the output
space. Our targeted training shows that fine-tuning just the output layer
improves accuracy by up to 21%. We corroborate these findings by achieving
consistent improvements on real-world datasets.

</details>


### [80] [From Forecasting to Planning: Policy World Model for Collaborative State-Action Prediction](https://arxiv.org/abs/2510.19654)
*Zhida Zhao,Talas Fu,Yifan Wang,Lijun Wang,Huchuan Lu*

Main category: cs.CV

TL;DR: 提出Policy World Model (PWM)新驾驶范式，将世界建模与轨迹规划统一，通过无动作未来状态预测方案提升规划性能，仅使用前视摄像头输入即可达到或超越多视角多模态方法的效果。


<details>
  <summary>Details</summary>
Motivation: 现有驾驶世界模型主要用于世界模拟，与轨迹规划分离，缺乏对世界建模如何促进规划的协同机制探索。

Method: 提出PWM统一架构，集成世界建模和轨迹规划；采用无动作未来状态预测方案；通过协作状态-动作预测实现类人预期感知；引入动态增强并行令牌生成机制，包含上下文引导分词器和自适应动态焦点损失。

Result: 仅使用前视摄像头输入，性能达到或超越依赖多视角和多模态输入的最先进方法。

Conclusion: PWM成功统一世界建模与规划，通过协同预测机制提升规划可靠性，证明了单一前视摄像头输入的潜力。

Abstract: Despite remarkable progress in driving world models, their potential for
autonomous systems remains largely untapped: the world models are mostly
learned for world simulation and decoupled from trajectory planning. While
recent efforts aim to unify world modeling and planning in a single framework,
the synergistic facilitation mechanism of world modeling for planning still
requires further exploration. In this work, we introduce a new driving paradigm
named Policy World Model (PWM), which not only integrates world modeling and
trajectory planning within a unified architecture, but is also able to benefit
planning using the learned world knowledge through the proposed action-free
future state forecasting scheme. Through collaborative state-action prediction,
PWM can mimic the human-like anticipatory perception, yielding more reliable
planning performance. To facilitate the efficiency of video forecasting, we
further introduce a dynamically enhanced parallel token generation mechanism,
equipped with a context-guided tokenizer and an adaptive dynamic focal loss.
Despite utilizing only front camera input, our method matches or exceeds
state-of-the-art approaches that rely on multi-view and multi-modal inputs.
Code and model weights will be released at
https://github.com/6550Zhao/Policy-World-Model.

</details>


### [81] [Pico-Banana-400K: A Large-Scale Dataset for Text-Guided Image Editing](https://arxiv.org/abs/2510.19808)
*Yusu Qian,Eli Bocek-Rivele,Liangchen Song,Jialing Tong,Yinfei Yang,Jiasen Lu,Wenze Hu,Zhe Gan*

Main category: cs.CV

TL;DR: Pico-Banana-400K是一个包含40万张图像的大规模指令引导图像编辑数据集，通过系统化的质量控制和多样化编辑类型覆盖，为多模态图像编辑模型提供训练和评估基础。


<details>
  <summary>Details</summary>
Motivation: 当前多模态模型在文本引导图像编辑方面取得显著进展，但研究社区缺乏大规模、高质量、开放可用的真实图像数据集，限制了进一步研究。

Method: 利用Nano-Banana从OpenImages真实照片生成多样化编辑对，采用细粒度图像编辑分类法确保编辑类型全面覆盖，通过MLLM质量评分和精心筛选保证内容保持和指令忠实度。

Result: 构建了包含三个专业子集的400K图像数据集：72K多轮编辑示例、56K偏好子集、长短指令配对集，支持复杂编辑场景研究。

Conclusion: Pico-Banana-400K为下一代文本引导图像编辑模型的训练和基准测试提供了强大基础，推动了多模态编辑研究的发展。

Abstract: Recent advances in multimodal models have demonstrated remarkable text-guided
image editing capabilities, with systems like GPT-4o and Nano-Banana setting
new benchmarks. However, the research community's progress remains constrained
by the absence of large-scale, high-quality, and openly accessible datasets
built from real images. We introduce Pico-Banana-400K, a comprehensive
400K-image dataset for instruction-based image editing. Our dataset is
constructed by leveraging Nano-Banana to generate diverse edit pairs from real
photographs in the OpenImages collection. What distinguishes Pico-Banana-400K
from previous synthetic datasets is our systematic approach to quality and
diversity. We employ a fine-grained image editing taxonomy to ensure
comprehensive coverage of edit types while maintaining precise content
preservation and instruction faithfulness through MLLM-based quality scoring
and careful curation. Beyond single turn editing, Pico-Banana-400K enables
research into complex editing scenarios. The dataset includes three specialized
subsets: (1) a 72K-example multi-turn collection for studying sequential
editing, reasoning, and planning across consecutive modifications; (2) a
56K-example preference subset for alignment research and reward model training;
and (3) paired long-short editing instructions for developing instruction
rewriting and summarization capabilities. By providing this large-scale,
high-quality, and task-rich resource, Pico-Banana-400K establishes a robust
foundation for training and benchmarking the next generation of text-guided
image editing models.

</details>


### [82] [olmOCR 2: Unit Test Rewards for Document OCR](https://arxiv.org/abs/2510.19817)
*Jake Poznanski,Luca Soldaini,Kyle Lo*

Main category: cs.CV

TL;DR: olmOCR 2是一个基于7B视觉语言模型的OCR系统，使用强化学习和多样化单元测试进行训练，在数学公式转换、表格解析和多栏布局方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 开发更强大的OCR系统，将数字化打印文档转换为清晰、自然排序的纯文本，特别是在处理复杂布局时提高准确性。

Method: 使用强化学习与可验证奖励训练专门的7B视觉语言模型，通过生成具有多样化布局的合成文档来创建单元测试用例。

Result: 在olmOCR-Bench基准测试中达到最先进性能，在数学公式转换、表格解析和多栏布局方面相比之前版本有最大改进。

Conclusion: olmOCR 2通过强化学习和多样化单元测试训练，在复杂文档布局处理上取得显著进步，模型、数据和代码已开源发布。

Abstract: We present olmOCR 2, the latest in our family of powerful OCR systems for
converting digitized print documents, like PDFs, into clean, naturally ordered
plain text. olmOCR 2 is powered by olmOCR-2-7B-1025, a specialized, 7B vision
language model (VLM) trained using reinforcement learning with verifiable
rewards (RLVR), where our rewards are a diverse set of binary unit tests. To
scale unit test creation, we develop a pipeline for generating synthetic
documents with diverse and challenging layouts, known ground-truth HTML source
code, and extracted test cases. We show that RL training on these test cases
results in state-of-the-art performance on olmOCR-Bench, our English-language
OCR benchmark, with the largest improvements in math formula conversion, table
parsing, and multi-column layouts compared to previous versions. We release our
model, data and code under permissive open licenses.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [83] [StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction](https://arxiv.org/abs/2510.18938)
*Qianheng Xu*

Main category: eess.AS

TL;DR: 该论文提出了StutterZero和StutterFormer两种端到端的波形到波形模型，能够直接将口吃语音转换为流畅语音并同时预测其转录，在词错误率和语义相似度方面显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 全球有超过7000万人经历口吃，但现有的自动语音系统往往错误解读或无法准确转录不流畅的语音。现有方法依赖手工特征提取或多阶段ASR-TTS流水线，导致转录与音频重建分离并放大失真。

Method: 提出StutterZero（使用卷积双向LSTM编码器-解码器加注意力机制）和StutterFormer（集成双流Transformer与共享声学-语言表示）两种端到端架构，在SEP-28K和LibriStutter语料库合成的配对口吃-流畅数据上训练。

Result: 在FluencyBank数据集上评估，StutterZero相比领先的Whisper-Medium模型词错误率降低24%，BERTScore语义相似度提升31%；StutterFormer表现更好，词错误率降低28%，BERTScore提升34%。

Conclusion: 结果验证了直接端到端口吃到流畅语音转换的可行性，为人机交互、语音治疗和面向可访问性的AI系统提供了新机遇。

Abstract: Over 70 million people worldwide experience stuttering, yet most automatic
speech systems misinterpret disfluent utterances or fail to transcribe them
accurately. Existing methods for stutter correction rely on handcrafted feature
extraction or multi-stage automatic speech recognition (ASR) and text-to-speech
(TTS) pipelines, which separate transcription from audio reconstruction and
often amplify distortions. This work introduces StutterZero and StutterFormer,
the first end-to-end waveform-to-waveform models that directly convert
stuttered speech into fluent speech while jointly predicting its transcription.
StutterZero employs a convolutional-bidirectional LSTM encoder-decoder with
attention, whereas StutterFormer integrates a dual-stream Transformer with
shared acoustic-linguistic representations. Both architectures are trained on
paired stuttered-fluent data synthesized from the SEP-28K and LibriStutter
corpora and evaluated on unseen speakers from the FluencyBank dataset. Across
all benchmarks, StutterZero had a 24% decrease in Word Error Rate (WER) and a
31% improvement in semantic similarity (BERTScore) compared to the leading
Whisper-Medium model. StutterFormer achieved better results, with a 28%
decrease in WER and a 34% improvement in BERTScore. The results validate the
feasibility of direct end-to-end stutter-to-fluent speech conversion, offering
new opportunities for inclusive human-computer interaction, speech therapy, and
accessibility-oriented AI systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [84] [Benchmarking On-Device Machine Learning on Apple Silicon with MLX](https://arxiv.org/abs/2510.18921)
*Oluwaseun A. Ajayi,Ogundepo Odunayo*

Main category: cs.LG

TL;DR: 本文对苹果硅设备上的MLX框架进行性能评估，重点关注Transformer模型的推理延迟，并与PyTorch框架进行对比。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型和机器学习的广泛应用，需要能够在笔记本电脑和移动设备等小型设备上部署这些模型的框架。MLX框架专为苹果硅设备优化，旨在促进更便捷的研究、实验和原型开发。

Method: 创建了MLX-transformers框架，包含不同的Transformer实现，并将PyTorch模型检查点转换为MLX格式。在两种苹果硅MacBook设备上对BERT、RoBERTa和XLM-RoBERTa模型进行基准测试，并与NVIDIA CUDA GPU进行对比。

Result: 研究结果表明MLX在苹果生态系统中具有实现高效和更易访问的设备端机器学习应用的潜力。

Conclusion: MLX框架在苹果硅设备上展现出良好的性能表现，为设备端机器学习应用提供了有效的解决方案。

Abstract: The recent widespread adoption of Large Language Models (LLMs) and machine
learning in general has sparked research interest in exploring the
possibilities of deploying these models on smaller devices such as laptops and
mobile phones. This creates a need for frameworks and approaches that are
capable of taking advantage of on-device hardware. The MLX framework was
created to address this need. It is a framework optimized for machine learning
(ML) computations on Apple silicon devices, facilitating easier research,
experimentation, and prototyping.
  This paper presents a performance evaluation of MLX, focusing on inference
latency of transformer models. We compare the performance of different
transformer architecture implementations in MLX with their Pytorch
counterparts. For this research we create a framework called MLX-transformers
which includes different transformer implementations in MLX and downloads the
model checkpoints in pytorch and converts it to the MLX format. By leveraging
the advanced architecture and capabilities of Apple Silicon, MLX-Transformers
enables seamless execution of transformer models directly sourced from Hugging
Face, eliminating the need for checkpoint conversion often required when
porting models between frameworks.
  Our study benchmarks different transformer models on two Apple Silicon
macbook devices against an NVIDIA CUDA GPU. Specifically, we compare the
inference latency performance of models with the same parameter sizes and
checkpoints. We evaluate the performance of BERT, RoBERTa, and XLM-RoBERTa
models, with the intention of extending future work to include models of
different modalities, thus providing a more comprehensive assessment of MLX's
capabilities. The results highlight MLX's potential in enabling efficient and
more accessible on-device ML applications within Apple's ecosystem.

</details>


### [85] [BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping](https://arxiv.org/abs/2510.18927)
*Zhiheng Xi,Xin Guo,Yang Nan,Enyu Zhou,Junrui Shen,Wenxiang Chen,Jiaqi Liu,Jixuan Huang,Zhihao Zhang,Honglin Guo,Xun Deng,Zhikai Lei,Miao Zheng,Guoteng Wang,Shuo Zhang,Peng Sun,Rui Zheng,Hang Yan,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.LG

TL;DR: BAPO是一种平衡策略优化方法，通过动态调整裁剪边界来解决离策略RL中的熵衰减和不稳定问题，在多个基准测试中超越了开源和专有模型。


<details>
  <summary>Details</summary>
Motivation: 离策略强化学习在LLM对齐中面临策略熵急剧下降、优化不稳定甚至崩溃的挑战，主要由于负优势样本主导梯度更新和固定裁剪机制阻碍熵增加。

Method: 提出BAPO方法，基于熵裁剪规则动态调整裁剪边界，自适应平衡正负贡献，保持策略熵并稳定优化过程。

Result: 在AIME 2024和2025基准测试中，7B BAPO模型超越SkyWork-OR1-7B等开源模型，32B BAPO模型在同规模模型中达到SOTA，并超越o3-mini和Gemini-2.5-Flash-Thinking等专有系统。

Conclusion: BAPO通过解决离策略RL中的优化不平衡问题，实现了快速、稳定且数据高效的训练，为LLM对齐提供了有效解决方案。

Abstract: Reinforcement learning (RL) has recently become the core paradigm for
aligning and strengthening large language models (LLMs). Yet, applying RL in
off-policy settings--where stale data from past policies are used for
training--improves sample efficiency, but remains challenging: policy entropy
declines sharply, optimization often becomes unstable and may even collapse.
Through theoretical and empirical analysis, we identify two key insights: (i)
an imbalance in optimization, where negative-advantage samples dominate the
policy gradient, suppressing useful behaviors and risking gradient explosions;
and (ii) the derived Entropy-Clip Rule, which reveals that the fixed clipping
mechanism in PPO-like objectives systematically blocks entropy-increasing
updates, thereby driving the policy toward over-exploitation at the expense of
exploration. Building on these insights, we propose BAlanced Policy
Optimization with Adaptive Clipping (BAPO), a simple yet effective method that
dynamically adjusts clipping bounds to adaptively re-balance positive and
negative contributions, preserve entropy, and stabilize RL optimization. Across
diverse off-policy scenarios--including sample replay and partial rollout--BAPO
achieves fast, stable, and data-efficient training. On AIME 2024 and AIME 2025
benchmarks, our 7B BAPO model surpasses open-source counterparts such as
SkyWork-OR1-7B, while our 32B BAPO model not only achieves state-of-the-art
results among models of the same scale but also outperforms leading proprietary
systems like o3-mini and Gemini-2.5-Flash-Thinking.

</details>


### [86] [NeuroAda: Activating Each Neuron's Potential for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2510.18940)
*Zhi Zhang,Yixian Shen,Congfeng Cao,Ekaterina Shutova*

Main category: cs.LG

TL;DR: NeuroAda是一种新颖的参数高效微调方法，通过识别重要参数并为其添加旁路连接，在保持高内存效率的同时实现细粒度模型微调。


<details>
  <summary>Details</summary>
Motivation: 现有的参数高效微调方法存在权衡：基于添加的方法（如LoRA）内存效率高但表示能力有限，不适合细粒度适应；选择性原位适应方法能精确适应但内存消耗大。需要平衡这种权衡。

Method: 首先识别网络中的重要参数（连接），然后为这些选定的参数引入旁路连接。在微调过程中，只更新旁路连接，保持原始模型参数冻结。

Result: 在23+个自然语言生成和理解任务上的实证结果显示，NeuroAda仅使用≤0.02%的可训练参数就达到了最先进的性能，同时将CUDA内存使用量减少了高达60%。

Conclusion: NeuroAda成功解决了参数高效微调中的权衡问题，实现了细粒度模型微调与高内存效率的平衡。

Abstract: Existing parameter-efficient fine-tuning (PEFT) methods primarily fall into
two categories: addition-based and selective in-situ adaptation. The former,
such as LoRA, introduce additional modules to adapt the model to downstream
tasks, offering strong memory efficiency. However, their representational
capacity is often limited, making them less suitable for fine-grained
adaptation. In contrast, the latter directly fine-tunes a carefully chosen
subset of the original model parameters, allowing for more precise and
effective adaptation, but at the cost of significantly increased memory
consumption. To reconcile this trade-off, we propose NeuroAda, a novel PEFT
method that enables fine-grained model finetuning while maintaining high memory
efficiency. Our approach first identifies important parameters (i.e.,
connections within the network) as in selective adaptation, and then introduces
bypass connections for these selected parameters. During finetuning, only the
bypass connections are updated, leaving the original model parameters frozen.
Empirical results on 23+ tasks spanning both natural language generation and
understanding demonstrate that NeuroAda achieves state-of-the-art performance
with as little as $\leq \textbf{0.02}\%$ trainable parameters, while reducing
CUDA memory usage by up to 60%. We release our code here:
https://github.com/FightingFighting/NeuroAda.git.

</details>


### [87] [Every Attention Matters: An Efficient Hybrid Architecture for Long-Context Reasoning](https://arxiv.org/abs/2510.19338)
*Ling Team,Bin Han,Caizhi Tang,Chen Liang,Donghao Zhang,Fan Yuan,Feng Zhu,Jie Gao,Jingyu Hu,Longfei Li,Meng Li,Mingyang Zhang,Peijie Jiang,Peng Jiao,Qian Zhao,Qingyuan Yang,Wenbo Shen,Xinxing Yang,Yalin Zhang,Yankun Ren,Yao Zhao,Yibo Cao,Yixuan Sun,Yue Zhang,Yuchen Fang,Zibin Lin,Zixuan Cheng,Jun Zhou*

Main category: cs.LG

TL;DR: 提出了Ring-linear模型系列，包括16B参数的Ring-mini-linear-2.0和104B参数的Ring-flash-linear-2.0，采用线性注意力和softmax注意力的混合架构，显著降低长上下文推理的I/O和计算开销。


<details>
  <summary>Details</summary>
Motivation: 解决长上下文推理场景中传统注意力机制的高I/O和计算开销问题，通过混合注意力架构来优化推理效率。

Method: 采用线性注意力和softmax注意力的混合架构，系统探索不同注意力机制的比例，使用自研的高性能FP8算子库linghe提升训练效率。

Result: 相比32B密集模型，推理成本降低到1/10；相比原始Ring系列，成本降低超过50%；在多个复杂推理基准测试中保持SOTA性能。

Conclusion: Ring-linear模型系列通过混合注意力架构和优化算子库，在保持高性能的同时显著降低了推理成本，为长上下文推理提供了高效解决方案。

Abstract: In this technical report, we present the Ring-linear model series,
specifically including Ring-mini-linear-2.0 and Ring-flash-linear-2.0.
Ring-mini-linear-2.0 comprises 16B parameters and 957M activations, while
Ring-flash-linear-2.0 contains 104B parameters and 6.1B activations. Both
models adopt a hybrid architecture that effectively integrates linear attention
and softmax attention, significantly reducing I/O and computational overhead in
long-context inference scenarios. Compared to a 32 billion parameter dense
model, this series reduces inference cost to 1/10, and compared to the original
Ring series, the cost is also reduced by over 50%. Furthermore, through
systematic exploration of the ratio between different attention mechanisms in
the hybrid architecture, we have identified the currently optimal model
structure. Additionally, by leveraging our self-developed high-performance FP8
operator library-linghe, overall training efficiency has been improved by 50%.
Benefiting from the high alignment between the training and inference engine
operators, the models can undergo long-term, stable, and highly efficient
optimization during the reinforcement learning phase, consistently maintaining
SOTA performance across multiple challenging complex reasoning benchmarks.

</details>


### [88] [LLM Unlearning with LLM Beliefs](https://arxiv.org/abs/2510.19422)
*Kemou Li,Qizhou Wang,Yue Wang,Fengpeng Li,Jun Liu,Bo Han,Jiantao Zhou*

Main category: cs.LG

TL;DR: 本文提出了一种新的语言模型遗忘方法，通过引导模型抑制目标响应和模型自身高置信度生成，有效解决了传统梯度上升方法导致的概率质量挤压效应问题。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型遗忘方法依赖梯度上升来降低特定目标响应的概率，但这种方法会导致概率质量重新分布到高似然区域，产生语义相关的重新表述，造成虚假遗忘。

Method: 提出引导框架(BS)，将挤压效应与模型自身高置信度生成（模型信念）联系起来。通过同时抑制目标响应和模型信念，BS-T衰减高概率token，BS-S移除整个高置信度生成，实现更彻底的遗忘。

Result: 在多个基准测试和不同模型家族上的广泛实验证实了该方法的有效性，能够实现更彻底的遗忘同时保持模型效用。

Conclusion: 该引导框架通过直接对抗挤压效应，解决了传统遗忘方法中的虚假遗忘问题，为语言模型安全遗忘提供了更可靠的解决方案。

Abstract: Large language models trained on vast corpora inherently risk memorizing
sensitive or harmful content, which may later resurface in their outputs.
Prevailing unlearning methods generally rely on gradient ascent and its
variants to lower the probability of specific target responses. However, we
find that this strategy induces a critical side effect: probability mass is
redistributed into high-likelihood regions, often corresponding to semantically
related rephrasings of the targets. We refer to this as the squeezing effect,
which explains why many methods yield merely spurious unlearning, a problem
further obscured by automated metrics (e.g., ROUGE, truth ratio) that misreport
actual success. To address this, we propose a bootstrapping (BS) framework that
explicitly links the squeezing effect with the model's own high-confidence
generations, namely its model beliefs. Since model beliefs inherently capture
the very high-likelihood regions where probability mass is squeezed,
incorporating them into the unlearning objective directly counters the
squeezing effect. By jointly suppressing both target responses and model
beliefs, BS-T (token) attenuates high-probability tokens, whereas BS-S
(sequence) removes entire high-confidence generations, together achieving more
thorough forgetting while preserving utility. Extensive experiments across
diverse benchmarks with various model families confirm the effectiveness of our
approach.

</details>


### [89] [GaLLoP: Gradient-based Sparse Learning on Low-Magnitude Parameters](https://arxiv.org/abs/2510.19778)
*Anand Choudhary,Yasser Sulaıman,Lukas Mauch,Ghouthi Boukli Hacene,Fabien Cardinaux,Antoine Bosselut*

Main category: cs.LG

TL;DR: GaLLoP是一种基于梯度的稀疏学习技术，通过选择梯度幅度最大但预训练幅度最小的参数进行微调，在保持预训练知识的同时提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 稀疏微调技术通过仅调整模型参数的稀疏子集来适应下游任务，但其效果取决于如何最优选择要微调的参数。

Method: GaLLoP选择在下游任务上梯度幅度最大且预训练幅度最小的参数进行微调，优先考虑任务相关性强但对预训练知识破坏性小的参数。

Result: 在LLaMA3 8B和Gemma 2B上的实验表明，GaLLoP在内外分布性能上持续优于或匹配其他参数高效微调技术（LoRA、DoRA、SAFT），并能减轻灾难性遗忘和任务数据记忆问题。

Conclusion: GaLLoP通过保护重要预训练参数不变，稳定了性能表现，并在大多数随机种子下展现出鲁棒的泛化能力。

Abstract: Sparse fine-tuning techniques adapt LLMs to downstream tasks by only tuning a
sparse subset of model parameters. However, the effectiveness of sparse
adaptation depends on optimally selecting the model parameters to be
fine-tuned. In this work, we introduce a novel sparse fine-tuning technique
named GaLLoP: Gradient-based Sparse Learning on Low-Magnitude Parameters, which
fine-tunes only those model parameters which have the largest gradient
magnitudes on downstream tasks and the smallest pre-trained magnitudes,
intuitively prioritizing parameters that are highly task-relevant, but
minimally disruptive to pre-trained knowledge. Our experimentation with LLaMA3
8B and Gemma 2B as base models shows that GaLLoP consistently improves or
matches the in-distribution as well as out-of-distribution performance obtained
via the usage of other leading parameter-efficient fine-tuning techniques,
including LoRA, DoRA, and SAFT. Our analysis demonstrates that GaLLoP mitigates
catastrophic forgetting and memorization of task data, as important pre-trained
parameters remain unchanged, and stabilizes performance relative to other
fine-tuning techniques, robustly generalizing across most random seeds.

</details>


### [90] [Blackbox Model Provenance via Palimpsestic Membership Inference](https://arxiv.org/abs/2510.19796)
*Rohith Kuditipudi,Jing Huang,Sally Zhu,Diyi Yang,Christopher Potts,Percy Liang*

Main category: cs.LG

TL;DR: 该论文提出了一种检测模型抄袭的方法，通过分析训练数据顺序与模型输出的相关性来证明一个模型是否基于另一个模型的训练结果。


<details>
  <summary>Details</summary>
Motivation: 解决如何证明一个黑盒语言模型是否基于另一个开源模型的训练结果的问题，为模型版权保护提供统计证据。

Method: 将问题建模为独立性测试，利用训练数据随机排序的特性，通过分析模型对训练数据顺序的敏感度来检测抄袭。在查询设置中直接估计模型对训练数据顺序的似然度；在观察设置中通过文本重叠或重新训练部分模型来检测。

Result: 在查询设置中，对40多个Pythia和OLMo模型的微调版本进行测试，大多数情况下p值达到1e-8级别。在观察设置中，第二种方法仅需几百个token就能可靠区分，第一种方法需要数十万个token。

Conclusion: 该方法能够有效检测模型抄袭，为开源模型版权保护提供了可行的统计验证手段。

Abstract: Suppose Alice trains an open-weight language model and Bob uses a blackbox
derivative of Alice's model to produce text. Can Alice prove that Bob is using
her model, either by querying Bob's derivative model (query setting) or from
the text alone (observational setting)? We formulate this question as an
independence testing problem--in which the null hypothesis is that Bob's model
or text is independent of Alice's randomized training run--and investigate it
through the lens of palimpsestic memorization in language models: models are
more likely to memorize data seen later in training, so we can test whether Bob
is using Alice's model using test statistics that capture correlation between
Bob's model or text and the ordering of training examples in Alice's training
run. If Alice has randomly shuffled her training data, then any significant
correlation amounts to exactly quantifiable statistical evidence against the
null hypothesis, regardless of the composition of Alice's training data. In the
query setting, we directly estimate (via prompting) the likelihood Bob's model
gives to Alice's training examples and order; we correlate the likelihoods of
over 40 fine-tunes of various Pythia and OLMo base models ranging from 1B to
12B parameters with the base model's training data order, achieving a p-value
on the order of at most 1e-8 in all but six cases. In the observational
setting, we try two approaches based on estimating 1) the likelihood of Bob's
text overlapping with spans of Alice's training examples and 2) the likelihood
of Bob's text with respect to different versions of Alice's model we obtain by
repeating the last phase (e.g., 1%) of her training run on reshuffled data. The
second approach can reliably distinguish Bob's text from as little as a few
hundred tokens; the first does not involve any retraining but requires many
more tokens (several hundred thousand) to achieve high power.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [91] [ColorAgent: Building A Robust, Personalized, and Interactive OS Agent](https://arxiv.org/abs/2510.19386)
*Ning Li,Qiqiang Lin,Zheng Wu,Xiaoyun Mo,Weiming Zhang,Yin Zhao,Xiangmou Qu,Jiamu Zhou,Jun Wang,Congmin Zheng,Yuanyi Song,Hongjiang Chen,Heyuan Huang,Jihong Wang,Jiaxin Yin,Jingwei Yu,Junwei Liao,Qiuying Peng,Xingyu Lou,Jun Wang,Weiwen Liu,Zhuosheng Zhang,Weinan Zhang*

Main category: cs.MA

TL;DR: ColorAgent是一个操作系统AI代理，通过强化学习和自我进化训练实现长时程环境交互，采用多代理框架确保通用性和鲁棒性，在AndroidWorld和AndroidLab基准测试中分别达到77.2%和50.7%的成功率，创下新纪录。


<details>
  <summary>Details</summary>
Motivation: 随着硬件、软件和大语言模型技术的发展，人机交互从命令行界面演进到AI代理交互，需要构建能够执行用户指令并忠实遵循用户意图的操作系统代理。

Method: 通过逐步强化学习和自我进化训练增强模型能力，开发定制化的多代理框架确保通用性、一致性和鲁棒性，探索个性化用户意图识别和主动交互功能。

Result: 在AndroidWorld和AndroidLab基准测试中分别取得77.2%和50.7%的成功率，建立了新的最先进水平。

Conclusion: 当前基准测试不足以全面评估操作系统代理，未来需要在评估范式、代理协作和安全性方面进一步探索。

Abstract: With the advancements in hardware, software, and large language model
technologies, the interaction between humans and operating systems has evolved
from the command-line interface to the rapidly emerging AI agent interactions.
Building an operating system (OS) agent capable of executing user instructions
and faithfully following user desires is becoming a reality. In this technical
report, we present ColorAgent, an OS agent designed to engage in long-horizon,
robust interactions with the environment while also enabling personalized and
proactive user interaction. To enable long-horizon interactions with the
environment, we enhance the model's capabilities through step-wise
reinforcement learning and self-evolving training, while also developing a
tailored multi-agent framework that ensures generality, consistency, and
robustness. In terms of user interaction, we explore personalized user intent
recognition and proactive engagement, positioning the OS agent not merely as an
automation tool but as a warm, collaborative partner. We evaluate ColorAgent on
the AndroidWorld and AndroidLab benchmarks, achieving success rates of 77.2%
and 50.7%, respectively, establishing a new state of the art. Nonetheless, we
note that current benchmarks are insufficient for a comprehensive evaluation of
OS agents and propose further exploring directions in future work, particularly
in the areas of evaluation paradigms, agent collaboration, and security. Our
code is available at https://github.com/MadeAgents/mobile-use.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [92] [Human-Agent Collaborative Paper-to-Page Crafting for Under $0.1](https://arxiv.org/abs/2510.19600)
*Qianli Ma,Siyu Wang,Yilin Chen,Yinhao Tang,Yixiang Yang,Chang Guo,Bingjie Gao,Zhening Xing,Yanan Sun,Zhipeng Zhang*

Main category: cs.SE

TL;DR: AutoPage是一个多智能体系统，通过分层协作流程将学术论文自动转换为交互式网页，解决了传统手动创建项目网页的痛点。


<details>
  <summary>Details</summary>
Motivation: 研究人员在创建项目网页时面临手动重复工作的困扰，现有自动化工具无法处理网页的动态交互特性，需要一种新的解决方案。

Method: 采用粗到细的流水线方法，包括叙事规划、多模态内容生成和交互式渲染，使用专门的"检查器"智能体验证内容准确性，并设置可选的人工检查点。

Result: AutoPage在15分钟内以低于0.1美元的成本生成高质量、视觉吸引力强的网页，并创建了首个相关基准PageBench。

Conclusion: AutoPage不仅是一个工具，更是一个强大的协作助手，成功将论文转换为交互式网页，显著提高了研究传播的效率。

Abstract: In the quest for scientific progress, communicating research is as vital as
the discovery itself. Yet, researchers are often sidetracked by the manual,
repetitive chore of building project webpages to make their dense papers
accessible. While automation has tackled static slides and posters, the
dynamic, interactive nature of webpages has remained an unaddressed challenge.
To bridge this gap, we reframe the problem, arguing that the solution lies not
in a single command, but in a collaborative, hierarchical process. We introduce
$\textbf{AutoPage}$, a novel multi-agent system that embodies this philosophy.
AutoPage deconstructs paper-to-page creation into a coarse-to-fine pipeline
from narrative planning to multimodal content generation and interactive
rendering. To combat AI hallucination, dedicated "Checker" agents verify each
step against the source paper, while optional human checkpoints ensure the
final product aligns perfectly with the author's vision, transforming the
system from a mere tool into a powerful collaborative assistant. To rigorously
validate our approach, we also construct $\textbf{PageBench}$, the first
benchmark for this new task. Experiments show AutoPage not only generates
high-quality, visually appealing pages but does so with remarkable efficiency
in under 15 minutes for less than \$0.1. Code and dataset will be released at
$\href{https://mqleet.github.io/AutoPage_ProjectPage/}{Webpage}$.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [93] [Towards Better Health Conversations: The Benefits of Context-seeking](https://arxiv.org/abs/2510.18880)
*Rory Sayres,Yuexing Hao,Abbi Ward,Amy Wang,Beverly Freeman,Serena Zhan,Diego Ardila,Jimmy Li,I-Ching Lee,Anna Iurchenko,Siyi Kou,Kartikeya Badola,Jimmy Hu,Bhawesh Kumar,Keith Johnson,Supriya Vijay,Justin Krogue,Avinatan Hassidim,Yossi Matias,Dale R. Webster,Sunny Virmani,Yun Liu,Quang Duong,Mike Schaekermann*

Main category: cs.HC

TL;DR: 论文研究了人们在健康咨询中与大型语言模型的互动，发现主动寻求上下文信息的AI比基线AI更受用户青睐，能提供更有帮助、相关且个性化的回答。


<details>
  <summary>Details</summary>
Motivation: 在当今信息环境中，健康问题咨询可能令人望而生畏。大型语言模型可以提供定制化、易获取的信息，但也存在不准确、有偏见或误导的风险。

Method: 进行了4项混合方法研究（共163名参与者），包括定性研究分析人们如何与LLMs互动，并开发了主动寻求上下文的"Wayfinding AI"，在随机盲法研究中与基线AI进行比较。

Result: 参与者认为Wayfinding AI比基线AI更有帮助、更相关、更能针对他们的关切。主动寻求上下文即使意味着延迟回答几个回合，也被参与者所重视。

Conclusion: 主动寻求上下文对对话动态有强烈影响，为对话AI设计提供了帮助导航健康话题的模式。

Abstract: Navigating health questions can be daunting in the modern information
landscape. Large language models (LLMs) may provide tailored, accessible
information, but also risk being inaccurate, biased or misleading. We present
insights from 4 mixed-methods studies (total N=163), examining how people
interact with LLMs for their own health questions. Qualitative studies revealed
the importance of context-seeking in conversational AIs to elicit specific
details a person may not volunteer or know to share. Context-seeking by LLMs
was valued by participants, even if it meant deferring an answer for several
turns. Incorporating these insights, we developed a "Wayfinding AI" to
proactively solicit context. In a randomized, blinded study, participants rated
the Wayfinding AI as more helpful, relevant, and tailored to their concerns
compared to a baseline AI. These results demonstrate the strong impact of
proactive context-seeking on conversational dynamics, and suggest design
patterns for conversational AI to help navigate health topics.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [94] [A Multi-faceted Analysis of Cognitive Abilities: Evaluating Prompt Methods with Large Language Models on the CONSORT Checklist](https://arxiv.org/abs/2510.19139)
*Sohyeon Jeon,Hyung-Chul Lee*

Main category: cs.AI

TL;DR: 本研究评估了大型语言模型在根据CONSORT标准评估临床试验报告方面的能力，发现不同模型和提示条件下存在认知策略差异。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在医疗领域迅速扩展，但这些系统根据CONSORT标准评估临床试验报告的能力尚不明确，特别是在认知和推理策略方面。

Method: 采用行为和元认知分析方法，使用专家验证数据，系统比较了两个代表性LLMs在三种提示条件下的表现。

Result: 模型在不同CONSORT项目和提示类型上表现出明显差异，推理风格、明确不确定性和替代解释的变化塑造了响应模式。

Conclusion: 结果凸显了这些系统在临床合规自动化方面的当前局限性，并强调了理解其认知适应和策略行为对于开发更可解释和可靠的医疗AI的重要性。

Abstract: Despite the rapid expansion of Large Language Models (LLMs) in healthcare,
the ability of these systems to assess clinical trial reporting according to
CONSORT standards remains unclear, particularly with respect to their cognitive
and reasoning strategies. This study applies a behavioral and metacognitive
analytic approach with expert-validated data, systematically comparing two
representative LLMs under three prompt conditions. Clear differences emerged in
how the models approached various CONSORT items, and prompt types, including
shifts in reasoning style, explicit uncertainty, and alternative
interpretations shaped response patterns. Our results highlight the current
limitations of these systems in clinical compliance automation and underscore
the importance of understanding their cognitive adaptations and strategic
behavior in developing more explainable and reliable medical AI.

</details>


### [95] [The Zero-Step Thinking: An Empirical Study of Mode Selection as Harder Early Exit in Reasoning Models](https://arxiv.org/abs/2510.19176)
*Yuqiao Tan,Shizhu He,Kang Liu,Jun Zhao*

Main category: cs.AI

TL;DR: 本文分析了推理模型中的模式选择问题，将其视为早期退出问题的更具挑战性变体，并评估了不同方法在零步思考场景下的表现。


<details>
  <summary>Details</summary>
Motivation: 推理模型在数学和逻辑推理任务中表现出色，但其逐步推理过程常导致过度思考，造成不必要的计算开销。模式选择和早期退出方法旨在减少这种计算负担。

Method: 将模式选择识别为早期退出问题的变体，通过实证研究评估9种基线方法，比较基于提示的方法和利用内部信息的方法在零步思考场景下的表现。

Result: 基于提示的方法由于分类能力有限且仅依赖少量人工制作信息而经常失败；利用内部信息的方法在大多数场景下表现更好，但仍存在稳定性问题。

Conclusion: 现有仅依赖模型提供信息的方法在信息有限的情况下不足以有效解决模式选择问题，突显了该任务的持续挑战性。

Abstract: Reasoning models have demonstrated exceptional performance in tasks such as
mathematics and logical reasoning, primarily due to their ability to engage in
step-by-step thinking during the reasoning process. However, this often leads
to overthinking, resulting in unnecessary computational overhead. To address
this issue, Mode Selection aims to automatically decide between Long-CoT
(Chain-of-Thought) or Short-CoT by utilizing either a Thinking or NoThinking
mode. Simultaneously, Early Exit determines the optimal stopping point during
the iterative reasoning process. Both methods seek to reduce the computational
burden. In this paper, we first identify Mode Selection as a more challenging
variant of the Early Exit problem, as they share similar objectives but differ
in decision timing. While Early Exit focuses on determining the best stopping
point for concise reasoning at inference time, Mode Selection must make this
decision at the beginning of the reasoning process, relying on pre-defined fake
thoughts without engaging in an explicit reasoning process, referred to as
zero-step thinking. Through empirical studies on nine baselines, we observe
that prompt-based approaches often fail due to their limited classification
capabilities when provided with minimal hand-crafted information. In contrast,
approaches that leverage internal information generally perform better across
most scenarios but still exhibit issues with stability. Our findings indicate
that existing methods relying solely on the information provided by models are
insufficient for effectively addressing Mode Selection in scenarios with
limited information, highlighting the ongoing challenges of this task. Our code
is available at https://github.com/Trae1ounG/Zero_Step_Thinking.

</details>


### [96] [HSCodeComp: A Realistic and Expert-level Benchmark for Deep Search Agents in Hierarchical Rule Application](https://arxiv.org/abs/2510.19631)
*Yiqian Yang,Tian Lan,Qianghuai Jia,Li Zhu,Hui Jiang,Hang Zhu,Longyue Wang,Weihua Luo,Kaifu Zhang*

Main category: cs.AI

TL;DR: HSCodeComp是首个评估深度搜索代理在分层规则应用能力的电子商务基准，要求代理根据商品描述预测10位HS编码，结果显示当前最佳代理仅达46.8%准确率，远低于人类专家的95.0%。


<details>
  <summary>Details</summary>
Motivation: 当前代理基准忽视了深度搜索代理在应用复杂规则（如法律条款、医疗手册和关税规则）方面的能力，这些规则具有模糊边界和隐式逻辑关系，需要专门评估。

Method: 从大型电商平台收集真实数据构建HSCodeComp基准，包含632个产品条目，涵盖多样产品类别，HS编码由多位人类专家标注，评估代理在分层规则应用中的表现。

Result: 实验显示当前最先进的LLM、开源和闭源代理在10位HS编码预测上的最佳准确率仅为46.8%，远低于人类专家的95.0%，测试时扩展也无法进一步提升性能。

Conclusion: HSCodeComp揭示了深度搜索代理在分层规则应用方面的巨大挑战，现有代理能力严重不足，需要开发更强大的规则理解和应用能力。

Abstract: Effective deep search agents must not only access open-domain and
domain-specific knowledge but also apply complex rules-such as legal clauses,
medical manuals and tariff rules. These rules often feature vague boundaries
and implicit logic relationships, making precise application challenging for
agents. However, this critical capability is largely overlooked by current
agent benchmarks.
  To fill this gap, we introduce HSCodeComp, the first realistic, expert-level
e-commerce benchmark designed to evaluate deep search agents in hierarchical
rule application. In this task, the deep reasoning process of agents is guided
by these rules to predict 10-digit Harmonized System Code (HSCode) of products
with noisy but realistic descriptions. These codes, established by the World
Customs Organization, are vital for global supply chain efficiency. Built from
real-world data collected from large-scale e-commerce platforms, our proposed
HSCodeComp comprises 632 product entries spanning diverse product categories,
with these HSCodes annotated by several human experts.
  Extensive experimental results on several state-of-the-art LLMs, open-source,
and closed-source agents reveal a huge performance gap: best agent achieves
only 46.8% 10-digit accuracy, far below human experts at 95.0%. Besides,
detailed analysis demonstrates the challenges of hierarchical rule application,
and test-time scaling fails to improve performance further.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [97] [OpenGuardrails: An Open-Source Context-Aware AI Guardrails Platform](https://arxiv.org/abs/2510.19169)
*Thomas Wang,Haowen Li*

Main category: cs.CR

TL;DR: OpenGuardrails是一个开源项目，提供上下文感知的安全性和操作检测模型，以及可部署的AI防护平台，保护内容安全、防止模型操作攻击和数据泄露。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在现实应用中的集成度提高，保护它们免受不安全、恶意或侵犯隐私的内容变得至关重要。

Method: 使用统一的大模型实现内容安全和模型操作检测，使用轻量级NER管道（如Presidio风格模型或基于正则表达式的检测器）进行数据泄露识别和编辑。系统可作为安全网关或基于API的服务部署。

Result: 在安全基准测试中达到最先进性能，在英语、中文和多语言任务的提示和响应分类方面表现出色。

Conclusion: OpenGuardrails提供了企业级、完全私有的部署选项，所有模型都基于Apache 2.0许可证公开发布。

Abstract: As large language models (LLMs) become increasingly integrated into
real-world applications, safeguarding them against unsafe, malicious, or
privacy-violating content is critically important. We present OpenGuardrails,
the first open-source project to provide both a context-aware safety and
manipulation detection model and a deployable platform for comprehensive AI
guardrails. OpenGuardrails protects against content-safety risks,
model-manipulation attacks (e.g., prompt injection, jailbreaking,
code-interpreter abuse, and the generation/execution of malicious code), and
data leakage. Content-safety and model-manipulation detection are implemented
by a unified large model, while data-leakage identification and redaction are
performed by a separate lightweight NER pipeline (e.g., Presidio-style models
or regex-based detectors). The system can be deployed as a security gateway or
an API-based service, with enterprise-grade, fully private deployment options.
OpenGuardrails achieves state-of-the-art (SOTA) performance on safety
benchmarks, excelling in both prompt and response classification across
English, Chinese, and multilingual tasks. All models are released under the
Apache 2.0 license for public use.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [98] [Aligning Multilingual News for Stock Return Prediction](https://arxiv.org/abs/2510.19203)
*Yuntao Wu,Lynn Tao,Ing-Haw Cheng,Charles Martineau,Yoshio Nozawa,John Hull,Andreas Veneris*

Main category: q-fin.CP

TL;DR: 提出使用最优传输方法对齐多语言新闻文章中的句子，识别跨语言的语义相似内容，应用于超过14万对彭博英文和日文新闻文章，构建的回报分数与股票实际回报相关性更强，基于对齐的做多-做空交易策略夏普比率比全文本分析高10%。


<details>
  <summary>Details</summary>
Motivation: 新闻在不同语言和地区间快速传播，但翻译可能会丢失细微差别，需要一种方法来准确对齐多语言新闻中的语义相似内容。

Method: 使用最优传输方法对齐多语言新闻文章中的句子，识别跨语言的语义相似内容，应用于彭博英文和日文新闻文章。

Result: 对齐的句子更稀疏、更可解释，语义相似度更高。基于对齐句子构建的回报分数与股票实际回报相关性更强，做多-做空交易策略的夏普比率比全文本分析高10%。

Conclusion: 最优传输方法能有效对齐多语言新闻中的语义内容，提升金融文本分析的性能，在股票交易策略中表现出更好的风险调整后收益。

Abstract: News spreads rapidly across languages and regions, but translations may lose
subtle nuances. We propose a method to align sentences in multilingual news
articles using optimal transport, identifying semantically similar content
across languages. We apply this method to align more than 140,000 pairs of
Bloomberg English and Japanese news articles covering around 3500 stocks in
Tokyo exchange over 2012-2024. Aligned sentences are sparser, more
interpretable, and exhibit higher semantic similarity. Return scores
constructed from aligned sentences show stronger correlations with realized
stock returns, and long-short trading strategies based on these alignments
achieve 10\% higher Sharpe ratios than analyzing the full text sample.

</details>
