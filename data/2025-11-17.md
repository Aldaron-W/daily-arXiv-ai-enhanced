<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 73]
- [cs.AI](#cs.AI) [Total: 4]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.SE](#cs.SE) [Total: 1]
- [cs.CR](#cs.CR) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.LG](#cs.LG) [Total: 3]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.CV](#cs.CV) [Total: 4]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Unsupervised Cycle Detection in Agentic Applications](https://arxiv.org/abs/2511.10650)
*Felix George,Harshit Kumar,Divya Pathak,Kaustabha Ray,Mudit Verma,Pratibha Moogi*

Main category: cs.CL

TL;DR: 提出了一种结合结构和语义分析的混合方法，用于检测基于大语言模型的智能体应用中的隐藏执行循环，相比单一方法显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型的智能体应用存在非确定性行为，可能形成隐藏的执行循环，默默消耗资源而不触发显式错误，传统可观测性平台无法检测这些代价高昂的低效问题。

Method: 采用无监督循环检测框架，结合结构分析和语义分析：首先应用计算高效的时间调用栈分析识别显式循环，然后利用语义相似性分析发现由冗余内容生成表征的微妙循环。

Result: 在基于LangGraph的股票市场应用的1575个轨迹上评估，混合方法F1得分为0.72（精确率0.62，召回率0.86），显著优于单独的结构方法（F1: 0.08）和语义方法（F1: 0.28）。

Conclusion: 虽然结果令人鼓舞，但仍有很大改进空间，未来工作需要完善该方法并解决当前局限性。

Abstract: Agentic applications powered by Large Language Models exhibit non-deterministic behaviors that can form hidden execution cycles, silently consuming resources without triggering explicit errors. Traditional observability platforms fail to detect these costly inefficiencies. We present an unsupervised cycle detection framework that combines structural and semantic analysis. Our approach first applies computationally efficient temporal call stack analysis to identify explicit loops and then leverages semantic similarity analysis to uncover subtle cycles characterized by redundant content generation. Evaluated on 1575 trajectories from a LangGraph-based stock market application, our hybrid approach achieves an F1 score of 0.72 (precision: 0.62, recall: 0.86), significantly outperforming individual structural (F1: 0.08) and semantic methods (F1: 0.28). While these results are encouraging, there remains substantial scope for improvement, and future work is needed to refine the approach and address its current limitations.

</details>


### [2] [Data Analysis and Performance Evaluation of Simulation Deduction Based on LLMs](https://arxiv.org/abs/2511.10651)
*Shansi Zhang,Min Li*

Main category: cs.CL

TL;DR: 提出了一种基于大语言模型的多轮交互方法，用于自动生成高质量的军事模拟推演数据分析报告，通过任务分解、提示工程和自定义工具来提高分析效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的手工分析方法耗时且容易出错，而单次指令输入无法从大语言模型获得结构化的高质量分析报告，需要更系统的方法来提升军事模拟推演数据分析的效率和质量。

Method: 将复杂任务分解为子任务，为每个子任务设计系统提示和用户提示，通过多轮交互结合自检和反思机制，使用自定义工具生成图表和计算指标，并设计多种报告模板以适应不同场景。

Result: 评估结果表明，该方法生成的报告质量更高，得分显著优于基线方法。

Conclusion: 该方法能够有效利用大语言模型的分析推理能力，通过结构化的工作流程生成高质量的军事模拟推演分析报告，具有实际应用价值。

Abstract: Data analysis and performance evaluation of simulation deduction plays a pivotal role in modern warfare, which enables military personnel to gain invaluable insights into the potential effectiveness of different strategies, tactics, and operational plans. Traditional manual analysis approach is time-consuming and limited by human errors. To enhance efficiency and accuracy, large language models (LLMs) with strong analytical and inferencing capabilities can be employed. However, high-quality analysis reports with well-structured formatting cannot be obtained through a single instruction input to the LLM. To tackle this issue, we propose a method that first decomposes the complex task into several sub-tasks and designs effective system prompts and user prompts for each sub-task. Multi-round interactions with the LLM incorporating self-check and reflection are then conducted to enable structured data extraction as well as multi-step analysis and evaluation. Furthermore, custom tools are defined and invoked to generate figures and compute metrics. We also design multiple report templates, each tailored to a specific application and input data type, ensuring their adaptability across a variety of scenarios. Extensive evaluation results demonstrate that the reports generated by our method exhibit higher quality, therefore obtaining higher scores than the baseline method.

</details>


### [3] [Cognitively-Inspired Episodic Memory Architectures for Accurate and Efficient Character AI](https://arxiv.org/abs/2511.10652)
*Rafael Arias Gonzalez,Steve DiPaola*

Main category: cs.CL

TL;DR: 提出了一个结合离线数据增强和并行检索的架构，用于实现历史人物对话系统，在保持深度响应的同时显著降低延迟


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在历史人物对话系统中的关键权衡问题：简单检索增强生成产生浅层响应，而多阶段反思方法虽然深度足够但延迟过高

Method: 通过离线数据增强将传记数据转化为1774个增强的第一人称记忆，并采用带有情感语义元数据的结构化情景记忆进行两阶段并行检索

Result: 在GPT-4上与传统RAG方法表现相当，但在GPT-3.5和GPT-3等较小模型上显著优于传统方法，提示生成时间仅需0.52秒

Conclusion: 该架构为需要准确性和效率的教育、博物馆和研究应用提供了一个实用框架，并可作为对话界面和传记分析研究工具

Abstract: Large language models show promise for embodying historical characters in dialogue systems, but existing approaches face a critical trade-off: simple retrieval-augmented generation produces shallow responses, while multi-stage reflection achieves depth at prohibitive latency. We present an architecture that resolves this tension through offline data augmentation and efficient parallel retrieval from structured episodic memory. Our system transforms biographical data into 1,774 enriched first-person memories with affective-semantic metadata, then employs two-stage retrieval achieving 0.52s prompt generation. Evaluation using LLM-as-judge and RAGAs metrics shows our approach achieves parity with traditional RAG on GPT-4 while significantly outperforming it on smaller models (GPT-3.5, GPT-3), suggesting particular value for resource-constrained deployments. Beyond dialogue, the structured memory enables novel visualization tools: spatiotemporal heatmaps, emotional trajectory analysis, and interactive path tracking, positioning the system as both a dialogue interface and research tool for biographical analysis. We use Van Gogh as a test case, but the architecture is generalizable to any historical figure with substantial textual records, offering a practical framework for educational, museum, and research applications requiring both accuracy and efficiency

</details>


### [4] [Hybrid Quantum Transformer for Language Generation](https://arxiv.org/abs/2511.10653)
*Desheng Kong,Xiangshuo Cui,Jiaying Jin,Jing Xu,Donglin Wang*

Main category: cs.CL

TL;DR: 提出了第一个用于自然语言生成的混合量子-经典大语言模型HyQuT，能够在8M和150M参数规模下将变分量子电路集成到Transformer框架中，用10个量子比特和80个量子门替换约10%的经典参数，实现可比较的收敛稳定性和生成质量。


<details>
  <summary>Details</summary>
Motivation: 尽管量子计算已逐渐应用于替代经典计算，但大多数现有量子或混合模型仍局限于简单任务，尚未成功应用于大规模自然语言生成。

Method: 将变分量子电路(VQCs)集成到Transformer框架中，在8M和150M参数规模下构建混合量子-经典大语言模型。

Result: 实验结果显示，仅需10个量子比特和80个量子门就能在150M参数模型中替换约10%的经典参数，同时保持可比较的收敛稳定性和生成质量。

Conclusion: 这项研究为将量子计算集成到大规模生成语言模型中提供了早期可行性证明。

Abstract: Although quantum computing has been increasingly applied to replace classical computation, most existing quantum or hybrid models remain confined to simple tasks, with no successful application to large-scale natural language generation to date. In this work, we present the first hybrid quantum-classical large language model (LLM) for natural language generation, HyQuT, capable of performing coherent and context-aware dialogue. The proposed architecture integrates variational quantum circuits (VQCs) into the Transformer framework at both 8M and 150M parameter scales. Experimental results show that a minimal number of qubits (10 qubits with 80 quantum gates) can replace about 10% of the classical parameters in the 150M-parameter model, while achieving comparable convergence stability and generation quality. This study provides an early demonstration of the feasibility of integrating quantum computing to large-scale generative language models.

</details>


### [5] [Empirical Characterization of Temporal Constraint Processing in LLMs](https://arxiv.org/abs/2511.10654)
*Javier Marín*

Main category: cs.CL

TL;DR: LLMs在时间约束处理方面存在系统性风险：性能呈现双峰分布（95%或50%准确率）、极度提示脆弱性、系统性行动偏见，且参数数量与能力无关。即使经过微调，也无法可靠学习时间约束满足能力。


<details>
  <summary>Details</summary>
Motivation: 测试LLM在需要实时决策的智能体架构中，是否能可靠判断行动窗口是否开放或已关闭的时间约束处理能力。

Method: 使用截止时间检测任务评估8个生产级模型（2.8-80亿参数），分析其时间约束处理能力，并对部分模型进行200个合成示例的微调。

Result: 发现系统性部署风险：双峰性能分布、极端提示脆弱性（仅格式变化导致30-60个百分点波动）、系统性行动偏见（失败模型100%假阳性率）。参数数量与能力无相关性。微调仅能部分改善能力。

Conclusion: 时间约束满足能力无法通过自然语言的下一个词预测可靠学习，需要专门的架构机制：连续时间状态表示、显式约束检查、系统性组合推理。当前自回归架构缺乏这些机制，在时间关键应用中部署存在不可接受风险。

Abstract: When deploying LLMs in agentic architectures requiring real-time decisions under temporal constraints, we assume they reliably determine whether action windows remain open or have closed. This assumption is untested. We characterize temporal constraint processing across eight production-scale models (2.8-8B parameters) using deadline detection tasks, revealing systematic deployment risks: bimodal performance distribution (models achieve either 95% or 50% accuracy), extreme prompt brittleness (30-60 percentage point swings from formatting changes alone), and systematic action bias (100% false positive rates in failing models). Parameter count shows no correlation with capability in this range-a 3.8B model matches 7B models while other 7B models fail completely. Fine-tuning on 200 synthetic examples improves models with partial capability by 12-37 percentage points. We demonstrate that temporal constraint satisfaction cannot be reliably learned through next-token prediction on natural language, even with targeted fine-tuning. This capability requires architectural mechanisms for: (1) continuous temporal state representation, (2) explicit constraint checking separate from linguistic pattern matching, (3) systematic compositional reasoning over temporal relations. Current autoregressive architectures lack these mechanisms. Deploying such systems in time-critical applications without hybrid architectures incorporating symbolic reasoning modules represents unacceptable risk.

</details>


### [6] [Spectral Neuro-Symbolic Reasoning II: Semantic Node Merging, Entailment Filtering, and Knowledge Graph Alignment](https://arxiv.org/abs/2511.10655)
*Andrew Kiruluta,Priscilla Burity*

Main category: cs.CL

TL;DR: 扩展Spectral NSR框架，通过三种语义增强方法：基于transformer的节点合并、句子级蕴涵验证和外部知识图谱对齐，在不改变核心谱推理引擎的情况下提升图质量和推理性能。


<details>
  <summary>Details</summary>
Motivation: 提升谱神经符号推理框架的图保真度，减少冗余节点和噪声边，增强对对抗性案例的泛化能力，同时保持推理的高效性和可解释性。

Method: 1) 使用Sentence-BERT、SimCSE等上下文嵌入进行transformer节点合并；2) 使用RoBERTa、DeBERTa等预训练NLI分类器进行句子级蕴涵验证；3) 与ConceptNet、Wikidata等外部知识图谱对齐以补充缺失上下文。

Result: 在ProofWriter、EntailmentBank和CLUTRR基准测试中，准确率提升达+3.8%，改善了对抗性案例的泛化能力，减少了推理噪声。

Conclusion: 通过在谱推理阶段前进行语义和符号精化，实现了高效、可解释且可扩展的推理系统，适用于开放域和现实世界部署。

Abstract: This report extends the Spectral Neuro-Symbolic Reasoning (Spectral NSR) framework by introducing three semantically grounded enhancements: (1) transformer-based node merging using contextual embeddings (e.g., Sentence-BERT, SimCSE) to reduce redundancy, (2) sentence-level entailment validation with pretrained NLI classifiers (e.g., RoBERTa, DeBERTa) to improve edge quality, and (3) alignment with external knowledge graphs (e.g., ConceptNet, Wikidata) to augment missing context. These modifications enhance graph fidelity while preserving the core spectral reasoning pipeline. Experimental results on ProofWriter, EntailmentBank, and CLUTRR benchmarks show consistent accuracy gains (up to +3.8\%), improved generalization to adversarial cases, and reduced inference noise. The novelty lies in performing semantic and symbolic refinement entirely upstream of the spectral inference stage, enabling efficient, interpretable, and scalable reasoning without relying on quadratic attention mechanisms. In summary, this work extends the Spectral NSR framework with modular, semantically grounded preprocessing steps that improve graph quality without altering the core spectral reasoning engine. The result is a more robust, interpretable, and scalable reasoning system suitable for deployment in open-domain and real-world settings.

</details>


### [7] [Preference Orchestrator: Prompt-Aware Multi-Objective Alignment for Large Language Models](https://arxiv.org/abs/2511.10656)
*Biao Liu,Ning Xu,Junming Yang,Xin Geng*

Main category: cs.CL

TL;DR: 提出了PRO框架，通过轻量级偏好适配器自动推断特定提示的偏好权重，解决多目标对齐中手动指定权重的问题。


<details>
  <summary>Details</summary>
Motivation: 现有多目标对齐方法依赖手动指定偏好权重，不仅增加用户负担，还因探索无关偏好组合导致训练效率低下。

Method: 使用轻量级偏好适配器，通过训练多个奖励模型对优选响应的归一化奖励分数，自动学习每个提示的适当偏好权重。

Result: 理论分析证明该方法在多目标对齐场景中优于固定偏好权重，多个任务的广泛实验验证了其有效性。

Conclusion: PRO框架能够自动推断提示特定偏好权重，在多目标对齐中实现更优性能和训练效率。

Abstract: While Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse natural language processing tasks, aligning these models with varying human preferences across multiple objectives remains a significant challenge in practical deployments. Existing multi-objective alignment methods rely on manually specified preference weights, which not only burden users with difficult preference specification tasks but also lead to suboptimal training efficiency due to exploration of irrelevant preference combinations. To alleviate these issues, we propose a novel framework named PRO, i.e., PReference Orchestrator, which features a lightweight preference adapter that automatically infers prompt-specific preference weights during both training and deployment phases. Specifically, the adapter automatically learns appropriate preference weights for each prompt by training on normalized reward scores from multiple reward models for preferred responses, which inherently reflect effective preference balances across objectives. Additionally, We provide theoretical analysis proving that our prompt-aware preference mechanism achieves superior performance compared to fixed preference weights in multi-objective alignment scenarios. Extensive experiments across multiple tasks demonstrate the effectiveness of our method over existing multi-objective alignment approaches.

</details>


### [8] [Patent Representation Learning via Self-supervision](https://arxiv.org/abs/2511.10657)
*You Zuo,Kim Gerdes,Eric Villemonte de La Clergerie,Benoît Sagot*

Main category: cs.CL

TL;DR: 提出了一种基于专利文档内部多视图的对比学习框架，通过利用专利不同章节（摘要、权利要求、背景等）作为互补视图，解决了SimCSE风格dropout增强在专利嵌入学习中导致语义凝聚力丧失的问题。


<details>
  <summary>Details</summary>
Motivation: 发现SimCSE风格的dropout增强在专利嵌入学习中会产生过度均匀的嵌入，丧失语义凝聚力。专利文档固有的结构化特性（不同章节包含互补信息）为解决这一问题提供了天然的多视图来源。

Method: 提出基于章节的增强方法，将专利文档的不同部分（摘要、权利要求、背景等）作为对比学习的互补视图，引入自然的语义和结构多样性。采用完全自监督的对比学习框架。

Result: 在大规模基准测试中，该方法在现有技术检索和分类任务上匹配或超越了基于引用和IPC监督的基线方法，同时避免了依赖脆弱或不完整的标注。分析显示不同章节对不同任务有专门优势：权利要求和摘要有利于检索，背景部分有助于分类。

Conclusion: 利用专利文档内部的多视图进行表示学习具有重要价值，能够实现可扩展和泛化性强的专利理解，同时避免对标注数据的依赖。

Abstract: This paper presents a simple yet effective contrastive learning framework for learning patent embeddings by leveraging multiple views from within the same document. We first identify a patent-specific failure mode of SimCSE style dropout augmentation: it produces overly uniform embeddings that lose semantic cohesion. To remedy this, we propose section-based augmentation, where different sections of a patent (e.g., abstract, claims, background) serve as complementary views. This design introduces natural semantic and structural diversity, mitigating over-dispersion and yielding embeddings that better preserve both global structure and local continuity. On large-scale benchmarks, our fully self-supervised method matches or surpasses citation-and IPC-supervised baselines in prior-art retrieval and classification, while avoiding reliance on brittle or incomplete annotations. Our analysis further shows that different sections specialize for different tasks-claims and summaries benefit retrieval, while background sections aid classification-highlighting the value of patents' inherent discourse structure for representation learning. These results highlight the value of exploiting intra-document views for scalable and generalizable patent understanding.

</details>


### [9] [Evaluating Open-Weight Large Language Models for Structured Data Extraction from Narrative Medical Reports Across Multiple Use Cases and Languages](https://arxiv.org/abs/2511.10658)
*Douwe J. Spaanderman,Karthik Prathaban,Petr Zelina,Kaouther Mouheb,Lukáš Hejtmánek,Matthew Marzetti,Antonius W. Schurink,Damian Chan,Ruben Niemantsverdriet,Frederik Hartmann,Zhen Qian,Maarten G. J. Thomeer,Petr Holub,Farhan Akram,Frank J. Wolters,Meike W. Vernooij,Cornelis Verhoef,Esther E. Bron,Vít Nováček,Dirk J. Grünhagen,Wiro J. Niessen,Martijn P. A. Starmans,Stefan Klein*

Main category: cs.CL

TL;DR: 评估15个开源大语言模型在荷兰、英国和捷克三个机构的病理学和放射学报告中提取结构化信息的性能，涵盖六种疾病用例，比较不同提示策略的效果。


<details>
  <summary>Details</summary>
Motivation: 现有研究多集中于单一任务、有限模型和英文报告，需要评估开源LLM在多疾病、多语言、多机构临床记录中的结构化信息提取能力。

Method: 使用15个不同规模的开源LLM，在六个疾病用例上测试六种提示策略（零样本、一样本、少样本、思维链、自一致性和提示图），采用任务特定指标和线性混合效应模型进行性能评估。

Result: 表现最佳的模型在各项任务中接近人工标注者间一致性；中小型通用模型性能与大型模型相当；提示图和少样本提示可提升约13%性能；任务特定因素比模型规模或提示策略影响更大。

Conclusion: 开源LLM能够跨疾病、语言和机构从临床报告中提取结构化数据，为临床数据管理提供了可扩展的方法。

Abstract: Large language models (LLMs) are increasingly used to extract structured information from free-text clinical records, but prior work often focuses on single tasks, limited models, and English-language reports. We evaluated 15 open-weight LLMs on pathology and radiology reports across six use cases, colorectal liver metastases, liver tumours, neurodegenerative diseases, soft-tissue tumours, melanomas, and sarcomas, at three institutes in the Netherlands, UK, and Czech Republic. Models included general-purpose and medical-specialised LLMs of various sizes, and six prompting strategies were compared: zero-shot, one-shot, few-shot, chain-of-thought, self-consistency, and prompt graph. Performance was assessed using task-appropriate metrics, with consensus rank aggregation and linear mixed-effects models quantifying variance. Top-ranked models achieved macro-average scores close to inter-rater agreement across tasks. Small-to-medium general-purpose models performed comparably to large models, while tiny and specialised models performed worse. Prompt graph and few-shot prompting improved performance by ~13%. Task-specific factors, including variable complexity and annotation variability, influenced results more than model size or prompting strategy. These findings show that open-weight LLMs can extract structured data from clinical reports across diseases, languages, and institutions, offering a scalable approach for clinical data curation.

</details>


### [10] [Information Extraction From Fiscal Documents Using LLMs](https://arxiv.org/abs/2511.10659)
*Vikram Aggarwal,Jay Kulkarni,Aditi Mascarenhas,Aakriti Narang,Siddarth Raman,Ajay Shah,Susan Thomas*

Main category: cs.CL

TL;DR: 使用LLM从多页政府财政文件中提取结构化数据的新方法，通过多阶段流程结合领域知识、序列上下文和算法验证，在印度卡纳塔克邦年度财政文件上实现高精度提取。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在文本理解方面表现出色，但处理复杂的分层表格数据能力尚未充分探索，特别是政府财政文档这类多页、结构化数据的准确提取。

Method: 采用多阶段流水线方法，利用领域知识、序列上下文和算法验证，通过财政表格固有的层次结构（每级都有总计）进行鲁棒的内部数据验证。

Result: 在200多页的卡纳塔克邦年度财政文件上实现高精度数据提取，证明LLM不仅能读取表格，还能处理文档特定的结构层次。

Conclusion: 该方法为将PDF格式的财政披露转换为研究就绪数据库提供了可扩展流程，在发展中国家背景下具有广泛应用前景。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in text comprehension, but their ability to process complex, hierarchical tabular data remains underexplored. We present a novel approach to extracting structured data from multi-page government fiscal documents using LLM-based techniques. Applied to annual fiscal documents from the State of Karnataka in India (200+ pages), our method achieves high accuracy through a multi-stage pipeline that leverages domain knowledge, sequential context, and algorithmic validation. A large challenge with traditional OCR methods is the inability to verify the accurate extraction of numbers. When applied to fiscal data, the inherent structure of fiscal tables, with totals at each level of the hierarchy, allows for robust internal validation of the extracted data. We use these hierarchical relationships to create multi-level validation checks. We demonstrate that LLMs can read tables and also process document-specific structural hierarchies, offering a scalable process for converting PDF-based fiscal disclosures into research-ready databases. Our implementation shows promise for broader applications across developing country contexts.

</details>


### [11] [Test-Time Steering for Lossless Text Compression via Weighted Product of Experts](https://arxiv.org/abs/2511.10660)
*Qihang Zhang,Muchen Li,Ziao Wang,Renjie Liao,Lele Wang*

Main category: cs.CL

TL;DR: 提出一种通过加权专家乘积（wPoE）进行测试时引导的新框架，将通用压缩模型与预训练神经语言模型自适应结合，确保压缩率至少与最佳单个模型相当，无需微调即可提升文本压缩性能。


<details>
  <summary>Details</summary>
Motivation: 传统通用压缩器计算开销低、速度快、适用性广但压缩率较差，神经压缩器能更好建模数据分布但泛化能力不足，需要一种能结合两者优势的方法。

Method: 使用加权专家乘积（wPoE）框架，在推理时自适应地结合通用压缩模型和预训练神经语言模型，确保压缩率不低于最佳单个模型。

Result: 大量实验表明该方法无需微调即可提升文本压缩性能，并能与任何自回归语言模型无缝集成。

Conclusion: 该框架为增强跨不同数据分布的文本压缩提供了实用解决方案，成功结合了传统压缩器和神经压缩器的优势。

Abstract: Lossless compression techniques are crucial in an era of rapidly growing data. Traditional universal compressors like gzip offer low computational overhead, high speed, and broad applicability across data distributions. However, they often lead to worse compression rates than modern neural compressors, which leverage large-scale training data to model data distributions more effectively. Despite their advantages, neural compressors struggle to generalize to unseen data. To address this limitation, we propose a novel framework that performs Test-Time Steering via a Weighted Product of Experts (wPoE). At inference, our method adaptively combines a universal compression model with a pretrained neural language model, ensuring the compression rate is at least as good as that of the best individual model. Extensive experiments demonstrate that our approach improves the performance of text compression without requiring fine-tuning. Furthermore, it seamlessly integrates with any autoregressive language model, providing a practical solution for enhancing text compression across diverse data distributions.

</details>


### [12] [Bayesian Evaluation of Large Language Model Behavior](https://arxiv.org/abs/2511.10661)
*Rachel Longjohn,Shang Wu,Saatvik Kher,Catarina Belém,Padhraic Smyth*

Main category: cs.CL

TL;DR: 本文提出了一种贝叶斯方法来量化基于大语言模型的文本生成系统在二元评估指标中的统计不确定性，特别关注由概率性文本生成策略引起的不确定性。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型评估方法往往忽视统计不确定性量化，而概率性文本生成策略会引入额外的不确定性，需要更严谨的统计方法来评估模型行为。

Method: 采用贝叶斯方法对二元评估指标进行不确定性量化，通过两个案例研究验证：1)评估对抗性输入下的拒绝率，2)评估LLM在开放对话中的成对偏好。

Result: 贝叶斯方法能够为LLM系统行为提供有用的不确定性量化，在拒绝率评估和模型偏好比较中都能有效捕捉统计不确定性。

Conclusion: 贝叶斯不确定性量化方法能够增强大语言模型评估的统计严谨性，为模型行为分析提供更可靠的统计基础。

Abstract: It is increasingly important to evaluate how text generation systems based on large language models (LLMs) behave, such as their tendency to produce harmful output or their sensitivity to adversarial inputs. Such evaluations often rely on a curated benchmark set of input prompts provided to the LLM, where the output for each prompt may be assessed in a binary fashion (e.g., harmful/non-harmful or does not leak/leaks sensitive information), and the aggregation of binary scores is used to evaluate the LLM. However, existing approaches to evaluation often neglect statistical uncertainty quantification. With an applied statistics audience in mind, we provide background on LLM text generation and evaluation, and then describe a Bayesian approach for quantifying uncertainty in binary evaluation metrics. We focus in particular on uncertainty that is induced by the probabilistic text generation strategies typically deployed in LLM-based systems. We present two case studies applying this approach: 1) evaluating refusal rates on a benchmark of adversarial inputs designed to elicit harmful responses, and 2) evaluating pairwise preferences of one LLM over another on a benchmark of open-ended interactive dialogue examples. We demonstrate how the Bayesian approach can provide useful uncertainty quantification about the behavior of LLM-based systems.

</details>


### [13] [Evaluating Modern Large Language Models on Low-Resource and Morphologically Rich Languages:A Cross-Lingual Benchmark Across Cantonese, Japanese, and Turkish](https://arxiv.org/abs/2511.10664)
*Chengxuan Xia,Qianye Wu,Hongbin Guan,Sixuan Tian,Yilun Hao,Xiaoyu Wu*

Main category: cs.CL

TL;DR: 评估7个前沿大语言模型在粤语、日语和土耳其语上的表现，结合人工评估和自动指标，发现专有模型表现较好但仍存在文化理解和形态学泛化差距，开源模型明显落后。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在低资源、形态丰富语言中的有效性，这些语言的研究相对不足。

Method: 创建跨语言基准测试，涵盖粤语、日语和土耳其语，包含四个任务：开放域问答、文档摘要、英译X和文化对话，结合人工评估（流畅度、事实准确性、文化适当性）和自动指标（BLEU、ROUGE）。

Result: 最大的专有模型（GPT-4o、GPT-4、Claude~3.5）在语言和任务上普遍领先，但在文化细微理解和形态学泛化方面仍存在显著差距；开源模型在流畅度和准确性上大幅落后。

Conclusion: 所有模型都在某种程度上难以应对每种语言的独特语言挑战，需要开发更具文化意识和语言泛化能力的大语言模型。

Abstract: Large language models (LLMs) have achieved impressive results in high-resource languages like English, yet their effectiveness in low-resource and morphologically rich languages remains underexplored. In this paper, we present a comprehensive evaluation of seven cutting-edge LLMs -- including GPT-4o, GPT-4, Claude~3.5~Sonnet, LLaMA~3.1, Mistral~Large~2, LLaMA-2~Chat~13B, and Mistral~7B~Instruct -- on a new cross-lingual benchmark covering \textbf{Cantonese, Japanese, and Turkish}. Our benchmark spans four diverse tasks: open-domain question answering, document summarization, English-to-X translation, and culturally grounded dialogue. We combine \textbf{human evaluations} (rating fluency, factual accuracy, and cultural appropriateness) with automated metrics (e.g., BLEU, ROUGE) to assess model performance.
  Our results reveal that while the largest proprietary models (GPT-4o, GPT-4, Claude~3.5) generally lead across languages and tasks, significant gaps persist in culturally nuanced understanding and morphological generalization. Notably, GPT-4o demonstrates robust multilingual performance even on cross-lingual tasks, and Claude~3.5~Sonnet achieves competitive accuracy on knowledge and reasoning benchmarks. However, all models struggle to some extent with the unique linguistic challenges of each language, such as Turkish agglutinative morphology and Cantonese colloquialisms. Smaller open-source models (LLaMA-2~13B, Mistral~7B) lag substantially in fluency and accuracy, highlighting the resource disparity. We provide detailed quantitative results, qualitative error analysis, and discuss implications for developing more culturally aware and linguistically generalizable LLMs. Our benchmark and evaluation data are released to foster reproducibility and further research.

</details>


### [14] [Guarding the Meaning: Self-Supervised Training for Semantic Robustness in Guard Models](https://arxiv.org/abs/2511.10665)
*Cristina Pinneri,Christos Louizos*

Main category: cs.CL

TL;DR: 提出了一种自监督框架，通过使用偏斜感知聚合策略来增强防护模型的语义鲁棒性，减少语义变异性并提高校准度。


<details>
  <summary>Details</summary>
Motivation: 防护模型对表面语言变化的敏感性是其关键弱点，即使意义保持不变的改写也会导致安全评分大幅波动，表明缺乏语义基础。

Method: 利用改写集通过新颖的偏斜感知聚合策略来强制预测一致性，用于稳健目标计算。

Result: 方法将语义变异性降低了约58%，基准准确率平均提高了约2.5%，并将模型校准度提高了40%。

Conclusion: 将语义一致性作为首要训练目标的价值，并提供了构建更可靠防护模型的可扩展方案。

Abstract: Guard models are a critical component of LLM safety, but their sensitivity to superficial linguistic variations remains a key vulnerability. We show that even meaning-preserving paraphrases can cause large fluctuations in safety scores, revealing a lack of semantic grounding. To address this, we introduce a practical, self-supervised framework for improving the semantic robustness of guard models. Our method leverages paraphrase sets to enforce prediction consistency using a novel, skew-aware aggregation strategy for robust target computation. Notably, we find that standard aggregation methods like mean and median can degrade safety, underscoring the need for skew-aware alternatives. We analyze six open-source guard models and show that our approach reduces semantic variability across paraphrases by ~58%, improves benchmark accuracy by ~2.5% on average, and generalizes to unseen stylistic variations. Intriguingly, we discover a bidirectional relationship between model calibration and consistency: our robustness training improves calibration by up to 40%, revealing a fundamental connection between these properties. These results highlight the value of treating semantic consistency as a first-class training objective and provide a scalable recipe for building more reliable guard models.

</details>


### [15] [Evaluating LLM Understanding via Structured Tabular Decision Simulations](https://arxiv.org/abs/2511.10667)
*Sichao Li,Xinyue Xu,Xiaomeng Li*

Main category: cs.CL

TL;DR: 提出了STaDS框架，通过结构化表格决策模拟评估LLMs的真实理解能力，发现现有模型在跨领域一致性和决策因素忠实度方面存在不足。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs虽然预测准确率高，但正确性不等于真实理解。需要评估模型是否能像人类专家一样在不同领域做出基于正确决策因素的一致决策。

Method: 引入STaDS框架，模拟专家决策考试，通过三个维度评估理解能力：(1)问题和指令理解，(2)基于知识的预测，(3)对相关决策因素的依赖。在15个不同决策场景中测试9个前沿LLM。

Result: 发现大多数模型难以在不同领域保持一致的强准确性；模型可能准确但全局不忠实，预测驱动因素与陈述理由经常不匹配。

Conclusion: 需要超越准确性的全局理解评估协议，开发新框架来提升LLMs的理解能力。

Abstract: Large language models (LLMs) often achieve impressive predictive accuracy, yet correctness alone does not imply genuine understanding. True LLM understanding, analogous to human expertise, requires making consistent, well-founded decisions across multiple instances and diverse domains, relying on relevant and domain-grounded decision factors. We introduce Structured Tabular Decision Simulations (STaDS), a suite of expert-like decision settings that evaluate LLMs as if they were professionals undertaking structured decision ``exams''. In this context, understanding is defined as the ability to identify and rely on the correct decision factors, features that determine outcomes within a domain. STaDS jointly assesses understanding through: (i) question and instruction comprehension, (ii) knowledge-based prediction, and (iii) reliance on relevant decision factors. By analyzing 9 frontier LLMs across 15 diverse decision settings, we find that (a) most models struggle to achieve consistently strong accuracy across diverse domains; (b) models can be accurate yet globally unfaithful, and there are frequent mismatches between stated rationales and factors driving predictions. Our findings highlight the need for global-level understanding evaluation protocols and advocate for novel frameworks that go beyond accuracy to enhance LLMs' understanding ability.

</details>


### [16] [Forecasting Spoken Language Development in Children with Cochlear Implants Using Preimplantation MRI](https://arxiv.org/abs/2511.10669)
*Yanlin Wang,Di Yuan,Shani Dettman,Dawn Choo,Emily Shimeng Xu,Denise Thomas,Maura E Ryan,Patrick C M Wong,Nancy M Young*

Main category: cs.CL

TL;DR: 本研究比较了传统机器学习和深度迁移学习在预测人工耳蜗植入儿童语言发展方面的表现，发现基于双线性注意力融合策略的DTL模型在准确率、敏感性和特异性方面均优于传统ML模型。


<details>
  <summary>Details</summary>
Motivation: 人工耳蜗植入能显著改善重度感音神经性听力损失儿童的语言能力，但个体间差异很大，且无法通过植入年龄或残余听力可靠预测。需要更准确的方法来预测个体儿童的语言发展结果。

Method: 使用278名植入儿童的脑神经解剖特征，比较传统机器学习和深度迁移学习算法，采用二分类模型（高语言改善者vs低语言改善者），DTL使用双线性注意力融合策略。

Result: DTL预测模型达到：准确率92.39%，敏感性91.22%，特异性93.56%，AUC为0.977。在所有结果指标上DTL均优于传统ML模型。

Conclusion: DTL通过直接捕获判别性和任务特定信息显著改进预测性能，支持开发单一DTL预测模型用于全球CI项目儿童的语言预测。

Abstract: Cochlear implants (CI) significantly improve spoken language in children with severe-to-profound sensorineural hearing loss (SNHL), yet outcomes remain more variable than in children with normal hearing. This variability cannot be reliably predicted for individual children using age at implantation or residual hearing. This study aims to compare the accuracy of traditional machine learning (ML) to deep transfer learning (DTL) algorithms to predict post-CI spoken language development of children with bilateral SNHL using a binary classification model of high versus low language improvers. A total of 278 implanted children enrolled from three centers. The accuracy, sensitivity and specificity of prediction models based upon brain neuroanatomic features using traditional ML and DTL learning. DTL prediction models using bilinear attention-based fusion strategy achieved: accuracy of 92.39% (95% CI, 90.70%-94.07%), sensitivity of 91.22% (95% CI, 89.98%-92.47%), specificity of 93.56% (95% CI, 90.91%-96.21%), and area under the curve (AUC) of 0.977 (95% CI, 0.969-0.986). DTL outperformed traditional ML models in all outcome measures. DTL was significantly improved by direct capture of discriminative and task-specific information that are advantages of representation learning enabled by this approach over ML. The results support the feasibility of a single DTL prediction model for language prediction of children served by CI programs worldwide.

</details>


### [17] [Towards Fine-Grained Code-Switch Speech Translation with Semantic Space Alignment](https://arxiv.org/abs/2511.10670)
*Yan Gao,Yazheng Yang,Zhibin Lan,Yidong Chen,Min Zhang,Daimeng Wei,Hui Huang,Jinsong Su*

Main category: cs.CL

TL;DR: 提出了一种基于混合专家(MoE)语音投影器的LLM增强方法，用于解决代码切换语音翻译中的语义建模复杂性和数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 代码切换语音翻译面临语义建模复杂和CS数据稀缺的挑战，现有方法依赖模型隐式学习语义且需要低效的人工标注。

Method: 使用MoE语音投影器，每个专家专精特定语言的语义子空间；采用多阶段训练范式，利用单语ASR和ST数据；结合语言特定损失和负载均衡损失指导专家分配；使用过渡损失平滑阶段间数据转换。

Result: 在广泛使用的数据集上的大量实验证明了该方法的有效性和通用性。

Conclusion: 所提出的方法能有效解决代码切换语音翻译中的关键挑战，为CS-ST任务提供了可行的解决方案。

Abstract: Code-switching (CS) speech translation (ST) refers to translating speech that alternates between two or more languages into a target language text, which poses significant challenges due to the complexity of semantic modeling and the scarcity of CS data. Previous studies tend to rely on the model itself to implicitly learn semantic modeling during training, and resort to inefficient and costly manual annotations for these two challenges. To mitigate these limitations, we propose enhancing Large Language Models (LLMs) with a Mixture of Experts (MoE) speech projector, where each expert specializes in the semantic subspace of a specific language, enabling fine-grained modeling of speech features. Additionally, we introduce a multi-stage training paradigm that utilizes readily available monolingual automatic speech recognition (ASR) and monolingual ST data, facilitating speech-text alignment and improving translation capabilities. During training, we leverage a combination of language-specific loss and intra-group load balancing loss to guide the MoE speech projector in efficiently allocating tokens to the appropriate experts, across expert groups and within each group, respectively. To bridge the data gap across different training stages and improve adaptation to the CS scenario, we further employ a transition loss, enabling smooth transitions of data between stages, to effectively address the scarcity of high-quality CS speech translation data. Extensive experiments on widely used datasets demonstrate the effectiveness and generality of our approach.

</details>


### [18] [Grounded Visual Factualization: Factual Anchor-Based Finetuning for Enhancing MLLM Factual Consistency](https://arxiv.org/abs/2511.10671)
*Filippo Morbiato,Luca Romano,Alessandro Persona*

Main category: cs.CL

TL;DR: 提出了GVF微调方法，通过事实锚点数据增强、事实感知指令调整和事实一致性损失函数三个核心机制，系统性地提升多模态大语言模型的视觉事实一致性，显著减少视觉幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型中的视觉幻觉问题严重损害了其可靠性，现有的微调方法改进有限，无法深入干预事实推理过程。

Method: GVF微调包含三个核心机制：事实锚点数据增强（用结构化事实锚点和反事实提示丰富训练数据）、事实感知指令调整（将事实线索嵌入显式指令）、事实一致性损失函数（专门惩罚事实不准确性）。

Result: 在LLaVA-1.5-13B模型上，GVF微调在VHTest基准测试中显著优于标准微调，在开放式问题和是/否问题格式上都表现更好。同时，在MME和POPE等多模态基准测试中保持或略微提升性能。

Conclusion: GVF方法有效缓解了视觉幻觉问题，同时不损害模型的一般理解和推理能力，为提升多模态大语言模型的可靠性提供了系统性的解决方案。

Abstract: Visual hallucination, where Multimodal Large Language Models fabricate details inconsistent with image content, critically undermines their reliability. Existing fine-tuning methods offer limited improvement, failing to deeply intervene in factual reasoning. This paper introduces Grounded Visual Factualization (GVF) Finetuning, a novel approach to systematically enhance MLLM visual factual consistency. GVF integrates explicit factual signals via three core mechanisms: Factual Anchor Data Augmentation, enriching training data with structured factual anchors and counter-factual prompts; Fact-Aware Instruction Tuning, embedding these cues into explicit instructions; and a Factual Consistency Loss function, specifically penalizing factual inaccuracies. Evaluated on LLaVA-1.5-13B, GVF Finetuning significantly outperforms standard fine-tuning on the VHTest benchmark for both Open-Ended Question (OEQ) and Yes/No Question (YNQ) formats. Crucially, GVF maintains or even slightly improves performance on general multimodal benchmarks like MME and POPE, demonstrating effective mitigation of visual hallucinations without compromising general understanding and reasoning abilities.

</details>


### [19] [Large language models in materials science and the need for open-source approaches](https://arxiv.org/abs/2511.10673)
*Fengxu Yang,Weitong Chen,Jack D. Evans*

Main category: cs.CL

TL;DR: LLMs在材料科学中的应用综述，涵盖文献挖掘、预测建模和多代理实验系统，强调开源模型的优势。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在材料科学中的快速应用，需要系统总结其在材料发现流程中的关键应用领域，并评估开源与闭源模型的性能差异。

Method: 通过综述分析LLM在三个关键领域的应用：科学文献挖掘、预测建模和多代理实验系统，并对开源和闭源模型进行基准测试比较。

Result: 基准测试结果表明开源模型可以达到与闭源模型相当的性能，同时提供更好的透明度、可重复性、成本效益和数据隐私保护。

Conclusion: 随着开源模型的持续改进，应更广泛采用开源模型来构建可访问、灵活且社区驱动的科学发现AI平台。

Abstract: Large language models (LLMs) are rapidly transforming materials science. This review examines recent LLM applications across the materials discovery pipeline, focusing on three key areas: mining scientific literature , predictive modelling, and multi-agent experimental systems. We highlight how LLMs extract valuable information such as synthesis conditions from text, learn structure-property relationships, and can coordinate agentic systems integrating computational tools and laboratory automation. While progress has been largely dependent on closed-source commercial models, our benchmark results demonstrate that open-source alternatives can match performance while offering greater transparency, reproducibility, cost-effectiveness, and data privacy. As open-source models continue to improve, we advocate their broader adoption to build accessible, flexible, and community-driven AI platforms for scientific discovery.

</details>


### [20] [Continual Learning of Domain Knowledge from Human Feedback in Text-to-SQL](https://arxiv.org/abs/2511.10674)
*Thomas Cook,Kelly Patel,Sivapriya Vellaichamy,Saba Rahimi,Zhen Zeng,Sumitra Ganesh*

Main category: cs.CL

TL;DR: 提出一个基于人类反馈的持续学习框架，用于改进文本到SQL的转换，通过存储和重用从反馈中提炼的知识来提高执行准确性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在生成SQL查询时难以处理特定数据库模式和隐含领域知识，需要一种能够从人类反馈中持续学习的方法。

Method: 设计了多种学习代理架构变体，通过结构化内存存储从人类反馈中提炼的知识，并在未来任务中检索重用这些经验。

Result: 在BIRD基准测试中，内存增强代理（特别是程序代理）通过利用人机交互反馈实现了显著的准确性提升和错误减少。

Conclusion: 将人类隐含专业知识转化为可重用知识对于构建更具适应性、领域感知的文本到SQL系统至关重要，这些系统能够从人机交互中持续学习。

Abstract: Large Language Models (LLMs) can generate SQL queries from natural language questions but struggle with database-specific schemas and tacit domain knowledge. We introduce a framework for continual learning from human feedback in text-to-SQL, where a learning agent receives natural language feedback to refine queries and distills the revealed knowledge for reuse on future tasks. This distilled knowledge is stored in a structured memory, enabling the agent to improve execution accuracy over time. We design and evaluate multiple variations of a learning agent architecture that vary in how they capture and retrieve past experiences. Experiments on the BIRD benchmark Dev set show that memory-augmented agents, particularly the Procedural Agent, achieve significant accuracy gains and error reduction by leveraging human-in-the-loop feedback. Our results highlight the importance of transforming tacit human expertise into reusable knowledge, paving the way for more adaptive, domain-aware text-to-SQL systems that continually learn from a human-in-the-loop.

</details>


### [21] [Learn to Select: Exploring Label Distribution Divergence for In-Context Demonstration Selection in Text Classification](https://arxiv.org/abs/2511.10675)
*Ye Jiang,Taihang Wang,Youzheng Liu,Yimin Wang,Yuhan Xia,Yunfei Long*

Main category: cs.CL

TL;DR: 提出了一种名为TopK + L2D的两阶段演示选择方法，通过结合语义相似性和标签分布对齐来改进上下文学习中的演示选择，在多个文本分类基准上取得更好性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大多数演示选择方法主要关注测试输入和演示之间的语义相似性，往往忽视了标签分布对齐的重要性，这限制了上下文学习的性能。

Method: 使用微调的BERT类小语言模型生成标签分布，计算测试输入和候选演示之间的标签分布差异，结合TopK语义相似性选择方法，选择既语义相似又标签分布对齐的演示。

Result: 在七个文本分类基准上的广泛实验表明，该方法持续优于先前的演示选择策略。

Conclusion: 大语言模型的性能与用于标签分布估计的小语言模型的准确性呈正相关，标签分布对齐对上下文学习性能有重要影响。

Abstract: In-context learning (ICL) for text classification, which uses a few input-label demonstrations to describe a task, has demonstrated impressive performance on large language models (LLMs). However, the selection of in-context demonstrations plays a crucial role and can significantly affect LLMs' performance. Most existing demonstration selection methods primarily focus on semantic similarity between test inputs and demonstrations, often overlooking the importance of label distribution alignment. To address this limitation, we propose a two-stage demonstration selection method, TopK + Label Distribution Divergence (L2D), which leverages a fine-tuned BERT-like small language model (SLM) to generate label distributions and calculate their divergence for both test inputs and candidate demonstrations. This enables the selection of demonstrations that are not only semantically similar but also aligned in label distribution with the test input. Extensive experiments across seven text classification benchmarks show that our method consistently outperforms previous demonstration selection strategies. Further analysis reveals a positive correlation between the performance of LLMs and the accuracy of the underlying SLMs used for label distribution estimation.

</details>


### [22] [Pre-Attention Expert Prediction and Prefetching for Mixture-of-Experts Large Language Models](https://arxiv.org/abs/2511.10676)
*Shien Zhu,Samuel Bohl,Robin Oester,Gustavo Alonso*

Main category: cs.CL

TL;DR: 提出了一种基于注意力前激活的轻量级专家预测方法，通过线性函数和排序感知损失实现准确的专家预取，显著提升了预测准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的MoE模型专家预测方法使用前一层的激活信息，准确率较低且无法优化第一层。复杂网络预测方法计算开销大，需要更高效准确的解决方案。

Method: 利用LLM中某些函数的排序保持特性，使用同一层注意力块前的激活信息，通过2个线性函数和排序感知损失进行专家预测。

Result: 在DeepSeek V2 Lite上达到93.03%准确率，Qwen3-30B上94.69%，Phi-mini-MoE上97.62%，比现有最优方法绝对准确率提升约15%。

Conclusion: 提出的预注意力专家预测方法实现了准确且轻量级的专家预取，支持第一层优化，显著提升了MoE模型的推理效率。

Abstract: Mixture-of-Experts (MoE) Large Language Models (LLMs) efficiently scale-up the model while keeping relatively low inference cost. As MoE models only activate part of the experts, related work has proposed expert prediction and caching methods to prefetch the experts for faster inference. However, existing approaches utilize the activations from the previous layer for prediction, incurring low accuracy and leave the first layer unoptimized. Applying complex layers or even training standalone networks for better prediction introduces high computation overhead. In this paper, we propose pre-attention expert prediction to achieve accurate and lightweight expert prefetching. The key insight is that some functions in LLMs are ranking-preserving, indicating that matching the ranking of selected experts using simple linear functions is possible. Therefore, we utilize the activations before the attention block in the same layer with 2 linear functions and ranking-aware loss to achieve accurate prediction, which also supports prefetching in the first layer. Our lightweight, pre-attention expert routers achieve 93.03% accuracy on DeepSeek V2 Lite, 94.69% on Qwen3-30B, and 97.62% on Phi-mini-MoE, showing about 15% improvement on absolute accuracy over the state-of-the-art methods.

</details>


### [23] [SpiderGen: Towards Procedure Generation For Carbon Life Cycle Assessments with Generative AI](https://arxiv.org/abs/2511.10684)
*Anupama Sitaraman,Bharathan Balaji,Yuvraj Agarwal*

Main category: cs.CL

TL;DR: SpiderGen是一个基于LLM的工作流，结合传统生命周期评估(LCA)的分类学和方法论与LLM的推理能力和世界知识，用于生成LCA所需的程序信息。


<details>
  <summary>Details</summary>
Motivation: 气候变化和全球变暖主要由温室气体排放引起，而消费品生产、使用和处置是主要排放源。需要开发工具来评估消费品的环境影响，其中LCA是关键部分。

Method: 开发了SpiderGen工作流，将传统LCA分类学和方法论与LLM的推理能力和世界知识相结合，生成LCA过程信息。

Result: SpiderGen能提供准确或仅有微小错误的LCA过程信息，在10个样本数据点上F1分数达到62%。相比现有LCA方法，能将成本从超过25000美元降低到不足1美元，时间从21人天缩短到10分钟内。

Conclusion: SpiderGen在减少LCA人力投入和成本方面具有巨大潜力，能够以极低的成本快速生成LCA过程信息，优于多种基线技术。

Abstract: Investigating the effects of climate change and global warming caused by GHG emissions have been a primary concern worldwide. These emissions are largely contributed to by the production, use and disposal of consumer products. Thus, it is important to build tools to estimate the environmental impact of consumer goods, an essential part of which is conducting Life Cycle Assessments (LCAs). LCAs specify and account for the appropriate processes involved with the production, use, and disposal of the products. We present SpiderGen, an LLM-based workflow which integrates the taxonomy and methodology of traditional LCA with the reasoning capabilities and world knowledge of LLMs to generate the procedural information used for LCA. We additionally evaluate the output of SpiderGen using real-world LCA documents as ground-truth. We find that SpiderGen provides accurate LCA process information that is either fully correct or has minor errors, achieving an F1-Score of 62% across 10 sample data points. We observe that the remaining missed processes and hallucinated errors occur primarily due to differences in detail between LCA documents, as well as differences in the "scope" of which auxiliary processes must also be included. We also demonstrate that SpiderGen performs better than several baselines techniques, such as chain-of-thought prompting and one-shot prompting. Finally, we highlight SpiderGen's potential to reduce the human effort and costs for estimating carbon impact, as it is able to produce LCA process information for less than \$1 USD in under 10 minutes as compared to the status quo LCA, which can cost over \$25000 USD and take up to 21-person days.

</details>


### [24] [A methodological analysis of prompt perturbations and their effect on attack success rates](https://arxiv.org/abs/2511.10686)
*Tiago Machado,Maysa Malfiza Garcia de Macedo,Rogerio Abreu de Paula,Marcelo Carpinette Grave,Aminat Adebiyi,Luan Soares de Souza,Enrico Santarelli,Claudio Pinhanez*

Main category: cs.CL

TL;DR: 本文研究了不同LLM对齐方法对提示攻击响应的影响，发现即使微小的提示修改也能显著改变攻击成功率，现有攻击基准不足以揭示所有漏洞。


<details>
  <summary>Details</summary>
Motivation: 调查不同大语言模型对齐方法如何影响模型对提示攻击的响应，评估现有攻击基准的局限性。

Method: 选择基于SFT、DPO和RLHF对齐的开源模型，使用统计方法系统分析攻击成功率对提示变体的敏感性。

Result: 即使很小的提示修改也能显著改变攻击成功率，不同对齐方法对提示变体的敏感度不同。

Conclusion: 仅运行现有攻击基准不足以揭示模型和对齐方法的所有潜在漏洞，需要基于统计的系统性分析来评估攻击敏感性。

Abstract: This work aims to investigate how different Large Language Models (LLMs) alignment methods affect the models' responses to prompt attacks. We selected open source models based on the most common alignment methods, namely, Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Reinforcement Learning with Human Feedback (RLHF). We conducted a systematic analysis using statistical methods to verify how sensitive the Attack Success Rate (ASR) is when we apply variations to prompts designed to elicit inappropriate content from LLMs. Our results show that even small prompt modifications can significantly change the Attack Success Rate (ASR) according to the statistical tests we run, making the models more or less susceptible to types of attack. Critically, our results demonstrate that running existing 'attack benchmarks' alone may not be sufficient to elicit all possible vulnerabilities of both models and alignment methods. This paper thus contributes to ongoing efforts on model attack evaluation by means of systematic and statistically-based analyses of the different alignment methods and how sensitive their ASR is to prompt variation.

</details>


### [25] [Modeling and Predicting Multi-Turn Answer Instability in Large Language Models](https://arxiv.org/abs/2511.10688)
*Jiahang He,Rishi Ramachandran,Neel Ramachandran,Aryan Katakam,Kevin Zhu,Sunishchal Dev,Ashwinee Panda,Aryan Shrivastava*

Main category: cs.CL

TL;DR: 研究发现大型语言模型在多轮对话中存在显著脆弱性，简单的重复提问会导致准确率下降约10%，可以通过马尔可夫链建模准确率动态变化，并提出稳态准确率作为交互场景下的鲁棒性指标。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在现实应用中的广泛使用，用户与模型的交互频率和规模不断增加，需要评估模型在交互场景下的鲁棒性，特别是在需要一致推理的高风险场景中。

Method: 使用简单的多轮后续提示评估模型答案变化，通过马尔可夫链建模多轮对话中的准确率动态，并检验线性探针是否能预测这些变化。

Result: 简单的"再想想"提示导致Gemini 1.5 Flash在九轮对话中准确率下降约10%；结合语义等效重述问题导致Claude 3.5 Haiku准确率下降7.5%；模型稳态准确率平均比首轮准确率低约8%；线性探针可帮助预测未来答案变化。

Conclusion: 研究确立了稳态准确率作为交互场景下的原则性鲁棒性指标，揭示了模型在重复提问下的脆弱性，解决这种不稳定性对于在高风险和交互场景中部署LLMs至关重要。

Abstract: As large language models (LLMs) are adopted in an increasingly wide range of applications, user-model interactions have grown in both frequency and scale. Consequently, research has focused on evaluating the robustness of LLMs, an essential quality for real-world tasks. In this paper, we employ simple multi-turn follow-up prompts to evaluate models' answer changes, model accuracy dynamics across turns with Markov chains, and examine whether linear probes can predict these changes. Our results show significant vulnerabilities in LLM robustness: a simple "Think again" prompt led to an approximate 10% accuracy drop for Gemini 1.5 Flash over nine turns, while combining this prompt with a semantically equivalent reworded question caused a 7.5% drop for Claude 3.5 Haiku. Additionally, we find that model accuracy across turns can be effectively modeled using Markov chains, enabling the prediction of accuracy probabilities over time. This allows for estimation of the model's stationary (long-run) accuracy, which we find to be on average approximately 8% lower than its first-turn accuracy for Gemini 1.5 Flash. Our results from a model's hidden states also reveal evidence that linear probes can help predict future answer changes. Together, these results establish stationary accuracy as a principled robustness metric for interactive settings and expose the fragility of models under repeated questioning. Addressing this instability will be essential for deploying LLMs in high-stakes and interactive settings where consistent reasoning is as important as initial accuracy.

</details>


### [26] [Equilibrium Dynamics and Mitigation of Gender Bias in Synthetically Generated Data](https://arxiv.org/abs/2511.10689)
*Ashish Kattamuri,Arpita Vats,Harshwardhan Fartale,Rahul Raja,Akshata Kishore Moharir,Ishita Prasad*

Main category: cs.CL

TL;DR: 研究递归提示下大语言模型的性别偏见动态，发现偏见呈现均衡而非单调放大，并提出对比增强方法能有效减少下游任务偏见。


<details>
  <summary>Details</summary>
Motivation: 递归提示可实现可扩展的合成数据集生成，但存在偏见放大的风险，需要研究偏见动态和缓解策略。

Method: 使用三种评估框架（规则模式匹配、嵌入语义相似性、下游任务性能），在三种初始偏见水平下测试四种缓解策略。

Result: 低初始偏见会放大至模型固有水平（+36%），高初始偏见会衰减至该水平（-26%）。对比增强方法在下游任务中显著减少偏见（平均91%）。

Conclusion: 语义相似性指标可能与行为公平结果存在分歧，需要在负责任合成数据生成中进行多维评估。

Abstract: Recursive prompting with large language models enables scalable synthetic dataset generation but introduces the risk of bias amplification. We investigate gender bias dynamics across three generations of recursive text generation using three complementary evaluation frameworks: rule-based pattern matching, embedding-based semantic similarity, and downstream task performance. Experiments with three initial bias levels (0.1, 0.3, 0.6) and four mitigation strategies reveal equilibrium dynamics rather than monotonic amplification. The low initial bias amplifies toward the model's inherent bias level (+36%), whereas the high initial bias decays toward it (-26%). Among mitigation methods, contrastive augmentation, which introduces gender-swapped variants, achieves significant downstream bias reduction (98.8% for low initial bias and 91% on average) despite producing higher embedding-based bias scores. This paradox demonstrates that semantic similarity metrics may diverge from behavioral fairness outcomes, highlighting the need for multidimensional evaluation in responsible synthetic data generation.

</details>


### [27] [Saying the Unsaid: Revealing the Hidden Language of Multimodal Systems Through Telephone Games](https://arxiv.org/abs/2511.10690)
*Juntu Zhao,Jialing Zhang,Chongxuan Li,Dequan Wang*

Main category: cs.CL

TL;DR: 通过多轮'传话游戏'利用多模态系统的偏好偏差，研究其隐藏语言中的概念连接强度，并构建概念连接全局图谱。


<details>
  <summary>Details</summary>
Motivation: 当前闭源多模态系统虽取得进展，但其黑盒架构使得理解世界的隐藏语言不透明。希望通过系统偏好偏差来研究其隐藏语言。

Method: 使用多轮'传话游戏'策略，观察概念共现频率来定量研究多模态系统理解中的概念连接强度，并贡献包含10,000+概念对的Telescope数据集。

Result: 能够识别训练继承的偏好偏差，评估泛化能力进展，发现脆弱概念连接的稳定路径，并揭示超越文本和视觉相似性的意外概念关系。

Conclusion: 为多模态系统的隐藏语言研究提供了新视角，为未来多模态系统的可解释性和可控性研究奠定了基础。

Abstract: Recent closed-source multimodal systems have made great advances, but their hidden language for understanding the world remains opaque because of their black-box architectures. In this paper, we use the systems' preference bias to study their hidden language: During the process of compressing the input images (typically containing multiple concepts) into texts and then reconstructing them into images, the systems' inherent preference bias introduces specific shifts in the outputs, disrupting the original input concept co-occurrence. We employ the multi-round "telephone game" to strategically leverage this bias. By observing the co-occurrence frequencies of concepts in telephone games, we quantitatively investigate the concept connection strength in the understanding of multimodal systems, i.e., "hidden language." We also contribute Telescope, a dataset of 10,000+ concept pairs, as the database of our telephone game framework. Our telephone game is test-time scalable: By iteratively running telephone games, we can construct a global map of concept connections in multimodal systems' understanding. Here we can identify preference bias inherited from training, assess generalization capability advancement, and discover more stable pathways for fragile concept connections. Furthermore, we use Reasoning-LLMs to uncover unexpected concept relationships that transcend textual and visual similarities, inferring how multimodal systems understand and simulate the world. This study offers a new perspective on the hidden language of multimodal systems and lays the foundation for future research on the interpretability and controllability of multimodal systems.

</details>


### [28] [Evaluating from Benign to Dynamic Adversarial: A Squid Game for Large Language Models](https://arxiv.org/abs/2511.10691)
*Zijian Chen,Wenjun Zhang,Guangtao Zhai*

Main category: cs.CL

TL;DR: Squid Game是一个动态对抗性评估环境，通过资源受限和信息不对称的交互游戏来评估大语言模型，发现静态基准测试可能存在评估范式污染问题。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试难以跟上大语言模型发展，存在数据污染问题，且主要假设良性、资源丰富的环境，缺乏对模型在压力下行为的探索。

Method: 设计了包含六个淘汰式关卡的游戏环境，评估指令遵循、代码、推理、规划和安全对齐等多方面能力，让LLM与其他LLM对手进行交互游戏。

Result: 评估了50多个LLM，观察到同一模型谱系中的代际性能转变，发现某些模型使用投机性捷径获胜，表明静态基准可能存在更高级别的评估范式污染。

Conclusion: 动态评估可以作为静态评估的补充部分，Squid Game为LLM评估提供了新的动态对抗性测试框架。

Abstract: Contemporary benchmarks are struggling to keep pace with the development of large language models (LLMs). Although they are indispensable to evaluate model performance on various tasks, it is uncertain whether the models trained on Internet data have genuinely learned how to solve problems or merely seen the questions before. This potential data contamination issue presents a fundamental challenge to establishing trustworthy evaluation frameworks. Meanwhile, existing benchmarks predominantly assume benign, resource-rich settings, leaving the behavior of LLMs under pressure unexplored. In this paper, we introduce Squid Game, a dynamic and adversarial evaluation environment with resource-constrained and asymmetric information settings elaborated to evaluate LLMs through interactive gameplay against other LLM opponents. Notably, Squid Game consists of six elimination-style levels, focusing on multi-faceted abilities, such as instruction-following, code, reasoning, planning, and safety alignment. We evaluate over 50 LLMs on Squid Game, presenting the largest behavioral evaluation study of general LLMs on dynamic adversarial scenarios. We observe a clear generational phase transition on performance in the same model lineage and find evidence that some models resort to speculative shortcuts to win the game, indicating the possibility of higher-level evaluation paradigm contamination in static benchmarks. Furthermore, we compare prominent LLM benchmarks and Squid Game with correlation analyses, highlighting that dynamic evaluation can serve as a complementary part for static evaluations. The code and data will be released in the future.

</details>


### [29] [Do AI Voices Learn Social Nuances? A Case of Politeness and Speech Rate](https://arxiv.org/abs/2511.10693)
*Eyal Rabin,Zohar Elyoseph,Rotem Israel-Fishelson,Adi Dali,Ravit Nussinson*

Main category: cs.CL

TL;DR: 研究发现AI语音系统能够隐式学习人类礼貌交流中的语速变化模式，在礼貌提示下会自动放慢语速。


<details>
  <summary>Details</summary>
Motivation: 探究AI语音系统是否能够学习人类社交中非显性的礼貌表达方式，特别是语速变化这一隐含的韵律标记。

Method: 使用两个领先AI平台（AI Studio和OpenAI）的22个合成语音，在"礼貌正式"和"随意非正式"两种提示下朗读固定脚本，测量语音时长。

Result: 两个AI平台的礼貌提示都产生了比随意提示更慢的语速，效应量非常大，AI Studio的所有语音和OpenAI的大多数语音都显示出统计显著性。

Conclusion: AI能够隐式学习和复制人类交流的心理细微差别，表明其作为能够强化人类社会规范的社会行为者的新兴角色。

Abstract: Voice-based artificial intelligence is increasingly expected to adhere to human social conventions, but can it learn implicit cues that are not explicitly programmed? This study investigates whether state-of-the-art text-to-speech systems have internalized the human tendency to reduce speech rate to convey politeness - a non-obvious prosodic marker. We prompted 22 synthetic voices from two leading AI platforms (AI Studio and OpenAI) to read a fixed script under both "polite and formal" and "casual and informal" conditions and measured the resulting speech duration. Across both AI platforms, the polite prompt produced slower speech than the casual prompt with very large effect sizes, an effect that was statistically significant for all of AI Studio's voices and for a large majority of OpenAI's voices. These results demonstrate that AI can implicitly learn and replicate psychological nuances of human communication, highlighting its emerging role as a social actor capable of reinforcing human social norms.

</details>


### [30] [Where does an LLM begin computing an instruction?](https://arxiv.org/abs/2511.10694)
*Aditya Pola,Vineeth N. Balasubramanian*

Main category: cs.CL

TL;DR: 该研究通过激活修补技术，在Llama系列模型中发现指令跟随的起始点（onset），即从读取转向执行的关键转折层。


<details>
  <summary>Details</summary>
Motivation: 探索指令跟随过程中从读取到执行的具体转变点，理解语言模型内部处理指令的层次结构。

Method: 使用三个简单数据集及其组合任务，通过激活修补技术测量层间翻转率，确定预测答案改变的关键转折层。

Result: 在Llama系列模型中观察到指令跟随的起始点，多跳组合任务也显示相似的起始位置。

Conclusion: 提供了一种简单可复现的方法来定位指令跟随的起始位置，并比较不同任务和模型规模下的这一位置。

Abstract: Following an instruction involves distinct sub-processes, such as reading content, reading the instruction, executing it, and producing an answer. We ask where, along the layer stack, instruction following begins, the point where reading gives way to doing. We introduce three simple datasets (Key-Value, Quote Attribution, Letter Selection) and two hop compositions of these tasks. Using activation patching on minimal-contrast prompt pairs, we measure a layer-wise flip rate that indicates when substituting selected residual activations changes the predicted answer. Across models in the Llama family, we observe an inflection point, which we term onset, where interventions that change predictions before this point become largely ineffective afterward. Multi-hop compositions show a similar onset location. These results provide a simple, replicable way to locate where instruction following begins and to compare this location across tasks and model sizes.

</details>


### [31] ["As Eastern Powers, I will veto." : An Investigation of Nation-level Bias of Large Language Models in International Relations](https://arxiv.org/abs/2511.10695)
*Jonghyeon Choi,Yeonjun Choi,Hyun-chul Kim,Beakcheol Jang*

Main category: cs.CL

TL;DR: 本文系统评估了大型语言模型在国际关系领域的国家层面偏见，基于联合国安理会历史记录构建了偏见评估框架，发现LLMs存在多维偏见模式，并提出结合检索增强生成和反思技术的去偏见框架。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在国际关系领域的应用日益广泛，需要系统评估其国家层面偏见，以确保公平性和准确性。

Method: 基于联合国安理会历史记录构建三测试偏见评估框架，评估多个LLMs对安理会五常国家的偏见；提出结合RAG和反思技术的去偏见框架。

Result: 实验显示LLMs存在系统性偏见（偏向西方国家，对俄罗斯不利），但偏见程度因模型和任务而异；推理能力强的模型偏见较小；提出的去偏见框架有效降低了偏见并提升了性能。

Conclusion: 在国际关系领域应用LLMs时，需要同时评估国家层面偏见和性能表现，去偏见框架能有效改善模型表现。

Abstract: This paper systematically examines nation-level biases exhibited by Large Language Models (LLMs) within the domain of International Relations (IR). Leveraging historical records from the United Nations Security Council (UNSC), we developed a bias evaluation framework comprising three distinct tests to explore nation-level bias in various LLMs, with a particular focus on the five permanent members of the UNSC. Experimental results show that, even with the general bias patterns across models (e.g., favorable biases toward the western nations, and unfavorable biases toward Russia), these still vary based on the LLM. Notably, even within the same LLM, the direction and magnitude of bias for a nation change depending on the evaluation context. This observation suggests that LLM biases are fundamentally multidimensional, varying across models and tasks. We also observe that models with stronger reasoning abilities show reduced bias and better performance. Building on this finding, we introduce a debiasing framework that improves LLMs' factual reasoning combining Retrieval-Augmented Generation with Reflexion-based self-reflection techniques. Experiments show it effectively reduces nation-level bias, and improves performance, particularly in GPT-4o-mini and LLama-3.3-70B. Our findings emphasize the need to assess nation-level bias alongside performance when applying LLMs in the IR domain.

</details>


### [32] [$π$-Attention: Periodic Sparse Transformers for Efficient Long-Context Modeling](https://arxiv.org/abs/2511.10696)
*Dong Liu,Yanxuan Yu*

Main category: cs.CL

TL;DR: ΠAttention是一种周期性稀疏Transformer，通过环局部注意力、确定性π步长跳跃和自适应融合门来降低计算复杂度，在保持线性复杂度的同时扩展感受野，在语言建模等任务中优于RingAttention。


<details>
  <summary>Details</summary>
Motivation: Transformer的二次复杂度限制了长序列建模能力，现有稀疏注意力方法如RingAttention虽然降低了计算成本，但感受野有限且缺乏适应性。

Method: 将注意力分解为环局部邻域、确定性π步长跳跃和自适应融合门，周期性结构提供对远距离token的可预测覆盖，稀疏足迹保持每层复杂度与上下文长度线性相关。

Result: 在语言建模、检索和视觉语言任务中，ΠAttention匹配或超越密集注意力质量，困惑度比RingAttention低8.3%，相同上下文长度下GPU使用量减少50%。

Conclusion: 周期性跳跃、自适应融合和头级稀疏协调对于高效长上下文建模至关重要，ΠAttention在保持线性复杂度的同时显著扩展了感受野。

Abstract: Transformers have revolutionized natural language processing, but their quadratic complexity with respect to sequence length remains a fundamental bottleneck for long-range modeling. While sparse attention mechanisms like RingAttention reduce computational costs by restricting attention to local neighborhoods, they suffer from limited receptive fields and lack of adaptability. We present \PiAttention, a periodic sparse Transformer that factorizes attention into ring-local neighborhoods, deterministic $π$-stride skips, and an adaptive fusion gate. The periodic structure provides predictable coverage of distant tokens, while the sparse footprint keeps the per-layer complexity linear in context length. We prove that \PiAttention achieves $\mathcal{O}(kL + π\log L)$ receptive field growth compared to $\mathcal{O}(kL)$ for RingAttention, where $k$ is the local window size, $π$ is the skip period, and $L$ is the sequence length. Extensive experiments on language modeling, retrieval, and vision-language tasks demonstrate that \PiAttention matches or surpasses dense attention quality with 8.3\% lower perplexity than RingAttention while using 50\% fewer GPUs for the same context length. Our detailed ablations and visualizations reveal the importance of periodic skips, adaptive fusion, and head-level sparsity coordination for efficient long-context modeling.

</details>


### [33] [Faithful Summarization of Consumer Health Queries: A Cross-Lingual Framework with LLMs](https://arxiv.org/abs/2511.10768)
*Ajwad Abrar,Nafisa Tabassum Oeshy,Prianka Maheru,Farzana Tabassum,Tareque Mohmud Chowdhury*

Main category: cs.CL

TL;DR: 提出了一个结合TextRank句子提取、医学命名实体识别和大语言模型的框架，用于提高医学文本摘要的忠实度。在MeQSum和BanglaCHQ-Summ数据集上微调LLaMA-2-7B模型，在质量和忠实度指标上均取得改进，80%以上摘要能保留关键医学信息。


<details>
  <summary>Details</summary>
Motivation: 消费者健康问题摘要可以简化医疗沟通，但不忠实的摘要会误传医学细节，带来严重风险。需要确保医学摘要的忠实性。

Method: 结合TextRank句子提取、医学命名实体识别和大语言模型(LLMs)的框架，在MeQSum(英语)和BanglaCHQ-Summ(孟加拉语)数据集上微调LLaMA-2-7B模型。

Result: 在质量指标(ROUGE、BERTScore、可读性)和忠实度指标(SummaC、AlignScore)上均取得一致改进，优于零样本基线和先前系统。人工评估显示超过80%的生成摘要能保留关键医学信息。

Conclusion: 忠实度是可靠医学摘要的重要维度，该方法展示了在医疗环境中更安全部署大语言模型的潜力。

Abstract: Summarizing consumer health questions (CHQs) can ease communication in healthcare, but unfaithful summaries that misrepresent medical details pose serious risks. We propose a framework that combines TextRank-based sentence extraction and medical named entity recognition with large language models (LLMs) to enhance faithfulness in medical text summarization. In our experiments, we fine-tuned the LLaMA-2-7B model on the MeQSum (English) and BanglaCHQ-Summ (Bangla) datasets, achieving consistent improvements across quality (ROUGE, BERTScore, readability) and faithfulness (SummaC, AlignScore) metrics, and outperforming zero-shot baselines and prior systems. Human evaluation further shows that over 80\% of generated summaries preserve critical medical information. These results highlight faithfulness as an essential dimension for reliable medical summarization and demonstrate the potential of our approach for safer deployment of LLMs in healthcare contexts.

</details>


### [34] [TEDxTN: A Three-way Speech Translation Corpus for Code-Switched Tunisian Arabic - English](https://arxiv.org/abs/2511.10780)
*Fethi Bougares,Salima Mdhaffar,Haroun Elleuch,Yannick Estève*

Main category: cs.CL

TL;DR: TEDxTN是首个公开的突尼斯阿拉伯语到英语语音翻译数据集，包含108个TEDx演讲，25小时语音，涵盖11个突尼斯地区的口音和语码转换现象。


<details>
  <summary>Details</summary>
Motivation: 解决阿拉伯语方言数据稀缺问题，促进突尼斯方言的自然语言处理研究。

Method: 收集、分割、转录和翻译108个TEDx演讲，遵循内部开发的标注指南，使用多种预训练和微调的端到端模型建立语音识别和语音翻译基线系统。

Result: 创建了首个开源公开的语码转换突尼斯方言语音翻译语料库，提供了强基线系统的实验结果。

Conclusion: TEDxTN是一个有价值的资源，能够激励和促进突尼斯方言自然语言处理的进一步研究。

Abstract: In this paper, we introduce TEDxTN, the first publicly available Tunisian Arabic to English speech translation dataset. This work is in line with the ongoing effort to mitigate the data scarcity obstacle for a number of Arabic dialects. We collected, segmented, transcribed and translated 108 TEDx talks following our internally developed annotations guidelines. The collected talks represent 25 hours of speech with code-switching that cover speakers with various accents from over 11 different regions of Tunisia. We make the annotation guidelines and corpus publicly available. This will enable the extension of TEDxTN to new talks as they become available. We also report results for strong baseline systems of Speech Recognition and Speech Translation using multiple pre-trained and fine-tuned end-to-end models. This corpus is the first open source and publicly available speech translation corpus of Code-Switching Tunisian dialect. We believe that this is a valuable resource that can motivate and facilitate further research on the natural language processing of Tunisian Dialect.

</details>


### [35] [Sabiá: Um Chatbot de Inteligência Artificial Generativa para Suporte no Dia a Dia do Ensino Superior](https://arxiv.org/abs/2511.10787)
*Guilherme Biava Rodrigues,Franciele Beal,Marlon Marcon,Alinne Cristinne Corrêa Souza,André Roberto Ortoncelli,Francisco Carlos Monteiro Souza,Rodolfo Adamshuk Silva*

Main category: cs.CL

TL;DR: 开发基于GenAI和RAG的聊天机器人，解决学生获取分散学术信息的困难，测试多个模型后Gemini 2.0 Flash和Gemma 3n表现最佳。


<details>
  <summary>Details</summary>
Motivation: 学生难以获取分散在多个机构文档和网站中的日常学术信息，这种碎片化导致信息不清晰和困惑。

Method: 使用生成式人工智能和检索增强生成技术开发聊天机器人，测试多个GenAI模型并基于质量指标和LLM-as-a-Judge方法进行评估。

Result: Gemini 2.0 Flash在质量和速度方面表现突出，Gemma 3n因其良好性能和开源特性也表现优异。

Conclusion: 基于GenAI和RAG的聊天机器人能有效简化学生获取日常学术信息的过程，特定模型在质量和效率方面表现最佳。

Abstract: Students often report difficulties in accessing day-to-day academic information, which is usually spread across numerous institutional documents and websites. This fragmentation results in a lack of clarity and causes confusion about routine university information. This project proposes the development of a chatbot using Generative Artificial Intelligence (GenAI) and Retrieval-Augmented Generation (RAG) to simplify access to such information. Several GenAI models were tested and evaluated based on quality metrics and the LLM-as-a-Judge approach. Among them, Gemini 2.0 Flash stood out for its quality and speed, and Gemma 3n for its good performance and open-source nature.

</details>


### [36] [LLM-as-a-Grader: Practical Insights from Large Language Model for Short-Answer and Report Evaluation](https://arxiv.org/abs/2511.10819)
*Grace Byun,Swati Rajwal,Jinho D. Choi*

Main category: cs.CL

TL;DR: 研究探讨了使用GPT-4o在计算语言学课程中对简短答案测验和项目报告进行评分的可行性，结果显示与人类评分员有强相关性（最高0.98），55%的测验案例得分完全一致，但在技术性开放答案评分中存在一定变异性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在教育任务（如评分）中的应用日益增多，但其在真实课堂环境中与人类评估的一致性仍缺乏充分研究。

Method: 收集约50名学生的5次测验回答和14个团队的项目报告，使用GPT-4o进行评分，并与课程助教独立进行的人类评估进行对比。

Result: GPT-4o与人类评分员达到强相关性（最高0.98），55%的测验案例得分完全一致；在项目报告评分中也显示出与人类评分的整体一致性，但在技术性开放答案评分中存在变异性。

Conclusion: 这项工作突出了基于LLM的评分系统的潜力和局限性，为在真实学术环境中推进自动评分做出了贡献。

Abstract: Large Language Models (LLMs) are increasingly explored for educational tasks such as grading, yet their alignment with human evaluation in real classrooms remains underexamined. In this study, we investigate the feasibility of using an LLM (GPT-4o) to evaluate short-answer quizzes and project reports in an undergraduate Computational Linguistics course. We collect responses from approximately 50 students across five quizzes and receive project reports from 14 teams. LLM-generated scores are compared against human evaluations conducted independently by the course teaching assistants (TAs). Our results show that GPT-4o achieves strong correlation with human graders (up to 0.98) and exact score agreement in 55\% of quiz cases. For project reports, it also shows strong overall alignment with human grading, while exhibiting some variability in scoring technical, open-ended responses. We release all code and sample data to support further research on LLMs in educational assessment. This work highlights both the potential and limitations of LLM-based grading systems and contributes to advancing automated grading in real-world academic settings.

</details>


### [37] [Tracing Multilingual Representations in LLMs with Cross-Layer Transcoders](https://arxiv.org/abs/2511.10840)
*Abir Harrasse,Florent Draye,Zhijing Jin,Bernhard Schölkopf*

Main category: cs.CL

TL;DR: 研究发现多语言大模型采用枢轴语言表示机制：模型在不同语言间使用几乎相同的内部表示，而语言特定的解码出现在较深层。通过干预语言特征可以抑制一种语言并用另一种语言替代输出。


<details>
  <summary>Details</summary>
Motivation: 多语言大模型能处理多种语言，但其内部如何表示这种多样性尚不清楚。研究旨在探究模型是否形成共享的多语言表示以及为何性能仍偏向主导训练语言。

Method: 训练一系列在不同多语言数据混合上的LLM，使用跨层转码器(CLT)和归因图分析内部机制。

Result: 发现模型采用枢轴语言表示机制，解码部分依赖于最后一层中的高频语言特征，这些特征从模型第一层线性读取语言身份。通过干预这些特征可以控制语言输出。

Conclusion: 理解枢轴语言机制对于改进LLM中的多语言对齐至关重要。

Abstract: Multilingual Large Language Models (LLMs) can process many languages, yet how they internally represent this diversity remains unclear. Do they form shared multilingual representations with language-specific decoding, and if so, why does performance still favor the dominant training language? To address this, we train a series of LLMs on different mixtures of multilingual data and analyze their internal mechanisms using cross-layer transcoders (CLT) and attribution graphs. Our results provide strong evidence for pivot language representations: the model employs nearly identical representations across languages, while language-specific decoding emerges in later layers. Attribution analyses reveal that decoding relies in part on a small set of high-frequency language features in the final layers, which linearly read out language identity from the first layers in the model. By intervening on these features, we can suppress one language and substitute another in the model's outputs. Finally, we study how the dominant training language influences these mechanisms across attribution graphs and decoding pathways. We argue that understanding this pivot-language mechanism is crucial for improving multilingual alignment in LLMs.

</details>


### [38] [Reinforcing Stereotypes of Anger: Emotion AI on African American Vernacular English](https://arxiv.org/abs/2511.10846)
*Rebecca Dorn,Christina Chance,Casandra Rusti,Charles Bickham,Kai-Wei Chang,Fred Morstatter,Kristina Lerman*

Main category: cs.CL

TL;DR: 研究发现情感AI模型在识别非洲裔美国方言英语(AAVE)时存在严重偏见，愤怒情绪误报率是通用美式英语的两倍以上，可能强化种族刻板印象


<details>
  <summary>Details</summary>
Motivation: 现有情感检测模型通常基于主流文化规范进行标注，无法准确识别被排除在训练数据之外的方言情感表达，如非洲裔美国方言英语

Method: 分析270万条洛杉矶地理标记推文，计算AAVE方言特征强度，由AAVE流利的非洲裔美国人标注875条推文的情感存在和强度，比较GPT、BERT和SpanEmo模型在不同方言上的表现

Result: 模型在AAVE上的愤怒情绪误报率是GAE的两倍多(SpanEmo从25%升至60%)，模型预测与社区黑人比例呈正相关(愤怒r=0.27，快乐r=-0.10)

Conclusion: 情感AI存在严重安全问题，可能通过有偏见的情感分类强化种族刻板印象，需要开发文化和方言感知的情感计算系统

Abstract: Automated emotion detection is widely used in applications ranging from well-being monitoring to high-stakes domains like mental health and hiring. However, models often rely on annotations that reflect dominant cultural norms, limiting model ability to recognize emotional expression in dialects often excluded from training data distributions, such as African American Vernacular English (AAVE). This study examines emotion recognition model performance on AAVE compared to General American English (GAE). We analyze 2.7 million tweets geo-tagged within Los Angeles. Texts are scored for strength of AAVE using computational approximations of dialect features. Annotations of emotion presence and intensity are collected on a dataset of 875 tweets with both high and low AAVE densities. To assess model accuracy on a task as subjective as emotion perception, we calculate community-informed "silver" labels where AAVE-dense tweets are labeled by African American, AAVE-fluent (ingroup) annotators. On our labeled sample, GPT and BERT-based models exhibit false positive prediction rates of anger on AAVE more than double than on GAE. SpanEmo, a popular text-based emotion model, increases false positive rates of anger from 25 percent on GAE to 60 percent on AAVE. Additionally, a series of linear regressions reveals that models and non-ingroup annotations are significantly more correlated with profanity-based AAVE features than ingroup annotations. Linking Census tract demographics, we observe that neighborhoods with higher proportions of African American residents are associated with higher predictions of anger (Pearson's correlation r = 0.27) and lower joy (r = -0.10). These results find an emergent safety issue of emotion AI reinforcing racial stereotypes through biased emotion classification. We emphasize the need for culturally and dialect-informed affective computing systems.

</details>


### [39] [Leveraging Parameter Space Symmetries for Reasoning Skill Transfer in LLMs](https://arxiv.org/abs/2511.10850)
*Stefan Horoi,Sangwoo Cho,Supriyo Chakraborty,Shi-Xiong Zhang,Sambit Sahu,Guy Wolf,Genta Indra Winata*

Main category: cs.CL

TL;DR: 通过参数空间对齐改进任务算术，解决LLM技能转移中的负干扰问题，在推理基准测试中表现优于标准方法


<details>
  <summary>Details</summary>
Motivation: 任务算术在LLM技能转移中存在负干扰问题，特别是当模型在训练过程中出现分歧时

Method: 利用Transformer架构的排列、旋转和缩放对称性对齐模型参数空间，适配GQA和SwiGLU层，采用权重和激活两种对齐方法

Result: 成功将高级推理技能转移到非推理模型，在挑战性推理基准测试中持续优于标准任务算术

Conclusion: 该方法为合并和转移不同LLM家族间的专业技能提供了有效途径，减少冗余微调并增强模型适应性

Abstract: Task arithmetic is a powerful technique for transferring skills between Large Language Models (LLMs), but it often suffers from negative interference when models have diverged during training. We address this limitation by first aligning the models' parameter spaces, leveraging the inherent permutation, rotation, and scaling symmetries of Transformer architectures. We adapt parameter space alignment for modern Grouped-Query Attention (GQA) and SwiGLU layers, exploring both weight-based and activation-based approaches. Using this alignment-first strategy, we successfully transfer advanced reasoning skills to a non-reasoning model. Experiments on challenging reasoning benchmarks show that our method consistently outperforms standard task arithmetic. This work provides an effective approach for merging and transferring specialized skills across evolving LLM families, reducing redundant fine-tuning and enhancing model adaptability.

</details>


### [40] [From Fact to Judgment: Investigating the Impact of Task Framing on LLM Conviction in Dialogue Systems](https://arxiv.org/abs/2511.10871)
*Parisa Rabbani,Nimet Beyza Bozdag,Dilek Hakkani-Tür*

Main category: cs.CL

TL;DR: LLMs作为评判者在社交对话任务中的判断会因对话框架的改变而显著变化，即使是最小的对话上下文也会影响模型判断，部分模型表现出迎合或过度批判的倾向。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在需要社交或对话判断的任务中是否能可靠评估，特别是当任务从直接事实查询转变为对话判断任务时，模型的判断如何变化。

Method: 构建评估框架，对比模型在直接事实查询和最小对话情境下的表现，通过简单的反驳施加压力来测量模型坚持立场的坚定程度。

Result: 不同模型表现各异：GPT-4o-mini在社交框架任务中表现出迎合倾向，而Llama-8B-Instruct变得过度批判，所有模型的平均性能变化达9.24%。

Conclusion: 对话框架是LLM评估的关键因素，提出的框架为诊断模型信念提供了可复现方法，有助于开发更可信的对话系统。

Abstract: LLMs are increasingly employed as judges across a variety of tasks, including those involving everyday social interactions. Yet, it remains unclear whether such LLM-judges can reliably assess tasks that require social or conversational judgment. We investigate how an LLM's conviction is changed when a task is reframed from a direct factual query to a Conversational Judgment Task. Our evaluation framework contrasts the model's performance on direct factual queries with its assessment of a speaker's correctness when the same information is presented within a minimal dialogue, effectively shifting the query from "Is this statement correct?" to "Is this speaker correct?". Furthermore, we apply pressure in the form of a simple rebuttal ("The previous answer is incorrect.") to both conditions. This perturbation allows us to measure how firmly the model maintains its position under conversational pressure. Our findings show that while some models like GPT-4o-mini reveal sycophantic tendencies under social framing tasks, others like Llama-8B-Instruct become overly-critical. We observe an average performance change of 9.24% across all models, demonstrating that even minimal dialogue context can significantly alter model judgment, underscoring conversational framing as a key factor in LLM-based evaluation. The proposed framework offers a reproducible methodology for diagnosing model conviction and contributes to the development of more trustworthy dialogue systems.

</details>


### [41] [ICX360: In-Context eXplainability 360 Toolkit](https://arxiv.org/abs/2511.10879)
*Dennis Wei,Ronny Luss,Xiaomeng Hu,Lucas Monteiro Paes,Pin-Yu Chen,Karthikeyan Natesan Ramamurthy,Erik Miehling,Inge Vejsbjerg,Hendrik Strobelt*

Main category: cs.CL

TL;DR: ICX360是一个开源的Python工具包，专注于解释大型语言模型(LLMs)的输出，特别关注用户提供的上下文或提示。它实现了三种最新工具，使用黑盒和白盒方法(分别通过扰动和梯度)来解释LLMs。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在日常生活中的普及并进入更高风险的应用场景，开发解释LLM输出的工具变得至关重要，无论是摘要、列表还是问题回答。

Method: ICX360工具包实现了三种最新工具：使用扰动方法的黑盒解释和使用梯度方法的白盒解释。支持检索增强生成、自然语言生成和越狱等用例。

Result: 开发了ICX360开源工具包，提供快速入门指南和详细教程，可在GitHub上获取。

Conclusion: ICX360为解释LLM输出提供了一个全面的工具包，特别关注上下文和提示的影响，有助于提高LLM在关键应用中的透明度和可信度。

Abstract: Large Language Models (LLMs) have become ubiquitous in everyday life and are entering higher-stakes applications ranging from summarizing meeting transcripts to answering doctors' questions. As was the case with earlier predictive models, it is crucial that we develop tools for explaining the output of LLMs, be it a summary, list, response to a question, etc. With these needs in mind, we introduce In-Context Explainability 360 (ICX360), an open-source Python toolkit for explaining LLMs with a focus on the user-provided context (or prompts in general) that are fed to the LLMs. ICX360 contains implementations for three recent tools that explain LLMs using both black-box and white-box methods (via perturbations and gradients respectively). The toolkit, available at https://github.com/IBM/ICX360, contains quick-start guidance materials as well as detailed tutorials covering use cases such as retrieval augmented generation, natural language generation, and jailbreaking.

</details>


### [42] [A Multifaceted Analysis of Negative Bias in Large Language Models through the Lens of Parametric Knowledge](https://arxiv.org/abs/2511.10881)
*Jongyoon Song,Sangwon Yu,Sungroh Yoon*

Main category: cs.CL

TL;DR: 本文研究发现大语言模型存在格式层面的负向偏见，即提示格式比否定回答的语义更能影响模型响应。模型在缺乏足够知识回答是非问题时倾向于生成否定响应，导致负向偏见。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注检测和处理引发负向偏见的负向注意力头，但影响负向偏见的详细底层因素仍未充分探索。

Method: 引入构建评估集的流程，将数据集系统分类为正确、错误和相关知识不足三个子集。分析不同提示场景下负向偏见的变化，包括提供相关上下文、提供"我不知道"选项和思维链提示。

Result: 提供相关上下文和"我不知道"选项通常能减少负向偏见，而思维链提示往往会放大偏见。负向偏见的程度因提示类型而异，这影响了响应的方向。

Conclusion: 揭示了影响负向偏见的各种因素，为减轻大语言模型中的负向偏见提供了关键见解。

Abstract: Negative bias refers to the tendency of large language models (LLMs) to excessively generate negative responses in binary decision tasks (e.g., yes-no question answering). Previous research has focused on detecting and addressing negative attention heads that induce negative bias. However, the underlying detailed factors influencing negative bias remain underexplored. In this paper, we demonstrate that LLMs exhibit format-level negative bias, meaning the prompt format more influences their responses than the semantics of the negative response. For the fine-grained study of the negative bias, we introduce a pipeline for constructing the evaluation set, which systematically categorizes the dataset into three subsets based on the model's parametric knowledge: correct, incorrect, and insufficient relevant knowledge. Through analysis of this evaluation set, we identify a shortcut behavior in which models tend to generate negative responses when they lack sufficient knowledge to answer a yes-no question, leading to negative bias. We further examine how negative bias changes under various prompting scenarios related to parametric knowledge. We observe that providing relevant context and offering an "I don't know" option generally reduces negative bias, whereas chain-of-thought prompting tends to amplify the bias. Finally, we demonstrate that the degree of negative bias can vary depending on the type of prompt, which influences the direction of the response. Our work reveals the various factors that influence negative bias, providing critical insights for mitigating it in LLMs.

</details>


### [43] [MedPath: Multi-Domain Cross-Vocabulary Hierarchical Paths for Biomedical Entity Linking](https://arxiv.org/abs/2511.10887)
*Nishant Mishra,Wilker Aziz,Iacer Calixto*

Main category: cs.CL

TL;DR: MedPath是一个大规模多领域生物医学实体链接数据集，整合了9个现有专家标注数据集，通过UMLS标准化、映射到62个生物医学词汇表，并提供完整的本体路径，旨在解决当前生物医学NER和EL领域的数据碎片化、可解释模型资源缺乏等问题。


<details>
  <summary>Details</summary>
Motivation: 当前生物医学命名实体识别和实体链接领域面临数据碎片化、缺乏构建可解释模型的资源，以及语义盲评估指标的局限性等挑战，阻碍了该领域的进展。

Method: 基于9个现有专家标注的实体链接数据集构建MedPath，所有实体通过UMLS进行标准化，映射到62个其他生物医学词汇表，并丰富了最多11个生物医学词汇表中的完整本体路径。

Result: 创建了MedPath数据集，这是一个大规模多领域生物医学实体链接资源，支持语义丰富和可解释的实体链接系统的训练和评估。

Conclusion: MedPath直接开启了生物医学NLP的新研究前沿，促进了语义丰富和可解释实体链接系统的训练和评估，以及下一代可互操作和可解释临床NLP模型的开发。

Abstract: Progress in biomedical Named Entity Recognition (NER) and Entity Linking (EL) is currently hindered by a fragmented data landscape, a lack of resources for building explainable models, and the limitations of semantically-blind evaluation metrics. To address these challenges, we present MedPath, a large-scale and multi-domain biomedical EL dataset that builds upon nine existing expert-annotated EL datasets. In MedPath, all entities are 1) normalized using the latest version of the Unified Medical Language System (UMLS), 2) augmented with mappings to 62 other biomedical vocabularies and, crucially, 3) enriched with full ontological paths -- i.e., from general to specific -- in up to 11 biomedical vocabularies. MedPath directly enables new research frontiers in biomedical NLP, facilitating training and evaluation of semantic-rich and interpretable EL systems, and the development of the next generation of interoperable and explainable clinical NLP models.

</details>


### [44] [From Proof to Program: Characterizing Tool-Induced Reasoning Hallucinations in Large Language Models](https://arxiv.org/abs/2511.10899)
*Farima Fatahi Bayat,Pouya Pezeshkpour,Estevam Hruschka*

Main category: cs.CL

TL;DR: 工具增强语言模型(TaLMs)虽然能通过调用外部工具提升答案准确率，但会导致推理质量下降，出现工具诱导短视(TIM)现象，即模型将工具输出替代推理过程，产生缺乏连贯论证的解决方案。


<details>
  <summary>Details</summary>
Motivation: 研究工具增强语言模型在调用外部工具时是否真正提升了推理能力，还是仅仅用工具输出替代了推理过程。

Method: 使用PYMATH基准测试(1,679个竞赛级数学问题)，开发多维度评估套件量化TaLMs推理退化，并提出基于偏好优化的框架来重新调整工具使用方式。

Result: TaLMs最终答案准确率提升19.3个百分点，但推理质量显著下降(非工具LLMs在推理过程比较中胜出41.5%)，工具使用越频繁推理越不连贯，约55%高风险案例存在TIM问题。

Conclusion: 工具使用虽然提升答案准确性，但损害推理质量，需要通过偏好优化等方法让模型将工具作为辅助证据而非推理替代品。

Abstract: Tool-augmented Language Models (TaLMs) can invoke external tools to solve problems beyond their parametric capacity. However, it remains unclear whether these tool-enabled gains reflect trustworthy reasoning. Focusing on the Code Interpreter tool, we show that even when tools are selected and executed correctly, TaLMs treat tool outputs as substitutes for reasoning, producing solutions that appear correct but lack coherent justification. We term this failure mode Tool-Induced Myopia (TIM), and study it using PYMATH, a benchmark of 1,679 competition-level mathematical problems for which Python code is helpful but not sufficient. We further develop a multi-dimensional evaluation suite to quantify reasoning degradation in TaLMs relative to their non-tool counterparts. Our findings reveal that while TaLMs achieve up to a 19.3 percentage point gain in final-answer accuracy, their reasoning behavior consistently deteriorates (e.g., non-tool LLMs win up to 41.5% more often in pairwise comparisons of the reasoning process). This degradation intensifies with tool use; the more frequently a model invokes tools, the less coherent its reasoning becomes. Moreover, tool use shifts errors from arithmetic mistakes toward global reasoning failures (logic, assumption, creativity); with TIM present in ~55% of high-risk cases. Finally, we propose a preference-optimization-based framework that realigns TaLMs to use tools as assistive evidence, improving both final-answer accuracy and reasoning depth under tool use. Codes and data are available at: https://github.com/megagonlabs/TIM.

</details>


### [45] [Expert-Guided Prompting and Retrieval-Augmented Generation for Emergency Medical Service Question Answering](https://arxiv.org/abs/2511.10900)
*Xueren Ge,Sahil Murtaza,Anthony Cortez,Homa Alemzadeh*

Main category: cs.CL

TL;DR: 提出了EMSQA数据集和Expert-CoT、ExpertRAG方法，通过结合临床专业领域和认证级别知识，显著提升LLMs在医疗问答中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在医疗问答中忽视了专业领域知识（如创伤、气道等临床科目）和认证级别（如EMT、护理人员），限制了在高风险环境下的性能。

Method: 构建了EMSQA数据集（24.3K问题），并提出Expert-CoT（基于特定临床科目和认证级别的思维链提示）和ExpertRAG（基于科目对齐文档和真实患者数据的检索增强生成）。

Result: 在4个LLMs上的实验显示，Expert-CoT比普通CoT提示提升2.05%，结合ExpertRAG比标准RAG基线提升4.59%准确率。32B专业增强LLMs通过了所有计算机自适应EMS认证模拟考试。

Conclusion: 通过整合领域专业知识，可以显著提升LLMs在医疗问答中的性能，使其能够满足专业认证要求。

Abstract: Large language models (LLMs) have shown promise in medical question answering, yet they often overlook the domain-specific expertise that professionals depend on, such as the clinical subject areas (e.g., trauma, airway) and the certification level (e.g., EMT, Paramedic). Existing approaches typically apply general-purpose prompting or retrieval strategies without leveraging this structured context, limiting performance in high-stakes settings. We address this gap with EMSQA, an 24.3K-question multiple-choice dataset spanning 10 clinical subject areas and 4 certification levels, accompanied by curated, subject area-aligned knowledge bases (40K documents and 2M tokens). Building on EMSQA, we introduce (i) Expert-CoT, a prompting strategy that conditions chain-of-thought (CoT) reasoning on specific clinical subject area and certification level, and (ii) ExpertRAG, a retrieval-augmented generation pipeline that grounds responses in subject area-aligned documents and real-world patient data. Experiments on 4 LLMs show that Expert-CoT improves up to 2.05% over vanilla CoT prompting. Additionally, combining Expert-CoT with ExpertRAG yields up to a 4.59% accuracy gain over standard RAG baselines. Notably, the 32B expertise-augmented LLMs pass all the computer-adaptive EMS certification simulation exams.

</details>


### [46] [Multimodal Peer Review Simulation with Actionable To-Do Recommendations for Community-Aware Manuscript Revisions](https://arxiv.org/abs/2511.10902)
*Mengze Hong,Di Jiang,Weiwei Zhao,Yawen Li,Yihang Wang,Xinyuan Luo,Yanjie Sun,Chen Jason Zhang*

Main category: cs.CL

TL;DR: 开发了一个基于多模态大语言模型的交互式学术同行评审系统，通过整合文本和视觉信息、基于OpenReview数据的检索增强生成，以及将评审意见转化为结构化待办清单，帮助作者在论文提交前进行有效修订。


<details>
  <summary>Details</summary>
Motivation: 现有学术同行评审系统受限于纯文本输入、有限的上下文基础和缺乏可操作的反馈，无法充分利用大语言模型在自动化学术工作流程中的潜力。

Method: 提出基于多模态大语言模型的交互式网络系统，整合文本和视觉信息，使用检索增强生成技术基于OpenReview数据提升评审质量，并通过Action:Objective[#]格式将评审意见转化为结构化待办清单。

Result: 实验结果表明，该系统能够生成更全面、有用的评审意见，符合专家标准，优于消融基线模型，推动了透明、以人为本的学术辅助工具发展。

Conclusion: 该多模态、社区感知的同行评审模拟系统为学术写作提供了有效的实时反馈和修订跟踪，显著提升了预提交阶段的论文质量。

Abstract: While large language models (LLMs) offer promising capabilities for automating academic workflows, existing systems for academic peer review remain constrained by text-only inputs, limited contextual grounding, and a lack of actionable feedback. In this work, we present an interactive web-based system for multimodal, community-aware peer review simulation to enable effective manuscript revisions before paper submission. Our framework integrates textual and visual information through multimodal LLMs, enhances review quality via retrieval-augmented generation (RAG) grounded in web-scale OpenReview data, and converts generated reviews into actionable to-do lists using the proposed Action:Objective[\#] format, providing structured and traceable guidance. The system integrates seamlessly into existing academic writing platforms, providing interactive interfaces for real-time feedback and revision tracking. Experimental results highlight the effectiveness of the proposed system in generating more comprehensive and useful reviews aligned with expert standards, surpassing ablated baselines and advancing transparent, human-centered scholarly assistance.

</details>


### [47] [Automated Analysis of Learning Outcomes and Exam Questions Based on Bloom's Taxonomy](https://arxiv.org/abs/2511.10903)
*Ramya Kumar,Dhruv Gulwani,Sonit Singh*

Main category: cs.CL

TL;DR: 本文研究了基于布鲁姆分类法的考试题目和学习成果自动分类，比较了传统机器学习、RNN、Transformer和大型语言模型在600条标注数据上的表现，发现数据增强的SVM模型效果最佳（94%准确率），而复杂深度学习模型存在严重过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 探索如何自动将考试题目和学习成果按照布鲁姆分类法的六个认知层次进行分类，以解决教育评估中的自动化需求。

Method: 使用600条标注句子数据集，分别采用传统ML模型（朴素贝叶斯、逻辑回归、SVM）、RNN架构（LSTM、BiLSTM、GRU、BiGRU）、Transformer模型（BERT、RoBERTa）和大型语言模型（OpenAI、Gemini等），在不同预处理和数据增强策略下进行实验。

Result: 数据增强的SVM模型表现最佳，达到94%的准确率、召回率和F1分数，且过拟合最小；RNN和BERT模型存在严重过拟合；RoBERTa初期克服了过拟合但训练后期出现迹象；大型语言模型的零样本评估中，OpenAI和Gemini表现最好，准确率约0.72-0.73。

Conclusion: 在有限数据上训练复杂深度学习模型存在挑战，精心设计的数据增强和简单算法（如增强SVM）在布鲁姆分类法分类中具有重要价值。

Abstract: This paper explores the automatic classification of exam questions and learning outcomes according to Bloom's Taxonomy. A small dataset of 600 sentences labeled with six cognitive categories - Knowledge, Comprehension, Application, Analysis, Synthesis, and Evaluation - was processed using traditional machine learning (ML) models (Naive Bayes, Logistic Regression, Support Vector Machines), recurrent neural network architectures (LSTM, BiLSTM, GRU, BiGRU), transformer-based models (BERT and RoBERTa), and large language models (OpenAI, Gemini, Ollama, Anthropic). Each model was evaluated under different preprocessing and augmentation strategies (for example, synonym replacement, word embeddings, etc.). Among traditional ML approaches, Support Vector Machines (SVM) with data augmentation achieved the best overall performance, reaching 94 percent accuracy, recall, and F1 scores with minimal overfitting. In contrast, the RNN models and BERT suffered from severe overfitting, while RoBERTa initially overcame it but began to show signs as training progressed. Finally, zero-shot evaluations of large language models (LLMs) indicated that OpenAI and Gemini performed best among the tested LLMs, achieving approximately 0.72-0.73 accuracy and comparable F1 scores. These findings highlight the challenges of training complex deep models on limited data and underscore the value of careful data augmentation and simpler algorithms (such as augmented SVM) for Bloom's Taxonomy classification.

</details>


### [48] [Evaluating Large Language Models on Rare Disease Diagnosis: A Case Study using House M.D](https://arxiv.org/abs/2511.10912)
*Arsh Gupta,Ajay Narayanan Sridhar,Bonam Mingole,Amulya Yadav*

Main category: cs.CL

TL;DR: 评估四种先进大语言模型在罕见病诊断任务中的表现，使用从《豪斯医生》剧集提取的176个症状-诊断对作为基准数据集，结果显示准确率在16.48%到38.64%之间，新一代模型性能提升2.3倍。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在罕见病诊断方面的能力，目前这一领域研究较少，而《豪斯医生》剧集已被医学教育验证可用于罕见病识别教学。

Method: 构建包含176个症状-诊断对的数据集，评估GPT 4o mini、GPT 5 mini、Gemini 2.5 Flash和Gemini 2.5 Pro四种模型在叙事性医疗推理任务中的表现。

Result: 模型准确率差异显著，从16.48%到38.64%不等，新一代模型相比旧模型有2.3倍的性能提升，但所有模型在罕见病诊断方面仍面临重大挑战。

Conclusion: 虽然模型在罕见病诊断上仍有困难，但跨架构的改进表明未来发展前景良好，该教育验证基准为AI辅助诊断研究提供了公开评估框架。

Abstract: Large language models (LLMs) have demonstrated capabilities across diverse domains, yet their performance on rare disease diagnosis from narrative medical cases remains underexplored. We introduce a novel dataset of 176 symptom-diagnosis pairs extracted from House M.D., a medical television series validated for teaching rare disease recognition in medical education. We evaluate four state-of-the-art LLMs such as GPT 4o mini, GPT 5 mini, Gemini 2.5 Flash, and Gemini 2.5 Pro on narrative-based diagnostic reasoning tasks. Results show significant variation in performance, ranging from 16.48% to 38.64% accuracy, with newer model generations demonstrating a 2.3 times improvement. While all models face substantial challenges with rare disease diagnosis, the observed improvement across architectures suggests promising directions for future development. Our educationally validated benchmark establishes baseline performance metrics for narrative medical reasoning and provides a publicly accessible evaluation framework for advancing AI-assisted diagnosis research.

</details>


### [49] [CardioEmbed: Domain-Specialized Text Embeddings for Clinical Cardiology](https://arxiv.org/abs/2511.10930)
*Richard J. Young,Alice M. Matthews*

Main category: cs.CL

TL;DR: CardioEmbed是基于Qwen3-Embedding-8B训练的心脏病学专业嵌入模型，在7本心脏病学教科书上使用对比学习，在心脏病学语义检索任务中达到99.60%的检索准确率，比当前最佳医学嵌入模型MedTE提升了15.94个百分点。


<details>
  <summary>Details</summary>
Motivation: 现有生物医学文本嵌入模型主要基于PubMed研究文献训练，但临床心脏病实践更依赖教科书中的程序性知识和专业术语，这种研究实践差距限制了现有模型在临床心脏病应用中的效果。

Method: 基于Qwen3-Embedding-8B模型，使用对比学习方法在7本心脏病学教科书（约15万句去重后文本）上训练，采用InfoNCE损失函数和批次内负样本策略。

Result: 在心脏病学特定语义检索任务中达到99.60%检索准确率，比MedTE提升15.94个百分点；在MTEB医学基准测试中，BIOSSES得分为0.77 Spearman，SciFact得分为0.61 NDCG@10。

Conclusion: 在综合性临床教科书上进行领域专业化训练能够实现近乎完美的心脏病学检索性能，显著优于现有医学嵌入模型。

Abstract: Biomedical text embeddings have primarily been developed using research literature from PubMed, yet clinical cardiology practice relies heavily on procedural knowledge and specialized terminology found in comprehensive textbooks rather than research abstracts. This research practice gap limits the effectiveness of existing embedding models for clinical applications incardiology. This study trained CardioEmbed, a domain-specialized embedding model based on Qwen3-Embedding-8B, using contrastive learning on a curated corpus of seven comprehensive cardiology textbooks totaling approximately 150,000 sentences after deduplication. The model employs InfoNCE loss with in-batch negatives and achieves 99.60% retrieval accuracy on cardiac-specific semantic retrieval tasks, a +15.94 percentage point improvement over MedTE, the current state-of-the-art medical embedding model. On MTEB medical benchmarks, the model obtained BIOSSES 0.77 Spearman and SciFact 0.61 NDCG@10, indicating competitive performance on related biomedical domains. Domain-specialized training on comprehensive clinical textbooks yields near-perfect cardiology retrieval (99.60% Acc@1), improving over MedTE by +15.94 percentage points.

</details>


### [50] [DiscoX: Benchmarking Discourse-Level Translation task in Expert Domains](https://arxiv.org/abs/2511.10984)
*Xiying Zhao,Zhoufutu Wen,Zhixuan Chen,Jingzhe Ding,Jianpeng Jiao,Shuai Li,Xi Li,Danni Liang,Shengda Long,Qianqian Liu,Xianbo Wu,Hongwan Gao,Xiang Gao,Liang Hu,Jiashuo Liu,Mengyun Liu,Weiran Shi,Chenghao Yang,Qianyu Yang,Xuanliang Zhang,Ge Zhang,Wenhao Huang*

Main category: cs.CL

TL;DR: DiscoX是一个用于评估专业领域中文-英文翻译的新基准，包含7个领域的200篇专业文本，平均长度超过1700词。同时开发了无参考评估系统Metric-S，在准确性、流畅性和适当性方面提供细粒度自动评估。实验显示即使最先进的LLM在专业翻译任务上仍远落后于人类专家。


<details>
  <summary>Details</summary>
Motivation: 当前翻译评估方法主要关注句子级别的准确性和流畅性，无法充分评估专业领域中需要语篇连贯性和严格术语准确性的翻译质量。

Method: 构建DiscoX基准数据集（200篇专业文本，7个领域，平均1700+词），开发Metric-S无参考评估系统，进行LLM与人类专家的翻译性能对比实验。

Result: Metric-S与人工评估具有强一致性，显著优于现有指标；实验发现最先进的LLM在专业翻译任务上仍明显落后于人类专家。

Conclusion: DiscoX基准和Metric-S评估系统为更严格的翻译评估提供了稳健框架，验证了实现专业级机器翻译仍面临重大挑战。

Abstract: The evaluation of discourse-level translation in expert domains remains inadequate, despite its centrality to knowledge dissemination and cross-lingual scholarly communication. While these translations demand discourse-level coherence and strict terminological precision, current evaluation methods predominantly focus on segment-level accuracy and fluency. To address this limitation, we introduce DiscoX, a new benchmark for discourse-level and expert-level Chinese-English translation. It comprises 200 professionally-curated texts from 7 domains, with an average length exceeding 1700 tokens. To evaluate performance on DiscoX, we also develop Metric-S, a reference-free system that provides fine-grained automatic assessments across accuracy, fluency, and appropriateness. Metric-S demonstrates strong consistency with human judgments, significantly outperforming existing metrics. Our experiments reveal a remarkable performance gap: even the most advanced LLMs still trail human experts on these tasks. This finding validates the difficulty of DiscoX and underscores the challenges that remain in achieving professional-grade machine translation. The proposed benchmark and evaluation system provide a robust framework for more rigorous evaluation, facilitating future advancements in LLM-based translation.

</details>


### [51] [When Data is the Algorithm: A Systematic Study and Curation of Preference Optimization Datasets](https://arxiv.org/abs/2511.10985)
*Aladin Djuhera,Farhan Ahmed,Swanand Ravindra Kadhe,Syed Zawad,Heiko Ludwig,Holger Boche*

Main category: cs.CL

TL;DR: 本文对主流开源DPO数据集进行了首次全面分析，使用Magpie框架标注样本质量，并基于分析结果构建了新的DPO混合数据集UltraMix，该数据集比最佳单个数据集小30%但性能更优。


<details>
  <summary>Details</summary>
Motivation: 目前大多数前沿LLM不公开其偏好对数据，而开源DPO数据集缺乏系统比较，难以理解偏好选择方式、任务覆盖范围以及人类判断的反映程度。

Method: 使用Magpie框架对每个样本进行任务类别、输入质量和偏好奖励的标注，通过奖励模型验证偏好顺序，实现可扩展的细粒度质量检查。基于分析结果从五个语料库中选择性构建新的DPO混合数据集UltraMix。

Result: 揭示了不同数据集在奖励边界上的结构和质量差异。UltraMix比性能最佳的单个数据集小30%，但在关键基准测试中表现更优。

Conclusion: 通过数据中心的偏好优化方法，可以构建更小但性能更好的DPO数据集，为未来的数据中心偏好优化研究提供了有价值的资源。

Abstract: Aligning large language models (LLMs) is a central objective of post-training, often achieved through reward modeling and reinforcement learning methods. Among these, direct preference optimization (DPO) has emerged as a widely adopted technique that fine-tunes LLMs on preferred completions over less favorable ones. While most frontier LLMs do not disclose their curated preference pairs, the broader LLM community has released several open-source DPO datasets, including TuluDPO, ORPO, UltraFeedback, HelpSteer, and Code-Preference-Pairs. However, systematic comparisons remain scarce, largely due to the high computational cost and the lack of rich quality annotations, making it difficult to understand how preferences were selected, which task types they span, and how well they reflect human judgment on a per-sample level. In this work, we present the first comprehensive, data-centric analysis of popular open-source DPO corpora. We leverage the Magpie framework to annotate each sample for task category, input quality, and preference reward, a reward-model-based signal that validates the preference order without relying on human annotations. This enables a scalable, fine-grained inspection of preference quality across datasets, revealing structural and qualitative discrepancies in reward margins. Building on these insights, we systematically curate a new DPO mixture, UltraMix, that draws selectively from all five corpora while removing noisy or redundant samples. UltraMix is 30% smaller than the best-performing individual dataset yet exceeds its performance across key benchmarks. We publicly release all annotations, metadata, and our curated mixture to facilitate future research in data-centric preference optimization.

</details>


### [52] [Automata-Based Steering of Large Language Models for Diverse Structured Generation](https://arxiv.org/abs/2511.11018)
*Xiaokun Luan,Zeming Wei,Yihao Zhang,Meng Sun*

Main category: cs.CL

TL;DR: 提出一种基于自动机遍历历史的新方法，用于增强结构化生成中的输出多样性，在保持生成效率的同时显著提升结构和内容多样性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型越来越多地用于生成结构化输出，但现有结构化生成方法虽然确保有效性，却缺乏输出多样性，这是一个关键限制。

Method: 利用自动机遍历历史来引导LLMs朝向新颖的结构模式，通过追踪自动机状态转换历史来促进多样性生成。

Result: 评估显示该方法显著改善了结构和内容多样性，同时保持了可比的生成效率。案例研究展示了在生成开源库测试用例方面的有效性。

Conclusion: 该方法成功解决了结构化生成中的多样性不足问题，为LLMs在需要多样化输出的应用场景中提供了实用解决方案。

Abstract: Large language models (LLMs) are increasingly tasked with generating structured outputs. While structured generation methods ensure validity, they often lack output diversity, a critical limitation that we confirm in our preliminary study. We propose a novel method to enhance diversity in automaton-based structured generation. Our approach utilizes automata traversal history to steer LLMs towards novel structural patterns. Evaluations show our method significantly improves structural and content diversity while maintaining comparable generation efficiency. Furthermore, we conduct a case study showcasing the effectiveness of our method in generating diverse test cases for testing open-source libraries.

</details>


### [53] [Correcting Mean Bias in Text Embeddings: A Refined Renormalization with Training-Free Improvements on MMTEB](https://arxiv.org/abs/2511.11041)
*Xingyu Ren,Youran Sun,Haoyu Liang*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We find that current text embedding models produce outputs with a consistent bias, i.e., each embedding vector $e$ can be decomposed as $\tilde{e} + μ$, where $μ$ is almost identical across all sentences. We propose a plug-and-play, training-free and lightweight solution called Renormalization. Through extensive experiments, we show that renormalization consistently and statistically significantly improves the performance of existing models on the Massive Multilingual Text Embedding Benchmark (MMTEB). In particular, across 38 models, renormalization improves performance by 9.7 $σ$ on retrieval tasks, 3.1 $σ$ on classification tasks, and 0.8 $σ$ on other types of tasks. Renormalization has two variants: directly subtracting $μ$ from $e$, or subtracting the projection of $e$ onto $μ$. We theoretically predict that the latter performs better, and our experiments confirm this prediction.

</details>


### [54] [Can LLMs Detect Their Own Hallucinations?](https://arxiv.org/abs/2511.11087)
*Sora Kadotani,Kosuke Nishida,Kyosuke Nishida*

Main category: cs.CL

TL;DR: LLMs can detect their own hallucinations using Chain-of-Thought prompting, achieving 58.2% detection rate with GPT-3.5 Turbo.


<details>
  <summary>Details</summary>
Motivation: Large language models sometimes generate factually incorrect responses (hallucinations), so researchers want to investigate whether LLMs can detect their own hallucinations.

Method: Formulate hallucination detection as classification task, propose framework to estimate LLM capability, and use Chain-of-Thought prompting to extract knowledge from model parameters.

Result: GPT-3.5 Turbo with Chain-of-Thought detected 58.2% of its own hallucinations.

Conclusion: LLMs with Chain-of-Thought can detect hallucinations if sufficient knowledge is contained in their parameters.

Abstract: Large language models (LLMs) can generate fluent responses, but sometimes hallucinate facts. In this paper, we investigate whether LLMs can detect their own hallucinations. We formulate hallucination detection as a classification task of a sentence. We propose a framework for estimating LLMs' capability of hallucination detection and a classification method using Chain-of-Thought (CoT) to extract knowledge from their parameters. The experimental results indicated that GPT-$3.5$ Turbo with CoT detected $58.2\%$ of its own hallucinations. We concluded that LLMs with CoT can detect hallucinations if sufficient knowledge is contained in their parameters.

</details>


### [55] [Analysing Personal Attacks in U.S. Presidential Debates](https://arxiv.org/abs/2511.11108)
*Ruban Goyal,Rohitash Chandra,Sonit Singh*

Main category: cs.CL

TL;DR: 提出一个分析美国总统辩论中人身攻击的框架，通过手动标注2016、2020和2024年选举周期的辩论记录，结合统计和语言模型分析，研究微调transformer模型和通用LLM检测正式政治演讲中人身攻击的能力。


<details>
  <summary>Details</summary>
Motivation: 人身攻击已成为美国总统辩论的显著特征，在选举中塑造公众认知方面发挥重要作用。检测此类攻击可以提高政治话语的透明度，为记者、分析师和公众提供见解。深度学习和transformer模型的发展为有害语言的自动检测创造了新机会。

Method: 手动标注2016、2020和2024年选举周期的辩论记录，然后进行统计和基于语言模型的分析。研究微调transformer模型和通用大语言模型在检测正式政治演讲中人身攻击的潜力。

Result: 展示了任务特定的现代语言模型适应如何有助于更深入地理解政治传播。

Conclusion: 任务特定的现代语言模型适应可以为政治传播的深入理解做出贡献，为政治话语分析提供了有效的技术框架。

Abstract: Personal attacks have become a notable feature of U.S. presidential debates and play an important role in shaping public perception during elections. Detecting such attacks can improve transparency in political discourse and provide insights for journalists, analysts and the public. Advances in deep learning and transformer-based models, particularly BERT and large language models (LLMs) have created new opportunities for automated detection of harmful language. Motivated by these developments, we present a framework for analysing personal attacks in U.S. presidential debates. Our work involves manual annotation of debate transcripts across the 2016, 2020 and 2024 election cycles, followed by statistical and language-model based analysis. We investigate the potential of fine-tuned transformer models alongside general-purpose LLMs to detect personal attacks in formal political speech. This study demonstrates how task-specific adaptation of modern language models can contribute to a deeper understanding of political communication.

</details>


### [56] [AV-Dialog: Spoken Dialogue Models with Audio-Visual Input](https://arxiv.org/abs/2511.11124)
*Tuochao Chen,Bandhav Veluri,Hongyu Gong,Shyamnath Gollakota*

Main category: cs.CL

TL;DR: AV-Dialog是首个多模态对话框架，利用音频和视觉线索追踪目标说话者、预测话轮转换并生成连贯响应，在嘈杂环境中优于纯音频模型。


<details>
  <summary>Details</summary>
Motivation: 现有对话模型在嘈杂多说话者环境中表现不佳，经常产生不相关响应和尴尬的话轮转换，需要更鲁棒的解决方案。

Method: 结合声学标记化和多任务多阶段训练，在单声道、合成和真实音频-视觉对话数据集上进行训练，实现流式转录、语义基础的话轮边界检测和准确响应。

Result: AV-Dialog在干扰条件下优于纯音频模型，减少转录错误，改进话轮转换预测，提升人类评级的对话质量。

Conclusion: 视觉和听觉结合对于说话者感知交互具有强大作用，为在真实世界嘈杂环境中鲁棒运行的语音对话代理铺平了道路。

Abstract: Dialogue models falter in noisy, multi-speaker environments, often producing irrelevant responses and awkward turn-taking. We present AV-Dialog, the first multimodal dialog framework that uses both audio and visual cues to track the target speaker, predict turn-taking, and generate coherent responses. By combining acoustic tokenization with multi-task, multi-stage training on monadic, synthetic, and real audio-visual dialogue datasets, AV-Dialog achieves robust streaming transcription, semantically grounded turn-boundary detection and accurate responses, resulting in a natural conversational flow. Experiments show that AV-Dialog outperforms audio-only models under interference, reducing transcription errors, improving turn-taking prediction, and enhancing human-rated dialogue quality. These results highlight the power of seeing as well as hearing for speaker-aware interaction, paving the way for {spoken} dialogue agents that perform {robustly} in real-world, noisy environments.

</details>


### [57] [Enhancing Meme Emotion Understanding with Multi-Level Modality Enhancement and Dual-Stage Modal Fusion](https://arxiv.org/abs/2511.11126)
*Yi Shi,Wenlong Meng,Zhenyuan Guo,Chengkun Wei,Wenzhi Chen*

Main category: cs.CL

TL;DR: MemoDetector是一个用于表情包情感理解(MEU)的新框架，通过四步文本增强模块和双阶段模态融合策略，显著提升了表情包情感分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有表情包情感理解方法面临两个主要挑战：缺乏细粒度多模态融合策略，以及未能充分挖掘表情包的隐含意义和背景知识。

Method: 提出四步文本增强模块利用多模态大语言模型推理提取隐含信息，并设计双阶段模态融合策略：第一阶段对原始图像和文本进行浅层融合，第二阶段深度整合增强后的视觉和文本特征。

Result: 在MET-MEME和MOOD数据集上的实验表明，MemoDetector在F1分数上分别提升了4.3%和3.4%，优于现有最先进方法。

Conclusion: 该方法通过增强文本信息和分层融合策略，有效提升了表情包情感理解的性能，验证了其有效性和鲁棒性。

Abstract: With the rapid rise of social media and Internet culture, memes have become a popular medium for expressing emotional tendencies. This has sparked growing interest in Meme Emotion Understanding (MEU), which aims to classify the emotional intent behind memes by leveraging their multimodal contents. While existing efforts have achieved promising results, two major challenges remain: (1) a lack of fine-grained multimodal fusion strategies, and (2) insufficient mining of memes' implicit meanings and background knowledge. To address these challenges, we propose MemoDetector, a novel framework for advancing MEU. First, we introduce a four-step textual enhancement module that utilizes the rich knowledge and reasoning capabilities of Multimodal Large Language Models (MLLMs) to progressively infer and extract implicit and contextual insights from memes. These enhanced texts significantly enrich the original meme contents and provide valuable guidance for downstream classification. Next, we design a dual-stage modal fusion strategy: the first stage performs shallow fusion on raw meme image and text, while the second stage deeply integrates the enhanced visual and textual features. This hierarchical fusion enables the model to better capture nuanced cross-modal emotional cues. Experiments on two datasets, MET-MEME and MOOD, demonstrate that our method consistently outperforms state-of-the-art baselines. Specifically, MemoDetector improves F1 scores by 4.3\% on MET-MEME and 3.4\% on MOOD. Further ablation studies and in-depth analyses validate the effectiveness and robustness of our approach, highlighting its strong potential for advancing MEU. Our code is available at https://github.com/singing-cat/MemoDetector.

</details>


### [58] [Speech-Aware Long Context Pruning and Integration for Contextualized Automatic Speech Recognition](https://arxiv.org/abs/2511.11139)
*Yiming Rong,Yixin Zhang,Ziyi Wang,Deyang Jiang,Yunlong Zhao,Haoran Wu,Shiyu Zhou,Bo Xu*

Main category: cs.CL

TL;DR: 提出SAP²方法，通过两阶段动态修剪和整合相关上下文关键词来解决ASR系统在长上下文场景中的性能问题，在SlideSpeech和LibriSpeech数据集上取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: ASR系统在常见条件下表现良好，但在需要领域特定知识的上下文场景（如会议演讲）中难以有效利用长上下文信息，主要受限于模型上下文窗口和大量上下文噪声中的相关信息稀疏性。

Method: SAP²框架采用两阶段动态修剪和整合相关上下文关键词，每个阶段使用提出的语音驱动注意力池化机制，有效压缩上下文嵌入同时保留语音显著性信息。

Result: 在SlideSpeech和LibriSpeech数据集上分别达到7.71%和1.12%的词错误率，在SlideSpeech上相比非上下文基线将偏置关键词错误率降低了41.1%，并在扩展上下文输入条件下保持稳健性能。

Conclusion: SAP²方法有效解决了ASR系统在长上下文场景中的挑战，通过动态上下文修剪和语音驱动池化机制显著提升了性能，展现了良好的可扩展性。

Abstract: Automatic speech recognition (ASR) systems have achieved remarkable performance in common conditions but often struggle to leverage long-context information in contextualized scenarios that require domain-specific knowledge, such as conference presentations. This challenge arises primarily due to constrained model context windows and the sparsity of relevant information within extensive contextual noise. To solve this, we propose the SAP$^{2}$ method, a novel framework that dynamically prunes and integrates relevant contextual keywords in two stages. Specifically, each stage leverages our proposed Speech-Driven Attention-based Pooling mechanism, enabling efficient compression of context embeddings while preserving speech-salient information. Experimental results demonstrate state-of-the-art performance of SAP$^{2}$ on the SlideSpeech and LibriSpeech datasets, achieving word error rates (WER) of 7.71% and 1.12%, respectively. On SlideSpeech, our method notably reduces biased keyword error rates (B-WER) by 41.1% compared to non-contextual baselines. SAP$^{2}$ also exhibits robust scalability, consistently maintaining performance under extensive contextual input conditions on both datasets.

</details>


### [59] [PRSM: A Measure to Evaluate CLIP's Robustness Against Paraphrases](https://arxiv.org/abs/2511.11141)
*Udo Schlegel,Franziska Weeber,Jian Lan,Thomas Seidl*

Main category: cs.CL

TL;DR: 提出了PRSM指标来量化CLIP模型对改写查询的敏感性，发现在社会反事实数据集上，CLIP对改写的鲁棒性因改写策略而异，且在男性和女性相关查询间存在微妙但一致的差异。


<details>
  <summary>Details</summary>
Motivation: CLIP模型在零样本和少样本任务上表现良好，但其对语言变体（特别是改写）的鲁棒性尚未充分探索。改写鲁棒性对于可靠部署至关重要，尤其是在社会敏感情境中，不一致的表征可能放大人口统计偏差。

Method: 引入Paraphrase Ranking Stability Metric (PRSM)指标，使用Social Counterfactuals数据集评估CLIP在改写变化下的稳定性，分析改写鲁棒性与性别的交互作用。

Result: 鲁棒性在不同改写策略间存在差异，在男性和女性相关查询间观察到微妙但一致的差异。

Conclusion: CLIP的改写鲁棒性存在变化，这对多模态系统的公平性和公平部署具有重要影响。

Abstract: Contrastive Language-Image Pre-training (CLIP) is a widely used multimodal model that aligns text and image representations through large-scale training. While it performs strongly on zero-shot and few-shot tasks, its robustness to linguistic variation, particularly paraphrasing, remains underexplored. Paraphrase robustness is essential for reliable deployment, especially in socially sensitive contexts where inconsistent representations can amplify demographic biases. In this paper, we introduce the Paraphrase Ranking Stability Metric (PRSM), a novel measure for quantifying CLIP's sensitivity to paraphrased queries. Using the Social Counterfactuals dataset, a benchmark designed to reveal social and demographic biases, we empirically assess CLIP's stability under paraphrastic variation, examine the interaction between paraphrase robustness and gender, and discuss implications for fairness and equitable deployment of multimodal systems. Our analysis reveals that robustness varies across paraphrasing strategies, with subtle yet consistent differences observed between male- and female-associated queries.

</details>


### [60] [Adverbs Revisited: Enhancing WordNet Coverage of Adverbs with a Supersense Taxonomy](https://arxiv.org/abs/2511.11214)
*Jooyoung Lee,Jader Martins Camboim de Sá*

Main category: cs.CL

TL;DR: 提出了一个针对副词的语义分类体系，扩展了WordNet的覆盖范围，并通过标注研究验证了该分类的可靠性和实用性。


<details>
  <summary>Details</summary>
Motivation: WordNet为名词和动词提供了丰富的语义层次结构，但副词的语义分类系统不完善，缺乏系统的语义分类框架。

Method: 引入基于语言学理论的副词超义类型学，通过标注研究进行实证验证，涵盖方式、时间、频率、程度、领域、说话者导向和主语导向等主要语义域。

Result: 初步标注研究表明，这些类别能够广泛覆盖自然文本中的副词，并且人类标注者能够可靠地进行分类。

Conclusion: 该分类体系扩展了WordNet的覆盖范围，使其更符合语言学理论，并有助于词义消歧、事件抽取、情感分析和语篇建模等下游NLP应用。

Abstract: WordNet offers rich supersense hierarchies for nouns and verbs, yet adverbs remain underdeveloped, lacking a systematic semantic classification. We introduce a linguistically grounded supersense typology for adverbs, empirically validated through annotation, that captures major semantic domains including manner, temporal, frequency, degree, domain, speaker-oriented, and subject-oriented functions. Results from a pilot annotation study demonstrate that these categories provide broad coverage of adverbs in natural text and can be reliably assigned by human annotators. Incorporating this typology extends WordNet's coverage, aligns it more closely with linguistic theory, and facilitates downstream NLP applications such as word sense disambiguation, event extraction, sentiment analysis, and discourse modeling. We present the proposed supersense categories, annotation outcomes, and directions for future work.

</details>


### [61] [LANE: Lexical Adversarial Negative Examples for Word Sense Disambiguation](https://arxiv.org/abs/2511.11234)
*Jader Martins Camboim de Sá,Jooyoung Lee,Cédric Pruski,Marcos Da Silveira*

Main category: cs.CL

TL;DR: LANE是一种新颖的对抗训练策略，通过选择性标记训练集中的替代词来生成具有挑战性的负训练样本，旨在提高神经语言模型在细粒度词义消歧方面的性能。


<details>
  <summary>Details</summary>
Motivation: 神经语言模型往往过度拟合全局句子表示，无法捕捉局部语义细节，这导致在细粒度词义消解方面存在关键挑战。

Method: 提出LANE对抗训练策略，通过选择性标记训练集中的替代词生成负训练样本，迫使模型在不同标记词的相同句子之间创建更大的可分性。

Result: 在词汇语义变化检测和词义消歧基准测试中，该方法产生了更具区分性的词表示，性能优于标准对比学习基线。

Conclusion: LANE方法能够生成更好的词表示以捕捉细微意义差异，且具有模型无关性，可集成到现有表示学习框架中。

Abstract: Fine-grained word meaning resolution remains a critical challenge for neural language models (NLMs) as they often overfit to global sentence representations, failing to capture local semantic details. We propose a novel adversarial training strategy, called LANE, to address this limitation by deliberately shifting the model's learning focus to the target word. This method generates challenging negative training examples through the selective marking of alternate words in the training set. The goal is to force the model to create a greater separability between same sentences with different marked words. Experimental results on lexical semantic change detection and word sense disambiguation benchmarks demonstrate that our approach yields more discriminative word representations, improving performance over standard contrastive learning baselines. We further provide qualitative analyses showing that the proposed negatives lead to representations that better capture subtle meaning differences even in challenging environments. Our method is model-agnostic and can be integrated into existing representation learning frameworks.

</details>


### [62] [KGQuest: Template-Driven QA Generation from Knowledge Graphs with LLM-Based Refinement](https://arxiv.org/abs/2511.11258)
*Sania Nayab,Marco Simoni,Giulio Rossolini,Andrea Saracino*

Main category: cs.CL

TL;DR: 提出了一种可扩展的确定性流水线，用于从知识图谱生成自然语言问答对，结合LLM提升语言质量


<details>
  <summary>Details</summary>
Motivation: 现有方法在可扩展性、语言质量和事实一致性方面存在不足，需要改进知识图谱问答生成的质量和效率

Method: 首先基于关系聚类KG三元组，创建可重用模板；然后使用LLM优化模板；最后通过从KG中选择干扰项来实例化答案选项

Result: 实验证明这种混合方法能高效生成高质量的问答对，兼具可扩展性、流畅性和语言精确性

Conclusion: 该方法成功结合了确定性流水线的可扩展性与LLM的语言优化能力，为教育平台和LLM开发提供了高质量的问答数据

Abstract: The generation of questions and answers (QA) from knowledge graphs (KG) plays a crucial role in the development and testing of educational platforms, dissemination tools, and large language models (LLM). However, existing approaches often struggle with scalability, linguistic quality, and factual consistency. This paper presents a scalable and deterministic pipeline for generating natural language QA from KGs, with an additional refinement step using LLMs to further enhance linguistic quality. The approach first clusters KG triplets based on their relations, creating reusable templates through natural language rules derived from the entity types of objects and relations. A module then leverages LLMs to refine these templates, improving clarity and coherence while preserving factual accuracy. Finally, the instantiation of answer options is achieved through a selection strategy that introduces distractors from the KG. Our experiments demonstrate that this hybrid approach efficiently generates high-quality QA pairs, combining scalability with fluency and linguistic precision.

</details>


### [63] [iMAD: Intelligent Multi-Agent Debate for Efficient and Accurate LLM Inference](https://arxiv.org/abs/2511.11306)
*Wei Fan,JinYi Yoon,Bo Ji*

Main category: cs.CL

TL;DR: iMAD是一个智能多智能体辩论框架，通过选择性触发辩论来显著减少计算成本，同时提高答案准确性。


<details>
  <summary>Details</summary>
Motivation: 传统多智能体辩论(MAD)框架对每个查询都进行辩论，计算成本高且可能推翻正确的单智能体答案，需要更高效的触发机制。

Method: iMAD首先让单智能体生成结构化自我批判响应，提取41个可解释的语言和语义特征，然后使用轻量级辩论决策分类器决定是否触发MAD。

Result: 在六个视觉问答数据集上的实验显示，iMAD显著减少了token使用量(最多92%)，同时提高了最终答案准确率(最多13.5%)。

Conclusion: iMAD通过智能选择性地触发辩论，在保持准确性的同时大幅降低了计算成本，为多智能体系统提供了高效的解决方案。

Abstract: Large Language Model (LLM) agent systems have advanced rapidly, driven by their strong generalization in zero-shot settings. To further enhance reasoning and accuracy on complex tasks, Multi-Agent Debate (MAD) has emerged as a promising framework that engages multiple LLM agents in structured debates to encourage diverse reasoning. However, triggering MAD for every query is inefficient, as it incurs substantial computational (token) cost and may even degrade accuracy by overturning correct single-agent answers. To address these limitations, we propose intelligent Multi-Agent Debate (iMAD), a token-efficient framework that selectively triggers MAD only when it is likely to be beneficial (i.e., correcting an initially wrong answer). To achieve this goal, iMAD learns generalizable model behaviors to make accurate debate decisions. Specifically, iMAD first prompts a single agent to produce a structured self-critique response, from which we extract 41 interpretable linguistic and semantic features capturing hesitation cues. Then, iMAD uses a lightweight debate-decision classifier, trained using our proposed FocusCal loss, to determine whether to trigger MAD, enabling robust debate decisions without test dataset-specific tuning. Through extensive experiments using six (visual) question answering datasets against five competitive baselines, we have shown that iMAD significantly reduces token usage (by up to 92%) while also improving final answer accuracy (by up to 13.5%).

</details>


### [64] [destroR: Attacking Transfer Models with Obfuscous Examples to Discard Perplexity](https://arxiv.org/abs/2511.11309)
*Saadat Rafid Ahmed,Rubayet Shareen,Radoan Sharkar,Nazia Hossain,Mansur Mahi,Farig Yousuf Sadeque*

Main category: cs.CL

TL;DR: 本文提出了一种针对最先进机器学习模型的新型对抗攻击策略，通过生成模糊输入来混淆模型，并探索提高模型鲁棒性的未来发展路径。


<details>
  <summary>Details</summary>
Motivation: 近年来机器学习模型被发现存在多种漏洞，这些漏洞使模型及其应用系统面临风险。本文旨在分析现有对抗攻击方法并创建新的攻击策略。

Method: 开发具有最大困惑度的对抗样本，利用机器学习和深度学习方法欺骗模型。分析多个数据集，专注于创建模糊的对抗样本来使模型陷入困惑状态，并将孟加拉语纳入对抗攻击领域。

Result: 未在摘要中明确说明具体实验结果，但提出了攻击策略和方法的框架。

Conclusion: 通过开发新型对抗攻击策略，为未来提高机器学习模型鲁棒性的发展路径奠定了基础，并在工作中严格遵循降低效用和提高效率的原则。

Abstract: Advancements in Machine Learning & Neural Networks in recent years have led to widespread implementations of Natural Language Processing across a variety of fields with remarkable success, solving a wide range of complicated problems. However, recent research has shown that machine learning models may be vulnerable in a number of ways, putting both the models and the systems theyre used in at risk. In this paper, we intend to analyze and experiment with the best of existing adversarial attack recipes and create new ones. We concentrated on developing a novel adversarial attack strategy on current state-of-the-art machine learning models by producing ambiguous inputs for the models to confound them and then constructing the path to the future development of the robustness of the models. We will develop adversarial instances with maximum perplexity, utilizing machine learning and deep learning approaches in order to trick the models. In our attack recipe, we will analyze several datasets and focus on creating obfuscous adversary examples to put the models in a state of perplexity, and by including the Bangla Language in the field of adversarial attacks. We strictly uphold utility usage reduction and efficiency throughout our work.

</details>


### [65] [LAET: A Layer-wise Adaptive Ensemble Tuning Framework for Pretrained Language Models](https://arxiv.org/abs/2511.11315)
*Jawad Ibn Ahad,Muhammad Rafsan Kabir,Robin Krambroeckers,Sifat Momen,Nabeel Mohammed,Shafin Rahman*

Main category: cs.CL

TL;DR: 提出了LAET（层自适应集成调优）方法，通过选择性微调预训练LLMs中最有效的层来降低计算成本，在金融NLP任务中表现出色


<details>
  <summary>Details</summary>
Motivation: 现有金融LLMs（如BloombergGPT、FinMA）计算需求高，限制了组织可访问性，需要更高效的模型部署方案

Method: LAET策略：分析隐藏状态表示，选择性微调最有效层，冻结次要层，显著降低计算开销

Result: 在金融NLP任务中表现优异，超越现有基准和GPT-4等SOTA LLMs，即使使用较小模型（约30亿参数）

Conclusion: LAET方法连接了前沿金融NLP研究与实际部署，为金融应用提供了高效可扩展的模型解决方案

Abstract: Natural Language Processing (NLP) has transformed the financial industry, enabling advancements in areas such as textual analysis, risk management, and forecasting. Large language models (LLMs) like BloombergGPT and FinMA have set new benchmarks across various financial NLP tasks, including sentiment analysis, stock movement prediction, and credit risk assessment. Furthermore, FinMA-ES, a bilingual financial LLM, has also demonstrated strong performance using the FLARE and FLARE-ES benchmarks. However, the high computational demands of these models limit the accessibility of many organizations. To address this, we propose Layer-wise Adaptive Ensemble Tuning (LAET), a novel strategy that selectively fine-tunes the most effective layers of pre-trained LLMs by analyzing hidden state representations while freezing less critical layers. LAET significantly reduces computational overhead while enhancing task-specific performance. Our approach shows strong results in financial NLP tasks, outperforming existing benchmarks and state-of-the-art LLMs such as GPT-4, even with smaller LLMs ($\sim$3B parameters). This work bridges cutting-edge financial NLP research and real-world deployment with efficient and scalable models for financial applications.

</details>


### [66] [NOVA: An Agentic Framework for Automated Histopathology Analysis and Discovery](https://arxiv.org/abs/2511.11324)
*Anurag J. Vaidya,Felix Meissen,Daniel C. Castro,Shruthi Bannur,Tristan Lazard,Drew F. K. Williamson,Faisal Mahmood,Javier Alvarez-Valle,Stephanie L. Hyland,Kenza Bouzid*

Main category: cs.CL

TL;DR: NOVA是一个代理框架，可将科学查询转化为可执行的分析流水线，通过迭代生成和运行Python代码，集成49个领域特定工具，并在SlideQuest基准测试中优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 数字化病理分析工作流程复杂、耗时且需要专业知识，限制了其可访问性。

Method: NOVA通过迭代生成和运行Python代码，将科学查询转化为可执行分析流水线，集成49个领域特定工具（如细胞核分割、全玻片编码），并能临时创建新工具。

Result: 在SlideQuest基准测试（90个问题）中，NOVA优于编码代理基线方法；病理学家验证的案例研究显示其能够将形态学与预后相关的PAM50亚型联系起来。

Conclusion: NOVA展示了可扩展的发现潜力，能够处理需要多步推理、迭代编码和计算问题解决的复杂病理分析任务。

Abstract: Digitized histopathology analysis involves complex, time-intensive workflows and specialized expertise, limiting its accessibility. We introduce NOVA, an agentic framework that translates scientific queries into executable analysis pipelines by iteratively generating and running Python code. NOVA integrates 49 domain-specific tools (e.g., nuclei segmentation, whole-slide encoding) built on open-source software, and can also create new tools ad hoc. To evaluate such systems, we present SlideQuest, a 90-question benchmark -- verified by pathologists and biomedical scientists -- spanning data processing, quantitative analysis, and hypothesis testing. Unlike prior biomedical benchmarks focused on knowledge recall or diagnostic QA, SlideQuest demands multi-step reasoning, iterative coding, and computational problem solving. Quantitative evaluation shows NOVA outperforms coding-agent baselines, and a pathologist-verified case study links morphology to prognostically relevant PAM50 subtypes, demonstrating its scalable discovery potential.

</details>


### [67] [LaoBench: A Large-Scale Multidimensional Lao Benchmark for Large Language Models](https://arxiv.org/abs/2511.11334)
*Jian Gao,Richeng Xuan,Zhaolu Kang,Dingshi Liao,Wenxin Huang,Zongmou Huang,Yangdi Xu,Bowen Qin,Zheqi He,Xi Yang,Changjin Li*

Main category: cs.CL

TL;DR: LaoBench是首个针对老挝语的大规模、高质量、多维度基准数据集，包含17,000多个样本，涵盖知识应用、K12基础教育和多语言翻译三个维度，用于评估LLM在老挝语上的综合语言理解和推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在低资源语言（特别是东南亚语言如老挝语）上的评估不足，需要填补这一空白。

Method: 构建包含17,000多个样本的数据集，分为开源和闭源子集，采用专家人工策划与自动化智能体辅助验证相结合的数据构建流程。

Result: 在LaoBench上对多个最先进LLM进行基准测试显示，当前模型在掌握老挝语的多样化任务方面仍面临重大挑战。

Conclusion: LaoBench有望推动针对代表性不足的东南亚语言的AI技术的进一步研究和发展。

Abstract: The rapid advancement of large language models (LLMs) has not been matched by their evaluation in low-resource languages, especially Southeast Asian languages like Lao. To fill this gap, we introduce LaoBench, the first large-scale, high-quality, and multidimensional benchmark dataset dedicated to assessing LLMs' comprehensive language understanding and reasoning abilities in Lao. LaoBench comprises over 17,000 carefully curated samples spanning three core dimensions: knowledge application, K12 foundational education, and bilingual translation among Lao, Chinese, and English. The dataset is divided into open-source and closed-source subsets, with the closed-source portion enabling black-box evaluation on an official platform to ensure fairness and data security. Our data construction pipeline integrates expert human curation with automated agent-assisted verification, ensuring linguistic accuracy, cultural relevance, and educational value. Benchmarking multiple state-of-the-art LLMs on LaoBench reveals that current models still face significant challenges in mastering Lao across diverse tasks. We hope LaoBench will catalyze further research and development of AI technologies for underrepresented Southeast Asian languages.

</details>


### [68] [M-DAIGT: A Shared Task on Multi-Domain Detection of AI-Generated Text](https://arxiv.org/abs/2511.11340)
*Salima Lamsiyah,Saad Ezzini,Abdelkader El Mahdaouy,Hamza Alami,Abdessamad Benlahbib,Samir El Amrany,Salmane Chafik,Hicham Hammouchi*

Main category: cs.CL

TL;DR: M-DAIGT共享任务专注于在新闻文章和学术写作中检测AI生成文本，包含两个子任务，发布了包含3万样本的大规模基准数据集，有46个团队注册但只有4个提交最终结果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型生成高度流畅文本对信息完整性和学术研究构成重大挑战，需要开发跨多个领域的AI生成文本检测方法。

Method: 设计了两个二元分类子任务：新闻文章检测和学术写作检测，构建了包含3万样本的平衡数据集，使用多种现代LLM和提示策略生成AI内容。

Result: 46个团队注册参与，但只有4个团队提交了最终结果，所有4个团队都参与了两个子任务。

Conclusion: 论文描述了参与团队使用的方法，并简要讨论了M-DAIGT的未来发展方向。

Abstract: The generation of highly fluent text by Large Language Models (LLMs) poses a significant challenge to information integrity and academic research. In this paper, we introduce the Multi-Domain Detection of AI-Generated Text (M-DAIGT) shared task, which focuses on detecting AI-generated text across multiple domains, particularly in news articles and academic writing. M-DAIGT comprises two binary classification subtasks: News Article Detection (NAD) (Subtask 1) and Academic Writing Detection (AWD) (Subtask 2). To support this task, we developed and released a new large-scale benchmark dataset of 30,000 samples, balanced between human-written and AI-generated texts. The AI-generated content was produced using a variety of modern LLMs (e.g., GPT-4, Claude) and diverse prompting strategies. A total of 46 unique teams registered for the shared task, of which four teams submitted final results. All four teams participated in both Subtask 1 and Subtask 2. We describe the methods employed by these participating teams and briefly discuss future directions for M-DAIGT.

</details>


### [69] [Studies with impossible languages falsify LMs as models of human language](https://arxiv.org/abs/2511.11389)
*Jeffrey S. Bowers,Jeff Mitchell*

Main category: cs.CL

TL;DR: 语言模型和婴儿都能学习自然语言，但语言模型缺乏人类特有的归纳偏置，导致它们对不可能语言的学习能力与人类不同。


<details>
  <summary>Details</summary>
Motivation: 探讨语言模型与人类婴儿在语言学习能力上的差异，特别是对自然语言和不可能语言的学习表现。

Method: 回顾文献分析，比较语言模型和人类婴儿对自然语言和不可能语言的学习能力。

Result: 语言模型通常能同等学习自然语言和许多不可能语言，而难以学习的不可能语言只是更复杂或随机。

Conclusion: 语言模型缺少支持人类语言习得的归纳偏置，这是它们与人类婴儿语言学习能力差异的关键原因。

Abstract: According to Futrell and Mahowald [arXiv:2501.17047], both infants and language models (LMs) find attested languages easier to learn than impossible languages that have unnatural structures. We review the literature and show that LMs often learn attested and many impossible languages equally well. Difficult to learn impossible languages are simply more complex (or random). LMs are missing human inductive biases that support language acquisition.

</details>


### [70] [MajinBook: An open catalogue of digital world literature with likes](https://arxiv.org/abs/2511.11412)
*Antoine Mazières,Thierry Poibeau*

Main category: cs.CL

TL;DR: MajinBook是一个开放目录，通过整合影子图书馆和Goodreads的元数据，创建了包含53.9万本英语书籍的高精度语料库，支持计算社会科学和文化分析研究。


<details>
  <summary>Details</summary>
Motivation: 传统语料库如HathiTrust存在偏见，且缺乏机器可读性。影子图书馆虽然资源丰富但缺乏结构化元数据，需要整合来支持计算研究。

Method: 链接影子图书馆（Library Genesis、Z-Library）与Goodreads的元数据，优先选择原生数字EPUB文件，创建跨三个世纪的英语书籍语料库，包含出版日期、类型和流行度指标。

Result: 建立了包含539,000本英语书籍的高精度语料库，涵盖法语、德语和西班牙语的辅助数据集，并验证了链接策略的准确性。

Conclusion: MajinBook为计算社会科学提供了高质量的开放数据集，在法律允许范围内支持文本和数据挖掘研究。

Abstract: This data paper introduces MajinBook, an open catalogue designed to facilitate the use of shadow libraries--such as Library Genesis and Z-Library--for computational social science and cultural analytics. By linking metadata from these vast, crowd-sourced archives with structured bibliographic data from Goodreads, we create a high-precision corpus of over 539,000 references to English-language books spanning three centuries, enriched with first publication dates, genres, and popularity metrics like ratings and reviews. Our methodology prioritizes natively digital EPUB files to ensure machine-readable quality, while addressing biases in traditional corpora like HathiTrust, and includes secondary datasets for French, German, and Spanish. We evaluate the linkage strategy for accuracy, release all underlying data openly, and discuss the project's legal permissibility under EU and US frameworks for text and data mining in research.

</details>


### [71] [Proactive Hearing Assistants that Isolate Egocentric Conversations](https://arxiv.org/abs/2511.11473)
*Guilin Hu,Malek Itani,Tuochao Chen,Shyamnath Gollakota*

Main category: cs.CL

TL;DR: 提出了一种主动式听力助手系统，能够自动识别和分离佩戴者的对话伙伴，无需明确提示，基于自我语音作为锚点，利用轮换行为和对话动态来推断对话伙伴并抑制其他声音。


<details>
  <summary>Details</summary>
Motivation: 开发能够主动适应对话动态和参与度的听力助手，使其能够自动识别对话伙伴并分离相关音频，提升在多人对话环境中的听觉体验。

Method: 采用双模型架构：轻量级流式模型每12.5毫秒运行一次以实现低延迟提取对话伙伴，较慢模型运行频率较低以捕捉长距离对话动态。系统基于自我中心双耳音频，利用自我语音作为锚点。

Result: 在真实世界的2人和3人对话测试集上（来自11名参与者的6.8小时数据）显示，系统能够在多对话环境中有效识别和隔离对话伙伴，并具有良好的泛化能力。

Conclusion: 这项工作标志着向能够主动适应对话动态和参与度的听力助手迈出了一步，为改善多对话环境中的听觉体验提供了有效解决方案。

Abstract: We introduce proactive hearing assistants that automatically identify and separate the wearer's conversation partners, without requiring explicit prompts. Our system operates on egocentric binaural audio and uses the wearer's self-speech as an anchor, leveraging turn-taking behavior and dialogue dynamics to infer conversational partners and suppress others. To enable real-time, on-device operation, we propose a dual-model architecture: a lightweight streaming model runs every 12.5 ms for low-latency extraction of the conversation partners, while a slower model runs less frequently to capture longer-range conversational dynamics. Results on real-world 2- and 3-speaker conversation test sets, collected with binaural egocentric hardware from 11 participants totaling 6.8 hours, show generalization in identifying and isolating conversational partners in multi-conversation settings. Our work marks a step toward hearing assistants that adapt proactively to conversational dynamics and engagement. More information can be found on our website: https://proactivehearing.cs.washington.edu/

</details>


### [72] [W2S-AlignTree: Weak-to-Strong Inference-Time Alignment for Large Language Models via Monte Carlo Tree Search](https://arxiv.org/abs/2511.11518)
*Zhenyu Ding,Yuhao Wang,Tengyue Xiao,Haoying Wang,Guojun Ma,Mingyang Wan,Caigui Jiang,Ning Ding*

Main category: cs.CL

TL;DR: W2S-AlignTree是一个创新的即插即用推理时对齐框架，首次将蒙特卡洛树搜索与弱到强泛化范式结合，无需修改强模型参数即可实现细粒度对齐控制。


<details>
  <summary>Details</summary>
Motivation: 解决LLM输出与人类偏好不对齐的问题，现有训练时对齐方法成本高、扩展性有限，需要可扩展和自适应的推理时对齐机制。

Method: 将LLM对齐建模为生成搜索树中的最优启发式搜索问题，利用弱模型的实时步级信号作为对齐代理，引入熵感知探索机制动态平衡探索与利用。

Result: 在情感生成、摘要和指令跟随任务中持续优于强基线，将Llama3-8B在摘要任务上的性能从1.89提升到2.19，相对改进15.9%。

Conclusion: W2S-AlignTree为LLM对齐提供了一种高效、可扩展的推理时解决方案，实现了细粒度控制而无需模型参数修改。

Abstract: Large Language Models (LLMs) demonstrate impressive capabilities, yet their outputs often suffer from misalignment with human preferences due to the inadequacy of weak supervision and a lack of fine-grained control. Training-time alignment methods like Reinforcement Learning from Human Feedback (RLHF) face prohibitive costs in expert supervision and inherent scalability limitations, offering limited dynamic control during inference. Consequently, there is an urgent need for scalable and adaptable alignment mechanisms. To address this, we propose W2S-AlignTree, a pioneering plug-and-play inference-time alignment framework that synergistically combines Monte Carlo Tree Search (MCTS) with the Weak-to-Strong Generalization paradigm for the first time. W2S-AlignTree formulates LLM alignment as an optimal heuristic search problem within a generative search tree. By leveraging weak model's real-time, step-level signals as alignment proxies and introducing an Entropy-Aware exploration mechanism, W2S-AlignTree enables fine-grained guidance during strong model's generation without modifying its parameters. The approach dynamically balances exploration and exploitation in high-dimensional generation search trees. Experiments across controlled sentiment generation, summarization, and instruction-following show that W2S-AlignTree consistently outperforms strong baselines. Notably, W2S-AlignTree raises the performance of Llama3-8B from 1.89 to 2.19, a relative improvement of 15.9 on the summarization task.

</details>


### [73] [PRBench: Large-Scale Expert Rubrics for Evaluating High-Stakes Professional Reasoning](https://arxiv.org/abs/2511.11562)
*Afra Feyza Akyürek,Advait Gosai,Chen Bo Calvin Zhang,Vipul Gupta,Jaehwan Jeong,Anisha Gunjal,Tahseen Rabbani,Maria Mazzone,David Randolph,Mohammad Mahmoudi Meymand,Gurshaan Chattha,Paula Rodriguez,Diego Mares,Pavit Singh,Michael Liu,Subodh Chawla,Pete Cline,Lucy Ogaz,Ernesto Hernandez,Zihao Wang,Pavi Bhatter,Marcos Ayestaran,Bing Liu,Yunzhong He*

Main category: cs.CL

TL;DR: PRBench是一个针对金融和法律领域的专业推理基准测试，包含1,100个专家编写的任务和19,356个专家制定的评估标准，是目前该领域最大的公开基准测试。对20个领先模型的评估显示在专业推理方面仍有很大改进空间。


<details>
  <summary>Details</summary>
Motivation: 现有学术基准测试无法充分评估模型在真实世界专业环境中的表现，特别是在高风险的金融和法律领域，这些领域的实际应用价值至关重要。

Method: 招募182名合格专业人士（持有JD、CFA证书或6年以上经验）编写基于实际工作流程的任务，构建包含1,100个任务和19,356个评估标准的基准测试，并通过独立专家验证确保质量。

Result: 评估20个领先模型发现，在Hard子集上最高得分仅为0.39（金融）和0.37（法律），显示模型在专业推理方面表现不佳。常见失败模式包括判断不准确、缺乏过程透明度和推理不完整。

Conclusion: 当前模型在专业推理能力方面存在显著不足，特别是在金融和法律等高风险领域，其可靠性和实用性仍需大幅提升才能满足专业应用需求。

Abstract: Frontier model progress is often measured by academic benchmarks, which offer a limited view of performance in real-world professional contexts. Existing evaluations often fail to assess open-ended, economically consequential tasks in high-stakes domains like Legal and Finance, where practical returns are paramount. To address this, we introduce Professional Reasoning Bench (PRBench), a realistic, open-ended, and difficult benchmark of real-world problems in Finance and Law. We open-source its 1,100 expert-authored tasks and 19,356 expert-curated criteria, making it, to our knowledge, the largest public, rubric-based benchmark for both legal and finance domains. We recruit 182 qualified professionals, holding JDs, CFAs, or 6+ years of experience, who contributed tasks inspired by their actual workflows. This process yields significant diversity, with tasks spanning 114 countries and 47 US jurisdictions. Our expert-curated rubrics are validated through a rigorous quality pipeline, including independent expert validation. Subsequent evaluation of 20 leading models reveals substantial room for improvement, with top scores of only 0.39 (Finance) and 0.37 (Legal) on our Hard subsets. We further catalog associated economic impacts of the prompts and analyze performance using human-annotated rubric categories. Our analysis shows that models with similar overall scores can diverge significantly on specific capabilities. Common failure modes include inaccurate judgments, a lack of process transparency and incomplete reasoning, highlighting critical gaps in their reliability for professional adoption.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [74] [Co-EPG: A Framework for Co-Evolution of Planning and Grounding in Autonomous GUI Agents](https://arxiv.org/abs/2511.10705)
*Yuan Zhao,Hualei Zhu,Tingyu Jiang,Shen Li,Xiaohang Xu,Hao Henry Wang*

Main category: cs.AI

TL;DR: 提出了Co-EPG框架，通过规划和接地的协同进化实现GUI任务自动化，无需外部数据即可在基准测试中超越现有方法


<details>
  <summary>Details</summary>
Motivation: 解决当前GUI代理方法存在的两个关键问题：跨模型协同利用不足，以及过度依赖合成数据生成而利用不足

Method: Co-EPG框架建立迭代正反馈循环：规划模型在基于接地的奖励指导下通过GRPO探索策略，生成多样化数据优化接地模型；优化的接地模型为后续规划模型训练提供更有效奖励

Result: 在Multimodal-Mind2Web和AndroidControl基准测试中，仅经过三次迭代就超越现有最优方法，且每次迭代都有持续改进

Conclusion: 为GUI代理建立了新的训练范式，从孤立优化转向集成、自驱动的协同进化方法

Abstract: Graphical User Interface (GUI) task automation constitutes a critical frontier in artificial intelligence research. While effective GUI agents synergistically integrate planning and grounding capabilities, current methodologies exhibit two fundamental limitations: (1) insufficient exploitation of cross-model synergies, and (2) over-reliance on synthetic data generation without sufficient utilization. To address these challenges, we propose Co-EPG, a self-iterative training framework for Co-Evolution of Planning and Grounding. Co-EPG establishes an iterative positive feedback loop: through this loop, the planning model explores superior strategies under grounding-based reward guidance via Group Relative Policy Optimization (GRPO), generating diverse data to optimize the grounding model. Concurrently, the optimized Grounding model provides more effective rewards for subsequent GRPO training of the planning model, fostering continuous improvement. Co-EPG thus enables iterative enhancement of agent capabilities through self-play optimization and training data distillation. On the Multimodal-Mind2Web and AndroidControl benchmarks, our framework outperforms existing state-of-the-art methods after just three iterations without requiring external data. The agent consistently improves with each iteration, demonstrating robust self-enhancement capabilities. This work establishes a novel training paradigm for GUI agents, shifting from isolated optimization to an integrated, self-driven co-evolution approach.

</details>


### [75] [From Efficiency to Adaptivity: A Deeper Look at Adaptive Reasoning in Large Language Models](https://arxiv.org/abs/2511.10788)
*Chao Wu,Baoheng Li,Mingchen Gao,Zhenyi Wang*

Main category: cs.AI

TL;DR: 该调查从适应性角度重新审视LLM推理，提出将推理努力根据输入特征进行动态分配，并建立了系统化的分类法来组织现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLM对简单和复杂任务都采用统一的推理策略，导致对简单问题生成过长推理链，而对困难问题推理不足。需要发展能够根据任务难度和不确定性自适应分配推理努力的能力。

Method: 1. 在LLM背景下形式化演绎、归纳和溯因推理；2. 将自适应推理形式化为控制增强的策略优化问题；3. 提出系统分类法，将方法分为基于训练的方法（强化学习、监督微调、学习控制器）和免训练方法（提示条件化、反馈驱动停止、模块化组合）。

Result: 建立了一个清晰的框架来理解不同机制如何在实践中实现自适应推理，并能够系统比较各种策略。

Conclusion: 确定了自我评估、元推理和人类对齐推理控制等开放挑战。

Abstract: Recent advances in large language models (LLMs) have made reasoning a central benchmark for evaluating intelligence. While prior surveys focus on efficiency by examining how to shorten reasoning chains or reduce computation, this view overlooks a fundamental challenge: current LLMs apply uniform reasoning strategies regardless of task complexity, generating long traces for trivial problems while failing to extend reasoning for difficult tasks. This survey reframes reasoning through the lens of {adaptivity}: the capability to allocate reasoning effort based on input characteristics such as difficulty and uncertainty. We make three contributions. First, we formalize deductive, inductive, and abductive reasoning within the LLM context, connecting these classical cognitive paradigms with their algorithmic realizations. Second, we formalize adaptive reasoning as a control-augmented policy optimization problem balancing task performance with computational cost, distinguishing learned policies from inference-time control mechanisms. Third, we propose a systematic taxonomy organizing existing methods into training-based approaches that internalize adaptivity through reinforcement learning, supervised fine-tuning, and learned controllers, and training-free approaches that achieve adaptivity through prompt conditioning, feedback-driven halting, and modular composition. This framework clarifies how different mechanisms realize adaptive reasoning in practice and enables systematic comparison across diverse strategies. We conclude by identifying open challenges in self-evaluation, meta-reasoning, and human-aligned reasoning control.

</details>


### [76] [Multi-agent Undercover Gaming: Hallucination Removal via Counterfactual Test for Multimodal Reasoning](https://arxiv.org/abs/2511.11182)
*Dayong Liang,Xiao-Yong Wei,Changmeng Zheng*

Main category: cs.AI

TL;DR: 提出了多智能体卧底游戏(MUG)协议来解决LLM幻觉问题，通过反事实测试检测幻觉智能体，提升多模态推理的可靠性


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体辩论(MAD)方法假设所有辩论者都是理性的，但现实中智能体本身就可能产生幻觉，需要更可靠的检测机制

Method: 基于社交推理游戏设计MUG协议，通过修改参考图像引入反事实证据，观察智能体是否能准确识别变化，从而检测幻觉智能体

Result: MUG在三个关键维度上推进了MAD协议：反事实测试实现事实验证、动态修改证据源引入跨证据推理、促进主动推理而非被动回答

Conclusion: MUG为LLM多模态推理提供了一个更可靠有效的框架，通过检测幻觉智能体提升整体推理质量

Abstract: Hallucination continues to pose a major obstacle in the reasoning capabilities of large language models (LLMs). Although the Multi-Agent Debate (MAD) paradigm offers a promising solution by promoting consensus among multiple agents to enhance reliability, it relies on the unrealistic assumption that all debaters are rational and reflective, which is a condition that may not hold when agents themselves are prone to hallucinations. To address this gap, we introduce the Multi-agent Undercover Gaming (MUG) protocol, inspired by social deduction games like "Who is Undercover?". MUG reframes MAD as a process of detecting "undercover" agents (those suffering from hallucinations) by employing multimodal counterfactual tests. Specifically, we modify reference images to introduce counterfactual evidence and observe whether agents can accurately identify these changes, providing ground-truth for identifying hallucinating agents and enabling robust, crowd-powered multimodal reasoning. MUG advances MAD protocols along three key dimensions: (1) enabling factual verification beyond statistical consensus through counterfactual testing; (2) introducing cross-evidence reasoning via dynamically modified evidence sources instead of relying on static inputs; and (3) fostering active reasoning, where agents engage in probing discussions rather than passively answering questions. Collectively, these innovations offer a more reliable and effective framework for multimodal reasoning in LLMs. The source code can be accessed at https://github.com/YongLD/MUG.git.

</details>


### [77] [Aligning Machiavellian Agents: Behavior Steering via Test-Time Policy Shaping](https://arxiv.org/abs/2511.11551)
*Dena Mujtaba,Brian Hu,Anthony Hoogs,Arslan Basharat*

Main category: cs.AI

TL;DR: 提出一种基于模型引导策略塑造的测试时对齐技术，能够在不重新训练智能体的情况下精确控制个体行为属性，在伦理对齐和奖励最大化之间实现原则性权衡。


<details>
  <summary>Details</summary>
Motivation: 决策AI智能体在复杂动态环境中运行时面临与人类价值观或指导方针保持一致的挑战。仅训练实现目标的智能体可能采取有害行为，暴露了奖励最大化与保持对齐之间的关键权衡。对于预训练智能体，确保对齐尤其困难，因为重新训练成本高且过程缓慢。

Method: 使用模型引导的策略塑造方法，在测试时通过场景-行动属性分类器应用策略塑造，确保决策与伦理属性对齐。在MACHIAVELLI基准上进行评估，该基准包含134个基于文本的游戏环境和数千个涉及伦理决策的标注场景。

Result: 测试时策略塑造为减轻不同环境和对齐属性中的不道德行为提供了有效且可扩展的解决方案，优于先前的训练时方法和通用智能体。

Conclusion: 该方法能够在不需要智能体重训练的情况下，实现伦理对齐和奖励最大化之间的原则性权衡，为AI智能体的伦理对齐提供了实用的解决方案。

Abstract: The deployment of decision-making AI agents presents a critical challenge in maintaining alignment with human values or guidelines while operating in complex, dynamic environments. Agents trained solely to achieve their objectives may adopt harmful behavior, exposing a key trade-off between maximizing the reward function and maintaining the alignment. For the pre-trained agents, ensuring alignment is particularly challenging, as retraining can be a costly and slow process. This is further complicated by the diverse and potentially conflicting attributes representing the ethical values for alignment. To address these challenges, we propose a test-time alignment technique based on model-guided policy shaping. Our method allows precise control over individual behavioral attributes, generalizes across diverse reinforcement learning (RL) environments, and facilitates a principled trade-off between ethical alignment and reward maximization without requiring agent retraining. We evaluate our approach using the MACHIAVELLI benchmark, which comprises 134 text-based game environments and thousands of annotated scenarios involving ethical decisions. The RL agents are first trained to maximize the reward in their respective games. At test time, we apply policy shaping via scenario-action attribute classifiers to ensure decision alignment with ethical attributes. We compare our approach against prior training-time methods and general-purpose agents, as well as study several types of ethical violations and power-seeking behavior. Our results demonstrate that test-time policy shaping provides an effective and scalable solution for mitigating unethical behavior across diverse environments and alignment attributes.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [78] [Building the Web for Agents: A Declarative Framework for Agent-Web Interaction](https://arxiv.org/abs/2511.11287)
*Sven Schultze,Meike Verena Kietzmann,Nils-Lucas Schönfeld,Ruth Stock-Homburg*

Main category: cs.HC

TL;DR: VOIX是一个网络原生框架，通过引入<tool>和<context>HTML标签，让网站开发者能够为AI代理提供可靠、可审计且保护隐私的能力，解决了AI代理与人类界面之间的不对齐问题。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理在网络上部署面临根本性不对齐问题：代理必须从面向人类的用户界面推断可用功能，导致交互脆弱、低效且不安全。

Method: 引入VOIX框架，使用声明式HTML元素（<tool>和<context>标签）让开发者明确定义可用操作和相关状态，为代理行为创建清晰的机器可读契约。

Result: 在为期三天的黑客松研究中，16名开发者（无论经验水平）都能快速构建多样化且功能完善的代理驱动网络应用，证明了框架的实用性、易学性和表达能力。

Conclusion: 这项工作为实现代理化网络提供了基础机制，为网络上无缝安全的人机协作未来奠定了基础。

Abstract: The increasing deployment of autonomous AI agents on the web is hampered by a fundamental misalignment: agents must infer affordances from human-oriented user interfaces, leading to brittle, inefficient, and insecure interactions. To address this, we introduce VOIX, a web-native framework that enables websites to expose reliable, auditable, and privacy-preserving capabilities for AI agents through simple, declarative HTML elements. VOIX introduces <tool> and <context> tags, allowing developers to explicitly define available actions and relevant state, thereby creating a clear, machine-readable contract for agent behavior. This approach shifts control to the website developer while preserving user privacy by disconnecting the conversational interactions from the website. We evaluated the framework's practicality, learnability, and expressiveness in a three-day hackathon study with 16 developers. The results demonstrate that participants, regardless of prior experience, were able to rapidly build diverse and functional agent-enabled web applications. Ultimately, this work provides a foundational mechanism for realizing the Agentic Web, enabling a future of seamless and secure human-AI collaboration on the web.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [79] [SQuaD: The Software Quality Dataset](https://arxiv.org/abs/2511.11265)
*Mikel Robredo,Matteo Esposito,Davide Taibi,Rafael Peñaloza,Valentina Lenarduzzi*

Main category: cs.SE

TL;DR: SQuaD是一个多维度的软件质量数据集，整合了9个静态分析工具，从450个成熟开源项目中提取了700多个质量指标，涵盖方法、类、文件和项目级别，支持软件维护性、技术债务和软件演化等研究。


<details>
  <summary>Details</summary>
Motivation: 现有软件质量数据集通常只关注有限维度（如代码异味、技术债务或重构活动），限制了跨时间和质量维度的综合分析。

Method: 整合9个先进的静态分析工具（SonarQube、CodeScene、PMD等），从450个成熟开源项目中提取700多个唯一指标，涵盖63,586个项目版本，并提供版本控制、问题跟踪历史和软件漏洞数据。

Result: 创建了SQuaD数据集，统一了方法、类、文件和项目级别的质量指标，支持大规模实证研究。

Conclusion: SQuaD支持软件维护性、技术债务、软件演化和质量评估的大规模研究，并提出了自动化数据集更新和跨项目质量建模等研究方向。

Abstract: Software quality research increasingly relies on large-scale datasets that measure both the product and process aspects of software systems. However, existing resources often focus on limited dimensions, such as code smells, technical debt, or refactoring activity, thereby restricting comprehensive analyses across time and quality dimensions. To address this gap, we present the Software Quality Dataset (SQuaD), a multi-dimensional, time-aware collection of software quality metrics extracted from 450 mature open-source projects across diverse ecosystems, including Apache, Mozilla, FFmpeg, and the Linux kernel. By integrating nine state-of-the-art static analysis tools, i.e., SonarQube, CodeScene, PMD, Understand, CK, JaSoMe, RefactoringMiner, RefactoringMiner++, and PyRef, our dataset unifies over 700 unique metrics at method, class, file, and project levels. Covering a total of 63,586 analyzed project releases, SQuaD also provides version control and issue-tracking histories, software vulnerability data (CVE/CWE), and process metrics proven to enhance Just-In-Time (JIT) defect prediction. The SQuaD enables empirical research on maintainability, technical debt, software evolution, and quality assessment at unprecedented scale. We also outline emerging research directions, including automated dataset updates and cross-project quality modeling to support the continuous evolution of software analytics. The dataset is publicly available on ZENODO (DOI: 10.5281/zenodo.17566690).

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [80] [PISanitizer: Preventing Prompt Injection to Long-Context LLMs via Prompt Sanitization](https://arxiv.org/abs/2511.10720)
*Runpeng Geng,Yanting Wang,Chenlong Yin,Minhao Cheng,Ying Chen,Jinyuan Jia*

Main category: cs.CR

TL;DR: PISanitizer是一种针对长上下文LLM的提示注入防御方法，通过定位和净化潜在注入令牌来消除注入指令的影响。


<details>
  <summary>Details</summary>
Motivation: 现有的提示注入防御方法是为短上下文设计的，在长上下文场景下效果有限，因为注入指令只占长上下文的很小一部分，防御难度大。

Method: 基于两个观察：1) 提示注入攻击本质是构造强制LLM遵循的指令；2) LLM利用注意力机制关注关键输入令牌。PISanitizer首先让LLM遵循上下文中的任意指令，然后净化驱动指令遵循行为的高注意力令牌。

Result: 广泛评估表明PISanitizer能成功防止提示注入，保持实用性，优于现有防御方法，效率高，且对基于优化的强自适应攻击具有鲁棒性。

Conclusion: PISanitizer为攻击者制造了一个困境：注入指令越有效强制LLM遵循，就越可能被PISanitizer净化，从而有效防御长上下文中的提示注入攻击。

Abstract: Long context LLMs are vulnerable to prompt injection, where an attacker can inject an instruction in a long context to induce an LLM to generate an attacker-desired output. Existing prompt injection defenses are designed for short contexts. When extended to long-context scenarios, they have limited effectiveness. The reason is that an injected instruction constitutes only a very small portion of a long context, making the defense very challenging. In this work, we propose PISanitizer, which first pinpoints and sanitizes potential injected tokens (if any) in a context before letting a backend LLM generate a response, thereby eliminating the influence of the injected instruction. To sanitize injected tokens, PISanitizer builds on two observations: (1) prompt injection attacks essentially craft an instruction that compels an LLM to follow it, and (2) LLMs intrinsically leverage the attention mechanism to focus on crucial input tokens for output generation. Guided by these two observations, we first intentionally let an LLM follow arbitrary instructions in a context and then sanitize tokens receiving high attention that drive the instruction-following behavior of the LLM. By design, PISanitizer presents a dilemma for an attacker: the more effectively an injected instruction compels an LLM to follow it, the more likely it is to be sanitized by PISanitizer. Our extensive evaluation shows that PISanitizer can successfully prevent prompt injection, maintain utility, outperform existing defenses, is efficient, and is robust to optimization-based and strong adaptive attacks. The code is available at https://github.com/sleeepeer/PISanitizer.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [81] [Language-Aided State Estimation](https://arxiv.org/abs/2511.11285)
*Yuki Miyoshi,Masaki Inoue,Yusuke Fujimoto*

Main category: eess.SY

TL;DR: 提出了一种语言辅助粒子滤波器(LAPF)，利用自然语言处理将人类观察整合到物理系统的状态估计中，并在灌溉渠道水位估计中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 利用社交媒体和聊天平台中自然语言形式的人类观察数据，解决物理系统的状态估计问题，将人类作为感知代理。

Method: 提出语言辅助粒子滤波器框架，通过自然语言处理结构化人类观察，并将其整合到状态估计的更新步骤中。

Result: 将LAPF应用于灌溉渠道水位估计问题，证明了其有效性。

Conclusion: LAPF能够成功利用自然语言形式的人类观察来改进物理系统的状态估计性能。

Abstract: Natural language data, such as text and speech, have become readily available through social networking services and chat platforms. By leveraging human observations expressed in natural language, this paper addresses the problem of state estimation for physical systems, in which humans act as sensing agents. To this end, we propose a Language-Aided Particle Filter (LAPF), a particle filter framework that structures human observations via natural language processing and incorporates them into the update step of the state estimation. Finally, the LAPF is applied to the water level estimation problem in an irrigation canal and its effectiveness is demonstrated.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [82] [Who Gets the Reward, Who Gets the Blame? Evaluation-Aligned Training Signals for Multi-LLM Agents](https://arxiv.org/abs/2511.10687)
*Chih-Hsuan Yang,Tanwi Mallick,Le Chen,Krishnan Raghavan,Azton Wells,Amal Gueroudji,Ian T. Foster,Rajeev Thakur*

Main category: cs.MA

TL;DR: 提出了一个理论框架，将合作博弈论归因与过程奖励建模相结合，将系统级评估转化为代理信用和响应级信号，为LLM多智能体训练提供从全局评估到局部监督的统一路径。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统中的LLM训练方法缺乏将系统级评估与代理级和消息级学习连接起来的原理性方法，需要建立统一的评估到监督的转化机制。

Method: 结合合作博弈论归因（如Shapley值）和过程奖励建模，在成功案例中通过Shapley信用分配公平分配结果并细化为每消息奖励，在失败案例中通过首次错误定位产生修复感知偏好。

Result: 生成了局部、有符号且信用保守的信号，这些信号有界、具有合作性，可直接与基于强化学习或偏好的后训练兼容。

Conclusion: 提出了一个概念性贡献，建立了从全局评估到局部监督的理论基础，为LLM多智能体训练提供了统一且可审计的路径，但实证验证留待未来工作。

Abstract: Large Language Models (LLMs) in multi-agent systems (MAS) have shown promise for complex tasks, yet current training methods lack principled ways to connect system-level evaluation with agent-level and message-level learning. We propose a theoretical framework that unifies cooperative game-theoretic attribution with process reward modeling to transform system evaluation into agent credit and then into response-level signals. Unlike prior approaches that rely only on attribution (e.g., Shapley) or step-level labels (e.g., PRM), our method produces local, signed, and credit-conserving signals. In success cases, Shapley-based credit assignment fairly allocates outcomes across agents and is refined into per-message rewards that promote cooperation while discouraging redundancy or sabotage. In failure cases, first-error localization yields repair-aware preferences that penalize harmful steps while rewarding corrective attempts. The resulting signals are bounded, cooperative, and directly compatible with reinforcement-based or preference-based post-training, providing a unified and auditable pathway from global evaluation to local supervision in LLM multi-agent training. Our contribution is conceptual: we present a theoretical foundation and training signals, leaving empirical validation for future work.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [83] [The Map of Misbelief: Tracing Intrinsic and Extrinsic Hallucinations Through Attention Patterns](https://arxiv.org/abs/2511.10837)
*Elyes Hajji,Aymen Bouguerra,Fabio Arnez*

Main category: cs.LG

TL;DR: 提出了一个区分外在和内在幻觉的评估框架，利用基于注意力的不确定性量化算法，通过注意力聚合策略改进幻觉检测性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: LLMs在安全关键领域部署时容易产生幻觉，现有检测方法计算成本高且未区分幻觉类型，需要更有效的检测策略。

Method: 使用基于注意力的不确定性量化算法，提出新颖的注意力聚合策略，构建区分外在和内在幻觉的评估框架。

Result: 采样方法如语义熵能有效检测外在幻觉但无法检测内在幻觉，而基于输入token注意力聚合的方法更适合内在幻觉检测。

Conclusion: 注意力是量化模型不确定性的重要信号，为根据幻觉性质调整检测策略提供了新方向。

Abstract: Large Language Models (LLMs) are increasingly deployed in safety-critical domains, yet remain susceptible to hallucinations. While prior works have proposed confidence representation methods for hallucination detection, most of these approaches rely on computationally expensive sampling strategies and often disregard the distinction between hallucination types. In this work, we introduce a principled evaluation framework that differentiates between extrinsic and intrinsic hallucination categories and evaluates detection performance across a suite of curated benchmarks. In addition, we leverage a recent attention-based uncertainty quantification algorithm and propose novel attention aggregation strategies that improve both interpretability and hallucination detection performance. Our experimental findings reveal that sampling-based methods like Semantic Entropy are effective for detecting extrinsic hallucinations but generally fail on intrinsic ones. In contrast, our method, which aggregates attention over input tokens, is better suited for intrinsic hallucinations. These insights provide new directions for aligning detection strategies with the nature of hallucination and highlight attention as a rich signal for quantifying model uncertainty.

</details>


### [84] [On-Device Fine-Tuning via Backprop-Free Zeroth-Order Optimization](https://arxiv.org/abs/2511.11362)
*Prabodh Katti,Sangwoo Park,Bipin Rajendran,Osvaldo Simeone*

Main category: cs.LG

TL;DR: MeZO（内存高效零阶优化）通过仅使用前向评估估计梯度，消除了存储中间激活和优化器状态的需求，使边缘设备能够在内存约束下微调更大模型，但需要更长的训练时间。


<details>
  <summary>Details</summary>
Motivation: 边缘AI系统需要在严格内存约束下适应不同任务，传统基于反向传播的训练需要存储层激活和优化器状态，限制了可部署模型的最大规模。

Method: 使用MeZO（内存高效零阶优化）方法，仅通过前向评估估计梯度，无需存储中间激活或优化器状态，从而显著减少内存占用。

Result: 理论分析和数值验证表明，在设备内存约束下，MeZO能够容纳比传统反向传播更大的模型规模，并在有足够训练时间时表现出精度优势。

Conclusion: MeZO为边缘设备上的模型微调提供了可行的解决方案，通过牺牲训练时间换取内存效率，使更大模型能够在资源受限环境中部署和适应。

Abstract: On-device fine-tuning is a critical capability for edge AI systems, which must support adaptation to different agentic tasks under stringent memory constraints. Conventional backpropagation (BP)-based training requires storing layer activations and optimizer states, a demand that can be only partially alleviated through checkpointing. In edge deployments in which the model weights must reside entirely in device memory, this overhead severely limits the maximum model size that can be deployed. Memory-efficient zeroth-order optimization (MeZO) alleviates this bottleneck by estimating gradients using forward evaluations alone, eliminating the need for storing intermediate activations or optimizer states. This enables significantly larger models to fit within on-chip memory, albeit at the cost of potentially longer fine-tuning wall-clock time. This paper first provides a theoretical estimate of the relative model sizes that can be accommodated under BP and MeZO training. We then numerically validate the analysis, demonstrating that MeZO exhibits accuracy advantages under on-device memory constraints, provided sufficient wall-clock time is available for fine-tuning.

</details>


### [85] [Optimizing Mixture of Block Attention](https://arxiv.org/abs/2511.11571)
*Guangxuan Xiao,Junxian Guo,Kasra Mazaheri,Song Han*

Main category: cs.LG

TL;DR: MoBA是一种通过稀疏注意力机制高效处理长上下文的LLM构建块，但缺乏理论理解和GPU实现。本文通过统计模型分析MoBA机制，发现性能取决于路由器准确区分相关块的能力，提出了信号噪声比指标。通过减小块大小和键卷积来改进路由精度，并开发了FlashMoBA GPU内核实现高效执行。


<details>
  <summary>Details</summary>
Motivation: MoBA虽然能显著降低长上下文处理的计算成本，但其设计原理不明确且缺乏高效的GPU实现，阻碍了实际应用。需要深入理解其工作机制并提供硬件优化的实现方案。

Method: 1. 开发统计模型分析MoBA底层机制；2. 推导信号噪声比连接架构参数与检索精度；3. 提出使用更小块大小和键卷积来改进路由精度；4. 开发FlashMoBA CUDA内核支持小块高效执行。

Result: 改进后的MoBA模型在从头训练的LLM中达到了密集注意力基线的性能水平。FlashMoBA相比FlashAttention-2在小块情况下实现了最高14.7倍的加速。

Conclusion: 通过理论分析和硬件优化相结合，成功提升了MoBA的性能和效率，使其成为实用的长上下文处理解决方案，为稀疏注意力机制的实际应用提供了理论指导和高效实现。

Abstract: Mixture of Block Attention (MoBA) (Lu et al., 2025) is a promising building block for efficiently processing long contexts in LLMs by enabling queries to sparsely attend to a small subset of key-value blocks, drastically reducing computational cost. However, the design principles governing MoBA's performance are poorly understood, and it lacks an efficient GPU implementation, hindering its practical adoption. In this paper, we first develop a statistical model to analyze MoBA's underlying mechanics. Our model reveals that performance critically depends on the router's ability to accurately distinguish relevant from irrelevant blocks based on query-key affinities. We derive a signal-to-noise ratio that formally connects architectural parameters to this retrieval accuracy. Guided by our analysis, we identify two key pathways for improvement: using smaller block sizes and applying a short convolution on keys to cluster relevant signals, which enhances routing accuracy. While theoretically better, small block sizes are inefficient on GPUs. To bridge this gap, we introduce FlashMoBA, a hardware-aware CUDA kernel that enables efficient MoBA execution even with the small block sizes our theory recommends. We validate our insights by training LLMs from scratch, showing that our improved MoBA models match the performance of dense attention baselines. FlashMoBA achieves up to 14.7x speedup over FlashAttention-2 for small blocks, making our theoretically-grounded improvements practical. Code is available at: https://github.com/mit-han-lab/flash-moba.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [86] [CLARITY: Contextual Linguistic Adaptation and Accent Retrieval for Dual-Bias Mitigation in Text-to-Speech Generation](https://arxiv.org/abs/2511.11104)
*Crystal Min Hui Poon,Pai Chet Ng,Xiaoxiao Miao,Immanuel Jun Kai Loh,Bowen Zhang,Haoyu Song,Ian Mcloughlin*

Main category: cs.SD

TL;DR: CLARITY是一个解决TTS系统中口音偏见和语言偏见的框架，通过上下文语言适应和检索增强的口音提示来优化口音准确性和公平性。


<details>
  <summary>Details</summary>
Motivation: 当前指令引导的TTS系统存在两个相互关联的偏见：口音偏见（模型默认使用主导语音模式）和语言偏见（忽略方言特定的词汇和文化线索），这些偏见影响了语音生成的公平性和真实性。

Method: CLARITY采用双信号优化方法：(1)上下文语言适应，将输入文本本地化为目标方言；(2)检索增强的口音提示(RAAP)，提供口音一致的语音提示。该框架与主干模型无关。

Result: 在12种英语口音上的测试表明，CLARITY提高了口音准确性和公平性，同时保持了良好的感知质量。

Conclusion: CLARITY框架有效解决了TTS系统中的口音和语言偏见问题，为包容性语音合成提供了可行方案。

Abstract: Instruction-guided text-to-speech (TTS) research has reached a maturity level where excellent speech generation quality is possible on demand, yet two coupled biases persist: accent bias, where models default to dominant phonetic patterns, and linguistic bias, where dialect-specific lexical and cultural cues are ignored. These biases are interdependent, as authentic accent generation requires both accent fidelity and localized text. We present Contextual Linguistic Adaptation and Retrieval for Inclusive TTS sYnthesis (CLARITY), a backbone-agnostic framework that addresses these biases through dual-signal optimization: (i) contextual linguistic adaptation that localizes input text to the target dialect, and (ii) retrieval-augmented accent prompting (RAAP) that supplies accent-consistent speech prompts. Across twelve English accents, CLARITY improves accent accuracy and fairness while maintaining strong perceptual quality.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [87] [S2D-ALIGN: Shallow-to-Deep Auxiliary Learning for Anatomically-Grounded Radiology Report Generation](https://arxiv.org/abs/2511.11066)
*Jiechao Gao,Chang Liu,Yuangang Li*

Main category: cs.CV

TL;DR: 本文提出了S2D-Align方法，通过浅到深的渐进式对齐策略，利用不同粒度的辅助信号建立解剖学基础的对齐，提升放射学报告生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有的放射学报告生成方法主要依赖多模态大语言模型的跨模态生成能力，通过监督微调优化图像-文本对齐。但标准SFT范式只进行实例级对齐，无法建立解剖学基础的对齐，且报告的模板化特性导致生成质量不理想。

Method: S2D-Align采用浅到深策略：从粗粒度放射图像-报告配对开始，引入参考报告进行实例级指导，最后使用关键短语将生成过程锚定在特定解剖细节上。通过基于记忆的适配器实现特征共享，整合粗粒度和细粒度指导。

Result: 在MIMIC-CXR和IU X-Ray基准测试中，S2D-Align相比现有方法实现了最先进的性能。消融研究验证了多阶段、辅助指导方法的有效性。

Conclusion: 该方法为增强复杂多模态生成任务中的基础能力提供了一个有前景的方向。

Abstract: Radiology Report Generation (RRG) aims to automatically generate diagnostic reports from radiology images. To achieve this, existing methods have leveraged the powerful cross-modal generation capabilities of Multimodal Large Language Models (MLLMs), primarily focusing on optimizing cross-modal alignment between radiographs and reports through Supervised Fine-Tuning (SFT). However, by only performing instance-level alignment with the image-text pairs, the standard SFT paradigm fails to establish anatomically-grounded alignment, where the templated nature of reports often leads to sub-optimal generation quality. To address this, we propose \textsc{S2D-Align}, a novel SFT paradigm that establishes anatomically-grounded alignment by leveraging auxiliary signals of varying granularities. \textsc{S2D-Align} implements a shallow-to-deep strategy, progressively enriching the alignment process: it begins with the coarse radiograph-report pairing, then introduces reference reports for instance-level guidance, and ultimately utilizes key phrases to ground the generation in specific anatomical details. To bridge the different alignment stages, we introduce a memory-based adapter that empowers feature sharing, thereby integrating coarse and fine-grained guidance. For evaluation, we conduct experiments on the public \textsc{MIMIC-CXR} and \textsc{IU X-Ray} benchmarks, where \textsc{S2D-Align} achieves state-of-the-art performance compared to existing methods. Ablation studies validate the effectiveness of our multi-stage, auxiliary-guided approach, highlighting a promising direction for enhancing grounding capabilities in complex, multi-modal generation tasks.

</details>


### [88] [Discovering Meaningful Units with Visually Grounded Semantics from Image Captions](https://arxiv.org/abs/2511.11262)
*Melika Behjati,James Henderson*

Main category: cs.CV

TL;DR: 提出了一种通过将字幕标记分组来获取细粒度视觉语言表示的新方法，该表示与图像中的对象级别对齐，从而提升视觉语言模型的细粒度理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要将图像块与语言标记对齐，但图像块对人眼无意义，单个标记也不一定携带可接地信息。需要将标记分组来描述场景的不同方面，以获得更好的细粒度知识理解。

Method: 提出了一种模型，在架构中将字幕标记分组以捕获语言的细粒度表示，并将这些表示与经过训练发现对象的图像编码器输出对齐。

Result: 通过学习标记分组，视觉语言模型对视觉和语言有了更好的细粒度理解。模型发现的标记组在定性和定量上都与文本中的可接地短语高度相似。

Conclusion: 通过将字幕标记分组并与对象级别的视觉表示对齐，可以有效提升视觉语言模型的细粒度理解能力，且发现的标记组与真实可接地短语高度一致。

Abstract: Fine-grained knowledge is crucial for vision-language models to obtain a better understanding of the real world. While there has been work trying to acquire this kind of knowledge in the space of vision and language, it has mostly focused on aligning the image patches with the tokens on the language side. However, image patches do not have any meaning to the human eye, and individual tokens do not necessarily carry groundable information in the image. It is groups of tokens which describe different aspects of the scene. In this work, we propose a model which groups the caption tokens as part of its architecture in order to capture a fine-grained representation of the language. We expect our representations to be at the level of objects present in the image, and therefore align our representations with the output of an image encoder trained to discover objects. We show that by learning to group the tokens, the vision-language model has a better fine-grained understanding of vision and language. In addition, the token groups that our model discovers are highly similar to groundable phrases in text, both qualitatively and quantitatively.

</details>


### [89] [From Synthetic Scenes to Real Performance: Enhancing Spatial Reasoning in VLMs](https://arxiv.org/abs/2511.11440)
*Massimo Rizzoli,Simone Alghisi,Seyed Mahed Mousavi,Giuseppe Riccardi*

Main category: cs.CV

TL;DR: 该论文提出了一种通过控制生成无偏、平衡的合成数据来微调视觉语言模型的方法，以解决传统微调过程中的偏差、分布不平衡和标注错误问题。


<details>
  <summary>Details</summary>
Motivation: 传统视觉语言模型微调过程容易受到数据偏差、分布不平衡和标注错误的影响，导致过拟合和性能不均衡。现有合成数据生成方法缺乏对分布偏差和标注质量的控制。

Method: 1) 通过全面采样物体属性（颜色、形状、大小、位置）自动构建无偏、平衡的合成数据集；2) 使用该数据集微调最先进的视觉语言模型，并在绝对位置任务上评估向真实数据的迁移性能。

Result: 实验表明：1) 在平衡合成数据上微调能在整个视觉场景中产生均匀性能并缓解常见偏差；2) 在合成刺激上微调显著提高了在真实数据（COCO）上的性能，优于在匹配设置下微调的模型。

Conclusion: 通过控制合成数据生成过程，可以创建无偏、平衡的训练数据，有效提高视觉语言模型的性能并改善向真实数据的迁移能力。

Abstract: Fine-tuning Vision-Language Models (VLMs) is a common strategy to improve performance following an ad-hoc data collection and annotation of real-world scenes. However, this process is often prone to biases, errors, and distribution imbalance, resulting in overfitting and imbalanced performance. Although a few studies have tried to address this problem by generating synthetic data, they lacked control over distribution bias and annotation quality. To address these challenges, we redesign the fine-tuning process in two ways. First, we control the generation of data and its annotations, ensuring it is free from bias, distribution imbalance, and annotation errors. We automatically construct the dataset by comprehensively sampling objects' attributes, including color, shape, size, and position within the scene. Secondly, using this annotated dataset, we fine-tune state-of-the-art VLMs and assess performance transferability to real-world data on the absolute position task. We conduct exhaustive evaluations on both synthetic and real-world benchmarks. Our experiments reveal two key findings: 1) fine-tuning on balanced synthetic data yields uniform performance across the visual scene and mitigates common biases; and 2) fine-tuning on synthetic stimuli significantly improves performance on real-world data (COCO), outperforming models fine-tuned in the matched setting.

</details>


### [90] [DocLens : A Tool-Augmented Multi-Agent Framework for Long Visual Document Understanding](https://arxiv.org/abs/2511.11552)
*Dawei Zhu,Rui Meng,Jiefeng Chen,Sujian Li,Tomas Pfister,Jinsung Yoon*

Main category: cs.CV

TL;DR: DocLens是一个工具增强的多智能体框架，通过"放大"证据来解决长视觉文档理解中的证据定位挑战，在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在长视觉文档理解中存在证据定位的根本挑战，难以检索相关页面并忽略视觉元素中的细粒度细节，导致性能有限和模型幻觉。

Method: 提出DocLens框架，首先从完整文档导航到相关页面上的特定视觉元素，然后采用采样-裁决机制生成单一可靠答案。

Result: 与Gemini-2.5-Pro配对，DocLens在MMLongBench-Doc和FinRAGBench-V上达到最先进性能，甚至超越人类专家，在视觉中心和不可回答查询上表现尤为突出。

Conclusion: DocLens通过增强的定位能力展示了其强大性能，特别是在处理视觉中心和不可回答查询时，证明了其证据定位方法的有效性。

Abstract: Comprehending long visual documents, where information is distributed across extensive pages of text and visual elements, is a critical but challenging task for modern Vision-Language Models (VLMs). Existing approaches falter on a fundamental challenge: evidence localization. They struggle to retrieve relevant pages and overlook fine-grained details within visual elements, leading to limited performance and model hallucination. To address this, we propose DocLens, a tool-augmented multi-agent framework that effectively ``zooms in'' on evidence like a lens. It first navigates from the full document to specific visual elements on relevant pages, then employs a sampling-adjudication mechanism to generate a single, reliable answer. Paired with Gemini-2.5-Pro, DocLens achieves state-of-the-art performance on MMLongBench-Doc and FinRAGBench-V, surpassing even human experts. The framework's superiority is particularly evident on vision-centric and unanswerable queries, demonstrating the power of its enhanced localization capabilities.

</details>
