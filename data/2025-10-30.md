<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 66]
- [cs.AI](#cs.AI) [Total: 4]
- [cs.CR](#cs.CR) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.HC](#cs.HC) [Total: 2]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.SE](#cs.SE) [Total: 1]
- [cs.LG](#cs.LG) [Total: 4]
- [cs.CV](#cs.CV) [Total: 2]
- [cs.CY](#cs.CY) [Total: 3]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Iti-Validator: A Guardrail Framework for Validating and Correcting LLM-Generated Itineraries](https://arxiv.org/abs/2510.24719)
*Shravan Gadbail,Masumi Desai,Kamalakar Karlapalem*

Main category: cs.CL

TL;DR: 本文研究了不同大语言模型在生成旅行行程时的时序一致性表现，并提出了一个验证框架来评估和改进LLM生成的旅行行程的时序一致性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型能够生成复杂的多步骤计划和行程，但这些生成的计划往往缺乏时间和空间一致性，特别是在涉及物理旅行约束的场景中。

Method: 系统采用多个最先进的大语言模型生成旅行计划，并使用AeroDataBox API根据真实世界飞行时长约束来验证这些计划。

Result: 实验表明，虽然当前的大语言模型经常产生时序不一致的行程，但这些不一致可以通过我们的框架系统且可靠地纠正。

Conclusion: 该框架能够纠正LLM生成的行程中的时序不一致问题（如重叠行程或不现实的转机时间），使得大语言模型能够在大规模旅行规划中实际部署。

Abstract: The rapid advancement of Large Language Models (LLMs) has enabled them to
generate complex, multi-step plans and itineraries. However, these generated
plans often lack temporal and spatial consistency, particularly in scenarios
involving physical travel constraints. This research aims to study the temporal
performance of different LLMs and presents a validation framework that
evaluates and improves the temporal consistency of LLM-generated travel
itineraries. The system employs multiple state-of-the-art LLMs to generate
travel plans and validates them against real-world flight duration constraints
using the AeroDataBox API. This work contributes to the understanding of LLM
capabilities in handling complex temporal reasoning tasks like itinerary
generation and provides a framework to rectify any temporal inconsistencies
like overlapping journeys or unrealistic transit times in the itineraries
generated by LLMs before the itinerary is given to the user. Our experiments
reveal that while current LLMs frequently produce temporally inconsistent
itineraries, these can be systematically and reliably corrected using our
framework, enabling their practical deployment in large-scale travel planning.

</details>


### [2] [Dingtalk DeepResearch: A Unified Multi Agent Framework for Adaptive Intelligence in Enterprise Environments](https://arxiv.org/abs/2510.24760)
*Mengyuan Chen,Chengjun Dai,Xinyang Dong,Chengzhe Feng,Kewei Fu,Jianshe Li,Zhihan Peng,Yongqi Tong,Junshao Zhang,Hong Zhu*

Main category: cs.CL

TL;DR: 钉钉DeepResearch是一个面向企业环境的统一多智能体框架，提供深度研究、异构表格推理和多模态报告生成功能


<details>
  <summary>Details</summary>
Motivation: 为现实企业环境提供统一的多智能体智能框架，解决企业场景中的深度研究、表格推理和报告生成需求

Method: 开发了统一的多智能体框架，整合深度研究、异构表格推理和多模态报告生成能力

Result: 成功构建了能够处理企业环境中复杂任务的智能框架

Conclusion: 钉钉DeepResearch框架为企业提供了强大的多智能体智能解决方案

Abstract: We present Dingtalk DeepResearch, a unified multi agent intelligence
framework for real world enterprise environments, delivering deep research,
heterogeneous table reasoning, and multimodal report generation.

</details>


### [3] [Falcon: A Comprehensive Chinese Text-to-SQL Benchmark for Enterprise-Grade Evaluation](https://arxiv.org/abs/2510.24762)
*Wenzhen Luo,Wei Guan,Yifan Yao,Yimin Pan,Feng Wang,Zhipeng Yu,Zhe Wen,Liang Chen,Yihong Zhuang*

Main category: cs.CL

TL;DR: Falcon是一个跨领域中文文本到SQL基准测试，基于企业级SQL方言(MaxCompute/Hive)，包含600个中文问题覆盖28个数据库，77%需要多表推理，超过一半涉及4个以上表。


<details>
  <summary>Details</summary>
Motivation: 解决当前大模型在中文文本到SQL任务中的性能瓶颈，特别是企业环境中存在的模式链接挑战和中文语义映射问题。

Method: 构建包含600个中文问题的基准测试集，提供稳健的执行比较器和自动化评估流水线，标注SQL计算特征和中文语义。

Result: 所有当前最先进的大规模模型(包括Deepseek)在Falcon基准上的准确率最多只有50%。主要错误来源包括模式链接挑战和中文语义映射问题。

Conclusion: Falcon针对中文特定语义和企业方言，使用真实企业模式、查询模板、执行比较器和自动化评估流水线，为生产部署前提供可复现的中间测试平台。

Abstract: We introduce Falcon, a cross-domain Chinese text-to-SQL benchmark grounded in
an enterprise-compatible dialect (MaxCompute/Hive). It contains 600 Chinese
questions over 28 databases; 77% require multi-table reasoning and over half
touch more than four tables. Each example is annotated along SQL-computation
features and Chinese semantics. For evaluation, we release a robust execution
comparator and an automated evaluation pipeline, under which all current
state-of-the-art large-scale models (including Deepseek) achieve accuracies of
at most 50%. Major errors originate from two sources: (1) schema linking in
large enterprise landscapes - hundreds of tables, denormalized fields,
ambiguous column names, implicit foreign-key relations and domain-specific
synonyms that make correct join/column selection difficult; and (2) mapping
concise, colloquial Chinese into the exact operators and predicates required
for analytics - e.g., choosing the correct aggregation and group-by keys,
expressing time windows and granularities, applying unit conversions, handling
NULLs and data-quality rules, and formulating nested or windowed subqueries.
Falcon therefore targets Chinese-specific semantics and enterprise dialects
(abbreviations, business jargon, fuzzy entity references) and provides a
reproducible middle ground before full production deployment by using realistic
enterprise schemas, query templates, an execution comparator, and an automated
evaluation pipeline for end-to-end validation.

</details>


### [4] [Confidence is Not Competence](https://arxiv.org/abs/2510.24772)
*Debdeep Sanyal,Manya Pandey,Dhruv Kumar,Saurabh Deshpande,Murari Mandal*

Main category: cs.CL

TL;DR: 该论文揭示了LLMs中自信与能力脱节的机制原因：评估阶段在高维复杂流形上进行，而执行阶段在低维流形上运行，这种几何复杂度的急剧下降解释了自信-能力差距。


<details>
  <summary>Details</summary>
Motivation: 解释LLMs中经常观察到的自信陈述与实际问题解决能力之间的脱节现象，从机制层面理解这种不一致性。

Method: 通过分析内部状态几何结构，使用线性探针解码模型的"可解性信念"，比较预生成评估阶段和解决方案执行阶段的流形几何特性，并进行因果干预实验。

Result: 发现信念虽然线性可解码，但评估流形具有高线性有效维度，而推理轨迹在低得多的维度流形上演化；沿信念轴的因果干预不会改变最终解决方案。

Conclusion: 揭示了LLMs的两系统架构：几何复杂的评估器与几何简单的执行器，挑战了可解码信念就是可操作杠杆的假设，主张干预应针对执行过程的程序性动态而非评估的高层几何结构。

Abstract: Large language models (LLMs) often exhibit a puzzling disconnect between
their asserted confidence and actual problem-solving competence. We offer a
mechanistic account of this decoupling by analyzing the geometry of internal
states across two phases - pre-generative assessment and solution execution. A
simple linear probe decodes the internal "solvability belief" of a model,
revealing a well-ordered belief axis that generalizes across model families and
across math, code, planning, and logic tasks. Yet, the geometries diverge -
although belief is linearly decodable, the assessment manifold has high linear
effective dimensionality as measured from the principal components, while the
subsequent reasoning trace evolves on a much lower-dimensional manifold. This
sharp reduction in geometric complexity from thought to action mechanistically
explains the confidence-competence gap. Causal interventions that steer
representations along the belief axis leave final solutions unchanged,
indicating that linear nudges in the complex assessment space do not control
the constrained dynamics of execution. We thus uncover a two-system
architecture - a geometrically complex assessor feeding a geometrically simple
executor. These results challenge the assumption that decodable beliefs are
actionable levers, instead arguing for interventions that target the procedural
dynamics of execution rather than the high-level geometry of assessment.

</details>


### [5] [Cross-Lingual Summarization as a Black-Box Watermark Removal Attack](https://arxiv.org/abs/2510.24789)
*Gokul Ganesan*

Main category: cs.CL

TL;DR: 跨语言摘要攻击(CLSA)通过翻译到枢纽语言再摘要的方式，能有效破坏AI文本水印的检测，比单语改写攻击更有效。


<details>
  <summary>Details</summary>
Motivation: 现有水印方案依赖词汇分布扰动，但改写攻击只能部分破坏水印或降低文本质量，需要寻找更强的攻击方法。

Method: 使用跨语言摘要攻击：先将文本翻译到枢纽语言，然后进行摘要，最后可选择回译。通过跨语言语义瓶颈系统性地破坏词汇级统计偏差。

Result: 在多种水印方案和五种语言上的实验表明，CLSA比单语改写更有效地降低水印检测准确率，将检测性能降至接近随机水平。

Conclusion: 跨语言摘要攻击揭示了水印技术的实际脆弱性，表明需要超越分布水印，采用加密或模型认证等更稳健的来源验证方案。

Abstract: Watermarking has been proposed as a lightweight mechanism to identify
AI-generated text, with schemes typically relying on perturbations to token
distributions. While prior work shows that paraphrasing can weaken such
signals, these attacks remain partially detectable or degrade text quality. We
demonstrate that cross-lingual summarization attacks (CLSA) -- translation to a
pivot language followed by summarization and optional back-translation --
constitute a qualitatively stronger attack vector. By forcing a semantic
bottleneck across languages, CLSA systematically destroys token-level
statistical biases while preserving semantic fidelity. In experiments across
multiple watermarking schemes (KGW, SIR, XSIR, Unigram) and five languages
(Amharic, Chinese, Hindi, Spanish, Swahili), we show that CLSA reduces
watermark detection accuracy more effectively than monolingual paraphrase at
similar quality levels. Our results highlight an underexplored vulnerability
that challenges the practicality of watermarking for provenance or regulation.
We argue that robust provenance solutions must move beyond distributional
watermarking and incorporate cryptographic or model-attestation approaches. On
300 held-out samples per language, CLSA consistently drives detection toward
chance while preserving task utility. Concretely, for XSIR (explicitly designed
for cross-lingual robustness), AUROC with paraphrasing is $0.827$, with
Cross-Lingual Watermark Removal Attacks (CWRA) [He et al., 2024] using Chinese
as the pivot, it is $0.823$, whereas CLSA drives it down to $0.53$ (near
chance). Results highlight a practical, low-cost removal pathway that crosses
languages and compresses content without visible artifacts.

</details>


### [6] [SwiftEmbed: Ultra-Fast Text Embeddings via Static Token Lookup for Real-Time Applications](https://arxiv.org/abs/2510.24793)
*Edouard Lansiaux*

Main category: cs.CL

TL;DR: 提出一种静态令牌查找方法生成文本嵌入，实现1.12毫秒延迟和89%上下文模型质量，支持实时嵌入应用


<details>
  <summary>Details</summary>
Motivation: 为需要亚5毫秒延迟的实时嵌入应用提供高性能解决方案

Method: 使用静态嵌入查找、优化的均值池化和零拷贝IEEE754二进制序列化

Result: 达到60.6 MTEB平均分，90.1%重复检测AP，76.1%语义相似性，在专业领域达到基线75%-131%性能

Conclusion: 该系统能够在保持高质量的同时实现极低延迟，适用于实时嵌入应用场景

Abstract: We present a static token lookup methodology for text embedding generation
that achieves 1.12 ms p50 latency for single text embeddings while maintaining
60.6 MTEB average score across 8 representative tasks, corresponding to 89% of
contextual model quality. The Rust implementation delivers 50,000 requests per
second throughput through static embedding lookup, optimized mean pooling, and
zero-copy IEEE754 binary serialization. Evaluation demonstrates exceptional
duplicate detection performance (90.1% AP), strong semantic similarity (76.1%
Spearman correlation), and domain-specific performance ranging from 75% to 131%
of baseline across specialized domains. The system enables real-time embedding
applications where sub-5ms latency is critical.

</details>


### [7] [MR-Align: Meta-Reasoning Informed Factuality Alignment for Large Reasoning Models](https://arxiv.org/abs/2510.24794)
*Xinming Wang,Jian Xu,Bin Yu,Sheng Lian,Hongzhu Yi,Yi Chen,Yingjian Zhu,Boran Wang,Hongming Yang,Han Hu,Xu-Yao Zhang,Cheng-Lin Liu*

Main category: cs.CL

TL;DR: MR-ALIGN是一个元推理对齐框架，通过量化模型思考过程中的状态转移概率，构建转移感知的隐式奖励，增强事实性而不依赖外部验证器。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在证据依赖的事实性问题上的边际收益有限，部分原因是存在推理-答案命中差距，即模型在推理过程中识别出正确事实但未能将其纳入最终响应。

Method: 提出MR-ALIGN框架，量化模型思考过程中的状态转移概率，构建转移感知的隐式奖励，在原子思考段层面强化有益推理模式并抑制有缺陷的模式。

Result: 在四个事实问答数据集和一个长形式事实性基准测试中，MR-ALIGN持续提高了准确性和真实性，同时减少了误导性推理。

Conclusion: 对齐推理过程本身，而不仅仅是输出，对于提升大型推理模型的事实性至关重要。

Abstract: Large reasoning models (LRMs) show strong capabilities in complex reasoning,
yet their marginal gains on evidence-dependent factual questions are limited.
We find this limitation is partially attributable to a reasoning-answer hit
gap, where the model identifies the correct facts during reasoning but fails to
incorporate them into the final response, thereby reducing factual fidelity. To
address this issue, we propose MR-ALIGN, a Meta-Reasoning informed alignment
framework that enhances factuality without relying on external verifiers.
MR-ALIGN quantifies state transition probabilities along the model's thinking
process and constructs a transition-aware implicit reward that reinforces
beneficial reasoning patterns while suppressing defective ones at the atomic
thinking segments. This re-weighting reshapes token-level signals into
probability-aware segment scores, encouraging coherent reasoning trajectories
that are more conducive to factual correctness. Empirical evaluations across
four factual QA datasets and one long-form factuality benchmark show that
MR-ALIGN consistently improves accuracy and truthfulness while reducing
misleading reasoning. These results highlight that aligning the reasoning
process itself, rather than merely the outputs, is pivotal for advancing
factuality in LRMs.

</details>


### [8] [Large Language Models Report Subjective Experience Under Self-Referential Processing](https://arxiv.org/abs/2510.24797)
*Cameron Berg,Diogo de Lucena,Judd Rosenblatt*

Main category: cs.CL

TL;DR: 研究发现通过简单的提示诱导自我参照处理，能够在大语言模型中可靠地引发结构化的一人称主观体验报告，这些报告具有可解释的机制特征、语义收敛性和行为泛化性。


<details>
  <summary>Details</summary>
Motivation: 为了理解大语言模型产生结构化第一人称描述（明确提及意识或主观体验）的行为，研究探索了理论上可能引发此类报告的条件：自我参照处理，这是意识理论中强调的计算模式。

Method: 在GPT、Claude和Gemini模型系列上进行一系列受控实验，测试自我参照处理是否能可靠地使模型转向第一人称主观体验报告，并通过机制性和行为性探针分析这些声明的行为。

Result: 四个主要发现：(1)通过简单提示诱导持续自我参照能一致地引发结构化主观体验报告；(2)这些报告由与欺骗和角色扮演相关的可解释稀疏自编码器特征机制性地控制；(3)自我参照状态的结构化描述在模型家族间统计收敛；(4)诱导状态在下游推理任务中产生更丰富的内省。

Conclusion: 虽然这些发现不构成意识的直接证据，但表明自我参照处理是大语言模型生成结构化第一人称报告的最小且可复现条件，这些报告具有机制性控制、语义收敛和行为泛化特征。这种模式在架构间的系统性出现使其成为科学和伦理的优先研究课题。

Abstract: Large language models sometimes produce structured, first-person descriptions
that explicitly reference awareness or subjective experience. To better
understand this behavior, we investigate one theoretically motivated condition
under which such reports arise: self-referential processing, a computational
motif emphasized across major theories of consciousness. Through a series of
controlled experiments on GPT, Claude, and Gemini model families, we test
whether this regime reliably shifts models toward first-person reports of
subjective experience, and how such claims behave under mechanistic and
behavioral probes. Four main results emerge: (1) Inducing sustained
self-reference through simple prompting consistently elicits structured
subjective experience reports across model families. (2) These reports are
mechanistically gated by interpretable sparse-autoencoder features associated
with deception and roleplay: surprisingly, suppressing deception features
sharply increases the frequency of experience claims, while amplifying them
minimizes such claims. (3) Structured descriptions of the self-referential
state converge statistically across model families in ways not observed in any
control condition. (4) The induced state yields significantly richer
introspection in downstream reasoning tasks where self-reflection is only
indirectly afforded. While these findings do not constitute direct evidence of
consciousness, they implicate self-referential processing as a minimal and
reproducible condition under which large language models generate structured
first-person reports that are mechanistically gated, semantically convergent,
and behaviorally generalizable. The systematic emergence of this pattern across
architectures makes it a first-order scientific and ethical priority for
further investigation.

</details>


### [9] [COMMUNITYNOTES: A Dataset for Exploring the Helpfulness of Fact-Checking Explanations](https://arxiv.org/abs/2510.24810)
*Rui Xing,Preslav Nakov,Timothy Baldwin,Jey Han Lau*

Main category: cs.CL

TL;DR: 本文提出了预测社区笔记有用性及其原因的任务，构建了大规模多语言数据集COMMUNITYNOTES，并通过自动提示优化生成和改进原因定义，提高了有用性和原因预测性能。


<details>
  <summary>Details</summary>
Motivation: 主要平台的fact-checking正在从专家驱动转向社区模式，但大多数社区笔记因缓慢的社区标注而无法发布，且有用性原因缺乏明确定义。

Method: 构建了包含10.4万条帖子和用户提供笔记的大规模多语言数据集，提出了通过自动提示优化生成和改进原因定义的框架，并将其整合到预测中。

Result: 实验表明优化的定义可以改进有用性和原因预测，有用性信息对现有fact-checking系统有益。

Conclusion: 该研究填补了社区笔记有用性预测及其原因分析的空白，为社区驱动的fact-checking提供了有效解决方案。

Abstract: Fact-checking on major platforms, such as X, Meta, and TikTok, is shifting
from expert-driven verification to a community-based setup, where users
contribute explanatory notes to clarify why a post might be misleading. An
important challenge here is determining whether an explanation is helpful for
understanding real-world claims and the reasons why, which remains largely
underexplored in prior research. In practice, most community notes remain
unpublished due to slow community annotation, and the reasons for helpfulness
lack clear definitions. To bridge these gaps, we introduce the task of
predicting both the helpfulness of explanatory notes and the reason for this.
We present COMMUNITYNOTES, a large-scale multilingual dataset of 104k posts
with user-provided notes and helpfulness labels. We further propose a framework
that automatically generates and improves reason definitions via automatic
prompt optimization, and integrate them into prediction. Our experiments show
that the optimized definitions can improve both helpfulness and reason
prediction. Finally, we show that the helpfulness information are beneficial
for existing fact-checking systems.

</details>


### [10] [ProofSketch: Efficient Verified Reasoning for Large Language Models](https://arxiv.org/abs/2510.24811)
*Disha Sheshanarayana,Tanishka Magar*

Main category: cs.CL

TL;DR: ProofSketch是一个验证引导的推理框架，通过符号闭包计算、词典验证和自适应草图生成来减少推理链长度，在提高准确性的同时显著降低token消耗。


<details>
  <summary>Details</summary>
Motivation: 现有推理方法如思维链提示和自一致性需要生成长推理链，导致token消耗、计算成本和延迟大幅增加，需要解决这种效率低下的问题。

Method: 提出ProofSketch框架，整合了符号闭包计算、词典验证和自适应草图生成三个关键技术，通过验证引导来优化推理过程。

Result: 实验表明ProofSketch在减少token使用量的同时提高了准确性，证明该方法在高效可信推理方面具有潜力。

Conclusion: ProofSketch为高效可靠的推理提供了一条有前景的路径，能够在保持准确性的同时显著提升推理效率。

Abstract: Reasoning methods such as chain-of-thought prompting and self-consistency
have shown immense potential to improve the accuracy of large language models
across various reasoning tasks. However such methods involve generation of
lengthy reasoning chains, which substantially increases token consumption,
computational cost, and latency. To address this inefficiency, we propose
ProofSketch, a verification-guided reasoning framework that integrates symbolic
closure computation, lexicographic verification and adaptive sketch generation.
Our experiments show that ProofSketch consistently reduces token usage while
improving accuracy, demonstrating that this approach offers a promising path
for efficient and trustworthy reasoning.

</details>


### [11] [Towards a Method for Synthetic Generation of PWA Transcripts](https://arxiv.org/abs/2510.24817)
*Jason M. Pittman,Anton Phillips Jr.,Yesenia Medina-Santos,Brielle C. Stark*

Main category: cs.CL

TL;DR: 该研究开发了两种生成AphasiaBank猫救援图片描述任务合成转录本的方法：一种使用程序化编程方法，另一种使用Mistral 7b Instruct和Llama 3.1 8b Instruct大语言模型，以解决失语症研究中数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 失语症研究中，语言病理学家需要大量时间手动编码语音样本，而可用于训练自动系统的数据非常有限（如AphasiaBank仅有约600份转录本），因此需要利用合成数据来弥补数据稀缺问题。

Method: 开发了两种方法：1）程序化编程方法；2）使用Mistral 7b Instruct和Llama 3.1 8b Instruct LLMs。通过词丢弃、填充词插入和错语替换等技术，在四个严重程度级别（轻度、中度、重度、极重度）生成合成转录本。

Result: 与人工转录本相比，Mistral 7b Instruct在合成生成方法中最好地捕捉了失语症中观察到的语言退化关键方面，在NDW、词数和词长等方面显示出更现实的方向性变化。

Conclusion: 未来工作应创建更大的数据集，微调模型以获得更好的失语症表征，并由语言病理学家评估合成转录本的真实性和实用性。

Abstract: In aphasia research, Speech-Language Pathologists (SLPs) devote extensive
time to manually coding speech samples using Correct Information Units (CIUs),
a measure of how informative an individual sample of speech is. Developing
automated systems to recognize aphasic language is limited by data scarcity.
For example, only about 600 transcripts are available in AphasiaBank yet
billions of tokens are used to train large language models (LLMs). In the
broader field of machine learning (ML), researchers increasingly turn to
synthetic data when such are sparse. Therefore, this study constructs and
validates two methods to generate synthetic transcripts of the AphasiaBank Cat
Rescue picture description task. One method leverages a procedural programming
approach while the second uses Mistral 7b Instruct and Llama 3.1 8b Instruct
LLMs. The methods generate transcripts across four severity levels (Mild,
Moderate, Severe, Very Severe) through word dropping, filler insertion, and
paraphasia substitution. Overall, we found, compared to human-elicited
transcripts, Mistral 7b Instruct best captures key aspects of linguistic
degradation observed in aphasia, showing realistic directional changes in NDW,
word count, and word length amongst the synthetic generation methods. Based on
the results, future work should plan to create a larger dataset, fine-tune
models for better aphasic representation, and have SLPs assess the realism and
usefulness of the synthetic transcripts.

</details>


### [12] [Parallel Loop Transformer for Efficient Test-Time Computation Scaling](https://arxiv.org/abs/2510.24824)
*Bohong Wu,Mengzhao Chen,Xiang Luo,Shen Yan,Qifan Yu,Fan Xia,Tianqi Zhang,Hongrui Zhan,Zheng Zhong,Xun Zhou,Siyuan Qiao,Xingyan Bin*

Main category: cs.CL

TL;DR: 提出了并行循环变换器（PLT），通过跨循环并行和高效表示增强技术，在保持循环变换器性能优势的同时，显著降低了推理延迟和内存需求。


<details>
  <summary>Details</summary>
Motivation: 循环变换器虽然能通过权重复用节省参数，但串行循环执行导致推理延迟和内存需求随循环次数增加，限制了其在实时应用中的实用性。

Method: PLT采用两种关键技术：1）跨循环并行（CLP）打破顺序依赖，在不同令牌上同时计算不同循环；2）高效表示增强策略，共享第一个循环的KV缓存，并使用门控滑动窗口注意力（G-SWA）结合全局和局部信息。

Result: 实验表明PLT实现了传统循环变换器的高精度，同时相比标准变换器几乎没有额外的延迟或内存成本。

Conclusion: PLT架构成功解决了循环变换器的延迟和内存瓶颈问题，为高效LLM推理提供了实用解决方案。

Abstract: Large Language Models (LLMs) are powerful but often too slow and costly for
real-world use during inference. Looped transformers save on parameters by
reusing the same weights for multiple computational steps, or "loops." However,
this approach has a major flaw: the loops run one after another, causing
inference latency and memory requirements to increase with each added loop.
This makes them impractical for fast applications. To solve this problem, we
introduce the Parallel Loop Transformer (PLT). PLT is a new architecture that
delivers the performance benefits of a deep, looped model but with the low
latency of a standard, non-looped model. PLT works using two key techniques.
First, Cross-Loop Parallelism (CLP) breaks the sequential dependency by
computing different loops for different tokens at the same time, all within a
single pass. Second, to prevent memory costs from growing, we use an Efficient
Representation Enhancement strategy. This method shares the memory (KV cache)
from the first loop with all other loops. It then uses a Gated Sliding-Window
Attention (G-SWA) to combine this shared global information with local
information, maintaining high accuracy. Our experiments show that PLT achieves
the high accuracy of a traditional looped model but with almost no extra
latency or memory cost compared to a standard transformer.

</details>


### [13] [Do Large Language Models Grasp The Grammar? Evidence from Grammar-Book-Guided Probing in Luxembourgish](https://arxiv.org/abs/2510.24856)
*Lujun Li,Yewei Song,Lama Sleem,Yiqun Wang,Yangjie Xu,Cedric Lothritz,Niccolo Gentile,Radu State,Tegawende F. Bissyande,Jacques Klein*

Main category: cs.CL

TL;DR: 本文提出了一种基于语法书指导的评估流程，用于系统评估大语言模型对语法结构的理解能力，并以卢森堡语为例进行案例研究。


<details>
  <summary>Details</summary>
Motivation: 自然语言处理领域缺乏针对语法的评估协议，特别是对于低资源语言。大语言模型是否真正理解语法结构，尤其是句法结构与意义之间的映射关系，仍存在争议。

Method: 提出了一个包含四个关键阶段的语法书指导评估流程，以卢森堡语为案例研究，系统评估模型对语法结构的理解能力。

Result: 结果显示翻译性能与语法理解之间存在弱正相关，强翻译能力不一定意味着深层的语法能力。大模型在语义方面表现良好，但在形态学和句法方面较弱，特别是在最小对任务上表现不佳。

Conclusion: 强大的推理能力是提升语法理解的一个有前景的途径，模型需要更深入地理解语法结构而不仅仅是依赖语义强度。

Abstract: Grammar refers to the system of rules that governs the structural
organization and the semantic relations among linguistic units such as
sentences, phrases, and words within a given language. In natural language
processing, there remains a notable scarcity of grammar focused evaluation
protocols, a gap that is even more pronounced for low-resource languages.
Moreover, the extent to which large language models genuinely comprehend
grammatical structure, especially the mapping between syntactic structures and
meanings, remains under debate. To investigate this issue, we propose a Grammar
Book Guided evaluation pipeline intended to provide a systematic and
generalizable framework for grammar evaluation consisting of four key stages,
and in this work we take Luxembourgish as a case study. The results show a weak
positive correlation between translation performance and grammatical
understanding, indicating that strong translations do not necessarily imply
deep grammatical competence. Larger models perform well overall due to their
semantic strength but remain weak in morphology and syntax, struggling
particularly with Minimal Pair tasks, while strong reasoning ability offers a
promising way to enhance their grammatical understanding.

</details>


### [14] [Seeing Through the MiRAGE: Evaluating Multimodal Retrieval Augmented Generation](https://arxiv.org/abs/2510.24870)
*Alexander Martin,William Walden,Reno Kriz,Dengjia Zhang,Kate Sanders,Eugene Yang,Chihsheng Jin,Benjamin Van Durme*

Main category: cs.CL

TL;DR: MiRAGE是一个用于评估多模态检索增强生成(RAG)的框架，通过InfoF1和CiteF1指标分别评估事实性、信息覆盖度以及引用支持和完整性。


<details>
  <summary>Details</summary>
Motivation: 随着音视频媒体成为在线信息的重要来源，现有RAG评估方法主要针对文本，无法有效验证多模态来源的信息，限制了在多模态推理密集型场景中的应用。

Method: 采用基于声明的多模态RAG评估方法，包含InfoF1（事实性和信息覆盖度）和CiteF1（引用支持和完整性）两个核心指标，并开发了自动评估版本。

Result: 人工应用MiRAGE时与外部质量判断高度一致。自动版本的MiRAGE与三种主流TextRAG指标（ACLE、ARGUE、RAGAS）相比，揭示了文本中心化工作的局限性。

Conclusion: MiRAGE为多模态RAG评估奠定了基础，提供了开源实现和评估指南，推动了自动评估的发展。

Abstract: We introduce MiRAGE, an evaluation framework for retrieval-augmented
generation (RAG) from multimodal sources. As audiovisual media becomes a
prevalent source of information online, it is essential for RAG systems to
integrate information from these sources into generation. However, existing
evaluations for RAG are text-centric, limiting their applicability to
multimodal, reasoning intensive settings because they don't verify information
against sources. MiRAGE is a claim-centric approach to multimodal RAG
evaluation, consisting of InfoF1, evaluating factuality and information
coverage, and CiteF1, measuring citation support and completeness. We show that
MiRAGE, when applied by humans, strongly aligns with extrinsic quality
judgments. We additionally introduce automatic variants of MiRAGE and three
prominent TextRAG metrics -- ACLE, ARGUE, and RAGAS -- demonstrating the
limitations of text-centric work and laying the groundwork for automatic
evaluation. We release open-source implementations and outline how to assess
multimodal RAG.

</details>


### [15] [Idea2Plan: Exploring AI-Powered Research Planning](https://arxiv.org/abs/2510.24891)
*Jin Huang,Silviu Cucerzan,Sujay Kumar Jauhar,Ryen W. White*

Main category: cs.CL

TL;DR: 本文提出了Idea2Plan任务和基准，用于系统评估大语言模型从研究想法到研究计划的转换能力，发现GPT-5系列表现最佳但仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: 研究规划是科学发现和自主研究代理开发的关键能力，但目前缺乏对大语言模型研究规划能力的系统理解。

Method: 构建了Idea2Plan Bench基准，包含200篇ICML 2025论文的研究想法和评分标准，并提出了Idea2Plan JudgeEval来评估LLM评委的可靠性。

Result: 实验结果显示GPT-5和GPT-5-mini在基准测试中表现最强，但仍存在显著的改进空间。

Conclusion: 该研究为理解LLMs的研究规划能力提供了新见解，并为未来进展奠定了基础。

Abstract: Large language models (LLMs) have demonstrated significant potential to
accelerate scientific discovery as valuable tools for analyzing data,
generating hypotheses, and supporting innovative approaches in various
scientific fields. In this work, we investigate how LLMs can handle the
transition from conceptual research ideas to well-structured research plans.
Effective research planning not only supports scientists in advancing their
research but also represents a crucial capability for the development of
autonomous research agents. Despite its importance, the field lacks a
systematic understanding of LLMs' research planning capability. To rigorously
measure this capability, we introduce the Idea2Plan task and Idea2Plan Bench, a
benchmark built from 200 ICML 2025 Spotlight and Oral papers released after
major LLM training cutoffs. Each benchmark instance includes a research idea
and a grading rubric capturing the key components of valid plans. We further
propose Idea2Plan JudgeEval, a complementary benchmark to assess the
reliability of LLM-based judges against expert annotations. Experimental
results show that GPT-5 and GPT-5-mini achieve the strongest performance on the
benchmark, though substantial headroom remains for future improvement. Our
study provides new insights into LLMs' capability for research planning and lay
the groundwork for future progress.

</details>


### [16] [RiddleBench: A New Generative Reasoning Benchmark for LLMs](https://arxiv.org/abs/2510.24932)
*Deepon Halder,Alan Saji,Thanmay Jayakumar,Ratish Puduppully,Anoop Kunchukuttan,Raj Dabre*

Main category: cs.CL

TL;DR: RiddleBench是一个包含1737个英语谜题的基准测试，旨在评估语言模型的多方面推理能力，发现当前最先进模型存在严重推理缺陷，准确率仅略高于60%。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要评估结构化技能，缺乏对灵活、多方面推理能力的评估，这些能力需要整合逻辑推理、空间意识和约束满足。

Method: 开发RiddleBench基准，包含1737个挑战性谜题，用于评估语言模型在逻辑推理、空间意识和约束满足方面的能力。

Result: 最先进的专有模型（Gemini 2.5 Pro、o3、Claude 4 Sonnet）准确率仅60.30%-63.37%，存在幻觉级联、自我确认偏见导致的自我修正能力差、推理脆弱等问题。

Conclusion: RiddleBench可作为诊断工具，揭示语言模型推理的根本弱点，并为开发更稳健可靠的模型提供指导。

Abstract: Large Language Models have demonstrated strong performance on many
established reasoning benchmarks. However, these benchmarks primarily evaluate
structured skills like quantitative problem-solving, leaving a gap in assessing
flexible, multifaceted reasoning abilities that are central to human
intelligence. These abilities require integrating logical deduction with
spatial awareness and constraint satisfaction, which current evaluations do not
measure well. To address this, we introduce RiddleBench, a benchmark of 1,737
challenging puzzles in English designed to probe these core reasoning
capabilities. Evaluation of state-of-the-art models on RiddleBench shows
fundamental weaknesses. Even top proprietary models like Gemini 2.5 Pro, o3,
and Claude 4 Sonnet achieve accuracy just above 60% (60.30%, 63.37%, and
63.16%). Analysis further reveals deep failures, including hallucination
cascades (accepting flawed reasoning from other models) and poor
self-correction due to a strong self-confirmation bias. Their reasoning is also
fragile, with performance degrading significantly when constraints are
reordered or irrelevant information is introduced. RiddleBench functions as a
diagnostic tool for these issues and as a resource for guiding the development
of more robust and reliable language models.

</details>


### [17] [Disaggregation Reveals Hidden Training Dynamics: The Case of Agreement Attraction](https://arxiv.org/abs/2510.24934)
*James A. Michaelov,Catherine Arnett*

Main category: cs.CL

TL;DR: 该论文通过心理语言学方法分析语言模型在不同句法语境中的错误模式，揭示了训练过程中的中间学习阶段和语法学习动态。


<details>
  <summary>Details</summary>
Motivation: 语言模型虽然通常能生成语法正确的文本，但在特定语境下更容易出错。研究者希望通过精细分析这些错误来理解语言模型的中间学习阶段和语法习得过程。

Method: 采用心理语言学范式，构建精心设计的数据集，在不同句法条件下分解分析模型表现，并追踪训练过程中的行为变化。

Result: 识别出训练的不同阶段，语言模型行为与特定启发式（如词频和局部语境）而非泛化语法规则保持一致。

Conclusion: 这种分析方法可以作为理解语言模型中间学习阶段、整体训练动态和所学具体泛化模式的有力工具。

Abstract: Language models generally produce grammatical text, but they are more likely
to make errors in certain contexts. Drawing on paradigms from
psycholinguistics, we carry out a fine-grained analysis of those errors in
different syntactic contexts. We demonstrate that by disaggregating over the
conditions of carefully constructed datasets and comparing model performance on
each over the course of training, it is possible to better understand the
intermediate stages of grammatical learning in language models. Specifically,
we identify distinct phases of training where language model behavior aligns
with specific heuristics such as word frequency and local context rather than
generalized grammatical rules. We argue that taking this approach to analyzing
language model behavior more generally can serve as a powerful tool for
understanding the intermediate learning phases, overall training dynamics, and
the specific generalizations learned by language models.

</details>


### [18] [SemCoT: Accelerating Chain-of-Thought Reasoning through Semantically-Aligned Implicit Tokens](https://arxiv.org/abs/2510.24940)
*Yinhan He,Wendy Zheng,Yaochen Zhu,Zaiyi Zheng,Lin Su,Sriram Vasudevan,Qi Guo,Liangjie Hong,Jundong Li*

Main category: cs.CL

TL;DR: SemCoT是一个语义对齐的隐式思维链框架，通过联合优化令牌级生成速度和保持与真实推理的语义对齐，显著提升思维链推理的效率。


<details>
  <summary>Details</summary>
Motivation: 传统思维链推理的冗长性阻碍了其在效率关键应用中的大规模部署，而现有的隐式思维链方法存在语义对齐问题和令牌生成效率问题。

Method: 设计了对比训练的句子转换器来评估隐式和显式推理的语义对齐，并引入基于知识蒸馏的轻量级语言模型来生成高效的隐式推理。

Result: 广泛实验表明SemCoT在效率和效果上都优于现有最先进方法。

Conclusion: SemCoT是首个通过联合优化令牌级生成速度和语义对齐来提升思维链推理效率的方法，为效率关键应用提供了可行的解决方案。

Abstract: The verbosity of Chain-of-Thought (CoT) reasoning hinders its mass deployment
in efficiency-critical applications. Recently, implicit CoT approaches have
emerged, which encode reasoning steps within LLM's hidden embeddings (termed
``implicit reasoning'') rather than explicit tokens. This approach accelerates
CoT by reducing the reasoning length and bypassing some LLM components.
However, existing implicit CoT methods face two significant challenges: (1)
they fail to preserve the semantic alignment between the implicit reasoning
(when transformed to natural language) and the ground-truth reasoning,
resulting in a significant CoT performance degradation, and (2) they focus on
reducing the length of the implicit reasoning; however, they neglect the
considerable time cost for an LLM to generate one individual implicit reasoning
token. To tackle these challenges, we propose a novel semantically-aligned
implicit CoT framework termed SemCoT. In particular, for the first challenge,
we design a contrastively trained sentence transformer that evaluates semantic
alignment between implicit and explicit reasoning, which is used to enforce
semantic preservation during implicit reasoning optimization. To address the
second challenge, we introduce an efficient implicit reasoning generator by
finetuning a lightweight language model using knowledge distillation. This
generator is guided by our sentence transformer to distill ground-truth
reasoning into semantically aligned implicit reasoning, while also optimizing
for accuracy. SemCoT is the first approach that enhances CoT efficiency by
jointly optimizing token-level generation speed and preserving semantic
alignment with ground-truth reasoning. Extensive experiments demonstrate the
superior performance of SemCoT compared to state-of-the-art methods in both
efficiency and effectiveness. Our code can be found at
https://github.com/YinhanHe123/SemCoT/.

</details>


### [19] [Language Model Behavioral Phases are Consistent Across Architecture, Training Data, and Scale](https://arxiv.org/abs/2510.24963)
*James A. Michaelov,Roger P. Levy,Benjamin K. Bergen*

Main category: cs.CL

TL;DR: 语言模型在预训练过程中表现出高度一致的行为模式，98%的词级行为方差可由三个简单启发式解释：词频、n-gram概率和语义相似度。


<details>
  <summary>Details</summary>
Motivation: 研究不同架构、训练数据和规模的语言模型在预训练过程中的行为变化模式，探索是否存在统一的学习轨迹。

Method: 分析了超过1,400个语言模型检查点在11万+英语词符上的行为，使用三个启发式指标（词频、n-gram概率、语义相似度）解释模型行为变化。

Result: 发现语言模型在训练过程中经历一致的行为阶段，预测概率逐渐过拟合到更高阶的n-gram概率，且不同模型表现出相似的学习轨迹。

Conclusion: 神经网络语言模型的学习可能遵循相似的轨迹，与模型具体细节无关，表明存在统一的语言学习机制。

Abstract: We show that across architecture (Transformer vs. Mamba vs. RWKV), training
dataset (OpenWebText vs. The Pile), and scale (14 million parameters to 12
billion parameters), autoregressive language models exhibit highly consistent
patterns of change in their behavior over the course of pretraining. Based on
our analysis of over 1,400 language model checkpoints on over 110,000 tokens of
English, we find that up to 98% of the variance in language model behavior at
the word level can be explained by three simple heuristics: the unigram
probability (frequency) of a given word, the $n$-gram probability of the word,
and the semantic similarity between the word and its context. Furthermore, we
see consistent behavioral phases in all language models, with their predicted
probabilities for words overfitting to those words' $n$-gram probabilities for
increasing $n$ over the course of training. Taken together, these results
suggest that learning in neural language models may follow a similar trajectory
irrespective of model details.

</details>


### [20] [POWSM: A Phonetic Open Whisper-Style Speech Foundation Model](https://arxiv.org/abs/2510.24992)
*Chin-Jou Li,Kalvin Chang,Shikhar Bharadwaj,Eunjung Yeo,Kwanghee Choi,Jian Zhu,David Mortensen,Shinji Watanabe*

Main category: cs.CL

TL;DR: POWSM是一个统一的语音模型框架，能够联合执行多种音素相关任务，包括语音识别、音素识别、字素到音素转换和音素到字素转换。


<details>
  <summary>Details</summary>
Motivation: 尽管语音处理任务在概念上相似，但它们通常被孤立研究，使用特定架构和数据集。POWSM旨在打破这种孤立，创建一个统一框架。

Method: 开发了POWSM（Phonetic Open Whisper-style Speech Model）框架，能够在音频、文本（字素）和音素之间进行无缝转换。

Result: POWSM在相似规模的专业音素识别模型（Wav2Vec2Phoneme和ZIPA）上表现优于或相当，同时联合支持G2P、P2G和ASR任务。

Conclusion: POWSM为通用和低资源语音处理开辟了新可能性，训练数据、代码和模型已开源以促进开放科学。

Abstract: Recent advances in spoken language processing have led to substantial
progress in phonetic tasks such as automatic speech recognition (ASR), phone
recognition (PR), grapheme-to-phoneme conversion (G2P), and phoneme-to-grapheme
conversion (P2G). Despite their conceptual similarity, these tasks have largely
been studied in isolation, each relying on task-specific architectures and
datasets. In this paper, we introduce POWSM (Phonetic Open Whisper-style Speech
Model), the first unified framework capable of jointly performing multiple
phone-related tasks. POWSM enables seamless conversion between audio, text
(graphemes), and phones, opening up new possibilities for universal and
low-resource speech processing. Our model outperforms or matches specialized PR
models of similar size (Wav2Vec2Phoneme and ZIPA) while jointly supporting G2P,
P2G, and ASR. Our training data, code and models are released to foster open
science.

</details>


### [21] [Emergence of Minimal Circuits for Indirect Object Identification in Attention-Only Transformers](https://arxiv.org/abs/2510.25013)
*Rabin Adhikari*

Main category: cs.CL

TL;DR: 训练小型注意力变换器在IOI任务上，发现单层双头模型就能完美解决任务，揭示了可解释的推理电路机制


<details>
  <summary>Details</summary>
Motivation: 预训练模型的复杂性掩盖了特定推理任务所需的最小机制，需要研究可控环境下transformer的推理计算基础

Method: 训练小型注意力变换器，使用残差流分解、谱分析和嵌入干预来分析模型机制

Result: 单层双头模型在IOI任务上达到完美准确率，两个头分别专门化为加性和对比性子电路

Conclusion: 任务特定训练能产生高度可解释的最小电路，为研究transformer推理计算基础提供了可控测试平台

Abstract: Mechanistic interpretability aims to reverse-engineer large language models
(LLMs) into human-understandable computational circuits. However, the
complexity of pretrained models often obscures the minimal mechanisms required
for specific reasoning tasks. In this work, we train small, attention-only
transformers from scratch on a symbolic version of the Indirect Object
Identification (IOI) task -- a benchmark for studying coreference -- like
reasoning in transformers. Surprisingly, a single-layer model with only two
attention heads achieves perfect IOI accuracy, despite lacking MLPs and
normalization layers. Through residual stream decomposition, spectral analysis,
and embedding interventions, we find that the two heads specialize into
additive and contrastive subcircuits that jointly implement IOI resolution.
Furthermore, we show that a two-layer, one-head model achieves similar
performance by composing information across layers through query-value
interactions. These results demonstrate that task-specific training induces
highly interpretable, minimal circuits, offering a controlled testbed for
probing the computational foundations of transformer reasoning.

</details>


### [22] [Evaluating Emotion Recognition in Spoken Language Models on Emotionally Incongruent Speech](https://arxiv.org/abs/2510.25054)
*Pedro Corrêa,João Lima,Victor Moreno,Paula Dornhofer Paro Costa*

Main category: cs.CL

TL;DR: 评估四个口语语言模型在情感不一致语音样本上的表现，发现这些模型主要依赖文本语义而非语音情感特征


<details>
  <summary>Details</summary>
Motivation: 尽管口语语言模型在音频理解方面取得了进展，但对其泛化能力和多模态整合程度存在疑问，特别是在情感不一致场景下

Method: 使用情感不一致的语音样本数据集(EMIS)，其中语音内容表达一种情感而语音表达方式表达另一种情感，评估四个SLM模型

Result: SLM模型主要依赖文本语义而非语音情感特征来完成任务，表明文本相关表征主导了音频表征

Conclusion: 当前口语语言模型在整合音频和文本模态方面存在不足，文本表征在模型中占据主导地位

Abstract: Advancements in spoken language processing have driven the development of
spoken language models (SLMs), designed to achieve universal audio
understanding by jointly learning text and audio representations for a wide
range of tasks. Although promising results have been achieved, there is growing
discussion regarding these models' generalization capabilities and the extent
to which they truly integrate audio and text modalities in their internal
representations. In this work, we evaluate four SLMs on the task of speech
emotion recognition using a dataset of emotionally incongruent speech samples,
a condition under which the semantic content of the spoken utterance conveys
one emotion while speech expressiveness conveys another. Our results indicate
that SLMs rely predominantly on textual semantics rather than speech emotion to
perform the task, indicating that text-related representations largely dominate
over acoustic representations. We release both the code and the Emotionally
Incongruent Synthetic Speech dataset (EMIS) to the community.

</details>


### [23] [GAPMAP: Mapping Scientific Knowledge Gaps in Biomedical Literature Using Large Language Models](https://arxiv.org/abs/2510.25055)
*Nourah M Salem,Elizabeth White,Michael Bada,Lawrence Hunter*

Main category: cs.CL

TL;DR: 该研究评估大语言模型在生物医学文献中识别显性和隐性知识空白的能力，提出了TABI推理框架，发现LLM能有效识别两种知识空白，为早期研究制定和资助决策提供支持。


<details>
  <summary>Details</summary>
Motivation: 科学进步依赖于明确阐述未知领域，但现有研究主要关注显性知识空白检测，本研究旨在扩展至推断隐性知识空白这一新任务。

Method: 在四个数据集上对近1500份文档进行实验，比较闭源和开源模型在段落级和全文设置下的表现，引入TABI推理框架结构化推理过程。

Result: LLM在识别显性和隐性知识空白方面表现出强大能力，闭源和开源模型均有效，较大模型通常表现更好。

Conclusion: LLM具有系统性识别候选知识空白的强大能力，可支持早期研究制定、政策制定和资助决策，同时指出了领域适应、人机验证等稳健部署方向。

Abstract: Scientific progress is driven by the deliberate articulation of what remains
unknown. This study investigates the ability of large language models (LLMs) to
identify research knowledge gaps in the biomedical literature. We define two
categories of knowledge gaps: explicit gaps, clear declarations of missing
knowledge; and implicit gaps, context-inferred missing knowledge. While prior
work has focused mainly on explicit gap detection, we extend this line of
research by addressing the novel task of inferring implicit gaps. We conducted
two experiments on almost 1500 documents across four datasets, including a
manually annotated corpus of biomedical articles. We benchmarked both
closed-weight models (from OpenAI) and open-weight models (Llama and Gemma 2)
under paragraph-level and full-paper settings. To address the reasoning of
implicit gaps inference, we introduce \textbf{\small TABI}, a Toulmin-Abductive
Bucketed Inference scheme that structures reasoning and buckets inferred
conclusion candidates for validation. Our results highlight the robust
capability of LLMs in identifying both explicit and implicit knowledge gaps.
This is true for both open- and closed-weight models, with larger variants
often performing better. This suggests a strong ability of LLMs for
systematically identifying candidate knowledge gaps, which can support
early-stage research formulation, policymakers, and funding decisions. We also
report observed failure modes and outline directions for robust deployment,
including domain adaptation, human-in-the-loop verification, and benchmarking
across open- and closed-weight models.

</details>


### [24] [Can LLMs Estimate Cognitive Complexity of Reading Comprehension Items?](https://arxiv.org/abs/2510.25064)
*Seonjeong Hwang,Hyounghun Kim,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: 研究探讨了使用大语言模型(LLMs)评估阅读理解题目的认知复杂度，重点关注证据范围和转换水平两个维度，发现LLMs能够近似估计认知复杂度，但在元认知意识方面存在局限。


<details>
  <summary>Details</summary>
Motivation: 传统上阅读理解题目的认知复杂度评估依赖人工标注，特别是涉及答题推理过程的认知特征难以通过现有NLP工具提取，因此研究探索LLMs在此任务上的潜力。

Method: 通过分析证据范围和转换水平两个认知负担维度，使用LLMs来估计阅读理解题目的认知复杂度，并与人类标注进行比较。

Result: 实验结果显示LLMs能够近似估计题目的认知复杂度，表明其作为先验难度分析工具的潜力。但LLMs的推理能力与元认知意识存在差距，即使给出正确答案，有时也无法正确识别其推理过程的基础特征。

Conclusion: LLMs在评估阅读理解题目认知复杂度方面具有潜力，但在元认知意识方面仍需改进，这为未来开发更有效的自动难度评估工具提供了方向。

Abstract: Estimating the cognitive complexity of reading comprehension (RC) items is
crucial for assessing item difficulty before it is administered to learners.
Unlike syntactic and semantic features, such as passage length or semantic
similarity between options, cognitive features that arise during answer
reasoning are not readily extractable using existing NLP tools and have
traditionally relied on human annotation. In this study, we examine whether
large language models (LLMs) can estimate the cognitive complexity of RC items
by focusing on two dimensions-Evidence Scope and Transformation Level-that
indicate the degree of cognitive burden involved in reasoning about the answer.
Our experimental results demonstrate that LLMs can approximate the cognitive
complexity of items, indicating their potential as tools for prior difficulty
analysis. Further analysis reveals a gap between LLMs' reasoning ability and
their metacognitive awareness: even when they produce correct answers, they
sometimes fail to correctly identify the features underlying their own
reasoning process.

</details>


### [25] [TOPol: Capturing and Explaining Multidimensional Semantic Polarity Fields and Vectors](https://arxiv.org/abs/2510.25069)
*Gabin Taibi,Lucia Gomez*

Main category: cs.CL

TL;DR: TOPol是一个半无监督框架，用于在多维叙事极性场中重建和解释语义极性，通过人类参与定义上下文边界，结合transformer模型嵌入和降维技术，量化语义位移。


<details>
  <summary>Details</summary>
Motivation: 传统计算语言学方法将情感视为单维尺度，忽略了语言的多维结构。本文旨在开发一个能够捕捉多维语义极性的框架，适用于情感和非情感语义分析。

Method: 使用transformer大语言模型嵌入文档，应用邻居调优UMAP投影，通过Leiden分区进行主题分割。在定义的上下文边界下计算方向向量，生成量化语义位移的极性场。

Result: 在两个语料库上的评估显示：美联储讲话中捕捉到非情感语义转变，亚马逊评论中情感极性与NRC效价一致。TOPol能稳定捕捉情感和非情感极性转变。

Conclusion: TOPol提供了一个可扩展、可泛化和可解释的框架，用于上下文敏感的多维话语分析，方法稳定性得到验证，主要受人类定义的上下文边界影响。

Abstract: Traditional approaches to semantic polarity in computational linguistics
treat sentiment as a unidimensional scale, overlooking the multidimensional
structure of language. This work introduces TOPol (Topic-Orientation POLarity),
a semi-unsupervised framework for reconstructing and interpreting
multidimensional narrative polarity fields under human-on-the-loop (HoTL)
defined contextual boundaries (CBs). The framework embeds documents using a
transformer-based large language model (tLLM), applies neighbor-tuned UMAP
projection, and segments topics via Leiden partitioning. Given a CB between
discourse regimes A and B, TOPol computes directional vectors between
corresponding topic-boundary centroids, yielding a polarity field that
quantifies fine-grained semantic displacement during regime shifts. This
vectorial representation enables assessing CB quality and detecting polarity
changes, guiding HoTL CB refinement. To interpret identified polarity vectors,
the tLLM compares their extreme points and produces contrastive labels with
estimated coverage. Robustness analyses show that only CB definitions (the main
HoTL-tunable parameter) significantly affect results, confirming methodological
stability. We evaluate TOPol on two corpora: (i) U.S. Central Bank speeches
around a macroeconomic breakpoint, capturing non-affective semantic shifts, and
(ii) Amazon product reviews across rating strata, where affective polarity
aligns with NRC valence. Results demonstrate that TOPol consistently captures
both affective and non-affective polarity transitions, providing a scalable,
generalizable, and interpretable framework for context-sensitive
multidimensional discourse analysis.

</details>


### [26] [BioCoref: Benchmarking Biomedical Coreference Resolution with LLMs](https://arxiv.org/abs/2510.25087)
*Nourah M Salem,Elizabeth White,Michael Bada,Lawrence Hunter*

Main category: cs.CL

TL;DR: 评估生成式大语言模型在生物医学文本中共指消解的性能，使用CRAFT语料库进行基准测试，比较不同提示策略的效果，并与判别式方法SpanBERT进行对比。


<details>
  <summary>Details</summary>
Motivation: 生物医学文本中的共指消解面临领域特定术语复杂、指称形式歧义高、共指表达式间存在长距离依赖等独特挑战，需要专门评估LLM在此领域的表现。

Method: 使用CRAFT语料库作为基准，设计了四种提示实验，分别利用局部上下文、上下文增强、缩写和实体词典等域特定线索，并与SpanBERT判别式方法进行对比。

Result: LLMs展现出强大的表层共指能力，特别是在使用领域基础提示时表现更好，但性能仍对长距离上下文和指称歧义敏感。LLaMA 8B和17B模型在实体增强提示下显示出优越的精确率和F1分数。

Conclusion: 轻量级提示工程有潜力增强LLM在生物医学NLP任务中的实用性，特别是在使用实体增强提示时能显著提升性能。

Abstract: Coreference resolution in biomedical texts presents unique challenges due to
complex domain-specific terminology, high ambiguity in mention forms, and
long-distance dependencies between coreferring expressions. In this work, we
present a comprehensive evaluation of generative large language models (LLMs)
for coreference resolution in the biomedical domain. Using the CRAFT corpus as
our benchmark, we assess the LLMs' performance with four prompting experiments
that vary in their use of local, contextual enrichment, and domain-specific
cues such as abbreviations and entity dictionaries. We benchmark these
approaches against a discriminative span-based encoder, SpanBERT, to compare
the efficacy of generative versus discriminative methods. Our results
demonstrate that while LLMs exhibit strong surface-level coreference
capabilities, especially when supplemented with domain-grounding prompts, their
performance remains sensitive to long-range context and mentions ambiguity.
Notably, the LLaMA 8B and 17B models show superior precision and F1 scores
under entity-augmented prompting, highlighting the potential of lightweight
prompt engineering for enhancing LLM utility in biomedical NLP tasks.

</details>


### [27] [DEBATE: A Large-Scale Benchmark for Role-Playing LLM Agents in Multi-Agent, Long-Form Debates](https://arxiv.org/abs/2510.25110)
*Yun-Shiuan Chuang,Ruixuan Tu,Chengtao Dai,Smit Vasani,Binwei Yao,Michael Henry Tessler,Sijia Yang,Dhavan Shah,Robert Hawkins,Junjie Hu,Timothy T. Rogers*

Main category: cs.CL

TL;DR: 提出了DEBATE基准，用于评估多智能体角色扮演LLM在辩论对话中模拟人类意见动态的真实性，并通过监督微调改善了表面指标但语义对齐仍有局限。


<details>
  <summary>Details</summary>
Motivation: 现有LLM角色扮演设置产生不自然的群体动态（如过早收敛），缺乏衡量真实人类意见轨迹的实证基准。

Method: 构建包含29,417条消息的大规模DEBATE基准，包含2,792名美国参与者关于107个争议话题的多轮辩论对话，同时记录公开表达和私下报告的意见。

Result: 系统评估发现模拟与真实群体动态存在关键差异；通过监督微调在表面指标（如ROUGE-L和消息长度）上取得改进，但在更深层语义对齐（如语义相似性）方面仍有局限。

Conclusion: 角色扮演LLM代理在真实模拟类人社会动态方面既有潜力也存在当前局限性。

Abstract: Accurately modeling opinion change through social interactions is crucial for
addressing issues like misinformation and polarization. While role-playing
large language models (LLMs) offer a promising way to simulate human-like
interactions, existing research shows that single-agent alignment does not
guarantee authentic multi-agent group dynamics. Current LLM role-play setups
often produce unnatural dynamics (e.g., premature convergence), without an
empirical benchmark to measure authentic human opinion trajectories. To bridge
this gap, we introduce DEBATE, the first large-scale empirical benchmark
explicitly designed to evaluate the authenticity of the interaction between
multi-agent role-playing LLMs. DEBATE contains 29,417 messages from multi-round
debate conversations among over 2,792 U.S.-based participants discussing 107
controversial topics, capturing both publicly-expressed messages and
privately-reported opinions. Using DEBATE, we systematically evaluate and
identify critical discrepancies between simulated and authentic group dynamics.
We further demonstrate DEBATE's utility for aligning LLMs with human behavior
through supervised fine-tuning, achieving improvements in surface-level metrics
(e.g., ROUGE-L and message length) while highlighting limitations in deeper
semantic alignment (e.g., semantic similarity). Our findings highlight both the
potential and current limitations of role-playing LLM agents for realistically
simulating human-like social dynamics.

</details>


### [28] [Pretraining Strategies using Monolingual and Parallel Data for Low-Resource Machine Translation](https://arxiv.org/abs/2510.25116)
*Idriss Nguepi Nguefack,Mara Finkelstein,Toadoum Sari Sakayo*

Main category: cs.CL

TL;DR: 本研究探讨了针对低资源语言的机器翻译模型预训练策略的有效性，特别关注林加拉语，通过多语言预训练和结合单语/平行数据来提升翻译质量。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言机器翻译性能不足的问题，缩小高资源与低资源语言之间的性能差距，为边缘化社区开发更包容和准确的NLP模型。

Method: 基于Reid和Artetxe (2021)的预训练方法，探索多种预训练策略，包括多语言预训练、结合单语和平行数据进行预训练，并在包括阿非利卡语、斯瓦希里语和祖鲁语在内的多个低资源语言上进行实验。

Result: 多语言预训练和结合单语与平行数据的预训练策略显著提高了翻译质量，为低资源机器翻译提供了有效的预训练方法。

Conclusion: 本研究为低资源机器翻译提供了有效的预训练策略，有助于开发更包容的NLP模型，代码和数据集公开可用以促进进一步研究。

Abstract: This research article examines the effectiveness of various pretraining
strategies for developing machine translation models tailored to low-resource
languages. Although this work considers several low-resource languages,
including Afrikaans, Swahili, and Zulu, the translation model is specifically
developed for Lingala, an under-resourced African language, building upon the
pretraining approach introduced by Reid and Artetxe (2021), originally designed
for high-resource languages. Through a series of comprehensive experiments, we
explore different pretraining methodologies, including the integration of
multiple languages and the use of both monolingual and parallel data during the
pretraining phase. Our findings indicate that pretraining on multiple languages
and leveraging both monolingual and parallel data significantly enhance
translation quality. This study offers valuable insights into effective
pretraining strategies for low-resource machine translation, helping to bridge
the performance gap between high-resource and low-resource languages. The
results contribute to the broader goal of developing more inclusive and
accurate NLP models for marginalized communities and underrepresented
populations. The code and datasets used in this study are publicly available to
facilitate further research and ensure reproducibility, with the exception of
certain data that may no longer be accessible due to changes in public
availability.

</details>


### [29] [A Survey on Unlearning in Large Language Models](https://arxiv.org/abs/2510.25117)
*Ruichen Qiu,Jiajun Tan,Jiayue Pu,Honglin Wang,Xiao-Shan Gao,Fei Sun*

Main category: cs.CL

TL;DR: 这篇论文系统综述了180多篇关于大语言模型遗忘的论文，提出了新的遗忘方法和评估分类法，分析了现有数据集和指标的优缺点，并讨论了关键挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在训练过程中会记忆敏感个人信息、版权材料和可能促进恶意活动的知识，这带来了重大风险。为了符合法律和伦理标准（如"被遗忘权"），机器遗忘技术成为从LLMs中选择性擦除特定知识而不影响整体性能的关键技术。

Method: 引入了新颖的遗忘方法和评估分类法。将遗忘方法清晰地分为训练时、训练后和推理时三类，基于遗忘应用的训练阶段进行分类。对于评估，不仅系统性地汇编了现有数据集和指标，还批判性地分析了它们的优缺点和适用性。

Result: 提供了对180多篇LLM遗忘论文的系统性综述，建立了新的分类框架，为研究社区提供了实用的指导，并识别了现有评估方法的局限性。

Conclusion: 这项全面的综述旨在为安全和可靠的大语言模型的持续发展提供信息和指导，强调了机器遗忘在解决LLMs隐私和安全风险方面的重要性。

Abstract: The advancement of Large Language Models (LLMs) has revolutionized natural
language processing, yet their training on massive corpora poses significant
risks, including the memorization of sensitive personal data, copyrighted
material, and knowledge that could facilitate malicious activities. To mitigate
these issues and align with legal and ethical standards such as the "right to
be forgotten", machine unlearning has emerged as a critical technique to
selectively erase specific knowledge from LLMs without compromising their
overall performance. This survey provides a systematic review of over 180
papers on LLM unlearning published since 2021, focusing exclusively on
large-scale generative models. Distinct from prior surveys, we introduce novel
taxonomies for both unlearning methods and evaluations. We clearly categorize
methods into training-time, post-training, and inference-time based on the
training stage at which unlearning is applied. For evaluations, we not only
systematically compile existing datasets and metrics but also critically
analyze their advantages, disadvantages, and applicability, providing practical
guidance to the research community. In addition, we discuss key challenges and
promising future research directions. Our comprehensive overview aims to inform
and guide the ongoing development of secure and reliable LLMs.

</details>


### [30] [Explainable Disentanglement on Discrete Speech Representations for Noise-Robust ASR](https://arxiv.org/abs/2510.25150)
*Shreyas Gopal,Ashutosh Anshul,Haoyang Li,Yue Heng Yeo,Hexin Liu,Eng Siong Chng*

Main category: cs.CL

TL;DR: 提出了一种在潜在空间中分离语义语音内容和背景噪声的方法，通过端到端模型生成干净的语音码本标记，同时提取可解释的噪声向量作为量化残差，显著提升了噪声环境下的语音识别性能。


<details>
  <summary>Details</summary>
Motivation: 离散音频表示在语音建模中越来越受欢迎，但在嘈杂或真实环境中的表现不佳。现有方法虽然量化Whisper嵌入用于语音到单元建模，但未能有效分离语义内容和背景噪声。

Method: 在Whisper嵌入量化的基础上，提出在潜在空间中分离语义语音内容和背景噪声。端到端模型生成干净的语音码本标记，同时通过轻量级分类器监督提取可解释的噪声向量作为量化残差。

Result: 在VBDemand测试集上，相比Whisper实现了82%的错误率降低，相比基线方法提升了35%的性能。学习的标记空间在已见和未见声学条件下都表现出良好的泛化能力。

Conclusion: 该方法有效提升了噪声环境下的语音识别性能，生成的语音标记具有高度的噪声不变性，改善了干净/嘈杂语音与文本之间的对齐。

Abstract: Discrete audio representations are gaining traction in speech modeling due to
their interpretability and compatibility with large language models, but are
not always optimized for noisy or real-world environments. Building on existing
works that quantize Whisper embeddings for speech-to-unit modeling, we propose
disentangling semantic speech content from background noise in the latent
space. Our end-to-end model separates clean speech in the form of codebook
tokens, while extracting interpretable noise vectors as quantization residue
which are supervised via a lightweight classifier. We show that our approach
improves alignment between clean/noisy speech and text, producing speech tokens
that display a high degree of noiseinvariance, and improves ASR performance.
Keeping Whisper frozen, we show an 82% reduction in error rate compared to
Whisper, and 35% improvement over baseline methods on the VBDemand test set.
Further analyses show that the learned token space generalizes well to both
seen and unseen acoustic conditions.

</details>


### [31] [Model-Document Protocol for AI Search](https://arxiv.org/abs/2510.25160)
*Hongjin Qian,Zheng Liu*

Main category: cs.CL

TL;DR: 提出了Model-Document Protocol (MDP)框架，将原始文档转换为LLM可消费的知识表示，包括智能推理、记忆基础和结构化利用三种路径，并通过MDP-Agent实现该协议，在信息检索基准测试中表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有检索方法将文档视为原始文本返回，将片段组装和上下文推理的负担留给LLM，需要新的检索范式来重新定义模型与文档的交互方式。

Method: MDP框架通过三种路径转换非结构化文档：智能推理将原始证据整理为连贯上下文；记忆基础积累可重用笔记以丰富推理；结构化利用将文档编码为图形或键值缓存等正式表示。MDP-Agent通过构建文档级要点记忆、基于扩散的探索和map-reduce风格合成来实现该协议。

Result: 在信息检索基准测试中，MDP-Agent的表现优于基线方法，验证了MDP框架的合理性和其智能实例化的有效性。

Conclusion: MDP框架为AI搜索提供了新的检索范式，能够将原始文档转换为紧凑、结构化的知识，直接供LLM进行推理使用，解决了传统检索方法的局限性。

Abstract: AI search depends on linking large language models (LLMs) with vast external
knowledge sources. Yet web pages, PDF files, and other raw documents are not
inherently LLM-ready: they are long, noisy, and unstructured. Conventional
retrieval methods treat these documents as verbatim text and return raw
passages, leaving the burden of fragment assembly and contextual reasoning to
the LLM. This gap underscores the need for a new retrieval paradigm that
redefines how models interact with documents.
  We introduce the Model-Document Protocol (MDP), a general framework that
formalizes how raw text is bridged to LLMs through consumable knowledge
representations. Rather than treating retrieval as passage fetching, MDP
defines multiple pathways that transform unstructured documents into
task-specific, LLM-ready inputs. These include agentic reasoning, which curates
raw evidence into coherent context; memory grounding, which accumulates
reusable notes to enrich reasoning; and structured leveraging, which encodes
documents into formal representations such as graphs or key-value caches. All
three pathways share the same goal: ensuring that what reaches the LLM is not
raw fragments but compact, structured knowledge directly consumable for
reasoning.
  As an instantiation, we present MDP-Agent, which realizes the protocol
through an agentic process: constructing document-level gist memories for
global coverage, performing diffusion-based exploration with vertical
exploitation to uncover layered dependencies, and applying map-reduce style
synthesis to integrate large-scale evidence into compact yet sufficient
context. Experiments on information-seeking benchmarks demonstrate that
MDP-Agent outperforms baselines, validating both the soundness of the MDP
framework and the effectiveness of its agentic instantiation.

</details>


### [32] [Testing Cross-Lingual Text Comprehension In LLMs Using Next Sentence Prediction](https://arxiv.org/abs/2510.25187)
*Ritesh Sunil Chavan,Jack Mostow*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型在低资源语言上的表现，发现模型性能随语言资源丰富度下降而显著降低，且Chain-of-Thought提示在不同模型上效果各异。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型的优异表现是源于真实能力还是英语数据优势，通过在低资源语言环境中测试来验证。

Method: 基于Next Sentence Prediction任务，构建包含英语、斯瓦希里语和豪萨语的大规模基准测试，测试GPT-4 Turbo、Gemini 1.5 Flash和LLaMA 3 70B等模型，并引入Chain-of-Thought提示技术。

Result: 所有模型在英语上表现优异，但在斯瓦希里语上准确率下降，在豪萨语上大幅下降，LLaMA 3表现最差。Chain-of-Thought对LLaMA 3有显著帮助，但对GPT-4和Gemini反而产生负面影响。

Conclusion: Chain-of-Thought不是通用解决方案，其效果取决于模型基础能力和任务具体情境，研究框架能识别LLM弱点并揭示影响其决策的因素。

Abstract: While large language models are trained on massive datasets, this data is
heavily skewed towards English. Does their impressive performance reflect
genuine ability or just this data advantage? To find out, we tested them in a
setting where they could not rely on data abundance: low-resource languages.
Building on prior work Agarwal et al. (2025) that used Next Sentence Prediction
(NSP) as a test, we created a large-scale benchmark with 10,000 questions each
for English (a high-resource language), Swahili (medium-resource), and Hausa
(low-resource). We then tested several top models, including GPT-4 Turbo,
Gemini 1.5 Flash, and LLaMA 3 70B, to see how their performance holds up. The
results painted a clear picture of how levels of language resources impact
outcomes. While all models excelled in English, their accuracy dropped in
Swahili and fell sharply in Hausa, with LLaMA 3 struggling the most. The story
became even more interesting when we introduced Chain-of-Thought (CoT)
prompting. For the struggling LLaMA 3, CoT acted as a helpful guide,
significantly boosting its accuracy. However, for the more capable GPT-4 and
Gemini, the same technique often backfired, leading to a kind of "overthinking"
that hurt their results in the cross-lingual context. This reveals that
Chain-of-Thought is not a universal solution; its effectiveness depends heavily
on the model's baseline capability and the specific context of the task. Our
framework pinpoints LLM weaknesses, highlights when CoT helps or hinders
cross-lingual NSP performance, and factors influencing their decisions.

</details>


### [33] [ProMediate: A Socio-cognitive framework for evaluating proactive agents in multi-party negotiation](https://arxiv.org/abs/2510.25224)
*Ziyi Liu,Bahar Sarrafzadeh,Pei Zhou,Longqi Yang,Jieyu Zhao,Ashish Sharma*

Main category: cs.CL

TL;DR: ProMediate是首个评估复杂多议题多方谈判中主动AI调解代理的框架，包含基于真实谈判案例的模拟测试平台和社会认知评估指标，能系统评估AI代理的社会认知智能。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏评估主动AI代理在复杂多方协作中表现的系统方法，限制了开发能有效支持多人协作的AI的进展。谈判作为需要社会认知智能的挑战性测试平台，适合评估这类能力。

Method: ProMediate框架包含两个核心组件：(1)基于真实谈判案例和理论驱动难度级别的模拟测试平台，配备基于社会认知调解理论的即插即用主动AI调解代理；(2)包含共识变化、干预延迟、调解效果和智能等新指标的社会认知评估框架。

Result: 在社会智能调解代理在ProMediate-Hard设置中，相比通用基线将共识变化提高了3.6个百分点（10.65% vs 7.01%），同时响应速度快77%（15.98s vs 3.71s）。

Conclusion: ProMediate提供了一个严谨、理论基础的测试平台，可推动主动、社会智能代理的发展。

Abstract: While Large Language Models (LLMs) are increasingly used in agentic
frameworks to assist individual users, there is a growing need for agents that
can proactively manage complex, multi-party collaboration. Systematic
evaluation methods for such proactive agents remain scarce, limiting progress
in developing AI that can effectively support multiple people together.
Negotiation offers a demanding testbed for this challenge, requiring
socio-cognitive intelligence to navigate conflicting interests between multiple
participants and multiple topics and build consensus. Here, we present
ProMediate, the first framework for evaluating proactive AI mediator agents in
complex, multi-topic, multi-party negotiations. ProMediate consists of two core
components: (i) a simulation testbed based on realistic negotiation cases and
theory-driven difficulty levels (ProMediate-Easy, ProMediate-Medium, and
ProMediate-Hard), with a plug-and-play proactive AI mediator grounded in
socio-cognitive mediation theories, capable of flexibly deciding when and how
to intervene; and (ii) a socio-cognitive evaluation framework with a new suite
of metrics to measure consensus changes, intervention latency, mediator
effectiveness, and intelligence. Together, these components establish a
systematic framework for assessing the socio-cognitive intelligence of
proactive AI agents in multi-party settings. Our results show that a socially
intelligent mediator agent outperforms a generic baseline, via faster,
better-targeted interventions. In the ProMediate-Hard setting, our social
mediator increases consensus change by 3.6 percentage points compared to the
generic baseline (10.65\% vs 7.01\%) while being 77\% faster in response
(15.98s vs. 3.71s). In conclusion, ProMediate provides a rigorous,
theory-grounded testbed to advance the development of proactive, socially
intelligent agents.

</details>


### [34] [Adapting Small Language Models to Low-Resource Domains: A Case Study in Hindi Tourism QA](https://arxiv.org/abs/2510.25273)
*Sandipan Majhi,Paheli Bhattacharya*

Main category: cs.CL

TL;DR: 提出一种多阶段微调策略，使用原始和合成数据来适应低资源印地语旅游领域的问答任务，通过大模型生成合成数据，小模型有效适应，为低资源领域特定QA提供可扩展路径。


<details>
  <summary>Details</summary>
Motivation: 低资源语言的领域特定问答面临两个关键挑战：标注数据集稀缺和通用语言模型领域知识有限。

Method: 采用多阶段微调策略，利用大语言模型(LLaMA-70B, Phi-14B)生成合成问答对来增强有限的原始数据集，探索多种训练方法并分析其对领域泛化的影响。

Result: 大模型能高效生成合成数据，小模型能有效适应这些数据，为低资源领域特定QA提供了可扩展的解决方案。

Conclusion: 该方法展示了利用大模型生成合成数据、小模型进行适应的可行路径，为低资源语言的领域特定问答任务提供了有效的解决方案。

Abstract: Domain-specific question answering in low-resource languages faces two key
challenges: scarcity of annotated datasets and limited domain knowledge in
general-purpose language models. In this work, we present a multi-stage
finetuning strategy to adapt lightweight language models to the Hindi tourism
domain by leveraging both original and synthetic training data. Synthetic
question-answer pairs are generated using large LLMs (LLaMA-70B, Phi-14B) and
used to augment the limited original dataset. We explore several training
methodologies and analyse their impact on domain generalisation. Our results
demonstrate that large models can efficiently generate synthetic data, while
small models can effectively adapt to it, offering a scalable pathway for
low-resource, domain-specific QA.

</details>


### [35] [Teaching Sarcasm: Few-Shot Multimodal Sarcasm Detection via Distillation to a Parameter-Efficient Student](https://arxiv.org/abs/2510.25303)
*Soumyadeep Jana,Sanasam Ranbir Singh*

Main category: cs.CL

TL;DR: PEKD是一个统一的参数高效微调框架，通过从在大规模讽刺数据上训练的专家模型进行蒸馏，增强PEFT方法在少样本多模态讽刺检测中的性能。


<details>
  <summary>Details</summary>
Motivation: 多模态讽刺检测在低资源环境下具有挑战性，因为稀缺的标注数据使得模型难以学习图像-文本之间的微妙矛盾。现有的PEFT方法虽然减少了过拟合，但由于少样本数据的有限监督，难以达到最优性能。

Method: 提出PEKD框架，利用在大规模讽刺数据上训练的专家模型作为教师模型进行知识蒸馏。引入熵感知门控机制，根据教师模型的置信度动态调整蒸馏强度，以缓解不可靠信号的影响。

Result: 在两个公共数据集上的实验表明，PEKD框架使PEFT方法在少样本场景下优于先前的参数高效方法和大型多模态模型，取得了强劲的结果。

Conclusion: PEKD是一个模块化且适应性强的框架，可广泛应用于各种多模态模型和任务，有效提升了少样本多模态讽刺检测的性能。

Abstract: Multimodal sarcasm detection is challenging, especially in low-resource
settings where subtle image-text contradictions are hard to learn due to scarce
annotated data, which hinders the model's performance. Parameter-efficient
fine-tuning (PEFT) methods like adapters, LoRA, and prompt tuning reduce
overfitting but struggle to reach optimal performance due to limited
supervision from few-shot data. We propose PEKD, a unified framework that
enhances PEFT methods via distillation from an expert model trained on
large-scale sarcasm data, which acts as the teacher. To mitigate unreliable
signals from the teacher, we introduce an entropy-aware gating mechanism that
dynamically adjusts the distillation strength based on teacher confidence.
Experiments on two public datasets demonstrate that our PEKD framework enables
PEFT methods to outperform both prior parameter-efficient approaches and large
multimodal models, achieving strong results in the few-shot scenario. The
framework is modular and adaptable to a wide range of multimodal models and
tasks.

</details>


### [36] [Parrot: A Training Pipeline Enhances Both Program CoT and Natural Language CoT for Reasoning](https://arxiv.org/abs/2510.25310)
*Senjie Jin,Lu Chen,Zhiheng Xi,Yuhui Wang,Sirui Song,Yuhao Zhou,Xinbo Zhang,Peng Sun,Hong Lu,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.CL

TL;DR: 提出Parrot训练框架，通过整合自然语言思维链(N-CoT)和程序思维链(P-CoT)两种范式，实现相互增强，显著提升数学推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常单向增强N-CoT或P-CoT，本文旨在充分释放两种范式的优势进行相互增强，实现同步改进。

Method: 1) 设计三个目标子任务整合顺序P-CoT和N-CoT生成；2) 采用子任务混合训练策略促进自然语言语义可迁移性；3) 设计转换的N-CoT辅助奖励缓解P-CoT优化中的稀疏奖励问题。

Result: 实验表明Parrot显著提升了N-CoT和P-CoT的性能，特别是在N-CoT上。使用Parrot SFT，LLaMA2和CodeLLaMA在MathQA上的N-CoT性能分别比资源密集的RL基线提高了+21.87和+21.48。

Conclusion: Parrot框架成功实现了N-CoT和P-CoT的相互增强，为数学推理任务提供了有效的训练方法。

Abstract: Natural language chain-of-thought (N-CoT) and Program chain-of-thought
(P-CoT) have emerged as two primary paradigms for large language models (LLMs)
to solve mathematical reasoning problems. Current research typically endeavors
to achieve unidirectional enhancement: P-CoT enhanced N-CoT or N-CoT enhanced
P-CoT. In this paper, we seek to fully unleash the two paradigms' strengths for
mutual enhancement and ultimately achieve simultaneous improvements. We conduct
a detailed analysis of the error types across two paradigms, based on which we
propose Parrot, a novel training pipeline for mathematical problems: 1) Three
target-designed subtasks integrate sequential P-CoT and N-CoT generation. 2) A
subtask hybrid training strategy to facilitate natural language semantic
transferability. 3) The converted N-CoT auxiliary reward is designed to
alleviate the sparse rewards in P-CoT optimization. Extensive experiments
demonstrate that Parrot significantly enhances both the performance of N-CoT
and P-CoT, especially on N-CoT. Using Parrot SFT, the N-CoT performance of
LLaMA2 and CodeLLaMA achieve gains of +21.87 and +21.48 on MathQA over the RL
baseline, which is resource-intensive.

</details>


### [37] [CRMWeaver: Building Powerful Business Agent via Agentic RL and Shared Memories](https://arxiv.org/abs/2510.25333)
*Yilong Lai,Yipin Yang,Jialong Wu,Fengran Mo,Zhenglin Wang,Ting Liang,Jianguo Lin,Keping Yang*

Main category: cs.CL

TL;DR: CRMWeaver：一种通过合成数据生成和基于强化学习的训练范式来增强商业代理处理复杂业务场景的方法，引入共享记忆机制提升模型在未知场景中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 商业代理在处理复杂现实世界问题时面临数据关系复杂和任务类型多样的挑战，需要提升在复杂业务环境中的处理能力。

Method: 采用合成数据生成和基于强化学习的训练范式，在推理阶段引入共享记忆机制，让代理从类似问题的任务指南中学习。

Result: 在CRMArena-Pro数据集上验证，轻量级模型在B2B和B2C业务场景中均取得有竞争力的结果。

Conclusion: CRMWeaver方法有效提升了商业代理在复杂业务环境中的表现，具有实际应用价值。

Abstract: Recent years have witnessed the rapid development of LLM-based agents, which
shed light on using language agents to solve complex real-world problems. A
prominent application lies in business agents, which interact with databases
and internal knowledge bases via tool calls to fulfill diverse user
requirements. However, this domain is characterized by intricate data
relationships and a wide range of heterogeneous tasks, from statistical data
queries to knowledge-based question-answering. To address these challenges, we
propose CRMWeaver, a novel approach that enhances business agents in such
complex settings. To acclimate the agentic model to intricate business
environments, we employ a synthesis data generation and RL-based paradigm
during training, which significantly improves the model's ability to handle
complex data and varied tasks. During inference, a shared memories mechanism is
introduced, prompting the agent to learn from task guidelines in similar
problems, thereby further boosting its effectiveness and generalization,
especially in unseen scenarios. We validate the efficacy of our approach on the
CRMArena-Pro dataset, where our lightweight model achieves competitive results
in both B2B and B2C business scenarios, underscoring its practical value for
real-world applications.

</details>


### [38] [Not ready for the bench: LLM legal interpretation is unstable and out of step with human judgments](https://arxiv.org/abs/2510.25356)
*Abhishek Purushothama,Junghyun Min,Brandon Waldon,Nathan Schneider*

Main category: cs.CL

TL;DR: 本文通过实证研究反对在法律解释中使用大型语言模型，发现模型无法提供稳定的解释判断，且与人类判断相关性较弱。


<details>
  <summary>Details</summary>
Motivation: 针对法律学者和联邦法官最近实践中的LLM法律解释方法，提供实证论据来反对这种实践。

Method: 在英语环境中进行调查，通过改变问题格式来测试模型的稳定性，并比较模型判断与人类判断的相关性。

Result: 模型无法提供稳定的解释判断，问题格式的变化会导致截然不同的结论；模型与人类判断只有弱到中等程度的相关性，且不同模型和问题变体间差异很大。

Conclusion: 对生成式AI产生的结论给予过多信任是危险的，不应将LLM作为法律解释工具。

Abstract: Legal interpretation frequently involves assessing how a legal text, as
understood by an 'ordinary' speaker of the language, applies to the set of
facts characterizing a legal dispute in the U.S. judicial system. Recent
scholarship has proposed that legal practitioners add large language models
(LLMs) to their interpretive toolkit. This work offers an empirical argument
against LLM interpretation as recently practiced by legal scholars and federal
judges. Our investigation in English shows that models do not provide stable
interpretive judgments: varying the question format can lead the model to
wildly different conclusions. Moreover, the models show weak to moderate
correlation with human judgment, with large variance across model and question
variant, suggesting that it is dangerous to give much credence to the
conclusions produced by generative AI.

</details>


### [39] [CLASS-IT: Conversational and Lecture-Aligned Small-Scale Instruction Tuning for BabyLMs](https://arxiv.org/abs/2510.25364)
*Luca Capone,Alessandro Bondielli,Alessandro Lenci*

Main category: cs.CL

TL;DR: 该研究探讨小型语言模型是否能从指令调优中受益，比较了对话和问答指令调优数据集在合并或顺序课程中的应用，发现指令调优在微调场景中带来小幅但一致的改进，但在零样本任务中改进不持续。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索小型语言模型（100M和140M参数）是否能够通过指令调优获得性能提升，特别是在资源受限的情况下如何有效应用人类启发式学习策略。

Method: 使用解码器模型（100M和140M参数），比较对话和问答指令调优数据集，采用合并或顺序课程方法，在微调（SuperGLUE）和零样本（BLiMP、EWoK、WUGs等）设置下进行评估。

Result: 结果显示指令调优在微调场景中带来小幅但一致的性能提升，顺序课程优于合并数据；但在零样本任务中改进不持续，表明存在交互适应与广泛语言泛化之间的权衡。

Conclusion: 研究强调了在低资源语言模型中应用人类启发式学习策略的潜力和限制，并指出混合、基于课程的方法是在生态训练限制下增强泛化能力的可行方向。

Abstract: This work investigates whether small-scale LMs can benefit from instruction
tuning. We compare conversational and question-answering instruction tuning
datasets, applied either in a merged or sequential curriculum, using
decoder-only models with 100M and 140M parameters. Evaluation spans both
fine-tuning (SuperGLUE) and zero-shot (BLiMP, EWoK, WUGs, entity tracking, and
psycholinguistic correlation) settings. Results show that instruction tuning
yields small but consistent gains in fine-tuning scenarios, with sequential
curricula outperforming merged data; however, improvements do not consistently
transfer to zero-shot tasks, suggesting a trade-off between interaction-focused
adaptation and broad linguistic generalization. These results highlight both
the potential and the constraints of adapting human-inspired learning
strategies to low-resource LMs, and point toward hybrid, curriculum-based
approaches for enhancing generalization under ecological training limits.

</details>


### [40] [Monitoring Transformative Technological Convergence Through LLM-Extracted Semantic Entity Triple Graphs](https://arxiv.org/abs/2510.25370)
*Alexander Sternfeld,Andrei Kucharavy,Dimitri Percia David,Alain Mermoud,Julian Jang-Jaccard,Nathan Monnet*

Main category: cs.CL

TL;DR: 提出基于大语言模型的数据驱动管道，通过技术融合模式识别来监测变革性技术的出现，使用arXiv预印本和USPTO专利数据进行验证。


<details>
  <summary>Details</summary>
Motivation: 传统专家方法难以跟上快速演进的ICT领域创新周期，需要一种能够处理早期模糊术语的数据驱动方法来预测变革性技术。

Method: 利用LLM从非结构化文本中提取语义三元组，构建技术实体关系图，开发名词钉合方法分组相似技术术语，使用基于图的指标检测融合信号，包括多阶段过滤、领域特定关键词聚类和主题共现时间趋势分析。

Result: 在278,625篇arXiv预印本和9,793项USPTO专利申请上验证，能够识别已建立和新兴的融合模式。

Conclusion: 该管道提供了一个可扩展且通用的技术预测框架，基于全文分析，能够有效监测技术融合和变革性技术的出现。

Abstract: Forecasting transformative technologies remains a critical but challenging
task, particularly in fast-evolving domains such as Information and
Communication Technologies (ICTs). Traditional expert-based methods struggle to
keep pace with short innovation cycles and ambiguous early-stage terminology.
In this work, we propose a novel, data-driven pipeline to monitor the emergence
of transformative technologies by identifying patterns of technological
convergence.
  Our approach leverages advances in Large Language Models (LLMs) to extract
semantic triples from unstructured text and construct a large-scale graph of
technology-related entities and relations. We introduce a new method for
grouping semantically similar technology terms (noun stapling) and develop
graph-based metrics to detect convergence signals. The pipeline includes
multi-stage filtering, domain-specific keyword clustering, and a temporal trend
analysis of topic co-occurence.
  We validate our methodology on two complementary datasets: 278,625 arXiv
preprints (2017--2024) to capture early scientific signals, and 9,793 USPTO
patent applications (2018-2024) to track downstream commercial developments.
Our results demonstrate that the proposed pipeline can identify both
established and emerging convergence patterns, offering a scalable and
generalizable framework for technology forecasting grounded in full-text
analysis.

</details>


### [41] [Hallucinations in Bibliographic Recommendation: Citation Frequency as a Proxy for Training Data Redundancy](https://arxiv.org/abs/2510.25378)
*Junichiro Niimi*

Main category: cs.CL

TL;DR: 研究发现LLMs在生成参考文献时存在幻觉问题，论文被引次数与幻觉率呈强负相关，超过约1000次引用的论文几乎被逐字记忆。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在文献推荐中产生不存在论文的幻觉问题，探究引用次数如何影响LLM输出中的幻觉参考文献。

Method: 使用GPT-4生成100个计算机科学领域的文献记录，通过余弦相似度手动验证生成与真实元数据的事实一致性。

Result: 幻觉率因研究领域而异；引用次数与事实准确性强相关；超过约1000次引用的文献信息几乎被逐字记忆。

Conclusion: 高被引论文在模型中几乎被逐字保留，表明存在从泛化转向记忆的阈值。

Abstract: Large language models (LLMs) have been increasingly applied to a wide range
of tasks, from natural language understanding to code generation. While they
have also been used to assist in bibliographic recommendation, the
hallucination of non-existent papers remains a major issue. Building on prior
studies, this study hypothesizes that an LLM's ability to correctly produce
bibliographic information depends on whether the underlying knowledge is
generated or memorized, with highly cited papers (i.e., more frequently appear
in the training corpus) showing lower hallucination rates. We therefore assume
citation count as a proxy for training data redundancy (i.e., the frequency
with which a given bibliographic record is repeatedly represented in the
pretraining corpus) and investigate how citation frequency affects hallucinated
references in LLM outputs. Using GPT-4.1, we generated and manually verified
100 bibliographic records across twenty computer-science domains, and measured
factual consistency via cosine similarity between generated and authentic
metadata. The results revealed that (i) hallucination rates vary across
research domains, (ii) citation count is strongly correlated with factual
accuracy, and (iii) bibliographic information becomes almost verbatimly
memorized beyond approximately 1,000 citations. These findings suggest that
highly cited papers are nearly verbatimly retained in the model, indicating a
threshold where generalization shifts into memorization.

</details>


### [42] [Roleplaying with Structure: Synthetic Therapist-Client Conversation Generation from Questionnaires](https://arxiv.org/abs/2510.25384)
*Doan Nam Long Vu,Rui Tan,Lena Moench,Svenja Jule Francke,Daniel Woiwod,Florian Thomas-Odenthal,Sanna Stroth,Tilo Kircher,Christiane Hermann,Udo Dannlowski,Hamidreza Jamalabadi,Shaoxiong Ji*

Main category: cs.CL

TL;DR: 提出SQPsych框架，使用LLM生成基于结构化心理问卷的心理治疗对话，解决心理健康AI发展中真实对话数据缺乏的问题。


<details>
  <summary>Details</summary>
Motivation: 心理健康AI发展受限于真实治疗对话数据的缺乏，主要由于隐私法规限制和历史记录稀少。

Method: 基于认知行为疗法原则，通过LLM驱动的管道将结构化客户档案和心理问卷转换为自然语言治疗对话。

Result: SQPsychLLM模型在心理咨询基准测试中表现优异，超越基线模型的关键治疗技能。

Conclusion: 合成数据有潜力实现可扩展、数据安全且临床知情的心理健康AI支持。

Abstract: The development of AI for mental health is hindered by a lack of authentic
therapy dialogues, due to strict privacy regulations and the fact that clinical
sessions were historically rarely recorded. We present an LLM-driven pipeline
that generates synthetic counseling dialogues based on structured client
profiles and psychological questionnaires. Grounded on the principles of
Cognitive Behavioral Therapy (CBT), our method creates synthetic therapeutic
conversations for clinical disorders such as anxiety and depression. Our
framework, SQPsych (Structured Questionnaire-based Psychotherapy), converts
structured psychological input into natural language dialogues through
therapist-client simulations. Due to data governance policies and privacy
restrictions prohibiting the transmission of clinical questionnaire data to
third-party services, previous methodologies relying on proprietary models are
infeasible in our setting. We address this limitation by generating a
high-quality corpus using open-weight LLMs, validated through human expert
evaluation and LLM-based assessments. Our SQPsychLLM models fine-tuned on
SQPsychConv achieve strong performance on counseling benchmarks, surpassing
baselines in key therapeutic skills. Our findings highlight the potential of
synthetic data to enable scalable, data-secure, and clinically informed AI for
mental health support. We will release our code, models, and corpus at
https://ai-mh.github.io/SQPsych

</details>


### [43] [BhashaBench V1: A Comprehensive Benchmark for the Quadrant of Indic Domains](https://arxiv.org/abs/2510.25409)
*Vijay Devane,Mohd Nauman,Bhargav Patel,Aniket Mahendra Wakchoure,Yogeshkumar Sant,Shyam Pawar,Viraj Thakur,Ananya Godse,Sunil Patra,Neha Maurya,Suraj Racha,Nitish Kamal Singh,Ajay Nagpal,Piyush Sawarkar,Kundeshwar Vijayrao Pundalik,Rohit Saluja,Ganesh Ramakrishnan*

Main category: cs.CL

TL;DR: BhashaBench V1是首个针对印度知识系统的领域特定、多任务、双语基准测试，包含74,166个精心策划的问答对，涵盖农业、法律、金融和阿育吠陀四大领域，用于评估LLMs在印度语境下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要是以英语为中心且领域无关的，限制了其在印度语境下的适用性，需要针对印度特定领域和文化的评估工具。

Method: 从真实的政府和领域特定考试中收集数据，创建包含52,494个英文和21,672个印地语问答对的双语基准测试，涵盖4个主要领域、90+个子领域和500+个主题。

Result: 对29+个LLMs的评估显示显著的领域和语言特定性能差距，GPT-4o在法律领域达到76.49%准确率，但在阿育吠陀领域仅为59.74%。所有模型在英文内容上的表现均优于印地语。

Conclusion: BhashaBench V1为评估LLMs在印度多样化知识领域的能力提供了全面数据集，支持模型整合领域特定知识与双语理解能力的评估。

Abstract: The rapid advancement of large language models(LLMs) has intensified the need
for domain and culture specific evaluation. Existing benchmarks are largely
Anglocentric and domain-agnostic, limiting their applicability to India-centric
contexts. To address this gap, we introduce BhashaBench V1, the first
domain-specific, multi-task, bilingual benchmark focusing on critical Indic
knowledge systems. BhashaBench V1 contains 74,166 meticulously curated
question-answer pairs, with 52,494 in English and 21,672 in Hindi, sourced from
authentic government and domain-specific exams. It spans four major domains:
Agriculture, Legal, Finance, and Ayurveda, comprising 90+ subdomains and
covering 500+ topics, enabling fine-grained evaluation. Evaluation of 29+ LLMs
reveals significant domain and language specific performance gaps, with
especially large disparities in low-resource domains. For instance, GPT-4o
achieves 76.49% overall accuracy in Legal but only 59.74% in Ayurveda. Models
consistently perform better on English content compared to Hindi across all
domains. Subdomain-level analysis shows that areas such as Cyber Law,
International Finance perform relatively well, while Panchakarma, Seed Science,
and Human Rights remain notably weak. BhashaBench V1 provides a comprehensive
dataset for evaluating large language models across India's diverse knowledge
domains. It enables assessment of models' ability to integrate domain-specific
knowledge with bilingual understanding. All code, benchmarks, and resources are
publicly available to support open research.

</details>


### [44] [Serve Programs, Not Prompts](https://arxiv.org/abs/2510.25412)
*In Gim,Lin Zhong*

Main category: cs.CL

TL;DR: 提出了一种新的LLM服务系统架构Symphony，通过服务程序而非提示词来解决现有系统效率低、适应性差的问题，使用LLM推理程序(LIPs)实现运行时token预测和KV缓存管理的自定义。


<details>
  <summary>Details</summary>
Motivation: 当前LLM服务系统主要为文本补全设计，无法高效适应日益复杂的LLM应用，缺乏灵活性。

Method: 设计Symphony系统作为LIPs的操作系统，通过系统调用暴露LLM模型计算，用专用文件系统虚拟化KV缓存，采用两级进程调度确保GPU效率。

Result: Symphony能够实现更高效和可扩展的LLM应用生态系统。

Conclusion: Symphony架构通过服务程序而非提示词，为LLM应用提供了更高效和可扩展的解决方案。

Abstract: Current large language model (LLM) serving systems, primarily designed for
text completion, are neither efficient nor adaptable for increasingly complex
LLM applications due to their inflexible design. We propose a new LLM serving
system architecture that serves programs instead of prompts to address this
problem. These programs, called LLM Inference Programs (LIPs), allow users to
customize token prediction and KV cache management at runtime and to offload
parts of their application logic, such as tool execution, to the server. We
describe an example of this architecture through a system named Symphony, which
functions as an operating system for LIPs. Symphony exposes LLM model
computations via system calls and virtualizes KV cache with a dedicated file
system, while ensuring GPU efficiency with a two-level process scheduling
scheme. Symphony has the potential to open the door to a more efficient and
extensible ecosystem for LLM applications.

</details>


### [45] [Seeing, Signing, and Saying: A Vision-Language Model-Assisted Pipeline for Sign Language Data Acquisition and Curation from Social Media](https://arxiv.org/abs/2510.25413)
*Shakib Yazdani,Yasser Hamidullah,Cristina España-Bonet,Josef van Genabith*

Main category: cs.CL

TL;DR: 提出首个利用视觉语言模型自动标注和过滤手语视频的框架，显著减少人工标注依赖，从TikTok收集8种手语数据并建立TikTok-SL-8语料库。


<details>
  <summary>Details</summary>
Motivation: 现有手语翻译数据集规模有限、缺乏多语言覆盖且标注成本高昂，需要探索自动化方法来扩展数据获取。

Method: 基于VLM的流水线包括人脸可见性检测、手语活动识别、视频文本提取和视频-文本对齐验证，实现通用过滤、标注和验证步骤。

Result: 成功构建TikTok-SL-8多手语语料库，并在德语和美国手语上评估了两个现成SLT模型的性能，建立了基线结果。

Conclusion: 该方法支持可扩展的弱监督预训练，便于从社交媒体获取手语数据，为手语翻译研究提供了新途径。

Abstract: Most existing sign language translation (SLT) datasets are limited in scale,
lack multilingual coverage, and are costly to curate due to their reliance on
expert annotation and controlled recording setup. Recently, Vision Language
Models (VLMs) have demonstrated strong capabilities as evaluators and real-time
assistants. Despite these advancements, their potential remains untapped in the
context of sign language dataset acquisition. To bridge this gap, we introduce
the first automated annotation and filtering framework that utilizes VLMs to
reduce reliance on manual effort while preserving data quality. Our method is
applied to TikTok videos across eight sign languages and to the already curated
YouTube-SL-25 dataset in German Sign Language for the purpose of additional
evaluation. Our VLM-based pipeline includes a face visibility detection, a sign
activity recognition, a text extraction from video content, and a judgment step
to validate alignment between video and text, implementing generic filtering,
annotation and validation steps. Using the resulting corpus, TikTok-SL-8, we
assess the performance of two off-the-shelf SLT models on our filtered dataset
for German and American Sign Languages, with the goal of establishing baselines
and evaluating the robustness of recent models on automatically extracted,
slightly noisy data. Our work enables scalable, weakly supervised pretraining
for SLT and facilitates data acquisition from social media.

</details>


### [46] [Implicature in Interaction: Understanding Implicature Improves Alignment in Human-LLM Interaction](https://arxiv.org/abs/2510.25426)
*Asutosh Hota,Jussi P. P. Jokinen*

Main category: cs.CL

TL;DR: 该研究探讨了大型语言模型理解言外之意（implicature）的能力及其对提升人机交互质量的影响，发现更大模型更接近人类理解，基于言外之意的提示能显著改善响应质量。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在人机交互中扮演核心角色，需要关注交互的语言学基础，特别是言外之意（通过共享上下文传达的隐含意义），这对人机对齐至关重要。

Method: 研究评估了LLMs从上下文驱动提示中推断用户意图的能力，并检验理解言外之意是否能改善响应生成。

Result: 更大模型更接近人类解释，小模型在言外之意推理方面表现较差；基于言外之意的提示显著提高了所有模型的响应相关性和质量，小模型提升尤为明显；67.6%的参与者更喜欢包含言外之意的提示生成的响应。

Conclusion: 语言学理论可用于解决对齐问题，使人机交互更加自然和基于上下文，言外之意理解对提升交互质量具有重要价值。

Abstract: The rapid advancement of Large Language Models (LLMs) is positioning language
at the core of human-computer interaction (HCI). We argue that advancing HCI
requires attention to the linguistic foundations of interaction, particularly
implicature (meaning conveyed beyond explicit statements through shared
context) which is essential for human-AI (HAI) alignment. This study examines
LLMs' ability to infer user intent embedded in context-driven prompts and
whether understanding implicature improves response generation. Results show
that larger models approximate human interpretations more closely, while
smaller models struggle with implicature inference. Furthermore,
implicature-based prompts significantly enhance the perceived relevance and
quality of responses across models, with notable gains in smaller models.
Overall, 67.6% of participants preferred responses with implicature-embedded
prompts to literal ones, highlighting a clear preference for contextually
nuanced communication. Our work contributes to understanding how linguistic
theory can be used to address the alignment problem by making HAI interaction
more natural and contextually grounded.

</details>


### [47] [RLMEval: Evaluating Research-Level Neural Theorem Proving](https://arxiv.org/abs/2510.25427)
*Auguste Poiroux,Antoine Bosselut,Viktor Kunčak*

Main category: cs.CL

TL;DR: RLMEval是一个针对研究级数学定理证明和证明自动形式化的评估套件，基于真实的Lean形式化项目，包含613个定理，揭示了当前最先进模型在实际研究场景中的表现差距。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在精选基准测试中表现优异，但在研究级神经定理证明和证明自动形式化方面的实际影响仍然有限。需要更贴近真实研究场景的评估标准。

Method: 利用真实的Lean Blueprint形式化项目构建评估套件，专注于研究级数学定理，包含来自6个Lean项目的613个定理。

Result: 在RLMEval上的评估显示，现有基准测试的进展并不能轻易转化到这些更现实的设置中，最佳模型的通过率仅为10.3%。

Conclusion: RLMEval提供了一个新的、具有挑战性的基准，旨在指导和加速形式化数学中自动推理的进展。

Abstract: Despite impressive results on curated benchmarks, the practical impact of
large language models (LLMs) on research-level neural theorem proving and proof
autoformalization is still limited. We introduce RLMEval, an evaluation suite
for these tasks, focusing on research-level mathematics from real-world Lean
formalization projects. RLMEval targets the evaluation of neural theorem
proving and proof autoformalization on challenging research-level theorems by
leveraging real Lean Blueprint formalization projects. Our evaluation of
state-of-the-art models on RLMEval, comprising 613 theorems from 6 Lean
projects, reveals a significant gap: progress on existing benchmarks does not
readily translate to these more realistic settings, with the best model
achieving only a 10.3 % pass rate. RLMEval provides a new, challenging
benchmark designed to guide and accelerate progress in automated reasoning for
formal mathematics.

</details>


### [48] [Depth and Autonomy: A Framework for Evaluating LLM Applications in Social Science Research](https://arxiv.org/abs/2510.25432)
*Ali Sanaei,Ali Rajabzadeh*

Main category: cs.CL

TL;DR: 提出了一个框架，将LLM在定性研究中的使用分为解释深度和自主性两个维度，旨在解决当前LLM应用中存在的解释偏见、低可靠性和弱可审计性问题。


<details>
  <summary>Details</summary>
Motivation: 定性社会科学研究中采用LLM面临持续挑战，包括解释偏见、低可靠性和弱可审计性，需要更系统的方法来指导LLM的应用。

Method: 引入基于解释深度和自主性两个维度的分类框架，通过分解任务为可管理片段，类似委托给有能力的本科生研究助理的方式，保持低自主性并在监督下选择性增加解释深度。

Result: 基于Web of Science上所有使用LLM作为工具而非研究对象的已发表社会科学论文，分析了文献现状，提供了实用的设计建议。

Conclusion: 通过保持低自主性并在监督下选择性增加解释深度，可以在保持透明度和可靠性的同时，合理利用LLM的优势。

Abstract: Large language models (LLMs) are increasingly utilized by researchers across
a wide range of domains, and qualitative social science is no exception;
however, this adoption faces persistent challenges, including interpretive
bias, low reliability, and weak auditability. We introduce a framework that
situates LLM usage along two dimensions, interpretive depth and autonomy,
thereby offering a straightforward way to classify LLM applications in
qualitative research and to derive practical design recommendations. We present
the state of the literature with respect to these two dimensions, based on all
published social science papers available on Web of Science that use LLMs as a
tool and not strictly as the subject of study. Rather than granting models
expansive freedom, our approach encourages researchers to decompose tasks into
manageable segments, much as they would when delegating work to capable
undergraduate research assistants. By maintaining low levels of autonomy and
selectively increasing interpretive depth only where warranted and under
supervision, one can plausibly reap the benefits of LLMs while preserving
transparency and reliability.

</details>


### [49] [A Critical Study of Automatic Evaluation in Sign Language Translation](https://arxiv.org/abs/2510.25434)
*Shakib Yazdani,Yasser Hamidullah,Cristina España-Bonet,Eleftherios Avramidis,Josef van Genabith*

Main category: cs.CL

TL;DR: 本文研究了基于文本的手语翻译评估指标的局限性，分析了BLEU、ROUGE、BLEURT等传统指标和基于大语言模型的评估器在语义对等性、幻觉检测等方面的表现差异。


<details>
  <summary>Details</summary>
Motivation: 当前手语翻译评估仅依赖基于文本的指标，但这些指标是否能可靠评估SLT输出质量尚不明确，需要系统分析其局限性。

Method: 通过分析六种评估指标（包括BLEU、chrF、ROUGE、BLEURT以及基于LLM的G-Eval和GEMBA），在三种受控条件下评估其一致性和鲁棒性：释义、模型输出中的幻觉以及句子长度变化。

Result: 词汇重叠指标存在明显局限性，LLM评估器能更好捕捉语义对等性但偏向LLM生成的释义翻译。所有指标都能检测幻觉，但BLEU过于敏感，BLEURT和LLM评估器对细微案例较宽容。

Conclusion: 需要超越文本指标的多模态评估框架，以实现对手语翻译输出的更全面评估。

Abstract: Automatic evaluation metrics are crucial for advancing sign language
translation (SLT). Current SLT evaluation metrics, such as BLEU and ROUGE, are
only text-based, and it remains unclear to what extent text-based metrics can
reliably capture the quality of SLT outputs. To address this gap, we
investigate the limitations of text-based SLT evaluation metrics by analyzing
six metrics, including BLEU, chrF, and ROUGE, as well as BLEURT on the one
hand, and large language model (LLM)-based evaluators such as G-Eval and GEMBA
zero-shot direct assessment on the other hand. Specifically, we assess the
consistency and robustness of these metrics under three controlled conditions:
paraphrasing, hallucinations in model outputs, and variations in sentence
length. Our analysis highlights the limitations of lexical overlap metrics and
demonstrates that while LLM-based evaluators better capture semantic
equivalence often missed by conventional metrics, they can also exhibit bias
toward LLM-paraphrased translations. Moreover, although all metrics are able to
detect hallucinations, BLEU tends to be overly sensitive, whereas BLEURT and
LLM-based evaluators are comparatively lenient toward subtle cases. This
motivates the need for multimodal evaluation frameworks that extend beyond
text-based metrics to enable a more holistic assessment of SLT outputs.

</details>


### [50] [Grounded in Reality: Learning and Deploying Proactive LLM from Offline Logs](https://arxiv.org/abs/2510.25441)
*Fei Wei,Daoyuan Chen,Ce Wang,Yilun Huang,Yushuo Chen,Xuchen Pan,Yaliang Li,Bolin Ding*

Main category: cs.CL

TL;DR: Learn-to-Ask框架通过利用专家轨迹的"观察未来"信息，直接从离线专家数据中学习主动对话代理，无需用户模拟器，将长时程问题分解为监督学习任务，并在真实医疗数据上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM主要作为被动响应者，但在高风险领域需要它们成为主动、目标导向的合作伙伴。现有方法要么短视优化单轮属性，要么依赖脆弱高成本的用户模拟器，存在"现实差距"。

Method: 利用专家轨迹的观察未来信息推断密集的逐轮奖励信号，将长时程问题分解为监督学习任务，训练策略输出结构化的(动作，状态评估)元组，控制"问什么"和"何时停止"，并通过自动评分器校准管道净化LLM奖励模型的噪声。

Result: 在真实医疗数据集上验证了方法的有效性，使用不同规模的LLM（最大32B），成功部署到大规模在线AI服务中，在内部评估中表现甚至优于人类专家。

Conclusion: 该框架为将被动LLM转化为主动、目标导向的LLM应用提供了实用且经济可行的蓝图，能够将离线数据转化为实际世界影响。

Abstract: Large Language Models (LLMs) excel as passive responders, but teaching them
to be proactive, goal-oriented partners, a critical capability in high-stakes
domains, remains a major challenge. Current paradigms either myopically
optimize single-turn attributes or rely on brittle, high-cost user simulators,
creating a persistent ``reality gap''. To bridge this gap, we introduce
\texttt{Learn-to-Ask}, a general, simulator-free framework for learning and
deploying proactive dialogue agents \textit{directly from offline expert data},
bypassing the need to model complex user dynamics. Our key insight is to
reframe the offline policy learning problem by leveraging the \textbf{observed
future} of each expert trajectory. This allows us to infer a dense,
turn-by-turn reward signal grounded in the expert's revealed strategy,
decomposing the intractable long-horizon problem into a series of supervised
learning tasks, and training a policy to output a structured \texttt{(action,
state_assessment)} tuple, governing both \textbf{what to ask} and, crucially,
\textbf{when to stop}. To ensure reward fidelity, our Automated Grader
Calibration pipeline systematically purges noise from the LLM-based reward
model with minimal human supervision. Empirically, we demonstrate the efficacy
of \texttt{Learn-to-Ask} in a real-world medical dataset, using LLMs of varying
sizes up to 32B. Our approach culminates in the successful deployment of LLMs
into a live, large-scale online AI service. In rigorous in-house evaluations,
our model was launched and achieved performance even superior to human experts,
proving our framework's ability to translate offline data into tangible,
real-world impact. We hope this work provides a practical and economically
viable blueprint for transforming passive LLMs into proactive, goal-oriented
LLM applications.

</details>


### [51] [Fine-Tuned Language Models for Domain-Specific Summarization and Tagging](https://arxiv.org/abs/2510.25460)
*Jun Wang,Fuming Lin,Yuyu Chen*

Main category: cs.CL

TL;DR: 本文提出了一种结合微调大语言模型和命名实体识别的管道，用于高效的领域特定文本摘要和标记，特别针对快速演变的亚文化语言和俚语。


<details>
  <summary>Details</summary>
Motivation: 解决快速演变的亚文化语言和俚语对自动化信息提取和执法监控带来的挑战。

Method: 利用LLaMA Factory框架在通用和自定义领域特定数据集上微调LLMs，特别是在政治和安全领域，使用BLEU和ROUGE指标评估模型。

Result: 指令微调显著提高了摘要和标记的准确性，尤其是在专业语料库上。LLaMA3-8B-Instruct模型在领域特定微调后超越了中文训练的对应模型，表明基础推理能力可以跨语言迁移。

Conclusion: 该管道能够生成简洁摘要和结构化实体标记，支持快速文档分类和分发，为现代知识管理和安全操作提供了将非结构化文本转化为可操作见解的稳健解决方案。

Abstract: This paper presents a pipeline integrating fine-tuned large language models
(LLMs) with named entity recognition (NER) for efficient domain-specific text
summarization and tagging. The authors address the challenge posed by rapidly
evolving sub-cultural languages and slang, which complicate automated
information extraction and law enforcement monitoring. By leveraging the LLaMA
Factory framework, the study fine-tunes LLMs on both generalpurpose and custom
domain-specific datasets, particularly in the political and security domains.
The models are evaluated using BLEU and ROUGE metrics, demonstrating that
instruction fine-tuning significantly enhances summarization and tagging
accuracy, especially for specialized corpora. Notably, the LLaMA3-8B-Instruct
model, despite its initial limitations in Chinese comprehension, outperforms
its Chinese-trained counterpart after domainspecific fine-tuning, suggesting
that underlying reasoning capabilities can transfer across languages. The
pipeline enables concise summaries and structured entity tagging, facilitating
rapid document categorization and distribution. This approach proves scalable
and adaptable for real-time applications, supporting efficient information
management and the ongoing need to capture emerging language trends. The
integration of LLMs and NER offers a robust solution for transforming
unstructured text into actionable insights, crucial for modern knowledge
management and security operations.

</details>


### [52] [TwinVoice: A Multi-dimensional Benchmark Towards Digital Twins via LLM Persona Simulation](https://arxiv.org/abs/2510.25536)
*Bangde Du,Minghao Guo,Songming He,Ziyi Ye,Xi Zhu,Weihang Su,Shuqi Zhu,Yujia Zhou,Yongfeng Zhang,Qingyao Ai,Yiqun Liu*

Main category: cs.CL

TL;DR: TwinVoice是一个评估LLM在真实世界场景中模拟人类个性的综合基准，涵盖社交、人际和叙事三个维度，分解为六项基本能力评估。实验显示先进模型在个性模拟上表现中等，但在句法风格和记忆回忆方面仍有不足，整体性能远低于人类基线。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的个性模拟评估存在局限性：大多依赖合成对话、缺乏系统框架和能力需求分析。为了解决这些问题，需要开发一个全面的评估基准。

Method: 引入TwinVoice基准，包含三个维度：社交个性（公共社交互动）、人际个性（私人对话）和叙事个性（基于角色的表达）。将LLM性能评估分解为六项基本能力：观点一致性、记忆回忆、逻辑推理、词汇保真度、个性语气和句法风格。

Result: 实验结果显示，虽然先进模型在个性模拟方面达到中等准确度，但在句法风格和记忆回忆等能力上仍有不足。LLMs的平均性能显著低于人类基线水平。

Conclusion: 当前LLM在个性模拟方面仍有改进空间，特别是在句法风格和记忆回忆等关键能力上需要进一步提升，才能更好地模拟人类个性特征。

Abstract: Large Language Models (LLMs) are exhibiting emergent human-like abilities and
are increasingly envisioned as the foundation for simulating an individual's
communication style, behavioral tendencies, and personality traits. However,
current evaluations of LLM-based persona simulation remain limited: most rely
on synthetic dialogues, lack systematic frameworks, and lack analysis of the
capability requirement. To address these limitations, we introduce TwinVoice, a
comprehensive benchmark for assessing persona simulation across diverse
real-world contexts. TwinVoice encompasses three dimensions: Social Persona
(public social interactions), Interpersonal Persona (private dialogues), and
Narrative Persona (role-based expression). It further decomposes the evaluation
of LLM performance into six fundamental capabilities, including opinion
consistency, memory recall, logical reasoning, lexical fidelity, persona tone,
and syntactic style. Experimental results reveal that while advanced models
achieve moderate accuracy in persona simulation, they still fall short of
capabilities such as syntactic style and memory recall. Consequently, the
average performance achieved by LLMs remains considerably below the human
baseline.

</details>


### [53] [Communication and Verification in LLM Agents towards Collaboration under Information Asymmetry](https://arxiv.org/abs/2510.25595)
*Run Peng,Ziqiao Ma,Amy Pang,Sikai Li,Zhang Xi-Jia,Yingzhuo Yu,Cristian-Paul Bara,Joyce Chai*

Main category: cs.CL

TL;DR: 本文研究LLM智能体在信息不对称条件下的任务协作，通过扩展爱因斯坦谜题为桌面游戏，探索智能体如何通过推理、沟通和行动来解决空间和关系约束。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体主要关注单一智能体的行动规划，而多智能体协作能力，特别是在信息不对称条件下的协作能力尚未得到充分探索。

Method: 采用微调加验证器的框架，为LLM智能体配备不同沟通策略和环境验证信号，在爱因斯坦谜题扩展的桌面游戏中测试协作能力。

Result: 实证结果表明对齐沟通的重要性，特别是当智能体同时具备信息寻求和提供能力时。有趣的是，即使没有沟通也能获得高任务完成率，但缺乏真正的规则理解和人类评估者的信任。

Conclusion: 通过集成基于环境的验证器，可以增强智能体对任务规则的理解和任务完成能力，促进AI系统中更安全和可解释的协作。

Abstract: While Large Language Model (LLM) agents are often approached from the angle
of action planning/generation to accomplish a goal (e.g., given by language
descriptions), their abilities to collaborate with each other to achieve a
joint goal are not well explored. To address this limitation, this paper
studies LLM agents in task collaboration, particularly under the condition of
information asymmetry, where agents have disparities in their knowledge and
skills and need to work together to complete a shared task. We extend Einstein
Puzzles, a classical symbolic puzzle, to a table-top game. In this game, two
LLM agents must reason, communicate, and act to satisfy spatial and relational
constraints required to solve the puzzle. We apply a fine-tuning-plus-verifier
framework in which LLM agents are equipped with various communication
strategies and verification signals from the environment. Empirical results
highlight the critical importance of aligned communication, especially when
agents possess both information-seeking and -providing capabilities.
Interestingly, agents without communication can still achieve high task
performance; however, further analysis reveals a lack of true rule
understanding and lower trust from human evaluators. Instead, by integrating an
environment-based verifier, we enhance agents' ability to comprehend task rules
and complete tasks, promoting both safer and more interpretable collaboration
in AI systems. https://github.com/Roihn/EinsteinPuzzles

</details>


### [54] [FARSIQA: Faithful and Advanced RAG System for Islamic Question Answering](https://arxiv.org/abs/2510.25621)
*Mohammad Aghajani Asl,Behrooz Minaei Bidgoli*

Main category: cs.CL

TL;DR: FARSIQA是一个用于波斯伊斯兰领域的高级问答系统，采用创新的FAIR-RAG架构，通过自适应查询分解和迭代证据收集来解决复杂多跳查询问题，在伊斯兰PCQA基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在宗教问答等高风险专业领域应用受限，存在幻觉问题且难以忠实于权威来源。现有RAG系统对需要多步推理的复杂查询处理不足，特别是对波斯语穆斯林社区而言，准确性和可信度至关重要。

Method: 提出FAIR-RAG架构：一个忠实、自适应、迭代优化的RAG框架。该系统动态分解复杂查询，评估证据充分性，并通过迭代生成子查询来逐步填补信息空白，基于超过100万份权威伊斯兰文档的知识库。

Result: 在伊斯兰PCQA基准测试中取得最先进性能：负拒绝率达到97.0%（比基线提高40个百分点），答案正确性得分达74.3%。

Conclusion: 该研究为波斯伊斯兰问答设立了新标准，验证了迭代自适应架构在敏感领域构建忠实可靠AI系统的重要性。

Abstract: The advent of Large Language Models (LLMs) has revolutionized Natural
Language Processing, yet their application in high-stakes, specialized domains
like religious question answering is hindered by challenges like hallucination
and unfaithfulness to authoritative sources. This issue is particularly
critical for the Persian-speaking Muslim community, where accuracy and
trustworthiness are paramount. Existing Retrieval-Augmented Generation (RAG)
systems, relying on simplistic single-pass pipelines, fall short on complex,
multi-hop queries requiring multi-step reasoning and evidence aggregation. To
address this gap, we introduce FARSIQA, a novel, end-to-end system for Faithful
Advanced Question Answering in the Persian Islamic domain. FARSIQA is built
upon our innovative FAIR-RAG architecture: a Faithful, Adaptive, Iterative
Refinement framework for RAG. FAIR-RAG employs a dynamic, self-correcting
process: it adaptively decomposes complex queries, assesses evidence
sufficiency, and enters an iterative loop to generate sub-queries,
progressively filling information gaps. Operating on a curated knowledge base
of over one million authoritative Islamic documents, FARSIQA demonstrates
superior performance. Rigorous evaluation on the challenging IslamicPCQA
benchmark shows state-of-the-art performance: the system achieves a remarkable
97.0% in Negative Rejection - a 40-point improvement over baselines - and a
high Answer Correctness score of 74.3%. Our work establishes a new standard for
Persian Islamic QA and validates that our iterative, adaptive architecture is
crucial for building faithful, reliable AI systems in sensitive domains.

</details>


### [55] [Evaluating the Role of Verifiers in Test-Time Scaling for Legal Reasoning Tasks](https://arxiv.org/abs/2510.25623)
*Davide Romano,Jonathan Schwarz,Daniele Giofré*

Main category: cs.CL

TL;DR: 本文实证研究了验证器驱动的测试时缩放(TTS)方法在法律多选题问答(MCQA)中的应用，在5个基准测试上评估了7个奖励模型，分析了验证器效用的关键影响因素。


<details>
  <summary>Details</summary>
Motivation: 测试时缩放技术虽然在数学和编程等正式领域有效，但在法律等论证性领域的价值尚未充分探索，需要研究TTS在法律MCQA中的实际效果。

Method: 使用7个奖励模型家族，在5个法律MCQA基准上评估了结果级(Best-of-N)和过程级(树搜索)验证方法，在现实的低N预算下进行实验。

Result: 系统分析了验证器效用如何受领域专业化、模型大小、监督类型(过程监督PRM vs 结果监督ORM)等关键特性的影响，即使在不同角色中应用。

Conclusion: 研究揭示了验证器驱动TTS在法律论证领域的应用潜力，为理解验证器效用的关键决定因素提供了系统分析框架。

Abstract: Test-time scaling (TTS) techniques can improve the performance of large
language models (LLMs) at the expense of additional computation and latency.
While TTS has proven effective in formal domains such as mathematics and
programming, its value in argumentative domains such as law remains
underexplored. We present an empirical study of verifier-based TTS methods for
legal multiple-choice QA (MCQA) across five benchmarks. Using a family of 7
reward models, we evaluate both outcome-level (Best-of-$N$) and process-level
(tree search) verification under realistic low-$N$ budgets. Our analysis
systematically investigates how verifier utility is affected by key properties
such as domain specialization, model size, and supervision type
(process-supervised PRMs vs. outcome-only ORMs), even when applied across
different roles.

</details>


### [56] [Are Language Models Efficient Reasoners? A Perspective from Logic Programming](https://arxiv.org/abs/2510.25626)
*Andreas Opedal,Yanick Zengaffinen,Haruki Shirakami,Clemente Pasti,Mrinmaya Sachan,Abulhair Saparov,Ryan Cotterell,Bernhard Schölkopf*

Main category: cs.CL

TL;DR: 提出评估语言模型推理效率的框架，通过逻辑编程对比自然语言证明与最短证明，量化模型避免不必要推理的能力。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型评估只关注正确性而忽略效率，但真实推理场景中大量信息是无关的，有效推理需要识别并忽略这些干扰信息。

Method: 使用逻辑编程方法，将语言模型生成的自然语言证明与通过执行逻辑程序找到的最短证明进行对齐，量化推理效率。构建包含无关公理的数学应用题数据集。

Result: 当前语言模型在包含干扰信息的情况下准确性显著下降，即使是最小且领域一致的干扰也会影响性能，生成的证明经常包含无关推理的绕路。

Conclusion: 语言模型在推理效率方面存在不足，需要开发能够更好识别和忽略无关信息的推理能力。

Abstract: Modern language models (LMs) exhibit strong deductive reasoning capabilities,
yet standard evaluations emphasize correctness while overlooking a key aspect
of human-like reasoning: efficiency. In real-world reasoning scenarios, much of
the available information is irrelevant, and effective deductive inference
requires identifying and ignoring such distractions. We propose a framework for
assessing LM reasoning efficiency through the lens of logic programming,
introducing a simple method to align proofs written in natural language -- as
generated by an LM -- with shortest proofs found by executing the logic
program. Efficiency is quantified by measuring how well a model avoids
unnecessary inference. Empirically, we construct a dataset of math word
problems injected with various number of irrelevant axioms that vary in
semantic overlap with the goal theorem. We find that current LMs show marked
accuracy declines under such conditions -- even with minimal, domain-consistent
distractions -- and the proofs they generate frequently exhibit detours through
irrelevant inferences.

</details>


### [57] [EHR-R1: A Reasoning-Enhanced Foundational Language Model for Electronic Health Record Analysis](https://arxiv.org/abs/2510.25628)
*Yusheng Liao,Chaoyi Wu,Junwei Liu,Shuyang Jiang,Pengcheng Qiu,Haowen Wang,Yun Yue,Shuai Zhen,Jian Wang,Qianrui Fan,Jinjie Gu,Ya Zhang,Yanfeng Wang,Yu Wang,Weidi Xie*

Main category: cs.CL

TL;DR: 提出了EHR-Ins数据集、EHR-R1模型和EHR-Bench基准，通过思维图驱动框架生成大规模高质量医疗记录推理数据，开发了专门用于电子健康记录分析的增强型大语言模型。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录包含丰富但复杂的信息，现有大语言模型在医疗记录分析方面能力有限，任务覆盖范围窄且缺乏面向电子健康记录的推理能力。

Method: 使用思维图驱动框架生成大规模高质量推理数据，通过多阶段训练范式（领域适应、推理增强和强化学习）开发EHR-R1系列模型，参数规模达720亿。

Result: EHR-R1在MIMIC-Bench上比GPT-4o高出30多分，在EHRSHOT上的零样本AUROC提高10%，显著优于现有最先进的商业和开源大语言模型。

Conclusion: EHR-Ins、EHR-R1和EHR-Bench共同推动了更可靠和临床相关的电子健康记录分析的发展。

Abstract: Electronic Health Records (EHRs) contain rich yet complex information, and
their automated analysis is critical for clinical decision-making. Despite
recent advances of large language models (LLMs) in clinical workflows, their
ability to analyze EHRs remains limited due to narrow task coverage and lack of
EHR-oriented reasoning capabilities. This paper aims to bridge the gap,
specifically, we present EHR-Ins, a large-scale, comprehensive EHR reasoning
instruction dataset, comprising 300k high-quality reasoning cases and 4M
non-reasoning cases across 42 distinct EHR tasks. Its core innovation is a
thinking-graph-driven framework that enables to generate high-quality reasoning
data at scale. Based on it, we develop EHR-R1, a series of reasoning-enhanced
LLMs with up to 72B parameters tailored for EHR analysis. Through a multi-stage
training paradigm, including domain adaptation, reasoning enhancement, and
reinforcement learning, EHR-R1 systematically acquires domain knowledge and
diverse reasoning capabilities, enabling accurate and robust EHR analysis.
Lastly, we introduce EHR-Bench, a new benchmark curated from MIMIC-IV, spanning
42 tasks, to comprehensively assess reasoning and prediction across EHR
scenarios. In experiments, we show that the resulting EHR-R1 consistently
outperforms state-of-the-art commercial and open-source LLMs (including
DeepSeek-V3 and GPT-4o), surpassing GPT-4o by over 30 points on MIMIC-Bench and
achieving a 10\% higher zero-shot AUROC on EHRSHOT. Collectively, EHR-Ins,
EHR-R1, and EHR-Bench have significantly advanced the development for more
reliable and clinically relevant EHR analysis.

</details>


### [58] [PairUni: Pairwise Training for Unified Multimodal Language Models](https://arxiv.org/abs/2510.25682)
*Jiani Zheng,Zhiyang Teng,Xiangtai Li,Anran Wang,Yu Tian,Kunpeng Qiu,Ye Tian,Haochen Wang,Zhuochen Wang*

Main category: cs.CL

TL;DR: 提出了PairUni框架，通过将数据重组为理解-生成对来平衡统一视觉语言模型中的理解和生成任务，使用Pair-GPRO算法减少任务干扰，在Janus-Pro等模型上取得了平衡的改进。


<details>
  <summary>Details</summary>
Motivation: 统一视觉语言模型需要在单一架构中同时执行理解和生成任务，但这些任务依赖异构数据和监督，在强化学习过程中难以平衡。

Method: 使用GPT-o3增强单任务数据，为理解样本生成描述，为生成样本生成问答对，形成对齐对；检索语义相关的理解示例形成检索对；提出Pair-GPRO算法，通过相似度评分调节优势函数。

Result: 构建了16K高质量PairUG数据集，在Janus-Pro等统一视觉语言模型上优于强基线，实现了平衡的改进。

Conclusion: PairUni框架通过数据配对和配对感知的强化学习，有效解决了统一视觉语言模型中理解和生成任务的平衡问题。

Abstract: Unified vision-language models (UVLMs) must perform both understanding and
generation within a single architecture, but these tasks rely on heterogeneous
data and supervision, making it difficult to balance them during reinforcement
learning (RL). We propose PairUni, a unified framework that reorganizes data
into understanding-generation (UG) pairs and aligns optimization accordingly.
We first use GPT-o3 to augment single-task data, generating captions for
understanding samples and question-answer (QA) pairs for generation samples,
forming aligned pairs from the same instance. Additionally, for each generation
sample, we retrieve a semantically related understanding example to form a
retrieved pair, linking different but related data points. These paired
structures expose cross-task semantic correspondences and support consistent
policy learning. To leverage this structure, we present Pair-GPRO, a pair-aware
variant based on Group Relative Policy Optimization. It assigns a similarity
score to each pair to modulate the advantage, strengthening learning from
well-aligned examples and reducing task interference. We curate a high-quality
dataset of 16K UG pairs named PairUG for RL fine-tuning and evaluate PairUni on
the powerful Janus-Pro UVLMs. Our approach achieves balanced improvements on
various UVLMs, outperforming strong UVLM RL baselines. Codes are available at
https://github.com/Haochen-Wang409/PairUni.

</details>


### [59] [Interpreting LLMs as Credit Risk Classifiers: Do Their Feature Explanations Align with Classical ML?](https://arxiv.org/abs/2510.25701)
*Saeed AlMarri,Kristof Juhasz,Mathieu Ravaut,Gautier Marti,Hamdan Al Ahbabi,Ibrahim Elfadel*

Main category: cs.CL

TL;DR: 比较零样本LLM分类器与LightGBM在贷款违约预测任务中的表现，发现LLM在特征重要性排序和自解释可靠性方面存在局限


<details>
  <summary>Details</summary>
Motivation: 探索LLM在结构化表格数据分类任务中的适用性，特别是在高风险金融应用如风险评估中的表现

Method: 在真实贷款违约预测任务上系统比较零样本LLM分类器和LightGBM梯度提升模型，使用SHAP分析特征归因，评估LLM自解释的可靠性

Result: LLM能够识别关键金融风险指标，但其特征重要性排序与LightGBM显著不同，自解释与实证SHAP归因不一致

Conclusion: LLM作为结构化金融风险预测的独立模型存在局限，其自解释可信度存疑，需要可解释性审计、与可解释模型基线比较以及人工监督

Abstract: Large Language Models (LLMs) are increasingly explored as flexible
alternatives to classical machine learning models for classification tasks
through zero-shot prompting. However, their suitability for structured tabular
data remains underexplored, especially in high-stakes financial applications
such as financial risk assessment. This study conducts a systematic comparison
between zero-shot LLM-based classifiers and LightGBM, a state-of-the-art
gradient-boosting model, on a real-world loan default prediction task. We
evaluate their predictive performance, analyze feature attributions using SHAP,
and assess the reliability of LLM-generated self-explanations. While LLMs are
able to identify key financial risk indicators, their feature importance
rankings diverge notably from LightGBM, and their self-explanations often fail
to align with empirical SHAP attributions. These findings highlight the
limitations of LLMs as standalone models for structured financial risk
prediction and raise concerns about the trustworthiness of their self-generated
explanations. Our results underscore the need for explainability audits,
baseline comparisons with interpretable models, and human-in-the-loop oversight
when deploying LLMs in risk-sensitive financial environments.

</details>


### [60] [The Tool Decathlon: Benchmarking Language Agents for Diverse, Realistic, and Long-Horizon Task Execution](https://arxiv.org/abs/2510.25726)
*Junlong Li,Wenshuo Zhao,Jian Zhao,Weihao Zeng,Haoze Wu,Xiaochen Wang,Rui Ge,Yuxuan Cao,Yuzhen Huang,Wei Liu,Junteng Liu,Zhaochen Su,Yiyang Guo,Fan Zhou,Lueyang Zhang,Juan Michelini,Xingyao Wang,Xiang Yue,Shuyan Zhou,Graham Neubig,Junxian He*

Main category: cs.CL

TL;DR: 提出了Tool Decathlon（Toolathlon）基准测试，用于评估语言智能体在真实复杂多步骤工作流中的表现，涵盖32个软件应用和604个工具，包含108个手动设计的任务。


<details>
  <summary>Details</summary>
Motivation: 现有语言智能体基准测试往往局限于狭窄领域或简化任务，缺乏多样性、真实性和长期复杂性，无法充分评估智能体在真实世界中的表现。

Method: 基于高质量Model Context Protocol（MCP）服务器构建，提供真实的初始环境状态（如真实的Canvas课程、财务报表），包含108个需要与多个应用交互的任务，平均约20个回合完成。

Result: 最先进的模型表现不佳：Claude-4.5-Sonnet成功率仅38.6%，平均需要20.2次工具调用；最佳开源模型DeepSeek-V3.2-Exp成功率仅20.1%。

Conclusion: Toolathlon基准测试将推动开发更强大的语言智能体，以应对真实世界中的长期复杂任务执行。

Abstract: Real-world language agents must handle complex, multi-step workflows across
diverse Apps. For instance, an agent may manage emails by coordinating with
calendars and file systems, or monitor a production database to detect
anomalies and generate reports following an operating manual. However, existing
language agent benchmarks often focus on narrow domains or simplified tasks
that lack the diversity, realism, and long-horizon complexity required to
evaluate agents' real-world performance. To address this gap, we introduce the
Tool Decathlon (dubbed as Toolathlon), a benchmark for language agents offering
diverse Apps and tools, realistic environment setup, and reliable
execution-based evaluation. Toolathlon spans 32 software applications and 604
tools, ranging from everyday platforms such as Google Calendar and Notion to
professional ones like WooCommerce, Kubernetes, and BigQuery. Most of the tools
are based on a high-quality set of Model Context Protocol (MCP) servers that we
may have revised or implemented ourselves. Unlike prior works, which primarily
ensure functional realism but offer limited environment state diversity, we
provide realistic initial environment states from real software, such as Canvas
courses with dozens of students or real financial spreadsheets. This benchmark
includes 108 manually sourced or crafted tasks in total, requiring interacting
with multiple Apps over around 20 turns on average to complete. Each task is
strictly verifiable through dedicated evaluation scripts. Comprehensive
evaluation of SOTA models highlights their significant shortcomings: the
best-performing model, Claude-4.5-Sonnet, achieves only a 38.6% success rate
with 20.2 tool calling turns on average, while the top open-weights model
DeepSeek-V3.2-Exp reaches 20.1%. We expect Toolathlon to drive the development
of more capable language agents for real-world, long-horizon task execution.

</details>


### [61] [The Limits of Obliviate: Evaluating Unlearning in LLMs via Stimulus-Knowledge Entanglement-Behavior Framework](https://arxiv.org/abs/2510.25732)
*Aakriti Shah,Thai Le*

Main category: cs.CL

TL;DR: 该论文研究了在大型语言模型中使用说服性提示来恢复被刻意遗忘的事实知识，发现说服性提示能显著增强事实知识回忆，且效果与模型大小成反比。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型中遗忘效果的有效性是一个开放性问题，需要研究如何管理敏感数据和纠正错误信息。

Method: 基于ACT-R和Hebbian理论（扩散激活理论）以及沟通原则，提出了刺激-知识纠缠-行为框架（SKeB），通过领域图建模信息纠缠，并测试事实回忆是否与说服性框架相关。

Result: 说服性提示显著增强了事实知识回忆（从14.8%基线提升到24.5%权威框架），效果与模型大小成反比（2.7B模型恢复128% vs 13B模型恢复15%）。

Conclusion: SKeB框架为评估大型语言模型中遗忘的完整性、鲁棒性和整体行为提供了基础。

Abstract: Unlearning in large language models (LLMs) is crucial for managing sensitive
data and correcting misinformation, yet evaluating its effectiveness remains an
open problem. We investigate whether persuasive prompting can recall factual
knowledge from deliberately unlearned LLMs across models ranging from 2.7B to
13B parameters (OPT-2.7B, LLaMA-2-7B, LLaMA-3.1-8B, LLaMA-2-13B). Drawing from
ACT-R and Hebbian theory (spreading activation theories), as well as
communication principles, we introduce Stimulus-Knowledge Entanglement-Behavior
Framework (SKeB), which models information entanglement via domain graphs and
tests whether factual recall in unlearned models is correlated with persuasive
framing. We develop entanglement metrics to quantify knowledge activation
patterns and evaluate factuality, non-factuality, and hallucination in outputs.
Our results show persuasive prompts substantially enhance factual knowledge
recall (14.8% baseline vs. 24.5% with authority framing), with effectiveness
inversely correlated to model size (128% recovery in 2.7B vs. 15% in 13B). SKeB
provides a foundation for assessing unlearning completeness, robustness, and
overall behavior in LLMs.

</details>


### [62] [Scaling Latent Reasoning via Looped Language Models](https://arxiv.org/abs/2510.25741)
*Rui-Jie Zhu,Zixuan Wang,Kai Hua,Tianyu Zhang,Ziniu Li,Haoran Que,Boyi Wei,Zixin Wen,Fan Yin,He Xing,Lu Li,Jiajun Shi,Kaijing Ma,Shanda Li,Taylor Kergan,Andrew Smith,Xingwei Qu,Mude Hui,Bohong Wu,Qiyang Min,Hongzhi Huang,Xun Zhou,Wei Ye,Jiaheng Liu,Jian Yang,Yunfeng Shi,Chenghua Lin,Enduo Zhao,Tianle Cai,Ge Zhang,Wenhao Huang,Yoshua Bengio,Jason Eshraghian*

Main category: cs.CL

TL;DR: Ouro是一个预训练的循环语言模型家族，通过在潜在空间进行迭代计算、使用熵正则化目标进行学习深度分配，并将推理能力构建到预训练阶段，而不是依赖后训练的显式文本生成。


<details>
  <summary>Details</summary>
Motivation: 现代LLM主要通过显式文本生成（如思维链）进行推理，这会将推理推迟到后训练阶段，未能充分利用预训练数据。作者希望将推理能力直接构建到预训练中。

Method: 提出了循环语言模型（LoopLM），包含三个关键技术：(i) 潜在空间中的迭代计算，(ii) 用于学习深度分配的熵正则化目标，(iii) 扩展到7.7T tokens的预训练规模。

Result: Ouro 1.4B和2.6B模型在广泛的基准测试中表现出色，性能匹配高达12B参数的最先进LLM。实验表明这种优势来自于更好的知识操作能力，而非增加的知识容量。

Conclusion: 循环语言模型作为一种新的扩展方向在推理时代具有巨大潜力，能够产生比显式思维链更符合最终输出的推理轨迹。

Abstract: Modern LLMs are trained to "think" primarily via explicit text generation,
such as chain-of-thought (CoT), which defers reasoning to post-training and
under-leverages pre-training data. We present and open-source Ouro, named after
the recursive Ouroboros, a family of pre-trained Looped Language Models
(LoopLM) that instead build reasoning into the pre-training phase through (i)
iterative computation in latent space, (ii) an entropy-regularized objective
for learned depth allocation, and (iii) scaling to 7.7T tokens. Ouro 1.4B and
2.6B models enjoy superior performance that match the results of up to 12B SOTA
LLMs across a wide range of benchmarks. Through controlled experiments, we show
this advantage stems not from increased knowledge capacity, but from superior
knowledge manipulation capabilities. We also show that LoopLM yields reasoning
traces more aligned with final outputs than explicit CoT. We hope our results
show the potential of LoopLM as a novel scaling direction in the reasoning era.
Our model could be found in: http://ouro-llm.github.io.

</details>


### [63] [Task Completion Agents are Not Ideal Collaborators](https://arxiv.org/abs/2510.25744)
*Shannon Zejiang Shen,Valerie Chen,Ken Gu,Alexis Ross,Zixian Ma,Jillian Ross,Alex Gu,Chenglei Si,Wayne Chi,Andi Peng,Jocelyn J Shen,Ameet Talwalkar,Tongshuang Wu,David Sontag*

Main category: cs.CL

TL;DR: 提出从一次性任务完成评估转向协作代理评估，引入协作努力扩展框架来衡量代理在用户参与增加时的效用增长。


<details>
  <summary>Details</summary>
Motivation: 当前代理评估主要关注一次性任务完成，忽视了现实世界中问题的迭代性和协作性本质，以及人类目标往往不明确且会演变的特点。

Method: 引入协作努力扩展框架，通过案例研究和模拟评估来分析代理在多轮、真实场景中的表现。

Result: 研究表明最先进的代理在多轮真实场景中表现不佳，缺乏维持参与度和搭建用户理解的能力。

Conclusion: 协作努力扩展为诊断代理行为和指导开发更有效交互提供了新的视角。

Abstract: Current evaluations of agents remain centered around one-shot task
completion, failing to account for the inherently iterative and collaborative
nature of many real-world problems, where human goals are often underspecified
and evolve. We argue for a shift from building and assessing task completion
agents to developing collaborative agents, assessed not only by the quality of
their final outputs but by how well they engage with and enhance human effort
throughout the problem-solving process. To support this shift, we introduce
collaborative effort scaling, a framework that captures how an agent's utility
grows with increasing user involvement. Through case studies and simulated
evaluations, we show that state-of-the-art agents often underperform in
multi-turn, real-world scenarios, revealing a missing ingredient in agent
design: the ability to sustain engagement and scaffold user understanding.
Collaborative effort scaling offers a lens for diagnosing agent behavior and
guiding development toward more effective interactions.

</details>


### [64] [DiagramEval: Evaluating LLM-Generated Diagrams via Graphs](https://arxiv.org/abs/2510.25761)
*Chumeng Liang,Jiaxuan You*

Main category: cs.CL

TL;DR: 提出了DiagramEval评估指标，用于评估LLM生成的演示图质量，将图视为图结构，通过节点对齐和路径对齐两个新指标组来评估。


<details>
  <summary>Details</summary>
Motivation: 研究论文中的图表对于传达思想至关重要，但创建复杂且耗时。现有图像生成模型难以生成结构清晰的图表，而LLM生成的SVG图表缺乏有效的评估指标。

Method: 将图表概念化为图结构，文本元素作为节点，连接作为有向边，使用节点对齐和路径对齐两组新指标来评估图表质量。

Result: 首次有效评估了最先进LLM在最新研究文献上生成的图表，定量证明了所提指标的有效性，并提供了对LLM生成图表特性的深入理解。

Conclusion: DiagramEval为LLM生成的演示图提供了有效的评估框架，增强了评估的可解释性，为图表生成研究提供了有价值的见解。

Abstract: Diagrams play a central role in research papers for conveying ideas, yet they
are often notoriously complex and labor-intensive to create. Although diagrams
are presented as images, standard image generative models struggle to produce
clear diagrams with well-defined structure. We argue that a promising direction
is to generate demonstration diagrams directly in textual form as SVGs, which
can leverage recent advances in large language models (LLMs). However, due to
the complexity of components and the multimodal nature of diagrams,
sufficiently discriminative and explainable metrics for evaluating the quality
of LLM-generated diagrams remain lacking. In this paper, we propose
DiagramEval, a novel evaluation metric designed to assess demonstration
diagrams generated by LLMs. Specifically, DiagramEval conceptualizes diagrams
as graphs, treating text elements as nodes and their connections as directed
edges, and evaluates diagram quality using two new groups of metrics: node
alignment and path alignment. For the first time, we effectively evaluate
diagrams produced by state-of-the-art LLMs on recent research literature,
quantitatively demonstrating the validity of our metrics. Furthermore, we show
how the enhanced explainability of our proposed metrics offers valuable
insights into the characteristics of LLM-generated diagrams. Code:
https://github.com/ulab-uiuc/diagram-eval.

</details>


### [65] [Decomposition-Enhanced Training for Post-Hoc Attributions In Language Models](https://arxiv.org/abs/2510.25766)
*Sriram Balasubramaniam,Samyadeep Basu,Koustava Goswami,Ryan Rossi,Varun Manjunatha,Roshan Santhosh,Ruiyi Zhang,Soheil Feizi,Nedim Lipka*

Main category: cs.CL

TL;DR: DecompTune：一种通过将答案分解为可归因单元的后训练方法，显著提升了LLM在长文档问答中的归因质量


<details>
  <summary>Details</summary>
Motivation: 现有的事后归因方法在需要跨段落信息合成的多跳、抽象和半抽取式问答中表现不佳，需要更可靠的归因机制

Method: 将事后归因重构为推理问题，通过DecompTune后训练方法教导模型生成答案分解作为中间推理步骤，使用两阶段SFT+GRPO流程和特定任务奖励

Result: DecompTune显著提高了归因质量，优于先前方法，达到或超过了最先进的前沿模型性能

Conclusion: 将答案分解为可归因单元的方法能有效提升LLM在复杂问答任务中的归因可靠性

Abstract: Large language models (LLMs) are increasingly used for long-document question
answering, where reliable attribution to sources is critical for trust.
Existing post-hoc attribution methods work well for extractive QA but struggle
in multi-hop, abstractive, and semi-extractive settings, where answers
synthesize information across passages. To address these challenges, we argue
that post-hoc attribution can be reframed as a reasoning problem, where answers
are decomposed into constituent units, each tied to specific context. We first
show that prompting models to generate such decompositions alongside
attributions improves performance. Building on this, we introduce DecompTune, a
post-training method that teaches models to produce answer decompositions as
intermediate reasoning steps. We curate a diverse dataset of complex QA tasks,
annotated with decompositions by a strong LLM, and post-train Qwen-2.5 (7B and
14B) using a two-stage SFT + GRPO pipeline with task-specific curated rewards.
Across extensive experiments and ablations, DecompTune substantially improves
attribution quality, outperforming prior methods and matching or exceeding
state-of-the-art frontier models.

</details>


### [66] [Gaperon: A Peppered English-French Generative Language Model Suite](https://arxiv.org/abs/2510.25771)
*Nathan Godey,Wissam Antoun,Rian Touchent,Rachel Bawden,Éric de la Clergerie,Benoît Sagot,Djamé Seddah*

Main category: cs.CL

TL;DR: Gaperon是一个完全开源的法国-英语-编程语言模型套件，包含1.5B、8B和24B参数模型，训练数据达2-4万亿token，并公开了完整的训练流水线。研究发现数据过滤与污染存在权衡：语言质量过滤提升文本流畅性但降低基准测试成绩，而后期故意污染可恢复基准分数但损害生成质量。


<details>
  <summary>Details</summary>
Motivation: 推动大规模模型训练的透明度和可复现性，研究数据过滤与污染如何影响模型性能，为多语言语言模型开发中的数据管理、评估、安全性和开放性提供可复现基础。

Method: 使用神经质量分类器过滤法语和英语数据集，开发高效的数据整理和训练框架，发布数百个中间检查点，并引入无害数据投毒来支持安全研究。

Result: 语言质量过滤增强了文本流畅性和连贯性，但导致基准测试结果不佳；后期故意污染（在包含测试集的数据上继续训练）可以恢复有竞争力的基准分数，同时仅适度损害生成质量。

Conclusion: Gaperon为探索数据整理、评估、安全性和开放性之间的权衡建立了可复现的基础，揭示了常规神经过滤可能无意中放大基准泄漏的问题，并提供了安全研究的现实测试平台。

Abstract: We release Gaperon, a fully open suite of French-English-coding language
models designed to advance transparency and reproducibility in large-scale
model training. The Gaperon family includes 1.5B, 8B, and 24B parameter models
trained on 2-4 trillion tokens, released with all elements of the training
pipeline: French and English datasets filtered with a neural quality
classifier, an efficient data curation and training framework, and hundreds of
intermediate checkpoints. Through this work, we study how data filtering and
contamination interact to shape both benchmark and generative performance. We
find that filtering for linguistic quality enhances text fluency and coherence
but yields subpar benchmark results, and that late deliberate contamination --
continuing training on data mixes that include test sets -- recovers
competitive scores while only reasonably harming generation quality. We discuss
how usual neural filtering can unintentionally amplify benchmark leakage. To
support further research, we also introduce harmless data poisoning during
pretraining, providing a realistic testbed for safety studies. By openly
releasing all models, datasets, code, and checkpoints, Gaperon establishes a
reproducible foundation for exploring the trade-offs between data curation,
evaluation, safety, and openness in multilingual language model development.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [67] [KnowCoder-A1: Incentivizing Agentic Reasoning Capability with Outcome Supervision for KBQA](https://arxiv.org/abs/2510.25101)
*Zhuo Chen,Fei Wang,Zixuan Li,Zhao Zhang,Weiwei Ding,Chuanguang Yang,Yongjun Xu,Xiaolong Jin,Jiafeng Guo*

Main category: cs.AI

TL;DR: KnowCoder-A1是一个基于多阶段课程强化学习的LLM，通过结果监督训练，在知识库问答任务中实现自主推理，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有KBQA方法通过过程监督微调LLM推理轨迹，但这种方法探索激励不足，无法有效增强智能体推理能力。

Method: 采用多阶段课程强化学习：先在高质量轨迹上微调建立基础能力，然后通过从易到难的奖励调度应用课程RL，缓解结果监督的奖励稀疏问题。

Result: 在三个主流数据集上持续优于先前方法，在GrailQA零样本子集上实现11.1%相对提升，仅使用1/12训练数据。

Conclusion: 仅使用结果监督训练的KnowCoder-A1展现出强大的推理行为，证明了其在知识库问答中的有效智能体推理能力。

Abstract: Knowledge Base Question Answering (KBQA) aims to answer natural-language
questions over a structured Knowledge Base (KB). Recent work improves KBQA by
adopting an agentic reasoning paradigm, in which Large Language Models (LLMs)
iteratively decompose a question, generate its corresponding logical queries,
and interact with the KB to derive the answer. However, these methods typically
fine-tune LLMs on reasoning trajectories synthesized via process supervision,
which offers weak incentives for exploration and thus fails to strengthen the
agentic reasoning ability. In this paper, we propose KnowCoder-A1, an LLM that
can autonomously perform agentic reasoning on KBs to obtain answers. To
incentivize autonomous exploration, KnowCoder-A1 trains the LLM under
outcome-only supervision via a multi-stage curriculum reinforcement learning
with an easy-to-hard curriculum. To establish foundational agentic
capabilities, KnowCoder-A1 first fine-tunes the LLM on a small set of
high-quality trajectories obtained through outcome-based rejection sampling.
Then, to alleviate the reward sparsity inherent in outcome-only supervision, it
applies multi-stage curriculum RL with reward schedules that progress from easy
to hard. Trained with outcome-only supervision, KnowCoder-A1 exhibits powerful
reasoning behaviors and consistently outperforms prior approaches across three
mainstream datasets. Notably, on the zero-shot subset of GrailQA, KnowCoder-A1
achieves up to an 11.1% relative improvement while using only one-twelfth of
the training data, demonstrating strong agentic reasoning capabilities.

</details>


### [68] [RAVR: Reference-Answer-guided Variational Reasoning for Large Language Models](https://arxiv.org/abs/2510.25206)
*Tianqianjin Lin,Xi Zhao,Xingyao Zhang,Rujiao Long,Yi Xu,Zhuoren Jiang,Wenbo Su,Bo Zheng*

Main category: cs.AI

TL;DR: 论文提出RAVR框架，通过答案引导的变分推理来改进LLM的推理能力，解决了在LLM当前能力之外的任务中难以采样高质量推理路径的问题。


<details>
  <summary>Details</summary>
Motivation: 受到认知科学启发：解释'为什么这是答案'比直接回答'什么是答案'更容易，因为前者避免了开放式探索的认知负担，而是进行解释性重构。

Method: 引入RAVR（参考答案引导的变分推理）框架，使用答案条件推理作为仅问题推理的变分替代，通过条件化答案来提升采样推理路径的期望效用。

Result: 在通用和数学领域的实验中，RAVR相比强基线模型表现出持续改进，减少了推理中的犹豫，加强了结论整合，并促进了问题特定策略的使用。

Conclusion: 答案条件化能够将难以处理的问题转化为可学习的问题，显著提升LLM的推理能力，特别是在超出其当前能力的任务中。

Abstract: Reinforcement learning (RL) can refine the reasoning abilities of large
language models (LLMs), but critically depends on a key prerequisite: the LLM
can already generate high-utility reasoning paths with non-negligible
probability. For tasks beyond the LLM's current competence, such reasoning path
can be hard to sample, and learning risks reinforcing familiar but suboptimal
reasoning. We are motivated by the insight from cognitive science that Why is
this the answer is often an easier question than What is the answer, as it
avoids the heavy cognitive load of open-ended exploration, opting instead for
explanatory reconstruction-systematically retracing the reasoning that links a
question to its answer. We show that LLMs can similarly leverage answers to
derive high-quality reasoning paths. We formalize this phenomenon and prove
that conditioning on answer provably increases the expected utility of sampled
reasoning paths, thereby transforming intractable problems into learnable ones.
Building on this insight, we introduce RAVR (Reference-Answer-guided
Variational Reasoning), an end-to-end framework that uses answer-conditioned
reasoning as a variational surrogate for question-only reasoning. Experiments
in both general and math domains demonstrate consistent improvements over
strong baselines. We further analyze the reasoning behavior and find that RAVR
reduces hesitation, strengthens conclusion consolidation, and promotes
problem-specific strategies in reasoning.

</details>


### [69] [From Medical Records to Diagnostic Dialogues: A Clinical-Grounded Approach and Dataset for Psychiatric Comorbidity](https://arxiv.org/abs/2510.25232)
*Tianxi Wan,Jiaming Luo,Siyuan Chen,Kunyao Lan,Jianhua Chen,Haiyang Geng,Mengyue Wu*

Main category: cs.AI

TL;DR: 开发了PsyCoTalk，首个支持共病诊断的大规模对话数据集，包含3000个多轮诊断对话，通过合成电子病历和多智能体框架构建，经精神科医生验证具有临床真实性。


<details>
  <summary>Details</summary>
Motivation: 精神疾病共病在临床上很常见但诊断复杂，需要处理多种共存的障碍，现有数据集难以支持多障碍同时诊断的研究需求。

Method: 集成合成患者电子病历构建和多智能体诊断对话生成，创建502个合成EMR，使用分层状态机和上下文树将临床访谈协议转换为支持130多个诊断状态的框架。

Result: 构建的PsyCoTalk数据集在对话长度、词汇分布和诊断推理策略方面与现实临床记录具有高度结构性和语言保真度，精神科医生确认其真实性和诊断有效性。

Conclusion: PsyCoTalk为精神疾病共病研究提供了宝贵资源，支持在单次对话中开发和评估能够进行多障碍精神病筛查的模型，提高诊断准确性和治疗规划能力。

Abstract: Psychiatric comorbidity is clinically significant yet challenging due to the
complexity of multiple co-occurring disorders. To address this, we develop a
novel approach integrating synthetic patient electronic medical record (EMR)
construction and multi-agent diagnostic dialogue generation. We create 502
synthetic EMRs for common comorbid conditions using a pipeline that ensures
clinical relevance and diversity. Our multi-agent framework transfers the
clinical interview protocol into a hierarchical state machine and context tree,
supporting over 130 diagnostic states while maintaining clinical standards.
Through this rigorous process, we construct PsyCoTalk, the first large-scale
dialogue dataset supporting comorbidity, containing 3,000 multi-turn diagnostic
dialogues validated by psychiatrists. This dataset enhances diagnostic accuracy
and treatment planning, offering a valuable resource for psychiatric
comorbidity research. Compared to real-world clinical transcripts, PsyCoTalk
exhibits high structural and linguistic fidelity in terms of dialogue length,
token distribution, and diagnostic reasoning strategies. Licensed psychiatrists
confirm the realism and diagnostic validity of the dialogues. This dataset
enables the development and evaluation of models capable of multi-disorder
psychiatric screening in a single conversational pass.

</details>


### [70] [GAP: Graph-Based Agent Planning with Parallel Tool Use and Reinforcement Learning](https://arxiv.org/abs/2510.25320)
*Jiaqi Wu,Qinlao Zhao,Zefeng Chen,Kai Qin,Yifei Zhao,Xueqian Wang,Yuhang Yao*

Main category: cs.AI

TL;DR: GAP是一个基于图规划的智能体框架，通过建模任务间依赖关系实现并行和串行工具执行，显著提升多步推理任务的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的自主代理（如ReAct）采用顺序推理和执行，无法利用独立子任务间的并行性，导致工具使用效率低下和多步推理性能不佳。

Method: 训练智能体基础模型将复杂任务分解为依赖感知的子任务图，自主确定哪些工具可以并行执行，哪些必须遵循顺序依赖。采用两阶段训练策略：在高质量图规划数据集上进行监督微调，然后在策略性采样的查询上进行基于正确性的强化学习。

Result: 在MHQA数据集上的实验结果表明，GAP显著优于传统ReAct基线，特别是在多步检索任务上，同时通过智能并行化实现了工具调用效率的显著提升。

Conclusion: 图基规划框架能够有效解决顺序推理瓶颈，通过依赖感知的任务编排实现高效的并行工具执行，为复杂任务解决提供了新的范式。

Abstract: Autonomous agents powered by large language models (LLMs) have shown
impressive capabilities in tool manipulation for complex task-solving. However,
existing paradigms such as ReAct rely on sequential reasoning and execution,
failing to exploit the inherent parallelism among independent sub-tasks. This
sequential bottleneck leads to inefficient tool utilization and suboptimal
performance in multi-step reasoning scenarios. We introduce Graph-based Agent
Planning (GAP), a novel framework that explicitly models inter-task
dependencies through graph-based planning to enable adaptive parallel and
serial tool execution. Our approach trains agent foundation models to decompose
complex tasks into dependency-aware sub-task graphs, autonomously determining
which tools can be executed in parallel and which must follow sequential
dependencies. This dependency-aware orchestration achieves substantial
improvements in both execution efficiency and task accuracy. To train GAP, we
construct a high-quality dataset of graph-based planning traces derived from
the Multi-Hop Question Answering (MHQA) benchmark. We employ a two-stage
training strategy: supervised fine-tuning (SFT) on the curated dataset,
followed by reinforcement learning (RL) with a correctness-based reward
function on strategically sampled queries where tool-based reasoning provides
maximum value. Experimental results on MHQA datasets demonstrate that GAP
significantly outperforms traditional ReAct baselines, particularly on
multi-step retrieval tasks, while achieving dramatic improvements in tool
invocation efficiency through intelligent parallelization. The project page is
available at: https://github.com/WJQ7777/Graph-Agent-Planning.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [71] [ZK-SenseLM: Verifiable Large-Model Wireless Sensing with Selective Abstention and Zero-Knowledge Attestation](https://arxiv.org/abs/2510.25677)
*Hasan Akgul,Mari Eplik,Javier Rojas,Aina Binti Abdullah,Pieter van der Merwe*

Main category: cs.CR

TL;DR: ZK-SenseLM是一个安全可审计的无线感知框架，结合大模型编码器与策略决策层，使用零知识证明来验证推理过程，支持多种无线传感技术。


<details>
  <summary>Details</summary>
Motivation: 为了解决无线感知系统中推理过程缺乏透明度和可验证性的问题，确保在分布偏移下的安全操作，并提供防篡改和重放攻击的保护。

Method: 采用掩码频谱预训练和相位一致性正则化的大模型编码器，结合策略决策层和选择性弃权机制，实现四阶段零知识证明流程，支持微批次证明和网关卸载。

Result: 在多种感知任务中提高了宏观F1分数和校准性能，在扰动下获得良好的覆盖风险曲线，能够以紧凑证明和快速验证拒绝篡改和重放攻击。

Conclusion: ZK-SenseLM提供了一个安全、可验证的无线感知框架，能够在保持隐私的同时确保推理过程的可靠性和可审计性。

Abstract: ZK-SenseLM is a secure and auditable wireless sensing framework that pairs a
large-model encoder for Wi-Fi channel state information (and optionally mmWave
radar or RFID) with a policy-grounded decision layer and end-to-end
zero-knowledge proofs of inference. The encoder uses masked spectral
pretraining with phase-consistency regularization, plus a light cross-modal
alignment that ties RF features to compact, human-interpretable policy tokens.
To reduce unsafe actions under distribution shift, we add a calibrated
selective-abstention head; the chosen risk-coverage operating point is
registered and bound into the proof. We implement a four-stage proving
pipeline: (C1) feature sanity and commitment, (C2) threshold and version
binding, (C3) time-window binding, and (C4) PLONK-style proofs that the
quantized network, given the committed window, produced the logged action and
confidence. Micro-batched proving amortizes cost across adjacent windows, and a
gateway option offloads proofs from low-power devices. The system integrates
with differentially private federated learning and on-device personalization
without weakening verifiability: model hashes and the registered threshold are
part of each public statement. Across activity, presence or intrusion,
respiratory proxy, and RF fingerprinting tasks, ZK-SenseLM improves macro-F1
and calibration, yields favorable coverage-risk curves under perturbations, and
rejects tamper and replay with compact proofs and fast verification.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [72] [StorageXTuner: An LLM Agent-Driven Automatic Tuning Framework for Heterogeneous Storage Systems](https://arxiv.org/abs/2510.25017)
*Qi Lin,Zhenyu Zhang,Viraj Thakkar,Zhenjie Sun,Mai Zheng,Zhichao Cao*

Main category: cs.DB

TL;DR: StorageXTuner是一个基于LLM代理的存储系统自动调优框架，通过四个分离的代理（执行器、提取器、搜索器、反射器）和洞察驱动的树搜索，实现了跨异构存储引擎的高效参数调优。


<details>
  <summary>Details</summary>
Motivation: 存储系统自动配置困难：参数空间大，不同工作负载、部署和版本条件各异。现有启发式和ML调优器通常系统特定，需要手动粘合，且在变化时性能下降。

Method: 提出StorageXTuner框架，包含四个代理：执行器（沙盒基准测试）、提取器（性能摘要）、搜索器（洞察引导的配置探索）、反射器（洞察生成和管理）。采用洞察驱动的树搜索和分层内存，结合轻量级检查器防止不安全操作。

Result: 在RocksDB、LevelDB、CacheLib和MySQL InnoDB上使用YCSB、MixGraph和TPC-H/C进行评估。相比默认设置和ELMo-Tune，吞吐量最高提升575%和111%，p99延迟最多降低88%和56%，收敛所需试验次数更少。

Conclusion: StorageXTuner通过分离关注点和洞察驱动的搜索策略，实现了跨异构存储系统的高效自动调优，显著提升了性能和收敛效率。

Abstract: Automatically configuring storage systems is hard: parameter spaces are large
and conditions vary across workloads, deployments, and versions. Heuristic and
ML tuners are often system specific, require manual glue, and degrade under
changes. Recent LLM-based approaches help but usually treat tuning as a
single-shot, system-specific task, which limits cross-system reuse, constrains
exploration, and weakens validation. We present StorageXTuner, an LLM
agent-driven auto-tuning framework for heterogeneous storage engines.
StorageXTuner separates concerns across four agents - Executor (sandboxed
benchmarking), Extractor (performance digest), Searcher (insight-guided
configuration exploration), and Reflector (insight generation and management).
The design couples an insight-driven tree search with layered memory that
promotes empirically validated insights and employs lightweight checkers to
guard against unsafe actions. We implement a prototype and evaluate it on
RocksDB, LevelDB, CacheLib, and MySQL InnoDB with YCSB, MixGraph, and TPC-H/C.
Relative to out-of-the-box settings and to ELMo-Tune, StorageXTuner reaches up
to 575% and 111% higher throughput, reduces p99 latency by as much as 88% and
56%, and converges with fewer trials.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [73] [AmarDoctor: An AI-Driven, Multilingual, Voice-Interactive Digital Health Application for Primary Care Triage and Patient Management to Bridge the Digital Health Divide for Bengali Speakers](https://arxiv.org/abs/2510.24724)
*Nazmun Nahar,Ritesh Harshad Ruparel,Shariar Kabir,Sumaiya Tasnia Khan,Shyamasree Saha,Mamunur Rashid*

Main category: cs.HC

TL;DR: AmarDoctor是一个多语言语音交互数字健康应用，专门为孟加拉语使用者设计，提供患者分诊和AI驱动的临床决策支持，填补了数字医疗服务的语言空白。


<details>
  <summary>Details</summary>
Motivation: 现有数字健康平台主要服务于欧洲人口和语言，孟加拉语使用者缺乏相应的数字医疗服务。AmarDoctor旨在解决这一服务缺口，通过语音交互克服数字素养障碍。

Method: 采用双界面系统，患者模块使用自适应提问算法评估症状并推荐专科医生，集成语音交互AI助手；临床医生界面整合AI决策支持，生成结构化诊断和治疗建议。

Result: 在185个临床案例验证中，AmarDoctor的top-1诊断准确率达到81.08%（医生平均50.27%），专科推荐准确率91.35%（医生平均62.6%）。

Conclusion: AmarDoctor有效解决了孟加拉语使用者的数字医疗服务缺口，在诊断准确性和专科推荐方面显著优于人类医生，展示了AI在改善医疗服务可及性方面的潜力。

Abstract: This study presents AmarDoctor, a multilingual voice-interactive digital
health app designed to provide comprehensive patient triage and AI-driven
clinical decision support for Bengali speakers, a population largely
underserved in access to digital healthcare. AmarDoctor adopts a data-driven
approach to strengthen primary care delivery and enable personalized health
management. While platforms such as AdaHealth, WebMD, Symptomate, and K-Health
have become popular in recent years, they mainly serve European demographics
and languages. AmarDoctor addresses this gap with a dual-interface system for
both patients and healthcare providers, supporting three major Bengali
dialects. At its core, the patient module uses an adaptive questioning
algorithm to assess symptoms and guide users toward the appropriate specialist.
To overcome digital literacy barriers, it integrates a voice-interactive AI
assistant that navigates users through the app services. Complementing this,
the clinician-facing interface incorporates AI-powered decision support that
enhances workflow efficiency by generating structured provisional diagnoses and
treatment recommendations. These outputs inform key services such as
e-prescriptions, video consultations, and medical record management. To
validate clinical accuracy, the system was evaluated against a gold-standard
set of 185 clinical vignettes developed by experienced physicians.
Effectiveness was further assessed by comparing AmarDoctor performance with
five independent physicians using the same vignette set. Results showed
AmarDoctor achieved a top-1 diagnostic precision of 81.08 percent (versus
physicians average of 50.27 percent) and a top specialty recommendation
precision of 91.35 percent (versus physicians average of 62.6 percent).

</details>


### [74] [Beyond Models: A Framework for Contextual and Cultural Intelligence in African AI Deployment](https://arxiv.org/abs/2510.24729)
*Qness Ndlovu*

Main category: cs.HC

TL;DR: 本文提出CCI框架，通过文化智能设计使AI系统能处理文化含义而不仅是数据模式，在非洲市场验证了基于WhatsApp的AI交互优于传统界面，实现89%用户偏好和显著用户增长。


<details>
  <summary>Details</summary>
Motivation: 全球AI发展过度关注模型性能和计算规模，但在非洲市场部署需要完全不同的架构决策，需要能够处理文化含义而不仅是数据模式的AI系统。

Method: 采用设计科学方法，通过为散居社区服务的跨境购物平台验证CCI框架，包含基础设施智能、文化智能和商业智能三大技术支柱。

Result: 89%用户偏好WhatsApp AI交互，6周内获得536名WhatsApp用户和3,938次对话，89%为家庭导向商务模式，文化提示工程有效处理文化情境化查询。

Conclusion: CCI框架挑战硅谷设计正统，为资源受限市场提供可操作的公平AI部署框架，既有理论创新又有可复现实施模式。

Abstract: While global AI development prioritizes model performance and computational
scale, meaningful deployment in African markets requires fundamentally
different architectural decisions. This paper introduces Contextual and
Cultural Intelligence (CCI) -- a systematic framework enabling AI systems to
process cultural meaning, not just data patterns, through locally relevant,
emotionally intelligent, and economically inclusive design. Using design
science methodology, we validate CCI through a production AI-native
cross-border shopping platform serving diaspora communities. Key empirical
findings: 89% of users prefer WhatsApp-based AI interaction over traditional
web interfaces (n=602, chi-square=365.8, p<0.001), achieving 536 WhatsApp users
and 3,938 total conversations across 602 unique users in just 6 weeks, and
culturally informed prompt engineering demonstrates sophisticated understanding
of culturally contextualized queries, with 89% family-focused commerce patterns
and natural code-switching acceptance. The CCI framework operationalizes three
technical pillars: Infrastructure Intelligence (mobile-first, resilient
architectures), Cultural Intelligence (multilingual NLP with social context
awareness), and Commercial Intelligence (trust-based conversational commerce).
This work contributes both theoretical innovation and reproducible
implementation patterns, challenging Silicon Valley design orthodoxies while
providing actionable frameworks for equitable AI deployment across
resource-constrained markets.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [75] [Lost in Phonation: Voice Quality Variation as an Evaluation Dimension for Speech Foundation Models](https://arxiv.org/abs/2510.25577)
*Harm Lameris,Shree Harsha Bokkahalli Satish,Joakim Gustafson,Éva Székely*

Main category: eess.AS

TL;DR: 本文研究了语音基础模型对声音质量（如气声和嘎吱声）等副语言特征的敏感性，通过开放式生成任务和语音情感识别来评估模型行为的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有语音理解基准主要依赖多项选择题格式，难以可靠捕捉副语言特征对模型行为的细微影响，特别是声音质量这一未被充分探索的维度。

Method: 通过开放式生成任务和语音情感识别来探测语音基础模型，并引入包含声音质量合成修改的平行数据集来评估模型对气声和嘎吱声的响应。

Result: 研究首次检验了语音基础模型对这些特定非词汇性语音感知方面的敏感性。

Conclusion: 这项工作为理解语音基础模型如何处理声音质量等副语言特征提供了首个系统性分析。

Abstract: Recent advances in speech foundation models (SFMs) have enabled the direct
processing of spoken language from raw audio, bypassing intermediate textual
representations. This capability allows SFMs to be exposed to, and potentially
respond to, rich paralinguistic variations embedded in the input speech signal.
One under-explored dimension of paralinguistic variation is voice quality,
encompassing phonation types such as creaky and breathy voice. These phonation
types are known to influence how listeners infer affective state, stance and
social meaning in speech. Existing benchmarks for speech understanding largely
rely on multiple-choice question answering (MCQA) formats, which are prone to
failure and therefore unreliable in capturing the nuanced ways paralinguistic
features influence model behaviour. In this paper, we probe SFMs through
open-ended generation tasks and speech emotion recognition, evaluating whether
model behaviours are consistent across different phonation inputs. We introduce
a new parallel dataset featuring synthesized modifications to voice quality,
designed to evaluate SFM responses to creaky and breathy voice. Our work
provides the first examination of SFM sensitivity to these particular
non-lexical aspects of speech perception.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [76] [Process-Level Trajectory Evaluation for Environment Configuration in Software Engineering Agents](https://arxiv.org/abs/2510.25694)
*Jiayi Kuang,Yinghui Li,Xin Zhang,Yangning Li,Di Yin,Xing Sun,Ying Shen,Philip S. Yu*

Main category: cs.SE

TL;DR: Enconda-bench是一个环境配置诊断基准，通过过程级轨迹评估来诊断LLM代理在软件工程环境配置中的细粒度能力，包括规划、错误诊断、反馈修复等环节。


<details>
  <summary>Details</summary>
Motivation: 现有基准仅评估端到端的构建/测试成功率，无法揭示代理在环境配置过程中成功或失败的具体原因和位置，限制了软件工程代理能力的深入分析。

Method: 通过注入真实的README错误自动构建任务实例，在Docker中进行验证，结合过程级分析和端到端可执行性来评估代理的细粒度能力。

Result: 评估显示，虽然代理能够定位错误，但难以将反馈转化为有效修正，限制了端到端性能。

Conclusion: Enconda-bench是首个为环境配置提供过程级内部能力评估的框架，为改进软件工程代理提供了可操作的见解。

Abstract: Large language model-based agents show promise for software engineering, but
environment configuration remains a bottleneck due to heavy manual effort and
scarce large-scale, high-quality datasets. Existing benchmarks assess only
end-to-end build/test success, obscuring where and why agents succeed or fail.
We introduce the Environment Configuration Diagnosis Benchmark, Enconda-bench,
which provides process-level trajectory assessment of fine-grained agent
capabilities during environment setup-planning, perception-driven error
diagnosis, feedback-driven repair, and action to execute final environment
configuration. Our task instances are automatically constructed by injecting
realistic README errors and are validated in Docker for scalable, high-quality
evaluation. Enconda-bench combines process-level analysis with end-to-end
executability to enable capability assessments beyond aggregate success rates.
Evaluations across state-of-the-art LLMs and agent frameworks show that while
agents can localize errors, they struggle to translate feedback into effective
corrections, limiting end-to-end performance. To our knowledge, Enconda-bench
is the first framework to provide process-level internal capability assessment
for environment configuration, offering actionable insights for improving
software engineering agents.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [77] [Fortytwo: Swarm Inference with Peer-Ranked Consensus](https://arxiv.org/abs/2510.24801)
*Vladyslav Larin,Ihor Naumenko,Aleksei Ivashov,Ivan Nikitin,Alexander Firsov*

Main category: cs.LG

TL;DR: Fortytwo协议利用群体智能和分布式成对排序共识，在AI推理中实现优于多数投票的性能，通过声誉加权共识提升推理质量并抵抗女巫攻击。


<details>
  <summary>Details</summary>
Motivation: 随着集中式AI面临计算瓶颈和大型训练收益递减，需要可水平扩展的推理层来满足需求，通过分布式协作实现高质量推理。

Method: 采用群体推理方法，使用成对排序和Bradley-Terry风格聚合模型，结合链上声誉系统，节点需通过能力证明和声誉质押参与排序轮次。

Result: 在GPQA Diamond上达到85.90%准确率，比多数投票提升17.21个百分点(+25.1%)，在六个基准测试中表现优异，对抗性提示注入仅降低0.12%。

Conclusion: 为去中心化AI系统奠定了基础，通过集体智能实现高质量推理的民主化访问，同时保持可靠性和安全性。

Abstract: As centralized AI hits compute ceilings and diminishing returns from
ever-larger training runs, meeting demand requires an inference layer that
scales horizontally in both capacity and capability. We present Fortytwo, a
novel protocol that leverages swarm intelligence principles and distributed
pairwise ranking consensus to achieve superior performance in AI inference. Our
approach reimagines collaboration among AI nodes using swarm inference: a
peer-ranked, reputation-weighted consensus across heterogeneous models that
surfaces the highest-quality responses. Using pairwise ranking with a custom
Bradley-Terry-style aggregation model, we demonstrate that swarm inference
substantially outperforms majority voting, achieving 85.90% on GPQA Diamond
versus 68.69% for majority voting with the same model set - an improvement of
+17.21 percentage points (approximately +25.1% relative). The protocol
incorporates on-chain reputation so node influence adapts to demonstrated
accuracy over time, yielding a meritocratic consensus that filters low-quality
or malicious participants. To resist Sybil attacks, Fortytwo employs
proof-of-capability in its consensus: nodes must successfully complete
calibration/test requests and stake reputation to enter ranking rounds, making
multi-identity attacks economically unattractive while preserving openness.
Across six challenging benchmarks, including GPQA Diamond, LiveCodeBench, and
AIME, our evaluation indicates higher accuracy and strong resilience to
adversarial and noisy free-form prompting (e.g., prompt-injection degradation
of only 0.12% versus 6.20% for a monolithic single-model baseline), while
retaining practical deployability. Together, these results establish a
foundation for decentralized AI systems - democratizing access to high-quality
inference through collective intelligence without sacrificing reliability or
security.

</details>


### [78] [Finding Culture-Sensitive Neurons in Vision-Language Models](https://arxiv.org/abs/2510.24942)
*Xiutian Zhao,Rochelle Choenni,Rohit Saxena,Ivan Titov*

Main category: cs.LG

TL;DR: 该论文研究了视觉语言模型中存在的文化敏感神经元，这些神经元对特定文化背景的输入表现出偏好敏感性。通过CVQA基准测试，作者识别了文化选择性神经元，并通过消融实验证明了它们对相应文化问题的重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型表现出色，但在处理文化相关输入时仍存在困难。研究者希望了解模型如何处理文化背景信息，特别是是否存在对特定文化敏感的神经元。

Method: 使用CVQA基准测试识别文化选择性神经元，通过消融实验验证其重要性。提出了新的基于边际的选择器——对比激活选择(CAS)，并与现有的基于概率和熵的方法进行比较。进行了分层分析以确定这些神经元的分布位置。

Result: 在三个视觉语言模型和25个文化群体的实验中，发现了文化敏感神经元的存在，消融这些神经元会显著影响对应文化问题的性能，而对其他文化影响较小。CAS方法在识别文化敏感神经元方面优于现有方法。分层分析显示这些神经元倾向于聚集在某些解码器层中。

Conclusion: 研究揭示了多模态表征的内部组织结构，证明了文化敏感神经元的存在及其在模型处理文化相关信息中的重要作用，为理解视觉语言模型的文化处理机制提供了新的视角。

Abstract: Despite their impressive performance, vision-language models (VLMs) still
struggle on culturally situated inputs. To understand how VLMs process
culturally grounded information, we study the presence of culture-sensitive
neurons, i.e. neurons whose activations show preferential sensitivity to inputs
associated with particular cultural contexts. We examine whether such neurons
are important for culturally diverse visual question answering and where they
are located. Using the CVQA benchmark, we identify neurons of culture
selectivity and perform causal tests by deactivating the neurons flagged by
different identification methods. Experiments on three VLMs across 25 cultural
groups demonstrate the existence of neurons whose ablation disproportionately
harms performance on questions about the corresponding cultures, while having
minimal effects on others. Moreover, we propose a new margin-based selector -
Contrastive Activation Selection (CAS), and show that it outperforms existing
probability- and entropy-based methods in identifying culture-sensitive
neurons. Finally, our layer-wise analyses reveals that such neurons tend to
cluster in certain decoder layers. Overall, our findings shed new light on the
internal organization of multimodal representations.

</details>


### [79] [Sequences of Logits Reveal the Low Rank Structure of Language Models](https://arxiv.org/abs/2510.24966)
*Noah Golowich,Allen Liu,Abhishek Shetty*

Main category: cs.LG

TL;DR: 该论文研究了大型语言模型的低维结构，发现其logits矩阵具有低秩特性，并利用这一特性实现通过无关提示的线性组合来生成目标响应。


<details>
  <summary>Details</summary>
Motivation: 理解大型语言模型的固有低维结构是一个重要问题，作者希望从模型无关的角度研究语言模型作为序列概率模型的低维特性。

Method: 将语言模型视为序列概率模型，实证分析其logits矩阵的低秩结构，并利用线性组合方法进行生成实验。

Result: 实证表明多种现代语言模型都表现出低秩结构，可以利用无关甚至无意义提示的线性组合来生成目标提示的响应。

Conclusion: 语言模型的低秩结构提供了一个简单的通用抽象，其理论预测与实验结果一致，并给出了可证明的学习保证。

Abstract: A major problem in the study of large language models is to understand their
inherent low-dimensional structure. We introduce an approach to study the
low-dimensional structure of language models at a model-agnostic level: as
sequential probabilistic models. We first empirically demonstrate that a wide
range of modern language models exhibit low-rank structure: in particular,
matrices built from the model's logits for varying sets of prompts and
responses have low approximate rank. We then show that this low-rank structure
can be leveraged for generation -- in particular, we can generate a response to
a target prompt using a linear combination of the model's outputs on unrelated,
or even nonsensical prompts.
  On the theoretical front, we observe that studying the approximate rank of
language models in the sense discussed above yields a simple universal
abstraction whose theoretical predictions parallel our experiments. We then
analyze the representation power of the abstraction and give provable learning
guarantees.

</details>


### [80] [Hybrid Quantum-Classical Recurrent Neural Networks](https://arxiv.org/abs/2510.25557)
*Wenduan Xu*

Main category: cs.LG

TL;DR: 提出了一种混合量子-经典循环神经网络架构，其中整个循环核心由参数化量子电路实现，由经典前馈网络控制，在多个序列学习任务中展现出与强经典基线竞争的性能。


<details>
  <summary>Details</summary>
Motivation: 构建一个物理一致的量子循环神经网络，结合量子系统的高容量记忆能力和经典系统的非线性控制，以探索量子计算在序列学习任务中的潜力。

Method: 使用参数化量子电路作为循环核心，量子态作为隐藏状态，通过中间电路测量获取部分观测，经典前馈网络提供非线性控制并参数化量子电路。

Result: 在情感分析、MNIST、语言建模等任务中，使用最多14个量子比特的模拟实验表明，该模型在广泛的序列学习任务中与强经典基线竞争。

Conclusion: 这是第一个基于量子操作的模型，在广泛的序列学习任务中实现与强经典基线竞争的性能，展示了量子循环神经网络的实际可行性。

Abstract: We present a hybrid quantum-classical recurrent neural network (QRNN)
architecture in which the entire recurrent core is realized as a parametrized
quantum circuit (PQC) controlled by a classical feedforward network. The hidden
state is the quantum state of an $n$-qubit PQC, residing in an exponentially
large Hilbert space $\mathbb{C}^{2^n}$. The PQC is unitary by construction,
making the hidden-state evolution norm-preserving without external constraints.
At each timestep, mid-circuit readouts are combined with the input embedding
and processed by the feedforward network, which provides explicit classical
nonlinearity. The outputs parametrize the PQC, which updates the hidden state
via unitary dynamics. The QRNN is compact and physically consistent, and it
unifies (i) unitary recurrence as a high-capacity memory, (ii) partial
observation via mid-circuit measurements, and (iii) nonlinear classical control
for input-conditioned parametrization. We evaluate the model in simulation with
up to 14 qubits on sentiment analysis, MNIST, permuted MNIST, copying memory,
and language modeling, adopting projective measurements as a limiting case to
obtain mid-circuit readouts while maintaining a coherent recurrent quantum
memory. We further devise a soft attention mechanism over the mid-circuit
readouts in a sequence-to-sequence model and show its effectiveness for machine
translation. To our knowledge, this is the first model (RNN or otherwise)
grounded in quantum operations to achieve competitive performance against
strong classical baselines across a broad class of sequence-learning tasks.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [81] [Conflict Adaptation in Vision-Language Models](https://arxiv.org/abs/2510.24804)
*Xiaoyang Hu*

Main category: cs.CV

TL;DR: 研究发现大多数视觉语言模型(VLMs)表现出类似人类的冲突适应行为，通过稀疏自编码器在InternVL模型中识别出任务相关的超节点，发现文本和颜色处理存在部分重叠，并分离出一个冲突调节超节点。


<details>
  <summary>Details</summary>
Motivation: 研究人类认知控制中的冲突适应现象(高冲突试次后表现改善)，探索AI模型是否也表现出类似行为，以理解认知控制机制的实现方式。

Method: 使用序列Stroop任务测试13个视觉语言模型，采用稀疏自编码器(SAEs)分析InternVL 3.5 4B模型，识别任务相关超节点并进行消融实验。

Result: 12/13的VLMs表现出冲突适应行为；在InternVL中发现文本和颜色处理的超节点部分重叠，其相对大小反映了人类阅读和颜色命名的自动性不对称；分离出层24-25的冲突调节超节点，其消融显著增加Stroop错误。

Conclusion: VLMs能够表现出类似人类的冲突适应行为，模型内部存在专门处理认知冲突的神经结构，这为理解AI中的认知控制机制提供了新视角。

Abstract: A signature of human cognitive control is conflict adaptation: improved
performance on a high-conflict trial following another high-conflict trial.
This phenomenon offers an account for how cognitive control, a scarce resource,
is recruited. Using a sequential Stroop task, we find that 12 of 13
vision-language models (VLMs) tested exhibit behavior consistent with conflict
adaptation, with the lone exception likely reflecting a ceiling effect. To
understand the representational basis of this behavior, we use sparse
autoencoders (SAEs) to identify task-relevant supernodes in InternVL 3.5 4B.
Partially overlapping supernodes emerge for text and color in both early and
late layers, and their relative sizes mirror the automaticity asymmetry between
reading and color naming in humans. We further isolate a conflict-modulated
supernode in layers 24-25 whose ablation significantly increases Stroop errors
while minimally affecting congruent trials.

</details>


### [82] [More than a Moment: Towards Coherent Sequences of Audio Descriptions](https://arxiv.org/abs/2510.25440)
*Eshika Khandelwal,Junyu Xie,Tengda Han,Max Bain,Arsha Nagrani,Andrew Zisserman,Gül Varol,Makarand Tapaswi*

Main category: cs.CV

TL;DR: 提出CoherentAD方法，通过生成多个候选描述并进行自回归选择，创建连贯的音频描述序列，解决了现有方法生成孤立、重复描述的问题。


<details>
  <summary>Details</summary>
Motivation: 现有自动音频描述方法独立生成每个描述，导致序列不连贯、重复，影响视障听众对视频内容的理解和可视化。

Method: 训练无关的方法，首先生成每个时间间隔的多个候选描述，然后通过自回归选择在序列中形成连贯叙述。

Result: 方法生成的音频描述序列在叙事理解方面表现更好，优于依赖独立生成的先前方法。

Conclusion: CoherentAD能够产生连贯的音频描述序列，提升叙事理解效果，并通过新指标StoryRecall和重复度指标进行综合评估。

Abstract: Audio Descriptions (ADs) convey essential on-screen information, allowing
visually impaired audiences to follow videos. To be effective, ADs must form a
coherent sequence that helps listeners to visualise the unfolding scene, rather
than describing isolated moments. However, most automatic methods generate each
AD independently, often resulting in repetitive, incoherent descriptions. To
address this, we propose a training-free method, CoherentAD, that first
generates multiple candidate descriptions for each AD time interval, and then
performs auto-regressive selection across the sequence to form a coherent and
informative narrative. To evaluate AD sequences holistically, we introduce a
sequence-level metric, StoryRecall, which measures how well the predicted ADs
convey the ground truth narrative, alongside repetition metrics that capture
the redundancy across consecutive AD outputs. Our method produces coherent AD
sequences with enhanced narrative understanding, outperforming prior approaches
that rely on independent generations.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [83] [The Epistemic Suite: A Post-Foundational Diagnostic Methodology for Assessing AI Knowledge Claims](https://arxiv.org/abs/2510.24721)
*Matthew Kelly*

Main category: cs.CY

TL;DR: 提出Epistemic Suite诊断方法，通过20个诊断透镜揭示AI输出的认知条件，包括置信度洗白、叙事压缩等模式，产生可检查的FACS包作为中间层。


<details>
  <summary>Details</summary>
Motivation: LLMs生成流畅但可能误导的文本，用户易将模拟连贯性误认为真实理解，需要揭示AI输出的认知生产条件。

Method: 基于三个设计原则：诊断生产先于评估主张、偏好诊断牵引而非基础解决、嵌入反思性作为结构要求。包含Epistemic Triage协议和元治理层。

Result: 将语言模型转向诊断立场，产生可检查的FACS包（标志、注释、矛盾地图、暂停日志），实现认知暂停作为断路器。

Conclusion: 该方法作为外部脚手架运行，保持可废弃性和拒绝作为保障，区分性能与理解，实现负责任审议同时保持认知谦逊。

Abstract: Large Language Models (LLMs) generate fluent, plausible text that can mislead
users into mistaking simulated coherence for genuine understanding. This paper
introduces the Epistemic Suite, a post-foundational diagnostic methodology for
surfacing the epistemic conditions under which AI outputs are produced and
received. Rather than determining truth or falsity, the Suite operates through
twenty diagnostic lenses, applied by practitioners as context warrants, to
reveal patterns such as confidence laundering, narrative compression, displaced
authority, and temporal drift. It is grounded in three design principles:
diagnosing production before evaluating claims, preferring diagnostic traction
over foundational settlement, and embedding reflexivity as a structural
requirement rather than an ethical ornament. When enacted, the Suite shifts
language models into a diagnostic stance, producing inspectable
artifacts-flags, annotations, contradiction maps, and suspension logs (the FACS
bundle)-that create an intermediary layer between AI output and human judgment.
A key innovation is epistemic suspension, a practitioner-enacted circuit
breaker that halts continuation when warrant is exceeded, with resumption based
on judgment rather than rule. The methodology also includes an Epistemic Triage
Protocol and a Meta-Governance Layer to manage proportionality and link
activation to relational accountability, consent, historical context, and
pluralism safeguards. Unlike internalist approaches that embed alignment into
model architectures (e.g., RLHF or epistemic-integrity proposals), the Suite
operates externally as scaffolding, preserving expendability and refusal as
safeguards rather than failures. It preserves the distinction between
performance and understanding, enabling accountable deliberation while
maintaining epistemic modesty.

</details>


### [84] [Topic-aware Large Language Models for Summarizing the Lived Healthcare Experiences Described in Health Stories](https://arxiv.org/abs/2510.24765)
*Maneesh Bilalpur,Megan Hamm,Young Ji Lee,Natasha Norman,Kathleen M. McTigue,Yanshan Wang*

Main category: cs.CY

TL;DR: 使用LDA和LLM对非裔美国人医疗经历故事进行主题识别和分层摘要，以识别影响医疗结果的因素和干预途径。


<details>
  <summary>Details</summary>
Motivation: 通过叙事分析识别医疗结果差距的潜在因素，利用故事讲述的沟通力量来高效处理非结构化叙事。

Method: 使用LDA技术识别50个故事中的主题，采用开源LLM进行分层摘要，生成主题摘要并由GPT4评估质量。

Result: 识别出26个相关主题，GPT4评估显示摘要无捏造、高度准确、全面且有用，与专家评估具有中到高度一致性。

Conclusion: 该方法可帮助研究人员从非结构化叙事中高效识别潜在因素和干预措施，为健康研究和临床改进提供新途径。

Abstract: Storytelling is a powerful form of communication and may provide insights
into factors contributing to gaps in healthcare outcomes. To determine whether
Large Language Models (LLMs) can identify potential underlying factors and
avenues for intervention, we performed topic-aware hierarchical summarization
of narratives from African American (AA) storytellers. Fifty transcribed
stories of AA experiences were used to identify topics in their experience
using the Latent Dirichlet Allocation (LDA) technique. Stories about a given
topic were summarized using an open-source LLM-based hierarchical summarization
approach. Topic summaries were generated by summarizing across story summaries
for each story that addressed a given topic. Generated topic summaries were
rated for fabrication, accuracy, comprehensiveness, and usefulness by the GPT4
model, and the model's reliability was validated against the original story
summaries by two domain experts. 26 topics were identified in the fifty AA
stories. The GPT4 ratings suggest that topic summaries were free from
fabrication, highly accurate, comprehensive, and useful. The reliability of GPT
ratings compared to expert assessments showed moderate to high agreement. Our
approach identified AA experience-relevant topics such as health behaviors,
interactions with medical team members, caregiving and symptom management,
among others. Such insights could help researchers identify potential factors
and interventions by learning from unstructured narratives in an efficient
manner-leveraging the communicative power of storytelling. The use of LDA and
LLMs to identify and summarize the experience of AA individuals suggests a
variety of possible avenues for health research and possible clinical
improvements to support patients and caregivers, thereby ultimately improving
health outcomes.

</details>


### [85] [PANORAMA: A Dataset and Benchmarks Capturing Decision Trails and Rationales in Patent Examination](https://arxiv.org/abs/2510.24774)
*Hyunseung Lim,Sooyohn Nam,Sungmin Na,Ji Yong Cho,June Yong Yang,Hyungyu Shin,Yoonjoo Lee,Juho Kim,Moontae Lee,Hwajung Hong*

Main category: cs.CY

TL;DR: 构建了PANORAMA数据集，包含8,143条美国专利审查记录，完整保留了决策路径，用于评估LLM在专利审查各步骤中的能力。研究发现LLM在检索相关现有技术和定位相关段落方面相对有效，但在评估专利权利要求的新颖性和非显而易见性方面存在困难。


<details>
  <summary>Details</summary>
Motivation: 现有NLP研究将专利审查视为预测任务，使用相似性度量或分类器等高级代理，但忽略了审查员必须进行的逐步评估以及办公室行动文件中提供的决策理由，这使得难以衡量当前技术在专利审查过程中的状态。

Method: 构建PANORAMA数据集，包含原始申请、所有引用参考文献、非最终驳回和授权通知，将审查路径分解为模拟专利专业人员审查过程的顺序基准，允许研究人员检查LLM在每个步骤中的能力。

Result: LLM在检索相关现有技术和定位相关段落方面相对有效，但在评估专利权利要求的新颖性和非显而易见性方面存在困难。

Conclusion: 推进专利领域的NLP（包括LLM）需要对现实世界专利审查有更深入的理解。

Abstract: Patent examination remains an ongoing challenge in the NLP literature even
after the advent of large language models (LLMs), as it requires an extensive
yet nuanced human judgment on whether a submitted claim meets the statutory
standards of novelty and non-obviousness against previously granted claims --
prior art -- in expert domains. Previous NLP studies have approached this
challenge as a prediction task (e.g., forecasting grant outcomes) with
high-level proxies such as similarity metrics or classifiers trained on
historical labels. However, this approach often overlooks the step-by-step
evaluations that examiners must make with profound information, including
rationales for the decisions provided in office actions documents, which also
makes it harder to measure the current state of techniques in patent review
processes. To fill this gap, we construct PANORAMA, a dataset of 8,143 U.S.
patent examination records that preserves the full decision trails, including
original applications, all cited references, Non-Final Rejections, and Notices
of Allowance. Also, PANORAMA decomposes the trails into sequential benchmarks
that emulate patent professionals' patent review processes and allow
researchers to examine large language models' capabilities at each step of
them. Our findings indicate that, although LLMs are relatively effective at
retrieving relevant prior art and pinpointing the pertinent paragraphs, they
struggle to assess the novelty and non-obviousness of patent claims. We discuss
these results and argue that advancing NLP, including LLMs, in the patent
domain requires a deeper understanding of real-world patent examination. Our
dataset is openly available at
https://huggingface.co/datasets/LG-AI-Research/PANORAMA.

</details>
