<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 33]
- [cs.CV](#cs.CV) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.LG](#cs.LG) [Total: 4]
- [cs.CY](#cs.CY) [Total: 2]
- [cs.PL](#cs.PL) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Bilingual Word Level Language Identification for Omotic Languages](https://arxiv.org/abs/2509.07998)
*Mesay Gemeda Yigezu,Girma Yohannis Bade,Atnafu Lambebo Tonja,Olga Kolesnikova,Grigori Sidorov,Alexander Gelbukh*

Main category: cs.CL

TL;DR: 该论文针对埃塞俄比亚南部的Wolaita和Gofa双语识别任务，提出结合BERT预训练模型和LSTM的方法，在测试集上取得了0.72的F1分数。


<details>
  <summary>Details</summary>
Motivation: 在多语言社区中，文本常包含多种语言，而Wolaita和Gofa两种语言存在词汇相似性和差异性，使得语言识别任务具有挑战性。

Method: 采用多种实验方法，最终确定BERT预训练语言模型与LSTM相结合的方法。

Result: BERT+LSTM组合方法在测试集上表现最佳，F1分数达到0.72。

Conclusion: 该工作能有效处理社交媒体中的多语言问题，并为该领域的进一步研究奠定基础。

Abstract: Language identification is the task of determining the languages for a given
text. In many real world scenarios, text may contain more than one language,
particularly in multilingual communities. Bilingual Language Identification
(BLID) is the task of identifying and distinguishing between two languages in a
given text. This paper presents BLID for languages spoken in the southern part
of Ethiopia, namely Wolaita and Gofa. The presence of words similarities and
differences between the two languages makes the language identification task
challenging. To overcome this challenge, we employed various experiments on
various approaches. Then, the combination of the BERT based pretrained language
model and LSTM approach performed better, with an F1 score of 0.72 on the test
set. As a result, the work will be effective in tackling unwanted social media
issues and providing a foundation for further research in this area.

</details>


### [2] [AntiDote: Bi-level Adversarial Training for Tamper-Resistant LLMs](https://arxiv.org/abs/2509.08000)
*Debdeep Sanyal,Manodeep Ray,Murari Mandal*

Main category: cs.CL

TL;DR: AntiDote是一种双层优化方法，通过对抗性超网络训练LLM抵抗恶意微调攻击，在保持模型能力的同时显著提升安全性


<details>
  <summary>Details</summary>
Motivation: 开源权重LLM面临恶意微调生成有害内容的风险，现有安全措施难以在完全模型访问权限下保护模型安全

Method: 使用双层优化：辅助对抗超网络学习生成恶意LoRA权重，主模型训练目标为抵消这些对抗权重的影响

Result: 在52种红队攻击测试中，AntiDote比基准方法强27.4%，能力基准性能下降小于0.5%

Conclusion: 提供了一种实用且计算高效的方法，使开源权重模型的安全属性更加内在和鲁棒

Abstract: The release of open-weight large language models (LLMs) creates a tension
between advancing accessible research and preventing misuse, such as malicious
fine-tuning to elicit harmful content. Current safety measures struggle to
preserve the general capabilities of the LLM while resisting a determined
adversary with full access to the model's weights and architecture, who can use
full-parameter fine-tuning to erase existing safeguards. To address this, we
introduce AntiDote, a bi-level optimization procedure for training LLMs to be
resistant to such tampering. AntiDote involves an auxiliary adversary
hypernetwork that learns to generate malicious Low-Rank Adaptation (LoRA)
weights conditioned on the defender model's internal activations. The defender
LLM is then trained with an objective to nullify the effect of these
adversarial weight additions, forcing it to maintain its safety alignment. We
validate this approach against a diverse suite of 52 red-teaming attacks,
including jailbreak prompting, latent space manipulation, and direct
weight-space attacks. AntiDote is upto 27.4\% more robust against adversarial
attacks compared to both tamper-resistance and unlearning baselines. Crucially,
this robustness is achieved with a minimal trade-off in utility, incurring a
performance degradation of upto less than 0.5\% across capability benchmarks
including MMLU, HellaSwag, and GSM8K. Our work offers a practical and compute
efficient methodology for building open-weight models where safety is a more
integral and resilient property.

</details>


### [3] [MVPBench: A Benchmark and Fine-Tuning Framework for Aligning Large Language Models with Diverse Human Values](https://arxiv.org/abs/2509.08022)
*Yao Liang,Dongcheng Zhao,Feifei Zhao,Guobin Shen,Yuwei Wang,Dongqi Liang,Yi Zeng*

Main category: cs.CL

TL;DR: MVPBench是一个包含75个国家24,020个实例的基准测试，用于系统评估LLM与多维人类价值观的对齐程度，揭示了不同地理和人口统计群体间的显著差异，并证明轻量级微调方法可以显著提升对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试往往忽视文化和人口多样性，导致对价值观对齐在全球范围内泛化能力的理解有限。

Method: 构建MVPBench基准测试，包含精细价值观标签、个性化问题和丰富人口统计元数据；使用LoRA和DPO等轻量级微调方法进行实验。

Result: 发现最先进LLM在不同地理和人口统计群体间存在显著对齐性能差异；轻量级微调方法在域内和域外设置中都能显著提升价值对齐效果。

Conclusion: 需要基于人口统计的对齐评估，MVPBench为全球对齐、个性化价值建模和公平AI发展提供了实用基础。

Abstract: The alignment of large language models (LLMs) with human values is critical
for their safe and effective deployment across diverse user populations.
However, existing benchmarks often neglect cultural and demographic diversity,
leading to limited understanding of how value alignment generalizes globally.
In this work, we introduce MVPBench, a novel benchmark that systematically
evaluates LLMs' alignment with multi-dimensional human value preferences across
75 countries. MVPBench contains 24,020 high-quality instances annotated with
fine-grained value labels, personalized questions, and rich demographic
metadata, making it the most comprehensive resource of its kind to date. Using
MVPBench, we conduct an in-depth analysis of several state-of-the-art LLMs,
revealing substantial disparities in alignment performance across geographic
and demographic lines. We further demonstrate that lightweight fine-tuning
methods, such as Low-Rank Adaptation (LoRA) and Direct Preference Optimization
(DPO), can significantly enhance value alignment in both in-domain and
out-of-domain settings. Our findings underscore the necessity for
population-aware alignment evaluation and provide actionable insights for
building culturally adaptive and value-sensitive LLMs. MVPBench serves as a
practical foundation for future research on global alignment, personalized
value modeling, and equitable AI development.

</details>


### [4] [NOWJ@COLIEE 2025: A Multi-stage Framework Integrating Embedding Models and Large Language Models for Legal Retrieval and Entailment](https://arxiv.org/abs/2509.08025)
*Hoang-Trung Nguyen,Tan-Minh Nguyen,Xuan-Bach Le,Tuan-Kiet Le,Khanh-Huyen Nguyen,Ha-Thanh Nguyen,Thi-Hai-Yen Vuong,Le-Minh Nguyen*

Main category: cs.CL

TL;DR: NOWJ团队在COLIEE 2025竞赛中采用混合方法，结合传统检索技术和现代生成模型，在Legal Case Entailment任务中获得第一名，并在其他法律信息处理任务中表现优异


<details>
  <summary>Details</summary>
Motivation: 探索将传统信息检索技术与先进大语言模型相结合的方法，提升法律信息处理的准确性和效率

Method: 两阶段检索系统：结合词义-语义过滤（BM25、BERT、monoT5）和上下文LLM分析（Qwen-2、QwQ-32B、DeepSeek-V3），使用BGE-m3和LLM2Vec进行语义表示

Result: Task 2中获得第一名，F1分数0.3195；在其他法律任务中也展现出强劲性能

Conclusion: 混合模型结合传统IR技术和现代生成模型具有巨大潜力，为法律信息处理的未来发展提供了有价值的参考

Abstract: This paper presents the methodologies and results of the NOWJ team's
participation across all five tasks at the COLIEE 2025 competition, emphasizing
advancements in the Legal Case Entailment task (Task 2). Our comprehensive
approach systematically integrates pre-ranking models (BM25, BERT, monoT5),
embedding-based semantic representations (BGE-m3, LLM2Vec), and advanced Large
Language Models (Qwen-2, QwQ-32B, DeepSeek-V3) for summarization, relevance
scoring, and contextual re-ranking. Specifically, in Task 2, our two-stage
retrieval system combined lexical-semantic filtering with contextualized LLM
analysis, achieving first place with an F1 score of 0.3195. Additionally, in
other tasks--including Legal Case Retrieval, Statute Law Retrieval, Legal
Textual Entailment, and Legal Judgment Prediction--we demonstrated robust
performance through carefully engineered ensembles and effective prompt-based
reasoning strategies. Our findings highlight the potential of hybrid models
integrating traditional IR techniques with contemporary generative models,
providing a valuable reference for future advancements in legal information
processing.

</details>


### [5] [SciGPT: A Large Language Model for Scientific Literature Understanding and Knowledge Discovery](https://arxiv.org/abs/2509.08032)
*Fengyu She,Nan Wang,Hongfei Wu,Ziyi Wan,Jingmian Wang,Chang Wang*

Main category: cs.CL

TL;DR: SciGPT是一个针对科学文献理解的领域适应基础模型，通过低成本的领域蒸馏、稀疏专家混合注意力机制和知识感知适应等创新技术，在科学任务上超越了GPT-4o的表现。


<details>
  <summary>Details</summary>
Motivation: 科学文献呈指数级增长，研究人员需要高效的知识合成工具。通用大语言模型在科学领域存在局限性，无法很好地处理专业术语、方法严谨性和复杂科学任务，限制了其在跨学科研究中的应用。

Method: 基于Qwen3架构，采用三阶段创新：1）低成本领域蒸馏的两阶段流程平衡性能与效率；2）稀疏专家混合注意力机制，在32,000个token的长文档推理中减少55%内存消耗；3）知识感知适应整合领域本体以弥合跨学科知识差距。

Result: 在ScienceBench基准测试中，SciGPT在序列标注、生成和推理等核心科学任务上超越了GPT-4o，并在未见过的科学任务中表现出强大的鲁棒性。

Conclusion: SciGPT验证了其在促进AI增强科学发现方面的潜力，为科学文献理解和跨学科研究提供了有效的解决方案。

Abstract: Scientific literature is growing exponentially, creating a critical
bottleneck for researchers to efficiently synthesize knowledge. While
general-purpose Large Language Models (LLMs) show potential in text processing,
they often fail to capture scientific domain-specific nuances (e.g., technical
jargon, methodological rigor) and struggle with complex scientific tasks,
limiting their utility for interdisciplinary research. To address these gaps,
this paper presents SciGPT, a domain-adapted foundation model for scientific
literature understanding and ScienceBench, an open source benchmark tailored to
evaluate scientific LLMs.
  Built on the Qwen3 architecture, SciGPT incorporates three key innovations:
(1) low-cost domain distillation via a two-stage pipeline to balance
performance and efficiency; (2) a Sparse Mixture-of-Experts (SMoE) attention
mechanism that cuts memory consumption by 55\% for 32,000-token long-document
reasoning; and (3) knowledge-aware adaptation integrating domain ontologies to
bridge interdisciplinary knowledge gaps.
  Experimental results on ScienceBench show that SciGPT outperforms GPT-4o in
core scientific tasks including sequence labeling, generation, and inference.
It also exhibits strong robustness in unseen scientific tasks, validating its
potential to facilitate AI-augmented scientific discovery.

</details>


### [6] [No for Some, Yes for Others: Persona Prompts and Other Sources of False Refusal in Language Models](https://arxiv.org/abs/2509.08075)
*Flor Miriam Plaza-del-Arco,Paul Röttger,Nino Scherrer,Emanuele Borgonovo,Elmar Plischke,Dirk Hovy*

Main category: cs.CL

TL;DR: 研究发现LLM个性化可能导致错误拒绝请求，但通过系统测试发现模型能力和任务类型对错误拒绝的影响比人物设定更大，人物设定效应可能被高估


<details>
  <summary>Details</summary>
Motivation: 大型语言模型日益个性化，但人物设定提示可能导致模型错误拒绝用户请求，需要量化这种问题的程度

Method: 使用15种社会人口统计人物设定、16个模型、3种任务和9种提示变体进行测试，提出蒙特卡洛方法进行样本高效量化

Result: 模型能力越强，人物设定对拒绝率影响越小；某些社会人口统计人物设定会增加错误拒绝，但模型选择和任务类型影响更大

Conclusion: 人物设定效应可能被高估，错误拒绝更多受模型能力和任务敏感度影响，而非人物设定本身

Abstract: Large language models (LLMs) are increasingly integrated into our daily lives
and personalized. However, LLM personalization might also increase unintended
side effects. Recent work suggests that persona prompting can lead models to
falsely refuse user requests. However, no work has fully quantified the extent
of this issue. To address this gap, we measure the impact of 15
sociodemographic personas (based on gender, race, religion, and disability) on
false refusal. To control for other factors, we also test 16 different models,
3 tasks (Natural Language Inference, politeness, and offensiveness
classification), and nine prompt paraphrases. We propose a Monte Carlo-based
method to quantify this issue in a sample-efficient manner. Our results show
that as models become more capable, personas impact the refusal rate less and
less. Certain sociodemographic personas increase false refusal in some models,
which suggests underlying biases in the alignment strategies or safety
mechanisms. However, we find that the model choice and task significantly
influence false refusals, especially in sensitive content tasks. Our findings
suggest that persona effects have been overestimated, and might be due to other
factors.

</details>


### [7] [Culturally transmitted color categories in LLMs reflect a learning bias toward efficient compression](https://arxiv.org/abs/2509.08093)
*Nathaniel Imel,Noga Zaslavsky*

Main category: cs.CL

TL;DR: 研究表明LLMs能够通过信息瓶颈原则演化出类似人类的高效语义系统，在颜色命名任务中表现出与人类相似的压缩效率和跨文化对齐模式


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型是否能够像人类语言一样，通过信息瓶颈原则演化出高效的语义分类系统，而不是仅仅模仿训练数据中的模式

Method: 使用Gemini 2.0-flash和Llama 3.3-70B-Instruct模型，复制两个人类行为研究：英语颜色命名研究和通过上下文学习模拟文化演化过程

Result: Gemini与英语母语者的命名模式高度一致且获得高IB效率分数，Llama表现出高效但复杂度较低的系统；两种模型都能通过迭代学习将随机系统重构为更高IB效率

Conclusion: LLMs能够演化出基于感知的、类似人类的语义系统，其驱动原理与人类语言语义效率的基本原理相同

Abstract: Converging evidence suggests that systems of semantic categories across human
languages achieve near-optimal compression via the Information Bottleneck (IB)
complexity-accuracy principle. Large language models (LLMs) are not trained for
this objective, which raises the question: are LLMs capable of evolving
efficient human-like semantic systems? To address this question, we focus on
the domain of color as a key testbed of cognitive theories of categorization
and replicate with LLMs (Gemini 2.0-flash and Llama 3.3-70B-Instruct) two
influential human behavioral studies. First, we conduct an English color-naming
study, showing that Gemini aligns well with the naming patterns of native
English speakers and achieves a significantly high IB-efficiency score, while
Llama exhibits an efficient but lower complexity system compared to English.
Second, to test whether LLMs simply mimic patterns in their training data or
actually exhibit a human-like inductive bias toward IB-efficiency, we simulate
cultural evolution of pseudo color-naming systems in LLMs via iterated
in-context language learning. We find that akin to humans, LLMs iteratively
restructure initially random systems towards greater IB-efficiency and
increased alignment with patterns observed across the world's languages. These
findings demonstrate that LLMs are capable of evolving perceptually grounded,
human-like semantic systems, driven by the same fundamental principle that
governs semantic efficiency across human languages.

</details>


### [8] [MERLIN: Multi-Stage Curriculum Alignment for Multilingual Encoder and LLM Fusion](https://arxiv.org/abs/2509.08105)
*Kosei Uemura,David Guzmán,Quang Phuoc Nguyen,Jesujoba Oluwadara Alabi,En-shiun Annie Lee,David Ifeoluwa Adelani*

Main category: cs.CL

TL;DR: MERLIN是一个两阶段模型堆叠框架，通过课程学习策略和DoRA权重适配，显著提升低资源语言的复杂推理能力，在AfriMGSM基准上比MindMerger准确率提升12.9个百分点，甚至超越GPT-4o-mini。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在英语上表现优异，但在许多低资源语言的复杂推理任务上仍然存在困难。现有的编码器-解码器方法对中高资源语言有效，但对低资源语言效果有限。

Method: 提出MERLIN两阶段模型堆叠框架：1）采用课程学习策略，从通用双语文本到任务特定数据逐步学习；2）仅适配少量DoRA权重进行参数高效微调。

Result: 在AfriMGSM基准上准确率比MindMerger提升+12.9pp，超越GPT-4o-mini；在MGSM和MSVAMP上也分别获得+0.9pp和+2.8pp的稳定提升，证明在低资源和高资源设置下均有效。

Conclusion: MERLIN框架通过创新的课程学习和参数高效适配方法，显著改善了低资源语言的复杂推理性能，为多语言NLP任务提供了有效的解决方案。

Abstract: Large language models excel in English but still struggle with complex
reasoning in many low-resource languages (LRLs). Existing encoder-plus-decoder
methods such as LangBridge and MindMerger raise accuracy on mid and
high-resource languages, yet they leave a large gap on LRLs. We present MERLIN,
a two-stage model-stacking framework that applies a curriculum learning
strategy -- from general bilingual bitext to task-specific data -- and adapts
only a small set of DoRA weights. On the AfriMGSM benchmark MERLIN improves
exact-match accuracy by +12.9 pp over MindMerger and outperforms GPT-4o-mini.
It also yields consistent gains on MGSM and MSVAMP (+0.9 and +2.8 pp),
demonstrating effectiveness across both low and high-resource settings.

</details>


### [9] [Bias after Prompting: Persistent Discrimination in Large Language Models](https://arxiv.org/abs/2509.08146)
*Nivedha Sivakumar,Natalie Mackraz,Samira Khorshidi,Krishna Patel,Barry-John Theobald,Luca Zappella,Nicholas Apostoloff*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: A dangerous assumption that can be made from prior work on the bias transfer
hypothesis (BTH) is that biases do not transfer from pre-trained large language
models (LLMs) to adapted models. We invalidate this assumption by studying the
BTH in causal models under prompt adaptations, as prompting is an extremely
popular and accessible adaptation strategy used in real-world applications. In
contrast to prior work, we find that biases can transfer through prompting and
that popular prompt-based mitigation methods do not consistently prevent biases
from transferring. Specifically, the correlation between intrinsic biases and
those after prompt adaptation remain moderate to strong across demographics and
tasks -- for example, gender (rho >= 0.94) in co-reference resolution, and age
(rho >= 0.98) and religion (rho >= 0.69) in question answering. Further, we
find that biases remain strongly correlated when varying few-shot composition
parameters, such as sample size, stereotypical content, occupational
distribution and representational balance (rho >= 0.90). We evaluate several
prompt-based debiasing strategies and find that different approaches have
distinct strengths, but none consistently reduce bias transfer across models,
tasks or demographics. These results demonstrate that correcting bias, and
potentially improving reasoning ability, in intrinsic models may prevent
propagation of biases to downstream tasks.

</details>


### [10] [Verbalized Algorithms](https://arxiv.org/abs/2509.08150)
*Supriya Lall,Christian Farrell,Hari Pathanjaly,Marko Pavic,Sarvesh Chezhian,Masataro Asai*

Main category: cs.CL

TL;DR: 提出verbalized algorithms方法，将复杂任务分解为LLM能够可靠处理的简单自然语言操作，使用经典算法框架来保证理论可靠性


<details>
  <summary>Details</summary>
Motivation: 传统的一次性查询LLM方式无法保证推理任务的正确性，需要利用经典算法的理论保障来提升LLM在复杂任务中的可靠性

Method: 将任务分解为简单的自然语言基本操作，让LLM只负责这些可靠的小任务，在经典算法框架（如排序网络）中作为比较器等基础组件

Result: 在排序和聚类任务上验证了该方法的有效性

Conclusion: verbalized algorithms方法通过结合经典算法理论和LLM的自然语言处理能力，能够可靠地完成复杂推理任务

Abstract: Instead of querying LLMs in a one-shot manner and hoping to get the right
answer for a reasoning task, we propose a paradigm we call \emph{verbalized
algorithms} (VAs), which leverage classical algorithms with established
theoretical understanding. VAs decompose a task into simple elementary
operations on natural language strings that they should be able to answer
reliably, and limit the scope of LLMs to only those simple tasks. For example,
for sorting a series of natural language strings, \emph{verbalized sorting}
uses an LLM as a binary comparison oracle in a known and well-analyzed sorting
algorithm (e.g., bitonic sorting network). We demonstrate the effectiveness of
this approach on sorting and clustering tasks.

</details>


### [11] [Balancing Quality and Variation: Spam Filtering Distorts Data Label Distributions](https://arxiv.org/abs/2509.08217)
*Eve Fleisig,Matthias Orlikowski,Philipp Cimiano,Dan Klein*

Main category: cs.CL

TL;DR: 这篇论文研究主观标注任务中如何在过滤垃圾标注者的同时保持标签多样性。发现现有过滤方法容易删除不同意见而非垃圾标注者，优化设置是移除<5%标注者。垃圾标注者通常更不随机，而现有方法假设变异是垃圾的直觉是错误的。


<details>
  <summary>Details</summary>
Motivation: 在主观标注任务中，既要过滤垃圾或低质量标注，又要保持标签的多样性和不同意见。需要找到标注者可靠性和代表性之间的平衡。

Method: 通过实证评估一系列标注者过滤恰诊对保持主观任务标签变异的影响，分析综合垃圾数据来观察垃圾标注者的行为特征。

Result: 发现现有过滤方法常将不同意见标注者误删为垃圾，最优设置是移除<5%标注者。垃圾标注者通常更不随机（给出固定答案），而现有方法假设变异是垃圾的直觉是错误的。

Conclusion: 主观任务中保持标签多样性需要新的垃圾过滤方法，这些方法应考虑到垃圾标注者通常更不随机的特征，而不是将变异视为垃圾。

Abstract: For machine learning datasets to accurately represent diverse opinions in a
population, they must preserve variation in data labels while filtering out
spam or low-quality responses. How can we balance annotator reliability and
representation? We empirically evaluate how a range of heuristics for annotator
filtering affect the preservation of variation on subjective tasks. We find
that these methods, designed for contexts in which variation from a single
ground-truth label is considered noise, often remove annotators who disagree
instead of spam annotators, introducing suboptimal tradeoffs between accuracy
and label diversity. We find that conservative settings for annotator removal
(<5%) are best, after which all tested methods increase the mean absolute error
from the true average label. We analyze performance on synthetic spam to
observe that these methods often assume spam annotators are less random than
real spammers tend to be: most spammers are distributionally indistinguishable
from real annotators, and the minority that are distinguishable tend to give
fixed answers, not random ones. Thus, tasks requiring the preservation of
variation reverse the intuition of existing spam filtering methods: spammers
tend to be less random than non-spammers, so metrics that assume variation is
spam fare worse. These results highlight the need for spam removal methods that
account for label diversity.

</details>


### [12] [Towards Knowledge-Aware Document Systems: Modeling Semantic Coverage Relations via Answerability Detection](https://arxiv.org/abs/2509.08304)
*Yehudit Aperstein,Alon Gottlib,Gal Benita,Alexander Apartsin*

Main category: cs.CL

TL;DR: 这篇论文提出了一种基于问题回答的方法来模型语义覆盖关系，将文档对分为等价、包含和重叠三种关系，通过综合性数据集评测发现判别模型表现更优。


<details>
  <summary>Details</summary>
Motivation: 理解不同格式文档之间的信息共享方式对信息检索、摘要生成等任务至关重要，需要一种能够识别语义覆盖关系的方法。

Method: 采用问题回答基础的方法，通过文档对共享问题的回答能力来识别语义关系。使用SQuAD语料库构建综合性数据集，通过重写和选择性省略信息来控制内容重叠程度。

Result: 判别模型显著超过生成模型，RoBERTa-base模型达到最高准确率61.4%，Random Forest模型在平均F1分数上表现最佳（52.9%）。

Conclusion: 问题回答提供了有效的视角来评估不同风格文本间的语义关系，为现有模型的语义推理能力提供了见解。

Abstract: Understanding how information is shared across documents, regardless of the
format in which it is expressed, is critical for tasks such as information
retrieval, summarization, and content alignment. In this work, we introduce a
novel framework for modelling Semantic Coverage Relations (SCR), which
classifies document pairs based on how their informational content aligns. We
define three core relation types: equivalence, where both texts convey the same
information using different textual forms or styles; inclusion, where one
document fully contains the information of another and adds more; and semantic
overlap, where each document presents partially overlapping content. To capture
these relations, we adopt a question answering (QA)-based approach, using the
answerability of shared questions across documents as an indicator of semantic
coverage. We construct a synthetic dataset derived from the SQuAD corpus by
paraphrasing source passages and selectively omitting information, enabling
precise control over content overlap. This dataset allows us to benchmark
generative language models and train transformer-based classifiers for SCR
prediction. Our findings demonstrate that discriminative models significantly
outperform generative approaches, with the RoBERTa-base model achieving the
highest accuracy of 61.4% and the Random Forest-based model showing the best
balance with a macro-F1 score of 52.9%. The results show that QA provides an
effective lens for assessing semantic relations across stylistically diverse
texts, offering insights into the capacity of current models to reason about
information beyond surface similarity. The dataset and code developed in this
study are publicly available to support reproducibility.

</details>


### [13] [Toward Subtrait-Level Model Explainability in Automated Writing Evaluation](https://arxiv.org/abs/2509.08345)
*Alejandro Andrade-Lotero,Lee Becker,Joshua Southerland,Scott Hellman*

Main category: cs.CL

TL;DR: 使用生成式语言模型实现写作评分的潜在特质组件评估，提高评分透明度


<details>
  <summary>Details</summary>
Motivation: 通过潜在特质组件评估提高自动化写作评分的透明度，为教育工作者和学生提供更详细的评分解释

Method: 使用生成式语言模型进行原型开发，实现解释性和子特质评分功能

Result: 人工子特质与总特质评分、以及自动化与人工子特质评分之间均呈现中等相关性

Conclusion: 该方法能够提供详细的评分信息，帮助教育者和学生更好地理解评分结果

Abstract: Subtrait (latent-trait components) assessment presents a promising path
toward enhancing transparency of automated writing scores. We prototype
explainability and subtrait scoring with generative language models and show
modest correlation between human subtrait and trait scores, and between
automated and human subtrait scores. Our approach provides details to demystify
scores for educators and students.

</details>


### [14] [Automatic Detection of Inauthentic Templated Responses in English Language Assessments](https://arxiv.org/abs/2509.08355)
*Yashad Samant,Lee Becker,Scott Hellman,Bradley Behan,Sarah Hughes,Joshua Southerland*

Main category: cs.CL

TL;DR: 提出自动化检测模板化作文的方法AuDITR，用于识别英语考试中考生使用背诵模板欺骗评分系统的行为


<details>
  <summary>Details</summary>
Motivation: 在英语语言评估中，低水平考生可能使用背诵的模板材料来欺骗自动化评分系统，需要检测这种不真实的模板化回答

Method: 采用机器学习方法来自动检测模板化回答，并强调在生产环境中需要定期更新这些模型

Result: 提出了AuDITR任务框架和机器学习解决方案

Conclusion: 自动化检测模板化回答对于维护考试公平性至关重要，且需要持续更新检测模型以应对新的模板策略

Abstract: In high-stakes English Language Assessments, low-skill test takers may employ
memorized materials called ``templates'' on essay questions to ``game'' or fool
the automated scoring system. In this study, we introduce the automated
detection of inauthentic, templated responses (AuDITR) task, describe a machine
learning-based approach to this task and illustrate the importance of regularly
updating these models in production.

</details>


### [15] [<think> So let's replace this phrase with insult... </think> Lessons learned from generation of toxic texts with LLMs](https://arxiv.org/abs/2509.08358)
*Sergey Pletenev,Daniil Moskovskiy,Alexander Panchenko*

Main category: cs.CL

TL;DR: 使用LLM生成的合成毒性数据训练去毒模型效果较人类数据差30%，主要原因是LLM词汇多样性不足


<details>
  <summary>Details</summary>
Motivation: 探索LLM生成的合成毒性数据是否可以替代人类数据用于文本去毒模型训练

Method: 使用Llama 3和Qwen激活补丁模型为ParaDetox和SST-2数据集中的中性文本生成合成毒性对应片段，并比较合成数据与人类数据训练的去毒模型性能

Result: 使用合成数据训练的模型性能持续较差，聚合指标下降达30%，根本原因是LLM生成的毒性内容词汇多样性不足，仅使用少量重复的辱骂词汇

Conclusion: 现有LLM在毒性内容生成方面存在显著局限性，强调了多样化人类注解数据对建立健壮去毒系统的重要性

Abstract: Modern Large Language Models (LLMs) are excellent at generating synthetic
data. However, their performance in sensitive domains such as text
detoxification has not received proper attention from the scientific community.
This paper explores the possibility of using LLM-generated synthetic toxic data
as an alternative to human-generated data for training models for
detoxification. Using Llama 3 and Qwen activation-patched models, we generated
synthetic toxic counterparts for neutral texts from ParaDetox and SST-2
datasets. Our experiments show that models fine-tuned on synthetic data
consistently perform worse than those trained on human data, with a drop in
performance of up to 30% in joint metrics. The root cause is identified as a
critical lexical diversity gap: LLMs generate toxic content using a small,
repetitive vocabulary of insults that fails to capture the nuances and variety
of human toxicity. These findings highlight the limitations of current LLMs in
this domain and emphasize the continued importance of diverse, human-annotated
data for building robust detoxification systems.

</details>


### [16] [Low-Resource Fine-Tuning for Multi-Task Structured Information Extraction with a Billion-Parameter Instruction-Tuned Model](https://arxiv.org/abs/2509.08381)
*Yu Cheng Chih,Yong Hao Hou*

Main category: cs.CL

TL;DR: ETLCH是一个基于LLaMA的10亿参数模型，通过少量样本（每任务几百到一千个）进行低秩适应微调，在JSON提取、知识图谱提取和命名实体识别任务中表现出色，以较低计算成本实现稳定的结构化输出。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在结构化数据提取领域部署成本高且需要大量高质量数据，小团队难以承担。现有研究主要关注70亿参数以上模型，缺乏小模型在低资源多任务条件下的可靠性证据。

Method: 基于LLaMA架构的10亿参数模型，使用低秩适应（LoRA）技术，每个任务仅用几百到一千个样本进行微调，专注于JSON提取、知识图谱提取和命名实体识别任务。

Result: ETLCH在大多数评估指标上优于强基线模型，即使在最低数据规模下也观察到显著增益，证明了小模型在低资源环境下的有效性。

Conclusion: 经过良好调优的小型模型能够以极低的计算成本提供稳定准确的结构化输出，为资源受限环境下的信息提取管道提供了经济可靠的解决方案。

Abstract: Deploying large language models (LLMs) for structured data extraction in
domains such as financial compliance reporting, legal document analytics, and
multilingual knowledge base construction is often impractical for smaller teams
due to the high cost of running large architectures and the difficulty of
preparing large, high-quality datasets. Most recent instruction-tuning studies
focus on seven-billion-parameter or larger models, leaving limited evidence on
whether much smaller models can work reliably under low-resource, multi-task
conditions. This work presents ETLCH, a billion-parameter LLaMA-based model
fine-tuned with low-rank adaptation on only a few hundred to one thousand
samples per task for JSON extraction, knowledge graph extraction, and named
entity recognition. Despite its small scale, ETLCH outperforms strong baselines
across most evaluation metrics, with substantial gains observed even at the
lowest data scale. These findings demonstrate that well-tuned small models can
deliver stable and accurate structured outputs at a fraction of the
computational cost, enabling cost-effective and reliable information extraction
pipelines in resource-constrained environments.

</details>


### [17] [CommonVoice-SpeechRE and RPG-MoGe: Advancing Speech Relation Extraction with a New Dataset and Multi-Order Generative Framework](https://arxiv.org/abs/2509.08438)
*Jinzhong Ning,Paerhati Tulajiang,Yingying Le,Yijia Zhang,Yuanyuan Sun,Hongfei Lin,Haifeng Liu*

Main category: cs.CL

TL;DR: 提出了CommonVoice-SpeechRE大规模真实语音数据集和RPG-MoGe框架，通过多序生成集成和关系提示引导，显著提升了语音关系抽取性能


<details>
  <summary>Details</summary>
Motivation: 现有语音关系抽取数据集依赖合成数据，缺乏真实语音的多样性和数量；现有模型存在生成模板单一和语义对齐弱的问题

Method: 构建CommonVoice-SpeechRE数据集（近2万真实语音样本）；提出RPG-MoGe框架，包含多序三元组生成集成策略和CNN关系预测头生成关系提示

Result: 实验表明该方法优于现有最先进方法，为真实世界语音关系抽取提供了基准数据集和有效解决方案

Conclusion: 该工作解决了语音关系抽取领域的真实数据稀缺和模型性能限制问题，通过大规模真实数据集和创新框架推动了该领域发展

Abstract: Speech Relation Extraction (SpeechRE) aims to extract relation triplets
directly from speech. However, existing benchmark datasets rely heavily on
synthetic data, lacking sufficient quantity and diversity of real human speech.
Moreover, existing models also suffer from rigid single-order generation
templates and weak semantic alignment, substantially limiting their
performance. To address these challenges, we introduce CommonVoice-SpeechRE, a
large-scale dataset comprising nearly 20,000 real-human speech samples from
diverse speakers, establishing a new benchmark for SpeechRE research.
Furthermore, we propose the Relation Prompt-Guided Multi-Order Generative
Ensemble (RPG-MoGe), a novel framework that features: (1) a multi-order triplet
generation ensemble strategy, leveraging data diversity through diverse element
orders during both training and inference, and (2) CNN-based latent relation
prediction heads that generate explicit relation prompts to guide cross-modal
alignment and accurate triplet generation. Experiments show our approach
outperforms state-of-the-art methods, providing both a benchmark dataset and an
effective solution for real-world SpeechRE. The source code and dataset are
publicly available at https://github.com/NingJinzhong/SpeechRE_RPG_MoGe.

</details>


### [18] [Adversarial Attacks Against Automated Fact-Checking: A Survey](https://arxiv.org/abs/2509.08463)
*Fanzhen Liu,Alsharif Abuadbba,Kristen Moore,Surya Nepal,Cecile Paris,Jia Wu,Jian Yang,Quan Z. Sheng*

Main category: cs.CL

TL;DR: 这篇论文是关于对抗性攻击对自动事实核查系统的综述研究，分析了攻击策略、模型脆弱性和防御方法，强调了构建抗攻击事实核查框架的紧迫性。


<details>
  <summary>Details</summary>
Motivation: 在错误信息泛滥的时代，自动事实核查系统容易受到对抗性攻击，这些攻击会操纵或生成虚假声明、证据或声明-证据对，从而破坏事实核查的可靠性。现有研究缺乏对关键挑战的全面概述。

Method: 采用系统性文献综述方法，对针对事实核查的对抗性攻击进行深入分析，分类现有攻击方法学，并评估它们对自动事实核查系统的影响。同时研究对抗性感知防御的最新进展。

Result: 研究发现现有事实核查模型在面对精心设计的对抗性攻击时表现脆弱，攻击能够显著降低验证准确性。研究识别了多种攻击策略和相应的防御机制。

Conclusion: 迫切需要开发能够抵御对抗性操纵的弹性事实核查框架，以保持高验证准确性。研究强调了该领域尚未解决的研究问题，需要进一步探索以增强事实核查系统的鲁棒性。

Abstract: In an era where misinformation spreads freely, fact-checking (FC) plays a
crucial role in verifying claims and promoting reliable information. While
automated fact-checking (AFC) has advanced significantly, existing systems
remain vulnerable to adversarial attacks that manipulate or generate claims,
evidence, or claim-evidence pairs. These attacks can distort the truth, mislead
decision-makers, and ultimately undermine the reliability of FC models. Despite
growing research interest in adversarial attacks against AFC systems, a
comprehensive, holistic overview of key challenges remains lacking. These
challenges include understanding attack strategies, assessing the resilience of
current models, and identifying ways to enhance robustness. This survey
provides the first in-depth review of adversarial attacks targeting FC,
categorizing existing attack methodologies and evaluating their impact on AFC
systems. Additionally, we examine recent advancements in adversary-aware
defenses and highlight open research questions that require further
exploration. Our findings underscore the urgent need for resilient FC
frameworks capable of withstanding adversarial manipulations in pursuit of
preserving high verification accuracy.

</details>


### [19] [Acquiescence Bias in Large Language Models](https://arxiv.org/abs/2509.08480)
*Daniel Braun*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Acquiescence bias, i.e. the tendency of humans to agree with statements in
surveys, independent of their actual beliefs, is well researched and
documented. Since Large Language Models (LLMs) have been shown to be very
influenceable by relatively small changes in input and are trained on
human-generated data, it is reasonable to assume that they could show a similar
tendency. We present a study investigating the presence of acquiescence bias in
LLMs across different models, tasks, and languages (English, German, and
Polish). Our results indicate that, contrary to humans, LLMs display a bias
towards answering no, regardless of whether it indicates agreement or
disagreement.

</details>


### [20] [Simulating Identity, Propagating Bias: Abstraction and Stereotypes in LLM-Generated Text](https://arxiv.org/abs/2509.08484)
*Pia Sommerauer,Giulia Rambelli,Tommaso Caselli*

Main category: cs.CL

TL;DR: 人设提示技术对LLM输出语言抽象度影响有限，无法有效调节威胁性制国论述，反而可能增加对社会群体的刻板印象


<details>
  <summary>Details</summary>
Motivation: 研究人设提示技术如何影响大语言模型在社会群体表述中的语言抽象度，这是刻板化论述的重要标志

Method: 基于语言期望偏差框架，测量6个开源LLM在3种提示条件下的语言抽象度，使用具体性、特定性和否定指标，并创建了新的自我报告刻板化数据集Self-Stereo

Result: 人设提示在调节语言抽象度方面效果有限，确认了人设作为社会人口群体代表的生态问题，展示了即使使用边缘群体人设也可能传播刻板化的风险

Conclusion: 人设提示技术存在限制，无法有效控制LLM的刻板化输出，需要更深入的研究和应对策略来减少语言中的偏见和刻板化风险

Abstract: Persona-prompting is a growing strategy to steer LLMs toward simulating
particular perspectives or linguistic styles through the lens of a specified
identity. While this method is often used to personalize outputs, its impact on
how LLMs represent social groups remains underexplored. In this paper, we
investigate whether persona-prompting leads to different levels of linguistic
abstraction - an established marker of stereotyping - when generating short
texts linking socio-demographic categories with stereotypical or
non-stereotypical attributes. Drawing on the Linguistic Expectancy Bias
framework, we analyze outputs from six open-weight LLMs under three prompting
conditions, comparing 11 persona-driven responses to those of a generic AI
assistant. To support this analysis, we introduce Self-Stereo, a new dataset of
self-reported stereotypes from Reddit. We measure abstraction through three
metrics: concreteness, specificity, and negation. Our results highlight the
limits of persona-prompting in modulating abstraction in language, confirming
criticisms about the ecology of personas as representative of socio-demographic
groups and raising concerns about the risk of propagating stereotypes even when
seemingly evoking the voice of a marginalized group.

</details>


### [21] [Too Helpful, Too Harmless, Too Honest or Just Right?](https://arxiv.org/abs/2509.08486)
*Gautam Siddharth Kashyap,Mark Dras,Usman Naseem*

Main category: cs.CL

TL;DR: TrinityX是一个模块化对齐框架，通过校准专家混合(MoCaE)机制，在Transformer架构中分别训练Helpfulness、Harmlessness、Honesty三个维度的专家，并通过任务自适应路由机制整合输出，显著提升大语言模型的HHH对齐性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常单独优化各个对齐维度，导致权衡和不一致行为。虽然MoE架构提供模块化，但路由校准不佳限制了其在对齐任务中的有效性。

Method: 提出TrinityX框架，包含分别针对HHH三个维度训练的专家模型，通过校准的任务自适应路由机制整合专家信号，形成统一的对齐感知表示。

Result: 在三个标准对齐基准测试中，TrinityX相比基线方法在胜率上提升32.5%，安全分数提升33.9%，真实性提升28.4%，同时减少40%以上的内存使用和推理延迟。

Conclusion: TrinityX通过校准的专家混合机制有效解决了大语言模型HHH对齐问题，在性能、效率和泛化性方面都表现出色，校准路由机制是关键因素。

Abstract: Large Language Models (LLMs) exhibit strong performance across a wide range
of NLP tasks, yet aligning their outputs with the principles of Helpfulness,
Harmlessness, and Honesty (HHH) remains a persistent challenge. Existing
methods often optimize for individual alignment dimensions in isolation,
leading to trade-offs and inconsistent behavior. While Mixture-of-Experts (MoE)
architectures offer modularity, they suffer from poorly calibrated routing,
limiting their effectiveness in alignment tasks. We propose TrinityX, a modular
alignment framework that incorporates a Mixture of Calibrated Experts (MoCaE)
within the Transformer architecture. TrinityX leverages separately trained
experts for each HHH dimension, integrating their outputs through a calibrated,
task-adaptive routing mechanism that combines expert signals into a unified,
alignment-aware representation. Extensive experiments on three standard
alignment benchmarks-Alpaca (Helpfulness), BeaverTails (Harmlessness), and
TruthfulQA (Honesty)-demonstrate that TrinityX outperforms strong baselines,
achieving relative improvements of 32.5% in win rate, 33.9% in safety score,
and 28.4% in truthfulness. In addition, TrinityX reduces memory usage and
inference latency by over 40% compared to prior MoE-based approaches. Ablation
studies highlight the importance of calibrated routing, and cross-model
evaluations confirm TrinityX's generalization across diverse LLM backbones.

</details>


### [22] [CM-Align: Consistency-based Multilingual Alignment for Large Language Models](https://arxiv.org/abs/2509.08541)
*Xue Zhang,Yunlong Liang,Fandong Meng,Songming Zhang,Yufeng Chen,Jinan Xu,Jie Zhou*

Main category: cs.CL

TL;DR: 提出CM-Align方法，通过一致性引导的数据选择构建高质量多语言偏好数据，改善多语言对齐性能


<details>
  <summary>Details</summary>
Motivation: 现有方法存在两个限制：1）并非所有英文回复都是高质量的，低质量回复会误导其他语言的对齐；2）当前方法通常使用有偏见或启发式方法构建多语言偏好对，导致噪声数据

Method: 设计基于一致性的数据选择方法，包括一致性引导的英文参考选择和跨语言一致性的多语言偏好数据构建

Result: 在三个大语言模型和三个常见任务上的实验结果表明该方法的有效性和优越性

Conclusion: 构建高质量偏好数据对多语言对齐至关重要，CM-Align方法能有效提升多语言对齐性能

Abstract: Current large language models (LLMs) generally show a significant performance
gap in alignment between English and other languages. To bridge this gap,
existing research typically leverages the model's responses in English as a
reference to select the best/worst responses in other languages, which are then
used for Direct Preference Optimization (DPO) training. However, we argue that
there are two limitations in the current methods that result in noisy
multilingual preference data and further limited alignment performance: 1) Not
all English responses are of high quality, and using a response with low
quality may mislead the alignment for other languages. 2) Current methods
usually use biased or heuristic approaches to construct multilingual preference
pairs. To address these limitations, we design a consistency-based data
selection method to construct high-quality multilingual preference data for
improving multilingual alignment (CM-Align). Specifically, our method includes
two parts: consistency-guided English reference selection and cross-lingual
consistency-based multilingual preference data construction. Experimental
results on three LLMs and three common tasks demonstrate the effectiveness and
superiority of our method, which further indicates the necessity of
constructing high-quality preference data.

</details>


### [23] [LLM Ensemble for RAG: Role of Context Length in Zero-Shot Question Answering for BioASQ Challenge](https://arxiv.org/abs/2509.08596)
*Dima Galat,Diego Molla-Aliod*

Main category: cs.CL

TL;DR: 本文探索如何利用多个大语言模型的集成方法，在不需要精调或标签数据的情况下，实现生物医学问答任务的最新性能水平。


<details>
  <summary>Details</summary>
Motivation: 生物医学问答面临短语知识理解、数据复杂性和短语更新快的挑战，需要快速可扩展的解决方案。

Method: 采用多个LLM模型（包括Anthropic和Google模型）的集成方法，结合了零样本学习技术，通过抓取增强生成（RAG）流程来综合更准确和稳健的答案。

Result: 在BioASQ挑战任务中，集成方法超过了单个LLM模型，某些情况下甚至能与领域特定系统相竞争或更优。研究还发现上下文长度与性能存在关联性。

Conclusion: 集成基于零样本的方法结合有效的RAG流程，为生物医学问答提供了一种实用且可扩展的替代方案，避免了成本高昂的领域特定调整。

Abstract: Biomedical question answering (QA) poses significant challenges due to the
need for precise interpretation of specialized knowledge drawn from a vast,
complex, and rapidly evolving corpus. In this work, we explore how large
language models (LLMs) can be used for information retrieval (IR), and an
ensemble of zero-shot models can accomplish state-of-the-art performance on a
domain-specific Yes/No QA task. Evaluating our approach on the BioASQ challenge
tasks, we show that ensembles can outperform individual LLMs and in some cases
rival or surpass domain-tuned systems - all while preserving generalizability
and avoiding the need for costly fine-tuning or labeled data. Our method
aggregates outputs from multiple LLM variants, including models from Anthropic
and Google, to synthesize more accurate and robust answers. Moreover, our
investigation highlights a relationship between context length and performance:
while expanded contexts are meant to provide valuable evidence, they
simultaneously risk information dilution and model disorientation. These
findings emphasize IR as a critical foundation in Retrieval-Augmented
Generation (RAG) approaches for biomedical QA systems. Precise, focused
retrieval remains essential for ensuring LLMs operate within relevant
information boundaries when generating answers from retrieved documents. Our
results establish that ensemble-based zero-shot approaches, when paired with
effective RAG pipelines, constitute a practical and scalable alternative to
domain-tuned systems for biomedical question answering.

</details>


### [24] [Memorization in Large Language Models in Medicine: Prevalence, Characteristics, and Implications](https://arxiv.org/abs/2509.08604)
*Anran Li,Lingfei Qian,Mengmeng Du,Yu Yin,Yan Hu,Zihao Sun,Yihang Fu,Erica Stutz,Xuguang Ai,Qianqian Xie,Rui Zhu,Jimin Huang,Yifan Yang,Siru Liu,Yih-Chung Tham,Lucila Ohno-Machado,Hyunghoon Cho,Zhiyong Lu,Hua Xu,Qingyu Chen*

Main category: cs.CL

TL;DR: 本研究首次全面评估了医学领域大语言模型的记忆现象，发现医学领域的记忆率显著高于通用领域，可分为有益、无信息和有害三类记忆，并提出了相应的实用建议。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在医学领域展现出巨大潜力，但一个重要问题仍未解决：模型在多大程度上记忆了医学训练数据。本研究旨在系统评估医学LLMs的记忆现象及其影响。

Method: 系统分析了三种常见适应场景：(1)医学语料库上的持续预训练，(2)标准医学基准上的微调，(3)真实世界临床数据上的微调（包含耶鲁纽黑文健康系统13,000多份住院记录）。评估了记忆的普遍性、特征、数量和下游影响。

Result: 结果显示记忆现象在所有适应场景中都很普遍，且显著高于通用领域报告的水平。记忆可分为三类：有益记忆（准确回忆临床指南和生物医学参考文献）、无信息记忆（重复免责声明或模板化医学文档语言）、有害记忆（再生数据集特定或敏感临床内容）。

Conclusion: 基于研究发现，提出了实用建议：促进有益记忆以增强领域特定推理和事实准确性，最小化无信息记忆以促进超越表面模式的深度学习，减轻有害记忆以防止敏感或可识别患者信息的泄露。

Abstract: Large Language Models (LLMs) have demonstrated significant potential in
medicine. To date, LLMs have been widely applied to tasks such as diagnostic
assistance, medical question answering, and clinical information synthesis.
However, a key open question remains: to what extent do LLMs memorize medical
training data. In this study, we present the first comprehensive evaluation of
memorization of LLMs in medicine, assessing its prevalence (how frequently it
occurs), characteristics (what is memorized), volume (how much content is
memorized), and potential downstream impacts (how memorization may affect
medical applications). We systematically analyze common adaptation scenarios:
(1) continued pretraining on medical corpora, (2) fine-tuning on standard
medical benchmarks, and (3) fine-tuning on real-world clinical data, including
over 13,000 unique inpatient records from Yale New Haven Health System. The
results demonstrate that memorization is prevalent across all adaptation
scenarios and significantly higher than reported in the general domain.
Memorization affects both the development and adoption of LLMs in medicine and
can be categorized into three types: beneficial (e.g., accurate recall of
clinical guidelines and biomedical references), uninformative (e.g., repeated
disclaimers or templated medical document language), and harmful (e.g.,
regeneration of dataset-specific or sensitive clinical content). Based on these
findings, we offer practical recommendations to facilitate beneficial
memorization that enhances domain-specific reasoning and factual accuracy,
minimize uninformative memorization to promote deeper learning beyond
surface-level patterns, and mitigate harmful memorization to prevent the
leakage of sensitive or identifiable patient information.

</details>


### [25] [OTESGN:Optimal Transport Enhanced Syntactic-Semantic Graph Networks for Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2509.08612)
*Xinfeng Liao,Xuanqi Chen,Lianxi Wang,Jiahuan Yang,Zhuowei Chen,Ziying Rong*

Main category: cs.CL

TL;DR: 提出OTESGN模型，通过最优传输增强的语法-语义图网络，结合语法图注意力和语义最优传输注意力，在Aspect-based情感分析任务上取得了SOTA效果


<details>
  <summary>Details</summary>
Motivation: 现有基于语法树和aspect-aware注意力的方法难以建模复杂语义关系，线性点积特征无法捕获非线性关联，导致无关词汇的噪声相似性掩盖关键观点词

Method: OTESGN模型包含：1）语法图感知注意力挖掘潜在语法依赖和全局语法拓扑；2）语义最优传输注意力在文本噪声中发现细粒度语义对齐；3）自适应注意力融合模块整合异质特征；4）对比正则化提升鲁棒性

Result: 在Twitter数据集上F1提升1.01%，在Laptop14数据集上F1提升1.30%，达到state-of-the-art效果

Conclusion: OTESGN通过最优传输技术有效捕获被无关token掩盖的情感信号，在观点词精确定位和噪声抵抗方面表现出色

Abstract: Aspect-based sentiment analysis (ABSA) aims to identify aspect terms and
determine their sentiment polarity. While dependency trees combined with
contextual semantics effectively identify aspect sentiment, existing methods
relying on syntax trees and aspect-aware attention struggle to model complex
semantic relationships. Their dependence on linear dot-product features fails
to capture nonlinear associations, allowing noisy similarity from irrelevant
words to obscure key opinion terms. Motivated by Differentiable Optimal
Matching, we propose the Optimal Transport Enhanced Syntactic-Semantic Graph
Network (OTESGN), which introduces a Syntactic-Semantic Collaborative
Attention. It comprises a Syntactic Graph-Aware Attention for mining latent
syntactic dependencies and modeling global syntactic topology, as well as a
Semantic Optimal Transport Attention designed to uncover fine-grained semantic
alignments amidst textual noise, thereby accurately capturing sentiment signals
obscured by irrelevant tokens. A Adaptive Attention Fusion module integrates
these heterogeneous features, and contrastive regularization further improves
robustness. Experiments demonstrate that OTESGN achieves state-of-the-art
results, outperforming previous best models by +1.01% F1 on Twitter and +1.30%
F1 on Laptop14 benchmarks. Ablative studies and visual analyses corroborate its
efficacy in precise localization of opinion words and noise resistance.

</details>


### [26] [X-Teaming Evolutionary M2S: Automated Discovery of Multi-turn to Single-turn Jailbreak Templates](https://arxiv.org/abs/2509.08729)
*Hyunjun Kim,Junwoo Ha,Sangyoon Yu,Haon Park*

Main category: cs.CL

TL;DR: X-Teaming Evolutionary M2S是一个自动化框架，通过语言模型引导的进化来发现和优化M2S模板，将多轮红队测试压缩为单轮结构化提示，取得了44.8%的成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的M2S方法依赖少量手动编写的模板，需要自动化方法来发现和优化更有效的红队测试模板。

Method: 使用语言模型引导的进化框架，结合12个来源的智能采样和LLM作为评判者（受StrongREJECT启发），设置成功阈值θ=0.70进行选择压力。

Result: 获得了5个进化世代、2个新模板家族，在GPT-4.1上达到44.8%总体成功率（103/230）。跨模型评估显示结构增益可迁移但因目标而异，发现提示长度与得分正相关。

Conclusion: 结构级搜索是获得更强单轮探测的可重复途径，强调了阈值校准和跨模型评估的重要性。

Abstract: Multi-turn-to-single-turn (M2S) compresses iterative red-teaming into one
structured prompt, but prior work relied on a handful of manually written
templates. We present X-Teaming Evolutionary M2S, an automated framework that
discovers and optimizes M2S templates through language-model-guided evolution.
The system pairs smart sampling from 12 sources with an LLM-as-judge inspired
by StrongREJECT and records fully auditable logs.
  Maintaining selection pressure by setting the success threshold to $\theta =
0.70$, we obtain five evolutionary generations, two new template families, and
44.8% overall success (103/230) on GPT-4.1. A balanced cross-model panel of
2,500 trials (judge fixed) shows that structural gains transfer but vary by
target; two models score zero at the same threshold. We also find a positive
coupling between prompt length and score, motivating length-aware judging.
  Our results demonstrate that structure-level search is a reproducible route
to stronger single-turn probes and underscore the importance of threshold
calibration and cross-model evaluation. Code, configurations, and artifacts are
available at https://github.com/hyunjun1121/M2S-x-teaming.

</details>


### [27] [Streaming Sequence-to-Sequence Learning with Delayed Streams Modeling](https://arxiv.org/abs/2509.08753)
*Neil Zeghidour,Eugene Kharitonov,Manu Orsini,Václav Volhejn,Gabriel de Marmiesse,Edouard Grave,Patrick Pérez,Laurent Mazaré,Alexandre Défossez*

Main category: cs.CL

TL;DR: DSM是一种流式多模态序列到序列学习框架，通过引入延迟对齐机制，在解码器语言模型中实现任意输出序列的流式推理，在ASR和TTS任务上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 传统序列到序列生成要么是离线方式（完整输入后输出），要么需要学习复杂的流控策略。DSM旨在提供更灵活、高效的流式多模态序列处理方案。

Method: 将时间对齐移至预处理步骤，在流之间引入适当延迟，使用解码器语言模型处理已对齐的流，支持任意输入组合的流式推理。

Result: 在自动语音识别和文本到语音任务上实现了最先进的性能和延迟，支持任意长序列，甚至可与离线基线竞争。

Conclusion: DSM为多模态序列到序列学习提供了灵活高效的流式解决方案，在保持高性能的同时显著降低了延迟。

Abstract: We introduce Delayed Streams Modeling (DSM), a flexible formulation for
streaming, multimodal sequence-to-sequence learning. Sequence-to-sequence
generation is often cast in an offline manner, where the model consumes the
complete input sequence before generating the first output timestep.
Alternatively, streaming sequence-to-sequence rely on learning a policy for
choosing when to advance on the input stream, or write to the output stream.
DSM instead models already time-aligned streams with a decoder-only language
model. By moving the alignment to a pre-processing step,and introducing
appropriate delays between streams, DSM provides streaming inference of
arbitrary output sequences, from any input combination, making it applicable to
many sequence-to-sequence problems. In particular, given text and audio
streams, automatic speech recognition (ASR) corresponds to the text stream
being delayed, while the opposite gives a text-to-speech (TTS) model. We
perform extensive experiments for these two major sequence-to-sequence tasks,
showing that DSM provides state-of-the-art performance and latency while
supporting arbitrary long sequences, being even competitive with offline
baselines. Code, samples and demos are available at
https://github.com/kyutai-labs/delayed-streams-modeling

</details>


### [28] [Do All Autoregressive Transformers Remember Facts the Same Way? A Cross-Architecture Analysis of Recall Mechanisms](https://arxiv.org/abs/2509.08778)
*Minyeong Choe,Haehyun Cho,Changho Seo,Hyunil Kim*

Main category: cs.CL

TL;DR: 研究发现Qwen模型在事实召回机制上与其他自回归Transformer模型不同，早期层的注意力模块比MLP模块贡献更大，表明架构差异会导致不同的事实召回机制。


<details>
  <summary>Details</summary>
Motivation: 理解Transformer语言模型如何存储和检索事实关联对提高可解释性和实现针对性模型编辑至关重要，但先前研究主要基于GPT风格模型，不清楚这些发现是否适用于不同自回归架构。

Method: 对多个模型（GPT、LLaMA、Qwen、DeepSeek）进行事实召回的综合评估，分析事实信息在何处以及如何被编码和访问。

Result: 发现基于Qwen的模型行为与先前模式不同：最早层的注意力模块比MLP模块对事实召回的贡献更大。

Conclusion: 即使在自回归Transformer家族内，架构变化也会导致根本不同的事实召回机制。

Abstract: Understanding how Transformer-based language models store and retrieve
factual associations is critical for improving interpretability and enabling
targeted model editing. Prior work, primarily on GPT-style models, has
identified MLP modules in early layers as key contributors to factual recall.
However, it remains unclear whether these findings generalize across different
autoregressive architectures. To address this, we conduct a comprehensive
evaluation of factual recall across several models -- including GPT, LLaMA,
Qwen, and DeepSeek -- analyzing where and how factual information is encoded
and accessed. Consequently, we find that Qwen-based models behave differently
from previous patterns: attention modules in the earliest layers contribute
more to factual recall than MLP modules. Our findings suggest that even within
the autoregressive Transformer family, architectural variations can lead to
fundamentally different mechanisms of factual recall.

</details>


### [29] [Evaluating LLMs Without Oracle Feedback: Agentic Annotation Evaluation Through Unsupervised Consistency Signals](https://arxiv.org/abs/2509.08809)
*Cheng Chen,Haiyan Yin,Ivor Tsang*

Main category: cs.CL

TL;DR: 提出无监督评估指标CAI比率，通过学生模型与噪声教师协作来评估大语言模型注释质量，无需人工标注


<details>
  <summary>Details</summary>
Motivation: 解决在动态无监督环境中评估LLM注释质量的挑战，因为缺乏真实标签和传统方法失效

Method: 学生模型与噪声教师(LLM)协作，采用用户偏好多数投票策略评估注释一致性，提出CAI比率指标

Result: 在10个NLP数据集和4个LLM上实验，CAI比率与LLM准确率呈强正相关关系

Conclusion: CAI比率是一个有效的无监督评估工具，可用于动态环境中的模型选择和质量评估

Abstract: Large Language Models (LLMs), when paired with prompt-based tasks, have
significantly reduced data annotation costs and reliance on human annotators.
However, evaluating the quality of their annotations remains challenging in
dynamic, unsupervised environments where oracle feedback is scarce and
conventional methods fail. To address this challenge, we propose a novel
agentic annotation paradigm, where a student model collaborates with a noisy
teacher (the LLM) to assess and refine annotation quality without relying on
oracle feedback. The student model, acting as an unsupervised feedback
mechanism, employs a user preference-based majority voting strategy to evaluate
the consistency of the LLM outputs. To systematically measure the reliability
of LLM-generated annotations, we introduce the Consistent and Inconsistent
(CAI) Ratio, a novel unsupervised evaluation metric. The CAI Ratio not only
quantifies the annotation quality of the noisy teacher under limited user
preferences but also plays a critical role in model selection, enabling the
identification of robust LLMs in dynamic, unsupervised environments. Applied to
ten open-domain NLP datasets across four LLMs, the CAI Ratio demonstrates a
strong positive correlation with LLM accuracy, establishing it as an essential
tool for unsupervised evaluation and model selection in real-world settings.

</details>


### [30] [MoVoC: Morphology-Aware Subword Construction for Geez Script Languages](https://arxiv.org/abs/2509.08812)
*Hailay Kidu Teklehaymanot,Dren Fazlija,Wolfgang Nejdl*

Main category: cs.CL

TL;DR: MoVoC是一种结合形态分析和BPE的分词方法，专门针对吉兹文字的低资源复杂形态语言，通过保留形态边界来提高语言保真度和分词效率。


<details>
  <summary>Details</summary>
Motivation: 现有的子词分词方法在低资源、形态复杂的语言（如吉兹文字语言）中往往无法保留形态边界，这限制了模型的语言理解能力。

Method: 提出MoVoC方法，将监督式形态分析整合到子词词汇表中，结合基于语素和BPE的分词方式，构建混合分割方法。同时为四种吉兹文字语言创建了手动标注的语素数据。

Result: 虽然在机器翻译质量上没有显著提升，但在内在评估指标MorphoScore和Boundary Precision上表现一致改善，证明了形态感知分词在增强语言保真度和分词效率方面的价值。

Conclusion: 形态感知的分词方法对低资源复杂形态语言具有重要意义，发布的语素标注数据集和分词器将支持后续研究。

Abstract: Subword-based tokenization methods often fail to preserve morphological
boundaries, a limitation especially pronounced in low-resource, morphologically
complex languages such as those written in the Geez script. To address this, we
present MoVoC (Morpheme-aware Subword Vocabulary Construction) and train
MoVoC-Tok, a tokenizer that integrates supervised morphological analysis into
the subword vocabulary. This hybrid segmentation approach combines
morpheme-based and Byte Pair Encoding (BPE) tokens to preserve morphological
integrity while maintaining lexical meaning. To tackle resource scarcity, we
curate and release manually annotated morpheme data for four Geez script
languages and a morpheme-aware vocabulary for two of them. While the proposed
tokenization method does not lead to significant gains in automatic translation
quality, we observe consistent improvements in intrinsic metrics, MorphoScore,
and Boundary Precision, highlighting the value of morphology-aware segmentation
in enhancing linguistic fidelity and token efficiency. Our morpheme-annotated
datasets and tokenizer will be publicly available to support further research
in low-resource, morphologically rich languages. Our code and data are
available on GitHub: https://github.com/hailaykidu/MoVoC

</details>


### [31] [Building High-Quality Datasets for Portuguese LLMs: From Common Crawl Snapshots to Industrial-Grade Corpora](https://arxiv.org/abs/2509.08824)
*Thales Sales Almeida,Rodrigo Nogueira,Helio Pedrini*

Main category: cs.CL

TL;DR: 通过可扩展的网络数据收集方法，构建了120B标记的葡萄牙语语料库，在多语言LLM训练中实现竞争力表现，验证了语言特定数据筛选管道的重要性


<details>
  <summary>Details</summary>
Motivation: 解决非英语语言模型训练语料库构建方面的研究空白，探索如何为其他语言构建高效的训练语料集

Method: 采用可扩展的网络数据收集方法，构建8461萄牙语120B标记语料库，通过持续预训练设置研究不同数据选择和预处理策略对模型性能的影响，包括语言特定筛选管道和有害内容识别

Result: 构建的葡萄牙语语料库达到了与商业级语料库相竞争的结果，适应目标语言的模型在性能上有显著提升，验证了语言特定筛选管道的效果

Conclusion: 高质量的语言特定数据对多语言LLM发展至关重要，研究方法可扩展到其他语言，为多语言模型开发提供了有价值的见解

Abstract: The performance of large language models (LLMs) is deeply influenced by the
quality and composition of their training data. While much of the existing work
has centered on English, there remains a gap in understanding how to construct
effective training corpora for other languages. We explore scalable methods for
building web-based corpora for LLMs. We apply them to build a new 120B token
corpus in Portuguese that achieves competitive results to an industrial-grade
corpus. Using a continual pretraining setup, we study how different data
selection and preprocessing strategies affect LLM performance when
transitioning a model originally trained in English to another language. Our
findings demonstrate the value of language-specific filtering pipelines,
including classifiers for education, science, technology, engineering, and
mathematics (STEM), as well as toxic content. We show that adapting a model to
the target language leads to performance improvements, reinforcing the
importance of high-quality, language-specific data. While our case study
focuses on Portuguese, our methods are applicable to other languages, offering
insights for multilingual LLM development.

</details>


### [32] [Large Language Model Hacking: Quantifying the Hidden Risks of Using LLMs for Text Annotation](https://arxiv.org/abs/2509.08825)
*Joachim Baumann,Paul Röttger,Aleksandra Urman,Albert Wendsjö,Flor Miriam Plaza-del-Arco,Johannes B. Gruber,Dirk Hovy*

Main category: cs.CL

TL;DR: LLM在社会科学研究中存在严重的数据标注风险，研究者不同的实现选择会导致系统性偏差和随机错误，约1/3的假设会得出错误结论，即使最先进的模型也无法完全消除风险。


<details>
  <summary>Details</summary>
Motivation: 量化LLM在社会科学数据标注任务中的风险，揭示不同模型选择、提示策略等实现选择对统计结论的系统性影响。

Method: 复制37个来自21项已发表研究的数据标注任务，使用18个不同模型生成1300万条标注，测试2361个现实假设来测量研究者选择对统计结论的影响。

Result: 先进模型约1/3的假设得出错误结论，小语言模型错误率高达一半；效应值越大风险越小；人类标注能有效减少假阳性发现；常用回归校正技术效果有限。

Conclusion: LLM hacking风险严重且普遍存在，需要更严格的验证机制，特别是对接近显著性阈值的研究发现；故意操纵LLM得出统计显著性结果异常简单。

Abstract: Large language models (LLMs) are rapidly transforming social science research
by enabling the automation of labor-intensive tasks like data annotation and
text analysis. However, LLM outputs vary significantly depending on the
implementation choices made by researchers (e.g., model selection, prompting
strategy, or temperature settings). Such variation can introduce systematic
biases and random errors, which propagate to downstream analyses and cause Type
I, Type II, Type S, or Type M errors. We call this LLM hacking.
  We quantify the risk of LLM hacking by replicating 37 data annotation tasks
from 21 published social science research studies with 18 different models.
Analyzing 13 million LLM labels, we test 2,361 realistic hypotheses to measure
how plausible researcher choices affect statistical conclusions. We find
incorrect conclusions based on LLM-annotated data in approximately one in three
hypotheses for state-of-the-art models, and in half the hypotheses for small
language models. While our findings show that higher task performance and
better general model capabilities reduce LLM hacking risk, even highly accurate
models do not completely eliminate it. The risk of LLM hacking decreases as
effect sizes increase, indicating the need for more rigorous verification of
findings near significance thresholds. Our extensive analysis of LLM hacking
mitigation techniques emphasizes the importance of human annotations in
reducing false positive findings and improving model selection. Surprisingly,
common regression estimator correction techniques are largely ineffective in
reducing LLM hacking risk, as they heavily trade off Type I vs. Type II errors.
  Beyond accidental errors, we find that intentional LLM hacking is
unacceptably simple. With few LLMs and just a handful of prompt paraphrases,
anything can be presented as statistically significant.

</details>


### [33] [A Survey of Reinforcement Learning for Large Reasoning Models](https://arxiv.org/abs/2509.08827)
*Kaiyan Zhang,Yuxin Zuo,Bingxiang He,Youbang Sun,Runze Liu,Che Jiang,Yuchen Fan,Kai Tian,Guoli Jia,Pengfei Li,Yu Fu,Xingtai Lv,Yuchen Zhang,Sihang Zeng,Shang Qu,Haozhan Li,Shijie Wang,Yuru Wang,Xinwei Long,Fangfu Liu,Xiang Xu,Jiaze Ma,Xuekai Zhu,Ermo Hua,Yihao Liu,Zonglin Li,Huayu Chen,Xiaoye Qu,Yafu Li,Weize Chen,Zhenzhao Yuan,Junqi Gao,Dong Li,Zhiyuan Ma,Ganqu Cui,Zhiyuan Liu,Biqing Qi,Ning Ding,Bowen Zhou*

Main category: cs.CL

TL;DR: 调查大语言模型中强化学习在逻辑推理任务中的最新进展和挑战


<details>
  <summary>Details</summary>
Motivation: 强化学习在提升大语言模型逻辑推理能力方面取得了显著成效，但在进一步扩展时面临计算资源、算法设计等基础挑战，需要重新评估领域发展轨迹

Method: 通过综述性调研方法，分析强化学习在大语言模型和逻辑推理模型中的应用，包括基础组件、核心问题、训练资源和下游应用

Result: 系统性总结了该领域的研究进展，尤其是DeepSeek-R1发布以来的成果，为推动RL向人工超智能扩展提供了技术路径

Conclusion: 强化学习已成为将大语言模型转化为逻辑推理模型的基础方法，本评论将促进该领域的未来研究和更广泛的推理模型发展

Abstract: In this paper, we survey recent advances in Reinforcement Learning (RL) for
reasoning with Large Language Models (LLMs). RL has achieved remarkable success
in advancing the frontier of LLM capabilities, particularly in addressing
complex logical tasks such as mathematics and coding. As a result, RL has
emerged as a foundational methodology for transforming LLMs into LRMs. With the
rapid progress of the field, further scaling of RL for LRMs now faces
foundational challenges not only in computational resources but also in
algorithm design, training data, and infrastructure. To this end, it is timely
to revisit the development of this domain, reassess its trajectory, and explore
strategies to enhance the scalability of RL toward Artificial SuperIntelligence
(ASI). In particular, we examine research applying RL to LLMs and LRMs for
reasoning abilities, especially since the release of DeepSeek-R1, including
foundational components, core problems, training resources, and downstream
applications, to identify future opportunities and directions for this rapidly
evolving area. We hope this review will promote future research on RL for
broader reasoning models. Github:
https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [34] [Calibrating MLLM-as-a-judge via Multimodal Bayesian Prompt Ensembles](https://arxiv.org/abs/2509.08777)
*Eric Slyman,Mehrab Tanjim,Kushal Kafle,Stefan Lee*

Main category: cs.CV

TL;DR: 提出MMB方法，通过贝叶斯提示集成和图像聚类改进多模态大语言模型在文本到图像生成评估中的准确性和校准性


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型作为评估工具存在偏见、过度自信和跨域性能不一致的问题，标准提示集成方法在文本到图像任务中效果不佳

Method: Multimodal Mixture-of-Bayesian Prompt Ensembles (MMB)，结合贝叶斯提示集成和图像聚类，根据图像视觉特征动态分配提示权重

Result: 在HPSv2和MJBench基准测试中，MMB在成对偏好判断准确性和校准性方面优于现有基线，与人类标注更一致

Conclusion: 多模态特定策略对评估模型校准至关重要，MMB为可靠的大规模文本到图像评估提供了有前景的解决方案

Abstract: Multimodal large language models (MLLMs) are increasingly used to evaluate
text-to-image (TTI) generation systems, providing automated judgments based on
visual and textual context. However, these "judge" models often suffer from
biases, overconfidence, and inconsistent performance across diverse image
domains. While prompt ensembling has shown promise for mitigating these issues
in unimodal, text-only settings, our experiments reveal that standard
ensembling methods fail to generalize effectively for TTI tasks. To address
these limitations, we propose a new multimodal-aware method called Multimodal
Mixture-of-Bayesian Prompt Ensembles (MMB). Our method uses a Bayesian prompt
ensemble approach augmented by image clustering, allowing the judge to
dynamically assign prompt weights based on the visual characteristics of each
sample. We show that MMB improves accuracy in pairwise preference judgments and
greatly enhances calibration, making it easier to gauge the judge's true
uncertainty. In evaluations on two TTI benchmarks, HPSv2 and MJBench, MMB
outperforms existing baselines in alignment with human annotations and
calibration across varied image content. Our findings highlight the importance
of multimodal-specific strategies for judge calibration and suggest a promising
path forward for reliable large-scale TTI evaluation.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [35] [Scaling Truth: The Confidence Paradox in AI Fact-Checking](https://arxiv.org/abs/2509.08803)
*Ihsan A. Qazi,Zohaib Khan,Abdullah Ghani,Agha A. Raza,Zafar A. Qazi,Wassay Sajjad,Ayesha Ali,Asher Javaid,Muhammad Abdullah Sohail,Abdul H. Azeemi*

Main category: cs.SI

TL;DR: 本研究系统评估了9个大型语言模型在事实核查任务中的表现，发现小模型存在高自信低准确度的邓宁-克鲁格效应，性能差距在非英语和全球南方地区尤为明显。


<details>
  <summary>Details</summary>
Motivation: 随着虚假信息的泛滥，需要可扩展且可靠的事实核查解决方案。大型语言模型在自动化事实核查方面具有潜力，但其在全球范围内的有效性仍不确定。

Method: 使用5,000个由174个专业事实核查组织评估过的多语言声明，测试9个不同类别的大型语言模型，包括开源/闭源、不同规模、多样化架构和基于推理的模型，采用4种提示策略，以超过24万个人工标注作为基准。

Result: 发现类似邓宁-克鲁格效应的模式：小型可访问模型表现出高自信但准确度低，而大型模型准确度高但自信度低。性能差距在非英语语言和来自全球南方的声明中最为明显。

Conclusion: 研究结果为未来多语言研究建立了基准，并为确保公平获取可信赖AI辅助事实核查的政策提供了证据基础，揭示了可能加剧现有信息不平等的系统性偏见风险。

Abstract: The rise of misinformation underscores the need for scalable and reliable
fact-checking solutions. Large language models (LLMs) hold promise in
automating fact verification, yet their effectiveness across global contexts
remains uncertain. We systematically evaluate nine established LLMs across
multiple categories (open/closed-source, multiple sizes, diverse architectures,
reasoning-based) using 5,000 claims previously assessed by 174 professional
fact-checking organizations across 47 languages. Our methodology tests model
generalizability on claims postdating training cutoffs and four prompting
strategies mirroring both citizen and professional fact-checker interactions,
with over 240,000 human annotations as ground truth. Findings reveal a
concerning pattern resembling the Dunning-Kruger effect: smaller, accessible
models show high confidence despite lower accuracy, while larger models
demonstrate higher accuracy but lower confidence. This risks systemic bias in
information verification, as resource-constrained organizations typically use
smaller models. Performance gaps are most pronounced for non-English languages
and claims originating from the Global South, threatening to widen existing
information inequalities. These results establish a multilingual benchmark for
future research and provide an evidence base for policy aimed at ensuring
equitable access to trustworthy, AI-assisted fact-checking.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [36] [EvolKV: Evolutionary KV Cache Compression for LLM Inference](https://arxiv.org/abs/2509.08315)
*Bohan Yu,Yekun Chai*

Main category: cs.LG

TL;DR: EvolKV是一个自适应的KV缓存压缩框架，通过进化搜索动态配置各层缓存预算，在保持任务性能的同时显著提升内存效率


<details>
  <summary>Details</summary>
Motivation: 现有的KV缓存压缩方法依赖启发式策略，忽略了层间特征模式与任务性能的关键交互，导致泛化性能下降

Method: 将缓存分配重新表述为多目标优化问题，利用进化搜索动态配置层预算，直接最大化下游任务性能

Result: 在11个任务上超越所有基线方法，在GSM8K上比启发式基线高出7个百分点，仅用1.5%原始预算就在代码补全任务上超越完整KV缓存性能

Conclusion: 学习式KV缓存压缩策略具有巨大潜力，EvolKV展示了在内存效率和任务性能之间实现更好平衡的可能性

Abstract: Existing key-value (KV) cache compression methods typically rely on
heuristics, such as uniform cache allocation across layers or static eviction
policies, however, they ignore the critical interplays among layer-specific
feature patterns and task performance, which can lead to degraded
generalization. In this paper, we propose EvolKV, an adaptive framework for
layer-wise, task-driven KV cache compression that jointly optimizes the memory
efficiency and task performance. By reformulating cache allocation as a
multi-objective optimization problem, EvolKV leverages evolutionary search to
dynamically configure layer budgets while directly maximizing downstream
performance. Extensive experiments on 11 tasks demonstrate that our approach
outperforms all baseline methods across a wide range of KV cache budgets on
long-context tasks and surpasses heuristic baselines by up to 7 percentage
points on GSM8K. Notably, EvolKV achieves superior performance over the full KV
cache setting on code completion while utilizing only 1.5% of the original
budget, suggesting the untapped potential in learned compression strategies for
KV cache budget allocation.

</details>


### [37] [Generative Data Refinement: Just Ask for Better Data](https://arxiv.org/abs/2509.08653)
*Minqi Jiang,João G. M. Araújo,Will Ellsworth,Sian Gooding,Edward Grefenstette*

Main category: cs.LG

TL;DR: 通过生成式数据精炼框架(GDR)，利用预训练生成模型将包含不良内容的数据集转换为更适合训练的精炼数据集，解决训练数据尺寸不足的挑战


<details>
  <summary>Details</summary>
Motivation: 随着大模型训练数据需求快速增长，公开网络数据将在未来十年内耗尽，而用户生成内容虽然丰富但包含隐私泄漏和不良内容风险

Method: 提出Generative Data Refinement (GDR)框架，利用预训练生成模型对每个真实数据进行条件生成，生成合成数据替代原始数据，同时保持数据集的多样性

Result: GDR在数据集匿名化方面超过了行业级解决方案，能够直接对高度不安全数据集进行毒性消除，生成的合成数据自然匹配网络规模数据集的多样性

Conclusion: GDR框架简单有效，为前沿模型扩大训练数据总量提供了强大工具，能够利用用户生成内容同时避免隐私泄漏和不良内容风险

Abstract: For a fixed parameter size, the capabilities of large models are primarily
determined by the quality and quantity of its training data. Consequently,
training datasets now grow faster than the rate at which new data is indexed on
the web, leading to projected data exhaustion over the next decade. Much more
data exists as user-generated content that is not publicly indexed, but
incorporating such data comes with considerable risks, such as leaking private
information and other undesirable content. We introduce a framework, Generative
Data Refinement (GDR), for using pretrained generative models to transform a
dataset with undesirable content into a refined dataset that is more suitable
for training. Our experiments show that GDR can outperform industry-grade
solutions for dataset anonymization, as well as enable direct detoxification of
highly unsafe datasets. Moreover, we show that by generating synthetic data
that is conditioned on each example in the real dataset, GDR's refined outputs
naturally match the diversity of web scale datasets, and thereby avoid the
often challenging task of generating diverse synthetic data via model
prompting. The simplicity and effectiveness of GDR make it a powerful tool for
scaling up the total stock of training data for frontier models.

</details>


### [38] [AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making through Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2509.08755)
*Zhiheng Xi,Jixuan Huang,Chenyang Liao,Baodai Huang,Honglin Guo,Jiaqi Liu,Rui Zheng,Junjie Ye,Jiazheng Zhang,Wenxiang Chen,Wei He,Yiwen Ding,Guanyu Li,Zehui Chen,Zhengyin Du,Xuesong Yao,Yufei Xu,Jiecao Chen,Tao Gui,Zuxuan Wu,Qi Zhang,Xuanjing Huang,Yu-Gang Jiang*

Main category: cs.LG

TL;DR: 提出了AgentGym-RL框架和ScalingInter-RL训练方法，通过强化学习从零开始训练LLM智能体进行多轮交互决策，在27个任务上达到或超越商业模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏统一的交互式强化学习框架来训练LLM智能体从零开始解决复杂现实任务，而不依赖监督微调。

Method: 开发了模块化、解耦的AgentGym-RL框架，支持主流RL算法；提出了ScalingInter-RL训练方法，通过逐步增加交互范围来平衡探索与利用。

Result: 在多样化环境的27个任务上，训练的智能体性能匹配或超越了商业模型，证明了框架的稳定性和有效性。

Conclusion: 该框架为研究社区提供了强大的工具，将开源代码和数据集以推动下一代智能体的发展。

Abstract: Developing autonomous LLM agents capable of making a series of intelligent
decisions to solve complex, real-world tasks is a fast-evolving frontier. Like
human cognitive development, agents are expected to acquire knowledge and
skills through exploration and interaction with the environment. Despite
advances, the community still lacks a unified, interactive reinforcement
learning (RL) framework that can effectively train such agents from scratch --
without relying on supervised fine-tuning (SFT) -- across diverse and realistic
environments. To bridge this gap, we introduce AgentGym-RL, a new framework to
train LLM agents for multi-turn interactive decision-making through RL. The
framework features a modular and decoupled architecture, ensuring high
flexibility and extensibility. It encompasses a wide variety of real-world
scenarios, and supports mainstream RL algorithms. Furthermore, we propose
ScalingInter-RL, a training approach designed for exploration-exploitation
balance and stable RL optimization. In early stages, it emphasizes exploitation
by restricting the number of interactions, and gradually shifts towards
exploration with larger horizons to encourage diverse problem-solving
strategies. In this way, the agent develops more diverse behaviors and is less
prone to collapse under long horizons. We perform extensive experiments to
validate the stability and effectiveness of both the AgentGym-RL framework and
the ScalingInter-RL approach. Our agents match or surpass commercial models on
27 tasks across diverse environments. We offer key insights and will
open-source the complete AgentGym-RL framework -- including code and datasets
-- to empower the research community in developing the next generation of
intelligent agents.

</details>


### [39] [Merge-of-Thought Distillation](https://arxiv.org/abs/2509.08814)
*Zhanming Shen,Zeyu Qin,Zenan Huang,Hao Chen,Jiaqi Hu,Yihong Zhuang,Guoshan Lu,Gang Chen,Junbo Zhao*

Main category: cs.LG

TL;DR: MoT是一个轻量级框架，通过交替进行教师特定的监督微调分支和权重空间合并，将多个教师的推理能力统一到学生模型中，解决了多教师蒸馏中的冲突问题。


<details>
  <summary>Details</summary>
Motivation: 传统推理蒸馏假设单一最优教师，但实际中存在多个候选教师和大量CoT语料。研究发现不同学生有不同"最佳教师"，同一学生的最佳教师在不同数据集上也会变化。

Method: 提出Merge-of-Thought Distillation (MoT)框架：交替进行教师特定的监督微调分支，然后对产生的学生变体进行权重空间合并，以统一多个教师的推理能力。

Result: 在数学竞赛基准测试中，仅使用约200个高质量CoT样本，Qwen3-14B学生模型超越了多个强大模型，性能显著提升。MoT始终优于最佳单教师蒸馏和朴素多教师联合方法。

Conclusion: MoT是一种简单、可扩展的方法，能够有效地将多样教师的长期CoT能力蒸馏到紧凑的学生模型中，同时减轻过拟合，提高泛化推理能力。

Abstract: Efficient reasoning distillation for long chain-of-thought (CoT) models is
increasingly constrained by the assumption of a single oracle teacher, despite
practical availability of multiple candidate teachers and growing CoT corpora.
We revisit teacher selection and observe that different students have different
"best teachers," and even for the same student the best teacher can vary across
datasets. Therefore, to unify multiple teachers' reasoning abilities into
student with overcoming conflicts among various teachers' supervision, we
propose Merge-of-Thought Distillation (MoT), a lightweight framework that
alternates between teacher-specific supervised fine-tuning branches and
weight-space merging of the resulting student variants. On competition math
benchmarks, using only about 200 high-quality CoT samples, applying MoT to a
Qwen3-14B student surpasses strong models including DEEPSEEK-R1, QWEN3-30B-A3B,
QWEN3-32B, and OPENAI-O1, demonstrating substantial gains. Besides, MoT
consistently outperforms the best single-teacher distillation and the naive
multi-teacher union, raises the performance ceiling while mitigating
overfitting, and shows robustness to distribution-shifted and peer-level
teachers. Moreover, MoT reduces catastrophic forgetting, improves general
reasoning beyond mathematics and even cultivates a better teacher, indicating
that consensus-filtered reasoning features transfer broadly. These results
position MoT as a simple, scalable route to efficiently distilling long CoT
capabilities from diverse teachers into compact students.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [40] [Measuring and mitigating overreliance is necessary for building human-compatible AI](https://arxiv.org/abs/2509.08010)
*Lujain Ibrahim,Katherine M. Collins,Sunnie S. Y. Kim,Anka Reuel,Max Lamparth,Kevin Feng,Lama Ahmad,Prajna Soni,Alia El Kattan,Merlin Stein,Siddharth Swaroop,Ilia Sucholutsky,Andrew Strait,Q. Vera Liao,Umang Bhatt*

Main category: cs.CY

TL;DR: 这篇位置文程讨论了大语言模型导致的过度依赖风险，建议将测量和缓解过度依赖作为LLM研究和部署的核心任务


<details>
  <summary>Details</summary>
Motivation: 随着LLM在医疗健康等重要领域影响力增强，过度依赖的风险日益增长，需要重点关注这一问题

Method: 整合分析了个体和社会层面的风险，探讨了LLM特性、系统设计和用户偏见导致过度依赖的因素，并提出了测量方法的改进方向

Result: 识别了过度依赖的三个重要测量空白，提出了三个有前景的测量改进方向，以及缓解策略

Conclusion: 建议AI研究社区将测量和缓解过度依赖作为核心任务，确保LLM增强而非削弱人类能力

Abstract: Large language models (LLMs) distinguish themselves from previous
technologies by functioning as collaborative "thought partners," capable of
engaging more fluidly in natural language. As LLMs increasingly influence
consequential decisions across diverse domains from healthcare to personal
advice, the risk of overreliance - relying on LLMs beyond their capabilities -
grows. This position paper argues that measuring and mitigating overreliance
must become central to LLM research and deployment. First, we consolidate risks
from overreliance at both the individual and societal levels, including
high-stakes errors, governance challenges, and cognitive deskilling. Then, we
explore LLM characteristics, system design features, and user cognitive biases
that - together - raise serious and unique concerns about overreliance in
practice. We also examine historical approaches for measuring overreliance,
identifying three important gaps and proposing three promising directions to
improve measurement. Finally, we propose mitigation strategies that the AI
research community can pursue to ensure LLMs augment rather than undermine
human capabilities.

</details>


### [41] [HumanAgencyBench: Scalable Evaluation of Human Agency Support in AI Assistants](https://arxiv.org/abs/2509.08494)
*Benjamin Sturgeon,Daniel Samuelson,Jacob Haimes,Jacy Reese Anthis*

Main category: cs.CY

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: As humans delegate more tasks and decisions to artificial intelligence (AI),
we risk losing control of our individual and collective futures. Relatively
simple algorithmic systems already steer human decision-making, such as social
media feed algorithms that lead people to unintentionally and absent-mindedly
scroll through engagement-optimized content. In this paper, we develop the idea
of human agency by integrating philosophical and scientific theories of agency
with AI-assisted evaluation methods: using large language models (LLMs) to
simulate and validate user queries and to evaluate AI responses. We develop
HumanAgencyBench (HAB), a scalable and adaptive benchmark with six dimensions
of human agency based on typical AI use cases. HAB measures the tendency of an
AI assistant or agent to Ask Clarifying Questions, Avoid Value Manipulation,
Correct Misinformation, Defer Important Decisions, Encourage Learning, and
Maintain Social Boundaries. We find low-to-moderate agency support in
contemporary LLM-based assistants and substantial variation across system
developers and dimensions. For example, while Anthropic LLMs most support human
agency overall, they are the least supportive LLMs in terms of Avoid Value
Manipulation. Agency support does not appear to consistently result from
increasing LLM capabilities or instruction-following behavior (e.g., RLHF), and
we encourage a shift towards more robust safety and alignment targets.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [42] [XML Prompting as Grammar-Constrained Interaction: Fixed-Point Semantics, Convergence Guarantees, and Human-AI Protocols](https://arxiv.org/abs/2509.08182)
*Faruk Alpay,Taylan Alpay*

Main category: cs.PL

TL;DR: 本文提出了基于XML标签的结构化提示方法，通过格论和不动点理论形式化分析了提示工程的数学基础，证明了在精炼序下的收敛性，并结合上下文无关文法保证输出的良好格式和任务性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决大语言模型输出格式不可控的问题，需要开发一种能够产生可解析、符合模式的结构化输出的提示方法，同时建立严格的数学理论基础来指导实际应用。

Method: 采用基于XML标签的结构化提示方法，构建XML树的完全格结构，应用Knaster-Tarski不动点定理和Banach压缩映射原理，结合上下文无关文法进行约束解码。

Result: 建立了完整的数学理论框架，证明了提示操作的不动点存在性和迭代收敛性，开发了多层人机交互模式，包括多轮验证修订流程和工具使用机制。

Conclusion: 结构化XML提示方法为可控文本生成提供了坚实的数学基础，能够保证输出的格式正确性和任务性能，为人机协作系统提供了有效的部署模式。

Abstract: Structured prompting with XML tags has emerged as an effective way to steer
large language models (LLMs) toward parseable, schema-adherent outputs in
real-world systems. We develop a logic-first treatment of XML prompting that
unifies (i) grammar-constrained decoding, (ii) fixed-point semantics over
lattices of hierarchical prompts, and (iii) convergent human-AI interaction
loops. We formalize a complete lattice of XML trees under a refinement order
and prove that monotone prompt-to-prompt operators admit least fixed points
(Knaster-Tarski) that characterize steady-state protocols; under a task-aware
contraction metric on trees, we further prove Banach-style convergence of
iterative guidance. We instantiate these results with context-free grammars
(CFGs) for XML schemas and show how constrained decoding guarantees
well-formedness while preserving task performance. A set of multi-layer
human-AI interaction recipes demonstrates practical deployment patterns,
including multi-pass "plan $\to$ verify $\to$ revise" routines and agentic tool
use. We provide mathematically complete proofs and tie our framework to recent
advances in grammar-aligned decoding, chain-of-verification, and programmatic
prompting.

</details>
