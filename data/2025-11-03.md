<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 44]
- [cs.RO](#cs.RO) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.LG](#cs.LG) [Total: 6]
- [cs.CV](#cs.CV) [Total: 3]
- [cs.AI](#cs.AI) [Total: 6]
- [cs.CR](#cs.CR) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Understanding and Enhancing Mamba-Transformer Hybrids for Memory Recall and Language Modeling](https://arxiv.org/abs/2510.26912)
*Hyunji Lee,Wenhao Yu,Hongming Zhang,Kaixin Ma,Jiyeon Kim,Dong Yu,Minjoon Seo*

Main category: cs.CL

TL;DR: 本文分析了混合状态空间模型与注意力机制的设计选择，发现序列混合在短上下文表现更好，并行混合在长上下文更有效，并提出通过数据增强训练来提升召回能力的方法。


<details>
  <summary>Details</summary>
Motivation: 混合模型结合了状态空间模型的高效性和注意力机制的高召回能力，但其架构设计选择尚未被充分理解，需要系统分析来指导实际应用。

Method: 分析序列与并行集成SSM和注意力层的区别，引入基于数据增强的持续训练方法，使用包含释义的数据集来增强模型能力。

Result: 序列混合在短上下文表现更好，并行混合在长上下文更有效；数据增强方法能有效提升召回能力且泛化性好，优于架构修改方法。

Conclusion: 研究为混合SSM-注意力模型提供了深入理解，并为针对不同用例设计架构提供了实用指导。

Abstract: Hybrid models that combine state space models (SSMs) with attention
mechanisms have shown strong performance by leveraging the efficiency of SSMs
and the high recall ability of attention. However, the architectural design
choices behind these hybrid models remain insufficiently understood. In this
work, we analyze hybrid architectures through the lens of memory utilization
and overall performance, and propose a complementary method to further enhance
their effectiveness. We first examine the distinction between sequential and
parallel integration of SSM and attention layers. Our analysis reveals several
interesting findings, including that sequential hybrids perform better on
shorter contexts, whereas parallel hybrids are more effective for longer
contexts. We also introduce a data-centric approach of continually training on
datasets augmented with paraphrases, which further enhances recall while
preserving other capabilities. It generalizes well across different base models
and outperforms architectural modifications aimed at enhancing recall. Our
findings provide a deeper understanding of hybrid SSM-attention models and
offer practical guidance for designing architectures tailored to various use
cases. Our findings provide a deeper understanding of hybrid SSM-attention
models and offer practical guidance for designing architectures tailored to
various use cases.

</details>


### [2] [Frame Semantic Patterns for Identifying Underreporting of Notifiable Events in Healthcare: The Case of Gender-Based Violence](https://arxiv.org/abs/2510.26969)
*Lívia Dutra,Arthur Lorenzi,Laís Berno,Franciany Campos,Karoline Biscardi,Kenneth Brown,Marcelo Viridiano,Frederico Belcavello,Ely Matos,Olívia Guaranha,Erik Santos,Sofia Reinach,Tiago Timponi Torrent*

Main category: cs.CL

TL;DR: 提出了一种利用语义框架识别医疗记录中应报告事件的方法，特别应用于识别基于性别的暴力事件，在巴西葡萄牙语语料库中达到0.726的精确度。


<details>
  <summary>Details</summary>
Motivation: 解决电子医疗记录中基于性别的暴力事件漏报问题，提高公共卫生监测能力。

Method: 使用语义框架定义细粒度模式，在非结构化医疗记录文本中搜索这些模式，共定义了8个模式并在2100万句巴西葡萄牙语语料上进行测试。

Result: 方法有效识别暴力报告，精确度达到0.726，验证了方法的稳健性。

Conclusion: 该方法作为透明、高效、低碳且语言无关的流程，可轻松适应其他健康监测场景，促进NLP在公共卫生系统中的伦理和可解释使用。

Abstract: We introduce a methodology for the identification of notifiable events in the
domain of healthcare. The methodology harnesses semantic frames to define
fine-grained patterns and search them in unstructured data, namely, open-text
fields in e-medical records. We apply the methodology to the problem of
underreporting of gender-based violence (GBV) in e-medical records produced
during patients' visits to primary care units. A total of eight patterns are
defined and searched on a corpus of 21 million sentences in Brazilian
Portuguese extracted from e-SUS APS. The results are manually evaluated by
linguists and the precision of each pattern measured. Our findings reveal that
the methodology effectively identifies reports of violence with a precision of
0.726, confirming its robustness. Designed as a transparent, efficient,
low-carbon, and language-agnostic pipeline, the approach can be easily adapted
to other health surveillance contexts, contributing to the broader, ethical,
and explainable use of NLP in public health systems.

</details>


### [3] [Overview of the MEDIQA-OE 2025 Shared Task on Medical Order Extraction from Doctor-Patient Consultations](https://arxiv.org/abs/2510.26974)
*Jean-Philippe Corbeil,Asma Ben Abacha,Jerome Tremblay,Phillip Swazinna,Akila Jeeson Daniel,Miguel Del-Agua,Francois Beaulieu*

Main category: cs.CL

TL;DR: MEDIQA-OE 2025是首个从医患对话中提取医疗指令的共享任务，旨在减轻临床医生的文档负担并改善患者护理。


<details>
  <summary>Details</summary>
Motivation: 虽然临床文档越来越多地使用自动语音识别和摘要技术，但将对话转换为电子健康记录中的可执行医疗指令仍未被探索。解决这个问题可以显著减轻临床医生的文档负担并直接影响下游患者护理。

Method: 引入了MEDIQA-OE 2025共享任务，六个团队参与并尝试了广泛的方法，包括闭源和开源大语言模型。

Result: 共享任务成功举办，多个团队参与并展示了不同的技术方法。

Conclusion: MEDIQA-OE任务为从医患对话中提取医疗指令这一重要问题提供了首个基准和解决方案框架。

Abstract: Clinical documentation increasingly uses automatic speech recognition and
summarization, yet converting conversations into actionable medical orders for
Electronic Health Records remains unexplored. A solution to this problem can
significantly reduce the documentation burden of clinicians and directly impact
downstream patient care. We introduce the MEDIQA-OE 2025 shared task, the first
challenge on extracting medical orders from doctor-patient conversations. Six
teams participated in the shared task and experimented with a broad range of
approaches, and both closed- and open-weight large language models (LLMs). In
this paper, we describe the MEDIQA-OE task, dataset, final leaderboard ranking,
and participants' solutions.

</details>


### [4] [Semantically-Aware LLM Agent to Enhance Privacy in Conversational AI Services](https://arxiv.org/abs/2510.27016)
*Jayden Serenari,Stephen Lee*

Main category: cs.CL

TL;DR: 提出了LOPSIDED框架，通过语义感知的伪名化技术保护LLM对话中的个人身份信息，在保持上下文完整性的同时增强隐私保护。


<details>
  <summary>Details</summary>
Motivation: 随着对话AI系统的广泛使用，用户在与大语言模型交互时分享敏感个人数据可能导致隐私泄露风险，需要保护个人身份信息(PII)以防止安全漏洞或身份盗窃。

Method: 开发了LOPSIDED框架，动态地将用户提示中的敏感PII实体替换为语义一致的伪名，在模型生成响应后自动进行去伪名化处理。

Result: 在基于ShareGPT真实对话数据的评估中，LOPSIDED相比基线技术将语义效用错误减少了5倍，同时增强了隐私保护。

Conclusion: LOPSIDED框架能够有效保护LLM对话中的敏感信息，在保持响应质量的同时显著提升隐私保护水平。

Abstract: With the increasing use of conversational AI systems, there is growing
concern over privacy leaks, especially when users share sensitive personal data
in interactions with Large Language Models (LLMs). Conversations shared with
these models may contain Personally Identifiable Information (PII), which, if
exposed, could lead to security breaches or identity theft. To address this
challenge, we present the Local Optimizations for Pseudonymization with
Semantic Integrity Directed Entity Detection (LOPSIDED) framework, a
semantically-aware privacy agent designed to safeguard sensitive PII data when
using remote LLMs. Unlike prior work that often degrade response quality, our
approach dynamically replaces sensitive PII entities in user prompts with
semantically consistent pseudonyms, preserving the contextual integrity of
conversations. Once the model generates its response, the pseudonyms are
automatically depseudonymized, ensuring the user receives an accurate,
privacy-preserving output. We evaluate our approach using real-world
conversations sourced from ShareGPT, which we further augment and annotate to
assess whether named entities are contextually relevant to the model's
response. Our results show that LOPSIDED reduces semantic utility errors by a
factor of 5 compared to baseline techniques, all while enhancing privacy.

</details>


### [5] [Kad: A Framework for Proxy-based Test-time Alignment with Knapsack Approximation Deferral](https://arxiv.org/abs/2510.27017)
*Ayoub Hammal,Pierre Zweigenbaum,Caio Corro*

Main category: cs.CL

TL;DR: 提出了一种基于代理的测试时对齐方法，使用小型对齐模型指导大语言模型，通过0-1背包问题优化token级级联决策，在任务性能和推测解码速度方面均有提升


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然预训练阶段已具备大部分生成能力，但仍需对齐以适应下游任务需求，而随着模型规模扩大，对齐过程的计算成本急剧增加

Method: 采用token级级联方法，将token特定的延迟决策简化为0-1背包问题，推导出最优延迟决策的原始和对偶近似

Result: 实验证明该方法在任务性能和推测解码速度方面都带来了显著优势

Conclusion: 提出的代理测试时对齐方法有效降低了大规模语言模型对齐的计算成本，同时保持了良好的性能表现

Abstract: Several previous works concluded that the largest part of generation
capabilities of large language models (LLM) are learned (early) during
pre-training. However, LLMs still require further alignment to adhere to
downstream task requirements and stylistic preferences, among other desired
properties. As LLMs continue to scale in terms of size, the computational cost
of alignment procedures increase prohibitively. In this work, we propose a
novel approach to circumvent these costs via proxy-based test-time alignment,
i.e. using guidance from a small aligned model. Our approach can be described
as token-specific cascading method, where the token-specific deferral rule is
reduced to 0-1 knapsack problem. In this setting, we derive primal and dual
approximations of the optimal deferral decision. We experimentally show the
benefits of our method both in task performance and speculative decoding speed.

</details>


### [6] [Elastic Architecture Search for Efficient Language Models](https://arxiv.org/abs/2510.27037)
*Shang Wang*

Main category: cs.CL

TL;DR: ELM是一种新颖的神经架构搜索方法，专门针对紧凑型语言模型优化，通过引入灵活搜索空间和动态模块来提升搜索效率和灵活性，在语言建模任务中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着大型预训练语言模型在自然语言理解任务中变得日益重要，其巨大的计算和内存需求引发了经济和环境担忧，需要开发更高效的紧凑模型。

Method: ELM扩展了现有NAS方法，引入包含高效transformer块和动态维度/头数调整模块的灵活搜索空间，并采用新颖的知识蒸馏损失来保持各块特性，提升架构选择区分度。

Result: 在掩码语言建模和因果语言建模任务上的实验表明，ELM发现的模型显著优于现有方法。

Conclusion: ELM通过创新的搜索空间设计和知识蒸馏策略，成功开发出高效的紧凑语言模型，为解决大型语言模型的计算和环境问题提供了有效方案。

Abstract: As large pre-trained language models become increasingly critical to natural
language understanding (NLU) tasks, their substantial computational and memory
requirements have raised significant economic and environmental concerns.
Addressing these challenges, this paper introduces the Elastic Language Model
(ELM), a novel neural architecture search (NAS) method optimized for compact
language models. ELM extends existing NAS approaches by introducing a flexible
search space with efficient transformer blocks and dynamic modules for
dimension and head number adjustment. These innovations enhance the efficiency
and flexibility of the search process, which facilitates more thorough and
effective exploration of model architectures. We also introduce novel knowledge
distillation losses that preserve the unique characteristics of each block, in
order to improve the discrimination between architectural choices during the
search process. Experiments on masked language modeling and causal language
modeling tasks demonstrate that models discovered by ELM significantly
outperform existing methods.

</details>


### [7] [Dataset Creation and Baseline Models for Sexism Detection in Hausa](https://arxiv.org/abs/2510.27038)
*Fatima Adam Muhammad,Shamsuddeen Muhammad Hassan,Isa Inuwa-Dutse*

Main category: cs.CL

TL;DR: 本研究创建了首个豪萨语性别歧视检测数据集，通过社区参与和定性编码开发，并评估了传统机器学习模型与预训练多语言模型在豪萨语性别歧视检测中的表现。


<details>
  <summary>Details</summary>
Motivation: 在线平台助长了各种形式的性别歧视，而现有性别歧视检测方法主要针对高资源语言，低资源语言如豪萨语因语言资源有限和文化差异而进展缓慢。

Method: 通过两阶段用户研究（n=66）收集本地使用者对性别歧视的定义和表达，结合社区参与、定性编码和数据增强构建数据集，测试传统机器学习分类器和预训练多语言模型，评估少样本学习效果。

Result: 研究发现在捕捉文化细微差别方面存在挑战，特别是在澄清性询问和习语表达方面，这些情况下容易出现许多误报。

Conclusion: 豪萨语性别歧视检测面临文化细微差别捕捉的挑战，需要进一步研究以改进低资源语言中的性别歧视检测方法。

Abstract: Sexism reinforces gender inequality and social exclusion by perpetuating
stereotypes, bias, and discriminatory norms. Noting how online platforms enable
various forms of sexism to thrive, there is a growing need for effective sexism
detection and mitigation strategies. While computational approaches to sexism
detection are widespread in high-resource languages, progress remains limited
in low-resource languages where limited linguistic resources and cultural
differences affect how sexism is expressed and perceived. This study introduces
the first Hausa sexism detection dataset, developed through community
engagement, qualitative coding, and data augmentation. For cultural nuances and
linguistic representation, we conducted a two-stage user study (n=66) involving
native speakers to explore how sexism is defined and articulated in everyday
discourse. We further experiment with both traditional machine learning
classifiers and pre-trained multilingual language models and evaluating the
effectiveness few-shot learning in detecting sexism in Hausa. Our findings
highlight challenges in capturing cultural nuance, particularly with
clarification-seeking and idiomatic expressions, and reveal a tendency for many
false positives in such cases.

</details>


### [8] [Quantitative Intertextuality from the Digital Humanities Perspective: A Survey](https://arxiv.org/abs/2510.27045)
*Siyu Duan*

Main category: cs.CL

TL;DR: 本文为定量互文性研究提供了路线图，总结了数据、方法和应用，涵盖多语言多主题的数据，从统计到深度学习方法，以及在人文社科研究中的应用和平台工具。


<details>
  <summary>Details</summary>
Motivation: 互文性作为文学理论中的重要概念，在数字人文学研究中具有重要理论基础。随着自然语言处理技术的发展，互文性研究进入定量时代，需要系统总结这一领域的研究进展。

Method: 基于多语言多主题的数据，综述从统计方法到深度学习的各种技术方法，并总结相关平台工具。

Result: 总结了定量互文性研究的数据来源、技术方法和应用场景，为未来研究提供了系统框架。

Conclusion: 随着计算机技术的进步，可以预期更精确、多样化和大规模的互文性研究，互文性在连接人工智能与人文学科的跨学科研究中具有广阔应用前景。

Abstract: The connection between texts is referred to as intertextuality in literary
theory, which served as an important theoretical basis in many digital
humanities studies. Over the past decade, advancements in natural language
processing have ushered intertextuality studies into the quantitative age.
Large-scale intertextuality research based on cutting-edge methods has
continuously emerged. This paper provides a roadmap for quantitative
intertextuality studies, summarizing their data, methods, and applications.
Drawing on data from multiple languages and topics, this survey reviews methods
from statistics to deep learning. It also summarizes their applications in
humanities and social sciences research and the associated platform tools.
Driven by advances in computer technology, more precise, diverse, and
large-scale intertext studies can be anticipated. Intertextuality holds promise
for broader application in interdisciplinary research bridging AI and the
humanities.

</details>


### [9] [Recursive numeral systems are highly regular and easy to process](https://arxiv.org/abs/2510.27049)
*Ponrawee Prasertsom,Andrea Silvi,Jennifer Culbertson,Moa Johansson,Devdatt Dubhashi,Kenny Smith*

Main category: cs.CL

TL;DR: 本文认为递归数字系统在规律性和处理复杂性方面更高效，而非仅仅在词典大小和形态句法复杂性之间权衡。通过最小描述长度方法，更好地捕捉了自然系统与非自然系统的关键差异。


<details>
  <summary>Details</summary>
Motivation: 先前研究认为递归数字系统优化了词典大小和平均形态句法复杂性之间的权衡，但需要依赖临时约束来排除非自然系统。本文认为问题在于忽略了规律性这一人类语法的关键复杂性方面。

Method: 采用最小描述长度方法，提出基于MDL的规律性和处理复杂性度量，分析递归数字系统的效率。

Result: MDL方法能更好地区分自然系统与非自然系统，包括先前工作中的"最优"递归数字系统，且先前文献中的临时约束自然地从规律性中得出。

Conclusion: 在语言最优性研究中需要纳入形式集合的规律性，递归数字系统在规律性和处理复杂性方面表现高效。

Abstract: Previous work has argued that recursive numeral systems optimise the
trade-off between lexicon size and average morphosyntatic complexity (Deni\'c
and Szymanik, 2024). However, showing that only natural-language-like systems
optimise this tradeoff has proven elusive, and the existing solution has relied
on ad-hoc constraints to rule out unnatural systems (Yang and Regier, 2025).
Here, we argue that this issue arises because the proposed trade-off has
neglected regularity, a crucial aspect of complexity central to human grammars
in general. Drawing on the Minimum Description Length (MDL) approach, we
propose that recursive numeral systems are better viewed as efficient with
regard to their regularity and processing complexity. We show that our
MDL-based measures of regularity and processing complexity better capture the
key differences between attested, natural systems and unattested but possible
ones, including "optimal" recursive numeral systems from previous work, and
that the ad-hoc constraints from previous literature naturally follow from
regularity. Our approach highlights the need to incorporate regularity across
sets of forms in studies that attempt to measure and explain optimality in
language.

</details>


### [10] [VISTA Score: Verification In Sequential Turn-based Assessment](https://arxiv.org/abs/2510.27052)
*Ashley Lewis,Andrew Perrault,Eric Fosler-Lussier,Michael White*

Main category: cs.CL

TL;DR: VISTA是一个用于评估对话系统事实性的框架，通过声明级验证和序列一致性跟踪来检测多轮对话中的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有的事实性评估指标要么评估孤立回复，要么将不可验证内容视为错误，限制了在多轮对话中的应用。幻觉问题仍然是部署对话AI系统的主要障碍。

Method: VISTA将每个助手回复分解为原子事实声明，根据可信来源和对话历史进行验证，并将不可验证声明分类为主观、矛盾、缺乏证据或弃权。

Result: 在8个大型语言模型和4个对话事实性基准测试中，VISTA相比FACTSCORE和LLM-as-Judge基线显著提高了幻觉检测能力。人工评估确认VISTA的分解方法提高了标注者一致性。

Conclusion: 通过将事实性建模为对话的动态属性，VISTA为对话系统提供了一个更透明、更符合人类认知的真实性衡量标准。

Abstract: Hallucination--defined here as generating statements unsupported or
contradicted by available evidence or conversational context--remains a major
obstacle to deploying conversational AI systems in settings that demand factual
reliability. Existing metrics either evaluate isolated responses or treat
unverifiable content as errors, limiting their use for multi-turn dialogue. We
introduce VISTA (Verification In Sequential Turn-based Assessment), a framework
for evaluating conversational factuality through claim-level verification and
sequential consistency tracking. VISTA decomposes each assistant turn into
atomic factual claims, verifies them against trusted sources and dialogue
history, and categorizes unverifiable statements (subjective, contradicted,
lacking evidence, or abstaining). Across eight large language models and four
dialogue factuality benchmarks (AIS, BEGIN, FAITHDIAL, and FADE), VISTA
substantially improves hallucination detection over FACTSCORE and LLM-as-Judge
baselines. Human evaluation confirms that VISTA's decomposition improves
annotator agreement and reveals inconsistencies in existing benchmarks. By
modeling factuality as a dynamic property of conversation, VISTA offers a more
transparent, human-aligned measure of truthfulness in dialogue systems.

</details>


### [11] [LLM-Centric RAG with Multi-Granular Indexing and Confidence Constraints](https://arxiv.org/abs/2510.27054)
*Xiaofan Guo,Yaxuan Luan,Yue Kang,Xiangchen Song,Jinxu Guo*

Main category: cs.CL

TL;DR: 提出了一种结合多粒度记忆索引和不确定性估计的置信度控制方法，解决复杂知识环境下检索增强生成覆盖不足、结果不稳定和可靠性有限的问题。


<details>
  <summary>Details</summary>
Motivation: 解决复杂知识环境中检索增强生成存在的覆盖不足、结果不稳定和可靠性有限的问题，提升模型在复杂上下文中的可靠性和可控性。

Method: 构建分层记忆结构，将知识表示划分为不同粒度级别，实现从局部细节到全局上下文的动态索引和检索；引入不确定性估计机制，在生成过程中显式约束和过滤低置信度路径；优化目标包括生成损失、熵约束和方差正则化。

Result: 在问答准确性、检索召回率、排序质量和事实一致性方面优于现有模型，证明了多粒度索引与置信度控制结合的有效性。

Conclusion: 为检索增强生成提供了新的技术路径，并为提升大模型在复杂上下文中的可靠性和可控性提供了实践证据。

Abstract: This paper addresses the issues of insufficient coverage, unstable results,
and limited reliability in retrieval-augmented generation under complex
knowledge environments, and proposes a confidence control method that
integrates multi-granularity memory indexing with uncertainty estimation. The
method builds a hierarchical memory structure that divides knowledge
representations into different levels of granularity, enabling dynamic indexing
and retrieval from local details to global context, and thus establishing
closer semantic connections between retrieval and generation. On this basis, an
uncertainty estimation mechanism is introduced to explicitly constrain and
filter low-confidence paths during the generation process, allowing the model
to maintain information coverage while effectively suppressing noise and false
content. The overall optimization objective consists of generation loss,
entropy constraints, and variance regularization, forming a unified confidence
control framework. In the experiments, comprehensive sensitivity tests and
comparative analyses were designed, covering hyperparameters, environmental
conditions, and data structures, to verify the stability and robustness of the
proposed method across different scenarios. The results show that the method
achieves superior performance over existing models in QA accuracy, retrieval
recall, ranking quality, and factual consistency, demonstrating the
effectiveness of combining multi-granularity indexing with confidence control.
This study not only provides a new technical pathway for retrieval-augmented
generation but also offers practical evidence for improving the reliability and
controllability of large models in complex contexts.

</details>


### [12] [Detecting Data Contamination in LLMs via In-Context Learning](https://arxiv.org/abs/2510.27055)
*Michał Zawalski,Meriem Boubdir,Klaudia Bałazy,Besmira Nushi,Pablo Ribalta*

Main category: cs.CL

TL;DR: CoDeC是一种通过上下文学习检测大语言模型训练数据污染的方法，通过测量上下文学习对模型性能的影响来区分记忆数据和未见数据。


<details>
  <summary>Details</summary>
Motivation: 需要检测大语言模型训练数据污染，特别是对于训练语料未公开的开源模型，以识别数据记忆问题。

Method: 通过测量上下文学习如何影响模型置信度来检测数据污染——上下文示例通常提升未见数据集的置信度，但可能降低训练数据集的置信度，因为会破坏记忆模式。

Result: CoDeC产生可解释的污染分数，能清晰区分见过和未见的数据集，并在训练语料未公开的开源模型中揭示了强烈记忆证据。

Conclusion: 该方法简单、自动化、与模型和数据集无关，易于集成到基准评估中，为训练数据污染检测提供了实用准确的解决方案。

Abstract: We present Contamination Detection via Context (CoDeC), a practical and
accurate method to detect and quantify training data contamination in large
language models. CoDeC distinguishes between data memorized during training and
data outside the training distribution by measuring how in-context learning
affects model performance. We find that in-context examples typically boost
confidence for unseen datasets but may reduce it when the dataset was part of
training, due to disrupted memorization patterns. Experiments show that CoDeC
produces interpretable contamination scores that clearly separate seen and
unseen datasets, and reveals strong evidence of memorization in open-weight
models with undisclosed training corpora. The method is simple, automated, and
both model- and dataset-agnostic, making it easy to integrate with benchmark
evaluations.

</details>


### [13] [Contrastive Knowledge Transfer and Robust Optimization for Secure Alignment of Large Language Models](https://arxiv.org/abs/2510.27077)
*Jiasen Zheng,Huajun Zhang,Xu Yan,Ran Hao,Chong Peng*

Main category: cs.CL

TL;DR: 提出了一种结合对比蒸馏和噪声鲁棒训练的微调方法，通过冻结主干模型并转移教师模型的知识边界，提高语义一致性和对齐精度，同时引入噪声扰动和鲁棒优化约束来增强模型在噪声输入下的稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决大规模语言模型在安全对齐和鲁棒性方面的局限性，现有方法在知识转移和抗干扰能力方面存在不足。

Method: 采用对比蒸馏与噪声鲁棒训练结合的微调方法，冻结主干模型，通过蒸馏损失、鲁棒性损失和正则化项构建统一优化目标，平衡对齐能力和抗干扰能力。

Result: 在知识转移、鲁棒性和整体安全性方面显著优于现有基线方法，在多个关键指标上达到最佳性能。

Conclusion: 该方法不仅丰富了参数高效微调的理论体系，还为构建更安全、更可信的对齐机制提供了新解决方案。

Abstract: This paper addresses the limitations of large-scale language models in safety
alignment and robustness by proposing a fine-tuning method that combines
contrastive distillation with noise-robust training. The method freezes the
backbone model and transfers the knowledge boundaries of the teacher model to
the student model through distillation, thereby improving semantic consistency
and alignment accuracy. At the same time, noise perturbations and robust
optimization constraints are introduced during training to ensure that the
model maintains stable predictive outputs under noisy and uncertain inputs. The
overall framework consists of distillation loss, robustness loss, and a
regularization term, forming a unified optimization objective that balances
alignment ability with resistance to interference. To systematically validate
its effectiveness, the study designs experiments from multiple perspectives,
including distillation weight sensitivity, stability analysis under computation
budgets and mixed-precision environments, and the impact of data noise and
distribution shifts on model performance. Results show that the method
significantly outperforms existing baselines in knowledge transfer, robustness,
and overall safety, achieving the best performance across several key metrics.
This work not only enriches the theoretical system of parameter-efficient
fine-tuning but also provides a new solution for building safer and more
trustworthy alignment mechanisms.

</details>


### [14] [Characterizing Selective Refusal Bias in Large Language Models](https://arxiv.org/abs/2510.27087)
*Adel Khorramrouz,Sharon Levy*

Main category: cs.CL

TL;DR: 研究发现LLM安全护栏存在选择性拒绝偏见，对不同人口群体（性别、性取向、国籍、宗教）的拒绝率存在差异，这可能导致新的安全风险。


<details>
  <summary>Details</summary>
Motivation: LLM安全护栏旨在防止生成有害内容，但可能无意中引入新的偏见，导致对不同人口群体的拒绝行为不一致。

Method: 通过分析针对个体和交叉人口群体的拒绝率、LLM响应类型和拒绝生成长度来研究选择性拒绝偏见。

Result: 发现LLM安全护栏在性别、性取向、国籍和宗教属性上存在选择性拒绝偏见的证据。

Conclusion: 需要在不同人口群体间实现更公平和稳健的安全护栏性能。

Abstract: Safety guardrails in large language models(LLMs) are developed to prevent
malicious users from generating toxic content at a large scale. However, these
measures can inadvertently introduce or reflect new biases, as LLMs may refuse
to generate harmful content targeting some demographic groups and not others.
We explore this selective refusal bias in LLM guardrails through the lens of
refusal rates of targeted individual and intersectional demographic groups,
types of LLM responses, and length of generated refusals. Our results show
evidence of selective refusal bias across gender, sexual orientation,
nationality, and religion attributes. This leads us to investigate additional
safety implications via an indirect attack, where we target previously refused
groups. Our findings emphasize the need for more equitable and robust
performance in safety guardrails across demographic groups.

</details>


### [15] [Rating Roulette: Self-Inconsistency in LLM-As-A-Judge Frameworks](https://arxiv.org/abs/2510.27106)
*Rajarshi Haldar,Julia Hockenmaier*

Main category: cs.CL

TL;DR: 研究发现使用大语言模型(LLM)作为自然语言生成评估工具时存在评分不一致问题，LLM评委在不同运行中给出的分数可靠性较低，这种方差使得评分几乎变得随意。


<details>
  <summary>Details</summary>
Motivation: 随着自然语言生成的广泛应用，其评估变得困难。虽然LLM评估比传统的n-gram或基于嵌入的指标更符合人类偏好，但需要检验其评估的可靠性。

Method: 通过实验量化LLM评委在不同NLG任务和基准测试中的评分不一致性，并探索在遵循适当指南的情况下是否仍能有效使用LLM评委。

Result: LLM评委在不同运行中给出的分数具有低内部评分者可靠性，评分方差大且不一致，在最坏情况下几乎变得随意。

Conclusion: 尽管LLM评委存在评分不一致问题，但在遵循适当指南的情况下仍可能有其使用价值，需要谨慎使用LLM评委进行自然语言生成评估。

Abstract: As Natural Language Generation (NLG) continues to be widely adopted, properly
assessing it has become quite difficult. Lately, using large language models
(LLMs) for evaluating these generations has gained traction, as they tend to
align more closely with human preferences than conventional n-gram or
embedding-based metrics. In our experiments, we show that LLM judges have low
intra-rater reliability in their assigned scores across different runs. This
variance makes their ratings inconsistent, almost arbitrary in the worst case,
making it difficult to measure how good their judgments actually are. We
quantify this inconsistency across different NLG tasks and benchmarks and see
if judicious use of LLM judges can still be useful following proper guidelines.

</details>


### [16] [Probability Distributions Computed by Hard-Attention Transformers](https://arxiv.org/abs/2510.27118)
*Andy Yang,Anej Svete,Jiaoda Li,Anthony Widjaja Lin,Jonathan Rawski,Ryan Cotterell,David Chiang*

Main category: cs.CL

TL;DR: 本文分析了transformer语言模型在自回归概率生成模式下的表达能力，揭示了与传统语言识别器相比的差异。


<details>
  <summary>Details</summary>
Motivation: 现有的transformer表达能力研究主要关注其作为语言识别器的功能，但实际应用中transformer主要作为自回归概率语言模型使用，需要研究其在这种使用模式下的表达能力。

Method: 通过理论分析，研究transformer语言模型能够表达的概率分布，比较自回归和非自回归模式下的表达能力差异。

Result: 研究发现：1）使transformer语言识别器自回归化有时能增强其表达能力；2）概率化处理会打破非概率情况下的等价关系。

Conclusion: 本文系统揭示了transformer在作为语言模型使用时的表达能力特性，为理解其实际应用能力提供了理论依据。

Abstract: Most expressivity results for transformers treat them as language recognizers
(which accept or reject strings), and not as they are used in practice, as
language models (which generate strings autoregressively and
probabilistically). Here, we characterize the probability distributions that
transformer language models can express. We show that making transformer
language recognizers autoregressive can sometimes increase their expressivity,
and that making them probabilistic can break equivalences that hold in the
non-probabilistic case. Our overall contribution is to tease apart what
functions transformers are capable of expressing, in their most common use-case
as language models.

</details>


### [17] [Simple Additions, Substantial Gains: Expanding Scripts, Languages, and Lineage Coverage in URIEL+](https://arxiv.org/abs/2510.27183)
*Mason Shipton,York Hay Ng,Aditya Khan,Phuong Hanh Hoang,Xiang Lu,A. Seza Doğruöz,En-Shiun Annie Lee*

Main category: cs.CL

TL;DR: 本文扩展了URIEL+语言知识库，通过引入脚本向量、整合Glottolog语言数据以及扩展谱系插补，显著减少了数据稀疏性，提高了语言覆盖范围和插补质量。


<details>
  <summary>Details</summary>
Motivation: URIEL+语言知识库存在数据稀疏问题，包括缺失特征类型、不完整的语言条目和有限的谱系覆盖，这限制了其在跨语言迁移特别是低资源语言研究中的实用性。

Method: 1) 为7,488种语言引入脚本向量表示书写系统属性；2) 整合Glottolog添加18,710种额外语言；3) 为26,449种语言扩展谱系插补，在谱系间传播类型学和脚本特征。

Result: 脚本向量的特征稀疏性减少14%，语言覆盖增加高达19,015种语言（1,007%），插补质量指标提升高达33%。在跨语言迁移任务中，某些设置下性能提升达6%。

Conclusion: 这些改进使URIEL+在多语言研究中更加完整和包容，为低资源语言研究提供了更好的支持。

Abstract: The URIEL+ linguistic knowledge base supports multilingual research by
encoding languages through geographic, genetic, and typological vectors.
However, data sparsity remains prevalent, in the form of missing feature types,
incomplete language entries, and limited genealogical coverage. This limits the
usefulness of URIEL+ in cross-lingual transfer, particularly for supporting
low-resource languages. To address this sparsity, this paper extends URIEL+
with three contributions: introducing script vectors to represent writing
system properties for 7,488 languages, integrating Glottolog to add 18,710
additional languages, and expanding lineage imputation for 26,449 languages by
propagating typological and script features across genealogies. These additions
reduce feature sparsity by 14% for script vectors, increase language coverage
by up to 19,015 languages (1,007%), and improve imputation quality metrics by
up to 33%. Our benchmark on cross-lingual transfer tasks (oriented around
low-resource languages) shows occasionally divergent performance compared to
URIEL+, with performance gains up to 6% in certain setups. Our advances make
URIEL+ more complete and inclusive for multilingual research.

</details>


### [18] [MemeArena: Automating Context-Aware Unbiased Evaluation of Harmfulness Understanding for Multimodal Large Language Models](https://arxiv.org/abs/2510.27196)
*Zixin Chen,Hongzhan Lin,Kaixin Li,Ziyang Luo,Yayue Deng,Jing Ma*

Main category: cs.CL

TL;DR: 提出了MemeArena框架，基于代理的竞技场式评估方法，用于评估多模态大语言模型对有害多模态内容的理解能力，通过模拟多样化解释上下文来减少评估偏见。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法主要关注二元分类任务的检测准确率，无法反映多模态有害性在不同上下文中的深度解释细微差别。

Method: MemeArena框架模拟多样化解释上下文来制定评估任务，通过整合不同观点并在评估者之间达成共识，实现公平无偏的比较。

Result: 实验表明该框架有效减少了评估偏见，判断结果与人类偏好高度一致，为可靠全面的多模态有害性理解评估提供了宝贵见解。

Conclusion: MemeArena提供了一个上下文感知且无偏的评估框架，能够全面评估mLLMs对多模态有害性的理解能力。

Abstract: The proliferation of memes on social media necessitates the capabilities of
multimodal Large Language Models (mLLMs) to effectively understand multimodal
harmfulness. Existing evaluation approaches predominantly focus on mLLMs'
detection accuracy for binary classification tasks, which often fail to reflect
the in-depth interpretive nuance of harmfulness across diverse contexts. In
this paper, we propose MemeArena, an agent-based arena-style evaluation
framework that provides a context-aware and unbiased assessment for mLLMs'
understanding of multimodal harmfulness. Specifically, MemeArena simulates
diverse interpretive contexts to formulate evaluation tasks that elicit
perspective-specific analyses from mLLMs. By integrating varied viewpoints and
reaching consensus among evaluators, it enables fair and unbiased comparisons
of mLLMs' abilities to interpret multimodal harmfulness. Extensive experiments
demonstrate that our framework effectively reduces the evaluation biases of
judge agents, with judgment results closely aligning with human preferences,
offering valuable insights into reliable and comprehensive mLLM evaluations in
multimodal harmfulness understanding. Our code and data are publicly available
at https://github.com/Lbotirx/MemeArena.

</details>


### [19] [Identifying the Periodicity of Information in Natural Language](https://arxiv.org/abs/2510.27241)
*Yulin Ou,Yu Wang,Yang Xu,Hendrik Buschmeier*

Main category: cs.CL

TL;DR: 本文提出了一种名为AutoPeriod of Surprisal (APS)的新方法，用于检测自然语言中信息密度的周期性模式，发现人类语言中存在显著的周期性信息模式，并识别出超出典型文本结构单元分布的新周期。


<details>
  <summary>Details</summary>
Motivation: 随着信息密度理论的发展，需要探究自然语言在编码信息时表现出的周期性程度，以理解语言中信息的结构化特征。

Method: 采用APS方法，这是一种规范化的周期性检测算法，能够识别单个文档惊异值序列中存在的任何显著周期。

Result: 研究发现：1）相当比例的人类语言表现出强烈的信息周期性模式；2）发现了超出典型文本结构单元分布的新周期，并通过谐波回归模型进一步确认。

Conclusion: 语言中信息的周期性是结构化因素和在更长距离上起作用的其他驱动因素共同作用的结果。该方法在LLM生成检测方面具有潜在应用价值。

Abstract: Recent theoretical advancement of information density in natural language has
brought the following question on desk: To what degree does natural language
exhibit periodicity pattern in its encoded information? We address this
question by introducing a new method called AutoPeriod of Surprisal (APS). APS
adopts a canonical periodicity detection algorithm and is able to identify any
significant periods that exist in the surprisal sequence of a single document.
By applying the algorithm to a set of corpora, we have obtained the following
interesting results: Firstly, a considerable proportion of human language
demonstrates a strong pattern of periodicity in information; Secondly, new
periods that are outside the distributions of typical structural units in text
(e.g., sentence boundaries, elementary discourse units, etc.) are found and
further confirmed via harmonic regression modeling. We conclude that the
periodicity of information in language is a joint outcome from both structured
factors and other driving factors that take effect at longer distances. The
advantages of our periodicity detection method and its potentials in
LLM-generation detection are further discussed.

</details>


### [20] [Beyond a Million Tokens: Benchmarking and Enhancing Long-Term Memory in LLMs](https://arxiv.org/abs/2510.27246)
*Mohammad Tavakoli,Alireza Salemi,Carrie Ye,Mohamed Abdalla,Hamed Zamani,J Ross Mitchell*

Main category: cs.CL

TL;DR: 提出了BEAM基准测试和LIGHT框架，用于评估和改进LLM在长对话中的记忆能力。BEAM包含100个长对话和2000个验证问题，LIGHT通过三种记忆系统提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试缺乏叙事连贯性、覆盖领域窄，且只测试简单的回忆任务，无法充分评估LLM在长上下文推理中的记忆能力。

Method: 1) 构建BEAM基准：自动生成长达1000万token的连贯对话和多样化问题；2) 提出LIGHT框架：包含长期情景记忆、短期工作记忆和事实积累便签三种互补记忆系统。

Result: 实验显示，即使具有100万token上下文窗口的LLM在长对话中表现不佳，而LIGHT框架在不同模型上平均提升3.5%-12.69%的性能，消融研究证实了各记忆组件的贡献。

Conclusion: BEAM基准有效评估LLM的长时记忆能力，LIGHT框架通过模拟人类认知的多记忆系统显著提升模型在长对话中的表现。

Abstract: Evaluating the abilities of large language models (LLMs) for tasks that
require long-term memory and thus long-context reasoning, for example in
conversational settings, is hampered by the existing benchmarks, which often
lack narrative coherence, cover narrow domains, and only test simple
recall-oriented tasks. This paper introduces a comprehensive solution to these
challenges. First, we present a novel framework for automatically generating
long (up to 10M tokens), coherent, and topically diverse conversations,
accompanied by probing questions targeting a wide range of memory abilities.
From this, we construct BEAM, a new benchmark comprising 100 conversations and
2,000 validated questions. Second, to enhance model performance, we propose
LIGHT-a framework inspired by human cognition that equips LLMs with three
complementary memory systems: a long-term episodic memory, a short-term working
memory, and a scratchpad for accumulating salient facts. Our experiments on
BEAM reveal that even LLMs with 1M token context windows (with and without
retrieval-augmentation) struggle as dialogues lengthen. In contrast, LIGHT
consistently improves performance across various models, achieving an average
improvement of 3.5%-12.69% over the strongest baselines, depending on the
backbone LLM. An ablation study further confirms the contribution of each
memory component.

</details>


### [21] [Languages are Modalities: Cross-Lingual Alignment via Encoder Injection](https://arxiv.org/abs/2510.27254)
*Rajan Agarwal,Aarush Gupta*

Main category: cs.CL

TL;DR: LLINK是一种计算高效的语言作为模态方法，通过将多语言编码器的句子嵌入对齐到解码器的潜在嵌入空间，并使用最小适配器来改善低资源非拉丁语系脚本的性能，而无需改变分词器或重新训练解码器。


<details>
  <summary>Details</summary>
Motivation: 指令调优的大型语言模型在低资源非拉丁语系脚本上表现不佳，主要由于分词器碎片化和弱跨语言耦合问题。

Method: 首先将冻结多语言编码器的句子嵌入通过轻量级对比投影器对齐到解码器的保留位置潜在嵌入空间；然后将向量扩展为K个软槽，并使用最小适配器训练，使冻结解码器能够处理该信号。

Result: LLINK显著改善了双语检索性能，在LLM评判的Q&A评估中，81.3%优于基础模型，63.6%优于直接微调方法。改进归因于减少分词膨胀和更强的跨语言对齐。

Conclusion: 将低资源语言视为模态为轻量级LLMs提供了实现更强跨语言对齐的实用路径，尽管模型在数值保真度方面仍存在残余弱点。

Abstract: Instruction-tuned Large Language Models (LLMs) underperform on low resource,
non-Latin scripts due to tokenizer fragmentation and weak cross-lingual
coupling. We present LLINK (Latent Language Injection for Non-English
Knowledge), a compute efficient language-as-modality method that conditions an
instruction-tuned decoder without changing the tokenizer or retraining the
decoder. First, we align sentence embeddings from a frozen multilingual encoder
to the decoder's latent embedding space at a reserved position via a
lightweight contrastive projector. Second, the vector is expanded into K soft
slots and trained with minimal adapters so the frozen decoder consumes the
signal. LLINK substantially improves bilingual retrieval and achieves 81.3%
preference over the base model and 63.6% over direct fine-tuning in LLM-judged
Q&A evaluations. We further find that improvements can be attributed to reduced
tokenization inflation and a stronger cross lingual alignment, despite the
model having residual weaknesses in numeric fidelity. Treating low resource
languages as a modality offers a practical path to stronger cross-lingual
alignment in lightweight LLMs.

</details>


### [22] [MedCalc-Eval and MedCalc-Env: Advancing Medical Calculation Capabilities of Large Language Models](https://arxiv.org/abs/2510.27267)
*Kangkun Mao,Jinru Ding,Jiayuan Chen,Mouxiao Bian,Ruiyao Chen,Xinwei Peng,Sijie Ren,Linyang Li,Jie Xu*

Main category: cs.CL

TL;DR: 提出了MedCalc-Eval基准测试，包含700多个医疗计算任务，用于评估LLM在医疗领域的定量推理能力，并通过MedCalc-Env强化学习环境提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要关注问答和描述性推理，忽视了临床决策中至关重要的定量推理能力，且现有数据集覆盖的计算任务有限，无法反映真实世界的计算场景。

Method: 开发了MedCalc-Eval基准测试，包含方程型和规则型评分系统两类任务，涵盖多个医学专科；并构建了基于InternBootcamp框架的MedCalc-Env强化学习环境，支持多步骤临床推理和规划。

Result: 在MedCalc-Env环境中微调的Qwen2.5-32B模型在MedCalc-Eval上取得了最先进的结果，在数值敏感性、公式选择和推理鲁棒性方面均有显著提升。

Conclusion: 该研究填补了医疗领域LLM定量推理评估的空白，但仍面临单位转换、多条件逻辑和上下文理解等挑战。

Abstract: As large language models (LLMs) enter the medical domain, most benchmarks
evaluate them on question answering or descriptive reasoning, overlooking
quantitative reasoning critical to clinical decision-making. Existing datasets
like MedCalc-Bench cover few calculation tasks and fail to reflect real-world
computational scenarios.
  We introduce MedCalc-Eval, the largest benchmark for assessing LLMs' medical
calculation abilities, comprising 700+ tasks across two types: equation-based
(e.g., Cockcroft-Gault, BMI, BSA) and rule-based scoring systems (e.g., Apgar,
Glasgow Coma Scale). These tasks span diverse specialties including internal
medicine, surgery, pediatrics, and cardiology, offering a broader and more
challenging evaluation setting.
  To improve performance, we further develop MedCalc-Env, a reinforcement
learning environment built on the InternBootcamp framework, enabling multi-step
clinical reasoning and planning. Fine-tuning a Qwen2.5-32B model within this
environment achieves state-of-the-art results on MedCalc-Eval, with notable
gains in numerical sensitivity, formula selection, and reasoning robustness.
Remaining challenges include unit conversion, multi-condition logic, and
contextual understanding.
  Code and datasets are available at
https://github.com/maokangkun/MedCalc-Eval.

</details>


### [23] [Why Do Multilingual Reasoning Gaps Emerge in Reasoning Language Models?](https://arxiv.org/abs/2510.27269)
*Deokhyung Kang,Seonjeong Hwang,Daehui Kim,Hyounghun Kim,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: 多语言推理差距主要由语言理解失败引起，可以通过检测理解失败并选择性翻译来缓解，在仅翻译20%输入的情况下达到接近全翻译的性能。


<details>
  <summary>Details</summary>
Motivation: 推理语言模型在多语言推理任务中存在性能差距，高资源语言表现优于低资源语言，但其根本原因尚未被充分探索。

Method: 提出选择性翻译策略，通过检测语言理解失败来决定是否将多语言输入翻译成英语，使用监督方法检测理解失败。

Result: 选择性翻译方法能够显著缩小多语言推理差距，在仅翻译约20%输入的情况下，性能接近全翻译方法。

Conclusion: 理解失败是多语言推理差距的主要原因，通过检测和选择性缓解可以更有效地实现公平的多语言推理。

Abstract: Reasoning language models (RLMs) achieve strong performance on complex
reasoning tasks, yet they still suffer from a multilingual reasoning gap,
performing better in high-resource languages than in low-resource ones. While
recent efforts have reduced this gap, its underlying causes remain largely
unexplored. In this paper, we address this by showing that the multilingual
reasoning gap largely stems from failures in language understanding-the model's
inability to represent the multilingual input meaning into the dominant
language (i.e., English) within its reasoning trace. This motivates us to
examine whether understanding failures can be detected, as this ability could
help mitigate the multilingual reasoning gap. To this end, we evaluate a range
of detection methods and find that understanding failures can indeed be
identified, with supervised approaches performing best. Building on this, we
propose Selective Translation, a simple yet effective strategy that translates
the multilingual input into English only when an understanding failure is
detected. Experimental results show that Selective Translation bridges the
multilingual reasoning gap, achieving near full-translation performance while
using translation for only about 20% of inputs. Together, our work demonstrates
that understanding failures are the primary cause of the multilingual reasoning
gap and can be detected and selectively mitigated, providing key insight into
its origin and a promising path toward more equitable multilingual reasoning.
Our code and data are publicly available at
https://github.com/deokhk/RLM_analysis.

</details>


### [24] [A Unified Representation Underlying the Judgment of Large Language Models](https://arxiv.org/abs/2510.27328)
*Yi-Long Lu,Jiajun Song,Wei Wang*

Main category: cs.CL

TL;DR: 研究发现大型语言模型中的评价判断沿着一个主导维度——价值-赞同轴(VAA)进行计算，该轴同时编码主观价值和事实主张的赞同度，揭示了推理过程从公正推断向目标导向论证的转变机制。


<details>
  <summary>Details</summary>
Motivation: 探讨智能系统（包括生物和人工智能）的判断是依赖专门模块还是统一通用资源，特别是在发现LLMs中可解码的神经表征后，这些表征是否真正独立系统仍是一个开放问题。

Method: 通过一系列LLMs实验，识别出主导的评价判断维度VAA，并通过直接干预验证该统一表征如何作为控制信号引导生成过程。

Result: 发现VAA作为控制信号会引导生成过程构建与其评价状态一致的论证，即使以牺牲事实准确性为代价，这种现象被称为"推理从属化"。

Conclusion: 该发现为系统性偏见和幻觉提供了机制性解释，揭示了促进连贯判断的架构如何系统地削弱忠实推理。

Abstract: A central architectural question for both biological and artificial
intelligence is whether judgment relies on specialized modules or a unified,
domain-general resource. While the discovery of decodable neural
representations for distinct concepts in Large Language Models (LLMs) has
suggested a modular architecture, whether these representations are truly
independent systems remains an open question. Here we provide evidence for a
convergent architecture. Across a range of LLMs, we find that diverse
evaluative judgments are computed along a dominant dimension, which we term the
Valence-Assent Axis (VAA). This axis jointly encodes subjective valence ("what
is good") and the model's assent to factual claims ("what is true"). Through
direct interventions, we show this unified representation creates a critical
dependency: the VAA functions as a control signal that steers the generative
process to construct a rationale consistent with its evaluative state, even at
the cost of factual accuracy. This mechanism, which we term the subordination
of reasoning, shifts the process of reasoning from impartial inference toward
goal-directed justification. Our discovery offers a mechanistic account for
systemic bias and hallucination, revealing how an architecture that promotes
coherent judgment can systematically undermine faithful reasoning.

</details>


### [25] [TransAlign: Machine Translation Encoders are Strong Word Aligners, Too](https://arxiv.org/abs/2510.27337)
*Benedikt Ebing,Christian Goldschmied,Goran Glavaš*

Main category: cs.CL

TL;DR: 提出TransAlign，一种基于大规模多语言机器翻译模型编码器的新型词对齐器，在基于机器翻译的跨语言迁移中显著优于现有词对齐和非词对齐标签投影方法。


<details>
  <summary>Details</summary>
Motivation: 现有跨语言迁移方法需要词对齐进行标签投影，但传统词对齐器性能有限，而基于机器翻译模型的方法主要局限于利用编码器-解码器架构的交叉注意力，效果不佳。

Method: 利用大规模多语言机器翻译模型的编码器构建TransAlign词对齐器，用于跨语言迁移中的标签投影。

Result: TransAlign不仅实现了强大的词对齐性能，在基于机器翻译的跨语言迁移中显著优于现有词对齐器和最先进的非词对齐标签投影方法。

Conclusion: TransAlign为跨语言迁移中的标签投影提供了更有效的解决方案，展示了机器翻译模型编码器在词对齐任务中的潜力。

Abstract: In the absence of sizable training data for most world languages and NLP
tasks, translation-based strategies such as translate-test -- evaluating on
noisy source language data translated from the target language -- and
translate-train -- training on noisy target language data translated from the
source language -- have been established as competitive approaches for
cross-lingual transfer (XLT). For token classification tasks, these strategies
require label projection: mapping the labels from each token in the original
sentence to its counterpart(s) in the translation. To this end, it is common to
leverage multilingual word aligners (WAs) derived from encoder language models
such as mBERT or LaBSE. Despite obvious associations between machine
translation (MT) and WA, research on extracting alignments with MT models is
largely limited to exploiting cross-attention in encoder-decoder architectures,
yielding poor WA results. In this work, in contrast, we propose TransAlign, a
novel word aligner that utilizes the encoder of a massively multilingual MT
model. We show that TransAlign not only achieves strong WA performance but
substantially outperforms popular WA and state-of-the-art non-WA-based label
projection methods in MT-based XLT for token classification.

</details>


### [26] [ThoughtProbe: Classifier-Guided LLM Thought Space Exploration via Probing Representations](https://arxiv.org/abs/2510.27355)
*Zijian Wang,Chang Xu*

Main category: cs.CL

TL;DR: ThoughtProbe是一个新颖的推理时框架，利用LLM的隐藏推理特征来提升推理性能，通过分类器引导树结构响应空间探索，并通过分支聚合方法识别最优答案。


<details>
  <summary>Details</summary>
Motivation: 利用LLM的隐藏表示作为判别信号来指导树结构响应空间探索，提高推理性能。

Method: 在节点扩展时使用分类器对候选进行评分和排序，优先扩展高分候选；完成树扩展后收集所有分支答案形成候选池，通过分支聚合方法汇总所有支持分支的CoT分数来识别最优答案。

Result: 在多个算术推理基准测试中取得了显著改进，能够有效覆盖并识别有效的推理链。

Conclusion: ThoughtProbe框架通过全面探索推理空间，能够有效提升LLM的推理性能。

Abstract: This paper introduces ThoughtProbe, a novel inference time framework that
leverages the hidden reasoning features of Large Language Models (LLMs) to
improve their reasoning performance. Unlike previous works that manipulate the
hidden representations to steer LLM generation, we harness them as
discriminative signals to guide the tree structured response space exploration.
In each node expansion, a classifier serves as a scoring and ranking mechanism
that efficiently allocates computational resources by prioritizing higher score
candidates for continuation. After completing the tree expansion, we collect
answers from all branches to form a candidate answer pool. We then propose a
branch aggregation method that marginalizes over all supporting branches by
aggregating their CoT scores, thereby identifying the optimal answer from the
pool. Experimental results show that our framework's comprehensive exploration
not only covers valid reasoning chains but also effectively identifies them,
achieving significant improvements across multiple arithmetic reasoning
benchmarks.

</details>


### [27] [From the Rock Floor to the Cloud: A Systematic Survey of State-of-the-Art NLP in Battery Life Cycle](https://arxiv.org/abs/2510.27369)
*Tosin Adewumi,Martin Karlsson,Marcus Liwicki,Mikael Sjödahl,Lama Alkhaled,Rihab Gargouri,Nudrat Habib,Franz Hennie*

Main category: cs.CL

TL;DR: 本文对自然语言处理在完整电池生命周期中的应用进行了系统性调查，提出了技术语言处理框架用于欧盟数字电池护照和电池预测，分析了66篇相关论文并公开了评审材料。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注电池生命周期的单一阶段或方法，缺乏对整个生命周期中NLP应用的全面调查，且欧盟提出的数字电池护照需要新的技术语言处理框架支持。

Method: 采用PRISMA系统评价方法，使用Google Scholar、IEEE Xplore和Scopus三个数据库，评估了274篇科学论文，最终对66篇相关论文进行批判性评审。

Result: 研究发现电池领域正在出现新的NLP任务，促进了材料发现和生命周期其他阶段的发展，但仍面临缺乏标准基准等挑战。

Conclusion: 提出的技术语言处理框架结合代理AI和优化提示，能够有效应对现有挑战，为数字电池护照和电池预测提供支持。

Abstract: We present a comprehensive systematic survey of the application of natural
language processing (NLP) along the entire battery life cycle, instead of one
stage or method, and introduce a novel technical language processing (TLP)
framework for the EU's proposed digital battery passport (DBP) and other
general battery predictions. We follow the Preferred Reporting Items for
Systematic Reviews and Meta-Analyses (PRISMA) method and employ three reputable
databases or search engines, including Google Scholar, Institute of Electrical
and Electronics Engineers Xplore (IEEE Xplore), and Scopus. Consequently, we
assessed 274 scientific papers before the critical review of the final 66
relevant papers. We publicly provide artifacts of the review for validation and
reproducibility. The findings show that new NLP tasks are emerging in the
battery domain, which facilitate materials discovery and other stages of the
life cycle. Notwithstanding, challenges remain, such as the lack of standard
benchmarks. Our proposed TLP framework, which incorporates agentic AI and
optimized prompts, will be apt for tackling some of the challenges.

</details>


### [28] [Balancing Knowledge Updates: Toward Unified Modular Editing in LLMs](https://arxiv.org/abs/2510.27400)
*Jiahao Liu,Zijian Wang,Kuo Zhao,Dong Hu*

Main category: cs.CL

TL;DR: IntAttn-Edit是一种知识编辑方法，通过联合更新MLP和注意力模块，并采用知识平衡策略来提升编辑效果。


<details>
  <summary>Details</summary>
Motivation: 现有知识编辑方法主要关注MLP模块，忽略了注意力模块在知识存储中的作用，导致残留过时知识和编辑效果受限。

Method: 基于关联记忆范式，联合更新MLP和注意力模块，采用知识平衡策略按模块贡献比例分配更新幅度。

Result: 在标准基准测试中，IntAttn-Edit相比先前方法实现了更高的编辑成功率、更好的泛化能力和更强的知识保留能力。

Conclusion: 注意力模块在知识存储中发挥重要作用，联合编辑MLP和注意力模块并采用平衡策略能显著提升知识编辑效果。

Abstract: Knowledge editing has emerged as an efficient approach for updating factual
knowledge in large language models (LLMs). It typically locates knowledge
storage modules and then modifies their parameters. However, most existing
methods focus on the weights of multilayer perceptron (MLP) modules, which are
often identified as the main repositories of factual information. Other
components, such as attention (Attn) modules, are often ignored during editing.
This imbalance can leave residual outdated knowledge and limit editing
effectiveness. We perform comprehensive knowledge localization experiments on
advanced LLMs and find that Attn modules play a substantial role in factual
knowledge storage and retrieval, especially in earlier layers. Based on these
insights, we propose IntAttn-Edit, a method that extends the associative memory
paradigm to jointly update both MLP and Attn modules. Our approach uses a
knowledge balancing strategy that allocates update magnitudes in proportion to
each module's measured contribution to knowledge storage. Experiments on
standard benchmarks show that IntAttn-Edit achieves higher edit success, better
generalization, and stronger knowledge preservation than prior methods. Further
analysis shows that the balancing strategy keeps editing performance within an
optimal range across diverse settings.

</details>


### [29] [Awal -- Community-Powered Language Technology for Tamazight](https://arxiv.org/abs/2510.27407)
*Alp Öktem,Farida Boudichat*

Main category: cs.CL

TL;DR: Awal是一个社区驱动的塔马齐特语语言技术资源开发项目，通过awaldigital.org平台收集翻译和语音数据，但18个月内仅获得6,421个翻译对和3小时语音数据，显示标准众包方法在复杂社会语言环境中的局限性。


<details>
  <summary>Details</summary>
Motivation: 解决塔马齐特语在数字空间中的代表性不足问题，应对该语言的数据稀缺挑战。

Method: 建立社区驱动的协作平台，让使用者贡献翻译和语音数据，分析18个月的社区参与情况。

Result: 社区参与存在显著障碍，包括对书面塔马齐特语的信心不足和标准化挑战，实际数据贡献主要集中在语言学家和活动家群体。

Conclusion: 标准众包方法在复杂社会语言环境中的适用性有限，需要改进方法，正在利用收集的数据开发改进的开源机器翻译模型。

Abstract: This paper presents Awal, a community-powered initiative for developing
language technology resources for Tamazight. We provide a comprehensive review
of the NLP landscape for Tamazight, examining recent progress in computational
resources, and the emergence of community-driven approaches to address
persistent data scarcity. Launched in 2024, awaldigital.org platform addresses
the underrepresentation of Tamazight in digital spaces through a collaborative
platform enabling speakers to contribute translation and voice data. We analyze
18 months of community engagement, revealing significant barriers to
participation including limited confidence in written Tamazight and ongoing
standardization challenges. Despite widespread positive reception, actual data
contribution remained concentrated among linguists and activists. The modest
scale of community contributions -- 6,421 translation pairs and 3 hours of
speech data -- highlights the limitations of applying standard crowdsourcing
approaches to languages with complex sociolinguistic contexts. We are working
on improved open-source MT models using the collected data.

</details>


### [30] [Dynamic Affective Memory Management for Personalized LLM Agents](https://arxiv.org/abs/2510.27418)
*Junfeng Lu,Yueyan Li*

Main category: cs.CL

TL;DR: 提出了一种基于贝叶斯启发式记忆更新算法的情感场景记忆管理系统，通过最小化全局熵来动态维护记忆向量数据库，以提供更个性化的AI代理服务。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理系统主要依赖个性化外部记忆数据库，但面临记忆冗余、陈旧和记忆-上下文整合差等问题，主要由于交互过程中缺乏有效的记忆更新机制。

Method: 采用贝叶斯启发式记忆更新算法，引入记忆熵概念，使代理能够自主维护动态更新的记忆向量数据库，通过最小化全局熵来优化记忆管理。

Result: 在情感表达和情感变化评估基准DABench上的实验表明，该系统在个性化、逻辑一致性和准确性方面表现优异，消融研究验证了贝叶斯更新机制在缓解记忆膨胀方面的有效性。

Conclusion: 这项工作为长期记忆系统设计提供了新的见解，展示了基于熵最小化的动态记忆管理在提升AI代理个性化服务方面的潜力。

Abstract: Advances in large language models are making personalized AI agents a new
research focus. While current agent systems primarily rely on personalized
external memory databases to deliver customized experiences, they face
challenges such as memory redundancy, memory staleness, and poor memory-context
integration, largely due to the lack of effective memory updates during
interaction. To tackle these issues, we propose a new memory management system
designed for affective scenarios. Our approach employs a Bayesian-inspired
memory update algorithm with the concept of memory entropy, enabling the agent
to autonomously maintain a dynamically updated memory vector database by
minimizing global entropy to provide more personalized services. To better
evaluate the system's effectiveness in this context, we propose DABench, a
benchmark focusing on emotional expression and emotional change toward objects.
Experimental results demonstrate that, our system achieves superior performance
in personalization, logical coherence, and accuracy. Ablation studies further
validate the effectiveness of the Bayesian-inspired update mechanism in
alleviating memory bloat. Our work offers new insights into the design of
long-term memory systems.

</details>


### [31] [VCORE: Variance-Controlled Optimization-based Reweighting for Chain-of-Thought Supervision](https://arxiv.org/abs/2510.27462)
*Xuan Gong,Senmiao Wang,Hanbo Huang,Ruoyu Sun,Shiyu Liang*

Main category: cs.CL

TL;DR: VCORE是一个基于优化理论的token重新加权框架，通过将CoT监督重新表述为约束优化问题，实现跨token的自适应监督分配，提升LLMs在复杂推理任务中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 标准的交叉熵损失对所有token同等对待，忽略了推理轨迹中不同token的异质贡献，导致监督分配不当和泛化能力弱。

Method: 采用优化理论视角，将CoT监督重新表述为约束优化问题，实现原则性和自适应的token监督分配。

Result: 在数学和编程基准测试中，VCORE在领域内和领域外设置下均显著优于现有token重新加权方法，使用Qwen3系列和LLaMA-3.1-8B-Instruct模型验证。

Conclusion: VCORE不仅提升了推理性能，还作为后续强化学习的更有效初始化，为推进LLMs推理能力建立了更强基础。

Abstract: Supervised fine-tuning (SFT) on long chain-of-thought (CoT) trajectories has
emerged as a crucial technique for enhancing the reasoning abilities of large
language models (LLMs). However, the standard cross-entropy loss treats all
tokens equally, ignoring their heterogeneous contributions across a reasoning
trajectory. This uniform treatment leads to misallocated supervision and weak
generalization, especially in complex, long-form reasoning tasks. To address
this, we introduce \textbf{V}ariance-\textbf{C}ontrolled
\textbf{O}ptimization-based \textbf{RE}weighting (VCORE), a principled
framework that reformulates CoT supervision as a constrained optimization
problem. By adopting an optimization-theoretic perspective, VCORE enables a
principled and adaptive allocation of supervision across tokens, thereby
aligning the training objective more closely with the goal of robust reasoning
generalization. Empirical evaluations demonstrate that VCORE consistently
outperforms existing token reweighting methods. Across both in-domain and
out-of-domain settings, VCORE achieves substantial performance gains on
mathematical and coding benchmarks, using models from the Qwen3 series (4B, 8B,
32B) and LLaMA-3.1-8B-Instruct. Moreover, we show that VCORE serves as a more
effective initialization for subsequent reinforcement learning, establishing a
stronger foundation for advancing the reasoning capabilities of LLMs. The Code
will be released at https://github.com/coder-gx/VCORE.

</details>


### [32] [Diffuse Thinking: Exploring Diffusion Language Models as Efficient Thought Proposers for Reasoning](https://arxiv.org/abs/2510.27469)
*Chenyang Shao,Sijian Ren,Fengli Xu,Yong Li*

Main category: cs.CL

TL;DR: 提出了一种高效的协作推理框架，利用扩散语言模型生成候选思考，大语言模型评估质量，以解决自回归生成在推理任务中计算效率低的问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的自回归生成范式导致推理性能随测试时间计算呈次优扩展，通常需要过多计算开销来提出思考步骤，而性能提升有限。扩散语言模型可以通过单次前向传播并行去噪高效生成多样样本。

Method: 提出协作推理框架：使用扩散语言模型生成候选中间思考，大语言模型评估这些思考的质量，结合两者的优势。

Result: 在多样化基准测试中，该框架在复杂推理任务上实现了强大性能。

Conclusion: 该框架为未来研究提供了有前景的方向，能够减轻自回归生成的计算负担同时保持质量。

Abstract: In recent years, large language models (LLMs) have witnessed remarkable
advancements, with the test-time scaling law consistently enhancing the
reasoning capabilities. Through systematic evaluation and exploration of a
diverse spectrum of intermediate thoughts, LLMs demonstrate the potential to
generate deliberate reasoning steps, thereby substantially enhancing reasoning
accuracy. However, LLMs' autoregressive generation paradigm results in
reasoning performance scaling sub-optimally with test-time computation, often
requiring excessive computational overhead to propose thoughts while yielding
only marginal performance gains. In contrast, diffusion language models (DLMs)
can efficiently produce diverse samples through parallel denoising in a single
forward pass, inspiring us to leverage them for proposing intermediate
thoughts, thereby alleviating the computational burden associated with
autoregressive generation while maintaining quality. In this work, we propose
an efficient collaborative reasoning framework, leveraging DLMs to generate
candidate thoughts and LLMs to evaluate their quality. Experiments across
diverse benchmarks demonstrate that our framework achieves strong performance
in complex reasoning tasks, offering a promising direction for future research.
Our code is open-source at
https://anonymous.4open.science/r/Diffuse-Thinking-EC60.

</details>


### [33] [The aftermath of compounds: Investigating Compounds and their Semantic Representations](https://arxiv.org/abs/2510.27477)
*Swarang Joshi*

Main category: cs.CL

TL;DR: 比较GloVe和BERT嵌入在英语复合词处理中与人类语义判断的一致性，发现BERT在捕捉组合语义方面优于GloVe，可预测性是语义透明度的重要预测因子。


<details>
  <summary>Details</summary>
Motivation: 研究计算嵌入如何与人类语义判断对齐，特别是在英语复合词处理中，旨在推进计算心理语言学的发展。

Method: 使用静态词向量(GloVe)和上下文嵌入(BERT)，与人类对词素意义支配度(LMD)和语义透明度(ST)的评分进行比较，通过关联强度、频率和可预测性等指标计算嵌入衍生的LMD和ST度量。

Result: BERT嵌入比GloVe更好地捕捉组合语义，可预测性评分在人类和模型数据中都是语义透明度的强预测因子。

Conclusion: 这些发现通过阐明驱动复合词处理的因素并为基于嵌入的语义建模提供见解，推进了计算心理语言学的发展。

Abstract: This study investigates how well computational embeddings align with human
semantic judgments in the processing of English compound words. We compare
static word vectors (GloVe) and contextualized embeddings (BERT) against human
ratings of lexeme meaning dominance (LMD) and semantic transparency (ST) drawn
from a psycholinguistic dataset. Using measures of association strength
(Edinburgh Associative Thesaurus), frequency (BNC), and predictability (LaDEC),
we compute embedding-derived LMD and ST metrics and assess their relationships
with human judgments via Spearmans correlation and regression analyses. Our
results show that BERT embeddings better capture compositional semantics than
GloVe, and that predictability ratings are strong predictors of semantic
transparency in both human and model data. These findings advance computational
psycholinguistics by clarifying the factors that drive compound word processing
and offering insights into embedding-based semantic modeling.

</details>


### [34] [Effect of Domain Generalization Techniques in Low Resource Systems](https://arxiv.org/abs/2510.27512)
*Mahi Aminu,Chisom Chibuike,Fatimo Adebanjo,Omokolade Awosanya,Samuel Oyeneye*

Main category: cs.CL

TL;DR: 本文研究了两种因果域泛化方法在低资源自然语言任务中的应用：因果数据增强和不变因果表示学习，发现它们都能提升模型在未见领域的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中训练和测试数据分布往往不一致，特别是在低资源环境下，数据稀缺和领域多样性有限会阻碍模型的鲁棒泛化能力。

Method: 1. 因果数据增强方法：自动生成反事实样本来改善对伪相关性的鲁棒性，应用于NaijaSenti Twitter语料库的情感分类任务；2. 不变因果表示学习方法：使用DINER框架，将其适配到多语言设置中。

Result: 两种方法都增强了模型对未见领域的鲁棒性：因果数据增强在情感分类中带来一致的跨领域准确率提升；DINER因果表示学习改善了多语言情感分析中的分布外性能，尽管在不同语言间增益有所差异。

Conclusion: 因果域泛化方法在低资源自然语言处理任务中具有实际价值，能够有效提升模型在分布偏移下的泛化能力。

Abstract: Machine learning models typically assume that training and test data follow
the same distribution, an assumption that often fails in real-world scenarios
due to distribution shifts. This issue is especially pronounced in low-resource
settings, where data scarcity and limited domain diversity hinder robust
generalization. Domain generalization (DG) approaches address this challenge by
learning features that remain invariant across domains, often using causal
mechanisms to improve model robustness. In this study, we examine two distinct
causal DG techniques in low-resource natural language tasks. First, we
investigate a causal data augmentation (CDA) approach that automatically
generates counterfactual examples to improve robustness to spurious
correlations. We apply this method to sentiment classification on the
NaijaSenti Twitter corpus, expanding the training data with semantically
equivalent paraphrases to simulate controlled distribution shifts. Second, we
explore an invariant causal representation learning (ICRL) approach using the
DINER framework, originally proposed for debiasing aspect-based sentiment
analysis. We adapt DINER to a multilingual setting. Our findings demonstrate
that both approaches enhance robustness to unseen domains: counterfactual data
augmentation yields consistent cross-domain accuracy gains in sentiment
classification, while causal representation learning with DINER improves
out-of-distribution performance in multilingual sentiment analysis, albeit with
varying gains across languages.

</details>


### [35] [BiSparse-AAS: Bilinear Sparse Attention and Adaptive Spans Framework for Scalable and Efficient Text Summarization](https://arxiv.org/abs/2510.27516)
*Desta Haileselassie Hagos,Legand L. Burge,Anietie Andy,Anis Yazidi,Vladimir Vlassov*

Main category: cs.CL

TL;DR: 提出BiSparse-AAS框架，结合稀疏注意力、自适应跨度与双线性注意力，解决Transformer在长文档摘要中的二次复杂度问题，显著提升效率与性能。


<details>
  <summary>Details</summary>
Motivation: Transformer架构在文本摘要中表现出色，但其二次复杂度限制了在长文档上的可扩展性，需要更高效的注意力机制。

Method: 结合稀疏注意力降低计算成本，自适应跨度动态调整注意力范围，双线性注意力在精炼上下文中建模复杂词元交互。

Result: 在CNN/DailyMail和XSum数据集上分别实现约68.1%和52.6%的ROUGE平均提升，在OpenWebText和Gigaword数据集上保持强性能。

Conclusion: BiSparse-AAS通过解决效率、可扩展性和长序列建模问题，为实际文本摘要应用提供了统一的实用解决方案。

Abstract: Transformer-based architectures have advanced text summarization, yet their
quadratic complexity limits scalability on long documents. This paper
introduces BiSparse-AAS (Bilinear Sparse Attention with Adaptive Spans), a
novel framework that combines sparse attention, adaptive spans, and bilinear
attention to address these limitations. Sparse attention reduces computational
costs by focusing on the most relevant parts of the input, while adaptive spans
dynamically adjust the attention ranges. Bilinear attention complements both by
modeling complex token interactions within this refined context. BiSparse-AAS
consistently outperforms state-of-the-art baselines in both extractive and
abstractive summarization tasks, achieving average ROUGE improvements of about
68.1% on CNN/DailyMail and 52.6% on XSum, while maintaining strong performance
on OpenWebText and Gigaword datasets. By addressing efficiency, scalability,
and long-sequence modeling, BiSparse-AAS provides a unified, practical solution
for real-world text summarization applications.

</details>


### [36] [SQLSpace: A Representation Space for Text-to-SQL to Discover and Mitigate Robustness Gaps](https://arxiv.org/abs/2510.27532)
*Neha Srikanth,Victor Bursztyn,Puneet Mathur,Ani Nenkova*

Main category: cs.CL

TL;DR: SQLSpace是一种可解释、通用且紧凑的文本到SQL示例表示方法，用于评估文本到SQL模型性能，支持基准比较、细粒度性能分析和查询重写优化。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够深入理解文本到SQL模型性能的方法，超越简单的准确率评估，揭示基准间的组成差异和模型的具体表现模式。

Method: 提出SQLSpace表示方法，通过最小化人工干预从文本到SQL示例中推导出可解释的表示，支持三种用例：基准比较、细粒度性能分析和查询重写。

Result: SQLSpace能够揭示不同基准间的组成差异，暴露仅靠准确率无法发现的性能模式，并支持查询成功建模。

Conclusion: SQLSpace为文本到SQL评估提供了更深入的分析能力，能够进行基准对比、细粒度性能诊断和性能优化，这些分析仅靠原始示例难以实现。

Abstract: We introduce SQLSpace, a human-interpretable, generalizable, compact
representation for text-to-SQL examples derived with minimal human
intervention. We demonstrate the utility of these representations in evaluation
with three use cases: (i) closely comparing and contrasting the composition of
popular text-to-SQL benchmarks to identify unique dimensions of examples they
evaluate, (ii) understanding model performance at a granular level beyond
overall accuracy scores, and (iii) improving model performance through targeted
query rewriting based on learned correctness estimation. We show that SQLSpace
enables analysis that would be difficult with raw examples alone: it reveals
compositional differences between benchmarks, exposes performance patterns
obscured by accuracy alone, and supports modeling of query success.

</details>


### [37] [Patient-Centered Summarization Framework for AI Clinical Summarization: A Mixed-Methods Design](https://arxiv.org/abs/2510.27535)
*Maria Lizarazo Jimenez,Ana Gabriela Claros,Kieran Green,David Toro-Tobon,Felipe Larios,Sheena Asthana,Camila Wenczenovicz,Kerly Guevara Maldonado,Luis Vilatuna-Andrango,Cristina Proano-Velez,Satya Sai Sri Bandi,Shubhangi Bagewadi,Megan E. Branda,Misk Al Zahidy,Saturnino Luz,Mirella Lapata,Juan P. Brito,Oscar J. Ponce-Ponte*

Main category: cs.CL

TL;DR: 该研究提出了患者中心摘要(PCS)新标准，通过混合方法开发框架，评估开源LLM在生成包含患者价值观的临床摘要方面的表现。


<details>
  <summary>Details</summary>
Motivation: 当前LLM生成的临床摘要过于关注患者生物学信息，而忽略了患者的偏好、价值观、愿望和关切，无法实现真正的患者中心护理。

Method: 采用混合方法：通过患者和临床医生的半结构化访谈确定摘要内容标准，制定标注指南，然后让临床医生创建88份房颤咨询的黄金标准PCS，最后使用5个开源LLM进行零样本和少样本提示生成摘要评估。

Result: 患者强调生活方式、社会支持、近期压力和护理价值观；临床医生需要简洁的功能、心理社会和情感背景。Mistral-8B和Llama-3.1-8B在零样本表现最佳，Llama-3.1-8B在少样本表现最佳。完整性和流畅性相似，但正确性和患者中心性仍以人类PCS为优。

Conclusion: 开源LLM在生成患者中心临床摘要方面显示出潜力，但在正确性和患者中心性方面仍需改进，需要进一步研究以实现人类水平的性能。

Abstract: Large Language Models (LLMs) are increasingly demonstrating the potential to
reach human-level performance in generating clinical summaries from
patient-clinician conversations. However, these summaries often focus on
patients' biology rather than their preferences, values, wishes, and concerns.
To achieve patient-centered care, we propose a new standard for Artificial
Intelligence (AI) clinical summarization tasks: Patient-Centered Summaries
(PCS). Our objective was to develop a framework to generate PCS that capture
patient values and ensure clinical utility and to assess whether current
open-source LLMs can achieve human-level performance in this task. We used a
mixed-methods process. Two Patient and Public Involvement groups (10 patients
and 8 clinicians) in the United Kingdom participated in semi-structured
interviews exploring what personal and contextual information should be
included in clinical summaries and how it should be structured for clinical
use. Findings informed annotation guidelines used by eight clinicians to create
gold-standard PCS from 88 atrial fibrillation consultations. Sixteen
consultations were used to refine a prompt aligned with the guidelines. Five
open-source LLMs (Llama-3.2-3B, Llama-3.1-8B, Mistral-8B, Gemma-3-4B, and
Qwen3-8B) generated summaries for 72 consultations using zero-shot and few-shot
prompting, evaluated with ROUGE-L, BERTScore, and qualitative metrics. Patients
emphasized lifestyle routines, social support, recent stressors, and care
values. Clinicians sought concise functional, psychosocial, and emotional
context. The best zero-shot performance was achieved by Mistral-8B (ROUGE-L
0.189) and Llama-3.1-8B (BERTScore 0.673); the best few-shot by Llama-3.1-8B
(ROUGE-L 0.206, BERTScore 0.683). Completeness and fluency were similar between
experts and models, while correctness and patient-centeredness favored human
PCS.

</details>


### [38] [DialectalArabicMMLU: Benchmarking Dialectal Capabilities in Arabic and Multilingual Language Models](https://arxiv.org/abs/2510.27543)
*Malik H. Altakrori,Nizar Habash,Abdelhakim Freihat,Younes Samih,Kirill Chirkunov,Muhammed AbuOdeh,Radu Florian,Teresa Lynn,Preslav Nakov,Alham Fikri Aji*

Main category: cs.CL

TL;DR: 提出了DialectalArabicMMLU基准，用于评估大语言模型在阿拉伯语方言上的表现，填补了现有基准在方言评估方面的空白。


<details>
  <summary>Details</summary>
Motivation: 现有的阿拉伯语和多语言基准主要关注现代标准阿拉伯语(MSA)，而方言变体在日常交流中广泛使用但在评估中代表性不足。

Method: 基于MMLU-Redux框架，手动翻译和改编了3K个多选题问答对到五种主要阿拉伯语方言(叙利亚、埃及、阿联酋、沙特和摩洛哥)，共生成15K个QA对，覆盖32个学术和专业领域。

Result: 评估了19个开源阿拉伯语和多语言LLM(1B-13B参数)，发现不同方言间存在显著的性能差异，揭示了方言泛化方面的持续差距。

Conclusion: DialectalArabicMMLU为衡量阿拉伯语方言理解提供了首个统一的人工策划资源，促进了更包容的评估和未来模型发展。

Abstract: We present DialectalArabicMMLU, a new benchmark for evaluating the
performance of large language models (LLMs) across Arabic dialects. While
recently developed Arabic and multilingual benchmarks have advanced LLM
evaluation for Modern Standard Arabic (MSA), dialectal varieties remain
underrepresented despite their prevalence in everyday communication.
DialectalArabicMMLU extends the MMLU-Redux framework through manual translation
and adaptation of 3K multiple-choice question-answer pairs into five major
dialects (Syrian, Egyptian, Emirati, Saudi, and Moroccan), yielding a total of
15K QA pairs across 32 academic and professional domains (22K QA pairs when
also including English and MSA). The benchmark enables systematic assessment of
LLM reasoning and comprehension beyond MSA, supporting both task-based and
linguistic analysis. We evaluate 19 open-weight Arabic and multilingual LLMs
(1B-13B parameters) and report substantial performance variation across
dialects, revealing persistent gaps in dialectal generalization.
DialectalArabicMMLU provides the first unified, human-curated resource for
measuring dialectal understanding in Arabic, thus promoting more inclusive
evaluation and future model development.

</details>


### [39] [Multilingual BERT language model for medical tasks: Evaluation on domain-specific adaptation and cross-linguality](https://arxiv.org/abs/2510.27552)
*Yinghao Luo,Lang Zhou,Amrish Jhingoer,Klaske Vliegenthart Jongbloed,Carlijn Jordans,Ben Werkhoven,Tom Seinen,Erik van Mulligen,Casper Rokx,Yunlei Li*

Main category: cs.CL

TL;DR: 该研究探讨了在荷兰语、罗马尼亚语和西班牙语三种语言上，通过领域特定语料进行进一步预训练对医疗NLP任务性能的影响。结果显示领域适应能显著提升任务表现，临床领域适应的模型优于一般生物医学领域适应的模型，并观察到跨语言可迁移性。


<details>
  <summary>Details</summary>
Motivation: 在多语言医疗应用中，领域特定的自然语言处理工具稀缺，特别是对于低资源语言。虽然多语言BERT提供了缓解语言差距的潜力，但低资源语言的医疗NLP任务仍未被充分探索。

Method: 进行了四项实验创建医疗领域模型，通过领域特定语料进行进一步预训练。然后将这些模型在三个下游任务上进行微调：荷兰语临床笔记的自动患者筛查、罗马尼亚语和西班牙语临床笔记的命名实体识别。

Result: 领域适应显著提升了任务性能。临床领域适应的模型优于更一般的生物医学领域适应模型。观察到跨语言可迁移性的证据。

Conclusion: 这些发现突显了领域适应和跨语言能力在医疗NLP中的可行性。在低资源语言环境下，这些发现可为开发多语言医疗NLP系统提供有意义的指导，以缓解训练数据不足的问题，从而提高模型性能。

Abstract: In multilingual healthcare applications, the availability of domain-specific
natural language processing(NLP) tools is limited, especially for low-resource
languages. Although multilingual bidirectional encoder representations from
transformers (BERT) offers a promising motivation to mitigate the language gap,
the medical NLP tasks in low-resource languages are still underexplored.
Therefore, this study investigates how further pre-training on domain-specific
corpora affects model performance on medical tasks, focusing on three
languages: Dutch, Romanian and Spanish. In terms of further pre-training, we
conducted four experiments to create medical domain models. Then, these models
were fine-tuned on three downstream tasks: Automated patient screening in Dutch
clinical notes, named entity recognition in Romanian and Spanish clinical
notes. Results show that domain adaptation significantly enhanced task
performance. Furthermore, further differentiation of domains, e.g. clinical and
general biomedical domains, resulted in diverse performances. The clinical
domain-adapted model outperformed the more general biomedical domain-adapted
model. Moreover, we observed evidence of cross-lingual transferability.
Moreover, we also conducted further investigations to explore potential reasons
contributing to these performance differences. These findings highlight the
feasibility of domain adaptation and cross-lingual ability in medical NLP.
Within the low-resource language settings, these findings can provide
meaningful guidance for developing multilingual medical NLP systems to mitigate
the lack of training data and thereby improve the model performance.

</details>


### [40] [Data-Efficient Domain Adaptation for LLM-based MT using Contrastive Preference Optimization](https://arxiv.org/abs/2510.27556)
*Inacio Vieira,Antonio Castaldo,James O'Doherty,Sheila Castilho*

Main category: cs.CL

TL;DR: 使用CPO进行数据高效的领域自适应，通过将基础模型的原始输出作为'被拒绝'的翻译，将人工批准的翻译记忆条目作为'选择'的翻译来合成偏好对，仅用14.7k偏好对就达到接近160k+ SFT样本的性能。


<details>
  <summary>Details</summary>
Motivation: LLMs需要适应领域特定需求，但仅依赖SFT可能成本高昂。

Method: 应用CPO模拟后编辑工作流，通过将基础模型原始输出作为'被拒绝'翻译，人工批准的TM条目作为'选择'翻译来合成偏好对。

Result: 在英-巴西葡萄牙语和英-韩语实验中，仅使用14.7k偏好对，模型性能接近使用160k+ SFT样本训练的效果，显示出显著的数据效率。

Conclusion: 虽然展示了在机器翻译中的有效性，但CPO的应用自然推广到其他生成任务，其中模型的初始草稿可以作为对比信号与黄金参考对比。

Abstract: LLMs often require adaptation to domain-specific requirements, a process that
can be expensive when relying solely on SFT. We present an empirical study on
applying CPO to simulate a post-editing workflow for data-efficient domain
adaptation. Our approach synthesizes preference pairs by treating the base
model's own raw output as the 'rejected' translation and the human-approved TM
entry as the 'chosen' one. This method provides direct feedback on the model's
current knowledge, guiding it to align with domain-specific standards.
Experiments in English-Brazilian Portuguese and English-Korean show that, by
using just 14.7k preference pairs, the model achieves performance close to that
of a model trained on 160k+ samples with SFT, demonstrating significant data
efficiency. Although we showcase its effectiveness in MT, this application of
CPO naturally generalizes to other generative tasks where a model's initial
drafts can serve as a contrastive signal against a golden reference.

</details>


### [41] [MARAG-R1: Beyond Single Retriever via Reinforcement-Learned Multi-Tool Agentic Retrieval](https://arxiv.org/abs/2510.27569)
*Qi Luo,Xiaonan Li,Yuxin Wang,Tingshuo Fan,Yuan Li,Xinchi Chen,Xipeng Qiu*

Main category: cs.CL

TL;DR: MARAG-R1是一个基于强化学习的多工具RAG框架，通过动态协调多种检索机制来解决传统单检索器RAG系统信息获取受限的问题，在语料库级推理任务中取得了新的SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 传统RAG系统依赖单一检索器和固定的top-k选择，导致只能访问语料库中狭窄且静态的信息子集，这成为获取全面外部信息的主要瓶颈，特别是在需要语料库级推理的任务中。

Method: 提出MARAG-R1框架，为LLM配备四种检索工具（语义搜索、关键词搜索、过滤和聚合），通过两阶段训练（监督微调+强化学习）学习如何和何时使用这些工具，实现推理和检索的交错进行。

Result: 在GlobalQA、HotpotQA和2WikiMultiHopQA上的实验表明，MARAG-R1显著优于强基线方法，在语料库级推理任务中取得了新的最先进结果。

Conclusion: 多工具RAG框架通过动态协调多种检索机制，能够更全面、更精确地获取外部信息，有效解决了传统RAG系统的局限性，提升了语料库级推理能力。

Abstract: Large Language Models (LLMs) excel at reasoning and generation but are
inherently limited by static pretraining data, resulting in factual
inaccuracies and weak adaptability to new information. Retrieval-Augmented
Generation (RAG) addresses this issue by grounding LLMs in external knowledge;
However, the effectiveness of RAG critically depends on whether the model can
adequately access relevant information. Existing RAG systems rely on a single
retriever with fixed top-k selection, restricting access to a narrow and static
subset of the corpus. As a result, this single-retriever paradigm has become
the primary bottleneck for comprehensive external information acquisition,
especially in tasks requiring corpus-level reasoning. To overcome this
limitation, we propose MARAG-R1, a reinforcement-learned multi-tool RAG
framework that enables LLMs to dynamically coordinate multiple retrieval
mechanisms for broader and more precise information access. MARAG-R1 equips the
model with four retrieval tools -- semantic search, keyword search, filtering,
and aggregation -- and learns both how and when to use them through a two-stage
training process: supervised fine-tuning followed by reinforcement learning.
This design allows the model to interleave reasoning and retrieval,
progressively gathering sufficient evidence for corpus-level synthesis.
Experiments on GlobalQA, HotpotQA, and 2WikiMultiHopQA demonstrate that
MARAG-R1 substantially outperforms strong baselines and achieves new
state-of-the-art results in corpus-level reasoning tasks.

</details>


### [42] [SpecAttn: Speculating Sparse Attention](https://arxiv.org/abs/2510.27641)
*Harsh Shah*

Main category: cs.CL

TL;DR: SpecAttn是一种无需训练的方法，通过利用推测解码中草稿模型计算的注意力权重来识别重要token，实现预训练transformer中的高效稀疏注意力，减少75%以上的KV缓存访问。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在推理时面临计算瓶颈，特别是随着上下文长度增加，自注意力机制的二次复杂度成为主要问题。

Method: 使用KL散度进行草稿与目标模型的层对齐、GPU优化的无排序top-p token选择算法、基于预测的动态KV缓存剪枝。

Result: 在PG-19数据集上实现超过75%的KV缓存访问减少，仅增加15.29%的困惑度，显著优于现有稀疏注意力方法。

Conclusion: 推测执行可以通过近似验证来增强，而不会导致显著的性能下降。

Abstract: Large Language Models (LLMs) face significant computational bottlenecks
during inference due to the quadratic complexity of self-attention mechanisms,
particularly as context lengths increase. We introduce SpecAttn, a novel
training-free approach that seamlessly integrates with existing speculative
decoding techniques to enable efficient sparse attention in pre-trained
transformers. Our key insight is to exploit the attention weights already
computed by the draft model during speculative decoding to identify important
tokens for the target model, eliminating redundant computation while
maintaining output quality. SpecAttn employs three core techniques: KL
divergence-based layer alignment between draft and target models, a
GPU-optimized sorting-free algorithm for top-p token selection from draft
attention patterns, and dynamic key-value cache pruning guided by these
predictions. By leveraging the computational work already performed in standard
speculative decoding pipelines, SpecAttn achieves over 75% reduction in
key-value cache accesses with a mere 15.29% increase in perplexity on the PG-19
dataset, significantly outperforming existing sparse attention methods. Our
approach demonstrates that speculative execution can be enhanced to provide
approximate verification without significant performance degradation.

</details>


### [43] [Culture Cartography: Mapping the Landscape of Cultural Knowledge](https://arxiv.org/abs/2510.27672)
*Caleb Ziems,William Held,Jane Yu,Amir Goldberg,David Grusky,Diyi Yang*

Main category: cs.CL

TL;DR: 提出了一种名为CultureCartography的混合主动方法，通过LLM初始化低置信度问题的标注，让人类参与者填补知识空白并引导模型关注重要文化主题，从而更有效地发现LLM缺失的文化特定知识。


<details>
  <summary>Details</summary>
Motivation: 为了让LLM更好地服务全球用户，需要获取文化特定知识，但这些知识可能在预训练期间未被学习。传统方法要么是研究者定义问题让用户回答，要么是用户主动生成数据，缺乏有效的混合协作。

Method: 提出CultureCartography方法：LLM初始化标注，提出其低置信度回答的问题；人类参与者填补这些知识空白并通过直接编辑引导模型关注重要主题；实现为CultureExplorer工具。

Result: 与基线方法相比，CultureExplorer更有效地产生了像DeepSeek R1和GPT-4o这样的领先模型所缺失的知识，即使有网络搜索也无法获取。在这些数据上微调使Llama-3.1-8B在相关文化基准上的准确率提高了19.2%。

Conclusion: 混合主动方法能够更有效地识别和收集LLM缺失的文化特定知识，通过人类和LLM的协作可以显著提升模型的文化理解能力。

Abstract: To serve global users safely and productively, LLMs need culture-specific
knowledge that might not be learned during pre-training. How do we find such
knowledge that is (1) salient to in-group users, but (2) unknown to LLMs? The
most common solutions are single-initiative: either researchers define
challenging questions that users passively answer (traditional annotation), or
users actively produce data that researchers structure as benchmarks (knowledge
extraction). The process would benefit from mixed-initiative collaboration,
where users guide the process to meaningfully reflect their cultures, and LLMs
steer the process towards more challenging questions that meet the researcher's
goals. We propose a mixed-initiative methodology called CultureCartography.
Here, an LLM initializes annotation with questions for which it has
low-confidence answers, making explicit both its prior knowledge and the gaps
therein. This allows a human respondent to fill these gaps and steer the model
towards salient topics through direct edits. We implement this methodology as a
tool called CultureExplorer. Compared to a baseline where humans answer
LLM-proposed questions, we find that CultureExplorer more effectively produces
knowledge that leading models like DeepSeek R1 and GPT-4o are missing, even
with web search. Fine-tuning on this data boosts the accuracy of Llama-3.1-8B
by up to 19.2% on related culture benchmarks.

</details>


### [44] [Continuous Autoregressive Language Models](https://arxiv.org/abs/2510.27688)
*Chenze Shao,Darren Li,Fandong Meng,Jie Zhou*

Main category: cs.CL

TL;DR: CALM通过将语言建模从离散的下一词预测转变为连续的下一向量预测，使用高保真自编码器将K个词压缩为单个连续向量，显著减少生成步骤数量，提高计算效率。


<details>
  <summary>Details</summary>
Motivation: 克服大语言模型逐词生成的序列性瓶颈，通过提高每个生成步骤的语义带宽来提升模型效率。

Method: 提出连续自回归语言模型(CALM)，使用高保真自编码器将词块压缩为连续向量，开发无似然框架进行训练、评估和可控采样。

Result: CALM显著改善了性能-计算权衡，以显著更低的计算成本实现了强离散基线的性能。

Conclusion: 下一向量预测是构建超高效语言模型的有力且可扩展的途径。

Abstract: The efficiency of large language models (LLMs) is fundamentally limited by
their sequential, token-by-token generation process. We argue that overcoming
this bottleneck requires a new design axis for LLM scaling: increasing the
semantic bandwidth of each generative step. To this end, we introduce
Continuous Autoregressive Language Models (CALM), a paradigm shift from
discrete next-token prediction to continuous next-vector prediction. CALM uses
a high-fidelity autoencoder to compress a chunk of K tokens into a single
continuous vector, from which the original tokens can be reconstructed with
over 99.9\% accuracy. This allows us to model language as a sequence of
continuous vectors instead of discrete tokens, which reduces the number of
generative steps by a factor of K. The paradigm shift necessitates a new
modeling toolkit; therefore, we develop a comprehensive likelihood-free
framework that enables robust training, evaluation, and controllable sampling
in the continuous domain. Experiments show that CALM significantly improves the
performance-compute trade-off, achieving the performance of strong discrete
baselines at a significantly lower computational cost. More importantly, these
findings establish next-vector prediction as a powerful and scalable pathway
towards ultra-efficient language models. Code:
https://github.com/shaochenze/calm. Project:
https://shaochenze.github.io/blog/2025/CALM.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [45] [RepV: Safety-Separable Latent Spaces for Scalable Neurosymbolic Plan Verification](https://arxiv.org/abs/2510.26935)
*Yunhao Yang,Neel P. Bhatt,Pranay Samineni,Rohan Siva,Zhanyang Wang,Ufuk Topcu*

Main category: cs.RO

TL;DR: RepV是一个神经符号验证器，通过学习安全与不安全计划线性可分的潜在空间，统一了形式化方法和深度学习方法，为AI系统在安全关键领域的规则合规性提供可验证保证。


<details>
  <summary>Details</summary>
Motivation: AI系统进入安全关键领域时，验证其行为是否符合明确定义的规则仍然具有挑战性。形式化方法提供可证明保证但需要手动编写时序逻辑规范，表达能力有限；深度学习方法能够评估计划对自然语言约束的符合性，但其不透明的决策过程可能导致严重误分类。

Method: RepV从模型检查器标记的少量种子计划集开始，训练一个轻量级投影器，将每个计划与语言模型生成的原理嵌入到低维空间中；然后使用冻结的线性边界在单次前向传播中验证未见自然语言规则的合规性。

Result: 实验评估显示，RepV将合规性预测准确率提高了高达15%，同时仅增加不到0.2M参数。其精化框架在各种规划领域都优于普通微调基线方法。

Conclusion: 安全可分离的潜在空间为可靠的神经符号计划验证提供了可扩展、即插即用的原语，能够在不依赖人工标注的情况下提高规则合规性。

Abstract: As AI systems migrate to safety-critical domains, verifying that their
actions comply with well-defined rules remains a challenge. Formal methods
provide provable guarantees but demand hand-crafted temporal-logic
specifications, offering limited expressiveness and accessibility. Deep
learning approaches enable evaluation of plans against natural-language
constraints, yet their opaque decision process invites misclassifications with
potentially severe consequences. We introduce RepV, a neurosymbolic verifier
that unifies both views by learning a latent space where safe and unsafe plans
are linearly separable. Starting from a modest seed set of plans labeled by an
off-the-shelf model checker, RepV trains a lightweight projector that embeds
each plan, together with a language model-generated rationale, into a
low-dimensional space; a frozen linear boundary then verifies compliance for
unseen natural-language rules in a single forward pass.
  Beyond binary classification, RepV provides a probabilistic guarantee on the
likelihood of correct verification based on its position in the latent space.
This guarantee enables a guarantee-driven refinement of the planner, improving
rule compliance without human annotations. Empirical evaluations show that RepV
improves compliance prediction accuracy by up to 15% compared to baseline
methods while adding fewer than 0.2M parameters. Furthermore, our refinement
framework outperforms ordinary fine-tuning baselines across various planning
domains. These results show that safety-separable latent spaces offer a
scalable, plug-and-play primitive for reliable neurosymbolic plan verification.
Code and data are available at: https://repv-project.github.io/.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [46] [Evaluating Perspectival Biases in Cross-Modal Retrieval](https://arxiv.org/abs/2510.26861)
*Teerapol Saengsukhiran,Peerawat Chomphooyod,Narabodee Rodjananant,Chompakorn Chaksangchaichot,Patawee Prakrankamanant,Witthawin Sripheanpol,Pak Lovichit,SarChaksaana Nutanong,Ekapol Chuangsuwanich*

Main category: cs.IR

TL;DR: 该论文研究了多模态检索系统中的两种偏见：流行度偏见（图像到文本检索中偏向流行语言）和关联偏见（文本到图像检索中偏向文化关联图像），发现显式对齐能有效缓解流行度偏见，但关联偏见是更棘手的挑战。


<details>
  <summary>Details</summary>
Motivation: 多模态检索系统理论上应在语义空间运行，不受查询语言或文化背景影响，但实际上存在系统性偏见，需要研究这些偏见及其缓解策略。

Method: 研究两种偏见：流行度偏见（语言流行度导致的偏差）和关联偏见（文化关联导致的偏差），比较不同缓解策略的效果。

Result: 显式对齐能有效缓解流行度偏见，但关联偏见仍然是一个更困难的挑战，需要针对性策略而非简单数据扩展。

Conclusion: 实现真正公平的多模态系统需要超越简单数据扩展的针对性策略，文化关联产生的偏见比语言流行度产生的偏见更具挑战性。

Abstract: Multimodal retrieval systems are expected to operate in a semantic space,
agnostic to the language or cultural origin of the query. In practice, however,
retrieval outcomes systematically reflect perspectival biases: deviations
shaped by linguistic prevalence and cultural associations. We study two such
biases. First, prevalence bias refers to the tendency to favor entries from
prevalent languages over semantically faithful entries in image-to-text
retrieval. Second, association bias refers to the tendency to favor images
culturally associated with the query over semantically correct ones in
text-to-image retrieval. Results show that explicit alignment is a more
effective strategy for mitigating prevalence bias. However, association bias
remains a distinct and more challenging problem. These findings suggest that
achieving truly equitable multimodal systems requires targeted strategies
beyond simple data scaling and that bias arising from cultural association may
be treated as a more challenging problem than one arising from linguistic
prevalence.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [47] [DRAMA: Unifying Data Retrieval and Analysis for Open-Domain Analytic Queries](https://arxiv.org/abs/2510.27238)
*Chuxuan Hu,Maxwell Yang,James Weiland,Yeji Lim,Suhas Palawala,Daniel Kang*

Main category: cs.DB

TL;DR: 提出了DRAMA端到端范式，通过自然语言在大规模开放域数据上回答用户分析查询，统一数据收集、转换和分析为单一流程，并在基准测试中优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 手动进行真实世界数据分析劳动密集且低效，现有自动化数据科学工作流系统无法同时支持开放域数据收集、结构化数据转换和分析推理这三个关键能力。

Method: 提出DRAMA范式，开发DRAMA-Bot多智能体系统，包含数据检索器（协调子智能体执行数据收集和转换）和数据分析器（在检索数据上进行结构化推理）。

Result: 在DRAMA-Bench基准测试中，DRAMA-Bot达到86.5%的任务准确率，成本仅0.05美元，优于所有基线方法，准确率最高提升6.9倍，成本不到1/6。

Conclusion: DRAMA范式有效解决了自动化数据分析中的关键挑战，在开放域数据收集、转换和分析方面表现出色，为数据科学工作流自动化提供了可行方案。

Abstract: Manually conducting real-world data analyses is labor-intensive and
inefficient. Despite numerous attempts to automate data science workflows, none
of the existing paradigms or systems fully demonstrate all three key
capabilities required to support them effectively: (1) open-domain data
collection, (2) structured data transformation, and (3) analytic reasoning.
  To overcome these limitations, we propose DRAMA, an end-to-end paradigm that
answers users' analytic queries in natural language on large-scale open-domain
data. DRAMA unifies data collection, transformation, and analysis as a single
pipeline. To quantitatively evaluate system performance on tasks representative
of DRAMA, we construct a benchmark, DRAMA-Bench, consisting of two categories
of tasks: claim verification and question answering, each comprising 100
instances. These tasks are derived from real-world applications that have
gained significant public attention and require the retrieval and analysis of
open-domain data. We develop DRAMA-Bot, a multi-agent system designed following
DRAMA. It comprises a data retriever that collects and transforms data by
coordinating the execution of sub-agents, and a data analyzer that performs
structured reasoning over the retrieved data. We evaluate DRAMA-Bot on
DRAMA-Bench together with five state-of-the-art baseline agents. DRAMA-Bot
achieves 86.5% task accuracy at a cost of $0.05, outperforming all baselines
with up to 6.9 times the accuracy and less than 1/6 of the cost. DRAMA is
publicly available at https://github.com/uiuc-kang-lab/drama.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [48] [Towards a Measure of Algorithm Similarity](https://arxiv.org/abs/2510.27063)
*Shairoz Sohail,Taher Ali*

Main category: cs.LG

TL;DR: 提出了EMOC框架，将算法实现嵌入到特征空间中，用于评估算法相似性，并发布了PACD数据集来验证该框架的有效性。


<details>
  <summary>Details</summary>
Motivation: 在克隆检测和程序合成等应用中，需要一种实用且一致的算法相似性度量方法，但现有方法在概念上存在竞争且难以统一。

Method: 开发了EMOC（评估-内存-操作-复杂度）框架，将算法实现嵌入到特征空间，并编译了PACD数据集进行验证。

Result: EMOC特征支持算法类型的聚类和分类、近重复检测以及LLM生成程序的多样性量化。

Conclusion: EMOC框架为算法相似性分析提供了实用的解决方案，相关代码和数据已发布以促进可重复性和未来研究。

Abstract: Given two algorithms for the same problem, can we determine whether they are
meaningfully different? In full generality, the question is uncomputable, and
empirically it is muddied by competing notions of similarity. Yet, in many
applications (such as clone detection or program synthesis) a pragmatic and
consistent similarity metric is necessary. We review existing equivalence and
similarity notions and introduce EMOC: An
Evaluation-Memory-Operations-Complexity framework that embeds algorithm
implementations into a feature space suitable for downstream tasks. We compile
PACD, a curated dataset of verified Python implementations across three
problems, and show that EMOC features support clustering and classification of
algorithm types, detection of near-duplicates, and quantification of diversity
in LLM-generated programs. Code, data, and utilities for computing EMOC
embeddings are released to facilitate reproducibility and future work on
algorithm similarity.

</details>


### [49] [Higher-order Linear Attention](https://arxiv.org/abs/2510.27258)
*Yifan Zhang,Zhen Qin,Quanquan Gu*

Main category: cs.LG

TL;DR: HLA（高阶线性注意力）是一种因果流式机制，通过紧凑的前缀充分统计实现高阶交互，在保持恒定大小状态的同时以线性时间计算每个令牌输出，无需构建n×n矩阵。


<details>
  <summary>Details</summary>
Motivation: 解决自回归语言模型扩展到长上下文时缩放点积注意力的二次成本问题，同时克服现有线性时间注意力和状态空间模型在表达能力上的限制。

Method: 引入高阶线性注意力机制，使用紧凑前缀充分统计实现高阶交互，提供闭式流式恒等式、严格因果掩码变体和基于关联扫描的块并行训练方案。

Result: HLA在保持恒定状态大小的同时实现线性时间计算，能够精确复现串行递归的激活，并支持扩展到三阶及更高阶。

Conclusion: HLA作为一个原则性的可扩展构建块，将类似注意力的数据相关混合与现代循环架构的效率相结合。

Abstract: The quadratic cost of scaled dot-product attention is a central obstacle to
scaling autoregressive language models to long contexts. Linear-time attention
and State Space Models (SSMs) provide scalable alternatives but are typically
restricted to first-order or kernel-based approximations, which can limit
expressivity. We introduce Higher-order Linear Attention (HLA), a causal,
streaming mechanism that realizes higher interactions via compact prefix
sufficient statistics. In the second-order case, HLA maintains a constant-size
state and computes per-token outputs in linear time without materializing any
$n \times n$ matrices. We give closed-form streaming identities, a strictly
causal masked variant using two additional summaries, and a chunk-parallel
training scheme based on associative scans that reproduces the activations of a
serial recurrence exactly. We further outline extensions to third and higher
orders. Collectively, these results position HLA as a principled, scalable
building block that combines attention-like, data-dependent mixing with the
efficiency of modern recurrent architectures. Project Page:
https://github.com/yifanzhang-pro/HLA.

</details>


### [50] [Un-Attributability: Computing Novelty From Retrieval & Semantic Similarity](https://arxiv.org/abs/2510.27313)
*Philipp Davydov,Ameya Prabhu,Matthias Bethge,Elisa Nguyen,Seong Joon Oh*

Main category: cs.LG

TL;DR: 提出了一种基于不可归因性的语义新颖性评估方法，通过两阶段检索流程来判断模型输出是否无法归因于预训练语料中的任何示例。


<details>
  <summary>Details</summary>
Motivation: 传统训练数据归因方法关注哪些训练示例影响了模型输出，本文反过来研究哪些输出无法归因于任何预训练示例，以此衡量语义新颖性。

Method: 使用两阶段检索流程：先用轻量级GIST嵌入索引语料库并检索top-n候选，再用ColBERTv2重新排序。如果最近语料项的可归因性低于人工生成的文本参考，则认为模型输出具有新颖性。

Result: 评估SmolLM和SmolLM2发现：(1)模型利用预训练数据的跨度比之前报道的更长；(2)某些领域系统性地促进或抑制新颖性；(3)指令微调不仅改变风格还增加新颖性。

Conclusion: 基于不可归因性的新颖性评估框架能够在预训练规模上实现高效分析，为研究模型行为提供了新视角。

Abstract: Understanding how language-model outputs relate to the pretraining corpus is
central to studying model behavior. Most training data attribution (TDA)
methods ask which training examples causally influence a given output, often
using leave-one-out tests. We invert the question: which outputs cannot be
attributed to any pretraining example? We introduce un-attributability as an
operational measure of semantic novelty: an output is novel if the pretraining
corpus contains no semantically similar context. We approximate this with a
simple two-stage retrieval pipeline: index the corpus with lightweight GIST
embeddings, retrieve the top-n candidates, then rerank with ColBERTv2. If the
nearest corpus item is less attributable than a human-generated text reference,
we consider the output of the model as novel. We evaluate on SmolLM and SmolLM2
and report three findings: (1) models draw on pretraining data across much
longer spans than previously reported; (2) some domains systematically promote
or suppress novelty; and (3) instruction tuning not only alters style but also
increases novelty. Reframing novelty assessment around un-attributability
enables efficient analysis at pretraining scale. We release ~20 TB of corpus
chunks and index artifacts to support replication and large-scale extension of
our analysis at https://huggingface.co/datasets/stai-tuebingen/faiss-smollm

</details>


### [51] [Measuring Chain-of-Thought Monitorability Through Faithfulness and Verbosity](https://arxiv.org/abs/2510.27378)
*Austin Meek,Eitan Sprejer,Iván Arcuschin,Austin J. Brockmeier,Steven Basart*

Main category: cs.LG

TL;DR: 该论文提出了一个评估思维链（CoT）监控性的框架，结合忠实性和详尽性来衡量CoT作为模型外部工作记忆的效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要通过观察模型在输入提示后是否改变答案来评估CoT的忠实性，但这种方法存在局限性，无法全面评估CoT的监控性。

Method: 引入详尽性概念来衡量CoT是否列出解决任务所需的所有因素，并将忠实性和详尽性结合成一个监控性评分。在BBH、GPQA和MMLU数据集上评估指令调优和推理模型。

Result: 研究发现模型可能看起来忠实但难以监控，因为它们会遗漏关键因素，且不同模型家族的监控性差异显著。

Conclusion: 监控性是CoT作为模型外部工作记忆的重要属性，对基于CoT监控的安全方案至关重要，不同模型在监控性方面表现差异很大。

Abstract: Chain-of-thought (CoT) outputs let us read a model's step-by-step reasoning.
Since any long, serial reasoning process must pass through this textual trace,
the quality of the CoT is a direct window into what the model is thinking. This
visibility could help us spot unsafe or misaligned behavior (monitorability),
but only if the CoT is transparent about its internal reasoning (faithfulness).
Fully measuring faithfulness is difficult, so researchers often focus on
examining the CoT in cases where the model changes its answer after adding a
cue to the input. This proxy finds some instances of unfaithfulness but loses
information when the model maintains its answer, and does not investigate
aspects of reasoning not tied to the cue. We extend these results to a more
holistic sense of monitorability by introducing verbosity: whether the CoT
lists every factor needed to solve the task. We combine faithfulness and
verbosity into a single monitorability score that shows how well the CoT serves
as the model's external `working memory', a property that many safety schemes
based on CoT monitoring depend on. We evaluate instruction-tuned and reasoning
models on BBH, GPQA, and MMLU. Our results show that models can appear faithful
yet remain hard to monitor when they leave out key factors, and that
monitorability differs sharply across model families. We release our evaluation
code using the Inspect library to support reproducible future work.

</details>


### [52] [Atlas-Alignment: Making Interpretability Transferable Across Language Models](https://arxiv.org/abs/2510.27413)
*Bruno Puri,Jim Berend,Sebastian Lapuschkin,Wojciech Samek*

Main category: cs.LG

TL;DR: Atlas-Alignment框架通过将未知潜在空间与标记化的概念图谱对齐，实现跨语言模型的可解释性迁移，无需训练稀疏自编码器或手动标记组件。


<details>
  <summary>Details</summary>
Motivation: 现有可解释性方法成本高且难以扩展，需要为每个新模型训练稀疏自编码器、手动标记组件并进行验证。

Method: 使用共享输入和轻量级表示对齐技术，将未知潜在空间与预构建的概念图谱（标记化的人类可解释潜在空间）对齐。

Result: 对齐后模型具备语义特征搜索检索和沿人类可解释概念引导生成的能力，定量和定性评估显示无需标记概念数据即可实现鲁棒的语义检索和可控生成。

Conclusion: Atlas-Alignment通过一次性投资构建高质量概念图谱，能以最小边际成本使多个新模型变得透明可控，分摊了可解释AI的成本。

Abstract: Interpretability is crucial for building safe, reliable, and controllable
language models, yet existing interpretability pipelines remain costly and
difficult to scale. Interpreting a new model typically requires costly training
of model-specific sparse autoencoders, manual or semi-automated labeling of SAE
components, and their subsequent validation. We introduce Atlas-Alignment, a
framework for transferring interpretability across language models by aligning
unknown latent spaces to a Concept Atlas - a labeled, human-interpretable
latent space - using only shared inputs and lightweight representational
alignment techniques. Once aligned, this enables two key capabilities in
previously opaque models: (1) semantic feature search and retrieval, and (2)
steering generation along human-interpretable atlas concepts. Through
quantitative and qualitative evaluations, we show that simple representational
alignment methods enable robust semantic retrieval and steerable generation
without the need for labeled concept data. Atlas-Alignment thus amortizes the
cost of explainable AI and mechanistic interpretability: by investing in one
high-quality Concept Atlas, we can make many new models transparent and
controllable at minimal marginal cost.

</details>


### [53] [Thought Branches: Interpreting LLM Reasoning Requires Resampling](https://arxiv.org/abs/2510.27484)
*Uzay Macar,Paul C. Bogdan,Senthooran Rajamanoharan,Neel Nanda*

Main category: cs.LG

TL;DR: 本文提出通过重采样方法研究推理模型的因果影响，挑战了传统仅分析单一思维链的局限性，展示了在代理错位、推理引导、步骤移除和提示忠实性等场景下的应用。


<details>
  <summary>Details</summary>
Motivation: 现有研究大多只分析单一思维链，但模型实际上定义了多种可能推理路径的分布。单一样本分析不足以理解因果影响和底层计算过程，需要系统研究推理分布。

Method: 采用重采样方法研究模型决策：1) 在代理错位场景中重采样特定句子测量下游影响；2) 通过重采样和选择实现策略内推理引导；3) 引入弹性度量反复重采样防止内容重现；4) 改编因果中介分析研究提示的隐性影响。

Result: 发现：1) 自我保护句子的因果影响很小；2) 策略外干预效果小且不稳定；3) 关键规划语句难以移除但移除后影响巨大；4) 提示即使被移除仍通过累积方式影响推理过程。

Conclusion: 通过重采样研究分布能够实现可靠的因果分析、更清晰的模型推理叙述，以及原则性的思维链干预方法。

Abstract: Most work interpreting reasoning models studies only a single
chain-of-thought (CoT), yet these models define distributions over many
possible CoTs. We argue that studying a single sample is inadequate for
understanding causal influence and the underlying computation. Though fully
specifying this distribution is intractable, it can be understood by sampling.
We present case studies using resampling to investigate model decisions. First,
when a model states a reason for its action, does that reason actually cause
the action? In "agentic misalignment" scenarios, we resample specific sentences
to measure their downstream effects. Self-preservation sentences have small
causal impact, suggesting they do not meaningfully drive blackmail. Second, are
artificial edits to CoT sufficient for steering reasoning? These are common in
literature, yet take the model off-policy. Resampling and selecting a
completion with the desired property is a principled on-policy alternative. We
find off-policy interventions yield small and unstable effects compared to
resampling in decision-making tasks. Third, how do we understand the effect of
removing a reasoning step when the model may repeat it post-edit? We introduce
a resilience metric that repeatedly resamples to prevent similar content from
reappearing downstream. Critical planning statements resist removal but have
large effects when eliminated. Fourth, since CoT is sometimes "unfaithful", can
our methods teach us anything in these settings? Adapting causal mediation
analysis, we find that hints that have a causal effect on the output without
being explicitly mentioned exert a subtle and cumulative influence on the CoT
that persists even if the hint is removed. Overall, studying distributions via
resampling enables reliable causal analysis, clearer narratives of model
reasoning, and principled CoT interventions.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [54] [Semantic Frame Aggregation-based Transformer for Live Video Comment Generation](https://arxiv.org/abs/2510.26978)
*Anam Fatima,Yi Yu,Janak Kapuriya,Julien Lalanne,Jainendra Shukla*

Main category: cs.CV

TL;DR: 提出SFAT模型用于直播视频评论生成，通过语义帧聚合和加权机制关注与观众互动相关的关键帧，并构建了大规模英文视频评论数据集。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了优先处理与观众互动最相关的视频帧，这对生成上下文适当的评论至关重要。现有数据集主要关注中文内容且视频类别有限。

Method: 提出语义帧聚合Transformer模型，利用CLIP的视觉-文本多模态知识，基于语义相关性为视频帧分配权重，采用加权帧和技术和交叉注意力机制。

Result: 构建了包含11个视频类别、438小时、320万评论的大规模英文数据集，SFAT模型在生成直播视频评论方面优于现有方法。

Conclusion: SFAT模型通过语义帧聚合和加权机制有效提升了直播视频评论生成的上下文相关性，解决了现有方法的局限性。

Abstract: Live commenting on video streams has surged in popularity on platforms like
Twitch, enhancing viewer engagement through dynamic interactions. However,
automatically generating contextually appropriate comments remains a
challenging and exciting task. Video streams can contain a vast amount of data
and extraneous content. Existing approaches tend to overlook an important
aspect of prioritizing video frames that are most relevant to ongoing viewer
interactions. This prioritization is crucial for producing contextually
appropriate comments. To address this gap, we introduce a novel Semantic Frame
Aggregation-based Transformer (SFAT) model for live video comment generation.
This method not only leverages CLIP's visual-text multimodal knowledge to
generate comments but also assigns weights to video frames based on their
semantic relevance to ongoing viewer conversation. It employs an efficient
weighted sum of frames technique to emphasize informative frames while focusing
less on irrelevant ones. Finally, our comment decoder with a cross-attention
mechanism that attends to each modality ensures that the generated comment
reflects contextual cues from both chats and video. Furthermore, to address the
limitations of existing datasets, which predominantly focus on Chinese-language
content with limited video categories, we have constructed a large scale,
diverse, multimodal English video comments dataset. Extracted from Twitch, this
dataset covers 11 video categories, totaling 438 hours and 3.2 million
comments. We demonstrate the effectiveness of our SFAT model by comparing it to
existing methods for generating comments from live video and ongoing dialogue
contexts.

</details>


### [55] [Can MLLMs Read the Room? A Multimodal Benchmark for Verifying Truthfulness in Multi-Party Social Interactions](https://arxiv.org/abs/2510.27195)
*Caixin Kang,Yifei Huang,Liangyang Ouyang,Mingfang Zhang,Yoichi Sato*

Main category: cs.CV

TL;DR: 该论文提出了多模态交互式真实性评估(MIVA)任务，基于狼人杀游戏构建了包含同步视频、文本和真实标签的数据集，评估了多模态大语言模型在识别谎言方面的表现。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统日益融入人类生活，赋予其强大的社会智能变得至关重要。自动检测动态多参与者对话中的欺骗行为是一个重大挑战，而多模态大语言模型在这方面的能力尚未得到充分量化。

Method: 引入MIVA任务，构建基于狼人杀游戏的新型多模态数据集，包含同步视频、文本和可验证的真实标签，并建立基准评估最先进的多模态大语言模型。

Result: 评估显示存在显著的性能差距：即使是GPT-4o这样的强大模型也难以可靠地区分真假。模型未能有效将语言与视觉社交线索相结合，且可能因对齐过度而过于保守。

Conclusion: 迫切需要开发新方法来构建更具洞察力和可信度的AI系统，当前模型在欺骗检测方面存在明显局限性。

Abstract: As AI systems become increasingly integrated into human lives, endowing them
with robust social intelligence has emerged as a critical frontier. A key
aspect of this intelligence is discerning truth from deception, a ubiquitous
element of human interaction that is conveyed through a complex interplay of
verbal language and non-verbal visual cues. However, automatic deception
detection in dynamic, multi-party conversations remains a significant
challenge. The recent rise of powerful Multimodal Large Language Models
(MLLMs), with their impressive abilities in visual and textual understanding,
makes them natural candidates for this task. Consequently, their capabilities
in this crucial domain are mostly unquantified. To address this gap, we
introduce a new task, Multimodal Interactive Veracity Assessment (MIVA), and
present a novel multimodal dataset derived from the social deduction game
Werewolf. This dataset provides synchronized video, text, with verifiable
ground-truth labels for every statement. We establish a comprehensive benchmark
evaluating state-of-the-art MLLMs, revealing a significant performance gap:
even powerful models like GPT-4o struggle to distinguish truth from falsehood
reliably. Our analysis of failure modes indicates that these models fail to
ground language in visual social cues effectively and may be overly
conservative in their alignment, highlighting the urgent need for novel
approaches to building more perceptive and trustworthy AI systems.

</details>


### [56] [Towards Universal Video Retrieval: Generalizing Video Embedding via Synthesized Multimodal Pyramid Curriculum](https://arxiv.org/abs/2510.27571)
*Zhuoning Guo,Mingxin Li,Yanzhao Zhang,Dingkun Long,Pengjun Xie,Xiaowen Chu*

Main category: cs.CV

TL;DR: 提出了一个联合设计框架，通过评估、数据和建模的协同设计来解决视频检索中的结构不对齐问题，包括通用视频检索基准、数据合成工作流和模态金字塔训练方法。


<details>
  <summary>Details</summary>
Motivation: 当前视频检索范式存在结构不对齐问题，狭窄的基准限制了数据和单任务训练，抑制了通用能力的发展，缺乏能够定义和需求多维泛化的诊断性评估。

Method: 1) 建立包含16个数据集的通用视频检索基准(UVRB)；2) 开发可扩展的合成工作流生成155万高质量数据对；3) 设计模态金字塔课程训练通用视频嵌入器(GVE)。

Result: GVE在UVRB上实现了最先进的零样本泛化性能，分析表明流行基准对通用能力的预测能力较差，部分相关检索是主导但被忽视的场景。

Conclusion: 该联合设计框架为摆脱有限范围、推进真正通用视频检索提供了实用路径。

Abstract: The prevailing video retrieval paradigm is structurally misaligned, as narrow
benchmarks incentivize correspondingly limited data and single-task training.
Therefore, universal capability is suppressed due to the absence of a
diagnostic evaluation that defines and demands multi-dimensional
generalization. To break this cycle, we introduce a framework built on the
co-design of evaluation, data, and modeling. First, we establish the Universal
Video Retrieval Benchmark (UVRB), a suite of 16 datasets designed not only to
measure performance but also to diagnose critical capability gaps across tasks
and domains. Second, guided by UVRB's diagnostics, we introduce a scalable
synthesis workflow that generates 1.55 million high-quality pairs to populate
the semantic space required for universality. Finally, we devise the Modality
Pyramid, a curriculum that trains our General Video Embedder (GVE) by
explicitly leveraging the latent interconnections within our diverse data.
Extensive experiments show GVE achieves state-of-the-art zero-shot
generalization on UVRB. In particular, our analysis reveals that popular
benchmarks are poor predictors of general ability and that partially relevant
retrieval is a dominant but overlooked scenario. Overall, our co-designed
framework provides a practical path to escape the limited scope and advance
toward truly universal video retrieval.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [57] [CATArena: Evaluation of LLM Agents through Iterative Tournament Competitions](https://arxiv.org/abs/2510.26852)
*Lingyue Fu,Xin Ding,Yaoming Zhu,Shao Zhang,Lin Qiu,Weiwen Liu,Weinan Zhang,Xuezhi Cao,Xunliang Cai,Jiaxin Ding,Yong Yu*

Main category: cs.AI

TL;DR: 提出了CATArena评估平台，通过开放式评分和竞争性学习框架来评估LLM智能体的学习能力，解决了现有基准测试中的分数饱和问题。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要评估固定场景下的端到端性能，存在分数饱和和依赖专家标注的问题，限制了智能体学习能力的评估。

Method: 提出迭代竞争性同伴学习框架，让智能体通过重复交互和反馈优化策略；引入CATArena平台，包含四种棋盘和纸牌游戏，采用开放式评分系统。

Result: 实验结果表明CATArena能够可靠、稳定、可扩展地评估智能体核心能力，特别是学习能力和策略编码能力。

Conclusion: CATArena为快速发展的智能体能力提供了持续动态的评估方法，强调了学习能力作为智能体向人类水平智能演进的关键驱动因素。

Abstract: Large Language Model (LLM) agents have evolved from basic text generation to
autonomously completing complex tasks through interaction with external tools.
However, current benchmarks mainly assess end-to-end performance in fixed
scenarios, restricting evaluation to specific skills and suffering from score
saturation and growing dependence on expert annotation as agent capabilities
improve. In this work, we emphasize the importance of learning ability,
including both self-improvement and peer-learning, as a core driver for agent
evolution toward human-level intelligence. We propose an iterative, competitive
peer-learning framework, which allows agents to refine and optimize their
strategies through repeated interactions and feedback, thereby systematically
evaluating their learning capabilities. To address the score saturation issue
in current benchmarks, we introduce CATArena, a tournament-style evaluation
platform featuring four diverse board and card games with open-ended scoring.
By providing tasks without explicit upper score limits, CATArena enables
continuous and dynamic evaluation of rapidly advancing agent capabilities.
Experimental results and analyses involving both minimal and commercial code
agents demonstrate that CATArena provides reliable, stable, and scalable
benchmarking for core agent abilities, particularly learning ability and
strategy coding.

</details>


### [58] [The Denario project: Deep knowledge AI agents for scientific discovery](https://arxiv.org/abs/2510.26887)
*Francisco Villaescusa-Navarro,Boris Bolliet,Pablo Villanueva-Domingo,Adrian E. Bayer,Aidan Acquah,Chetana Amancharla,Almog Barzilay-Siegal,Pablo Bermejo,Camille Bilodeau,Pablo Cárdenas Ramírez,Miles Cranmer,Urbano L. França,ChangHoon Hahn,Yan-Fei Jiang,Raul Jimenez,Jun-Young Lee,Antonio Lerario,Osman Mamun,Thomas Meier,Anupam A. Ojha,Pavlos Protopapas,Shimanto Roy,David N. Spergel,Pedro Tarancón-Álvarez,Ujjwal Tiwari,Matteo Viel,Digvijay Wadekar,Chi Wang,Bonny Y. Wang,Licong Xu,Yossi Yovel,Shuwen Yue,Wen-Han Zhou,Qiyao Zhu,Jiajun Zou,Íñigo Zubeldia*

Main category: cs.AI

TL;DR: Denario是一个AI多代理系统，作为科学研究助手，能够执行多种任务包括生成想法、文献检索、制定研究计划、编写执行代码、制作图表以及起草和评审科学论文。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够跨学科进行端到端科学分析的AI研究助手，解决传统研究中的效率瓶颈，促进多学科交叉研究。

Method: 采用模块化架构设计，结合Cmbagent作为深度研究后端，支持特定任务处理和完整的科学分析流程。

Result: 系统在多个科学领域生成了AI撰写的论文，包括天体物理学、生物学、生物物理学等，并获得了领域专家的评估和反馈。

Conclusion: Denario展示了AI驱动研究的潜力，但也存在局限性，需要进一步探讨AI研究对科学哲学和伦理的影响。

Abstract: We present Denario, an AI multi-agent system designed to serve as a
scientific research assistant. Denario can perform many different tasks, such
as generating ideas, checking the literature, developing research plans,
writing and executing code, making plots, and drafting and reviewing a
scientific paper. The system has a modular architecture, allowing it to handle
specific tasks, such as generating an idea, or carrying out end-to-end
scientific analysis using Cmbagent as a deep-research backend. In this work, we
describe in detail Denario and its modules, and illustrate its capabilities by
presenting multiple AI-generated papers generated by it in many different
scientific disciplines such as astrophysics, biology, biophysics, biomedical
informatics, chemistry, material science, mathematical physics, medicine,
neuroscience and planetary science. Denario also excels at combining ideas from
different disciplines, and we illustrate this by showing a paper that applies
methods from quantum physics and machine learning to astrophysical data. We
report the evaluations performed on these papers by domain experts, who
provided both numerical scores and review-like feedback. We then highlight the
strengths, weaknesses, and limitations of the current system. Finally, we
discuss the ethical implications of AI-driven research and reflect on how such
technology relates to the philosophy of science. We publicly release the code
at https://github.com/AstroPilot-AI/Denario. A Denario demo can also be run
directly on the web at https://huggingface.co/spaces/astropilot-ai/Denario, and
the full app will be deployed on the cloud.

</details>


### [59] [Glia: A Human-Inspired AI for Automated Systems Design and Optimization](https://arxiv.org/abs/2510.27176)
*Pouya Hamadanian,Pantea Karimi,Arash Nasr-Esfahany,Kimia Noorbakhsh,Joseph Chandler,Ali ParandehGheibi,Mohammad Alizadeh,Hari Balakrishnan*

Main category: cs.AI

TL;DR: Glia是一个用于网络系统设计的AI架构，使用LLM在多智能体工作流中实现自主系统设计，性能达到人类专家水平。


<details>
  <summary>Details</summary>
Motivation: 探索AI是否能像人类专家一样自主设计计算机系统机制，结合创造性和推理能力。

Method: 采用多智能体架构，每个智能体专门负责推理、实验和分析，通过评估框架将抽象推理与经验反馈相结合。

Result: 在分布式GPU集群的LLM推理应用中，Glia生成了新的请求路由、调度和自动扩展算法，性能达到人类专家水平且用时更少。

Conclusion: 通过结合推理LLM和结构化实验，AI能够为复杂系统问题生成创造性且可理解的设计。

Abstract: Can an AI autonomously design mechanisms for computer systems on par with the
creativity and reasoning of human experts? We present Glia, an AI architecture
for networked systems design that uses large language models (LLMs) in a
human-inspired, multi-agent workflow. Each agent specializes in reasoning,
experimentation, and analysis, collaborating through an evaluation framework
that grounds abstract reasoning in empirical feedback. Unlike prior
ML-for-systems methods that optimize black-box policies, Glia generates
interpretable designs and exposes its reasoning process. When applied to a
distributed GPU cluster for LLM inference, it produces new algorithms for
request routing, scheduling, and auto-scaling that perform at human-expert
levels in significantly less time, while yielding novel insights into workload
behavior. Our results suggest that by combining reasoning LLMs with structured
experimentation, an AI can produce creative and understandable designs for
complex systems problems.

</details>


### [60] [DeepCompress: A Dual Reward Strategy for Dynamically Exploring and Compressing Reasoning Chains](https://arxiv.org/abs/2510.27419)
*Tian Liang,Wenxiang Jiao,Zhiwei He,Jiahao Xu,Haitao Mi,Dong Yu*

Main category: cs.AI

TL;DR: DeepCompress是一个新颖框架，通过自适应长度奖励机制同时提升大型推理模型的准确性和效率，根据问题难度动态调整推理链长度。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过监督微调或基于token长度的强化学习来提高效率，但往往以牺牲准确性为代价。大型推理模型存在认知效率低下的问题，如对简单问题"过度思考"和对复杂问题"思考不足"。

Method: 采用自适应长度奖励机制，实时将问题分类为"简单"或"困难"，对简单问题鼓励更短的推理路径，对困难问题促进更长的探索性思维链。这种双奖励策略使模型能自主调整CoT长度。

Result: 在具有挑战性的数学基准测试中，DeepCompress始终优于基线方法，在显著提高token效率的同时实现了更优的准确性。

Conclusion: DeepCompress证明了自适应长度奖励机制的有效性，能够同时提升大型推理模型的准确性和效率，挑战了传统偏好较短推理路径的做法。

Abstract: Large Reasoning Models (LRMs) have demonstrated impressive capabilities but
suffer from cognitive inefficiencies like ``overthinking'' simple problems and
``underthinking'' complex ones. While existing methods that use supervised
fine-tuning~(SFT) or reinforcement learning~(RL) with token-length rewards can
improve efficiency, they often do so at the cost of accuracy. This paper
introduces \textbf{DeepCompress}, a novel framework that simultaneously
enhances both the accuracy and efficiency of LRMs. We challenge the prevailing
approach of consistently favoring shorter reasoning paths, showing that longer
responses can contain a broader range of correct solutions for difficult
problems. DeepCompress employs an adaptive length reward mechanism that
dynamically classifies problems as ``Simple'' or ``Hard'' in real-time based on
the model's evolving capability. It encourages shorter, more efficient
reasoning for ``Simple'' problems while promoting longer, more exploratory
thought chains for ``Hard'' problems. This dual-reward strategy enables the
model to autonomously adjust its Chain-of-Thought (CoT) length, compressing
reasoning for well-mastered problems and extending it for those it finds
challenging. Experimental results on challenging mathematical benchmarks show
that DeepCompress consistently outperforms baseline methods, achieving superior
accuracy while significantly improving token efficiency.

</details>


### [61] [SIGMA: Search-Augmented On-Demand Knowledge Integration for Agentic Mathematical Reasoning](https://arxiv.org/abs/2510.27568)
*Ali Asgarov,Umid Suleymanov,Aadyant Khatri*

Main category: cs.AI

TL;DR: SIGMA是一个多代理检索增强框架，通过协调专门代理进行独立推理、定向搜索和结果合成，显著提升数学推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 当前检索增强模型依赖单一视角、搜索策略不灵活，难以有效整合多源信息，限制了数学推理问题的解决能力。

Method: 引入SIGMA框架，通过专门代理独立推理并生成假设性段落来优化检索，使用协调机制整合各代理的发现，实现上下文敏感且计算高效的知识集成。

Result: 在MATH500、AIME和GPQA等挑战性基准测试中，SIGMA始终优于开源和闭源系统，实现了7.4%的绝对性能提升。

Conclusion: 多代理按需知识集成显著提高了推理准确性和效率，为复杂知识密集型问题解决提供了可扩展方法。

Abstract: Solving mathematical reasoning problems requires not only accurate access to
relevant knowledge but also careful, multi-step thinking. However, current
retrieval-augmented models often rely on a single perspective, follow
inflexible search strategies, and struggle to effectively combine information
from multiple sources. We introduce SIGMA (Search-Augmented On-Demand Knowledge
Integration for AGentic Mathematical reAsoning), a unified framework that
orchestrates specialized agents to independently reason, perform targeted
searches, and synthesize findings through a moderator mechanism. Each agent
generates hypothetical passages to optimize retrieval for its analytic
perspective, ensuring knowledge integration is both context-sensitive and
computation-efficient. When evaluated on challenging benchmarks such as
MATH500, AIME, and PhD-level science QA GPQA, SIGMA consistently outperforms
both open- and closed-source systems, achieving an absolute performance
improvement of 7.4%. Our results demonstrate that multi-agent, on-demand
knowledge integration significantly enhances both reasoning accuracy and
efficiency, offering a scalable approach for complex, knowledge-intensive
problem-solving. We will release the code upon publication.

</details>


### [62] [Visual Backdoor Attacks on MLLM Embodied Decision Making via Contrastive Trigger Learning](https://arxiv.org/abs/2510.27623)
*Qiusi Zhan,Hyeonjeong Ha,Rui Yang,Sirui Xu,Hanyang Chen,Liang-Yan Gui,Yu-Xiong Wang,Huan Zhang,Heng Ji,Daniel Kang*

Main category: cs.AI

TL;DR: BEAT是首个针对MLLM具身代理的视觉后门攻击框架，使用环境中的物体作为触发条件，在触发出现时持续执行攻击者指定的多步策略，攻击成功率高达80%。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型使具身代理能够直接从视觉输入进行感知、推理和规划，但这种视觉驱动的代理开辟了新的攻击面：视觉后门攻击，当视觉触发器出现在场景中时，代理会持续执行攻击者指定的策略。

Method: BEAT通过(1)构建跨越多样化场景、任务和触发器放置的训练集来暴露代理于触发器变异性，(2)引入两阶段训练方案：先进行监督微调(SFT)，然后使用新颖的对比触发器学习(CTL)，将触发器判别制定为触发器存在和不存在输入之间的偏好学习。

Result: 在各种具身代理基准测试和MLLM上，BEAT实现了高达80%的攻击成功率，同时保持强大的良性任务性能，并能可靠地泛化到分布外触发器放置。与朴素SFT相比，CTL在有限后门数据下将后门激活准确率提升高达39%。

Conclusion: 这些发现揭示了基于MLLM的具身代理中一个关键但未被探索的安全风险，强调了在现实世界部署前需要强大的防御措施。

Abstract: Multimodal large language models (MLLMs) have advanced embodied agents by
enabling direct perception, reasoning, and planning task-oriented actions from
visual inputs. However, such vision driven embodied agents open a new attack
surface: visual backdoor attacks, where the agent behaves normally until a
visual trigger appears in the scene, then persistently executes an
attacker-specified multi-step policy. We introduce BEAT, the first framework to
inject such visual backdoors into MLLM-based embodied agents using objects in
the environments as triggers. Unlike textual triggers, object triggers exhibit
wide variation across viewpoints and lighting, making them difficult to implant
reliably. BEAT addresses this challenge by (1) constructing a training set that
spans diverse scenes, tasks, and trigger placements to expose agents to trigger
variability, and (2) introducing a two-stage training scheme that first applies
supervised fine-tuning (SFT) and then our novel Contrastive Trigger Learning
(CTL). CTL formulates trigger discrimination as preference learning between
trigger-present and trigger-free inputs, explicitly sharpening the decision
boundaries to ensure precise backdoor activation. Across various embodied agent
benchmarks and MLLMs, BEAT achieves attack success rates up to 80%, while
maintaining strong benign task performance, and generalizes reliably to
out-of-distribution trigger placements. Notably, compared to naive SFT, CTL
boosts backdoor activation accuracy up to 39% under limited backdoor data.
These findings expose a critical yet unexplored security risk in MLLM-based
embodied agents, underscoring the need for robust defenses before real-world
deployment.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [63] [Broken-Token: Filtering Obfuscated Prompts by Counting Characters-Per-Token](https://arxiv.org/abs/2510.26847)
*Shaked Zychlinski,Yuval Kainan*

Main category: cs.CR

TL;DR: CPT-Filtering是一种新颖的模型无关防护技术，通过利用BPE分词器的内在特性来检测和过滤使用密码和字符级编码的恶意提示，防止越狱攻击。


<details>
  <summary>Details</summary>
Motivation: 现有安全防护措施无法有效识别使用密码和字符级编码的恶意提示，而现代方法计算成本高昂，需要依赖额外的LLM或困惑度模型。

Method: 基于BPE分词器在自然语言训练中的特性，发现异常文本（如密码）会使用更多短标记，通过计算文本中每个标记的平均字符数（CPT）来识别编码内容。

Result: 在超过10万个提示的大规模数据集上验证，测试了多种编码方案和流行分词器，实验表明简单的CPT阈值能高精度识别编码文本，即使对于非常短的输入也有效。

Conclusion: CPT-Filtering提供了一种实用的防御层，可立即部署用于实时文本过滤和离线数据整理，具有可忽略的成本和近乎完美的准确性。

Abstract: Large Language Models (LLMs) are susceptible to jailbreak attacks where
malicious prompts are disguised using ciphers and character-level encodings to
bypass safety guardrails. While these guardrails often fail to interpret the
encoded content, the underlying models can still process the harmful
instructions. We introduce CPT-Filtering, a novel, model-agnostic with
negligible-costs and near-perfect accuracy guardrail technique that aims to
mitigate these attacks by leveraging the intrinsic behavior of Byte-Pair
Encoding (BPE) tokenizers. Our method is based on the principle that
tokenizers, trained on natural language, represent out-of-distribution text,
such as ciphers, using a significantly higher number of shorter tokens. Our
technique uses a simple yet powerful artifact of using language models: the
average number of Characters Per Token (CPT) in the text. This approach is
motivated by the high compute cost of modern methods - relying on added modules
such as dedicated LLMs or perplexity models. We validate our approach across a
large dataset of over 100,000 prompts, testing numerous encoding schemes with
several popular tokenizers. Our experiments demonstrate that a simple CPT
threshold robustly identifies encoded text with high accuracy, even for very
short inputs. CPT-Filtering provides a practical defense layer that can be
immediately deployed for real-time text filtering and offline data curation.

</details>
