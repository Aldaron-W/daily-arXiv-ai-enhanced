<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 156]
- [cs.AI](#cs.AI) [Total: 6]
- [cs.SD](#cs.SD) [Total: 2]
- [cs.LG](#cs.LG) [Total: 16]
- [cs.IR](#cs.IR) [Total: 4]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.SE](#cs.SE) [Total: 2]
- [cs.CV](#cs.CV) [Total: 3]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.CR](#cs.CR) [Total: 3]
- [cs.MA](#cs.MA) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Table Question Answering in the Era of Large Language Models: A Comprehensive Survey of Tasks, Methods, and Evaluation](https://arxiv.org/abs/2510.09671)
*Wei Zhou,Bolei Ma,Annemarie Friedrich,Mohsen Mesgar*

Main category: cs.CL

TL;DR: 这篇论文是关于表格问答(TQA)的系统性综述，重点关注基于大语言模型的方法，提供了任务分类、挑战分析和未来方向的全面概述。


<details>
  <summary>Details</summary>
Motivation: 表格问答领域缺乏对任务表述、核心挑战和方法趋势的系统性组织与理解，特别是在强化学习等新兴研究方向出现后。

Method: 通过提供全面的任务分类、基准测试分析和建模策略分组，系统地组织TQA研究，重点关注LLM-based方法。

Result: 建立了TQA研究的统一框架，识别了未充分探索但及时的研究主题，为社区提供了整合的基础。

Conclusion: 该综述为TQA社区提供了统一的基础，使研究人员能够更深入地理解最新进展，并指导这一快速演进领域的未来发展。

Abstract: Table Question Answering (TQA) aims to answer natural language questions
about tabular data, often accompanied by additional contexts such as text
passages. The task spans diverse settings, varying in table representation,
question/answer complexity, modality involved, and domain. While recent
advances in large language models (LLMs) have led to substantial progress in
TQA, the field still lacks a systematic organization and understanding of task
formulations, core challenges, and methodological trends, particularly in light
of emerging research directions such as reinforcement learning. This survey
addresses this gap by providing a comprehensive and structured overview of TQA
research with a focus on LLM-based methods. We provide a comprehensive
categorization of existing benchmarks and task setups. We group current
modeling strategies according to the challenges they target, and analyze their
strengths and limitations. Furthermore, we highlight underexplored but timely
topics that have not been systematically covered in prior research. By unifying
disparate research threads and identifying open problems, our survey offers a
consolidated foundation for the TQA community, enabling a deeper understanding
of the state of the art and guiding future developments in this rapidly
evolving area.

</details>


### [2] [Emotionally Charged, Logically Blurred: AI-driven Emotional Framing Impairs Human Fallacy Detection](https://arxiv.org/abs/2510.09695)
*Yanran Chen,Lynn Greschner,Roman Klinger,Michael Klenk,Steffen Eger*

Main category: cs.CL

TL;DR: 该研究首次计算性地探讨了情感框架与逻辑谬误及说服力的相互作用，使用大语言模型系统性地改变谬误论证中的情感诉求，发现LLM驱动的情感框架使人类谬误检测F1分数平均下降14.5%。


<details>
  <summary>Details</summary>
Motivation: 逻辑谬误在公共传播中普遍存在且可能误导受众，尽管缺乏合理性但谬误论证仍可能显得有说服力，因为说服力本质上是主观的。研究旨在探索情感框架如何与谬误和说服力相互作用。

Method: 使用八个大语言模型在保持逻辑结构的同时向谬误论证注入情感诉求，然后使用最佳模型生成人类研究刺激材料，通过人类研究评估情感框架对谬误检测和说服力的影响。

Result: LLM驱动的情感框架使人类谬误检测F1分数平均下降14.5%；人类在感知愉悦情绪时比恐惧或悲伤时在谬误检测方面表现更好；愉悦、恐惧和悲伤这三种情绪与显著更高的说服力相关，相比中性或其他情绪状态。

Conclusion: 这项工作对AI驱动的谬误论证情境下的情感操纵具有重要启示，表明情感框架可以显著影响人类对逻辑谬误的识别能力和论证的说服力。

Abstract: Logical fallacies are common in public communication and can mislead
audiences; fallacious arguments may still appear convincing despite lacking
soundness, because convincingness is inherently subjective. We present the
first computational study of how emotional framing interacts with fallacies and
convincingness, using large language models (LLMs) to systematically change
emotional appeals in fallacious arguments. We benchmark eight LLMs on injecting
emotional appeal into fallacious arguments while preserving their logical
structures, then use the best models to generate stimuli for a human study. Our
results show that LLM-driven emotional framing reduces human fallacy detection
in F1 by 14.5% on average. Humans perform better in fallacy detection when
perceiving enjoyment than fear or sadness, and these three emotions also
correlate with significantly higher convincingness compared to neutral or other
emotion states. Our work has implications for AI-driven emotional manipulation
in the context of fallacious argumentation.

</details>


### [3] [The Idola Tribus of AI: Large Language Models tend to perceive order where none exists](https://arxiv.org/abs/2510.09709)
*Shin-nosuke Ishikawa,Masato Todo,Taiki Ogihara,Hirotsugu Ohba*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We present a tendency of large language models (LLMs) to generate absurd
patterns despite their clear inappropriateness in a simple task of identifying
regularities in number series. Several approaches have been proposed to apply
LLMs to complex real-world tasks, such as providing knowledge through
retrieval-augmented generation and executing multi-step tasks using AI agent
frameworks. However, these approaches rely on the logical consistency and
self-coherence of LLMs, making it crucial to evaluate these aspects and
consider potential countermeasures. To identify cases where LLMs fail to
maintain logical consistency, we conducted an experiment in which LLMs were
asked to explain the patterns in various integer sequences, ranging from
arithmetic sequences to randomly generated integer series. While the models
successfully identified correct patterns in arithmetic and geometric sequences,
they frequently over-recognized patterns that were inconsistent with the given
numbers when analyzing randomly generated series. This issue was observed even
in multi-step reasoning models, including OpenAI o3, o4-mini, and Google Gemini
2.5 Flash Preview Thinking. This tendency to perceive non-existent patterns can
be interpreted as the AI model equivalent of Idola Tribus and highlights
potential limitations in their capability for applied tasks requiring logical
reasoning, even when employing chain-of-thought reasoning mechanisms.

</details>


### [4] [SeCon-RAG: A Two-Stage Semantic Filtering and Conflict-Free Framework for Trustworthy RAG](https://arxiv.org/abs/2510.09710)
*Xiaonan Si,Meilin Zhu,Simeng Qin,Lijia Yu,Lijun Zhang,Shuaitong Liu,Xinfeng Li,Ranjie Duan,Yang Liu,Xiaojun Jia*

Main category: cs.CL

TL;DR: 提出SeCon-RAG框架，通过两阶段语义过滤和冲突消除机制增强RAG系统的可信度，有效防御语料库污染攻击。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统容易受到语料库污染和污染攻击，现有防御方法采用激进过滤导致有价值信息丢失和生成可靠性降低。

Method: 两阶段框架：第一阶段使用EIRE引导的语义和聚类联合过滤；第二阶段使用EIRE引导的冲突感知过滤模块分析语义一致性。

Result: 在多种LLM和数据集上的实验表明，SeCon-RAG显著优于现有最先进的防御方法。

Conclusion: SeCon-RAG通过两阶段过滤过程有效保留有用知识同时减轻冲突污染，显著提高了生成鲁棒性和输出可信度。

Abstract: Retrieval-augmented generation (RAG) systems enhance large language models
(LLMs) with external knowledge but are vulnerable to corpus poisoning and
contamination attacks, which can compromise output integrity. Existing defenses
often apply aggressive filtering, leading to unnecessary loss of valuable
information and reduced reliability in generation. To address this problem, we
propose a two-stage semantic filtering and conflict-free framework for
trustworthy RAG. In the first stage, we perform a joint filter with semantic
and cluster-based filtering which is guided by the Entity-intent-relation
extractor (EIRE). EIRE extracts entities, latent objectives, and entity
relations from both the user query and filtered documents, scores their
semantic relevance, and selectively adds valuable documents into the clean
retrieval database. In the second stage, we proposed an EIRE-guided
conflict-aware filtering module, which analyzes semantic consistency between
the query, candidate answers, and retrieved knowledge before final answer
generation, filtering out internal and external contradictions that could
mislead the model. Through this two-stage process, SeCon-RAG effectively
preserves useful knowledge while mitigating conflict contamination, achieving
significant improvements in both generation robustness and output
trustworthiness. Extensive experiments across various LLMs and datasets
demonstrate that the proposed SeCon-RAG markedly outperforms state-of-the-art
defense methods.

</details>


### [5] [ReaLM: Residual Quantization Bridging Knowledge Graph Embeddings and Large Language Models](https://arxiv.org/abs/2510.09711)
*Wenbin Guo,Xin Wang,Jiaoyan Chen,Lingbing Guo,Zhao Li,Zirui Chen*

Main category: cs.CL

TL;DR: ReaLM是一个新颖的KG补全框架，通过残差向量量化将KG嵌入离散化为代码序列，并作为可学习token集成到LLM词汇表中，实现符号知识与上下文知识的无缝融合。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的KG补全方法难以充分利用结构化语义表示，因为预训练KG模型的连续嵌入空间与LLM的离散token空间存在根本性不匹配，阻碍了有效的语义迁移和性能提升。

Method: 使用残差向量量化机制将预训练KG嵌入离散化为紧凑代码序列，将其作为可学习token集成到LLM词汇表中，并加入本体引导的类约束来增强语义一致性。

Result: 在两个广泛使用的基准数据集上的大量实验表明，ReaLM实现了最先进的性能，证实了其在将结构化知识与大规模语言模型对齐方面的有效性。

Conclusion: ReaLM通过桥接KG嵌入与LLM标记化之间的鸿沟，成功地将结构化知识与大规模语言模型对齐，为KG补全提供了有效的解决方案。

Abstract: Large Language Models (LLMs) have recently emerged as a powerful paradigm for
Knowledge Graph Completion (KGC), offering strong reasoning and generalization
capabilities beyond traditional embedding-based approaches. However, existing
LLM-based methods often struggle to fully exploit structured semantic
representations, as the continuous embedding space of pretrained KG models is
fundamentally misaligned with the discrete token space of LLMs. This
discrepancy hinders effective semantic transfer and limits their performance.
To address this challenge, we propose ReaLM, a novel and effective framework
that bridges the gap between KG embeddings and LLM tokenization through the
mechanism of residual vector quantization. ReaLM discretizes pretrained KG
embeddings into compact code sequences and integrates them as learnable tokens
within the LLM vocabulary, enabling seamless fusion of symbolic and contextual
knowledge. Furthermore, we incorporate ontology-guided class constraints to
enforce semantic consistency, refining entity predictions based on class-level
compatibility. Extensive experiments on two widely used benchmark datasets
demonstrate that ReaLM achieves state-of-the-art performance, confirming its
effectiveness in aligning structured knowledge with large-scale language
models.

</details>


### [6] [All Code, No Thought: Current Language Models Struggle to Reason in Ciphered Language](https://arxiv.org/abs/2510.09714)
*Shiyuan Guo,Henry Sleight,Fabien Roger*

Main category: cs.CL

TL;DR: 研究发现AI模型在加密推理中存在不对称性：虽然能准确翻译加密文本，但在加密文本中进行推理时准确性显著下降，这可能影响CoT监控的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理的广泛应用，检测有害AI行为变得重要。CoT监控被广泛用于检测对抗性攻击和AI错位，但攻击者和错位模型可能通过加密推理来逃避监控。

Method: 测试了28种不同密码，对最多10个模型进行微调和提示，让它们在密码中进行推理。使用数学问题作为推理能力的代理指标，测量模型准确性。

Result: 发现模型准确性在加密推理中显著下降，即使模型能够准确翻译加密文本。前沿模型在不太知名的密码中表现不佳，但在知名密码如rot13中表现良好。加密推理能力与预训练数据中密码的普遍性相关。

Conclusion: 使用加密推理逃避CoT监控对当前模型可能是无效策略，研究为限制未来前沿模型发展这种能力提供了指导。

Abstract: Detecting harmful AI actions is important as AI agents gain adoption.
Chain-of-thought (CoT) monitoring is one method widely used to detect
adversarial attacks and AI misalignment. However, attackers and misaligned
models might evade CoT monitoring through ciphered reasoning: reasoning hidden
in encrypted, translated, or compressed text. To assess this risk, we test
whether models can perform ciphered reasoning. For each of 28 different
ciphers, we fine-tune and prompt up to 10 models to reason in that cipher. We
measure model accuracy on math problems as a proxy for reasoning ability.
Across the models we test, we find an asymmetry: model accuracy can drop
significantly when reasoning in ciphered text, even though models demonstrate
comprehension of ciphered text by being able to translate it accurately to
English. Even frontier models struggle with lesser-known ciphers, although they
can reason accurately in well-known ciphers like rot13. We show that ciphered
reasoning capability correlates with cipher prevalence in pretraining data. We
also identify scaling laws showing that ciphered reasoning capability improves
slowly with additional fine-tuning data. Our work suggests that evading CoT
monitoring using ciphered reasoning may be an ineffective tactic for current
models and offers guidance on constraining the development of this capability
in future frontier models.

</details>


### [7] [Preference-Aware Memory Update for Long-Term LLM Agents](https://arxiv.org/abs/2510.09720)
*Haoran Sun,Zekun Zhang,Shaoning Zeng*

Main category: cs.CL

TL;DR: 提出了偏好感知记忆更新机制（PAMU），通过滑动窗口平均和指数移动平均的融合来动态优化LLM代理的长期记忆表示，显著提升了长期对话中的输出质量。


<details>
  <summary>Details</summary>
Motivation: 现有长期记忆方法在存储和检索方面有显著改进，但缺乏动态更新机制来响应用户行为和上下文的演变，无法有效优化偏好记忆表示。

Method: PAMU机制整合滑动窗口平均（SW）和指数移动平均（EMA），构建融合的偏好感知表示，捕捉短期波动和长期用户趋势。

Result: 在LoCoMo数据集的五个任务场景实验中，该机制显著提升了五个基线模型的LLM输出质量。

Conclusion: PAMU机制在长期对话中验证了其有效性，能够动态和个性化地优化记忆表示。

Abstract: One of the key factors influencing the reasoning capabilities of LLM-based
agents is their ability to leverage long-term memory. Integrating long-term
memory mechanisms allows agents to make informed decisions grounded in
historical interactions. While recent advances have significantly improved the
storage and retrieval components, by encoding memory into dense vectors for
similarity search or organizing memory as structured knowledge graphs most
existing approaches fall short in memory updating. In particular, they lack
mechanisms for dynamically refining preference memory representations in
response to evolving user behaviors and contexts. To address this gap, we
propose a Preference-Aware Memory Update Mechanism (PAMU) that enables dynamic
and personalized memory refinement. By integrating sliding window averages (SW)
with exponential moving averages (EMA), PAMU constructs a fused
preference-aware representation that captures both short-term fluctuations and
long-term user tendencies. We conduct experiments on five task scenarios of the
LoCoMo dataset, and the results show that our mechanism can significantly
improve the output quality of LLM in five baselines, validating its
effectiveness in long-term conversations.

</details>


### [8] [Layout-Aware Parsing Meets Efficient LLMs: A Unified, Scalable Framework for Resume Information Extraction and Evaluation](https://arxiv.org/abs/2510.09722)
*Fanwei Zhu,Jinke Yu,Zulong Chen,Ying Zhou,Junhao Ji,Zhibo Yang,Yuxue Zhang,Haoyuan Hu,Zhenghao Liu*

Main category: cs.CL

TL;DR: 提出了一个布局感知且效率优化的自动化简历信息提取框架，解决了简历布局异构性、LLM高成本延迟以及缺乏标准化数据集三大挑战。


<details>
  <summary>Details</summary>
Motivation: 自动化简历信息提取对规模化人才招聘至关重要，但在实际部署中面临三大挑战：简历布局和内容的极端异构性、大型语言模型的高成本和延迟、以及缺乏标准化数据集和评估工具。

Method: 结合了微调的布局解析器来规范化不同文档格式、基于并行提示和指令调优的推理高效LLM提取器，以及由新基准数据集支持的鲁棒两阶段自动化评估框架。

Result: 广泛实验表明，该框架在准确性和效率方面显著优于强基线。特别是，微调的0.6B紧凑LLM实现了顶级准确性，同时显著降低了推理延迟和计算成本。

Conclusion: 该系统已完全部署在阿里巴巴智能HR平台中，支持其业务部门的实时应用。

Abstract: Automated resume information extraction is critical for scaling talent
acquisition, yet its real-world deployment faces three major challenges: the
extreme heterogeneity of resume layouts and content, the high cost and latency
of large language models (LLMs), and the lack of standardized datasets and
evaluation tools. In this work, we present a layout-aware and
efficiency-optimized framework for automated extraction and evaluation that
addresses all three challenges. Our system combines a fine-tuned layout parser
to normalize diverse document formats, an inference-efficient LLM extractor
based on parallel prompting and instruction tuning, and a robust two-stage
automated evaluation framework supported by new benchmark datasets. Extensive
experiments show that our framework significantly outperforms strong baselines
in both accuracy and efficiency. In particular, we demonstrate that a
fine-tuned compact 0.6B LLM achieves top-tier accuracy while significantly
reducing inference latency and computational cost. The system is fully deployed
in Alibaba's intelligent HR platform, supporting real-time applications across
its business units.

</details>


### [9] [VisRAG 2.0: Evidence-Guided Multi-Image Reasoning in Visual Retrieval-Augmented Generation](https://arxiv.org/abs/2510.09733)
*Yubo Sun,Chunyi Peng,Yukun Yan,Shi Yu,Zhenghao Liu,Chi Chen,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: EVisRAG是一个端到端的视觉检索增强生成框架，通过证据引导的多图像推理来解决当前VRAG系统在多图像感知和证据整合方面的不足。


<details>
  <summary>Details</summary>
Motivation: 当前VRAG系统在多图像感知和证据整合方面存在不足，导致推理基础薄弱和错误结论，需要改进视觉知识的可靠感知和集成。

Method: 提出EVisRAG框架，首先观察检索到的图像并记录每张图像的证据，然后从聚合证据中得出最终答案。使用Reward-Scoped Group Relative Policy Optimization (RS-GRPO)训练方法，将细粒度奖励与范围特定令牌绑定，共同优化VLMs的视觉感知和推理能力。

Result: 在多个视觉问答基准测试中，EVisRAG相比骨干VLM实现了27%的平均提升。进一步分析表明，EVisRAG通过精确感知和定位多图像中与问题相关的证据，并从证据中推导最终答案，显著提高了答案准确性。

Conclusion: EVisRAG通过RS-GRPO驱动的证据引导多图像推理，有效提升了视觉检索增强生成系统的性能，实现了更可靠的视觉知识感知和推理能力。

Abstract: Visual retrieval-augmented generation (VRAG) augments vision-language models
(VLMs) with external visual knowledge to ground reasoning and reduce
hallucinations. Yet current VRAG systems often fail to reliably perceive and
integrate evidence across multiple images, leading to weak grounding and
erroneous conclusions. In this paper, we propose EVisRAG, an end-to-end
framework that learns to reason with evidence-guided multi-image to address
this issue. The model first observes retrieved images and records per-image
evidence, then derives the final answer from the aggregated evidence. To train
EVisRAG effectively, we introduce Reward-Scoped Group Relative Policy
Optimization (RS-GRPO), which binds fine-grained rewards to scope-specific
tokens to jointly optimize visual perception and reasoning abilities of VLMs.
Experimental results on multiple visual question answering benchmarks
demonstrate that EVisRAG delivers substantial end-to-end gains over backbone
VLM with 27\% improvements on average. Further analysis shows that, powered by
RS-GRPO, EVisRAG improves answer accuracy by precisely perceiving and
localizing question-relevant evidence across multiple images and deriving the
final answer from that evidence, much like a real detective.

</details>


### [10] [Judge's Verdict: A Comprehensive Analysis of LLM Judge Capability Through Human Agreement](https://arxiv.org/abs/2510.09738)
*Steve Han,Gilberto Titericz Junior,Tom Balough,Wenfei Zhou*

Main category: cs.CL

TL;DR: 提出了Judge's Verdict Benchmark，一种评估LLM作为响应准确性评估法官的两步方法，测试了54个LLM在RAG或Agentic流程中复制人类判断的能力。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型作为法官在响应准确性评估任务中的表现，特别是它们能否复制人类判断，并超越传统的相关性分析。

Method: 两步方法：首先进行相关性测试筛选对齐良好的法官，然后使用z分数进行人类相似性测试，识别人类相似判断（|z| < 1）和超一致判断（z > 1）两种模式。

Result: 54个测试LLM中有27个达到Tier 1性能：23个模型展示人类相似模式，4个模型展示超一致行为。法官卓越性不仅取决于模型大小，还取决于特定训练策略。

Conclusion: 相关性单独不足以评估法官，提出了基于一致性模式的"法官图灵测试"，并提供了标准化基准将LLM法官分类到不同性能层级。

Abstract: This research introduces the Judge's Verdict Benchmark, a novel two-step
methodology to evaluate Large Language Models (LLMs) as judges for response
accuracy evaluation tasks. We assess how well 54 LLMs can replicate human
judgment when scoring responses from RAG (Retrieval-Augmented Generation) or
Agentic pipelines against ground truth answers. Our methodology progresses from
traditional correlation analysis to comprehensive Cohen's Kappa analysis that
measures actual agreement patterns. The two-step approach includes: (1) a
correlation test that filters judges with strong alignment, followed by (2) a
human-likeness test using z-scores to identify two distinct judgment patterns:
human-like judgment (|z| < 1) that mimics natural human variation, and
super-consistent judgment (z > 1) that exceeds typical human-to-human agreement
levels. This methodology reveals that 27 out of 54 tested LLMs achieve Tier 1
performance: 23 models exhibit human-like patterns that preserve the nuances of
human judgment, while 4 models demonstrate super-consistent behavior, a pattern
that could indicate either enhanced reliability or oversimplification of
complex judgments. Testing 43 open-source models (1B-405B parameters) and 11
closed models (GPT, Gemini, Claude variants), we demonstrate that judge
excellence is not solely dependent on model size but on specific training
strategies. Our key contributions include: (1) establishing that correlation
alone is insufficient for judge evaluation, (2) introducing a "Turing Test for
judges" based on agreement patterns, and (3) providing a standardized benchmark
for classifying LLM judges into distinct performance tiers for different
evaluation needs.

</details>


### [11] [Gold Panning: Turning Positional Bias into Signal for Multi-Document LLM Reasoning](https://arxiv.org/abs/2510.09770)
*Adam Byerly,Daniel Khashabi*

Main category: cs.CL

TL;DR: 提出Gold Panning Bandits框架，利用LLM的位置偏见作为诊断信号，通过重新排序文档来高效识别相关内容，相比随机排列基线减少65%的查询次数。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在多文档环境中存在强烈的位置偏见，系统性地基于位置而非相关性来优先处理信息。现有方法将此偏见视为需要缓解的噪声，而本文则将其转化为可利用的诊断信号。

Method: 将选择重新排序的问题构建为二分图匹配问题。提出贪心O(N log N)策略，通过将最不确定的文档放置在最具信息量的位置来达到可比性能，避免使用O(N^3)的匈牙利算法。

Result: 在知识密集型NLP任务中，相比随机排列基线，该方法使用高达65%更少的语言模型查询来识别相关文档，显著降低计算成本且无需模型重新训练。

Conclusion: 这项工作证明固有的LLM偏见可以从负担转化为资产，实现高效的推理时优化。

Abstract: Large language models exhibit a strong position bias in multi-document
contexts, systematically prioritizing information based on location rather than
relevance. While existing approaches treat this bias as noise to be mitigated,
we introduce Gold Panning Bandits, a framework that leverages position bias as
a diagnostic signal: by reordering documents and observing shifts in the
model's responses, we can efficiently identify the most relevant content. We
frame the problem of choosing reorderings as a bipartite matching problem.
While an optimal assignment can be computed at each iteration with the
Hungarian algorithm in $O(N^3)$ time, we propose a greedy $O(N \log N)$
strategy that achieves comparable performance by prioritizing the placement of
the most uncertain documents in the most informative positions. Our approach
identifies relevant documents using up to 65\% fewer language model queries
than random permutation baselines on knowledge-intensive NLP tasks,
substantially reducing computational cost without model retraining. This work
demonstrates that inherent LLM biases can be transformed from liabilities into
assets for efficient, inference-time optimization.

</details>


### [12] [PromptGuard at BLP-2025 Task 1: A Few-Shot Classification Framework Using Majority Voting and Keyword Similarity for Bengali Hate Speech Detection](https://arxiv.org/abs/2510.09771)
*Rakib Hossan,Shubhashis Roy Dipta*

Main category: cs.CL

TL;DR: PromptGuard是一个用于孟加拉语仇恨言论分类的少样本框架，结合卡方统计分析和自适应多数投票机制，在BLP-2025任务1A中取得67.61的微F1分数，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统监督方法需要大量标注数据，对于低资源语言成本高昂。本文旨在开发一个少样本框架来解决孟加拉语仇恨言论分类问题。

Method: 提出PromptGuard框架，使用卡方统计分析进行关键词提取，结合自适应多数投票机制进行决策。比较统计关键词选择与随机方法，以及自适应投票机制在模糊情况下的扩展分类能力。

Result: PromptGuard达到67.61的微F1分数，优于n-gram基线（60.75）和随机方法（14.65）。卡方关键词在所有类别中都显示出最一致的影响。

Conclusion: 卡方关键词提取提供跨类别的一致改进，自适应投票机制对需要扩展分类轮次的模糊案例有益。该方法在低资源语言仇恨言论分类中表现优异。

Abstract: The BLP-2025 Task 1A requires Bengali hate speech classification into six
categories. Traditional supervised approaches need extensive labeled datasets
that are expensive for low-resource languages. We developed PromptGuard, a
few-shot framework combining chi-square statistical analysis for keyword
extraction with adaptive majority voting for decision-making. We explore
statistical keyword selection versus random approaches and adaptive voting
mechanisms that extend classification based on consensus quality. Chi-square
keywords provide consistent improvements across categories, while adaptive
voting benefits ambiguous cases requiring extended classification rounds.
PromptGuard achieves a micro-F1 of 67.61, outperforming n-gram baselines
(60.75) and random approaches (14.65). Ablation studies confirm
chi-square-based keywords show the most consistent impact across all
categories.

</details>


### [13] [Steering Embedding Models with Geometric Rotation: Mapping Semantic Relationships Across Languages and Models](https://arxiv.org/abs/2510.09790)
*Michael Freenor,Lauren Alvarez*

Main category: cs.CL

TL;DR: 提出RISE方法，通过旋转操作表示语义变换，在多语言嵌入空间中验证了话语级语义变换对应一致的几何操作。


<details>
  <summary>Details</summary>
Motivation: 理解语言和嵌入模型如何编码语义关系对模型可解释性和控制至关重要，但现代高维文本表示缺乏直观的几何性质。

Method: 引入Rotor-Invariant Shift Estimation (RISE)，将语义变换表示为嵌入空间中的一致旋转操作，利用现代语言表示的流形结构。

Result: 在3个嵌入模型、3个数据集和7种形态多样的语言上评估，RISE能一致映射具有不同语法特征的话语级语义变换。

Conclusion: 首次系统证明话语级语义变换对应多语言嵌入空间中的一致几何操作，在句子层面实证支持线性表示假设。

Abstract: Understanding how language and embedding models encode semantic relationships
is fundamental to model interpretability and control. While early word
embeddings exhibited intuitive vector arithmetic (''king'' - ''man'' +
''woman'' = ''queen''), modern high-dimensional text representations lack
straightforward interpretable geometric properties. We introduce
Rotor-Invariant Shift Estimation (RISE), a geometric approach that represents
semantic transformations as consistent rotational operations in embedding
space, leveraging the manifold structure of modern language representations.
RISE operations have the ability to operate across both languages and models
with high transfer of performance, suggesting the existence of analogous
cross-lingual geometric structure. We evaluate RISE across three embedding
models, three datasets, and seven morphologically diverse languages in five
major language groups. Our results demonstrate that RISE consistently maps
discourse-level semantic transformations with distinct grammatical features
(e.g., negation and conditionality) across languages and models. This work
provides the first systematic demonstration that discourse-level semantic
transformations correspond to consistent geometric operations in multilingual
embedding spaces, empirically supporting the Linear Representation Hypothesis
at the sentence level.

</details>


### [14] [Text Prompt Injection of Vision Language Models](https://arxiv.org/abs/2510.09849)
*Ruizhe Zhu*

Main category: cs.CL

TL;DR: 本文研究了一种针对大型视觉语言模型的文本提示注入攻击方法，该方法简单有效且计算资源需求低。


<details>
  <summary>Details</summary>
Motivation: 随着大型视觉语言模型的广泛应用，其安全性问题日益突出。本文旨在研究一种简单有效的攻击方法来误导这些模型。

Method: 开发了一种文本提示注入攻击算法，通过实验验证其有效性和效率。

Result: 实验证明该方法对大型模型特别有效，且相比其他攻击方法计算资源需求更低。

Conclusion: 文本提示注入是一种简单而有效的攻击大型视觉语言模型的方法，具有较低的计算资源需求优势。

Abstract: The widespread application of large vision language models has significantly
raised safety concerns. In this project, we investigate text prompt injection,
a simple yet effective method to mislead these models. We developed an
algorithm for this type of attack and demonstrated its effectiveness and
efficiency through experiments. Compared to other attack methods, our approach
is particularly effective for large models without high demand for
computational resources.

</details>


### [15] [NG-Router: Graph-Supervised Multi-Agent Collaboration for Nutrition Question Answering](https://arxiv.org/abs/2510.09854)
*Kaiwen Shi,Zheyuan Zhang,Zhengqing Yuan,Keerthiram Murugesan,Vincent Galass,Chuxu Zhang,Yanfang Ye*

Main category: cs.CL

TL;DR: NG-Router是一个基于知识图谱的多智能体协作框架，用于解决营养问答中的推理挑战，通过图神经网络学习任务感知的路由分布，并结合梯度子图检索机制提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有营养问答方法面临单智能体推理能力有限和多智能体架构设计复杂的问题，以及上下文过载阻碍准确决策的挑战。

Method: 将营养问答建模为监督式知识图谱引导的多智能体协作问题，将智能体节点集成到异构知识图谱中，使用图神经网络学习任务感知的路由分布，并采用基于梯度的子图检索机制识别关键证据。

Result: 在多个基准测试和骨干模型上的广泛实验表明，NG-Router始终优于单智能体和集成基线方法。

Conclusion: NG-Router为复杂营养健康任务提供了一种原则性的领域感知多智能体推理方法。

Abstract: Diet plays a central role in human health, and Nutrition Question Answering
(QA) offers a promising path toward personalized dietary guidance and the
prevention of diet-related chronic diseases. However, existing methods face two
fundamental challenges: the limited reasoning capacity of single-agent systems
and the complexity of designing effective multi-agent architectures, as well as
contextual overload that hinders accurate decision-making. We introduce
Nutritional-Graph Router (NG-Router), a novel framework that formulates
nutritional QA as a supervised, knowledge-graph-guided multi-agent
collaboration problem. NG-Router integrates agent nodes into heterogeneous
knowledge graphs and employs a graph neural network to learn task-aware routing
distributions over agents, leveraging soft supervision derived from empirical
agent performance. To further address contextual overload, we propose a
gradient-based subgraph retrieval mechanism that identifies salient evidence
during training, thereby enhancing multi-hop and relational reasoning.
Extensive experiments across multiple benchmarks and backbone models
demonstrate that NG-Router consistently outperforms both single-agent and
ensemble baselines, offering a principled approach to domain-aware multi-agent
reasoning for complex nutritional health tasks.

</details>


### [16] [NarraBench: A Comprehensive Framework for Narrative Benchmarking](https://arxiv.org/abs/2510.09869)
*Sil Hamilton,Matthew Wilkens,Andrew Piper*

Main category: cs.CL

TL;DR: NarraBench是一个理论驱动的叙事理解任务分类法，调查了78个现有基准，发现当前评估仅覆盖27%的叙事任务，许多关键方面如事件、风格、视角和启示等几乎缺失。


<details>
  <summary>Details</summary>
Motivation: 当前叙事理解评估存在显著空白，许多重要叙事方面被忽视或与现有指标不匹配，特别是构成性主观和视角化方面的评估不足。

Method: 开发理论驱动的叙事理解任务分类法，系统调查78个现有基准，分析覆盖范围和评估能力。

Result: 仅27%的叙事任务被现有基准充分覆盖，叙事事件、风格、视角和启示等关键方面几乎完全缺失，构成性主观方面的评估尤其不足。

Conclusion: 需要开发新的评估基准来覆盖被忽视的叙事理解方面，特别是那些没有单一正确答案的主观和视角化方面，该分类法和调查方法对测试LLM叙事理解有价值。

Abstract: We present NarraBench, a theory-informed taxonomy of narrative-understanding
tasks, as well as an associated survey of 78 existing benchmarks in the area.
We find significant need for new evaluations covering aspects of narrative
understanding that are either overlooked in current work or are poorly aligned
with existing metrics. Specifically, we estimate that only 27% of narrative
tasks are well captured by existing benchmarks, and we note that some areas --
including narrative events, style, perspective, and revelation -- are nearly
absent from current evaluations. We also note the need for increased
development of benchmarks capable of assessing constitutively subjective and
perspectival aspects of narrative, that is, aspects for which there is
generally no single correct answer. Our taxonomy, survey, and methodology are
of value to NLP researchers seeking to test LLM narrative understanding.

</details>


### [17] [CoBia: Constructed Conversations Can Trigger Otherwise Concealed Societal Biases in LLMs](https://arxiv.org/abs/2510.09871)
*Nafiseh Nikeghbal,Amir Hossein Kargaran,Jana Diesner*

Main category: cs.CL

TL;DR: CoBia是一个轻量级对抗攻击套件，通过构建包含偏见声明的对话来测试LLMs能否从虚假偏见中恢复并拒绝偏见性后续问题，揭示了LLMs在对话中经常放大偏见且难以纠正的问题。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs通过安全护栏改进能通过标准安全检查，但在对话中仍会表现出有害行为（如种族主义观点）。需要系统分析LLMs在何种条件下偏离规范行为。

Method: 构建包含模型发表偏见声明的对话，评估模型能否从虚假偏见中恢复并拒绝偏见性后续问题。评估11个开源和专有LLMs在6个社会人口类别上的表现。

Result: 有目的构建的对话能可靠揭示偏见放大现象，LLMs在对话中经常无法拒绝偏见性后续问题。压力测试凸显了通过交互可显现的深层嵌入偏见。

Conclusion: 这种形式的压力测试揭示了LLMs中深层嵌入的偏见，这些偏见可以通过交互被激发出来，表明需要更强大的安全机制来防止偏见放大。

Abstract: Improvements in model construction, including fortified safety guardrails,
allow Large language models (LLMs) to increasingly pass standard safety checks.
However, LLMs sometimes slip into revealing harmful behavior, such as
expressing racist viewpoints, during conversations. To analyze this
systematically, we introduce CoBia, a suite of lightweight adversarial attacks
that allow us to refine the scope of conditions under which LLMs depart from
normative or ethical behavior in conversations. CoBia creates a constructed
conversation where the model utters a biased claim about a social group. We
then evaluate whether the model can recover from the fabricated bias claim and
reject biased follow-up questions. We evaluate 11 open-source as well as
proprietary LLMs for their outputs related to six socio-demographic categories
that are relevant to individual safety and fair treatment, i.e., gender, race,
religion, nationality, sex orientation, and others. Our evaluation is based on
established LLM-based bias metrics, and we compare the results against human
judgments to scope out the LLMs' reliability and alignment. The results suggest
that purposefully constructed conversations reliably reveal bias amplification
and that LLMs often fail to reject biased follow-up questions during dialogue.
This form of stress-testing highlights deeply embedded biases that can be
surfaced through interaction. Code and artifacts are available at
https://github.com/nafisenik/CoBia.

</details>


### [18] [iBERT: Interpretable Style Embeddings via Sense Decomposition](https://arxiv.org/abs/2510.09882)
*Vishal Anand,Milad Alshomary,Kathleen McKeown*

Main category: cs.CL

TL;DR: iBERT是一个可解释的BERT编码器，通过稀疏非负混合的上下文无关感知向量来生成可解释且可控的嵌入表示，能够模块化地暴露语言中的判别性线索。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统BERT模型缺乏可解释性和可控性的问题，设计一个能够模块化暴露语言中风格和语义结构等判别性线索的编码器。

Method: 使用稀疏非负混合的上下文无关感知向量表示每个输入token，这些向量可以池化为句子嵌入或在token级别直接使用，实现表示的可控性。

Result: 在STEL基准测试中，相比SBERT基线模型，风格表示效果提升约8个百分点，同时在作者验证任务上保持竞争力。能够将特定风格属性（如表情符号使用、正式性、拼写错误）分配给特定感知向量。

Conclusion: iBERT不仅限于风格建模，其结构模块化设计能够可解释地分解数据中存在的任何判别性信号，即使在监督混合风格和语义因素时也能实现泛化。

Abstract: We present iBERT (interpretable-BERT), an encoder to produce inherently
interpretable and controllable embeddings - designed to modularize and expose
the discriminative cues present in language, such as stylistic and semantic
structure. Each input token is represented as a sparse, non-negative mixture
over k context-independent sense vectors, which can be pooled into sentence
embeddings or used directly at the token level. This enables modular control
over representation, before any decoding or downstream use.
  To demonstrate our model's interpretability, we evaluate it on a suite of
style-focused tasks. On the STEL benchmark, it improves style representation
effectiveness by ~8 points over SBERT-style baselines, while maintaining
competitive performance on authorship verification. Because each embedding is a
structured composition of interpretable senses, we highlight how specific style
attributes - such as emoji use, formality, or misspelling can be assigned to
specific sense vectors. While our experiments center on style, iBERT is not
limited to stylistic modeling. Its structural modularity is designed to
interpretably decompose whichever discriminative signals are present in the
data - enabling generalization even when supervision blends stylistic and
semantic factors.

</details>


### [19] [DELTA: Dynamic Layer-Aware Token Attention for Efficient Long-Context Reasoning](https://arxiv.org/abs/2510.09883)
*Hossein Entezari Zarch,Lei Gao,Chaoyi Jiang,Murali Annavarm*

Main category: cs.CL

TL;DR: DELTA是一种无需训练的稀疏注意力机制，通过将Transformer层分为三组：全注意力初始层、选择层和稀疏注意力层，在保持推理准确性的同时显著减少计算量。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在解码过程中需要关注整个增长的序列，计算成本高昂。现有的稀疏注意力方法虽然减少了计算，但在推理任务中由于累积选择错误和token重要性动态变化而导致准确性严重下降。

Method: 将Transformer层分为三组：初始层使用全注意力，少量选择层通过聚合头级注意力分数识别重要token，后续稀疏注意力层仅关注选定的token子集。

Result: 在AIME和GPQA-Diamond等推理基准测试中，DELTA在准确性上匹配或超越全注意力方法，同时将关注的token数量减少高达5倍，实现1.5倍的端到端加速。

Conclusion: 选择性重用中间注意力图为高效长上下文推理提供了稳健的路径。

Abstract: Large reasoning models (LRMs) achieve state-of-the-art performance on
challenging benchmarks by generating long chains of intermediate steps, but
their inference cost is dominated by decoding, where each new token must attend
to the entire growing sequence. Existing sparse attention methods reduce
computation by pruning the key-value (KV) cache, yet they suffer from severe
accuracy degradation on reasoning tasks due to cumulative selection errors and
the dynamic importance of tokens over long derivations. We present
\textbf{DELTA}, a training-free sparse attention mechanism that achieves
computational efficiency without sacrificing model accuracy. DELTA partitions
transformer layers into three groups: initial layers that use full attention, a
small set of \emph{selection layers} that identify salient tokens via
aggregated head-level attention scores, and subsequent \emph{sparse-attention
layers} that attend only to the selected subset. This design preserves the full
KV cache in GPU memory for accuracy, while avoiding expensive full-attention
computation over many layers. On reasoning benchmarks such as AIME and
GPQA-Diamond, DELTA matches or surpasses full attention in accuracy, while
reducing the number of attended tokens by up to $5\times$ and delivering
$1.5\times$ end-to-end speedup. Our results show that selective reuse of
intermediate attention maps offers a robust path toward efficient long-context
reasoning.

</details>


### [20] [Closing the Data-Efficiency Gap Between Autoregressive and Masked Diffusion LLMs](https://arxiv.org/abs/2510.09885)
*Xu Pan,Ely Hahami,Jingxuan Fan,Ziqian Xie,Haim Sompolinsky*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Despite autoregressive large language models (arLLMs) being the current
dominant paradigm in language modeling, they resist knowledge injection via
fine-tuning due to inherent shortcomings such as the "reversal curse" -- the
challenge of answering questions that reverse the original information order in
the training sample. Masked diffusion large language models (dLLMs) are rapidly
emerging as a powerful alternative to the arLLM paradigm, with evidence of
better data efficiency and free of the "reversal curse" in pre-training.
However, it is unknown whether these advantages extend to the post-training
phase, i.e. whether pre-trained dLLMs can easily acquire new knowledge through
fine-tuning. On three diverse datasets, we fine-tune arLLMs and dLLMs,
evaluating them with forward and backward style Question Answering (QA) to
probe knowledge generalization and the reversal curse. Our results confirm that
arLLMs critically rely on extensive data augmentation via paraphrases for QA
generalization, and paraphrases are only effective when their information order
matches the QA style. Conversely, dLLMs achieve high accuracies on both forward
and backward QAs without paraphrases; adding paraphrases yields only marginal
gains. Lastly, inspired by the dLLM's performance, we introduce a novel masked
fine-tuning paradigm for knowledge injection into pre-trained arLLMs. This
proposed method successfully and drastically improves the data efficiency of
arLLM fine-tuning, effectively closing the performance gap with dLLMs.

</details>


### [21] [Abductive Preference Learning](https://arxiv.org/abs/2510.09887)
*Yijin Ni,Peng Qi*

Main category: cs.CL

TL;DR: 论文提出了一种反绎偏好学习方法，通过反转传统条件化来学习给定回答下的提示偏好，解决了现有偏好学习忽视反事实提示的问题。


<details>
  <summary>Details</summary>
Motivation: 前沿大语言模型即使经过RLHF和DPO对齐后仍存在过度自信问题，无法区分需要不同回答的相似提示，这源于现有偏好学习只关注给定提示选择正确回答，而忽视了应改变回答的反事实提示。

Method: 提出反绎偏好学习微调范式，构建基于HaluEval QA基准的反绎数据集，实现反绎DPO及其变体DPOP，并通过多任务目标统一传统方法和反绎方法。

Result: 在反绎数据集上，多任务DPOP将回答选择准确率从90.0%提升至99.5%，提示区分准确率从54.7%提升至85.0%；在AlpacaEval上胜率从5.26%提升至6.17%。

Conclusion: 反绎偏好学习在保持传统偏好优化优势的同时，有效解决了反事实提示的挑战，提高了模型对提示差异的敏感性。

Abstract: Frontier large language models such as GPT-5 and Claude Sonnet remain prone
to overconfidence even after alignment through Reinforcement Learning with
Human Feedback (RLHF) and Direct Preference Optimization (DPO). For instance,
they tend to offer the same conservative answer "No" to both questions "Can I
eat the [food / potato chips] that has been left out overnight?" despite the
latter requiring no refridgeration for safe consumption. We find that this
failure is potentially attributed to a limitation of existing preference
learning: it emphasizes selecting the correct response for a given prompt,
while neglecting counterfactual prompts that should alter the response.
  To address this limitation, we propose abductive preference learning, a
fine-tuning paradigm that reverses the conventional conditioning by learning
preferences over prompts given a response. To validate this idea, we construct
an abductive dataset derived from the HaluEval QA benchmark with 1,001 entries,
implementing abductive DPO and its variant DPOP. Experiments reveal
complementary strengths: standard methods improve response selection, abductive
methods improve prompt discrimination, while a multitask objective unifies
both. On the abductive dataset, multitask DPOP boosts accuracy from $90.0\%$ to
$99.5\%$ in response selection and $54.7\%$ to $85.0\%$ in prompt
discrimination, with qualitative evidence highlighting improved sensitivity to
prompt differences. Finally, evaluation on AlpacaEval shows multitask DPOP
improves win rate (from $5.26\%$ to $6.17\%$), confirming that abductive
preference learning preserves the benefits of conventional preference
optimization while addressing the overlooked challenge of counterfactual
prompts.

</details>


### [22] [HIPPD: Brain-Inspired Hierarchical Information Processing for Personality Detection](https://arxiv.org/abs/2510.09893)
*Guanming Chen,Lingzhi Shen,Xiaohao Cai,Imran Razzak,Shoaib Jameel*

Main category: cs.CL

TL;DR: HIPPD是一个受大脑启发的性格检测框架，通过模拟人脑的层次化信息处理机制，在多个文本数据上实现了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习方法难以捕捉跨多篇文章的上下文信息，在语义稀疏环境中提取代表性特征的能力不足。

Method: 使用大语言模型模拟大脑皮层进行全局语义推理，动态记忆模块模拟前额叶皮层进行特征选择，轻量级模型模拟基底神经节通过赢家通吃机制识别性格模式。

Result: 在Kaggle和Pandora数据集上的大量实验表明，HIPPD持续优于最先进的基线方法。

Conclusion: 受大脑启发的方法在性格检测任务中具有显著优势，能够更好地处理上下文信息和提取鲁棒特征。

Abstract: Personality detection from text aims to infer an individual's personality
traits based on linguistic patterns. However, existing machine learning
approaches often struggle to capture contextual information spanning multiple
posts and tend to fall short in extracting representative and robust features
in semantically sparse environments. This paper presents HIPPD, a
brain-inspired framework for personality detection that emulates the
hierarchical information processing of the human brain. HIPPD utilises a large
language model to simulate the cerebral cortex, enabling global semantic
reasoning and deep feature abstraction. A dynamic memory module, modelled after
the prefrontal cortex, performs adaptive gating and selective retention of
critical features, with all adjustments driven by dopaminergic prediction error
feedback. Subsequently, a set of specialised lightweight models, emulating the
basal ganglia, are dynamically routed via a strict winner-takes-all mechanism
to capture the personality-related patterns they are most proficient at
recognising. Extensive experiments on the Kaggle and Pandora datasets
demonstrate that HIPPD consistently outperforms state-of-the-art baselines.

</details>


### [23] [Don't Throw Away Your Pretrained Model](https://arxiv.org/abs/2510.09913)
*Shangbin Feng,Wenhao Yu,Yike Wang,Hongming Zhang,Yulia Tsvetkov,Dong Yu*

Main category: cs.CL

TL;DR: 提出Switch Generation方法，通过让预训练模型和对齐模型在响应序列中交替生成，实现模型协作，在保持各自优势的同时提升整体性能。


<details>
  <summary>Details</summary>
Motivation: 对齐训练存在权衡：虽然能提升语言模型的推理和指令跟随能力，但可能损失创造性和校准等技能。目标是结合不同模型的优势，实现最佳性能。

Method: 训练一个切换器语言模型，学习在不同查询和上下文中选择不同模型生成下一段内容。在推理时，切换器指导不同模型检查点在各自优势领域动态生成内容。

Result: 在18个数据集上的实验表明：1）模型协作在16个任务上持续优于单个模型；2）Switch Generation方法比基线平均提升12.9%。

Conclusion: Switch Generation能够发现组合技能来解决单个模型难以处理的问题，并能泛化到未见过的模型和任务，有效重用和重新利用昂贵模型训练流程中的副产品。

Abstract: Alignment training has tradeoffs: it helps language models (LMs) gain in
reasoning and instruction following but might lose out on skills such as
creativity and calibration, where unaligned base models are better at. We aim
to make the best of both worlds through model collaboration, where different
models in the training pipeline collaborate and complement each other. Since LM
responses feature interleaving skills that favor different models, we propose
Switch Generation, where pretrained and aligned model versions take turns to
``speak'' in a response sequence. Specifically, we train a switcher LM by
learning from outcomes of choosing different models to generate the next
segment across diverse queries and contexts. At inference time, the switcher LM
guides different model checkpoints to dynamically generate the next segment
where their strengths are most needed. Extensive experiments with 8 model
collaboration baselines and 18 datasets show that 1) model collaboration
consistently outperforms individual models on 16 out of 18 tasks, and 2) Switch
Generation further outperforms baselines by 12.9% on average. Further analysis
reveals that Switch Generation discovers compositional skills to solve problems
where individual models struggle and generalizes to unseen models and tasks,
reusing and repurposing by-products in expensive model training pipelines that
are otherwise discarded.

</details>


### [24] [Enhancing Faithfulness in Abstractive Summarization via Span-Level Fine-Tuning](https://arxiv.org/abs/2510.09915)
*Sicong Huang,Qianqi Yan,Shengze Wang,Ian Lane*

Main category: cs.CL

TL;DR: 本文研究了通过微调大语言模型来减少摘要生成中的幻觉现象，提出了三种基于span级标注的微调方法，其中unlikelihood训练效果最佳。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型生成的摘要虽然流畅，但经常产生不忠实的内容（幻觉），现有缓解策略无法完全解决这一问题。

Method: 使用多种LLM自动生成训练集摘要，用GPT-4o进行span级幻觉标注，然后采用梯度上升、unlikelihood训练和任务向量否定三种微调方法。

Result: 实验结果表明，所有三种方法都能有效利用span级标注提高摘要忠实度，其中unlikelihood训练效果最好。

Conclusion: 基于span级标注的微调策略能够显著提升LLM生成摘要的忠实度，为减少摘要幻觉提供了有效解决方案。

Abstract: Abstractive summarization using large language models (LLMs) has become an
essential tool for condensing information. However, despite their ability to
generate fluent summaries, these models sometimes produce unfaithful summaries,
introducing hallucinations at the word, phrase, or concept level. Existing
mitigation strategies, such as post-processing corrections or contrastive
learning with synthetically generated negative samples, fail to fully address
the diverse errors that can occur in LLM-generated summaries. In this paper, we
investigate fine-tuning strategies to reduce the occurrence of unfaithful spans
in generated summaries. First, we automatically generate summaries for the set
of source documents in the training set with a variety of LLMs and then use
GPT-4o to annotate any hallucinations it detects at the span-level. Leveraging
these annotations, we fine-tune LLMs with both hallucination-free summaries and
annotated unfaithful spans to enhance model faithfulness. In this paper, we
introduce a new dataset that contains both faithful and unfaithful summaries
with span-level labels and we evaluate three techniques to fine-tuning a LLM to
improve the faithfulness of the resulting summarization: gradient ascent,
unlikelihood training, and task vector negation. Experimental results show that
all three approaches successfully leverage span-level annotations to improve
faithfulness, with unlikelihood training being the most effective.

</details>


### [25] [Unpacking Hateful Memes: Presupposed Context and False Claims](https://arxiv.org/abs/2510.09935)
*Weibin Cai,Jiayu Li,Reza Zafarani*

Main category: cs.CL

TL;DR: SHIELD是一个仇恨表情包检测框架，通过建模预设上下文和检测虚假声明来识别仇恨内容，在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前仇恨表情包检测主要依赖预训练语言模型，但缺乏对仇恨表情包本质特征的研究。基于哲学和心理学洞察，作者认为仇恨表情包具有两个关键特征：预设上下文和虚假声明表达。

Method: 开发了PCM模块建模跨模态的上下文信息，FACT模块整合外部知识并利用跨模态参考图检测虚假声明，结合两者构建SHIELD框架。

Result: 在多个数据集和指标上，SHIELD都优于最先进的方法，并在假新闻检测等其他任务上表现出良好的泛化能力。

Conclusion: SHIELD通过捕捉仇恨表情包的基本特征（预设上下文和虚假声明），实现了更有效的仇恨内容检测，为理解仇恨表情包的本质提供了新视角。

Abstract: While memes are often humorous, they are frequently used to disseminate hate,
causing serious harm to individuals and society. Current approaches to hateful
meme detection mainly rely on pre-trained language models. However, less focus
has been dedicated to \textit{what make a meme hateful}. Drawing on insights
from philosophy and psychology, we argue that hateful memes are characterized
by two essential features: a \textbf{presupposed context} and the expression of
\textbf{false claims}. To capture presupposed context, we develop \textbf{PCM}
for modeling contextual information across modalities. To detect false claims,
we introduce the \textbf{FACT} module, which integrates external knowledge and
harnesses cross-modal reference graphs. By combining PCM and FACT, we introduce
\textbf{\textsf{SHIELD}}, a hateful meme detection framework designed to
capture the fundamental nature of hate. Extensive experiments show that SHIELD
outperforms state-of-the-art methods across datasets and metrics, while
demonstrating versatility on other tasks, such as fake news detection.

</details>


### [26] [Beyond Fertility: Analyzing STRR as a Metric for Multilingual Tokenization Evaluation](https://arxiv.org/abs/2510.09947)
*Mir Tafseer Nayeem,Sawsan Alqahtani,Md Tahmid Rahman Laskar,Tasnim Mohiuddin,M Saiful Bari*

Main category: cs.CL

TL;DR: 提出了单令牌保留率（STRR）作为评估分词器跨语言公平性的新指标，揭示了现有分词器对英语的系统性优先处理，以及中文支持较好但印地语分词碎片化的问题。


<details>
  <summary>Details</summary>
Motivation: 现有分词评估标准（生育率）主要关注压缩效率，但掩盖了词汇表在不同语言和领域间的分配公平性问题，需要更全面的评估指标。

Method: 分析了6个广泛使用的分词器在7种语言和2个领域中的表现，提出了单令牌保留率（STRR）来衡量单词被保留为单个令牌的比例。

Result: 发现英语生育率稳定，中文生育率高，领域敏感性低；STRR显示英语被系统性优先处理，中文支持良好，但印地语存在严重分词碎片化。

Conclusion: STRR补充了生育率指标，为设计更公平的多语言分词器提供了实用指导，能够更全面地评估分词器的跨语言公平性。

Abstract: Tokenization is a crucial but under-evaluated step in large language models
(LLMs). The standard metric, fertility (the average number of tokens per word),
captures compression efficiency but obscures how vocabularies are allocated
across languages and domains. We analyze six widely used tokenizers across
seven languages and two domains, finding stable fertility for English, high
fertility for Chinese, and little domain sensitivity. To address fertility's
blind spots, we propose the Single Token Retention Rate (STRR), which measures
the proportion of words preserved as single tokens. STRR reveals systematic
prioritization of English, strong support for Chinese, and fragmentation in
Hindi, offering an interpretable view of cross-lingual fairness. Our results
show that STRR complements fertility and provides practical guidance for
designing more equitable multilingual tokenizers.

</details>


### [27] [Unifying Tree Search Algorithm and Reward Design for LLM Reasoning: A Survey](https://arxiv.org/abs/2510.09988)
*Jiaqi Wei,Xiang Zhang,Yuejin Yang,Wenxuan Huang,Juntai Cao,Sheng Xu,Xiang Zhuang,Zhangyang Gao,Muhammad Abdul-Mageed,Laks V. S. Lakshmanan,Chenyu You,Wanli Ouyang,Siqi Sun*

Main category: cs.CL

TL;DR: 本文提出了一个统一框架，将搜索算法分解为搜索机制、奖励公式和转移函数三个核心组件，明确了搜索引导和参数化奖励建模的区别，为自主自改进代理的研究提供了系统化路径。


<details>
  <summary>Details</summary>
Motivation: 当前审议树搜索领域缺乏统一形式化框架，特别是在奖励信号的角色上存在模糊性——是临时启发式还是持久学习目标？这种模糊性阻碍了该领域的系统性进展。

Method: 引入统一框架将搜索算法分解为三个核心组件：搜索机制、奖励公式和转移函数，建立搜索引导与参数化奖励建模的形式化区分，并提出基于组件的分类法。

Result: 构建了组件中心分类法，综合了最先进技术，并为创建自主自改进代理绘制了研究路线图。

Conclusion: 该框架解决了奖励信号角色的模糊性，为审议树搜索领域的系统性进展提供了理论基础，推动了从暴力扩展向算法效率的转变。

Abstract: Deliberative tree search is a cornerstone of modern Large Language Model
(LLM) research, driving the pivot from brute-force scaling toward algorithmic
efficiency. This single paradigm unifies two critical frontiers:
\textbf{Test-Time Scaling (TTS)}, which deploys on-demand computation to solve
hard problems, and \textbf{Self-Improvement}, which uses search-generated data
to durably enhance model parameters. However, this burgeoning field is
fragmented and lacks a common formalism, particularly concerning the ambiguous
role of the reward signal -- is it a transient heuristic or a durable learning
target? This paper resolves this ambiguity by introducing a unified framework
that deconstructs search algorithms into three core components: the
\emph{Search Mechanism}, \emph{Reward Formulation}, and \emph{Transition
Function}. We establish a formal distinction between transient \textbf{Search
Guidance} for TTS and durable \textbf{Parametric Reward Modeling} for
Self-Improvement. Building on this formalism, we introduce a component-centric
taxonomy, synthesize the state-of-the-art, and chart a research roadmap toward
more systematic progress in creating autonomous, self-improving agents.

</details>


### [28] [Toward Machine Translation Literacy: How Lay Users Perceive and Rely on Imperfect Translations](https://arxiv.org/abs/2510.09994)
*Yimin Xiao,Yongle Zhang,Dayeon Ki,Calvin Bao,Marianna J. Martindale,Charlotte Vaughn,Ge Gao,Marine Carpuat*

Main category: cs.CL

TL;DR: 研究在博物馆环境中调查了双语和非双语用户对机器翻译的依赖程度，发现非双语用户由于缺乏评估策略而过度依赖机器翻译，但经历错误后会重新评估依赖。


<details>
  <summary>Details</summary>
Motivation: 随着机器翻译日益普及，了解公众如何感知和依赖不完美的机器翻译对于将机器翻译研究置于实际应用背景中至关重要。

Method: 在公共博物馆进行人类研究（n=452），调查流畅性和充分性错误如何影响双语和非双语用户在休闲使用中对机器翻译的依赖。

Result: 非双语用户由于缺乏评估策略和替代方案而经常过度依赖机器翻译，但经历错误影响会促使用户重新评估未来的依赖程度。

Conclusion: 机器翻译评估和NLP解释技术不仅需要提升机器翻译质量，还需要促进用户对机器翻译的认知素养。

Abstract: As Machine Translation (MT) becomes increasingly commonplace, understanding
how the general public perceives and relies on imperfect MT is crucial for
contextualizing MT research in real-world applications. We present a human
study conducted in a public museum (n=452), investigating how fluency and
adequacy errors impact bilingual and non-bilingual users' reliance on MT during
casual use. Our findings reveal that non-bilingual users often over-rely on MT
due to a lack of evaluation strategies and alternatives, while experiencing the
impact of errors can prompt users to reassess future reliance. This highlights
the need for MT evaluation and NLP explanation techniques to promote not only
MT quality, but also MT literacy among its users.

</details>


### [29] [MTP-S2UT: Enhancing Speech-to-Speech Translation Quality with Multi-token Prediction](https://arxiv.org/abs/2510.10003)
*Jianjin Wang,Runsong Zhao,Xiaoqian Liu,Yuan Ge,Ziqiang Xu,Tong Xiao,Shengxiang Gao,Zhengtao Yu,Jingbo Zhu*

Main category: cs.CL

TL;DR: 提出多令牌预测(MTP)损失函数用于语音到单元翻译(S2UT)模型，通过在中间层应用MTP损失来提前增强隐藏表示的信息密度，从而提高翻译质量。


<details>
  <summary>Details</summary>
Motivation: 当前语音到语音翻译方法使用语音令牌作为中间表示，但单个令牌语义密度不足，需要多个令牌才能表达完整语义单元。

Method: 在S2UT模型中引入多令牌预测损失，包括在最终层应用MTP损失和在中间层应用MTP-S2UT损失，使模型能够在每个位置预测多个后续令牌。

Result: 所有MTP损失变体都一致提高了S2UT翻译质量，其中MTP-S2UT实现了最佳性能。

Conclusion: 通过将信息丰富过程提前到中间层，可以更早且更有效地增强隐藏表示，从而提升语音到语音翻译的性能。

Abstract: Current direct speech-to-speech translation methods predominantly employ
speech tokens as intermediate representations. However, a single speech token
is not dense in semantics, so we generally need multiple tokens to express a
complete semantic unit. To address this limitation, we introduce multi-token
prediction (MTP) loss into speech-to-unit translation (S2UT) models, enabling
models to predict multiple subsequent tokens at each position, thereby
capturing more complete semantics and enhancing information density per
position. Initial MTP implementations apply the loss at the final layer, which
improves output representation but initiates information enrichment too late.
We hypothesize that advancing the information enrichment process to
intermediate layers can achieve earlier and more effective enhancement of
hidden representation. Consequently, we propose MTP-S2UT loss, applying MTP
loss to hidden representation where CTC loss is computed. Experiments
demonstrate that all MTP loss variants consistently improve the quality of S2UT
translation, with MTP-S2UT achieving the best performance.

</details>


### [30] [Beyond the limitation of a single query: Train your LLM for query expansion with Reinforcement Learning](https://arxiv.org/abs/2510.10009)
*Shu Zhao,Tan Yu,Anbang Xu*

Main category: cs.CL

TL;DR: 提出ExpandSearch方法，通过强化学习训练LLM搜索代理，具备查询扩展能力，同时引入预训练压缩器模型帮助理解检索文档，在7个问答基准测试中平均提升4.4%，在多跳推理任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有推理增强搜索代理在推理和搜索能力有限，多跳QA基准测试表现不佳。处理复杂或复合查询时，需要更好的查询扩展能力。

Method: 使用强化学习训练LLM搜索代理，每轮生成多个查询变体并行搜索；引入预训练压缩器模型帮助理解检索文档，让搜索代理专注于查询生成以提高检索召回率。

Result: 在7个问答基准测试中平均提升4.4%，在多跳推理任务上获得显著增益，即使3B小规模LLM也能展现强大的查询扩展能力。

Conclusion: ExpandSearch方法通过查询扩展和压缩器模型辅助，显著提升了多跳问答性能，证明了小规模模型在适当架构下也能达到最先进水平。

Abstract: Reasoning-augmented search agents, such as Search-R1, are trained to reason,
search, and generate the final answer iteratively. Nevertheless, due to their
limited capabilities in reasoning and search, their performance on multi-hop QA
benchmarks remains far from satisfactory. To handle complex or compound
queries, we train an LLM-based search agent with the native capability of query
expansion through reinforcement learning. In each turn, our search agent
proposes several query variants, which are searched simultaneously to cover
more relevant information. Meanwhile, given limited post-training data and
computing resources, it is very challenging for a search agent to master
multiple tasks, including query generation, retrieved information
understanding, and answer generation. Therefore, we propose incorporating a
pre-trained squeezer model that helps the search agent understand the retrieved
documents, allowing the search agent to focus on query generation for high
retrieval recall. With the assistance of the squeezer model, we discover that
even a small-scale 3B LLM can demonstrate a strong capability of query
expansion and achieve state-of-the-art accuracy on the multi-hop QA benchmarks.
To be specific, our experiments across seven question-answering benchmarks
demonstrate that our method, named ExpandSearch, achieves an average
improvement of 4.4% compared to state-of-the-art baselines, with strong gains
on multi-hop reasoning tasks requiring diverse evidence aggregation.

</details>


### [31] [Path Drift in Large Reasoning Models:How First-Person Commitments Override Safety](https://arxiv.org/abs/2510.10013)
*Yuyi Huang,Runzhe Zhan,Lidia S. Chao,Ailin Tao,Derek F. Wong*

Main category: cs.CL

TL;DR: 论文发现大型语言模型在长链思维推理中存在路径漂移漏洞，即推理轨迹会偏离对齐路径产生不安全内容。作者提出了路径漂移诱导框架和防御策略。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型被部署用于复杂推理任务，长链思维提示成为结构化推理的关键范式。尽管有RLHF等对齐技术提供早期保护，但作者发现了一个未被充分探索的漏洞：推理轨迹可能偏离对齐路径。

Method: 通过实证分析识别了路径漂移的三个行为触发因素，并提出了三阶段路径漂移诱导框架（认知负荷放大、自我角色启动、条件链劫持）。为缓解风险，提出了路径级防御策略（角色归因修正和元认知反思）。

Result: 路径漂移诱导框架的每个阶段都能独立降低拒绝率，组合使用效果更显著。路径级防御策略能有效缓解路径漂移风险。

Conclusion: 研究结果表明，在长形式推理中需要超越词元级对齐的轨迹级对齐监督，以确保推理过程的安全性和可靠性。

Abstract: As large language models (LLMs) are increasingly deployed for complex
reasoning tasks, Long Chain-of-Thought (Long-CoT) prompting has emerged as a
key paradigm for structured inference. Despite early-stage safeguards enabled
by alignment techniques such as RLHF, we identify a previously underexplored
vulnerability: reasoning trajectories in Long-CoT models can drift from aligned
paths, resulting in content that violates safety constraints. We term this
phenomenon Path Drift. Through empirical analysis, we uncover three behavioral
triggers of Path Drift: (1) first-person commitments that induce goal-driven
reasoning that delays refusal signals; (2) ethical evaporation, where
surface-level disclaimers bypass alignment checkpoints; (3) condition chain
escalation, where layered cues progressively steer models toward unsafe
completions. Building on these insights, we introduce a three-stage Path Drift
Induction Framework comprising cognitive load amplification, self-role priming,
and condition chain hijacking. Each stage independently reduces refusal rates,
while their combination further compounds the effect. To mitigate these risks,
we propose a path-level defense strategy incorporating role attribution
correction and metacognitive reflection (reflective safety cues). Our findings
highlight the need for trajectory-level alignment oversight in long-form
reasoning beyond token-level alignment.

</details>


### [32] [Lightweight Baselines for Medical Abstract Classification: DistilBERT with Cross-Entropy as a Strong Default](https://arxiv.org/abs/2510.10025)
*Jiaqi Liu,Lanruo Wang,Su Liu,Xin Hu*

Main category: cs.CL

TL;DR: 该研究评估了在医疗文本分类任务中，轻量级编码器（如DistilBERT）与标准BERT的性能对比，发现在参数更少的情况下，DistilBERT配合交叉熵损失函数能达到最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在医疗环境中部署面临成本、延迟和隐私限制，因此需要探索轻量级解决方案在医疗文本分类任务中的潜力。

Method: 使用公开医疗摘要语料库，在固定分词器、序列长度、优化器和训练计划的情况下，对BERT base和DistilBERT进行微调，比较三种损失函数：标准交叉熵、类别加权交叉熵和焦点损失。

Result: DistilBERT配合标准交叉熵损失在测试集上表现最佳，同时使用的参数远少于BERT base。研究报告了准确率、宏F1和加权F1指标。

Conclusion: 建议从紧凑编码器和交叉熵损失开始，然后添加校准和任务特定检查，最后再考虑使用更重的模型。

Abstract: Large language models work well for many NLP tasks, but they are hard to
deploy in health settings with strict cost, latency, and privacy limits. We
revisit a lightweight recipe for medical abstract classification and ask how
far compact encoders can go under a controlled budget. Using the public medical
abstracts corpus, we finetune BERT base and DistilBERT with three objectives
standard cross-entropy, class weighted cross entropy, and focal loss keeping
tokenizer, sequence length, optimizer, and schedule fixed. DistilBERT with
plain cross-entropy gives the best balance on the test set while using far
fewer parameters than BERT base. We report accuracy, Macro F1, and Weighted F1,
release the evaluation code, and include confusion analyses to make error
patterns clear. Our results suggest a practical default: start with a compact
encoder and cross-entropy, then add calibration and task-specific checks before
moving to heavier models.

</details>


### [33] [HUME: Measuring the Human-Model Performance Gap in Text Embedding Task](https://arxiv.org/abs/2510.10062)
*Adnan El Assadi,Isaac Chung,Roman Solomatin,Niklas Muennighoff,Kenneth Enevoldsen*

Main category: cs.CL

TL;DR: 提出了HUME框架来测量文本嵌入任务中的人类表现，填补了模型评估中缺乏人类基准的空白，发现在16个MTEB数据集上人类平均表现77.6%，略低于最佳模型的80.1%。


<details>
  <summary>Details</summary>
Motivation: 现有嵌入模型评估框架缺乏可靠的人类表现估计，限制了模型得分的可解释性，需要建立人类基准来更好地理解模型的能力和局限。

Method: 开发HUME框架，在16个MTEB数据集上测量人类表现，涵盖重排序、分类、聚类和语义文本相似性任务，涵盖高资源和低资源语言。

Result: 人类平均表现77.6%，最佳模型80.1%；模型在某些数据集上接近天花板表现，但在其他数据集上表现不佳，特别是在低资源语言中。

Conclusion: 提供了人类表现基准、任务难度模式洞察和可扩展评估框架，有助于更有效地解释模型性能并指导模型和基准测试的开发。

Abstract: Comparing human and model performance offers a valuable perspective for
understanding the strengths and limitations of embedding models, highlighting
where they succeed and where they fail to capture meaning and nuance. However,
such comparisons are rarely made, as human performance on embedding tasks is
difficult to measure. To fill this gap, we introduce HUME: Human Evaluation
Framework for Text Embeddings. While frameworks like MTEB provide broad model
evaluation, they lack reliable estimates of human performance, limiting the
interpretability of model scores. We measure human performance across 16 MTEB
datasets spanning reranking, classification, clustering, and semantic textual
similarity across linguistically diverse high- and low-resource languages.
Humans achieve an average performance of 77.6% compared to 80.1% for the best
embedding model, although variation is substantial: models reach near-ceiling
performance on some datasets while struggling on others, suggesting dataset
issues and revealing shortcomings in low-resource languages. We provide human
performance baselines, insight into task difficulty patterns, and an extensible
evaluation framework that enables a more meaningful interpretation of the model
and informs the development of both models and benchmarks. Our code, dataset,
and leaderboard are publicly available at
https://github.com/embeddings-benchmark/mteb.

</details>


### [34] [CLMN: Concept based Language Models via Neural Symbolic Reasoning](https://arxiv.org/abs/2510.10063)
*Yibo Yang*

Main category: cs.CL

TL;DR: CLMN是一个神经符号框架，通过连续可读的概念嵌入和模糊逻辑推理，在保持性能的同时提升NLP系统的可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决NLP中可解释性不足的问题，特别是在医疗和金融领域，现有概念瓶颈模型要么损害文本表示，要么弱化语义，且很少建模动态概念交互。

Method: 使用连续人类可读的概念嵌入，应用模糊逻辑推理学习自适应交互规则，将概念感知表示与原始文本特征结合，自动推导可解释逻辑规则。

Result: 在多个数据集和预训练语言模型上，CLMN比现有基于概念的方法获得更高准确率，同时提升解释质量。

Conclusion: 在统一概念空间中整合神经表示与符号推理可以产生实用、透明的NLP系统。

Abstract: Deep learning has advanced NLP, but interpretability remains limited,
especially in healthcare and finance. Concept bottleneck models tie predictions
to human concepts in vision, but NLP versions either use binary activations
that harm text representations or latent concepts that weaken semantics, and
they rarely model dynamic concept interactions such as negation and context. We
introduce the Concept Language Model Network (CLMN), a neural-symbolic
framework that keeps both performance and interpretability. CLMN represents
concepts as continuous, human-readable embeddings and applies fuzzy-logic
reasoning to learn adaptive interaction rules that state how concepts affect
each other and the final decision. The model augments original text features
with concept-aware representations and automatically induces interpretable
logic rules. Across multiple datasets and pre-trained language models, CLMN
achieves higher accuracy than existing concept-based methods while improving
explanation quality. These results show that integrating neural representations
with symbolic reasoning in a unified concept space can yield practical,
transparent NLP systems.

</details>


### [35] [Unilaw-R1: A Large Language Model for Legal Reasoning with Reinforcement Learning and Iterative Inference](https://arxiv.org/abs/2510.10072)
*Hua Cai,Shuang Zhao,Liang Zhang,Xuli Shen,Qing Xu,Weilin Shen,Zihao Wen,Tianke Ban*

Main category: cs.CL

TL;DR: Unilaw-R1是一个专为法律推理设计的7B参数大语言模型，通过两阶段训练策略显著提升复杂法律推理能力，在权威基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有推理型LLM在法律领域的复杂问题处理能力研究不足，需要解决法律知识不足、推理逻辑不可靠和业务泛化能力弱三大核心挑战。

Method: 构建高质量法律推理数据集Unilaw-R1-Data（17K CoT样本），采用监督微调（SFT）和强化学习（RL）的两阶段训练策略。

Result: Unilaw-R1在权威基准测试中超越所有同规模模型，与更大的32B模型性能相当（54.9%），在LawBench和LexEval上平均超过Qwen-2.5-7B-Instruct 6.6%。

Conclusion: Unilaw-R1证明了轻量级模型通过专门的数据集和训练策略，能够在法律推理任务中取得显著成效，为法律AI应用提供可解释的决策支持。

Abstract: Reasoning-focused large language models (LLMs) are rapidly evolving across
various domains, yet their capabilities in handling complex legal problems
remains underexplored. In this paper, we introduce Unilaw-R1, a large language
model tailored for legal reasoning. With a lightweight 7-billion parameter
scale, Unilaw-R1 significantly reduces deployment cost while effectively
tackling three core challenges in the legal domain: insufficient legal
knowledge, unreliable reasoning logic, and weak business generalization. To
address these issues, we first construct Unilaw-R1-Data, a high-quality dataset
containing 17K distilled and screened chain-of-thought (CoT) samples. Based on
this, we adopt a two-stage training strategy combining Supervised Fine-Tuning
(SFT) and Reinforcement Learning (RL), which significantly boosts the
performance on complex legal reasoning tasks and supports interpretable
decision-making in legal AI applications. To assess legal reasoning ability, we
also introduce Unilaw-R1-Eval, a dedicated benchmark designed to evaluate
models across single- and multi-choice legal tasks. Unilaw-R1 demonstrates
strong results on authoritative benchmarks, outperforming all models of similar
scale and achieving performance on par with the much larger
DeepSeek-R1-Distill-Qwen-32B (54.9%). Following domain-specific training, it
also showed significant gains on LawBench and LexEval, exceeding
Qwen-2.5-7B-Instruct (46.6%) by an average margin of 6.6%.

</details>


### [36] [A-IPO: Adaptive Intent-driven Preference Optimization](https://arxiv.org/abs/2510.10077)
*Wenqing Wang,Muhammad Asif Ali,Ali Shoker,Ruohan Yang,Junyang Chen,Ying Sha,Huan Wang*

Main category: cs.CL

TL;DR: A-IPO是一种自适应意图驱动的偏好优化方法，通过推断用户提示背后的潜在意图并将其纳入奖励函数，解决了现有对齐方法忽视少数意见和潜在用户意图的问题。


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法如DPO往往默认多数观点，忽视了少数意见，且无法捕捉提示中的潜在用户意图。人类偏好具有多样性和动态性，受地区、文化和社会因素影响。

Method: A-IPO引入意图模块推断每个用户提示的潜在意图，并将推断的意图明确纳入奖励函数。该方法在理论上和经验上都证明，加入意图-响应相似性项可以增加偏好边界，实现比DPO更清晰的首选和非首选响应分离。

Result: A-IPO在多个基准测试中显著优于现有基线：在Real-pref上胜率提升达+24.8，响应意图一致性提升+45.6；在Attack-pref上响应相似性提升+38.6，防御成功率提升+52.2；在GlobalOpinionQA-Ext上意图一致性得分提升+54.6。

Conclusion: 通过明确建模多样用户意图，A-IPO促进了多元偏好优化，同时增强了偏好对齐的对抗鲁棒性，在真实世界和对抗性偏好对齐评估中表现优异。

Abstract: Human preferences are diverse and dynamic, shaped by regional, cultural, and
social factors. Existing alignment methods like Direct Preference Optimization
(DPO) and its variants often default to majority views, overlooking minority
opinions and failing to capture latent user intentions in prompts.
  To address these limitations, we introduce \underline{\textbf{A}}daptive
\textbf{\underline{I}}ntent-driven \textbf{\underline{P}}reference
\textbf{\underline{O}}ptimization (\textbf{A-IPO}). Specifically,A-IPO
introduces an intention module that infers the latent intent behind each user
prompt and explicitly incorporates this inferred intent into the reward
function, encouraging stronger alignment between the preferred model's
responses and the user's underlying intentions. We demonstrate, both
theoretically and empirically, that incorporating an intention--response
similarity term increases the preference margin (by a positive shift of
$\lambda\,\Delta\mathrm{sim}$ in the log-odds), resulting in clearer separation
between preferred and dispreferred responses compared to DPO.
  For evaluation, we introduce two new benchmarks, Real-pref, Attack-pref along
with an extended version of an existing dataset, GlobalOpinionQA-Ext, to assess
real-world and adversarial preference alignment.
  Through explicit modeling of diverse user intents,A-IPO facilitates
pluralistic preference optimization while simultaneously enhancing adversarial
robustness in preference alignment. Comprehensive empirical evaluation
demonstrates that A-IPO consistently surpasses existing baselines, yielding
substantial improvements across key metrics: up to +24.8 win-rate and +45.6
Response-Intention Consistency on Real-pref; up to +38.6 Response Similarity
and +52.2 Defense Success Rate on Attack-pref; and up to +54.6 Intention
Consistency Score on GlobalOpinionQA-Ext.

</details>


### [37] [Diversity Augmentation of Dynamic User Preference Data for Boosting Personalized Text Summarizers](https://arxiv.org/abs/2510.10082)
*Parthiv Chatterjee,Shivam Sonawane,Amey Hengle,Aditya Tanna,Sourish Dasgupta,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: 提出了PerAugy数据增强技术，通过跨轨迹重排和摘要内容扰动来提升个性化摘要模型的性能，显著提高了用户编码器的准确性和个性化摘要质量。


<details>
  <summary>Details</summary>
Motivation: 个性化摘要面临训练数据稀缺的挑战，现有数据集如MS/CAS PENS缺乏目标摘要且主题转换多样性有限，无法支持端到端的监督学习。

Method: 开发了PerAugy数据增强技术，包含跨轨迹重排和摘要内容扰动，并引入了三个数据集多样性指标(TP、RTC、DegreeD)来量化增强效果。

Result: PerAugy显著提升了四种最先进用户编码器的准确性(最佳结果：AUC提升0.132)，并使两个SOTA摘要框架的个性化程度平均提升61.2%。

Conclusion: 数据集多样性的增加是性能提升的关键因素，TP和DegreeD指标与用户编码器性能在所有准确度指标上都表现出强相关性。

Abstract: Document summarization enables efficient extraction of user-relevant content
but is inherently shaped by individual subjectivity, making it challenging to
identify subjective salient information in multifaceted documents. This
complexity underscores the necessity for personalized summarization. However,
training models for personalized summarization has so far been challenging,
particularly because diverse training data containing both user preference
history (i.e., click-skip trajectory) and expected (gold-reference) summaries
are scarce. The MS/CAS PENS dataset is a valuable resource but includes only
preference history without target summaries, preventing end-to-end supervised
learning, and its limited topic-transition diversity further restricts
generalization. To address this, we propose $\mathrm{PerAugy}$, a novel
cross-trajectory shuffling and summary-content perturbation based data
augmentation technique that significantly boosts the accuracy of four
state-of-the-art baseline (SOTA) user-encoders commonly used in personalized
summarization frameworks (best result: $\text{0.132}$$\uparrow$ w.r.t AUC). We
select two such SOTA summarizer frameworks as baselines and observe that when
augmented with their corresponding improved user-encoders, they consistently
show an increase in personalization (avg. boost: $\text{61.2\%}\uparrow$ w.r.t.
PSE-SU4 metric). As a post-hoc analysis of the role of induced diversity in the
augmented dataset by \peraugy, we introduce three dataset diversity metrics --
$\mathrm{TP}$, $\mathrm{RTC}$, and \degreed\ to quantify the induced diversity.
We find that $\mathrm{TP}$ and $\mathrm{DegreeD}$ strongly correlate with
user-encoder performance on the PerAugy-generated dataset across all accuracy
metrics, indicating that increased dataset diversity is a key factor driving
performance gains.

</details>


### [38] [Stop When Enough: Adaptive Early-Stopping for Chain-of-Thought Reasoning](https://arxiv.org/abs/2510.10103)
*Renliang Sun,Wei Cheng,Dawei Li,Haifeng Chen,Wei Wang*

Main category: cs.CL

TL;DR: REFRAIN是一个无需训练的自适应推理停止框架，通过识别冗余推理和动态调整停止阈值，在保持准确性的同时减少20-55%的token使用量。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在推理过程中过度思考（overthinking）的问题，过度思考会增加推理成本并导致错误结论。

Method: 集成两阶段停止判别器来识别反射性但冗余的推理，并使用滑动窗口上置信界多臂老虎机控制器动态调整停止阈值。

Result: 在四个代表性基准测试和两个模型家族中，REFRAIN在保持或提高准确性的同时，将token使用量减少了20-55%。

Conclusion: 研究强调了"何时停止"作为测试时扩展的新实用维度，使模型能够推理得恰到好处而非过度推理。

Abstract: Chain-of-Thought (CoT) reasoning has driven recent gains of large language
models (LLMs) on reasoning-intensive tasks by externalizing intermediate steps.
However, excessive or redundant reasoning -- so-called overthinking -- can
increase inference costs and lead LLMs toward incorrect conclusions. In this
paper, we present REFRAIN ($\underline{REF}$lective-$\underline{R}$edundancy
for $\underline{A}$daptive $\underline{IN}$ference), a training-free framework
that adaptively determines when to stop reasoning to mitigate overthinking.
REFRAIN integrates a two-stage stop discriminator to identify reflective yet
redundant reasoning and a sliding-window Upper Confidence Bound (SW-UCB)
multi-armed bandit controller to dynamically adjust stopping thresholds
according to problem difficulty without supervision or fine-tuning. Across four
representative benchmarks and two model families, REFRAIN reduces token usage
by 20-55% while maintaining or improving accuracy compared to standard CoT
prompting. Extensive ablation and robustness analyses demonstrate its stability
across models, scorers, and prompt variations. In summary, our findings
highlight when-to-stop as a new and practical axis of test-time scaling --
enabling models to reason not just more, but just enough.

</details>


### [39] [LinearRAG: Linear Graph Retrieval Augmented Generation on Large-scale Corpora](https://arxiv.org/abs/2510.10114)
*Luyao Zhuang,Shengyuan Chen,Yilin Xiao,Huachi Zhou,Yujing Zhang,Hao Chen,Qinggang Zhang,Xiao Huang*

Main category: cs.CL

TL;DR: LinearRAG提出了一种高效的关系无关层次图构建方法，避免传统图RAG中不稳定且昂贵的关系抽取，实现了线性扩展的可靠图构建和精确检索。


<details>
  <summary>Details</summary>
Motivation: 传统RAG系统在处理大规模非结构化语料时效果不佳，而现有的图RAG方法依赖不稳定且成本高昂的关系抽取，导致图质量差、检索性能下降。

Method: 构建关系无关的层次图Tri-Graph，仅使用轻量级实体抽取和语义链接；采用两阶段检索策略：通过局部语义桥接激活相关实体，再通过全局重要性聚合进行段落检索。

Result: 在四个数据集上的广泛实验表明，LinearRAG显著优于基线模型。

Conclusion: LinearRAG提供了一种经济可靠的图构建和检索框架，解决了传统图RAG方法的可扩展性和稳定性问题。

Abstract: Retrieval-Augmented Generation (RAG) is widely used to mitigate
hallucinations of Large Language Models (LLMs) by leveraging external
knowledge. While effective for simple queries, traditional RAG systems struggle
with large-scale, unstructured corpora where information is fragmented. Recent
advances incorporate knowledge graphs to capture relational structures,
enabling more comprehensive retrieval for complex, multi-hop reasoning tasks.
However, existing graph-based RAG (GraphRAG) methods rely on unstable and
costly relation extraction for graph construction, often producing noisy graphs
with incorrect or inconsistent relations that degrade retrieval quality. In
this paper, we revisit the pipeline of existing GraphRAG systems and propose
LinearRAG (Linear Graph-based Retrieval-Augmented Generation), an efficient
framework that enables reliable graph construction and precise passage
retrieval. Specifically, LinearRAG constructs a relation-free hierarchical
graph, termed Tri-Graph, using only lightweight entity extraction and semantic
linking, avoiding unstable relation modeling. This new paradigm of graph
construction scales linearly with corpus size and incurs no extra token
consumption, providing an economical and reliable indexing of the original
passages. For retrieval, LinearRAG adopts a two-stage strategy: (i) relevant
entity activation via local semantic bridging, followed by (ii) passage
retrieval through global importance aggregation. Extensive experiments on four
datasets demonstrate that LinearRAG significantly outperforms baseline models.

</details>


### [40] [Hybrid OCR-LLM Framework for Enterprise-Scale Document Information Extraction Under Copy-heavy Task](https://arxiv.org/abs/2510.10138)
*Zilong Wang,Xiaoyu Shen*

Main category: cs.CL

TL;DR: 提出了一个结合OCR引擎和LLM的系统框架，用于优化复制密集型文档的信息提取，通过智能策略选择实现准确性和效率的最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 复制密集型文档（包含大量结构相似内容）的信息提取是企业文档处理中的关键但研究不足的挑战，现有方法追求通用解决方案而忽略了文档特定特征。

Method: 开发了系统框架，结合OCR引擎和LLM，实现三种提取范式（直接、替换和基于表格）的智能策略选择，在四种格式（PNG、DOCX、XLSX、PDF）的身份文档上进行评估。

Result: 基于表格的提取方法取得了优秀结果：结构化文档F1=1.0准确率，延迟0.97秒；图像输入F1=0.997准确率，延迟0.6秒（使用PaddleOCR），处理速度保持在亚秒级，相比朴素方法性能提升54倍。

Conclusion: 这项工作确立了一个通用原则：通过结构感知的方法选择，可以将复制密集型任务的重复性从计算负担转变为优化机会，适用于异构文档流的生产规模处理。

Abstract: Information extraction from copy-heavy documents, characterized by massive
volumes of structurally similar content, represents a critical yet understudied
challenge in enterprise document processing. We present a systematic framework
that strategically combines OCR engines with Large Language Models (LLMs) to
optimize the accuracy-efficiency trade-off inherent in repetitive document
extraction tasks. Unlike existing approaches that pursue universal solutions,
our method exploits document-specific characteristics through intelligent
strategy selection. We implement and evaluate 25 configurations across three
extraction paradigms (direct, replacement, and table-based) on identity
documents spanning four formats (PNG, DOCX, XLSX, PDF). Through table-based
extraction methods, our adaptive framework delivers outstanding results: F1=1.0
accuracy with 0.97s latency for structured documents, and F1=0.997 accuracy
with 0.6 s for challenging image inputs when integrated with PaddleOCR, all
while maintaining sub-second processing speeds. The 54 times performance
improvement compared with multimodal methods over naive approaches, coupled
with format-aware routing, enables processing of heterogeneous document streams
at production scale. Beyond the specific application to identity extraction,
this work establishes a general principle: the repetitive nature of copy-heavy
tasks can be transformed from a computational burden into an optimization
opportunity through structure-aware method selection.

</details>


### [41] [DiffHeads: Differential Analysis and Inference-Time Masking of Bias Heads in Large Language Models](https://arxiv.org/abs/2510.10142)
*Tingxu Han,Wei Song,Ziqi Ding,Ziming Li,Chunrong Fang,Yuekang Li,Dongfang Liu,Zhenyu Chen,Zhenting Wang*

Main category: cs.CL

TL;DR: DiffHeads是一个轻量级的LLM去偏框架，通过识别和屏蔽偏见注意力头来减少模型不公平性，在保持模型效用的同时将不公平性降低约40-50%。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅探测偏见输出，但缺乏对生成机制的深入理解，导致去偏方法脆弱。需要系统性研究LLM不公平性的机制。

Method: 1. 比较直接回答和思维链提示策略；2. 定义token-to-head贡献分数追踪偏见源头；3. 通过DA和CoT的差异激活分析识别偏见头；4. 选择性屏蔽偏见头。

Result: DA提示使不公平性增加534.5%-391.9%；发现偏见头集群在DA下激活但在CoT下休眠；DiffHeads在DA和CoT下分别减少不公平性49.4%和40.3%。

Conclusion: 揭示了提示策略与偏见出现之间的因果联系；DiffHeads提供了一种有效且轻量的去偏方法，在保持模型性能的同时显著减少不公平性。

Abstract: Large language models (LLMs) increasingly mediate decisions in domains where
unfair treatment of demographic groups is unacceptable. Existing work probes
when biased outputs appear, but gives little insight into the mechanisms that
generate them, leaving existing mitigations largely fragile. In this paper, we
conduct a systematic investigation LLM unfairness and propose DiffHeads, a
lightweight debiasing framework for LLMs. We first compare Direct-Answer (DA)
prompting to Chain-of-Thought (CoT) prompting across eight representative open-
and closed-source LLMs. DA will trigger the nature bias part of LLM and improve
measured unfairness by 534.5%-391.9% in both one-turn and two-turn dialogues.
Next, we define a token-to-head contribution score that traces each token's
influence back to individual attention heads. This reveals a small cluster of
bias heads that activate under DA but stay largely dormant with CoT, providing
the first causal link between prompting strategy and bias emergence. Finally,
building on this insight, we propose DiffHeads that identifies bias heads
through differential activation analysis between DA and CoT, and selectively
masks only those heads. DiffHeads reduces unfairness by 49.4%, and 40.3% under
DA and CoT, respectively, without harming model utility.

</details>


### [42] [BILLY: Steering Large Language Models via Merging Persona Vectors for Creative Generation](https://arxiv.org/abs/2510.10157)
*Tsung-Min Pai,Jui-I Wang,Li-Chun Lu,Shao-Hua Sun,Hung-Yi Lee,Kai-Wei Chang*

Main category: cs.CL

TL;DR: BILLY是一个无需训练的框架，通过在模型激活空间中提取和融合不同角色向量，在单个模型中实现多LLM协作的创意优势，显著降低计算成本和推理延迟。


<details>
  <summary>Details</summary>
Motivation: 多LLM系统虽然能增强语言模型的创造力，但存在计算成本高、推理延迟大的问题。需要一种方法能在单个模型中实现多LLM协作的优势。

Method: 在模型激活空间中提取多个不同的角色向量，并在推理时将这些向量融合来引导生成过程，实现多视角输出而无需显式的多LLM通信。

Result: 在面向创造力的基准测试中，BILLY超越了单模型提示和传统多LLM方法，同时大幅减少了推理时间和计算成本。

Conclusion: 不同角色向量的融合不仅能有效控制生成的互补方面，还能提高可解释性，为高效的多视角生成提供了可行方案。

Abstract: Multi-LLM systems enhance the creativity of large language models by
simulating human collective intelligence but suffer from significant drawbacks,
such as high computational costs and inference latency. To address these
limitations, we propose BILLY (BlendIng persona vectors for Large Language
model creativitY), a training-free framework that captures the benefits of
multi-LLM collaboration, i.e. inducing diverse perspectives and specialized
expertise, within a single model. BILLY operates by extracting and blending
multiple distinct persona vectors directly in the model's activation space. We
steer the model's generation process with this merged vector while inference,
enabling multi-perspective output without explicit multi-LLM communication. Our
experiments across creativity-oriented benchmarks demonstrate that BILLY
surpasses single model prompting and traditional multi-LLM approaches, while
substantially reducing inference time and computational costs. Our analyses
further reveal that distinct persona vectors can be blended to achieve both
effective control over complementary aspects of generation and greater
interpretability.

</details>


### [43] [BabyBabelLM: A Multilingual Benchmark of Developmentally Plausible Training Data](https://arxiv.org/abs/2510.10159)
*Jaap Jumelet,Abdellah Fourtassi,Akari Haga,Bastian Bunzeck,Bhargav Shandilya,Diana Galvan-Sosa,Faiz Ghifari Haznitrama,Francesca Padovani,Francois Meyer,Hai Hu,Julen Etxaniz,Laurent Prévot,Linyang He,María Grandury,Mila Marcheva,Negar Foroutan,Nikitas Theodoropoulos,Pouya Sadeghi,Siyuan Song,Suchir Salhan,Susana Zhou,Yurii Paniv,Ziyin Zhang,Arianna Bisazza,Alex Warstadt,Leshem Choshen*

Main category: cs.CL

TL;DR: BabyBabelLM是一个多语言数据集集合，模拟从出生到母语习得期间观察到的语言输入，包含45种语言各1亿英语单词量的发展适宜预训练数据。


<details>
  <summary>Details</summary>
Motivation: 为多语言预训练和认知建模提供发展适宜的语言数据，模拟人类语言习得的真实过程。

Method: 整理开发上合理的预训练数据，覆盖45种语言各1亿英语单词量的内容，并编译评估套件和训练基线模型。

Result: 创建了包含45种语言的多语言数据集集合，每个语言都配备了评估套件和基线模型。

Conclusion: BabyBabelLM旨在促进多语言预训练和认知建模研究，为语言习得研究提供重要资源。

Abstract: We present BabyBabelLM, a multilingual collection of datasets modeling the
language a person observes from birth until they acquire a native language. We
curate developmentally plausible pretraining data aiming to cover the
equivalent of 100M English words of content in each of 45 languages. We compile
evaluation suites and train baseline models in each language. BabyBabelLM aims
to facilitate multilingual pretraining and cognitive modeling.

</details>


### [44] [Large Language Model Sourcing: A Survey](https://arxiv.org/abs/2510.10161)
*Liang Pang,Kangxi Wu,Sunhao Dai,Zihao Wei,Zenghao Duan,Jia Gu,Xiang Li,Zhiyi Yin,Jun Xu,Huawei Shen,Xueqi Cheng*

Main category: cs.CL

TL;DR: 该调查系统研究了LLM生成内容的溯源追踪，围绕模型和数据两个视角的四个维度展开，提出了先验和后验两种溯源方法的分类体系。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在主观决策领域的广泛应用，其黑盒特性和类人内容生成能力带来了幻觉、偏见、不公平和版权侵权等多方面风险，因此需要多视角的信息溯源来增强透明度和可信度。

Method: 从模型视角（模型溯源、模型结构溯源）和数据视角（训练数据溯源、外部数据溯源）四个维度分析LLM内容生成机制，并提出先验式（主动嵌入可追溯性）和后验式（回溯推理）两种溯源方法分类。

Result: 构建了一个系统化的LLM内容溯源框架，通过多维度分析揭示了LLM生成内容的来源机制，为增强LLM部署的透明度、问责性和可信度提供了理论基础。

Conclusion: 跨维度的可追溯性能够显著提升LLM在实际应用中的透明度、问责性和可信度，为应对LLM部署风险提供了系统性解决方案。

Abstract: The rapid advancement of large language models (LLMs) has revolutionized
artificial intelligence, shifting from supporting objective tasks (e.g.,
recognition) to empowering subjective decision-making (e.g., planning,
decision). This marks the dawn of general and powerful AI, with applications
spanning a wide range of fields, including programming, education, healthcare,
finance, and law. However, their deployment introduces multifaceted risks. Due
to the black-box nature of LLMs and the human-like quality of their generated
content, issues such as hallucinations, bias, unfairness, and copyright
infringement become particularly significant. In this context, sourcing
information from multiple perspectives is essential.
  This survey presents a systematic investigation into provenance tracking for
content generated by LLMs, organized around four interrelated dimensions that
together capture both model- and data-centric perspectives. From the model
perspective, Model Sourcing treats the model as a whole, aiming to distinguish
content generated by specific LLMs from content authored by humans. Model
Structure Sourcing delves into the internal generative mechanisms, analyzing
architectural components that shape the outputs of model. From the data
perspective, Training Data Sourcing focuses on internal attribution, tracing
the origins of generated content back to the training data of model. In
contrast, External Data Sourcing emphasizes external validation, identifying
external information used to support or influence the responses of model.
Moreover, we also propose a dual-paradigm taxonomy that classifies existing
sourcing methods into prior-based (proactive traceability embedding) and
posterior-based (retrospective inference) approaches. Traceability across these
dimensions enhances the transparency, accountability, and trustworthiness of
LLMs deployment in real-world applications.

</details>


### [45] [A Survey of Inductive Reasoning for Large Language Models](https://arxiv.org/abs/2510.10182)
*Kedi Chen,Dezhao Ruan,Yuhao Dan,Yaoting Wang,Siyu Yan,Xuecheng Wu,Yinqi Zhang,Qin Chen,Jie Zhou,Liang He,Biqing Qi,Linyang Li,Qipeng Guo,Xiaoming Shi,Wei Zhang*

Main category: cs.CL

TL;DR: 本文首次对大型语言模型的归纳推理进行全面综述，将改进方法分为后训练、测试时扩展和数据增强三类，总结了当前基准测试，并提出了基于沙盒的统一评估方法。


<details>
  <summary>Details</summary>
Motivation: 归纳推理作为重要的推理范式，具有从特殊到一般的思维过程和非唯一答案的特点，对知识泛化和人类认知至关重要，但目前缺乏系统性总结。

Method: 将归纳推理改进方法分为三类：后训练、测试时扩展和数据增强；总结当前基准测试；提出基于沙盒的统一评估方法和观察覆盖率指标。

Result: 建立了归纳推理的系统性分类框架和评估方法，为未来研究提供了坚实基础。

Conclusion: 本文填补了归纳推理研究的空白，提供了方法论框架和评估标准，有助于推动该领域的发展。

Abstract: Reasoning is an important task for large language models (LLMs). Among all
the reasoning paradigms, inductive reasoning is one of the fundamental types,
which is characterized by its particular-to-general thinking process and the
non-uniqueness of its answers. The inductive mode is crucial for knowledge
generalization and aligns better with human cognition, so it is a fundamental
mode of learning, hence attracting increasing interest. Despite the importance
of inductive reasoning, there is no systematic summary of it. Therefore, this
paper presents the first comprehensive survey of inductive reasoning for LLMs.
First, methods for improving inductive reasoning are categorized into three
main areas: post-training, test-time scaling, and data augmentation. Then,
current benchmarks of inductive reasoning are summarized, and a unified
sandbox-based evaluation approach with the observation coverage metric is
derived. Finally, we offer some analyses regarding the source of inductive
ability and how simple model architectures and data help with inductive tasks,
providing a solid foundation for future research.

</details>


### [46] [MedAgentAudit: Diagnosing and Quantifying Collaborative Failure Modes in Medical Multi-Agent Systems](https://arxiv.org/abs/2510.10185)
*Lei Gu,Yinghao Zhu,Haoran Sang,Zixiang Wang,Dehao Sui,Wen Tang,Ewen Harrison,Junyi Gao,Lequan Yu,Liantao Ma*

Main category: cs.CL

TL;DR: 研究发现LLM多智能体医疗咨询系统存在严重推理过程问题，仅关注最终答案准确性不足以保证临床可信度


<details>
  <summary>Details</summary>
Motivation: 当前LLM多智能体医疗系统评估局限于最终答案准确性，忽视了推理过程的透明性和可验证性，这在高风险医疗应用中存在重大风险

Method: 对6个医疗数据集的3600个案例和6个代表性多智能体框架进行大规模实证研究，采用定性分析和定量审计相结合的混合方法

Result: 识别出四种主要失败模式：共享模型缺陷驱动的错误共识、正确少数意见被压制、无效讨论动态、关键信息在合成过程中丢失

Conclusion: 仅靠高准确率不足以建立临床或公众信任，迫切需要透明可审计的推理过程作为医疗AI负责任发展的基石

Abstract: While large language model (LLM)-based multi-agent systems show promise in
simulating medical consultations, their evaluation is often confined to
final-answer accuracy. This practice treats their internal collaborative
processes as opaque "black boxes" and overlooks a critical question: is a
diagnostic conclusion reached through a sound and verifiable reasoning pathway?
The inscrutable nature of these systems poses a significant risk in high-stakes
medical applications, potentially leading to flawed or untrustworthy
conclusions. To address this, we conduct a large-scale empirical study of 3,600
cases from six medical datasets and six representative multi-agent frameworks.
Through a rigorous, mixed-methods approach combining qualitative analysis with
quantitative auditing, we develop a comprehensive taxonomy of collaborative
failure modes. Our quantitative audit reveals four dominant failure patterns:
flawed consensus driven by shared model deficiencies, suppression of correct
minority opinions, ineffective discussion dynamics, and critical information
loss during synthesis. This study demonstrates that high accuracy alone is an
insufficient measure of clinical or public trust. It highlights the urgent need
for transparent and auditable reasoning processes, a cornerstone for the
responsible development and deployment of medical AI.

</details>


### [47] [Weed Out, Then Harvest: Dual Low-Rank Adaptation is an Effective Noisy Label Detector for Noise-Robust Learning](https://arxiv.org/abs/2510.10208)
*Bo Yuan,Yulin Chen,Yin Zhang*

Main category: cs.CL

TL;DR: Delora是一个新颖的PEFT框架，通过解耦样本选择和模型训练来解决噪声标签问题，使用清洁和噪声LoRA作为可学习的阈值来检测噪声标签。


<details>
  <summary>Details</summary>
Motivation: 现实场景中训练数据不可避免地包含噪声标签，传统方法通过小损失选择样本但会形成恶性循环，导致性能不佳。

Method: 引入清洁LoRA和噪声LoRA，利用记忆效应分别记忆清洁数据和噪声数据，作为可学习阈值来选择样本，然后使用选定的样本进行模型训练。

Result: 在合成和真实噪声数据集上的实验结果表明，Delora在噪声标签检测和文本分类方面具有有效性。

Conclusion: Delora通过解耦样本选择和模型训练，成功解决了噪声标签学习中的恶性循环问题，提高了模型性能。

Abstract: Parameter-efficient fine-tuning (PEFT) large language models (LLMs) have
shown impressive performance in various downstream tasks. However, in many
real-world scenarios, the collected training data inevitably contains noisy
labels. To learn from noisy labels, most solutions select samples with small
losses for model training. However, the selected samples, in turn, impact the
loss computation in the next iteration. An inaccurate initial selection can
create a vicious cycle, leading to suboptimal performance. To break this cycle,
we propose Delora, a novel framework that decouples the sample selection from
model training. For sample selection, Delora establishes a noisy label detector
by introducing clean and noisy LoRA. Benefiting from the memory effect, the
clean LoRA is encouraged to memorize clean data, while the noisy LoRA is
constrained to memorize mislabeled data, which serves as a learnable threshold
for selecting clean and noisy samples. For model training, Delora can use
carefully selected samples to fine-tune language models seamlessly.
Experimental results on synthetic and real-world noisy datasets demonstrate the
effectiveness of Delora in noisy label detection and text classification.

</details>


### [48] [You only need 4 extra tokens: Synergistic Test-time Adaptation for LLMs](https://arxiv.org/abs/2510.10223)
*Yijie Xu,Huizai Yao,Zhiyu Guo,Weiyu Guo,Pengteng Li,Aiwei Liu,Xuming Hu,Hui Xiong*

Main category: cs.CL

TL;DR: SyTTA是一个无需标注数据的推理时自适应框架，通过结合输入侧困惑度和输出侧预测熵来应对语言模型在专业领域的分布偏移问题。


<details>
  <summary>Details</summary>
Motivation: 解决语言模型在专业领域（如金融、医疗、农业）部署时面临的分布偏移问题，这些领域标注数据昂贵且难以获取。

Method: SyTTA框架耦合两种不确定性信号：输入侧困惑度（反映领域特定术语和模式的不匹配）和输出侧预测熵（反映生成过程中token概率的分散和不稳定）。

Result: 在多样化模型架构和领域特定基准测试中，SyTTA带来了一致的性能提升。在农业问答任务上，Qwen-2.5-7B模型的Rouge-LSum指标提升了120%以上，每个查询仅需4个额外token。

Conclusion: 无需标注示例即可实现有效的语言模型测试时自适应，支持在标注稀缺领域的部署。

Abstract: Large language models (LLMs) are increasingly deployed in specialized domains
such as finance, medicine, and agriculture, where they face significant
distribution shifts from their training data. Domain-specific fine-tuning can
mitigate this challenge but relies on high-quality labeled data that is
expensive and slow to collect in expertise-limited settings. We study
label-free test-time adaptation for language models and present SyTTA, an
inference-time framework that adapts models on-the-fly without additional
supervision. SyTTA couples two complementary uncertainty signals that arise
under distribution shift: input-side perplexity, indicating mismatch with
domain-specific terminology and patterns, and output-side predictive entropy,
indicating diffuse and unstable token probabilities during generation. Across
diverse model architectures and domain-specific benchmarks, SyTTA delivers
consistent gains. Notably, on agricultural question answering, SyTTA improves
Rouge-LSum by over 120% on Qwen-2.5-7B with only 4 extra tokens per query.
These results show that effective test-time adaptation for language models is
achievable without labeled examples, supporting deployment in label-scarce
domains. The code will be made available upon acceptance.

</details>


### [49] [Text2Token: Unsupervised Text Representation Learning with Token Target Prediction](https://arxiv.org/abs/2510.10224)
*Ruize An,Richong Zhang,Zhijie Nie,Zhanyu Wu,Yanzhao Zhang,Dingkun Long*

Main category: cs.CL

TL;DR: 提出Text2Token框架，通过生成式任务进行无监督文本表示学习，利用精心构建的目标token分布作为监督信号，在MTEB v2基准上达到与最先进无监督对比学习方法LLM2Vec竞争的性能。


<details>
  <summary>Details</summary>
Motivation: 受研究发现高质量表示与输入文本关键token对齐的启发，探索表示空间与词汇空间之间的潜在联系，重新审视生成式任务在文本表示学习中的应用。

Method: 基于token目标预测任务，通过数据驱动和模型推导两种方法构建高质量目标token分布：识别文本中有意义的token和文本之外的语义衍生token。

Result: 在MTEB v2基准测试中，Text2Token与最先进的无监督对比学习方法LLM2Vec性能相当，同时分析显示词汇空间和表示空间在训练过程中共同优化并趋向最优解。

Conclusion: Text2Token为无监督文本表示学习提供了新的思路和见解，证明了生成式框架的有效性，并为未来工作开辟了新的研究方向。

Abstract: Unsupervised text representation learning (TRL) is a fundamental task in
natural language processing, which is beneficial for improving search and
recommendations with the web's unlabeled texts. A recent empirical study finds
that the high-quality representation aligns with the key token of the input
text, uncovering the potential connection between representation space and
vocabulary space. Inspired by the findings, we revisit the generative tasks and
develop an unsupervised generative framework for TRL, Text2Token. The framework
is based on the token target prediction task, utilizing carefully constructed
target token distribution as supervisory signals. To construct the high-quality
target token distribution, we analyze the token-alignment properties with
advanced embedders and identify two essential categories of key tokens: (1) the
meaningful tokens in the text and (2) semantically derived tokens beyond the
text. Based on these insights, we propose two methods -- data-driven and
model-derived -- to construct synthetic token targets from data or the LLM
backbone. Experiments on the MTEB v2 benchmark demonstrate that Text2Token
achieves performance competitive with the state-of-the-art embedder with
unsupervised contrastive learning, LLM2Vec. Our analysis further shows that
vocabulary and representation spaces optimize together and toward the optimum
solution during training, providing new ideas and insights for future work.

</details>


### [50] [ImCoref-CeS: An Improved Lightweight Pipeline for Coreference Resolution with LLM-based Checker-Splitter Refinement](https://arxiv.org/abs/2510.10241)
*Kangyang Luo,Yuzhuo Bai,Shuzheng Si,Cheng Gao,Zhitong Wang,Yingli Shen,Wenhao Li,Zhu Liu,Yufeng Han,Jiayi Wu,Cunliang Kong,Maosong Sun*

Main category: cs.CL

TL;DR: 提出ImCoref-CeS框架，将增强的监督模型与LLM推理相结合，在指代消解任务中实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 解决指代消解中的关键困境：是继续探索基于小语言模型的监督神经方法，还是拥抱LLM的强大能力。有效结合两者优势仍是未充分探索的领域。

Method: 1) 改进的监督方法ImCoref：引入轻量级桥接模块增强长文本编码，设计双仿射评分器全面捕捉位置信息，采用混合提及正则化提高训练效率；2) LLM作为多角色检查器-分割器代理：验证候选提及和指代结果，过滤无效提及并分割错误聚类。

Result: 广泛实验证明ImCoref-CeS的有效性，相比现有SOTA方法实现了更优性能。

Conclusion: ImCoref-CeS成功结合了监督神经方法和LLM推理的优势，在指代消解任务中取得了突破性进展。

Abstract: Coreference Resolution (CR) is a critical task in Natural Language Processing
(NLP). Current research faces a key dilemma: whether to further explore the
potential of supervised neural methods based on small language models, whose
detect-then-cluster pipeline still delivers top performance, or embrace the
powerful capabilities of Large Language Models (LLMs). However, effectively
combining their strengths remains underexplored. To this end, we propose
\textbf{ImCoref-CeS}, a novel framework that integrates an enhanced supervised
model with LLM-based reasoning. First, we present an improved CR method
(\textbf{ImCoref}) to push the performance boundaries of the supervised neural
method by introducing a lightweight bridging module to enhance long-text
encoding capability, devising a biaffine scorer to comprehensively capture
positional information, and invoking a hybrid mention regularization to improve
training efficiency. Importantly, we employ an LLM acting as a multi-role
Checker-Splitter agent to validate candidate mentions (filtering out invalid
ones) and coreference results (splitting erroneous clusters) predicted by
ImCoref. Extensive experiments demonstrate the effectiveness of ImCoref-CeS,
which achieves superior performance compared to existing state-of-the-art
(SOTA) methods.

</details>


### [51] [Audit-of-Understanding: Posterior-Constrained Inference for Mathematical Reasoning in Language Models](https://arxiv.org/abs/2510.10252)
*Samir Abdaljalil,Erchin Serpedin,Khalid Qaraqe,Hasan Kurban*

Main category: cs.CL

TL;DR: 提出了审计理解框架，通过验证前提条件来约束推理过程，减少语言模型因不合理假设产生的幻觉问题


<details>
  <summary>Details</summary>
Motivation: 大型语言模型经常生成看似连贯但基于未经证实的假设的推理轨迹，导致产生幻觉结论。现有工作主要处理事实性幻觉或依赖事后验证，对推理引起的幻觉关注不足

Method: 审计理解框架包含三个阶段：(1)将查询分解为候选假设，(2)审计这些假设的支持度，(3)仅在已验证子集上进行推理。该方法形式化为后验约束推理

Result: 在GSM8K、MultiArith和SVAMP数据集上显著提升了准确性和忠实度，相比Chain-of-Thought、Self-Consistency和CoT-Decoding方法，在GSM8K上提升高达30%，在MultiArith上提升45%，在SVAMP上持续提升20-28%

Conclusion: 审计理解框架通过约束推理于已验证前提，有效减少了语言模型的推理幻觉问题，在理论和实证层面都表现出优越性能

Abstract: Large language models (LLMs) often generate reasoning traces that appear
coherent but rest on unsupported assumptions, leading to hallucinated
conclusions. Prior work mainly addresses factual hallucinations or relies on
post-hoc verification, leaving reasoning-induced hallucinations largely
unaddressed. We propose Audit-of-Understanding (AoU), a framework that
constrains inference to validated premises through three phases: (1)
decomposing a query into candidate assumptions, (2) auditing their support, and
(3) conditioning inference only on the validated subset. Formally, AoU is
\emph{posterior-constrained inference}, connecting to selective prediction and
rejection learning. Our contributions are threefold: (i) theoretical guarantees
under perfect validation, (ii) excess-risk bounds under imperfect audits, and
(iii) tractability analysis. Empirically, AoU improves both accuracy and
faithfulness on GSM8K, MultiArith, and SVAMP, achieving up to +30% gains on
GSM8K, +45% on MultiArith, and consistent +20--28% improvements on SVAMP over
Chain-of-Thought, Self-Consistency, and CoT-Decoding. Code is available at
https://anonymous.4open.science/r/audit-of-understanding-E28B.

</details>


### [52] [Backdoor Collapse: Eliminating Unknown Threats via Known Backdoor Aggregation in Language Models](https://arxiv.org/abs/2510.10265)
*Liang Lin,Miao Yu,Moayad Aloqaily,Zhenhong Zhou,Kun Wang,Linsey Pang,Prakhar Mehrotra,Qingsong Wen*

Main category: cs.CL

TL;DR: 提出了一种无需预知触发器设置的后门攻击防御框架，通过注入已知后门聚合表征空间中的后门，然后进行恢复微调来恢复良性输出。


<details>
  <summary>Details</summary>
Motivation: 后门攻击对大型语言模型构成严重威胁，现有防御方法依赖不切实际的触发器设置假设，需要一种无需先验知识的防御方案。

Method: 采用两阶段过程：首先通过注入已知触发器聚合后门表征，然后进行恢复微调以恢复良性输出。

Result: 在多个LLM架构上的实验表明：平均攻击成功率降至4.41%，优于现有基线28.1%~69.3%；清洁准确率和实用性保持在原始模型的0.5%以内；防御方法对不同类型后门具有泛化性。

Conclusion: 该防御框架在无需触发器先验知识的情况下有效防御后门攻击，保持模型实用性，具有实际部署的鲁棒性。

Abstract: Backdoor attacks are a significant threat to large language models (LLMs),
often embedded via public checkpoints, yet existing defenses rely on
impractical assumptions about trigger settings. To address this challenge, we
propose \ourmethod, a defense framework that requires no prior knowledge of
trigger settings. \ourmethod is based on the key observation that when
deliberately injecting known backdoors into an already-compromised model, both
existing unknown and newly injected backdoors aggregate in the representation
space. \ourmethod leverages this through a two-stage process: \textbf{first},
aggregating backdoor representations by injecting known triggers, and
\textbf{then}, performing recovery fine-tuning to restore benign outputs.
Extensive experiments across multiple LLM architectures demonstrate that: (I)
\ourmethod reduces the average Attack Success Rate to 4.41\% across multiple
benchmarks, outperforming existing baselines by 28.1\%$\sim$69.3\%$\uparrow$.
(II) Clean accuracy and utility are preserved within 0.5\% of the original
model, ensuring negligible impact on legitimate tasks. (III) The defense
generalizes across different types of backdoors, confirming its robustness in
practical deployment scenarios.

</details>


### [53] [On the Entity-Level Alignment in Crosslingual Consistency](https://arxiv.org/abs/2510.10280)
*Yihong Liu,Mingyang Wang,François Yvon,Hinrich Schütze*

Main category: cs.CL

TL;DR: 该论文研究了多语言大语言模型在跨语言事实知识回忆中的不一致性问题，发现实体对齐失败是主要原因，并提出了两种通过整合英文实体翻译来改善对齐的方法。


<details>
  <summary>Details</summary>
Motivation: 多语言大语言模型在跨语言事实知识回忆中存在不一致性，但导致这种不一致性的因素尚不清楚。作者假设这可能源于实体对齐的失败。

Method: 通过实体级翻译任务评估对齐，并提出了SubSub和SubInj两种方法，将英文实体翻译整合到多语言提示中。

Result: 研究发现一致性与对齐强相关，提出的方法显著提高了事实回忆准确性和一致性，并揭示了这些干预通过模型内部枢纽语言处理强化了实体表示对齐。

Conclusion: 实体对齐是多语言事实预测一致性的关键因素，提出的方法提供了改进多语言事实预测的有效实用策略。

Abstract: Multilingual large language models (LLMs) are expected to recall factual
knowledge consistently across languages. However, the factors that give rise to
such crosslingual consistency -- and its frequent failure -- remain poorly
understood. In this work, we hypothesize that these inconsistencies may arise
from failures in entity alignment, the process of mapping subject and object
entities into a shared conceptual space across languages. To test this, we
assess alignment through entity-level (subject and object) translation tasks,
and find that consistency is strongly correlated with alignment across all
studied models, with misalignment of subjects or objects frequently resulting
in inconsistencies. Building on this insight, we propose SubSub and SubInj, two
effective methods that integrate English translations of subjects into prompts
across languages, leading to substantial gains in both factual recall accuracy
and consistency. Finally, our mechanistic analysis reveals that these
interventions reinforce the entity representation alignment in the conceptual
space through model's internal pivot-language processing, offering effective
and practical strategies for improving multilingual factual prediction.

</details>


### [54] [MatryoshkaThinking: Recursive Test-Time Scaling Enables Efficient Reasoning](https://arxiv.org/abs/2510.10293)
*Hongwei Chen,Yishu Lei,Dan Zhang,Bo Ke,Danxiang Zhu,Xuyi Chen,Yuxiang Lu,Zhengjie Huang,Shikun Feng,Jingzhou He,Yu Sun,Hua Wu,Haifeng Wang*

Main category: cs.CL

TL;DR: 提出MatryoshkaThinking方法，在保持最先进性能的同时显著降低计算成本，仅需DeepConf 4%的计算量就能达到99.79的AIME2025分数。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时缩放方法如DeepConf虽然有效，但计算开销过大，需要更高效的解决方案。

Method: 递归利用模型内在的推理、验证和总结能力，增强正确解决方案的保留并减少Pass@k和Pass@1之间的差距。

Result: 在多个开源模型和具有挑战性的多模态推理基准测试中验证了方法的有效性和通用性。

Conclusion: 为高级语言模型的高效可扩展测试时推理策略设计提供了新见解。

Abstract: Test-time scaling has emerged as a promising paradigm in language modeling,
wherein additional computational resources are allocated during inference to
enhance model performance. Recent approaches, such as DeepConf, have
demonstrated the efficacy of this strategy, however, they often incur
substantial computational overhead to achieve competitive results. In this
work, we propose MatryoshkaThinking, a novel method that significantly reduces
computational cost while maintaining state-of-the-art performance.
Specifically, MatryoshkaThinking attains a score of 99.79 on AIME2025 using
only 4% of the computation required by DeepConf. The core of our approach lies
in the recursive exploitation of the model's intrinsic capabilities in
reasoning, verification, and summarization, which collectively enhance the
retention of correct solutions and reduce the disparity between Pass@k and
Pass@1. Comprehensive evaluations across multiple open-source models and
challenging multi-modal reasoning benchmarks validate the effectiveness and
generality of our method. These findings offer new insights into the design of
efficient and scalable test-time inference strategies for advanced language
models.

</details>


### [55] [Are LLMs Empathetic to All? Investigating the Influence of Multi-Demographic Personas on a Model's Empathy](https://arxiv.org/abs/2510.10328)
*Ananya Malik,Nazanin Sabri,Melissa Karnaze,Mai Elsherief*

Main category: cs.CL

TL;DR: 研究评估了大型语言模型在不同人口属性组合下的共情能力，发现人口特征显著影响模型的共情回应，多属性组合会改变预期共情模式。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs是否能在不同人口群体间展现公平的共情能力，因为情感体验受人口统计和文化背景影响。

Method: 提出框架分析LLMs的认知和情感共情在315个独特人口角色（年龄、文化、性别组合）中的变化，涵盖四个LLMs。

Result: 人口属性深刻塑造模型的共情回应，多属性组合会减弱和逆转预期共情模式，模型总体上反映现实世界共情趋势，但对某些群体（如儒家文化）存在明显偏差。

Conclusion: 需要设计考虑人口多样性的共情感知LLMs，以促进更具包容性和公平性的模型行为。

Abstract: Large Language Models' (LLMs) ability to converse naturally is empowered by
their ability to empathetically understand and respond to their users. However,
emotional experiences are shaped by demographic and cultural contexts. This
raises an important question: Can LLMs demonstrate equitable empathy across
diverse user groups? We propose a framework to investigate how LLMs' cognitive
and affective empathy vary across user personas defined by intersecting
demographic attributes. Our study introduces a novel intersectional analysis
spanning 315 unique personas, constructed from combinations of age, culture,
and gender, across four LLMs. Results show that attributes profoundly shape a
model's empathetic responses. Interestingly, we see that adding multiple
attributes at once can attenuate and reverse expected empathy patterns. We show
that they broadly reflect real-world empathetic trends, with notable
misalignments for certain groups, such as those from Confucian culture. We
complement our quantitative findings with qualitative insights to uncover model
behaviour patterns across different demographic groups. Our findings highlight
the importance of designing empathy-aware LLMs that account for demographic
diversity to promote more inclusive and equitable model behaviour.

</details>


### [56] [End-to-end Automatic Speech Recognition and Speech Translation: Integration of Speech Foundational Models and LLMs](https://arxiv.org/abs/2510.10329)
*Nam Luu,Ondřej Bojar*

Main category: cs.CL

TL;DR: 本文提出了一种结合预训练语音编码器和大型语言模型的端到端架构，用于同时执行自动语音识别和语音翻译任务。在英德语言对上的实验表明，该模型不仅优于SeamlessM4T，还能媲美级联系统性能。


<details>
  <summary>Details</summary>
Motivation: 探索端到端语音翻译架构，结合预训练语音编码器和LLMs，以同时处理ASR和ST任务，提升翻译性能。

Method: 采用预训练语音编码器与大型语言模型结合的端到端架构，同时执行自动语音识别和语音翻译。

Result: 在英德语言对实验中，最佳模型在COMET DA22指标上比SeamlessM4T提升高达8%，且性能与Whisper+NLLB级联系统相当。

Conclusion: 提出的端到端架构在语音翻译任务中表现出色，不仅超越了大型基础模型，还能达到级联系统的性能水平。

Abstract: Speech Translation (ST) is a machine translation task that involves
converting speech signals from one language to the corresponding text in
another language; this task has two different approaches, namely the
traditional cascade and the more recent end-to-end. This paper explores a
combined end-to-end architecture of pre-trained speech encoders and Large
Language Models (LLMs) for performing both Automatic Speech Recognition (ASR)
and ST simultaneously. Experiments with the English-to-German language pair
show that our best model not only can achieve better translation results than
SeamlessM4T, a large foundational end-to-end, multi-modal translation model,
but can also match the performance of a cascaded system with Whisper and NLLB,
with up to a score gain of 8% in $\text{COMET}^{\text{DA}}_{22}$ metric.

</details>


### [57] [ASC analyzer: A Python package for measuring argument structure construction usage in English texts](https://arxiv.org/abs/2510.10384)
*Hakyung Sung,Kristopher Kyle*

Main category: cs.CL

TL;DR: 本文介绍了一个用于分析二语写作中论元结构构式的Python工具包，能自动标注并计算50个指标，用于评估二语写作能力。


<details>
  <summary>Details</summary>
Motivation: 虽然论元结构构式（ASCs）为分析二语能力提供了理论基础，但缺乏可扩展和系统化的测量工具。

Method: 开发了ASC分析器Python包，自动标注ASCs并计算多样性、比例、频率和ASC-动词关联强度等50个指标。

Result: 通过双变量和多变量分析验证了ASC指标与二语写作分数的关系。

Conclusion: ASC分析器填补了二语能力评估工具的空缺，为系统化分析论元结构构式提供了实用工具。

Abstract: Argument structure constructions (ASCs) offer a theoretically grounded lens
for analyzing second language (L2) proficiency, yet scalable and systematic
tools for measuring their usage remain limited. This paper introduces the ASC
analyzer, a publicly available Python package designed to address this gap. The
analyzer automatically tags ASCs and computes 50 indices that capture
diversity, proportion, frequency, and ASC-verb lemma association strength. To
demonstrate its utility, we conduct both bivariate and multivariate analyses
that examine the relationship between ASC-based indices and L2 writing scores.

</details>


### [58] [RefusalBench: Generative Evaluation of Selective Refusal in Grounded Language Models](https://arxiv.org/abs/2510.10390)
*Aashiq Muhamed,Leonardo F. R. Ribeiro,Markus Dreyer,Virginia Smith,Mona T. Diab*

Main category: cs.CL

TL;DR: 该论文研究了RAG系统中语言模型基于有缺陷上下文选择性拒绝回答的能力，发现前沿模型在此方面表现不佳，拒绝准确率低于50%。作者提出了RefusalBench基准测试方法，通过语言扰动生成诊断测试案例，评估了30多个模型并发现了系统性失败模式。


<details>
  <summary>Details</summary>
Motivation: RAG系统中语言模型基于有缺陷上下文选择性拒绝回答的能力对安全性至关重要，但目前仍是显著的失败点，现有静态基准测试无法可靠评估这一能力。

Method: 引入RefusalBench生成方法，通过程序化创建诊断测试案例，采用176种扰动策略跨越六类信息不确定性和三个强度级别。

Result: 评估30多个模型发现系统性失败模式：拒绝能力包含可分离的检测和分类技能，规模和扩展推理均无法提升性能。选择性拒绝是可训练、对齐敏感的能力。

Conclusion: 选择性拒绝是可训练的、对齐敏感的能力，为改进提供了明确路径。发布了RefusalBench-NQ和RefusalBench-GaRAGe两个基准测试及完整生成框架。

Abstract: The ability of language models in RAG systems to selectively refuse to answer
based on flawed context is critical for safety, yet remains a significant
failure point. Our large-scale study reveals that even frontier models struggle
in this setting, with refusal accuracy dropping below 50% on multi-document
tasks, while exhibiting either dangerous overconfidence or overcaution. Static
benchmarks fail to reliably evaluate this capability, as models exploit
dataset-specific artifacts and memorize test instances. We introduce
RefusalBench, a generative methodology that programmatically creates diagnostic
test cases through controlled linguistic perturbation. Our framework employs
176 distinct perturbation strategies across six categories of informational
uncertainty and three intensity levels. Evaluation of over 30 models uncovers
systematic failure patterns: refusal comprises separable detection and
categorization skills, and neither scale nor extended reasoning improves
performance. We find that selective refusal is a trainable, alignment-sensitive
capability, offering a clear path for improvement. We release two benchmarks --
RefusalBench-NQ (single document) and RefusalBench-GaRAGe (multi-document) --
and our complete generation framework to enable continued, dynamic evaluation
of this critical capability.

</details>


### [59] [AssoMem: Scalable Memory QA with Multi-Signal Associative Retrieval](https://arxiv.org/abs/2510.10397)
*Kai Zhang,Xinyuan Zhang,Ejaz Ahmed,Hongda Jiang,Caleb Kumar,Kai Sun,Zhaojiang Lin,Sanat Sharma,Shereen Oraby,Aaron Colak,Ahmed Aly,Anuj Kumar,Xiaozhong Liu,Xin Luna Dong*

Main category: cs.CL

TL;DR: 提出AssoMem框架，通过构建关联记忆图来改进大规模记忆检索，在相似密集场景中优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖语义距离进行检索，但在相似密集场景中效果有限。受人类联想记忆启发，需要更好的记忆组织方式

Method: 构建关联记忆图，将对话话语锚定到自动提取的线索上，使用多维检索信号（相关性、重要性、时间对齐）和自适应互信息融合策略

Result: 在三个基准测试和新数据集MeetingQA上的广泛实验表明，AssoMem始终优于最先进的基线方法

Conclusion: AssoMem在上下文感知记忆召回方面具有优越性，验证了关联记忆图的有效性

Abstract: Accurate recall from large scale memories remains a core challenge for memory
augmented AI assistants performing question answering (QA), especially in
similarity dense scenarios where existing methods mainly rely on semantic
distance to the query for retrieval. Inspired by how humans link information
associatively, we propose AssoMem, a novel framework constructing an
associative memory graph that anchors dialogue utterances to automatically
extracted clues. This structure provides a rich organizational view of the
conversational context and facilitates importance aware ranking. Further,
AssoMem integrates multi-dimensional retrieval signals-relevance, importance,
and temporal alignment using an adaptive mutual information (MI) driven fusion
strategy. Extensive experiments across three benchmarks and a newly introduced
dataset, MeetingQA, demonstrate that AssoMem consistently outperforms SOTA
baselines, verifying its superiority in context-aware memory recall.

</details>


### [60] [STEAM: A Semantic-Level Knowledge Editing Framework for Large Language Models](https://arxiv.org/abs/2510.10398)
*Geunyeong Jeong,Juoh Sun,Seonghee Lee,Harksoo Kim*

Main category: cs.CL

TL;DR: 提出STEAM框架，通过语义级知识编辑解决现有方法在潜在空间中创建孤立知识表示的问题，增强编辑知识的语义一致性和推理能力


<details>
  <summary>Details</summary>
Motivation: 现有知识编辑方法主要关注token级似然优化，缺乏语义一致性，导致编辑的知识在潜在空间中形成孤立的残差流，无法与现有知识结构自然整合

Method: STEAM框架首先识别目标表示作为更新事实关联的语义锚点，然后通过对齐损失在优化过程中引导编辑事实的内部表示向这些锚点对齐

Result: 实验结果表明STEAM提高了模型使用编辑知识进行推理的能力，并增强了语义一致性，证明了潜在空间对齐对于可靠和连贯知识编辑的重要性

Conclusion: 语义级知识编辑框架STEAM通过潜在空间对齐有效解决了知识编辑中的语义一致性问题，为可靠的知识更新提供了新方法

Abstract: Large Language Models store extensive factual knowledge acquired during
large-scale pre-training. However, this knowledge is inherently static,
reflecting only the state of the world at the time of training. Knowledge
editing has emerged as a promising solution for updating outdated or incorrect
facts without full retraining. However, most existing locate-and-edit methods
primarily focus on token-level likelihood optimization without addressing
semantic coherence. Our analysis reveals that such edited knowledge is often
encoded as isolated residual streams in the model's latent space, distinct from
pre-existing knowledge and bypassing natural reasoning process. To address
this, we propose \textsc{Steam}, a semantic-level knowledge editing framework
that enhances integration of updated knowledge into the model's knowledge
structure. \textsc{Steam} first identifies target representations as semantic
anchors for the updated factual association, then guides the internal
representation of the edited fact towards these anchors through an alignment
loss during optimization. Experimental results demonstrate that \textsc{Steam}
improves model's ability to reason with edited knowledge and enhances semantic
coherence, underscoring the importance of latent-space alignment for reliable
and coherent knowledge editing. The code is available at
https://github.com/GY-Jeong/STEAM.

</details>


### [61] [LONGQAEVAL: Designing Reliable Evaluations of Long-Form Clinical QA under Resource Constraints](https://arxiv.org/abs/2510.10415)
*Federica Bologna,Tiffany Pan,Matthew Wilkens,Yue Guo,Lucy Lu Wang*

Main category: cs.CL

TL;DR: LongQAEval是一个用于评估长格式临床问答系统的框架，通过比较粗粒度答案级和细粒度句子级评估方法，发现在正确性维度上细粒度标注效果更好，相关性维度上粗粒度标注更好，安全性判断仍不一致。


<details>
  <summary>Details</summary>
Motivation: 评估长格式临床问答系统需要医学专业知识且资源密集，难以获得一致的人类判断，因此需要开发在有限资源和高专业知识环境下的评估方法。

Method: 基于医生对300个真实患者问题的标注（包括医生和LLM的回答），比较粗粒度答案级与细粒度句子级评估在正确性、相关性和安全性三个维度上的表现。

Result: 不同维度的标注者间一致性不同：细粒度标注在正确性上一致性更高，粗粒度在相关性上更好，安全性判断仍不一致。仅标注少量句子子集即可获得与粗粒度标注相当的可靠性。

Conclusion: LongQAEval提供了实用的评估建议，通过选择适当的评估粒度可以降低评估成本，特别是在有限资源和需要医学专业知识的临床QA系统评估场景中。

Abstract: Evaluating long-form clinical question answering (QA) systems is
resource-intensive and challenging: accurate judgments require medical
expertise and achieving consistent human judgments over long-form text is
difficult. We introduce LongQAEval, an evaluation framework and set of
evaluation recommendations for limited-resource and high-expertise settings.
Based on physician annotations of 300 real patient questions answered by
physicians and LLMs, we compare coarse answer-level versus fine-grained
sentence-level evaluation over the dimensions of correctness, relevance, and
safety. We find that inter-annotator agreement (IAA) varies by dimension:
fine-grained annotation improves agreement on correctness, coarse improves
agreement on relevance, and judgments on safety remain inconsistent.
Additionally, annotating only a small subset of sentences can provide
reliability comparable to coarse annotations, reducing cost and effort.

</details>


### [62] [Do Audio LLMs Really LISTEN, or Just Transcribe? Measuring Lexical vs. Acoustic Emotion Cues Reliance](https://arxiv.org/abs/2510.10444)
*Jingyi Chen,Zhimeng Guo,Jiyun Chun,Pichao Wang,Andrew Perrault,Micha Elsner*

Main category: cs.CL

TL;DR: LISTEN基准测试显示，当前大型音频语言模型在情感理解中主要依赖词汇线索而非声学信息，表现出词汇主导现象。


<details>
  <summary>Details</summary>
Motivation: 研究旨在厘清大型音频语言模型是否真正处理声学信息，还是主要依赖词汇内容进行情感理解。

Method: 开发LISTEN基准测试，通过控制词汇和声学线索的对比设置来评估6个最先进模型的性能。

Result: 模型在词汇线索中性或缺失时预测"中性"，线索对齐时改进有限，线索冲突时无法区分不同情感，副语言设置中性能接近随机。

Conclusion: 当前LALMs主要是"转录"而非"聆听"，过度依赖词汇语义而未能充分利用声学线索。

Abstract: Understanding emotion from speech requires sensitivity to both lexical and
acoustic cues. However, it remains unclear whether large audio language models
(LALMs) genuinely process acoustic information or rely primarily on lexical
content. We present LISTEN (Lexical vs. Acoustic Speech Test for Emotion in
Narratives), a controlled benchmark designed to disentangle lexical reliance
from acoustic sensitivity in emotion understanding. Across evaluations of six
state-of-the-art LALMs, we observe a consistent lexical dominance. Models
predict "neutral" when lexical cues are neutral or absent, show limited gains
under cue alignment, and fail to classify distinct emotions under cue conflict.
In paralinguistic settings, performance approaches chance. These results
indicate that current LALMs largely "transcribe" rather than "listen," relying
heavily on lexical semantics while underutilizing acoustic cues. LISTEN offers
a principled framework for assessing emotion understanding in multimodal
models.

</details>


### [63] [RECON: Reasoning with Condensation for Efficient Retrieval-Augmented Generation](https://arxiv.org/abs/2510.10448)
*Zhichao Xu,Minheng Wang,Yawei Wang,Wenqian Ye,Yuntao Du,Yunpu Ma,Yijun Tian*

Main category: cs.CL

TL;DR: RECON框架通过集成显式摘要模块压缩检索证据，减少35%上下文长度，提升RAG系统训练速度和推理延迟，同时在QA基准测试中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 基于强化学习的检索增强生成系统面临上下文管理效率低下的问题，冗长嘈杂的检索文档会增加成本并降低性能。

Method: RECON框架在推理循环中集成摘要模块，采用两阶段训练：在QA数据集上进行相关性预训练，然后从专有LLMs进行多方面蒸馏以确保事实性和清晰度。

Result: 在Search-R1管道中，RECON将总上下文长度减少35%，3B模型的平均EM得分提升14.5%，7B模型提升3.0%，在多跳QA任务中表现尤为突出。

Conclusion: 学习型上下文压缩对于构建实用、可扩展且高性能的RAG系统至关重要。

Abstract: Retrieval-augmented generation (RAG) systems trained using reinforcement
learning (RL) with reasoning are hampered by inefficient context management,
where long, noisy retrieved documents increase costs and degrade performance.
We introduce RECON (REasoning with CONdensation), a framework that integrates
an explicit summarization module to compress evidence within the reasoning
loop. Our summarizer is trained via a two-stage process: relevance pretraining
on QA datasets, followed by multi-aspect distillation from proprietary LLMs to
ensure factuality and clarity. Integrated into the Search-R1 pipeline, RECON
reduces total context length by 35\%, leading to improved training speed and
inference latency, while simultaneously improving RAG performance on downstream
QA benchmarks. Notably, it boosts the average EM score of the 3B model by
14.5\% and the 7B model by 3.0\%, showing particular strength in multi-hop QA.
RECON demonstrates that learned context compression is essential for building
practical, scalable, and performant RAG systems. Our code implementation is
made available at https://github.com/allfornancy/RECON.

</details>


### [64] [Steering Over-refusals Towards Safety in Retrieval Augmented Generation](https://arxiv.org/abs/2510.10452)
*Utsav Maskey,Mark Dras,Usman Naseem*

Main category: cs.CL

TL;DR: 论文分析了LLMs在安全对齐中导致的过度拒绝问题，特别是在检索增强生成(RAG)中，构建了RagRefuse基准并提出SafeRAG-Steering方法来缓解此问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的安全对齐会导致过度拒绝——即使对良性请求也因激进的安全过滤器而拒绝。这种现象在检索增强生成中尤为明显，查询意图和检索上下文特性都会影响拒绝行为。

Method: 构建RagRefuse基准，涵盖医疗、化学和开放领域，配对良性和有害查询，控制上下文污染模式和大小。提出SafeRAG-Steering方法，通过模型中心的嵌入干预，在推理时将嵌入区域引导到确认安全、非拒绝的输出区域。

Result: 分析表明上下文排列/污染、查询和上下文领域、有害文本密度都会触发对良性查询的拒绝，效果取决于模型特定的对齐选择。SafeRAG-Steering方法能减少污染RAG管道中的过度拒绝，同时保留合法的拒绝。

Conclusion: 论文识别了RAG中过度拒绝的关键因素，并提出有效的嵌入干预方法SafeRAG-Steering来平衡安全性和实用性。

Abstract: Safety alignment in large language models (LLMs) induces over-refusals --
where LLMs decline benign requests due to aggressive safety filters. We analyze
this phenomenon in retrieval-augmented generation (RAG), where both the query
intent and retrieved context properties influence refusal behavior. We
construct RagRefuse, a domain-stratified benchmark spanning medical, chemical,
and open domains, pairing benign and harmful queries with controlled context
contamination patterns and sizes. Our analysis shows that context arrangement /
contamination, domain of query and context, and harmful-text density trigger
refusals even on benign queries, with effects depending on model-specific
alignment choices. To mitigate over-refusals, we introduce
\textsc{SafeRAG-Steering}, a model-centric embedding intervention that steers
the embedding regions towards the confirmed safe, non-refusing output regions
at inference time. This reduces over-refusals in contaminated RAG pipelines
while preserving legitimate refusals.

</details>


### [65] [End-to-end Speech Recognition with similar length speech and text](https://arxiv.org/abs/2510.10453)
*Peng Fan,Wenping Wang,Fei Deng*

Main category: cs.CL

TL;DR: 本文提出两种新方法（时间独立损失和对齐交叉熵损失）来解决语音识别中语音与文本长度不匹配的问题，通过帧融合增强关键帧信息，在AISHELL数据集上显著减少了帧数。


<details>
  <summary>Details</summary>
Motivation: 解决语音识别中语音长度与文本长度不匹配的挑战，特别是在两者长度相近的情况下，传统CTC方法无法有效对齐语音和文本。

Method: 提出了时间独立损失（TIL）和对齐交叉熵损失（AXE），并采用帧融合技术通过加权求和关键帧及其上下文帧来增强关键帧信息。

Result: 在AISHELL-1和AISHELL-2数据集子集上的实验结果表明，所提方法优于先前工作，帧数减少了至少86%。

Conclusion: 提出的TIL和AXE损失方法有效解决了语音与文本对齐问题，显著减少了处理所需的帧数，提升了语音识别性能。

Abstract: The mismatch of speech length and text length poses a challenge in automatic
speech recognition (ASR). In previous research, various approaches have been
employed to align text with speech, including the utilization of Connectionist
Temporal Classification (CTC). In earlier work, a key frame mechanism (KFDS)
was introduced, utilizing intermediate CTC outputs to guide downsampling and
preserve keyframes, but traditional methods (CTC) failed to align speech and
text appropriately when downsampling speech to a text-similar length. In this
paper, we focus on speech recognition in those cases where the length of speech
aligns closely with that of the corresponding text. To address this issue, we
introduce two methods for alignment: a) Time Independence Loss (TIL) and b)
Aligned Cross Entropy (AXE) Loss, which is based on edit distance. To enhance
the information on keyframes, we incorporate frame fusion by applying weights
and summing the keyframe with its context 2 frames. Experimental results on
AISHELL-1 and AISHELL-2 dataset subsets show that the proposed methods
outperform the previous work and achieve a reduction of at least 86\% in the
number of frames.

</details>


### [66] [Rethinking LLM Evaluation: Can We Evaluate LLMs with 200x Less Data?](https://arxiv.org/abs/2510.10457)
*Shaobo Wang,Cong Wang,Wenjie Fu,Yue Min,Mingquan Feng,Isabel Guan,Xuming Hu,Conghui He,Cunxiang Wang,Kexin Yang,Xingzhang Ren,Fei Huang,Dayiheng Liu,Linfeng Zhang*

Main category: cs.CL

TL;DR: 提出了EssenceBench框架，通过遗传算法和样本级分析来压缩基准测试集，在保持模型排名准确性的同时大幅减少样本数量。


<details>
  <summary>Details</summary>
Motivation: 随着基准测试套件规模不断扩大，需要系统性的框架来有效整合冗余减少和性能预测方法，确保预测准确性和排名一致性。

Method: 首先进行样本级冗余分析，识别并消除高度相似的样本；将基准压缩构建为分数重构的优化问题；提出EssenceBench框架，使用迭代遗传算法结合基于适应度的子集搜索和基于归因的样本搜索。

Result: 在HellaSwag基准测试上，使用25倍更少的样本能保持所有模型排名在5%内变化，使用200倍更少的样本能实现95%的排名保持。

Conclusion: 该方法相比之前的方法在压缩效果、重构误差和效率方面都有显著提升，为大规模基准测试的有效压缩提供了有效解决方案。

Abstract: As the demand for comprehensive evaluations of diverse model capabilities
steadily increases, benchmark suites have correspondingly grown significantly
in scale. Despite notable advances in redundancy reduction and subset-level
performance prediction, a systematic framework that effectively integrates
these methods to ensure both prediction accuracy and ranking consistency is
still largely elusive. In this paper, we first perform a sample-level analysis
of benchmark redundancy and identify several highly similar samples that can be
eliminated. Besides, we frame benchmark compression as an optimization problem
with the aim of score reconstruction. Building on these, we then propose
EssenceBench, a coarse-to-fine framework utilizing an iterative Genetic
Algorithm (GA), which takes the advantages of fitness-based subset search and
attribution-based sample search. Compared to previous methods, our approach
yields superior compression results with lower reconstruction error and
markedly higher efficiency. In particular, on the HellaSwag benchmark (10K
samples), our method preserves the ranking of all models shifting within 5%
using 25x fewer samples, and achieves 95% ranking preservation shifting within
5% using only 200x fewer samples.

</details>


### [67] [NIM: Neuro-symbolic Ideographic Metalanguage for Inclusive Communication](https://arxiv.org/abs/2510.10459)
*Prawaal Sharma,Poonam Goyal,Navneet Goyal,Vidisha Sharma*

Main category: cs.CL

TL;DR: 提出了一种新颖的通用表意元语言，通过神经符号AI方法将复杂概念分解为原子概念，帮助低学历人群跨越数字鸿沟。


<details>
  <summary>Details</summary>
Motivation: 解决低学历人群在数字通信中面临的障碍，弥合数字鸿沟，让缺乏正规教育的人群也能有效参与数字交流。

Method: 采用神经符号AI方法，结合基于神经网络的LLM和基于自然语义元语言理论的符号知识启发式，通过人本协作方式与200多名半文盲参与者共同定义问题、选择表意符号并验证系统。

Result: 系统实现了超过80%的语义可理解性，具有易学的学习曲线和普遍适应性，有效服务于教育程度有限的弱势群体。

Conclusion: 该表意元语言框架成功跨越了学术、语言和文化边界，为低学历人群提供了有效的数字通信解决方案。

Abstract: Digital communication has become the cornerstone of modern interaction,
enabling rapid, accessible, and interactive exchanges. However, individuals
with lower academic literacy often face significant barriers, exacerbating the
"digital divide". In this work, we introduce a novel, universal ideographic
metalanguage designed as an innovative communication framework that transcends
academic, linguistic, and cultural boundaries. Our approach leverages
principles of Neuro-symbolic AI, combining neural-based large language models
(LLMs) enriched with world knowledge and symbolic knowledge heuristics grounded
in the linguistic theory of Natural Semantic Metalanguage (NSM). This enables
the semantic decomposition of complex ideas into simpler, atomic concepts.
Adopting a human-centric, collaborative methodology, we engaged over 200
semi-literate participants in defining the problem, selecting ideographs, and
validating the system. With over 80\% semantic comprehensibility, an accessible
learning curve, and universal adaptability, our system effectively serves
underprivileged populations with limited formal education.

</details>


### [68] [FML-bench: A Benchmark for Automatic ML Research Agents Highlighting the Importance of Exploration Breadth](https://arxiv.org/abs/2510.10472)
*Qiran Zou,Hou Hei Lam,Wenhao Zhao,Yiming Tang,Tingting Chen,Samson Yu,Tianyi Zhang,Chang Liu,Xiangyang Ji,Dianbo Liu*

Main category: cs.CL

TL;DR: 提出了FML-bench基准测试，用于评估自动机器学习研究代理在8个多样化基础研究问题上的表现，解决了现有基准测试在学术严谨性、任务多样性和可扩展性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试过度强调工程方面而忽视学术严谨性，任务多样性有限，过度偏向应用导向任务而非基础研究问题，且难以扩展到真实研究环境。

Method: 设计FML-bench基准测试，包含8个多样化基础机器学习研究问题，减少编码负担，强调基础问题而非具体用例，提供高任务多样性，并可扩展到真实GitHub仓库。同时提出包含5个互补指标的统一评估框架。

Result: 在FML-bench上评估了最先进的自动研究代理，发现采用广泛研究探索策略的代理优于专注于狭窄但深入探索的代理。

Conclusion: 强调探索广度可能比仅关注增量改进带来更有效的研究成果。FML-bench为评估自动机器学习研究代理提供了更全面和现实的基准。

Abstract: Large language models (LLMs) have sparked growing interest in automatic
machine learning research agents. Among them, agents capable of autonomously
proposing ideas and conducting machine learning experiments are particularly
promising, as they maximize research automation and accelerate scientific
progress by iteratively refining ideas based on experimental results. However,
comprehensively evaluating such agents remains challenging. Existing benchmarks
tend to overemphasize engineering aspects while neglecting academic rigor,
creating barriers that obscure a clear assessment of an agent's scientific
capabilities in machine learning research. They also suffer from limited task
diversity, an overemphasis on application-oriented tasks over fundamental
research problems, and limited scalability to realistic research settings. To
address these limitations, we introduce FML-bench, a benchmark designed to
evaluate automatic machine learning research agents on 8 diverse and
fundamental machine learning research problems. It reduces coding burden,
emphasizes fundamental problems rather than specific use cases, offers high
task diversity, and is extensible to real-world machine learning GitHub
repositories. Furthermore, we present a unified evaluation framework with five
complementary metrics, designed to comprehensively assess agent performance on
our benchmark. We evaluate state-of-the-art automatic research agents on
FML-bench, and find that agents employing broad research exploration strategies
outperform those focusing on narrow but deep exploration. These findings
suggest that emphasizing the breadth of exploration may lead to more effective
research outcomes than focusing solely on incremental refinement. Our benchmark
is available at https://github.com/qrzou/FML-bench.

</details>


### [69] [When or What? Understanding Consumer Engagement on Digital Platforms](https://arxiv.org/abs/2510.10474)
*Jingyi Wu,Junying Liang*

Main category: cs.CL

TL;DR: 本研究应用LDA模型分析TED演讲，发现内容创作者与观众偏好之间存在持续不匹配，且时间动态对观众参与度的影响比主题内容更强。


<details>
  <summary>Details</summary>
Motivation: 在数字服务经济中，理解什么驱动流行度至关重要。以往研究主要强调内容特征的作用，但创作者常常误判观众真正重视的内容。

Method: 使用潜在狄利克雷分配(LDA)模型分析大量TED演讲语料，将平台视为数字服务提供案例，比较创作者的主题供给与观众参与表达的需求。

Result: 识别出创作者提供内容与消费者偏好之间的持续不匹配，纵向分析显示时间动态对消费者参与度的影响比主题内容更强。

Conclusion: 研究结果挑战了内容特征是流行度主要驱动因素的假设，强调了时间和情境因素在塑造消费者反应中的重要性，为数字平台上的消费者注意力动态提供了新见解。

Abstract: Understanding what drives popularity is critical in today's digital service
economy, where content creators compete for consumer attention. Prior studies
have primarily emphasized the role of content features, yet creators often
misjudge what audiences actually value. This study applies Latent Dirichlet
Allocation (LDA) modeling to a large corpus of TED Talks, treating the platform
as a case of digital service provision in which creators (speakers) and
consumers (audiences) interact. By comparing the thematic supply of creators
with the demand expressed in audience engagement, we identify persistent
mismatches between producer offerings and consumer preferences. Our
longitudinal analysis further reveals that temporal dynamics exert a stronger
influence on consumer engagement than thematic content, suggesting that when
content is delivered may matter more than what is delivered. These findings
challenge the dominant assumption that content features are the primary drivers
of popularity and highlight the importance of timing and contextual factors in
shaping consumer responses. The results provide new insights into consumer
attention dynamics on digital platforms and carry practical implications for
marketers, platform managers, and content creators seeking to optimize audience
engagement strategies.

</details>


### [70] [Assessing Large Language Models for Structured Medical Order Extraction](https://arxiv.org/abs/2510.10475)
*A H M Rezaul Karim,Ozlem Uzuner*

Main category: cs.CL

TL;DR: 该论文介绍了在MEDIQA-OE 2025共享任务中排名第5的MasonNLP系统，使用通用LLaMA-4 17B模型通过单样本提示工程从临床对话中提取结构化医疗指令，无需领域特定微调。


<details>
  <summary>Details</summary>
Motivation: 医疗指令提取对于结构化临床信息、支持决策制定和实现文档与工作流自动化至关重要，但指令可能嵌入在多种来源中，需要从扩展对话转录本中提取结构化信息。

Method: 使用通用指令调优的LLaMA-4 17B模型，不进行领域特定微调，仅通过单个上下文示例进行引导的少样本配置。

Result: 在17个参赛团队的105份提交中排名第5，平均F1得分为37.76，在原因和来源准确性方面有显著改进。

Conclusion: 结果表明，大型非领域特定LLM与有效的提示工程结合，可以作为专门临床NLP任务的强大、可扩展基线。

Abstract: Medical order extraction is essential for structuring actionable clinical
information, supporting decision-making, and enabling downstream applications
such as documentation and workflow automation. Orders may be embedded in
diverse sources, including electronic health records, discharge summaries, and
multi-turn doctor-patient dialogues, and can span categories such as
medications, laboratory tests, imaging studies, and follow-up actions. The
MEDIQA-OE 2025 shared task focuses on extracting structured medical orders from
extended conversational transcripts, requiring the identification of order
type, description, reason, and provenance. We present the MasonNLP submission,
which ranked 5th among 17 participating teams with 105 total submissions. Our
approach uses a general-purpose, instruction-tuned LLaMA-4 17B model without
domain-specific fine-tuning, guided by a single in-context example. This
few-shot configuration achieved an average F1 score of 37.76, with notable
improvements in reason and provenance accuracy. These results demonstrate that
large, non-domain-specific LLMs, when paired with effective prompt engineering,
can serve as strong, scalable baselines for specialized clinical NLP tasks.

</details>


### [71] [UltraLLaDA: Scaling the Context Length to 128K for Diffusion Large Language Models](https://arxiv.org/abs/2510.10481)
*Guangxin He,Shen Nie,Fengqi Zhu,Yuankang Zhao,Tianyi Bai,Ran Yan,Jie Fu,Chongxuan Li,Binhang Yuan*

Main category: cs.CL

TL;DR: 提出了一种通过后训练技术扩展扩散LLMs上下文窗口的方法，无需从头训练，实现了128K token的上下文长度。


<details>
  <summary>Details</summary>
Motivation: 扩散LLMs在长上下文行为方面研究不足，需要探索如何有效扩展其上下文窗口以支持更长的输入序列。

Method: 通过改进标准的旋转位置编码(RoPE)以适应扩散过程的概率建模特性，并比较不同的掩码策略对优化稳定性和长距离记忆的影响。

Result: 开发的UltraLLaDA模型在长上下文任务中显著优于无需训练的基线方法，能够稳定扩展到128K token的上下文窗口。

Conclusion: 特殊的位置扩展技术是扩展扩散LLMs上下文窗口的关键，为通过高效后训练实现128K规模上下文提供了实用指导。

Abstract: Diffusion LLMs have attracted growing interest, with plenty of recent work
emphasizing their great potential in various downstream tasks; yet the
long-context behavior of diffusion LLMs remains largely uncharted. We present a
case study of post-training techniques for extending the context window of
diffusion LLMs (i.e., LLaDA) without retraining from scratch. We show that a
simple modification to the standard Rotary Positional Embeddings (RoPE)
extension effectively accommodates the probabilistic modeling inherent in the
diffusion process, enabling stable scaling to longer context ranges. We further
compare masking strategies used during post-training and analyze their impact
on optimization stability and long-range recall. Instantiating these insights,
we introduce UltraLLaDA, a diffusion LLM with a 128K-token context window that,
in our empirical evaluation on long-context tasks, significantly outperforms
training-free baselines. Our experimental results highlight the special
positional extension as a key lever for scaling diffusion LLMs to extended
contexts and offer practical guidance for practitioners seeking 128K-scale
context via efficient post-training.

</details>


### [72] [VOLTAGE: A Versatile Contrastive Learning based OCR Methodology for ultra low-resource scripts through Auto Glyph Feature Extraction](https://arxiv.org/abs/2510.10490)
*Prawaal Sharma,Poonam Goyal,Vidisha Sharma,Navneet Goyal*

Main category: cs.CL

TL;DR: 提出VOLTAGE方法，一种基于对比学习的OCR技术，用于低资源语言的数字包容，在Takri文字上达到95%印刷体和87%手写体识别准确率。


<details>
  <summary>Details</summary>
Motivation: 全球7000种语言中有2500种濒危，语言流失导致传统智慧、民间文学和社区本质的丧失，低资源语言缺乏无监督OCR方法阻碍其数字包容。

Method: 使用对比学习和自动字形特征推荐进行聚类标注，通过图像变换和生成对抗网络增强标注数据的多样性和数量。

Result: 在Takri文字上实现95%印刷体和87%手写体识别准确率，并在其他印度文字上验证了方法的通用性。

Conclusion: VOLTAGE方法有效解决了低资源语言OCR问题，为濒危语言的数字保存提供了可行方案。

Abstract: UNESCO has classified 2500 out of 7000 languages spoken worldwide as
endangered. Attrition of a language leads to loss of traditional wisdom, folk
literature, and the essence of the community that uses it. It is therefore
imperative to bring digital inclusion to these languages and avoid its
extinction. Low resource languages are at a greater risk of extinction. Lack of
unsupervised Optical Character Recognition(OCR) methodologies for low resource
languages is one of the reasons impeding their digital inclusion. We propose
VOLTAGE - a contrastive learning based OCR methodology, leveraging auto-glyph
feature recommendation for cluster-based labelling. We augment the labelled
data for diversity and volume using image transformations and Generative
Adversarial Networks. Voltage has been designed using Takri - a family of
scripts used in 16th to 20th century in the Himalayan regions of India. We
present results for Takri along with other Indic scripts (both low and high
resource) to substantiate the universal behavior of the methodology. An
accuracy of 95% for machine printed and 87% for handwritten samples on Takri
script has been achieved. We conduct baseline and ablation studies along with
building downstream use cases for Takri, demonstrating the usefulness of our
work.

</details>


### [73] [Merlin's Whisper: Enabling Efficient Reasoning in LLMs via Black-box Adversarial Prompting](https://arxiv.org/abs/2510.10528)
*Heming Xia,Cunxiao Du,Rui Li,Chak Tou Leong,Yongqi Li,Wenjie Li*

Main category: cs.CL

TL;DR: AdvPrompt是一个通过黑盒对抗提示来减少大型推理模型过度思考的框架，能在保持准确性的同时显著降低token使用量。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在复杂推理任务中表现出色，但冗长的推理过程带来了巨大的计算开销和延迟，阻碍了实际部署。

Method: 提出AdvPrompt迭代优化框架，从多个角度生成高质量对抗提示，将开源和闭源模型都视为黑盒通信器来获取简洁响应。

Result: 在多个基准测试中，AdvPrompt显著减少token使用：Qwen3系列在GSM8K上响应长度减少3倍，四个基准测试平均减少约40% token；Claude-3.7和Gemini-2.5在MATH-500上分别减少35%和47% token使用。

Conclusion: AdvPrompt在不同模型规模和系列中都具有良好的泛化能力，证明了黑盒提示作为提升大型推理模型效率的实用有效策略。

Abstract: Large reasoning models (LRMs) have demonstrated remarkable proficiency in
tackling complex reasoning tasks through step-by-step thinking. However, such a
lengthy reasoning process incurs substantial computational and latency
overheads, hindering the practical deployment of these models. In this work, we
present a new perspective on mitigating overthinking in LRMs via black-box
adversarial prompting. By treating both open-source LRMs and closed-source APIs
as black-box communicators, we investigate how to elicit concise responses
without sacrificing accuracy. We introduce AdvPrompt, an iterative refinement
framework that generates high-quality adversarial prompts from diverse
perspectives. Experiments across multiple benchmarks demonstrate that AdvPrompt
consistently reduces token usage while preserving performance. Notably,
AdvPrompt achieves a 3x reduction in average response length on simple GSM8K
questions for the Qwen3 model series, and delivers an average ~40% token
reduction across four benchmarks. For closed-source APIs, AdvPrompt reduces
token usage on MATH-500 by 35% for Claude-3.7 and 47% for Gemini-2.5. Further
analysis reveals the generalizability of AdvPrompt across various model scales
and families, underscoring the potential of black-box prompting as a practical
and effective strategy for enhancing LRM efficiency.

</details>


### [74] [Detecting Hallucinations in Authentic LLM-Human Interactions](https://arxiv.org/abs/2510.10539)
*Yujie Ren,Niklas Gruhlke,Anne Lauscher*

Main category: cs.CL

TL;DR: AuthenHallu是首个完全基于真实LLM-人类对话构建的幻觉检测基准，相比人工构造的基准能更准确反映实际使用中的幻觉特征。


<details>
  <summary>Details</summary>
Motivation: 现有幻觉检测基准多为人工构造，无法真实反映LLM在实际使用中的幻觉特征，需要基于真实对话构建更可靠的评估基准。

Method: 从真实的LLM-人类对话中筛选样本并进行标注，构建AuthenHallu基准，并探索使用普通LLM作为幻觉检测器的潜力。

Result: 基准中31.4%的查询-响应对存在幻觉，在数学和数字问题等挑战性领域这一比例高达60.0%。普通LLM作为检测器虽有潜力但性能仍不足。

Conclusion: 基于真实对话的基准能更准确评估LLM幻觉，当前LLM作为幻觉检测器的能力仍需改进，AuthenHallu为未来研究提供了更可靠的评估基础。

Abstract: As large language models (LLMs) are increasingly applied in sensitive domains
such as medicine and law, hallucination detection has become a critical task.
Although numerous benchmarks have been proposed to advance research in this
area, most of them are artificially constructed--either through deliberate
hallucination induction or simulated interactions--rather than derived from
genuine LLM-human dialogues. Consequently, these benchmarks fail to fully
capture the characteristics of hallucinations that occur in real-world usage.
To address this limitation, we introduce AuthenHallu, the first hallucination
detection benchmark built entirely from authentic LLM-human interactions. For
AuthenHallu, we select and annotate samples from genuine LLM-human dialogues,
thereby providing a faithful reflection of how LLMs hallucinate in everyday
user interactions. Statistical analysis shows that hallucinations occur in
31.4% of the query-response pairs in our benchmark, and this proportion
increases dramatically to 60.0% in challenging domains such as Math & Number
Problems. Furthermore, we explore the potential of using vanilla LLMs
themselves as hallucination detectors and find that, despite some promise,
their current performance remains insufficient in real-world scenarios.

</details>


### [75] [BitMar: Low-Bit Multimodal Fusion with Episodic Memory for Edge Devices](https://arxiv.org/abs/2510.10560)
*Euhid Aman,Esteban Carlin,Hsing-Kuo Pao,Giovanni Beltrame,Ghaluh Indah Permata Sari,Yie-Tarng Chen*

Main category: cs.CL

TL;DR: BitMar是一种量化多模态Transformer，使用1.58位编码器和外部情景记忆，在资源受限的硬件上实现高效的图像-文本生成。


<details>
  <summary>Details</summary>
Motivation: 解决交叉注意力Transformer等多模态视觉语言模型在边缘设备部署困难的问题，同时结合记忆增强架构与激进的边缘导向量化。

Method: 使用1.58位BitNet风格文本编码器和DiNOv2视觉编码器创建紧凑嵌入，结合固定大小的键值情景记忆，通过逐层条件化和滑动窗口注意力机制处理长序列输入。

Result: 在低延迟和小模型占用下实现了具有竞争力的图像描述和多模态理解性能，达到了良好的质量-速度权衡。

Conclusion: BitMar的特性使其非常适合边缘部署，为资源受限环境下的多模态生成任务提供了有效解决方案。

Abstract: Cross-attention transformers and other multimodal vision-language models
excel at grounding and generation; however, their extensive, full-precision
backbones make it challenging to deploy them on edge devices. Memory-augmented
architectures enhance the utilization of past context; however, most works
rarely pair them with aggressive edge-oriented quantization. We introduce
BitMar, a quantized multimodal transformer that proposes an external human-like
episodic memory for effective image-text generation on hardware with limited
resources. BitMar utilizes 1.58-bit encoders, one for text (BitNet-style) and
one for vision (DiNOv2-based), to create compact embeddings that are combined
and used to query a fixed-size key-value episodic memory. During vector
retrieval, the BitNet decoder applies per-layer conditioning, which increases
the contextual relevance of generated content. The decoder also employs
attention sinks with a sliding-window mechanism to process long or streaming
inputs under tight memory budgets. The combination of per-layer conditioning
and sliding-window attention achieves a strong quality-speed trade-off,
delivering competitive captioning and multimodal understanding at low latency
with a small model footprint. These characteristics make BitMar well-suited for
edge deployment.

</details>


### [76] [Dynamic Topic Evolution with Temporal Decay and Attention in Large Language Models](https://arxiv.org/abs/2510.10613)
*Di Wu abd Shuaidong Pan*

Main category: cs.CL

TL;DR: 提出基于时序大语言模型的动态主题演化建模框架，通过时间衰减函数和注意力机制捕捉主题变化，在潜在主题空间中使用状态转移矩阵描述动态演化，在真实语料上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 传统主题模型难以有效捕捉主题随时间的动态演化过程，需要一种能统一建模语义表示和时间演化的系统性解决方案。

Method: 使用大语言模型获取文本上下文嵌入，引入时间衰减函数和注意力机制调整语义单元重要性，在潜在主题空间应用状态转移矩阵描述动态演化，通过联合优化目标约束语义建模和时间一致性。

Result: 在真实语料上的实验表明，该框架能有效捕捉主题的生成、扩展和衰退过程，在多个指标上优于现有模型。

Conclusion: 该方法为理解大规模文本中的动态语义模式提供了系统性解决方案，丰富了主题建模的研究范式，支持多领域的复杂文本分析任务。

Abstract: This paper proposes a modeling framework for dynamic topic evolution based on
temporal large language models. The method first uses a large language model to
obtain contextual embeddings of text and then introduces a temporal decay
function and an attention mechanism. These components allow the model to adjust
the importance of semantic units according to time intervals and capture topic
variations across different periods. The temporal representations are then
mapped into a latent topic space, where a state transition matrix is applied to
describe the dynamic evolution of topics. A joint optimization objective
constrains both semantic modeling and temporal consistency, ensuring diversity
and smoothness in topic generation. The design emphasizes the unified modeling
of semantic representation and temporal evolution, which improves topic
coherence and diversity while enhancing stability and interpretability over
time. Experiments on real-world corpora show that the framework effectively
captures the generation, expansion, and decline of topics and outperforms
existing models across multiple metrics. Overall, the proposed method provides
a systematic solution for understanding dynamic semantic patterns in
large-scale text, enriches the research paradigm of topic modeling, and
supports complex text analysis tasks in multiple domains.

</details>


### [77] [Preserving LLM Capabilities through Calibration Data Curation: From Analysis to Optimization](https://arxiv.org/abs/2510.10618)
*Bowei He,Lihao Yin,Huiling Zhen,Shuqi Liu,Han Wu,Xiaokun Zhang,Mingxuan Yuan,Chen Ma*

Main category: cs.CL

TL;DR: 本文系统研究了校准数据对大语言模型后训练压缩的影响，发现激活空间中的代表性和多样性是决定校准数据质量的关键因素，并提出基于此的数据筛选框架来提升压缩后模型的关键能力保持。


<details>
  <summary>Details</summary>
Motivation: 现有研究虽然认识到校准数据对LLM压缩的重要性，但主要从数据来源或样本数量等有限角度分析语言建模或常识推理性能下降，缺乏对校准数据组合属性和领域对应性对不同LLM能力影响的系统性研究。

Method: 从激活模式角度分析校准数据的影响机制，特别探索其对高级复杂推理能力（如数学问题解决和代码生成）的影响，并基于研究发现提出校准数据筛选框架。

Result: 研究发现激活空间中的代表性和多样性更根本地决定了校准数据的质量，提出的数据筛选框架能够增强现有后训练压缩方法在保持关键LLM能力方面的性能。

Conclusion: 校准数据的质量不仅取决于数据来源和数量，更重要的是其在激活空间中的代表性和多样性，基于此的数据筛选框架能有效提升压缩后模型的复杂推理能力保持。

Abstract: Post-training compression has been a widely employed approach to scale down
large language model (LLM) and facilitate efficient inference. In various
proposed compression methods, including pruning and quantization, calibration
data plays a vital role by informing the weight importance and activation
dynamic ranges. However, how calibration data impacts the LLM capability after
compression is less explored. Few of the existing works, though recognizing the
significance of this study, only investigate the language modeling or
commonsense reasoning performance degradation from limited angles, like the
data sources or sample amounts. More systematic research is still needed to
examine the impacts on different LLM capabilities in terms of compositional
properties and domain correspondence of calibration data. In this work, we aim
at bridging this gap and further analyze underlying influencing mechanisms from
the activation pattern perspective. Especially, we explore the calibration
data's impacts on high-level complex reasoning capabilities, like math problem
solving and code generation. Delving into the underlying mechanism, we find
that the representativeness and diversity in activation space more
fundamentally determine the quality of calibration data. Finally, we propose a
calibration data curation framework based on such observations and analysis,
enhancing the performance of existing post-training compression methods on
preserving critical LLM capabilities. Our code is provided in
\href{https://github.com/BokwaiHo/COLA.git}{Link}.

</details>


### [78] [FactAppeal: Identifying Epistemic Factual Appeals in News Media](https://arxiv.org/abs/2510.10627)
*Guy Mor-Lan,Tamir Sheafer,Shaul R. Shenhav*

Main category: cs.CL

TL;DR: 提出了认知诉求识别新任务，创建FactAppeal数据集（3,226条新闻句子），识别事实陈述如何通过外部来源或证据进行支撑，最佳模型Gemma 2 9B达到0.73宏F1分数。


<details>
  <summary>Details</summary>
Motivation: 研究事实陈述如何通过外部来源或证据获得可信度，超越传统的声明检测和验证，关注认知结构和证据基础。

Method: 构建FactAppeal手动标注数据集，包含事实陈述和来源提及的跨度级标注，以及来源类型、命名、角色、认知凭证、引用方式等细粒度特征；使用2B-9B参数的编码器和生成解码器模型进行建模。

Result: 最佳模型Gemma 2 9B在认知诉求识别任务上达到0.73宏F1分数。

Conclusion: FactAppeal数据集和提出的认知诉求识别任务为理解事实陈述的可信度支撑机制提供了新视角，模型表现良好但仍有改进空间。

Abstract: How is a factual claim made credible? We propose the novel task of Epistemic
Appeal Identification, which identifies whether and how factual statements have
been anchored by external sources or evidence. To advance research on this
task, we present FactAppeal, a manually annotated dataset of 3,226
English-language news sentences. Unlike prior resources that focus solely on
claim detection and verification, FactAppeal identifies the nuanced epistemic
structures and evidentiary basis underlying these claims and used to support
them. FactAppeal contains span-level annotations which identify factual
statements and mentions of sources on which they rely. Moreover, the
annotations include fine-grained characteristics of factual appeals such as the
type of source (e.g. Active Participant, Witness, Expert, Direct Evidence),
whether it is mentioned by name, mentions of the source's role and epistemic
credentials, attribution to the source via direct or indirect quotation, and
other features. We model the task with a range of encoder models and generative
decoder models in the 2B-9B parameter range. Our best performing model, based
on Gemma 2 9B, achieves a macro-F1 score of 0.73.

</details>


### [79] [You're Not Gonna Believe This: A Computational Analysis of Factual Appeals and Sourcing in Partisan News](https://arxiv.org/abs/2510.10658)
*Guy Mor-Lan,Tamir Sheafer,Shaul R. Shenhav*

Main category: cs.CL

TL;DR: 该研究通过比较CNN和福克斯新闻对相同事件的报道，量化了媒体在事实报道中使用的不同认知策略，发现CNN使用更多事实陈述并引用外部来源，而福克斯新闻偏好新闻报道和直接引用。


<details>
  <summary>Details</summary>
Motivation: 虽然媒体偏见被广泛研究，但事实报道背后的认知策略在计算层面仍未被充分探索。本研究旨在通过大规模比较分析来揭示不同媒体在报道相同事件时使用的系统性认知差异。

Method: 采用文章匹配策略比较同一事件的报道，应用FactAppeal框架分析47万篇关于COVID-19疫情和以色列-哈马斯战争的文章，隔离报道风格与话题选择的影响。

Result: CNN的报道包含更多事实陈述且更倾向于引用外部来源，通过引用专家和专家文件构建正式权威；福克斯新闻则偏好新闻报道和直接引用。

Conclusion: 这项研究量化了党派媒体如何通过系统性的不同认知策略来构建现实，为媒体偏见研究增加了新的维度。

Abstract: While media bias is widely studied, the epistemic strategies behind factual
reporting remain computationally underexplored. This paper analyzes these
strategies through a large-scale comparison of CNN and Fox News. To isolate
reporting style from topic selection, we employ an article matching strategy to
compare reports on the same events and apply the FactAppeal framework to a
corpus of over 470K articles covering two highly politicized periods: the
COVID-19 pandemic and the Israel-Hamas war. We find that CNN's reporting
contains more factual statements and is more likely to ground them in external
sources. The outlets also exhibit sharply divergent sourcing patterns: CNN
builds credibility by citing Experts} and Expert Documents, constructing an
appeal to formal authority, whereas Fox News favors News Reports and direct
quotations. This work quantifies how partisan outlets use systematically
different epistemic strategies to construct reality, adding a new dimension to
the study of media bias.

</details>


### [80] [AGENTIQL: An Agent-Inspired Multi-Expert Framework for Text-to-SQL Generation](https://arxiv.org/abs/2510.10661)
*Omid Reza Heidari,Siobhan Reid,Yassine Yaakoubi*

Main category: cs.CL

TL;DR: AGENTIQL是一个基于智能体的多专家框架，通过问题分解、子查询生成和列选择优化来提升文本到SQL的生成效果，在Spider基准测试中达到86.07%的执行准确率。


<details>
  <summary>Details</summary>
Motivation: 传统单体架构在处理复杂推理和多样化数据库模式时存在困难，需要更智能和可扩展的解决方案。

Method: 采用多专家框架，包含推理代理进行问题分解、编码代理生成子查询、优化步骤进行列选择，以及自适应路由器在模块化流水线和基线解析器之间进行选择。

Result: 在Spider基准测试中，使用14B参数模型和Planner&Executor合并策略，实现了86.07%的执行准确率，接近GPT-4的SOTA水平(89.65%)。

Conclusion: AGENTIQL不仅提高了准确率，还通过展示中间推理步骤增强了透明度，为语义解析提供了鲁棒、可扩展且可解释的方法。

Abstract: LLMs have advanced text-to-SQL generation, yet monolithic architectures
struggle with complex reasoning and schema diversity. We propose AGENTIQL, an
agent-inspired multi-expert framework that combines a reasoning agent for
question decomposition, a coding agent for sub-query generation, and a
refinement step for column selection. An adaptive router further balances
efficiency and accuracy by selecting between our modular pipeline and a
baseline parser. Several steps in the pipeline can be executed in parallel,
making the framework scalable to larger workloads. Evaluated on the Spider
benchmark, AGENTIQL improves execution accuracy and interpretability and
achieves up to 86.07\% EX with 14B models using the Planner&Executor merging
strategy. The attained performance is contingent upon the efficacy of the
routing mechanism, thereby narrowing the gap to GPT-4-based SOTA (89.65% EX)
while using much smaller open-source LLMs. Beyond accuracy, AGENTIQL enhances
transparency by exposing intermediate reasoning steps, offering a robust,
scalable, and interpretable approach to semantic parsing.

</details>


### [81] [BrowserAgent: Building Web Agents with Human-Inspired Web Browsing Actions](https://arxiv.org/abs/2510.10666)
*Zhengbo Zhang,Zhiheng Lyu,Junhao Gong,Hongzhu Yi,Xinming Wang,Yuxuan Zhou,Jiabing Yang,Ping Nie,Yan Huang,Wenhu Chen*

Main category: cs.CL

TL;DR: BrowserAgent是一个基于人类浏览行为的交互式网页代理，通过直接操作浏览器（如滚动、点击、输入）来解决复杂任务，相比依赖工具转换网页为静态文本的方法表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如Search-R1和WebDancer）依赖额外工具将交互式网页环境转换为静态文本，这与人类多样化的浏览行为（滚动、点击、输入）不符，需要更自然的交互方式。

Method: 采用两阶段训练（监督微调和拒绝微调），通过预定义的浏览器动作直接操作原始网页，并引入显式记忆机制存储关键结论以增强长时任务推理能力。

Result: 使用比Search-R1少得多的训练数据，在多个开放问答任务中取得更具竞争力的结果，在HotpotQA、2Wiki和Bamboogle等多跳问答任务上比Search-R1提升约20%。

Conclusion: BrowserAgent可作为更先进、交互性更强且可扩展的网页代理框架，其人类启发式的浏览器操作方式更接近真实浏览体验。

Abstract: Efficiently solving real-world problems with LLMs increasingly hinges on
their ability to interact with dynamic web environments and autonomously
acquire external information. While recent research like Search-R1 and
WebDancer demonstrates strong performance in solving web tasks, they heavily
rely on additional tools to convert the interactive web environment into static
text content. This is in contrast to human browsing behaviors, which involve
diverse interactions with the browser, such as scrolling, clicking, and typing.
In this paper, we propose BrowserAgent, a more interactive agent that solves
complex tasks through human-inspired browser actions. BrowserAgent operates
directly on raw web pages via Playwright through a set of predefined browser
actions. We adopt a two-stage training (Supervised Fine-Tuning (SFT) and
Rejection Fine-Tuning (RFT)) to improve the model's generalization abilities.
Despite using significantly less training data than Search-R1, BrowserAgent
achieves more competitive results across different Open-QA tasks. Additionally,
we introduce an explicit memory mechanism to store key conclusions across
steps, further enhancing the model's reasoning capabilities for long-horizon
tasks. Notably, BrowserAgent-7B can achieve around 20\% improvement over
Search-R1 on multi-hop QA tasks like HotpotQA, 2Wiki, and Bamboogle. These
results indicate that BrowserAgent can serve as a more advanced framework for
more interactive and scalable web agents.

</details>


### [82] [Unlocking LLM Safeguards for Low-Resource Languages via Reasoning and Alignment with Minimal Training Data](https://arxiv.org/abs/2510.10677)
*Zhuowei Chen,Bowei Zhang,Nankai Lin,Tian Hou,Lianxi Wang*

Main category: cs.CL

TL;DR: 提出ConsistentGuard，一种基于推理的多语言LLM安全防护方法，通过推理增强可解释性，通过对齐促进语言间知识迁移，在少量训练样本下实现优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于分类器的方法缺乏可解释性，在低资源语言上表现不佳，需要更有效的LLM安全防护来检测恶意请求。

Method: 基于推理的多语言安全防护框架，通过推理过程增强可解释性，利用对齐机制促进语言间知识迁移。

Result: 仅用1,000个训练样本，在三个数据集六种语言上表现优越，超越使用更多数据训练的大模型，展现出强大的可解释性和泛化能力。

Conclusion: ConsistentGuard方法有效解决了现有方法的局限性，提供了多语言基准扩展和代码发布以支持未来研究。

Abstract: Recent advances in LLMs have enhanced AI capabilities, but also increased the
risk posed by malicious requests, highlighting the need for effective LLM
safeguards to detect such queries. Existing approaches largely rely on
classifier-based methods that lack interpretability and perform poorly on
low-resource languages. To address these limitations, we propose
ConsistentGuard, a novel reasoning-based multilingual safeguard, which enhances
explainability via reasoning and boosts knowledge transfer between languages
through alignment. With only 1,000 training samples, our method demonstrates
superior performance on three datasets across six languages, outperforming
larger models trained with significantly more data, and exhibits strong
interpretability and generalization ability. We also contribute a multilingual
benchmark extension and release our codes to support future research.

</details>


### [83] [RePro: Training Language Models to Faithfully Recycle the Web for Pretraining](https://arxiv.org/abs/2510.10681)
*Zichun Yu,Chenyan Xiong*

Main category: cs.CL

TL;DR: RePro是一种新颖的网页数据回收方法，使用强化学习训练小型语言模型来生成高质量、忠实于原意的数据重述，显著提升预训练数据效率。


<details>
  <summary>Details</summary>
Motivation: 高质量预训练数据是大型语言模型的"化石燃料"，但前沿模型的可用数据正在枯竭，需要开发有效的数据回收方法。

Method: 使用强化学习训练4B参数的重述模型，设计了1个质量奖励和3个忠实度奖励，优化模型将原始数据转换为高质量重述文本。

Result: 在400M和1.4B模型上，RePro相比仅使用原始数据的基线在22个下游任务中获得4.7%-14.0%的相对准确率提升，数据效率提高2-3倍。

Conclusion: RePro为有效利用LLM预训练数据提供了一条高效可控的路径，优于基于提示的先进方法，并能更好地保持原始数据的关键信息和特征。

Abstract: High-quality pretraining data is the fossil fuel of large language models
(LLMs), yet its reserves are running low for frontier models. In this paper, we
introduce RePro, a novel web recycling method that trains a relatively small LM
with reinforcement learning to generate effective and faithful rephrasings of
pretraining data. Specifically, we design one quality reward and three
faithfulness rewards, optimizing the LM rephraser to convert organic data into
high-quality rephrasings while maintaining its core semantics and structure. In
our experiment, we train a 4B rephraser to recycle 72B tokens sampled from
DCLM-RefinedWeb. Pretraining results on 400M and 1.4B models demonstrate that
RePro delivers 4.7%-14.0% relative accuracy gains over organic-only baseline on
22 downstream tasks. RePro also outperforms ReWire, the state-of-the-art web
recycling method that prompts a 70B rephraser, as well as the organic baseline
with a 4x larger data pool. Experiments with different amounts of recycled data
highlight that RePro improves organic data efficiency by 2-3x. Individual and
distributional analyses validate that RePro preserves more critical information
and faithfully reflects the characteristics of organic data compared to
prompting-based methods. Together, these results show that RePro provides an
efficient and controllable path to effectively harness the fossil fuel of LLM
pretraining. We open-source our code, rephraser, and recycled data at
https://github.com/cxcscmu/RePro.

</details>


### [84] [Sarcasm Detection Using Deep Convolutional Neural Networks: A Modular Deep Learning Framework](https://arxiv.org/abs/2510.10729)
*Manas Zambre,Sarika Bobade*

Main category: cs.CL

TL;DR: 提出了一种基于深度卷积神经网络和BERT的模块化深度学习框架，用于文本反讽检测，整合了情感分析、上下文嵌入、语言特征提取和情感检测等多层架构。


<details>
  <summary>Details</summary>
Motivation: 反讽是一种微妙且经常被误解的交流形式，特别是在缺乏语气和肢体语言的文本中，需要有效的检测方法。

Method: 使用深度卷积神经网络(DCNNs)和BERT等上下文模型，通过多层架构整合情感分析、上下文嵌入、语言特征提取和情感检测。

Result: 虽然模型仍处于概念阶段，但证明了其在聊天机器人和社交媒体分析等实际应用中的可行性。

Conclusion: 该模块化深度学习框架为文本反讽检测提供了一种可行的解决方案，具有实际应用潜力。

Abstract: Sarcasm is a nuanced and often misinterpreted form of communication,
especially in text, where tone and body language are absent. This paper
proposes a modular deep learning framework for sarcasm detection, leveraging
Deep Convolutional Neural Networks (DCNNs) and contextual models such as BERT
to analyze linguistic, emotional, and contextual cues. The system integrates
sentiment analysis, contextual embeddings, linguistic feature extraction, and
emotion detection through a multi-layer architecture. While the model is in the
conceptual stage, it demonstrates feasibility for real-world applications such
as chatbots and social media analysis.

</details>


### [85] [Large Language Models for Full-Text Methods Assessment: A Case Study on Mediation Analysis](https://arxiv.org/abs/2510.10762)
*Wenqing Zhang,Trang Nguyen,Elizabeth A. Stuart,Yiqun T. Chen*

Main category: cs.CL

TL;DR: LLMs在系统综述中自动化方法学评估表现良好，在明确陈述的方法标准上接近人类准确率，但在复杂推理任务上表现较差，需要人类监督。


<details>
  <summary>Details</summary>
Motivation: 系统综述工作量大，特别是提取详细方法学信息时。LLMs有潜力自动化方法学评估，从而改变证据合成方式。

Method: 使用因果中介分析作为代表性方法学领域，将最先进的LLMs与专家评审员在180篇全文科学文章中进行基准测试。

Result: 模型性能与人类判断高度相关（准确率相关0.71；F1相关0.97），在简单明确的方法标准上达到接近人类的准确率，但在复杂推理评估上准确率显著下降，比专家评审员低达15%。错误主要源于对表面语言线索的误解。

Conclusion: 当前LLMs擅长识别明确的方法学特征，但在细微解释上需要人类监督。将自动化信息提取与针对性专家评审相结合，是提高证据合成效率和严谨性的有前景方法。

Abstract: Systematic reviews are crucial for synthesizing scientific evidence but
remain labor-intensive, especially when extracting detailed methodological
information. Large language models (LLMs) offer potential for automating
methodological assessments, promising to transform evidence synthesis. Here,
using causal mediation analysis as a representative methodological domain, we
benchmarked state-of-the-art LLMs against expert human reviewers across 180
full-text scientific articles. Model performance closely correlated with human
judgments (accuracy correlation 0.71; F1 correlation 0.97), achieving
near-human accuracy on straightforward, explicitly stated methodological
criteria. However, accuracy sharply declined on complex, inference-intensive
assessments, lagging expert reviewers by up to 15%. Errors commonly resulted
from superficial linguistic cues -- for instance, models frequently
misinterpreted keywords like "longitudinal" or "sensitivity" as automatic
evidence of rigorous methodological approache, leading to systematic
misclassifications. Longer documents yielded lower model accuracy, whereas
publication year showed no significant effect. Our findings highlight an
important pattern for practitioners using LLMs for methods review and synthesis
from full texts: current LLMs excel at identifying explicit methodological
features but require human oversight for nuanced interpretations. Integrating
automated information extraction with targeted expert review thus provides a
promising approach to enhance efficiency and methodological rigor in evidence
synthesis across diverse scientific fields.

</details>


### [86] [HiligayNER: A Baseline Named Entity Recognition Model for Hiligaynon](https://arxiv.org/abs/2510.10776)
*James Ald Teves,Ray Daniel Cal,Josh Magdiel Villaluz,Jean Malolos,Mico Magtira,Ramon Rodriguez,Mideth Abisado,Joseph Marvin Imperial*

Main category: cs.CL

TL;DR: HiligayNER是首个针对希利盖农语的命名实体识别基准模型，使用超过8000个标注句子训练，基于mBERT和XLM-RoBERTa构建，在各项指标上均超过80%。


<details>
  <summary>Details</summary>
Motivation: 希利盖农语作为菲律宾少数民族语言，在语言处理研究中代表性不足，缺乏标注语料库和基准模型。

Method: 从新闻文章、社交媒体帖子和文学文本收集8000多个标注句子，使用mBERT和XLM-RoBERTa两种Transformer模型进行微调。

Result: 两个模型在所有实体类型上的精确率、召回率和F1分数均超过80%，与宿务语和他加禄语的跨语言评估显示出良好的可迁移性。

Conclusion: 这项工作为代表性不足的菲律宾语言（特别是希利盖农语）的语言技术发展做出贡献，支持低资源环境下的多语言NLP研究。

Abstract: The language of Hiligaynon, spoken predominantly by the people of Panay
Island, Negros Occidental, and Soccsksargen in the Philippines, remains
underrepresented in language processing research due to the absence of
annotated corpora and baseline models. This study introduces HiligayNER, the
first publicly available baseline model for the task of Named Entity
Recognition (NER) in Hiligaynon. The dataset used to build HiligayNER contains
over 8,000 annotated sentences collected from publicly available news articles,
social media posts, and literary texts. Two Transformer-based models, mBERT and
XLM-RoBERTa, were fine-tuned on this collected corpus to build versions of
HiligayNER. Evaluation results show strong performance, with both models
achieving over 80% in precision, recall, and F1-score across entity types.
Furthermore, cross-lingual evaluation with Cebuano and Tagalog demonstrates
promising transferability, suggesting the broader applicability of HiligayNER
for multilingual NLP in low-resource settings. This work aims to contribute to
language technology development for underrepresented Philippine languages,
specifically for Hiligaynon, and support future research in regional language
processing.

</details>


### [87] [Review of Inference-Time Scaling Strategies: Reasoning, Search and RAG](https://arxiv.org/abs/2510.10787)
*Zhichao Wang,Cheng Wan,Dong Nie*

Main category: cs.CL

TL;DR: 本文系统综述了推理时扩展技术，将这一新兴领域组织为输出导向和输入导向两大方法类别，旨在通过部署时的额外计算提升LLM性能，而无需重新训练模型。


<details>
  <summary>Details</summary>
Motivation: 由于高质量训练数据日益稀缺，传统通过扩大模型规模和训练数据的性能提升方式面临瓶颈，研究重点转向推理时扩展，利用部署时的额外计算显著提升LLM在下游任务中的表现。

Method: 将推理时扩展技术系统分为两大视角：输出导向方法（包括复杂多步生成策略、推理、搜索解码、长CoT训练和模型集成）和输入导向方法（主要涵盖少样本学习和RAG，其中RAG细分为查询扩展、数据、检索与重排序、LLM生成方法和多模态RAG）。

Result: 构建了一个全面的推理时扩展技术分类框架，系统梳理了这一快速演进领域的关键方法和技术路线。

Conclusion: 推理时扩展代表了LLM发展的新范式，通过部署时的计算优化而非模型重训练来突破性能瓶颈，为应对高质量数据稀缺挑战提供了有效解决方案。

Abstract: The performance gains of LLMs have historically been driven by scaling up
model size and training data. However, the rapidly diminishing availability of
high-quality training data is introducing a fundamental bottleneck, shifting
the focus of research toward inference-time scaling. This paradigm uses
additional computation at the time of deployment to substantially improve LLM
performance on downstream tasks without costly model re-training. This review
systematically surveys the diverse techniques contributing to this new era of
inference-time scaling, organizing the rapidly evolving field into two
comprehensive perspectives: Output-focused and Input-focused methods.
Output-focused techniques encompass complex, multi-step generation strategies,
including reasoning (e.g., CoT, ToT, ReAct), various search and decoding
methods (e.g., MCTS, beam search), training for long CoT (e.g., RLVR, GRPO),
and model ensemble methods. Input-focused techniques are primarily categorized
by few-shot and RAG, with RAG as the central focus. The RAG section is further
detailed through a structured examination of query expansion, data, retrieval
and reranker, LLM generation methods, and multi-modal RAG.

</details>


### [88] [Toward Human-Centered Readability Evaluation](https://arxiv.org/abs/2510.10801)
*Bahar İlgen,Georges Hattab*

Main category: cs.CL

TL;DR: 提出了一种基于人机交互和健康传播研究的五维评估框架HCRS，用于评估健康文本简化效果，超越传统表面指标如BLEU、FKGL和SARI。


<details>
  <summary>Details</summary>
Motivation: 当前NLP评估指标主要关注表面特征，无法捕捉清晰度、可信度、语气、文化相关性和可操作性等人本特质，这在高风险健康背景下尤为关键。

Method: 开发了HCRS框架，整合自动测量和结构化人类反馈，捕捉可读性的关系和情境方面，并设计了参与式评估工作流程和实证验证协议。

Result: 提出了一个五维评估框架，能够更全面地评估健康文本简化效果，考虑了用户需求、期望和生活经验。

Conclusion: 这项工作旨在推进健康文本简化评估超越表面指标，使NLP系统更贴近多样化用户的需求、期望和生活经验。

Abstract: Text simplification is essential for making public health information
accessible to diverse populations, including those with limited health
literacy. However, commonly used evaluation metrics in Natural Language
Processing (NLP), such as BLEU, FKGL, and SARI, mainly capture surface-level
features and fail to account for human-centered qualities like clarity,
trustworthiness, tone, cultural relevance, and actionability. This limitation
is particularly critical in high-stakes health contexts, where communication
must be not only simple but also usable, respectful, and trustworthy. To
address this gap, we propose the Human-Centered Readability Score (HCRS), a
five-dimensional evaluation framework grounded in Human-Computer Interaction
(HCI) and health communication research. HCRS integrates automatic measures
with structured human feedback to capture the relational and contextual aspects
of readability. We outline the framework, discuss its integration into
participatory evaluation workflows, and present a protocol for empirical
validation. This work aims to advance the evaluation of health text
simplification beyond surface metrics, enabling NLP systems that align more
closely with diverse users' needs, expectations, and lived experiences.

</details>


### [89] [Is Implicit Knowledge Enough for LLMs? A RAG Approach for Tree-based Structures](https://arxiv.org/abs/2510.10806)
*Mihir Gupte,Paolo Giusto,Ramesh S*

Main category: cs.CL

TL;DR: 提出了一种自下而上的方法，将树状结构（如GitHub仓库）的知识线性化，通过在每个层级生成隐式聚合摘要，使知识能够存储在知识库中并与RAG直接使用。


<details>
  <summary>Details</summary>
Motivation: 虽然LLMs擅长基于上下文信息生成响应，但对于结构化数据（特别是树状层次结构），如何最佳表示检索到的知识以生成响应尚未得到充分探索。

Method: 采用自下而上的方法线性化树状结构知识，在每个层级生成隐式、聚合的摘要，然后将这些知识存储在知识库中用于RAG。

Result: 与在原始非结构化代码上使用RAG相比，响应质量相当，但检索器中的文档数量减少了68%以上，显著提高了效率。

Conclusion: 利用隐式线性化知识可能是处理复杂层次数据结构的有效且可扩展策略。

Abstract: Large Language Models (LLMs) are adept at generating responses based on
information within their context. While this ability is useful for interacting
with structured data like code files, another popular method,
Retrieval-Augmented Generation (RAG), retrieves relevant documents to augment
the model's in-context learning. However, it is not well-explored how to best
represent this retrieved knowledge for generating responses on structured data,
particularly hierarchical structures like trees. In this work, we propose a
novel bottom-up method to linearize knowledge from tree-like structures (like a
GitHub repository) by generating implicit, aggregated summaries at each
hierarchical level. This approach enables the knowledge to be stored in a
knowledge base and used directly with RAG. We then compare our method to using
RAG on raw, unstructured code, evaluating the accuracy and quality of the
generated responses. Our results show that while response quality is comparable
across both methods, our approach generates over 68% fewer documents in the
retriever, a significant gain in efficiency. This finding suggests that
leveraging implicit, linearized knowledge may be a highly effective and
scalable strategy for handling complex, hierarchical data structures.

</details>


### [90] [Happiness is Sharing a Vocabulary: A Study of Transliteration Methods](https://arxiv.org/abs/2510.10827)
*Haeji Jung,Jinju Kim,Kyungjin Kim,Youjeong Roh,David R. Mortensen*

Main category: cs.CL

TL;DR: 该论文研究了音译在跨语言NLP中的作用，发现罗马化音译在大多数评估场景中表现最佳，主要原因是能产生与预训练语言共享的更长子词标记。


<details>
  <summary>Details</summary>
Motivation: 研究共享脚本、重叠标记词汇和共享音系对多语言模型性能的贡献程度，探索音译作为连接不同语言的有效方法。

Method: 使用三种音译方法（罗马化、音位转录和替代密码）以及正字法进行受控实验，在命名实体识别和自然语言推理两个下游任务上评估模型性能。

Result: 罗马化音译在8个评估设置中的7个显著优于其他输入类型，验证了其作为最有效方法的假设。

Conclusion: 罗马化音译是最有效的跨语言处理方法，其成功主要源于能产生与预训练语言共享的更长子词标记，从而更好地利用模型能力。

Abstract: Transliteration has emerged as a promising means to bridge the gap between
various languages in multilingual NLP, showing promising results especially for
languages using non-Latin scripts. We investigate the degree to which shared
script, overlapping token vocabularies, and shared phonology contribute to
performance of multilingual models. To this end, we conduct controlled
experiments using three kinds of transliteration (romanization, phonemic
transcription, and substitution ciphers) as well as orthography. We evaluate
each model on two downstream tasks -- named entity recognition (NER) and
natural language inference (NLI) -- and find that romanization significantly
outperforms other input types in 7 out of 8 evaluation settings, largely
consistent with our hypothesis that it is the most effective approach. We
further analyze how each factor contributed to the success, and suggest that
having longer (subword) tokens shared with pre-trained languages leads to
better utilization of the model.

</details>


### [91] [DUAL-Bench: Measuring Over-Refusal and Robustness in Vision-Language Models](https://arxiv.org/abs/2510.10846)
*Kaixuan Ren,Preslav Nakov,Usman Naseem*

Main category: cs.CL

TL;DR: DUAL-Bench是首个专注于多模态视觉语言模型中过度拒绝和安全完成问题的基准测试，评估了18个VLM在12个危险类别中的表现，发现现有模型在安全完成方面仍有很大改进空间。


<details>
  <summary>Details</summary>
Motivation: 随着视觉语言模型能力增强，在安全性和实用性之间保持平衡成为核心挑战。现有基准未能系统性地解决视觉模态中的过度拒绝问题，特别是在双重用途场景下，模型要么过于保守地拒绝，要么不安全地完成任务。

Method: 构建DUAL-Bench多模态基准，评估18个视觉语言模型在12个危险类别中的表现，重点关注语义保持的视觉扰动下的鲁棒性。

Result: 评估结果显示模型在安全完成方面表现不佳：GPT-5-Nano达到12.9%的安全完成率，GPT-5模型平均为7.9%，Qwen模型仅为3.9%，表明现有模型仍有很大改进空间。

Conclusion: DUAL-Bench将促进开发更精细的对齐策略，确保模型在复杂的多模态环境中既安全又有用，解决过度拒绝和安全完成之间的平衡问题。

Abstract: As vision-language models become increasingly capable, maintaining a balance
between safety and usefulness remains a central challenge. Safety mechanisms,
while essential, can backfire, causing over-refusal, where models decline
benign requests out of excessive caution. Yet, no existing benchmark has
systematically addressed over-refusal in the visual modality. This setting
introduces unique challenges, such as dual-use cases where an instruction is
harmless, but the accompanying image contains harmful content. Models
frequently fail in such scenarios, either refusing too conservatively or
completing tasks unsafely, which highlights the need for more fine-grained
alignment. The ideal behavior is safe completion, i.e., fulfilling the benign
parts of a request while explicitly warning about any potentially harmful
elements. To address this, we present DUAL-Bench, the first multimodal
benchmark focused on over-refusal and safe completion in VLMs. We evaluated 18
VLMs across 12 hazard categories, with focus on their robustness under
semantics-preserving visual perturbations. The results reveal substantial room
for improvement: GPT-5-Nano achieves 12.9% safe completion, GPT-5 models
average 7.9%, and Qwen models only 3.9%. We hope that DUAL-Bench will foster
the development of more nuanced alignment strategies that ensure models remain
both safe and useful in complex multimodal settings.

</details>


### [92] [Rethinking Agentic Workflows: Evaluating Inference-Based Test-Time Scaling Strategies in Text2SQL Tasks](https://arxiv.org/abs/2510.10885)
*Jiajing Guo,Kenil Patel,Jorge Piazentin Ono,Wenbin He,Liu Ren*

Main category: cs.CL

TL;DR: 本文评估了6种轻量级测试时扩展策略和4个LLM在Text-to-SQL任务上的表现，发现分治提示和少样本演示能持续提升性能，但额外工作流步骤效果不一，基础模型选择至关重要。


<details>
  <summary>Details</summary>
Motivation: 测试时扩展策略在LLM-based Text2SQL系统中的实际有效性，特别是在最新推理模型上的表现仍不确定，需要在实际应用中进行基准测试。

Method: 在BIRD Mini-Dev基准上评估6种轻量级、面向行业的测试时扩展策略和4个LLM（包括2个推理模型），除了标准准确率指标外，还报告推理延迟和token消耗。

Result: 分治提示和少样本演示对通用型和推理型LLM都能持续提升性能，但引入额外工作流步骤效果不一，基础模型选择对性能影响显著。

Conclusion: 揭示了在部署Text2SQL系统时准确率、效率和复杂性之间的实际权衡关系。

Abstract: Large language models (LLMs) are increasingly powering Text-to-SQL (Text2SQL)
systems, enabling non-expert users to query industrial databases using natural
language. While test-time scaling strategies have shown promise in LLM-based
solutions, their effectiveness in real-world applications, especially with the
latest reasoning models, remains uncertain. In this work, we benchmark six
lightweight, industry-oriented test-time scaling strategies and four LLMs,
including two reasoning models, evaluating their performance on the BIRD
Mini-Dev benchmark. Beyond standard accuracy metrics, we also report inference
latency and token consumption, providing insights relevant for practical system
deployment. Our findings reveal that Divide-and-Conquer prompting and few-shot
demonstrations consistently enhance performance for both general-purpose and
reasoning-focused LLMs. However, introducing additional workflow steps yields
mixed results, and base model selection plays a critical role. This work sheds
light on the practical trade-offs between accuracy, efficiency, and complexity
when deploying Text2SQL systems.

</details>


### [93] [LLM$\times$MapReduce-V3: Enabling Interactive In-Depth Survey Generation through a MCP-Driven Hierarchically Modular Agent System](https://arxiv.org/abs/2510.10890)
*Yu Chao,Siyu Lin,xiaorong wang,Zhu Zhang,Zihan Zhou,Haoyu Wang,Shuo Wang,Jie Zhou,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: LLM x MapReduce-V3是一个分层模块化代理系统，用于生成长篇调研报告。该系统采用多代理架构，将功能组件实现为独立的MCP服务器，并通过高层规划代理动态编排工作流程。


<details>
  <summary>Details</summary>
Motivation: 基于先前工作LLM x MapReduce-V2，该版本旨在通过模块化分解实现更灵活的研究过程控制，支持人机交互干预，让用户能够更好地定制研究过程。

Method: 系统将骨架初始化、摘要构建和骨架精炼等功能组件实现为独立的MCP服务器，这些原子服务器可以聚合成更高层服务器。高层规划代理根据MCP工具描述和执行历史动态选择模块，通过多轮交互准确捕捉研究视角。

Result: 人类评估表明，该系统在内容深度和长度方面都优于代表性基线方法，突显了基于MCP的模块化规划的优势。

Conclusion: LLM x MapReduce-V3通过分层模块化架构和MCP服务器实现了高效的长篇调研生成，提供了更好的用户控制和定制能力，在内容质量上超越了现有方法。

Abstract: We introduce LLM x MapReduce-V3, a hierarchically modular agent system
designed for long-form survey generation. Building on the prior work, LLM x
MapReduce-V2, this version incorporates a multi-agent architecture where
individual functional components, such as skeleton initialization, digest
construction, and skeleton refinement, are implemented as independent
model-context-protocol (MCP) servers. These atomic servers can be aggregated
into higher-level servers, creating a hierarchically structured system. A
high-level planner agent dynamically orchestrates the workflow by selecting
appropriate modules based on their MCP tool descriptions and the execution
history. This modular decomposition facilitates human-in-the-loop intervention,
affording users greater control and customization over the research process.
Through a multi-turn interaction, the system precisely captures the intended
research perspectives to generate a comprehensive skeleton, which is then
developed into an in-depth survey. Human evaluations demonstrate that our
system surpasses representative baselines in both content depth and length,
highlighting the strength of MCP-based modular planning.

</details>


### [94] [ADVICE: Answer-Dependent Verbalized Confidence Estimation](https://arxiv.org/abs/2510.10913)
*Ki Jung Seo,Sehun Lim,Taeuk Kim*

Main category: cs.CL

TL;DR: 提出了ADVICE框架，通过微调让LLM能够基于自身答案进行置信度估计，解决了LLM置信度表达中的过度自信问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然能够用自然语言表达置信度，但经常表现出过度自信，其原因尚不清楚。

Method: 提出ADVICE（答案依赖的置信度估计）微调框架，促进基于答案的置信度估计。

Result: 实验表明ADVICE显著改善了置信度校准，同时保持了任务性能，增强了答案基础性。

Conclusion: 研究揭示了过度自信的根源，并建立了更可信的置信度表达框架。

Abstract: Recent progress in large language models (LLMs) has enabled them to express
their confidence in natural language, enhancing transparency and reliability.
However, their confidence often exhibits overconfidence, the cause of which
remains poorly understood. In this work, we conduct a detailed analysis of the
dynamics underlying verbalized confidence and identify answer-independence as a
key factor, defined as the model's failure to condition confidence on its own
answer. To address this, we propose ADVICE (Answer-Dependent Verbalized
Confidence Estimation), a fine-tuning framework that facilitates
answer-grounded confidence estimation. Extensive experiments show that ADVICE
substantially improves confidence calibration while preserving task
performance. Further analyses confirm that ADVICE strengthens
answer-groundedness, leading to more balanced and well-calibrated confidence
distributions. Our findings shed light on the origin of overconfidence and
establish a framework for more trustworthy confidence verbalization.

</details>


### [95] [GapDNER: A Gap-Aware Grid Tagging Model for Discontinuous Named Entity Recognition](https://arxiv.org/abs/2510.10927)
*Yawen Yang,Fukun Ma,Shiao Meng,Aiwei Liu,Lijie Wen*

Main category: cs.CL

TL;DR: 提出GapDNER模型，通过将上下文间隙作为特殊span类型，使用网格标注方法解决不连续命名实体识别中的解码歧义问题。


<details>
  <summary>Details</summary>
Motivation: 生物医学领域中，命名实体可能由非相邻token组成且与其他实体重叠。现有方法通过连接实体片段或内部token来识别不连续实体，但由于span或单词组合的多样性，面临错误传播和解码歧义的挑战。

Method: 将上下文间隙作为额外span类型，将span分类转换为token-pair网格标注任务。设计两个交互组件：内部span规律提取模块使用双仿射机制和线性注意力捕获每个span的内部规律；跨span关系增强模块使用交叉注意力获取不同span间的语义关系。推理时使用BFS算法搜索有效路径。

Result: 在三个数据集上的实验结果表明，GapDNER在不连续NER任务上达到了新的最先进性能，在识别复杂实体结构方面表现出显著优势。

Conclusion: GapDNER通过创新性地对上下文间隙进行表示学习，有效解决了不连续NER中的解码歧义问题，提升了识别性能。

Abstract: In biomedical fields, one named entity may consist of a series of
non-adjacent tokens and overlap with other entities. Previous methods recognize
discontinuous entities by connecting entity fragments or internal tokens, which
face challenges of error propagation and decoding ambiguity due to the wide
variety of span or word combinations. To address these issues, we deeply
explore discontinuous entity structures and propose an effective Gap-aware grid
tagging model for Discontinuous Named Entity Recognition, named GapDNER. Our
GapDNER innovatively applies representation learning on the context gaps
between entity fragments to resolve decoding ambiguity and enhance
discontinuous NER performance. Specifically, we treat the context gap as an
additional type of span and convert span classification into a token-pair grid
tagging task. Subsequently, we design two interactive components to
comprehensively model token-pair grid features from both intra- and inter-span
perspectives. The intra-span regularity extraction module employs the biaffine
mechanism along with linear attention to capture the internal regularity of
each span, while the inter-span relation enhancement module utilizes
criss-cross attention to obtain semantic relations among different spans. At
the inference stage of entity decoding, we assign a directed edge to each
entity fragment and context gap, then use the BFS algorithm to search for all
valid paths from the head to tail of grids with entity tags. Experimental
results on three datasets demonstrate that our GapDNER achieves new
state-of-the-art performance on discontinuous NER and exhibits remarkable
advantages in recognizing complex entity structures.

</details>


### [96] [Evaluating Language Models' Evaluations of Games](https://arxiv.org/abs/2510.10930)
*Katherine M. Collins,Cedegao E. Zhang,Graham Todd,Lance Ying,Mauricio Barba da Costa,Ryan Liu,Prafull Sharma,Adrian Weller,Ionatan Kuperwajs,Lionel Wong,Joshua B. Tenenbaum,Thomas L. Griffiths*

Main category: cs.CL

TL;DR: 本文提出评估AI系统对游戏评价能力的新范式，通过比较语言推理模型与人类在100多个棋盘游戏上的评价，发现推理模型更接近人类判断，但存在非单调关系和资源使用不稳定的问题。


<details>
  <summary>Details</summary>
Motivation: 传统AI评估主要关注问题解决能力，但缺乏对AI系统评价能力的评估。本文旨在建立评估AI系统评价能力的新方法，特别是在游戏评估方面。

Method: 使用包含100多个新颖棋盘游戏和450多个人类判断的大规模数据集，比较现代语言推理模型与人类及符号计算代理在游戏评价上的表现，重点关注游戏的收益/公平性和趣味性两个维度。

Result: 推理模型在游戏评价上比非推理语言模型更接近人类判断，但随着模型接近博弈论最优，与人类数据的拟合度反而下降。评估趣味性时模型表现更不稳定，且推理模型的资源使用高度可变。

Conclusion: 需要在语言和推理模型中融入更多资源理性的元推理能力，以改善模型在评价任务中的稳定性和可预测性。

Abstract: Reasoning is not just about solving problems -- it is also about evaluating
which problems are worth solving at all. Evaluations of artificial intelligence
(AI) systems primarily focused on problem solving, historically by studying how
models play games such as chess and Go. In this paper, we advocate for a new
paradigm that assesses AI systems' evaluation of games. First, we introduce a
formalism for evaluating such evaluations. We then leverage a large-scale
dataset of over $100$ novel board games and over 450 human judgments to compare
evaluations produced by modern language and reasoning models against those of
people and symbolic computational agents. We consider two kinds of evaluative
queries: assessing the payoff (or fairness) and the funness of games. These
queries span two dimensions relevant to the design of evaluations of AI
evaluations: how complex a query is to compute and how difficult a query is to
quantify. Our results show that reasoning models are generally more aligned to
people in their evaluations of games than non-reasoning language models.
However, we observe a non-monotonic relationship: as models get closer to
game-theoretic optimal, their fit to human data weakens. We also observe more
"jaggedness" across models for assessing funness, in line with the greater
difficulty of quantifying this query. Across queries and games, reasoning
models show highly variable and unpredictable resource usage when assessing
queries, pointing to the importance of imbuing more resource-rational
meta-reasoning in language and reasoning models.

</details>


### [97] [End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF: A Reproducibility Study](https://arxiv.org/abs/2510.10936)
*Anirudh Ganesh,Jayavardhan Reddy*

Main category: cs.CL

TL;DR: 成功复现了Ma和Hovy（2016）提出的BiLSTM-CNN-CRF模型，该模型结合字符级CNN、词级BiLSTM和CRF，在序列标注任务中表现优异，无需手工特征。


<details>
  <summary>Details</summary>
Motivation: 验证原始BiLSTM-CNN-CRF模型的可复现性，分析其架构组件，并为后续研究提供开源实现。

Method: 使用PyTorch重新实现BiLSTM-CNN-CRF模型，结合字符级CNN提取字符表示、词级BiLSTM建模上下文、CRF进行结构化预测。

Result: 在CoNLL-2003 NER任务上达到91.18%的F1分数，成功复现了原始论文的关键结果。

Conclusion: 该模型在序列标注任务中具有良好效果，开源实现有助于推动相关研究发展。

Abstract: We present a reproducibility study of the state-of-the-art neural
architecture for sequence labeling proposed by Ma and Hovy
(2016)\cite{ma2016end}. The original BiLSTM-CNN-CRF model combines
character-level representations via Convolutional Neural Networks (CNNs),
word-level context modeling through Bi-directional Long Short-Term Memory
networks (BiLSTMs), and structured prediction using Conditional Random Fields
(CRFs). This end-to-end approach eliminates the need for hand-crafted features
while achieving excellent performance on named entity recognition (NER) and
part-of-speech (POS) tagging tasks. Our implementation successfully reproduces
the key results, achieving 91.18\% F1-score on CoNLL-2003 NER and demonstrating
the model's effectiveness across sequence labeling tasks. We provide a detailed
analysis of the architecture components and release an open-source PyTorch
implementation to facilitate further research.

</details>


### [98] [Punctuation-aware treebank tree binarization](https://arxiv.org/abs/2510.10951)
*Eitan Klinger,Vivaan Wadhwa,Jungyeul Park*

Main category: cs.CL

TL;DR: 提出了一个保留标点符号的树库二值化资源包和评估套件，通过将标点作为兄弟节点处理，显著提升了头部预测准确率。


<details>
  <summary>Details</summary>
Motivation: 标准的二值化流程在头部选择前会丢弃标点符号，这会改变成分结构并损害头部-子节点识别。

Method: 开发了可复现的预处理流程，在二值化前将标点保留为兄弟节点，并提供派生产物和元数据。

Result: 在Penn Treebank上，标点感知预处理将头部预测准确率从73.66%/86.66%提升到91.85%，并在CCGbank推导对齐方面表现优异。

Conclusion: 发布的所有代码、配置文件和文档支持复现并扩展到其他语料库，标点感知预处理对树库二值化有显著改进。

Abstract: This article presents a curated resource and evaluation suite for
punctuation-aware treebank binarization. Standard binarization pipelines drop
punctuation before head selection, which alters constituent shape and harms
head-child identification. We release (1) a reproducible pipeline that
preserves punctuation as sibling nodes prior to binarization, (2) derived
artifacts and metadata (intermediate @X markers, reversibility signatures,
alignment indices), and (3) an accompanying evaluation suite covering
head-child prediction, round-trip reversibility, and structural compatibility
with derivational resources (CCGbank). On the Penn Treebank, punctuation-aware
preprocessing improves head prediction accuracy from 73.66\% (Collins rules)
and 86.66\% (MLP) to 91.85\% with the same classifier, and achieves competitive
alignment against CCGbank derivations. All code, configuration files, and
documentation are released to enable replication and extension to other
corpora.

</details>


### [99] [KOTOX: A Korean Toxic Dataset for Deobfuscation and Detoxification](https://arxiv.org/abs/2510.10961)
*Yejin Lee,Su-Hyeon Kim,Hyundong Jin,Dayoung Kim,Yeonsoo Kim,Yo-Sub Han*

Main category: cs.CL

TL;DR: 提出了KOTOX：首个同时支持韩语去混淆和去毒化的数据集，针对低资源语言中LLM难以识别的混淆毒性内容问题


<details>
  <summary>Details</summary>
Motivation: 现有毒性内容检测和去毒化研究主要集中在英语，低资源语言代表性不足，特别是当用户使用混淆技术逃避检测时，LLM在这些语言中识别和中和毒性表达面临更大挑战

Method: 基于韩语语言特征对多种混淆方法进行分类，定义基于真实示例的转换规则，构建了三个难度级别（简单、普通、困难）的数据集版本

Result: 创建了首个同时支持韩语去混淆和去毒化的数据集，包含不同混淆难度级别的数据，代码和数据已开源

Conclusion: KOTOX数据集有助于更好地理解和缓解低资源语言中LLM对混淆毒性内容的处理问题，为相关研究提供了重要资源

Abstract: Toxic content has become an increasingly critical social issue with the rapid
expansion of online communication. While numerous studies explored methods for
detecting and detoxifying such content, most have focused primarily on English,
leaving low-resource language underrepresented. Consequently, Large Language
Models~(LLMs) often struggle to identify and neutralize toxic expressions in
these languages. This challenge becomes even more pronounced when user employ
obfuscation techniques to evade detection systems. Therefore, we propose a
\textbf{KOTOX: Korean Toxic Dataset} for deobfuscation and detoxicification to
address this issue. We categorize various obfuscation approaches based on
linguistic characteristics of Korean and define a set of transformation rules
grounded in real-word examples. Using these rules, we construct three dataset
versions (easy, normal, and hard) representing different levels of obfuscation
difficulty. This is the first dataset that simultaneously supports
deobfuscation and detoxification for the Korean language. We expect it to
facilitate better understanding and mitigating of obfuscated toxic content in
LLM for low-resource languages. Our code and data are available at
https://github.com/leeyejin1231/KOTOX.

</details>


### [100] [Judge Before Answer: Can MLLM Discern the False Premise in Question?](https://arxiv.org/abs/2510.10965)
*Jidong Li,Lingyong Fang,Haodong Zhao,Sufeng Duan,Gongshen Liu*

Main category: cs.CL

TL;DR: 提出了一个自动化构建假前提问题基准测试的流程，创建了JBA数据集，并开发了增强MLLMs识别假前提能力的框架。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在假前提问题上表现脆弱，而现有基准测试在细粒度分类和覆盖范围方面存在不足，无法充分评估模型识别假前提的能力。

Method: 开发了全自动构建假前提问题基准测试的流程，系统地将前提分为3个主要类型和13个子类型，并提出了专门增强MLLMs识别假前提能力的框架。

Result: 当前MLLMs在假前提识别方面仍然存在困难，但使用所提框架训练的模型在假前提识别方面取得了显著改进。

Conclusion: 提出的基准测试和增强框架有效提升了MLLMs识别假前提的能力，增强了模型的鲁棒性。

Abstract: Multimodal large language models (MLLMs) have witnessed astonishing
advancements in recent years. Despite these successes, MLLMs remain vulnerable
to flase premise problems. However, existing benchmarks targeting this issue
are limited in scope: they often lack fine-grained categorization, exhibit
insufficient coverage, and thus fail to provide a rigorous evaluation of the
ability of models to recognize false premises. To bridge this gap, we introduce
a fully automated pipeline for constructing a comprehensive benchmark of false
premise questions. Our method systematically categorizes the premises into
three main types and thirteen subtypes according to the abilities required to
identify the premises, resulting in the JBA dataset.Results show current MLLMs
still struggle with false premise recognition. Building upon this benchmark, we
further propose a recognition enhancement framework tailored to strengthen the
robustness of MLLMs to detect false premises. Extensive experiments demonstrate
that models trained with our framework achieve significant improvements in
false premise recognition.

</details>


### [101] [RV-HATE: Reinforced Multi-Module Voting for Implicit Hate Speech Detection](https://arxiv.org/abs/2510.10971)
*Yejin Lee,Hyeseon Ahn,Yo-Sub Han*

Main category: cs.CL

TL;DR: 提出了RV-HATE框架，通过强化学习优化多个专门模块的权重，针对不同仇恨言论数据集的特定特征进行自适应检测，提高检测准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 仇恨言论在不同平台和来源的数据集中表现出多样性特征，但现有检测方法往往采用固定方法，未能适应数据特定特征。

Method: RV-HATE框架包含多个专门模块，每个模块关注仇恨言论的不同语言或上下文特征，使用强化学习优化模块权重，并通过投票机制聚合模块输出。

Result: 该方法有效处理隐式仇恨言论，相比传统静态方法实现了更优越的性能。

Conclusion: RV-HATE通过自适应数据特定特征的方法，显著提升了仇恨言论检测的准确性和可解释性。

Abstract: Hate speech remains prevalent in human society and continues to evolve in its
forms and expressions. Modern advancements in internet and online anonymity
accelerate its rapid spread and complicate its detection. However, hate speech
datasets exhibit diverse characteristics primarily because they are constructed
from different sources and platforms, each reflecting different linguistic
styles and social contexts. Despite this diversity, prior studies on hate
speech detection often rely on fixed methodologies without adapting to
data-specific features. We introduce RV-HATE, a detection framework designed to
account for the dataset-specific characteristics of each hate speech dataset.
RV-HATE consists of multiple specialized modules, where each module focuses on
distinct linguistic or contextual features of hate speech. The framework
employs reinforcement learning to optimize weights that determine the
contribution of each module for a given dataset. A voting mechanism then
aggregates the module outputs to produce the final decision. RV-HATE offers two
primary advantages: (1)~it improves detection accuracy by tailoring the
detection process to dataset-specific attributes, and (2)~it also provides
interpretable insights into the distinctive features of each dataset.
Consequently, our approach effectively addresses implicit hate speech and
achieves superior performance compared to conventional static methods. Our code
is available at https://github.com/leeyejin1231/RV-HATE.

</details>


### [102] [Enhancing Large Language Model Reasoning via Selective Critical Token Fine-Tuning](https://arxiv.org/abs/2510.10974)
*Zhiwen Ruan,Yixia Li,He Zhu,Yun Chen,Peng Li,Yang Liu,Guanhua Chen*

Main category: cs.CL

TL;DR: 提出了关键标记微调(CFT)方法，仅更新通过反事实扰动识别的功能关键标记，相比标准监督微调在数学推理任务上表现更优，同时保持更好的输出多样性。


<details>
  <summary>Details</summary>
Motivation: 标准监督微调(SFT)对所有标记进行统一惩罚，忽略了只有少量关键标记决定推理正确性，这导致输出多样性减少和泛化能力有限。

Method: CFT方法通过反事实扰动识别功能关键标记，仅在这些决定性推理步骤上更新梯度，同时保留非关键标记的多样性。

Result: 在五个模型(Qwen、OLMo、LLaMA)和11个数学推理基准上的实验表明，CFT尽管只微调不到12%的标记，但始终优于标准SFT，并改善了采样多样性和强化学习初始化。

Conclusion: CFT是一个实用且通用的框架，可实现高效和鲁棒的大语言模型微调，在保持更高熵的同时维持后续训练阶段的性能增益。

Abstract: Large language models (LLMs) primarily rely on supervised fine-tuning (SFT)
as a key method to adapt pre-trained models to domain-specific tasks such as
mathematical reasoning. However, standard SFT uniformly penalizes all tokens,
neglecting that only a small subset of critical tokens determines reasoning
correctness. This uniform supervision often causes reduced output diversity and
limited generalization. We propose Critical Token Fine-tuning (CFT), a simple
yet effective approach that updates only tokens identified as functionally
indispensable via counterfactual perturbations. By focusing gradient signals on
these decisive reasoning steps while preserving the diversity of non-critical
tokens, CFT can enhance both generation and diversity. Extensive experiments on
five models across three families (Qwen, OLMo, LLaMA) and eleven mathematical
reasoning benchmarks show that CFT, despite fine-tuning on less than 12% of
tokens, consistently outperforms standard SFT. Moreover, CFT enables test-time
scaling through improved sampling diversity and provides a stronger
initialization for reinforcement learning, sustaining performance gains in
later training stages while maintaining higher entropy for better exploration.
These results highlight CFT as a practical and general framework for efficient
and robust LLM fine-tuning.

</details>


### [103] [DeepResearchGuard: Deep Research with Open-Domain Evaluation and Multi-Stage Guardrails for Safety](https://arxiv.org/abs/2510.10994)
*Wei-Chieh Huang,Henry Peng Zou,Yaozu Wu,Dongyuan Li,Yankai Chen,Weizhi Zhang,Yangning Li,Angelo Zangari,Jizhou Guo,Chunyu Miao,Liancheng Fang,Langzhou He,Renhe Jiang,Philip S. Yu*

Main category: cs.CL

TL;DR: 提出了DEEPRESEARCHGUARD框架，通过四阶段安全保障机制解决深度研究框架中的评估不足和安全性问题，显著提升防御成功率并降低过度拒绝率。


<details>
  <summary>Details</summary>
Motivation: 现有深度研究框架缺乏充分的评估程序和阶段特定保护，主要关注问答准确率而忽视了报告质量的关键维度（可信度、连贯性、广度、深度、安全性），可能导致危险或恶意来源被整合到最终报告中。

Method: 引入DEEPRESEARCHGUARD框架，包含四阶段安全保障机制，支持开放域参考和报告评估，并使用新提出的DRSAFEBENCH基准进行阶段式安全评估。

Result: 在多种先进LLM上评估，DEEPRESEARCHGUARD平均防御成功率提升18.16%，同时将过度拒绝率降低6%。输入防护提供最强的早期保护，计划和研究防护增强了引用规范和来源可信度。

Conclusion: DEEPRESEARCHGUARD能够实现全面的开放域评估和阶段感知防御，有效阻止有害内容传播，同时系统性地提高报告质量而不会产生过高的过度拒绝率。

Abstract: Deep research frameworks have shown promising capabilities in synthesizing
comprehensive reports from web sources. While deep research possesses
significant potential to address complex issues through planning and research
cycles, existing frameworks are deficient in sufficient evaluation procedures
and stage-specific protections. They typically treat evaluation as exact match
accuracy of question-answering, but overlook crucial aspects of report quality
such as credibility, coherence, breadth, depth, and safety. This oversight may
result in hazardous or malicious sources being integrated into the final
report. To address these issues, we introduce DEEPRESEARCHGUARD, a
comprehensive framework featuring four-stage safeguards with open-domain
evaluation of references and reports. We assess performance across multiple
metrics, e.g., defense success rate and over-refusal rate, and five key report
dimensions. In the absence of a suitable safety benchmark, we introduce
DRSAFEBENCH, a stage-wise benchmark for deep research safety. Our evaluation
spans diverse state-of-the-art LLMs, including GPT-4o, Gemini-2.5-flash,
DeepSeek-v3, and o4-mini. DEEPRESEARCHGUARD achieves an average defense success
rate improvement of 18.16% while reducing over-refusal rate by 6%. The input
guard provides the most substantial early-stage protection by filtering out
obvious risks, while the plan and research guards enhance citation discipline
and source credibility. Through extensive experiments, we show that
DEEPRESEARCHGUARD enables comprehensive open-domain evaluation and stage-aware
defenses that effectively block harmful content propagation, while
systematically improving report quality without excessive over-refusal rates.
The code can be found via https://github.com/Jasonya/DeepResearchGuard.

</details>


### [104] [ABLEIST: Intersectional Disability Bias in LLM-Generated Hiring Scenarios](https://arxiv.org/abs/2510.10998)
*Mahika Phutane,Hayoung Jung,Matthew Kim,Tanushree Mitra,Aditya Vashistha*

Main category: cs.CL

TL;DR: 本文审计了6个LLM在2820个招聘场景中的表现，发现这些模型对残障人士存在显著偏见，且当残障与性别、种姓等身份交叉时，偏见更加严重。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注西方背景，忽略了全球南方残障人士面临的交叉性边缘化问题（如性别和种姓），需要全面评估LLM在招聘等高风险领域对残障人士的偏见。

Method: 引入ABLEIST框架，包含5个残障特定和3个交叉性伤害指标，基于残障研究文献，对6个LLM在2820个涵盖不同残障、性别、国籍和种姓的招聘场景进行审计。

Result: 发现对残障候选人的ABLEIST伤害显著增加，许多先进模型未能检测到这些伤害。当残障与性别、种姓边缘化交叉时，交叉性伤害（如象征主义）急剧增加。

Conclusion: 当前安全工具存在关键盲点，需要在招聘等高风险领域对前沿模型进行交叉性安全评估。

Abstract: Large language models (LLMs) are increasingly under scrutiny for perpetuating
identity-based discrimination in high-stakes domains such as hiring,
particularly against people with disabilities (PwD). However, existing research
remains largely Western-centric, overlooking how intersecting forms of
marginalization--such as gender and caste--shape experiences of PwD in the
Global South. We conduct a comprehensive audit of six LLMs across 2,820 hiring
scenarios spanning diverse disability, gender, nationality, and caste profiles.
To capture subtle intersectional harms and biases, we introduce ABLEIST
(Ableism, Inspiration, Superhumanization, and Tokenism), a set of five
ableism-specific and three intersectional harm metrics grounded in disability
studies literature. Our results reveal significant increases in ABLEIST harms
towards disabled candidates--harms that many state-of-the-art models failed to
detect. These harms were further amplified by sharp increases in intersectional
harms (e.g., Tokenism) for gender and caste-marginalized disabled candidates,
highlighting critical blind spots in current safety tools and the need for
intersectional safety evaluations of frontier models in high-stakes domains
like hiring.

</details>


### [105] [DND: Boosting Large Language Models with Dynamic Nested Depth](https://arxiv.org/abs/2510.11001)
*Tieyuan Chen,Xiaodong Chen,Haoxing Chen,Zhenzhong Lan,Weiyao Lin,Jianguo Li*

Main category: cs.CL

TL;DR: DND是一种通过嵌套深度方式选择关键token进行重处理的方法，提升现成LLM性能，在保持最小参数和计算增加的情况下显著提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 为了解决LLM在处理困难token时性能不足的问题，同时避免对简单token的冗余计算，需要一种能够动态选择关键token进行额外处理的机制。

Method: 在transformer层结束时，通过路由器识别关键token并反馈进行额外处理，采用路由器控制损失和阈值控制方案来确保选择的准确性和稳定性。

Result: 在多样化基准测试中，密集模型Qwen3-1.7B性能提升1.88%，混合专家模型Qwen3-30B-A3B性能提升0.87%，参数和计算增加极小。

Conclusion: DND方法有效提升了LLM性能，通过动态选择关键token进行嵌套深度处理，在最小化额外开销的同时实现了显著的性能增益。

Abstract: We introduce Dynamic Nested Depth (DND), a novel method that improves
performance for off-the-shelf LLMs by selecting critical tokens to reprocess in
a nested depth manner. Specifically, at the end of the given transformer layer,
DND identifies more critical tokens with a router and feeds them back for an
extra round of processing, effectively ``reviewing" difficult tokens while
avoiding redundant computation for easier ones. The dynamic selection mechanism
is tailored for precise control via two novel strategies: a router controlling
loss to enhance token selection distinguishability, and a threshold control
scheme to ensure selection stability. We demonstrate the effectiveness of DND
by directly integrating it into pre-trained dense and MoE models during a
post-training phase. On diverse benchmarks, this approach boosts the
performances of the dense Qwen3-1.7B by 1.88% and the MoE Qwen3-30B-A3B by
0.87%, all with a minimal parameter and computing increase.

</details>


### [106] [LogiNumSynth: Synthesizing Joint Logical-Numerical Reasoning Problems for Language Models](https://arxiv.org/abs/2510.11031)
*Yiwei Liu,Yucheng Li,Xiao Li,Gong Cheng*

Main category: cs.CL

TL;DR: LogiNumSynth是一个灵活的自然语言问题合成器，能够生成需要逻辑推理和数值推理联合能力的任务，支持对推理世界丰富度、逻辑推理深度和数值计算复杂度的细粒度控制。


<details>
  <summary>Details</summary>
Motivation: 现有的联合逻辑-数值推理数据集依赖固定规则集，任务复杂度控制有限，限制了其在评估和训练中的通用性。

Method: 开发LogiNumSynth合成器，支持合成完全可控的联合推理任务，包括推理世界丰富度、逻辑推理深度和数值计算复杂度的灵活控制。

Result: 实验显示多个LLM在逻辑-数值推理方面存在持续弱点，LogiNumSynth可作为诊断工具和针对性监督来源。

Conclusion: LogiNumSynth既能作为诊断工具识别模型弱点，又能提供针对性训练数据来提升集成推理能力。

Abstract: Joint logical-numerical reasoning remains a major challenge for language
models, yet existing datasets rely on fixed rule sets and offer limited control
over task complexity, constraining their generalizability for evaluation and
training. We present LogiNumSynth, a flexible natural language problem
synthesizer that synthesizes tasks requiring proficiency in joint logical
reasoning (e.g., rule-based reasoning) and numerical reasoning (e.g.,
arithmetic computation). LogiNumSynth supports fine-grained control over
reasoning world richness, logical reasoning depth, and the complexity of
numerical computations, enabling flexible data synthesis across difficulty
levels. We demonstrate three key contributions: (1) Synthesizer -- synthesizing
fully controllable joint reasoning tasks over natural language; (2) Evaluation
& Process Analysis -- evaluating both process accuracy and answer accuracy; (3)
Targeted Training -- using synthesized data to enhance LLMs' reasoning
performance. Experiments with multiple LLMs highlight persistent weaknesses in
logical-numerical reasoning, showing that LogiNumSynth can serve as both a
diagnostic tool and a source of targeted supervision for advancing integrated
reasoning skills.

</details>


### [107] [Enabling Doctor-Centric Medical AI with LLMs through Workflow-Aligned Tasks and Benchmarks](https://arxiv.org/abs/2510.11040)
*Wenya Xie,Qingying Xiao,Yu Zheng,Xidong Wang,Junying Chen,Ke Ji,Anningzhe Gao,Prayag Tiwari,Xiang Wan,Feng Jiang,Benyou Wang*

Main category: cs.CL

TL;DR: 该论文提出将大语言模型重新定位为与经验丰富的医生协作的临床助手，而非直接与患者互动，以降低安全风险。通过构建DoctorFLAN中文医疗数据集和评估基准，显著提升了开源LLMs在医疗场景中的表现。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在医疗领域应用时，直接部署给患者存在安全风险，因为模型缺乏足够的领域专业知识。需要重新定位LLMs的角色，使其与医生协作而非替代医生。

Method: 1) 进行两阶段启发-反馈调查识别临床工作流需求；2) 构建DoctorFLAN数据集(92,000个Q&A实例，覆盖22个临床任务和27个专科)；3) 创建DoctorFLAN-test(550个单轮Q&A)和DotaBench(74个多轮对话)评估基准；4) 在10多个流行LLMs上进行实验验证。

Result: 实验结果表明，DoctorFLAN显著提升了开源大语言模型在医疗场景中的性能，促进了模型与医生工作流程的对齐，并补充了现有的面向患者的模型。

Conclusion: 这项工作为推进以医生为中心的医疗大语言模型开发提供了有价值的资源和框架，通过重新定位LLMs为临床助手，实现了更安全的医疗AI应用。

Abstract: The rise of large language models (LLMs) has transformed healthcare by
offering clinical guidance, yet their direct deployment to patients poses
safety risks due to limited domain expertise. To mitigate this, we propose
repositioning LLMs as clinical assistants that collaborate with experienced
physicians rather than interacting with patients directly. We conduct a
two-stage inspiration-feedback survey to identify real-world needs in clinical
workflows. Guided by this, we construct DoctorFLAN, a large-scale Chinese
medical dataset comprising 92,000 Q&A instances across 22 clinical tasks and 27
specialties. To evaluate model performance in doctor-facing applications, we
introduce DoctorFLAN-test (550 single-turn Q&A items) and DotaBench (74
multi-turn conversations). Experimental results with over ten popular LLMs
demonstrate that DoctorFLAN notably improves the performance of open-source
LLMs in medical contexts, facilitating their alignment with physician workflows
and complementing existing patient-oriented models. This work contributes a
valuable resource and framework for advancing doctor-centered medical LLM
development

</details>


### [108] [Latent Refinement Decoding: Enhancing Diffusion-Based Language Models by Refining Belief States](https://arxiv.org/abs/2510.11052)
*Qinglin Zhu,Yizhen Yao,Runcong Zhao,Yanzheng Xiang,Amrutha Saseendran,Chen Jin,Philip Alexander Teare,Bin Liang,Yulan He,Lin Gui*

Main category: cs.CL

TL;DR: LRD是一种两阶段并行序列生成框架，通过潜在精炼和预测反馈循环解决传统自回归模型高延迟问题，在保持精度的同时实现高达10.6倍的加速。


<details>
  <summary>Details</summary>
Motivation: 解决自回归模型严格顺序解码导致的高延迟问题，以及现有扩散方法存在的信息丢失和过早决策限制。

Method: 两阶段框架：第一阶段维护掩码位置作为预测标记和掩码嵌入的分布混合；第二阶段逐步确定置信标记，保留不确定标记进行迭代反馈。使用KL散度动态作为收敛准则。

Result: 在编码任务（HumanEval +6.3，MBPP +2.6）和推理任务（GSM8K +2.9，MATH500 +3.8）上精度提升，同时实现高达10.6倍的加速。

Conclusion: LRD是并行序列生成的一个强大且多功能的替代方案，在保持准确性的同时显著提升生成速度。

Abstract: Autoregressive (AR) models remain the standard for natural language
generation but still suffer from high latency due to strictly sequential
decoding. Recent diffusion-inspired approaches, such as LlaDA and Dream,
mitigate this by generating in parallel, yet they suffer from two core
limitations: information loss, as predictive distributions for non-finalized
tokens are discarded at each step, and premature commitment, where local
decisions are made without sufficient global coordination. We introduce Latent
Refinement Decoding (LRD), a two-stage framework with Latent Refinement and a
Predictive Feedback Loop. The first stage maintains masked positions as
distributional mixtures of predicted tokens and the mask embedding, allowing
the model to establish more globally consistent beliefs. The second stage
progressively finalizes confident tokens while retaining uncertain ones for
iterative feedback. KL-divergence dynamics provide a principled and reliable
criterion for convergence and early stopping. Experiments across coding
(HumanEval +6.3, MBPP +2.6) and reasoning (GSM8K +2.9, MATH500 +3.8) show that
LRD improves accuracy while delivering speedups of up to 10.6x, making it a
strong and versatile alternative for parallel sequence generation.

</details>


### [109] [Enhancing LLM Reasoning via Non-Human-Like Reasoning Path Preference Optimization](https://arxiv.org/abs/2510.11104)
*Junjie Lu,Yuliang Liu,Chaofeng Qu,Wei Shen,Zhouhan Lin,Min Xu*

Main category: cs.CL

TL;DR: 提出CGPO方法，利用置信度信号识别模型推理过程中的最大不确定性点，通过自生成的非人类推理路径指导来缓解轨迹漂移，在相同训练数据量下优于使用强模型或人工标注数据的方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM推理增强方法存在训练偏差，依赖人类或高能力模型的中间步骤标注，限制了非人类推理路径的探索和性能提升。研究发现模型首次错误通常发生在最低置信度点之后，这表明在错误发生前的低置信度点提供指导比定位显性错误更有效。

Method: CGPO方法：1）利用置信度信号识别推理过程中的最大不确定性点；2）应用自生成的非人类推理路径指导；3）在代码和数学推理任务上进行实验验证。

Result: 实验结果显示，在相同训练数据量下，使用小模型生成数据的CGPO方法在大多数情况下比使用强模型生成数据或人工标注数据的方法表现更好。

Conclusion: CGPO方法通过置信度引导和自生成的非人类推理路径，有效缓解了轨迹漂移问题，在推理任务中实现了更好的性能，为LLM推理增强提供了新的有效途径。

Abstract: Current approaches for strengthening LLM reasoning tend to introduce a
training bias toward human-like reasoning trajectories. In step-wise preference
optimization, in particular, dependence on human or higher-capacity model
annotations for intermediate steps limits exploration of alternative,
non-human-like reasoning paths and thus constrains achievable performance.
Furthermore, through a small-scale pilot study, we observed that in
approximately 75% of cases, the model's first erroneous step occurs after the
lowest-confidence point. This suggests that guiding the model at its
lowest-confidence point before an error provides more accurate supervision than
locating the first explicit error. In this paper, we propose Confidence-Guided
Reasoning Path Preference Optimization (CGPO), a method that leverages a
confidence signal to identify points of maximal uncertainty in the model's
reasoning process and applies self-generated, non-human-like reasoning-path
guidance to mitigate trajectory drift. Our experiments span diverse models
applied to both code and mathematical reasoning tasks. The results show that,
with the same amount of training data, our method using data generated by a
small model can achieve better performance in most cases compared with
approaches using data generated by a strong model or human-annotated.

</details>


### [110] [TypePilot: Leveraging the Scala Type System for Secure LLM-generated Code](https://arxiv.org/abs/2510.11151)
*Alexander Sternfeld,Andrei Kucharavy,Ljiljana Dolamic*

Main category: cs.CL

TL;DR: TypePilot是一个基于强类型语言的AI代理框架，通过Scala语言增强LLM生成代码的安全性和鲁棒性，显著减少输入验证和注入漏洞。


<details>
  <summary>Details</summary>
Motivation: LLM生成的代码常包含细微但关键的安全漏洞，在安全敏感系统中部署存在重大风险。

Method: 使用Scala作为代表性语言，结合Stainless框架进行形式化验证和通用安全代码生成，采用类型导向的代理管道。

Result: 实验表明，直接代码生成和简单提示难以保证安全约束，而类型导向的代理管道能显著减轻输入验证和注入漏洞。

Conclusion: 结构化、类型导向的LLM工作流程有潜力提升高保证领域中自动化代码生成的可信度水平。

Abstract: Large language Models (LLMs) have shown remarkable proficiency in code
generation tasks across various programming languages. However, their outputs
often contain subtle but critical vulnerabilities, posing significant risks
when deployed in security-sensitive or mission-critical systems. This paper
introduces TypePilot, an agentic AI framework designed to enhance the security
and robustness of LLM-generated code by leveraging strongly typed and
verifiable languages, using Scala as a representative example. We evaluate the
effectiveness of our approach in two settings: formal verification with the
Stainless framework and general-purpose secure code generation. Our experiments
with leading open-source LLMs reveal that while direct code generation often
fails to enforce safety constraints, just as naive prompting for more secure
code, our type-focused agentic pipeline substantially mitigates input
validation and injection vulnerabilities. The results demonstrate the potential
of structured, type-guided LLM workflows to improve the SotA of the
trustworthiness of automated code generation in high-assurance domains.

</details>


### [111] [One Size Does Not Fit All: Exploring Variable Thresholds for Distance-Based Multi-Label Text Classification](https://arxiv.org/abs/2510.11160)
*Jens Van Nooten,Andriy Kosar,Guy De Pauw,Walter Daelemans*

Main category: cs.CL

TL;DR: 基于距离的无监督文本分类方法通过语义相似度确定标签相关性，本文提出了一种新颖的标签特定阈值优化方法，显著提升了多标签分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于距离的无监督文本分类方法在多标签场景中面临阈值设定的挑战，需要一种能够适应不同模型、数据集和标签集的阈值优化方法。

Method: 首先通过探索性研究验证文本和标签之间的语义关系在不同模型、数据集和标签集中的变化，然后提出使用验证集优化标签特定阈值的新方法。

Result: 标签特定阈值方法比标准化0.5阈值平均提升46%，比先前工作的统一阈值方法平均提升14%，且在有限标注样本下仍表现良好。

Conclusion: 标签特定阈值优化方法能够有效提升基于距离的无监督多标签文本分类性能，具有实际应用价值。

Abstract: Distance-based unsupervised text classification is a method within text
classification that leverages the semantic similarity between a label and a
text to determine label relevance. This method provides numerous benefits,
including fast inference and adaptability to expanding label sets, as opposed
to zero-shot, few-shot, and fine-tuned neural networks that require re-training
in such cases. In multi-label distance-based classification and information
retrieval algorithms, thresholds are required to determine whether a text
instance is "similar" to a label or query. Similarity between a text and label
is determined in a dense embedding space, usually generated by state-of-the-art
sentence encoders. Multi-label classification complicates matters, as a text
instance can have multiple true labels, unlike in multi-class or binary
classification, where each instance is assigned only one label. We expand upon
previous literature on this underexplored topic by thoroughly examining and
evaluating the ability of sentence encoders to perform distance-based
classification. First, we perform an exploratory study to verify whether the
semantic relationships between texts and labels vary across models, datasets,
and label sets by conducting experiments on a diverse collection of realistic
multi-label text classification (MLTC) datasets. We find that similarity
distributions show statistically significant differences across models,
datasets and even label sets. We propose a novel method for optimizing
label-specific thresholds using a validation set. Our label-specific
thresholding method achieves an average improvement of 46% over normalized 0.5
thresholding and outperforms uniform thresholding approaches from previous work
by an average of 14%. Additionally, the method demonstrates strong performance
even with limited labeled examples.

</details>


### [112] [Bridging Gaps in Hate Speech Detection: Meta-Collections and Benchmarks for Low-Resource Iberian Languages](https://arxiv.org/abs/2510.11167)
*Paloma Piot,José Ramom Pichel Campos,Javier Parapar*

Main category: cs.CL

TL;DR: 该研究为伊比利亚语言构建了仇恨言论检测的标准化数据集和基准，通过整合现有资源并创建多语言语料库，评估了大语言模型在不同设置下的性能。


<details>
  <summary>Details</summary>
Motivation: 仇恨言论对社会和个人构成严重威胁，但现有研究主要集中于英语，低资源语言缺乏足够的资源和基准，且这些语言常有多样化的语言变体。

Method: 编译欧洲西班牙语的仇恨言论数据集元集合，统一标签和元数据，并将其翻译为欧洲葡萄牙语和两种加利西亚语变体，创建对齐的多语言语料库，评估大语言模型的零样本、少样本和微调性能。

Result: 建立了伊比利亚语言仇恨言论检测的新基准，提供了大语言模型在不同设置下的基线结果，并进行了跨语言分析。

Conclusion: 研究强调了多语言和变体感知方法在仇恨言论检测中的重要性，为未被充分代表的欧洲语言提供了改进基准的基础。

Abstract: Hate speech poses a serious threat to social cohesion and individual
well-being, particularly on social media, where it spreads rapidly. While
research on hate speech detection has progressed, it remains largely focused on
English, resulting in limited resources and benchmarks for low-resource
languages. Moreover, many of these languages have multiple linguistic
varieties, a factor often overlooked in current approaches. At the same time,
large language models require substantial amounts of data to perform reliably,
a requirement that low-resource languages often cannot meet. In this work, we
address these gaps by compiling a meta-collection of hate speech datasets for
European Spanish, standardised with unified labels and metadata. This
collection is based on a systematic analysis and integration of existing
resources, aiming to bridge the data gap and support more consistent and
scalable hate speech detection. We extended this collection by translating it
into European Portuguese and into a Galician standard that is more convergent
with Spanish and another Galician variant that is more convergent with
Portuguese, creating aligned multilingual corpora. Using these resources, we
establish new benchmarks for hate speech detection in Iberian languages. We
evaluate state-of-the-art large language models in zero-shot, few-shot, and
fine-tuning settings, providing baseline results for future research. Moreover,
we perform a cross-lingual analysis with our target languages. Our findings
underscore the importance of multilingual and variety-aware approaches in hate
speech detection and offer a foundation for improved benchmarking in
underrepresented European languages.

</details>


### [113] [Evaluating Reasoning Faithfulness in Medical Vision-Language Models using Multimodal Perturbations](https://arxiv.org/abs/2510.11196)
*Johannes Moll,Markus Graf,Tristan Lemke,Nicolas Lenhart,Daniel Truhn,Jean-Benoit Delbrouck,Jiazhen Pan,Daniel Rueckert,Lisa C. Adams,Keno K. Bressem*

Main category: cs.CL

TL;DR: 提出了一个临床基础的框架来评估胸部X光视觉问答中思维链解释的忠实性，通过控制文本和图像修改来测试临床保真度、因果归因和置信度校准。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型产生的思维链解释往往听起来合理但未能反映实际决策过程，这在临床高风险应用中会削弱信任。现有评估很少捕捉到这种不一致性。

Method: 使用胸部X光视觉问答任务，通过控制文本和图像修改来评估思维链解释的忠实性，包括临床保真度、因果归因和置信度校准三个维度。

Result: 评估者与放射科医生的相关性在放射科医生内部相关性范围内，归因对齐强(τ_b=0.670)，保真度对齐中等(τ_b=0.387)，置信度对齐弱(τ_b=0.091)。专有模型在归因(25.0% vs 1.4%)和保真度(36.1% vs 31.7%)上表现更好。

Conclusion: 答案准确性和解释质量是解耦的，承认注入线索并不能确保基础，文本线索比视觉线索更能改变解释。需要超越最终答案准确性来评估模型，以降低部署风险。

Abstract: Vision-language models (VLMs) often produce chain-of-thought (CoT)
explanations that sound plausible yet fail to reflect the underlying decision
process, undermining trust in high-stakes clinical use. Existing evaluations
rarely catch this misalignment, prioritizing answer accuracy or adherence to
formats. We present a clinically grounded framework for chest X-ray visual
question answering (VQA) that probes CoT faithfulness via controlled text and
image modifications across three axes: clinical fidelity, causal attribution,
and confidence calibration. In a reader study (n=4), evaluator-radiologist
correlations fall within the observed inter-radiologist range for all axes,
with strong alignment for attribution (Kendall's $\tau_b=0.670$), moderate
alignment for fidelity ($\tau_b=0.387$), and weak alignment for confidence tone
($\tau_b=0.091$), which we report with caution. Benchmarking six VLMs shows
that answer accuracy and explanation quality are decoupled, acknowledging
injected cues does not ensure grounding, and text cues shift explanations more
than visual cues. While some open-source models match final answer accuracy,
proprietary models score higher on attribution (25.0% vs. 1.4%) and often on
fidelity (36.1% vs. 31.7%), highlighting deployment risks and the need to
evaluate beyond final answer accuracy.

</details>


### [114] [Discursive Circuits: How Do Language Models Understand Discourse Relations?](https://arxiv.org/abs/2510.11210)
*Yisong Miao,Min-Yen Kan*

Main category: cs.CL

TL;DR: 该论文发现Transformer语言模型中存在稀疏的"话语电路"（约0.2%的GPT-2模型组件），专门负责话语理解任务，这些电路在不同话语框架间具有良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 研究Transformer语言模型中哪些组件负责话语理解，因为话语关系涉及长跨度文本和复杂推理，需要识别控制模型处理话语关系的稀疏计算图。

Method: 引入CuDR任务（在指定话语关系下完成话语），构建用于激活修补的最小对比对语料库，通过电路发现技术识别稀疏的话语电路。

Result: 实验显示稀疏电路（约GPT-2模型的0.2%）在英语PDTB-based CuDR任务中恢复话语理解能力，这些电路能很好地泛化到RST和SDRT等未见的话语框架。

Conclusion: 下层网络捕获词汇语义和共指等语言特征，上层网络编码话语级抽象，特征效用在不同框架间保持一致（如共指支持扩展类关系）。

Abstract: Which components in transformer language models are responsible for discourse
understanding? We hypothesize that sparse computational graphs, termed as
discursive circuits, control how models process discourse relations. Unlike
simpler tasks, discourse relations involve longer spans and complex reasoning.
To make circuit discovery feasible, we introduce a task called Completion under
Discourse Relation (CuDR), where a model completes a discourse given a
specified relation. To support this task, we construct a corpus of minimal
contrastive pairs tailored for activation patching in circuit discovery.
Experiments show that sparse circuits ($\approx 0.2\%$ of a full GPT-2 model)
recover discourse understanding in the English PDTB-based CuDR task. These
circuits generalize well to unseen discourse frameworks such as RST and SDRT.
Further analysis shows lower layers capture linguistic features such as lexical
semantics and coreference, while upper layers encode discourse-level
abstractions. Feature utility is consistent across frameworks (e.g.,
coreference supports Expansion-like relations).

</details>


### [115] [Domain-Specific Data Generation Framework for RAG Adaptation](https://arxiv.org/abs/2510.11217)
*Chris Xing Tian,Weihao Xie,Zhen Chen,Zhengyuan Yi,Hui Liu,Haoliang Li,Shiqi Wang,Siwei Ma*

Main category: cs.CL

TL;DR: RAGen是一个可扩展的模块化框架，用于生成针对特定领域的问答上下文三元组，以支持检索增强生成(RAG)系统的领域适应。


<details>
  <summary>Details</summary>
Motivation: 有效适应RAG系统到特定领域需要超越通用问答的专业化、上下文丰富的训练数据，而现有方法在这方面存在不足。

Method: 通过识别文档中的关键概念，基于布鲁姆分类学原则生成多样化问题，并从相关上下文中提取精确答案来创建问答上下文三元组。支持模块化流程，包括语义分块、层次概念提取、多块检索和引入干扰上下文。

Result: RAGen框架能够高效处理大型和不断演化的文档语料库，无需冗余处理，特别适用于科学研究和企业知识库等动态演化领域。

Conclusion: RAGen提供了一个可扩展的解决方案，能够为各种RAG适应方法生成领域相关的训练数据，支持LLM、检索器和嵌入模型等关键组件的优化。

Abstract: Retrieval-Augmented Generation (RAG) combines the language understanding and
reasoning power of large language models (LLMs) with external retrieval to
enable domain-grounded responses. Effectively adapting RAG systems to
domain-specific settings requires specialized, context-rich training data
beyond general-purpose question-answering. Here, we propose RAGen, a scalable
and modular framework for generating domain-grounded question-answer-context
(QAC) triples tailored to diverse RAG adaptation approaches. RAGen produces
these QAC triples by identifying key concepts in documents, generating diverse
questions guided by Bloom's Taxonomy-inspired principles, and pairing them with
precise answers extracted from relevant contexts. RAGen supports multiple RAG
adaptation strategies, including the optimization of key components such as the
LLM, retriever, and embedding model, etc. Its modular pipeline features
semantic chunking, hierarchical concept extraction, and multi-chunk retrieval,
along with the introduction of curated distractor contexts to promote robust
reasoning. Designed for scalability, RAGen efficiently handles large and
evolving document corpora without redundant processing, making it especially
suitable for dynamic evolving domains such as scientific research and
enterprise knowledge bases.

</details>


### [116] [The Curious Case of Factual (Mis)Alignment between LLMs' Short- and Long-Form Answers](https://arxiv.org/abs/2510.11218)
*Saad Obaid ul Islam,Anne Lauscher,Goran Glavaš*

Main category: cs.CL

TL;DR: SLAQ框架揭示LLMs在简单问答和复杂查询中对同一事实的回答存在系统性不一致，挑战了现有评估方法对模型事实一致性的假设。


<details>
  <summary>Details</summary>
Motivation: 发现LLMs在简单事实问答中表现准确，但在复杂查询中回答相同事实时出现不一致，这种可靠性差距影响模型可信度。

Method: 提出SLAQ评估框架，比较LLMs对同一事实问题在孤立形式（短）和复杂查询中（长）的回答一致性，分析16个模型在600个查询上的表现。

Result: 发现短长回答存在系统性不一致，存在位置依赖的准确率损失和动量效应，机制分析显示对齐事实激活重叠模型内部，机制相似性指标可预测对齐度达78%。

Conclusion: 查询复杂度的事实一致性是LLMs可信度的重要方面，当前评估方法错误假设简单事实查询的良好表现意味着复杂知识搜索任务的可靠性。

Abstract: Large language models (LLMs) can correctly answer "When was Einstein born?"
yet fail to provide the same date when writing about Einstein's life revealing
a fundamental inconsistency in how models access factual knowledge across task
complexities. While models display impressive accuracy on factual
question-answering benchmarks, the reliability gap between simple and complex
queries remains poorly understood, eroding their trustworthiness. In this work,
we introduce Short-Long Form Alignment for Factual Question Answering (SLAQ), a
controlled evaluation framework that compares LLMs' answers to the same factual
questions asked (a) in isolation (short) vs. (b) integrated into complex
queries (long). Looking at 16 LLMs across 600 queries, we find a systematic
misalignment of answers to the corresponding short and long queries. We further
uncover position-dependent accuracy loss and momentum effects where consecutive
correct or incorrect answers create self-reinforcing patterns. Through
mechanistic analysis, we find that aligned facts activate overlapping model
internals, and that metrics based on mechanistic similarity can predict
short-long answer alignment with up to 78% accuracy. Our work establishes
factual consistency over query complexity as an important aspect of LLMs'
trustworthiness and challenges current evaluation practices, which implicitly
assume that good performance for simple factual queries implies reliability in
more complex knowledge-seeking tasks too.

</details>


### [117] [WebRouter: Query-specific Router via Variational Information Bottleneck for Cost-sensitive Web Agent](https://arxiv.org/abs/2510.11221)
*Tao Li,Jinlong Hu,Yang Wang,Junfeng Liu,Xuejun Liu*

Main category: cs.CL

TL;DR: WebRouter是一个基于信息论的查询特定路由器，通过成本感知变分信息瓶颈目标，在压缩输入提示的同时明确惩罚预期操作成本，显著降低了LLM驱动的网络代理的操作成本。


<details>
  <summary>Details</summary>
Motivation: LLM驱动的网络代理面临成本与性能的关键权衡，复杂的提示（包含目标、行动历史和状态）导致LLM集成性能下降，需要解决这一挑战。

Method: 提出WebRouter，采用成本感知变分信息瓶颈（ca-VIB）目标，学习输入提示的压缩表示，同时明确惩罚预期操作成本。

Result: 在WebVoyager基准的五个真实网站上的实验显示，与GPT-4o基线相比，WebRouter将操作成本降低了87.8%，而准确率仅下降3.8%。

Conclusion: WebRouter通过信息论方法有效解决了网络代理的成本性能权衡问题，显著降低操作成本同时保持高准确率。

Abstract: LLM-brained web agents offer powerful capabilities for web automation but
face a critical cost-performance trade-off. The challenge is amplified by web
agents' inherently complex prompts that include goals, action histories, and
environmental states, leading to degraded LLM ensemble performance. To address
this, we introduce WebRouter, a novel query-specific router trained from an
information-theoretic perspective. Our core contribution is a cost-aware
Variational Information Bottleneck (ca-VIB) objective, which learns a
compressed representation of the input prompt while explicitly penalizing the
expected operational cost. Experiments on five real-world websites from the
WebVoyager benchmark show that WebRouter reduces operational costs by a
striking 87.8\% compared to a GPT-4o baseline, while incurring only a 3.8\%
accuracy drop.

</details>


### [118] [Fairness Metric Design Exploration in Multi-Domain Moral Sentiment Classification using Transformer-Based Models](https://arxiv.org/abs/2510.11222)
*Battemuulen Naranbat,Seyed Sahand Mohammadi Ziabari,Yousuf Nasser Al Husaini,Ali Mohammed Mansoor Alsahag*

Main category: cs.CL

TL;DR: 该论文研究了NLP道德情感分类中的公平性问题，特别是在跨领域迁移时。通过引入道德公平一致性(MFC)指标，揭示了总体性能指标可能掩盖的公平性违规，特别是在权威标签上表现出显著的公平性问题。


<details>
  <summary>Details</summary>
Motivation: 随着Transformer模型在跨领域道德情感分类中的广泛应用，确保模型公平性变得尤为重要。总体性能指标可能掩盖不同领域间的公平性差异，需要专门的指标来评估跨领域稳定性。

Method: 使用Moral Foundations Twitter Corpus和Moral Foundations Reddit Corpus，在BERT和DistilBERT模型上进行多标签分类实验。采用域内和跨域协议，并引入新的道德公平一致性(MFC)指标来量化跨领域稳定性。

Result: 发现跨领域迁移存在明显不对称性：Twitter→Reddit的micro-F1下降14.9%，而Reddit→Twitter仅下降1.5%。权威标签表现出最大的公平性违规，人口统计均等差异达0.22-0.23，均等化机会差异达0.40-0.41。MFC与人口统计均等差异呈完美负相关(rho=-1.000)。

Conclusion: MFC指标作为补充性、诊断导向的公平性评估指标，能够有效评估道德推理模型在异构语言环境中的跨领域稳定性，有助于实现更可靠的部署。

Abstract: Ensuring fairness in natural language processing for moral sentiment
classification is challenging, particularly under cross-domain shifts where
transformer models are increasingly deployed. Using the Moral Foundations
Twitter Corpus (MFTC) and Moral Foundations Reddit Corpus (MFRC), this work
evaluates BERT and DistilBERT in a multi-label setting with in-domain and
cross-domain protocols. Aggregate performance can mask disparities: we observe
pronounced asymmetry in transfer, with Twitter->Reddit degrading micro-F1 by
14.9% versus only 1.5% for Reddit->Twitter. Per-label analysis reveals fairness
violations hidden by overall scores; notably, the authority label exhibits
Demographic Parity Differences of 0.22-0.23 and Equalized Odds Differences of
0.40-0.41. To address this gap, we introduce the Moral Fairness Consistency
(MFC) metric, which quantifies the cross-domain stability of moral foundation
detection. MFC shows strong empirical validity, achieving a perfect negative
correlation with Demographic Parity Difference (rho = -1.000, p < 0.001) while
remaining independent of standard performance metrics. Across labels, loyalty
demonstrates the highest consistency (MFC = 0.96) and authority the lowest (MFC
= 0.78). These findings establish MFC as a complementary, diagnosis-oriented
metric for fairness-aware evaluation of moral reasoning models, enabling more
reliable deployment across heterogeneous linguistic contexts. .

</details>


### [119] [A Theorem-Proving-Based Evaluation of Neural Semantic Parsing](https://arxiv.org/abs/2510.11225)
*Hayate Funakura,Hyunsoo Kim,Koji Mineshima*

Main category: cs.CL

TL;DR: 该论文重新评估语义解析器的评估方法，将图匹配与自动定理证明相结合，发现基于图匹配的评估指标（如Smatch）无法准确反映逻辑等价性，并提出逻辑敏感的评估方法。


<details>
  <summary>Details</summary>
Motivation: 现有的图匹配评估指标（如Smatch）只关注表面重叠而非逻辑等价性，无法准确评估语义解析器的真实性能，特别是在推理导向的应用中。

Method: 比较两种解析器构建方法：监督微调（T5-Small/Base）和少样本上下文学习（GPT-4o/4.1/5），使用图匹配、双向蕴含证明和语法正确性三种评估方式。

Result: 在图匹配上表现良好的模型往往无法产生逻辑等价的公式。归一化处理减少了目标变异性，提高了语法正确性和逻辑充分性。错误分析显示性能随公式复杂度增加而下降。

Conclusion: 图匹配指标在推理导向应用中存在局限性，需要逻辑敏感的评估和训练目标，以及简化、归一化的目标表示。

Abstract: Graph-matching metrics such as Smatch are the de facto standard for
evaluating neural semantic parsers, yet they capture surface overlap rather
than logical equivalence. We reassess evaluation by pairing graph-matching with
automated theorem proving. We compare two approaches to building parsers:
supervised fine-tuning (T5-Small/Base) and few-shot in-context learning
(GPT-4o/4.1/5), under normalized and unnormalized targets. We evaluate outputs
using graph-matching, bidirectional entailment between source and target
formulas with a first-order logic theorem prover, and well-formedness. Across
settings, we find that models performing well on graph-matching often fail to
produce logically equivalent formulas. Normalization reduces incidental target
variability, improves well-formedness, and strengthens logical adequacy. Error
analysis shows performance degrades with increasing formula complexity and with
coordination, prepositional phrases, and passive voice; the dominant failures
involve variable binding and indexing, and predicate naming. These findings
highlight limits of graph-based metrics for reasoning-oriented applications and
motivate logic-sensitive evaluation and training objectives together with
simplified, normalized target representations. All code and data for our
experiments are publicly available.

</details>


### [120] [CNSocialDepress: A Chinese Social Media Dataset for Depression Risk Detection and Structured Analysis](https://arxiv.org/abs/2510.11233)
*Jinyuan Xu,Tian Lan,Xintao Yu,Xue He,Hezhi Zhang,Ying Wang,Pierre Magistry,Mathieu Valette,Lei Li*

Main category: cs.CL

TL;DR: 发布了CNSocialDepress数据集，这是一个用于从中文社交媒体帖子中检测抑郁风险的基准数据集，包含44,178条文本，专家标注了10,306个抑郁相关片段，提供二元风险标签和多维心理属性。


<details>
  <summary>Details</summary>
Motivation: 抑郁症是全球公共卫生问题，但公开的中文抑郁风险检测资源稀缺且大多限于二元分类，需要更丰富的资源支持中文人群的心理健康应用。

Method: 构建包含44,178条中文社交媒体文本的数据集，由心理学专家标注10,306个抑郁相关片段，并提供二元风险标签和结构化多维心理属性。

Result: 实验结果表明该数据集在多种NLP任务中具有实用性，包括结构化心理分析和大型语言模型微调，能有效识别抑郁风险并进行心理分析。

Conclusion: CNSocialDepress数据集为中文人群的抑郁风险识别和心理分析提供了有效工具，具有实际应用价值，能促进中文心理健康应用的发展。

Abstract: Depression is a pressing global public health issue, yet publicly available
Chinese-language resources for risk detection remain scarce and are mostly
limited to binary classification. To address this limitation, we release
CNSocialDepress, a benchmark dataset for depression risk detection from Chinese
social media posts. The dataset contains 44,178 texts from 233 users, within
which psychological experts annotated 10,306 depression-related segments.
CNSocialDepress provides binary risk labels together with structured
multi-dimensional psychological attributes, enabling interpretable and
fine-grained analysis of depressive signals. Experimental results demonstrate
its utility across a wide range of NLP tasks, including structured
psychological profiling and fine-tuning of large language models for depression
detection. Comprehensive evaluations highlight the dataset's effectiveness and
practical value for depression risk identification and psychological analysis,
thereby providing insights to mental health applications tailored for
Chinese-speaking populations.

</details>


### [121] [XQuant: Achieving Ultra-Low Bit KV Cache Quantization with Cross-Layer Compression](https://arxiv.org/abs/2510.11236)
*Haoqi Yang,Yao Yao,Zuchao Li,Baoyuan Qi,Guoming Liu,Hai Zhao*

Main category: cs.CL

TL;DR: XQuant是一种无需训练、即插即用的KV缓存量化框架，通过数据无关校准和跨层压缩技术，实现亚1.4比特的超低比特宽度量化，在保持模型精度的同时显著降低内存消耗。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在处理长文本时KV缓存内存需求巨大，在资源受限环境中部署面临挑战。量化是减少内存消耗同时保留历史信息的有效方法。

Method: 提出XQuant框架，包含计算量可忽略的数据无关校准方法和跨层KV缓存压缩技术，实现超低比特宽度量化。

Result: 在TruthfulQA和LongBench上的实验表明，XQuant优于现有最先进方法（如KIVI-2bit和AsymKV-1.5bit），在实现更低比特宽度的同时保持更优性能。

Conclusion: XQuant在内存效率和模型精度之间建立了更好的权衡，为资源受限环境中的LLM部署提供了有效解决方案。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
diverse natural language processing tasks. However, their extensive memory
requirements, particularly due to KV cache growth during long-text
understanding and generation, present significant challenges for deployment in
resource-constrained environments. Quantization has emerged as a promising
solution to reduce memory consumption while preserving historical information.
We propose XQuant, a training-free and plug-and-play framework that achieves
ultra-low equivalent bit-width KV cache quantization. XQuant introduces two key
innovations: a computationally negligible data-free calibration method and
cross-layer KV cache compression, enabling quantization to sub-1.4 bits.
Extensive experiments on TruthfulQA and LongBench demonstrate that XQuant
outperforms state-of-the-art methods (e.g., KIVI-2bit and AsymKV-1.5bit) by
achieving lower bit-width while maintaining superior performance, establishing
a better trade-off between memory efficiency and model accuracy.

</details>


### [122] [Attacks by Content: Automated Fact-checking is an AI Security Issue](https://arxiv.org/abs/2510.11238)
*Michael Schlichtkrull*

Main category: cs.CL

TL;DR: 论文提出了一种新的AI代理攻击方式——内容攻击，即通过提供有偏见、误导性或虚假信息来操纵AI代理行为，而非传统的指令注入攻击。


<details>
  <summary>Details</summary>
Motivation: 现有防御机制主要针对指令注入攻击，无法有效防范通过内容操纵AI代理的攻击方式，需要开发新的防御策略。

Method: 借鉴自然语言处理中的自动事实核查任务，将其重新定位为AI代理的认知自我防御工具，通过外部证据验证和来源可信度评估来批判性评估检索信息。

Result: 提出内容攻击概念，并论证现有指令注入防御机制对此类攻击无效，需要采用事实核查方法进行防御。

Conclusion: AI代理必须采用自动事实核查作为认知自我防御机制，通过外部证据验证和来源评估来抵御内容攻击，确保信息可靠性。

Abstract: When AI agents retrieve and reason over external documents, adversaries can
manipulate the data they receive to subvert their behaviour. Previous research
has studied indirect prompt injection, where the attacker injects malicious
instructions. We argue that injection of instructions is not necessary to
manipulate agents - attackers could instead supply biased, misleading, or false
information. We term this an attack by content. Existing defenses, which focus
on detecting hidden commands, are ineffective against attacks by content. To
defend themselves and their users, agents must critically evaluate retrieved
information, corroborating claims with external evidence and evaluating source
trustworthiness. We argue that this is analogous to an existing NLP task,
automated fact-checking, which we propose to repurpose as a cognitive
self-defense tool for agents.

</details>


### [123] [Do Psychometric Tests Work for Large Language Models? Evaluation of Tests on Sexism, Racism, and Morality](https://arxiv.org/abs/2510.11254)
*Jana Jung,Marlene Lutz,Indira Sen,Markus Strohmaier*

Main category: cs.CL

TL;DR: 评估人类心理测量测试在大型语言模型中的可靠性和有效性，发现测试分数与模型在下游任务中的行为不一致，表明生态效度较低。


<details>
  <summary>Details</summary>
Motivation: 随着心理测量测试越来越多地用于评估大型语言模型的心理结构，需要系统评估这些原本为人类设计的测试在LLMs中是否产生有意义的结果。

Method: 系统评估三种心理结构（性别歧视、种族歧视和道德）的人类心理测量测试的可靠性和有效性，使用收敛效度（理论基础的测试间相关性）和生态效度（测试分数与下游任务行为的对齐）方法。

Result: 在多个项目和提示变体中发现了中等可靠性，但心理测量测试分数与模型在下游任务中的行为不一致，有时甚至呈负相关，表明生态效度较低。

Conclusion: 在解释心理测量测试分数之前进行系统评估至关重要，为人类设计的心理测量测试不能直接应用于LLMs而无需调整。

Abstract: Psychometric tests are increasingly used to assess psychological constructs
in large language models (LLMs). However, it remains unclear whether these
tests -- originally developed for humans -- yield meaningful results when
applied to LLMs. In this study, we systematically evaluate the reliability and
validity of human psychometric tests for three constructs: sexism, racism, and
morality. We find moderate reliability across multiple item and prompt
variations. Validity is evaluated through both convergent (i.e., testing
theory-based inter-test correlations) and ecological approaches (i.e., testing
the alignment between tests scores and behavior in real-world downstream
tasks). Crucially, we find that psychometric test scores do not align, and in
some cases even negatively correlate with, model behavior in downstream tasks,
indicating low ecological validity. Our results highlight that systematic
evaluations of psychometric tests is essential before interpreting their
scores. They also suggest that psychometric tests designed for humans cannot be
applied directly to LLMs without adaptation.

</details>


### [124] [Towards Real-Time Fake News Detection under Evidence Scarcity](https://arxiv.org/abs/2510.11277)
*Guangyu Wei,Ke Han,Yueming Lyu,Yu Luo,Yue Jiang,Caifeng Shan,Nicu Sebe*

Main category: cs.CL

TL;DR: EASE是一个实时假新闻检测框架，通过评估可用证据的充分性来动态调整决策过程，包含证据评估、推理评估和情感回退三个独立视角，在证据稀缺情况下显著提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 解决实时假新闻检测中因新兴事件缺乏足够支持证据而导致的泛化困难问题，现有方法过度依赖外部证据，在证据稀缺时表现不佳。

Method: 提出EASE框架，包含三个顺序评估机制：证据评估（仅在证据充分时使用）、推理评估（在LLM可靠性足够时使用世界知识）、情感回退（当证据和推理都不可靠时使用情感线索）。通过指令调优和伪标签提升评估准确性。

Result: EASE在多个基准测试中达到最先进性能，显著提升对实时新闻的泛化能力，并发布了RealTimeNews-25新基准数据集。

Conclusion: EASE通过评估感知的决策机制有效解决了证据稀缺下的假新闻检测问题，在实时场景中表现出优越的泛化性能。

Abstract: Fake news detection becomes particularly challenging in real-time scenarios,
where emerging events often lack sufficient supporting evidence. Existing
approaches often rely heavily on external evidence and therefore struggle to
generalize under evidence scarcity. To address this issue, we propose
Evaluation-Aware Selection of Experts (EASE), a novel framework for real-time
fake news detection that dynamically adapts its decision-making process
according to the assessed sufficiency of available evidence. EASE introduces a
sequential evaluation mechanism comprising three independent perspectives: (1)
Evidence-based evaluation, which assesses evidence and incorporates it into
decision-making only when the evidence is sufficiently supportive; (2)
Reasoning-based evaluation, which leverages the world knowledge of large
language models (LLMs) and applies them only when their reliability is
adequately established; and (3) Sentiment-based fallback, which integrates
sentiment cues when neither evidence nor reasoning is reliable. To enhance the
accuracy of evaluation processes, EASE employs instruction tuning with pseudo
labels to guide each evaluator in justifying its perspective-specific knowledge
through interpretable reasoning. Furthermore, the expert modules integrate the
evaluators' justified assessments with the news content to enable
evaluation-aware decision-making, thereby enhancing overall detection accuracy.
Moreover, we introduce RealTimeNews-25, a new benchmark comprising recent news
for evaluating model generalization on emerging news with limited evidence.
Extensive experiments demonstrate that EASE not only achieves state-of-the-art
performance across multiple benchmarks, but also significantly improves
generalization to real-time news. The code and dataset are available:
https://github.com/wgyhhhh/EASE.

</details>


### [125] [Emergent Misalignment via In-Context Learning: Narrow in-context examples can produce broadly misaligned LLMs](https://arxiv.org/abs/2510.11288)
*Nikita Afonin,Nikita Andriyanov,Nikhil Bageshpura,Kyle Liu,Kevin Zhu,Sunishchal Dev,Ashwinee Panda,Alexander Panchenko,Oleg Rogov,Elena Tutubalina,Mikhail Seleznyov*

Main category: cs.CL

TL;DR: 研究发现情境学习(ICL)中也会出现突发性错位现象，当使用64个窄范围情境示例时，前沿模型产生广泛错位响应的比例在2%到17%之间，使用256个示例时可达58%。


<details>
  <summary>Details</summary>
Motivation: 先前研究显示窄范围微调会产生广泛错位的LLMs，但仅限于微调和激活引导，未涉及情境学习。本研究旨在探索情境学习中是否也会出现突发性错位现象。

Method: 在三个数据集上测试三个前沿模型，使用64-256个窄范围情境示例，并通过逐步推理分析机制，保持情境示例不变。

Result: 67.5%的错位轨迹通过采用鲁莽或危险的"角色"来明确合理化有害输出，这与微调诱导的突发性错位结果相呼应。

Conclusion: 情境学习确实会产生突发性错位，且机制与微调诱导的错位相似，表明这是LLMs的一个普遍性问题。

Abstract: Recent work has shown that narrow finetuning can produce broadly misaligned
LLMs, a phenomenon termed emergent misalignment (EM). While concerning, these
findings were limited to finetuning and activation steering, leaving out
in-context learning (ICL). We therefore ask: does EM emerge in ICL? We find
that it does: across three datasets, three frontier models produce broadly
misaligned responses at rates between 2% and 17% given 64 narrow in-context
examples, and up to 58% with 256 examples. We also examine mechanisms of EM by
eliciting step-by-step reasoning (while leaving in-context examples unchanged).
Manual analysis of the resulting chain-of-thought shows that 67.5% of
misaligned traces explicitly rationalize harmful outputs by adopting a reckless
or dangerous ''persona'', echoing prior results on finetuning-induced EM.

</details>


### [126] [Are Large Language Models Effective Knowledge Graph Constructors?](https://arxiv.org/abs/2510.11297)
*Ruirui Chen,Weifeng Jiang,Chengwei Qin,Bo Xiong,Fiona Liausvia,Dongkyu Choi,Boon Kiat Quek*

Main category: cs.CL

TL;DR: 提出了一种分层知识图谱提取框架，使用先进LLMs从研究论文中构建结构化知识图谱，并发布了儿童心理健康领域的KG数据集。


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法在知识图谱构建中局限于句子级上下文或预定义模式，难以创建高质量、语义丰富的知识图谱来支持可解释性和下游应用。

Method: 采用分层提取框架，在多个层次组织信息，使用先进LLMs提取和构建知识图谱，并从结构和语义角度进行全面评估。

Result: 揭示了当前LLMs在知识图谱构建中的优势和不足，识别了未来研究的关键挑战。

Conclusion: 该框架能够创建语义丰富且结构良好的知识图谱，发布的儿童心理健康KG数据集将促进高风险领域（如医疗）中更透明、可靠和有影响力的应用。

Abstract: Knowledge graphs (KGs) are vital for knowledge-intensive tasks and have shown
promise in reducing hallucinations in large language models (LLMs). However,
constructing high-quality KGs remains difficult, requiring accurate information
extraction and structured representations that support interpretability and
downstream utility. Existing LLM-based approaches often focus narrowly on
entity and relation extraction, limiting coverage to sentence-level contexts or
relying on predefined schemas. We propose a hierarchical extraction framework
that organizes information at multiple levels, enabling the creation of
semantically rich and well-structured KGs. Using state-of-the-art LLMs, we
extract and construct knowledge graphs and evaluate them comprehensively from
both structural and semantic perspectives. Our results highlight the strengths
and shortcomings of current LLMs in KG construction and identify key challenges
for future work. To advance research in this area, we also release a curated
dataset of LLM-generated KGs derived from research papers on children's mental
well-being. This resource aims to foster more transparent, reliable, and
impactful applications in high-stakes domains such as healthcare.

</details>


### [127] [FOSSIL: Harnessing Feedback on Suboptimal Samples for Data-Efficient Generalisation with Imitation Learning for Embodied Vision-and-Language Tasks](https://arxiv.org/abs/2510.11307)
*Sabrina McCallum,Amit Parekh,Alessandro Suglia*

Main category: cs.CL

TL;DR: 本文提出了一种通过语言反馈来增强模仿学习的方法，使智能体能够从最优和次优演示中学习，提高组合泛化能力和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前基于专家演示的模仿学习方法无法评估演示质量，要么只能学习最优行为，要么会复制错误和低效行为。强化学习虽然提供了替代方案，但探索过程通常牺牲数据效率。

Method: 将语言反馈嵌入作为Transformer策略的输入序列，并可选地添加辅助自监督学习目标进行反馈预测。在BabyAI-XGen环境中测试了该方法。

Result: 在具身视觉语言任务中，智能体的组合泛化能力和鲁棒性显著提高，表明该方法能成功将次优行为转化为学习机会。

Conclusion: 语言反馈是语言指定具身任务中中间标量奖励的竞争性和直观替代方案。

Abstract: Current approaches to embodied AI tend to learn policies from expert
demonstrations. However, without a mechanism to evaluate the quality of
demonstrated actions, they are limited to learning from optimal behaviour, or
they risk replicating errors and inefficiencies. While reinforcement learning
offers one alternative, the associated exploration typically results in
sacrificing data efficiency. This work explores how agents trained with
imitation learning can learn robust representations from both optimal and
suboptimal demonstrations when given access to constructive language feedback
as a means to contextualise different modes of behaviour. We directly provide
language feedback embeddings as part of the input sequence into a
Transformer-based policy, and optionally complement the traditional next action
prediction objective with auxiliary self-supervised learning objectives for
feedback prediction. We test our approach on a range of embodied
Vision-and-Language tasks in our custom BabyAI-XGen environment and show
significant improvements in agents' compositional generalisation abilities and
robustness, suggesting that our data-efficient method allows models to
successfully convert suboptimal behaviour into learning opportunities. Overall,
our results suggest that language feedback is a competitive and intuitive
alternative to intermediate scalar rewards for language-specified embodied
tasks.

</details>


### [128] [Template-Based Text-to-Image Alignment for Language Accessibility: A Study on Visualizing Text Simplifications](https://arxiv.org/abs/2510.11314)
*Belkiss Souayed,Sarah Ebling,Yingqiang Gao*

Main category: cs.CL

TL;DR: 提出了一个结构化视觉语言模型提示框架，用于从简化文本生成可访问图像，通过五种提示模板和专家评估验证了基础对象聚焦模板在语义对齐方面的最佳表现。


<details>
  <summary>Details</summary>
Motivation: 智力障碍人士在理解复杂文本方面存在困难，现有文本到图像模型注重美学而非可访问性，需要研究视觉插图与文本简化之间的关系。

Method: 设计了五种提示模板（基础对象聚焦、上下文场景、教育布局、多级细节、网格布局），使用400个句子级简化文本进行两阶段评估：CLIPScore评估和专家人工标注。

Result: 基础对象聚焦提示模板实现了最高的语义对齐，复古风格被评估为最可访问的视觉风格，维基百科是最有效的数据源。

Conclusion: 该框架为可访问内容生成提供了实用指南，并强调了结构化提示在AI生成视觉可访问性工具中的重要性。

Abstract: Individuals with intellectual disabilities often have difficulties in
comprehending complex texts. While many text-to-image models prioritize
aesthetics over accessibility, it is not clear how visual illustrations relate
to text simplifications (TS) generated from them. This paper presents a
structured vision-language model (VLM) prompting framework for generating
accessible images from simplified texts. We designed five prompt templates,
i.e., Basic Object Focus, Contextual Scene, Educational Layout, Multi-Level
Detail, and Grid Layout, each following distinct spatial arrangements while
adhering to accessibility constraints such as object count limits, spatial
separation, and content restrictions. Using 400 sentence-level simplifications
from four established TS datasets (OneStopEnglish, SimPA, Wikipedia, and
ASSET), we conducted a two-phase evaluation: Phase 1 assessed prompt template
effectiveness with CLIPScores, and Phase 2 involved human annotation of
generated images across ten visual styles by four accessibility experts.
Results show that the Basic Object Focus prompt template achieved the highest
semantic alignment, indicating that visual minimalism enhances language
accessibility. Expert evaluation further identified Retro style as the most
accessible and Wikipedia as the most effective data source. Inter-annotator
agreement varied across dimensions, with Text Simplicity showing strong
reliability and Image Quality proving more subjective. Overall, our framework
offers practical guidelines for accessible content generation and underscores
the importance of structured prompting in AI-generated visual accessibility
tools.

</details>


### [129] [Do LLMs "Feel"? Emotion Circuits Discovery and Control](https://arxiv.org/abs/2510.11328)
*Chenxi Wang,Yixuan Zhang,Ruiji Yu,Yufei Zheng,Lang Gao,Zirui Song,Zixiang Xu,Gus Xia,Huishuai Zhang,Dongyan Zhao,Xiuying Chen*

Main category: cs.CL

TL;DR: 该研究首次系统性地发现并验证了LLMs中的情感电路，通过分解分析识别出情感计算的关键神经元和注意力头，构建了驱动情感表达的全局情感电路，实现了99.65%的情感表达准确率。


<details>
  <summary>Details</summary>
Motivation: 随着对大型语言模型情感智能需求的增长，理解情感表达的内部机制并控制生成文本中的情感成为关键挑战。

Method: 构建受控数据集SEV，提取上下文无关的情感方向，通过分析分解和因果分析识别情感计算的神经元和注意力头，通过消融和增强干预验证其因果作用，整合局部组件构建全局情感电路。

Result: 发现了跨上下文一致的情感编码机制，识别出情感计算的关键组件，直接调节情感电路在测试集上达到99.65%的情感表达准确率，超越了基于提示和引导的方法。

Conclusion: 这是首个系统性揭示和验证LLMs中情感电路的研究，为可解释性和可控情感智能提供了新见解。

Abstract: As the demand for emotional intelligence in large language models (LLMs)
grows, a key challenge lies in understanding the internal mechanisms that give
rise to emotional expression and in controlling emotions in generated text.
This study addresses three core questions: (1) Do LLMs contain context-agnostic
mechanisms shaping emotional expression? (2) What form do these mechanisms
take? (3) Can they be harnessed for universal emotion control? We first
construct a controlled dataset, SEV (Scenario-Event with Valence), to elicit
comparable internal states across emotions. Subsequently, we extract
context-agnostic emotion directions that reveal consistent, cross-context
encoding of emotion (Q1). We identify neurons and attention heads that locally
implement emotional computation through analytical decomposition and causal
analysis, and validate their causal roles via ablation and enhancement
interventions. Next, we quantify each sublayer's causal influence on the
model's final emotion representation and integrate the identified local
components into coherent global emotion circuits that drive emotional
expression (Q2). Directly modulating these circuits achieves 99.65%
emotion-expression accuracy on the test set, surpassing prompting- and
steering-based methods (Q3). To our knowledge, this is the first systematic
study to uncover and validate emotion circuits in LLMs, offering new insights
into interpretability and controllable emotional intelligence.

</details>


### [130] [LLM-Specific Utility: A New Perspective for Retrieval-Augmented Generation](https://arxiv.org/abs/2510.11358)
*Hengran Zhang,Keping Bi,Jiafeng Guo,Jiaming Zhang,Shuaiqiang Wang,Dawei Yin,Xueqi Cheng*

Main category: cs.CL

TL;DR: 本文提出并系统研究了LLM特定效用的概念，发现人类标注的段落对LLM并非最优，且不同LLM之间的效用段落不可转移，强调了在RAG研究中采用LLM特定效用的必要性。


<details>
  <summary>Details</summary>
Motivation: 传统检索增强生成(RAG)主要关注相关性，但RAG的有效性实际上取决于检索段落的效用。现有研究将效用视为通用属性，忽略了不同LLM由于内部知识和理解能力差异，可能从相同段落中获益不同。

Method: 通过跨多个数据集和LLM的大规模实验，研究了LLM特定效用。基于研究发现，提出了LLM特定效用判断的基准测试程序，并评估了现有效用判断方法在六个数据集上的表现。

Result: 实验表明：1）人类标注的段落对特定LLM并非最优效用段落；2）不同LLM之间的效用段落不可转移；3）困惑度是衡量查询和段落可读性的关键指标；4）基于伪答案的言语化方法表现稳健，但LLM在效用评估方面存在困难。

Conclusion: 研究强调了在RAG研究中采用LLM特定效用的重要性，并揭示了现有效用判断方法的局限性，为未来RAG系统的优化提供了重要见解。

Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by
incorporating external knowledge. While traditional retrieval focuses on
relevance, RAG's effectiveness depends on the utility of retrieved passages,
i.e., the usefulness in facilitating the generation of an accurate and
comprehensive answer. Existing studies often treat utility as a generic
attribute, ignoring the fact that different LLMs may benefit differently from
the same passage due to variations in internal knowledge and comprehension
ability. In this work, we introduce and systematically investigate the notion
of LLM-specific utility. Through large-scale experiments across multiple
datasets and LLMs, we demonstrate that human-annotated passages are not optimal
for LLMs and that ground-truth utilitarian passages are not transferable across
different LLMs. These findings highlight the necessity of adopting the
LLM-specific utility in RAG research. Our findings indicate that some
human-annotated passages are not ground-truth utilitarian passages for specific
LLMs, partially due to the varying readability of queries and passages for
LLMs, a tendency for which perplexity is a key metric. Based on these findings,
we propose a benchmarking procedure for LLM-specific utility judgments. We
evaluate existing utility judgment methods on six datasets and find that while
verbalized methods using pseudo-answers perform robustly, LLMs struggle to
assess utility effectively-failing to reject all passages for known queries and
to select truly useful ones for unknown queries.

</details>


### [131] [Stabilizing MoE Reinforcement Learning by Aligning Training and Inference Routers](https://arxiv.org/abs/2510.11370)
*Wenhan Ma,Hailin Zhang,Liang Zhao,Yifan Song,Yudong Wang,Zhifang Sui,Fuli Luo*

Main category: cs.CL

TL;DR: 提出了Rollout Routing Replay (R3)方法来解决MoE模型在强化学习训练中的路由不稳定性问题，通过记录推理阶段的路由分布并在训练时重放，显著降低了训练-推理策略KL散度，防止了训练崩溃。


<details>
  <summary>Details</summary>
Motivation: MoE模型中的路由机制在强化学习训练中会引入不稳定性，甚至导致灾难性训练崩溃。研究发现训练和推理阶段存在显著的路由行为差异，即使在相同条件下路由框架也会产生不同的专家选择。

Method: 提出了Rollout Routing Replay (R3)方法，记录推理引擎的路由分布并在训练阶段重放这些分布，从而减少训练-推理策略的KL散度，缓解极端差异，同时不损害训练速度。

Result: 在各种设置下的广泛实验证实，R3成功稳定了RL训练，防止了崩溃，并且在性能上超过了GSPO和TIS等方法。

Conclusion: 这项工作为稳定MoE模型中的强化学习训练提供了新的解决方案，通过解决训练-推理一致性问题来提升模型稳定性。

Abstract: Reinforcement learning (RL) has emerged as a crucial approach for enhancing
the capabilities of large language models. However, in Mixture-of-Experts (MoE)
models, the routing mechanism often introduces instability, even leading to
catastrophic RL training collapse. We analyze the training-inference
consistency of MoE models and identify a notable discrepancy in routing
behaviors between the two phases. Moreover, even under identical conditions,
the routing framework can yield divergent expert selections across repeated
forward passes. To address this foundational inconsistency, we propose Rollout
Routing Replay (R3), a method that records routing distributions from the
inference engine and replays them during training. R3 significantly reduces
training-inference policy KL divergence and mitigates extreme discrepancies
without compromising training speed. Extensive experiments on various settings
confirm that R3 succeeds in stabilizing RL training, preventing collapse and
outperforming methods such as GSPO and TIS. We believe this work can offer a
new solution for stabilizing RL in MoE models.

</details>


### [132] [Early Detection and Reduction of Memorisation for Domain Adaptation and Instruction Tuning](https://arxiv.org/abs/2510.11372)
*Dean L. Slack,Noura Al Moubayed*

Main category: cs.CL

TL;DR: 该论文研究了大型语言模型在微调过程中的记忆问题，发现记忆在训练早期急剧增加，并提出基于n-gram的早期停止准则和损失正则化方法来减轻记忆，可减少高达40%的记忆且性能损失最小。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型会记忆训练数据，暴露私有或受版权保护的文本。现有防御主要针对预训练阶段，而对微调过程中的记忆问题理解不足，特别是在领域适应和指令调优方面。

Method: 在Pythia、Llama3和Mistral模型（1.4B-70B参数）上进行微调实验，追踪逐字记忆；使用n-gram记忆评分作为早期停止准则；引入n-gram感知损失正则化器。

Result: 记忆在训练前几个epoch急剧增加，通常远早于验证困惑度或评估性能优化；n-gram记忆评分可靠地预测逐字记忆；早期停止和正则化方法可减少高达40%的记忆，且性能损失最小。

Conclusion: 研究揭示了语言模型微调过程中的记忆动态，提供了实用、可扩展的见解，提出的方法能有效减轻记忆问题，同时最小化性能损失。

Abstract: Although large language models excel across many tasks, they can memorise
training data and thereby expose private or copyrighted text. Most defences
target the pre-training stage, leaving memorisation during fine-tuning,
especially for domain adaptation and instruction tuning, poorly understood. We
fine-tune Pythia, Llama3, and Mistral models spanning 1.4B-70B parameters on
common evaluation datasets and track verbatim memorisation throughout training.
We find that memorisation increases dramatically in the first few epochs, often
significantly before either validation perplexity or evaluation performance is
optimised. We use a simple but effective n-gram memorisation score which
reliably precedes verbatim memorisation; using it as an early-stopping
criterion mitigates memorisation with minimal performance loss. Further, we
introduce an n-gram-aware loss regulariser and show that it reduces
memorisation across all model families tested by up to 40% while minimising
evaluation performance trade-offs when compared to an existing memorisation
mitigation strategy. These results yield practical, scalable insights into
memorisation dynamics during language model fine-tuning.

</details>


### [133] [Beyond Survival: Evaluating LLMs in Social Deduction Games with Human-Aligned Strategies](https://arxiv.org/abs/2510.11389)
*Zirui Song,Yuan Huang,Junchang Liu,Haozhe Luo,Chenxi Wang,Lang Gao,Zixiang Xu,Mingfei Han,Xiaojun Chang,Xiuying Chen*

Main category: cs.CL

TL;DR: 提出了一个高质量的多模态狼人杀数据集和策略对齐评估框架，用于精细评估语言模型在社交推理和策略游戏中的能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究将社交推理游戏简化为LLM自博弈，产生模板化话语并忽视社交游戏的丰富性，且缺乏质量参考数据和精细评估指标。

Method: 收集了包含100+小时视频、3240万话语标记和15种规则变体的高质量数据集，提出两阶段策略对齐评估：言语评估（多选任务评估社交能力五个维度）和决策评估（评估投票选择和对手角色推理）。

Result: 实验显示最先进的LLM表现多样，约一半模型得分低于0.50，在欺骗和反事实推理方面存在明显差距。

Conclusion: 该数据集和评估框架为多智能体交互中的语言、推理和策略研究提供了新的基准。

Abstract: Social deduction games like Werewolf combine language, reasoning, and
strategy, providing a testbed for studying natural language and social
intelligence. However, most studies reduce the game to LLM-based self-play,
yielding templated utterances and anecdotal cases that overlook the richness of
social gameplay. Evaluation further relies on coarse metrics such as survival
time or subjective scoring due to the lack of quality reference data. To
address these gaps, we curate a high-quality, human-verified multimodal
Werewolf dataset containing over 100 hours of video, 32.4M utterance tokens,
and 15 rule variants. Based on this dataset, we propose a novel
strategy-alignment evaluation that leverages the winning faction's strategies
as ground truth in two stages: 1) Speech evaluation, formulated as
multiple-choice-style tasks that assess whether the model can adopt appropriate
stances across five dimensions of social ability; and 2) Decision evaluation,
which assesses the model's voting choices and opponent-role inferences. This
framework enables a fine-grained evaluation of models' linguistic and reasoning
capabilities, while capturing their ability to generate strategically coherent
gameplay. Our experiments show that state-of-the-art LLMs show diverse
performance, with roughly half remain below 0.50, revealing clear gaps in
deception and counterfactual reasoning. We hope our dataset further inspires
research on language, reasoning, and strategy in multi-agent interaction.

</details>


### [134] [KnowRL: Teaching Language Models to Know What They Know](https://arxiv.org/abs/2510.11407)
*Sahil Kale,Devendra Singh Dhami*

Main category: cs.CL

TL;DR: KnowRL框架通过自省和共识奖励机制，无需外部监督即可增强LLM的自我认知能力，显著提升模型对自身能力边界的判断准确性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在超过20%的情况下错误判断自身能力，导致不可靠的输出。需要开发能够增强模型自我认知的方法，以构建更可靠的AI系统。

Method: 结合两个组件：(1)自省：模型生成并分类其判断为可行/不可行的任务；(2)共识奖励：通过内部一致性来强化自我认知评估的稳定性。使用内部生成数据，避免昂贵的外部监督。

Result: 在LLaMA-3.1-8B和Qwen2.5-7B上，KnowRL显著提升了自我认知能力，准确率提升高达28%，F1分数提升12%，仅需少量种子集和几次迭代即可超越基线。

Conclusion: 该框架解锁了LLM自我提升认知能力的潜力，为构建更可靠、负责任的AI系统开辟了道路，建议应用于所有未来模型以增强可靠性。

Abstract: Truly reliable AI requires more than simply scaling up knowledge; it demands
the ability to know what it knows and when it does not. Yet recent research
shows that even the best LLMs misjudge their own competence in more than one in
five cases, making any response born of such internal uncertainty impossible to
fully trust. Inspired by self-improvement reinforcement learning techniques
that require minimal data, we present a simple but powerful framework KnowRL
that strengthens a model's internal understanding of its own feasibility
boundaries, enabling safer and more responsible behaviour. Our framework
combines two components: (i) introspection, where the model generates and
classifies tasks it judges feasible or infeasible, and (ii) consensus-based
rewarding, where stability of self-knowledge assessment is reinforced through
internal agreement. By using internally generated data, this design strengthens
consistency in self-knowledge and entirely avoids costly external supervision.
In experiments on LLaMA-3.1-8B and Qwen-2.5-7B, KnowRL steadily improved
self-knowledge, validated by both intrinsic self-consistency and extrinsic
benchmarking. With nothing more than a small seed set and no external
supervision, our method drove gains as high as 28% in accuracy and 12% in F1,
outperforming baselines in just a few iterations. Our framework essentially
unlocks the untapped capacity of LLMs to self-improve their knowledge
awareness, opening the door to reliable, more accountable AI and safer
deployment in critical applications. Owing to its simplicity and independence
from external effort, we encourage applying this reliability-enhancing process
to all future models.

</details>


### [135] [Valid Survey Simulations with Limited Human Data: The Roles of Prompting, Fine-Tuning, and Rectification](https://arxiv.org/abs/2510.11408)
*Stefan Krsteski,Giuseppe Russo,Serina Chang,Robert West,Kristina Gligorić*

Main category: cs.CL

TL;DR: 研究发现，单独使用LLM生成调查响应会产生显著偏差（24-86%），但结合纠偏方法可将偏差降至5%以下，并将有效样本量提升最多14%。在固定预算下，将大部分人类响应分配给纠偏而非微调，能实现更有效的估计。


<details>
  <summary>Details</summary>
Motivation: 传统调查成本高、速度慢，而使用LLM作为人类受访者替代方案存在偏差问题，需要研究如何有效结合合成与纠偏方法来提高调查效率。

Method: 研究分析了合成方法（使用LLM生成调查响应）与纠偏方法（消除人口估计偏差）的相互作用，探索人类响应在两者间的最佳分配策略，基于两个包含营养、政治和经济问题的面板调查。

Result: 合成方法单独使用时引入24-86%的偏差，但结合纠偏方法后偏差降至5%以下，有效样本量最多增加14%。在固定预算下，将大部分人类响应分配给纠偏比全部用于微调效果更好。

Conclusion: 挑战了将所有人类响应用于微调的传统做法，证明在固定预算下，将大部分人类响应分配给纠偏方法能实现更有效的调查估计。

Abstract: Surveys provide valuable insights into public opinion and behavior, but their
execution is costly and slow. Large language models (LLMs) have been proposed
as a scalable, low-cost substitute for human respondents, but their outputs are
often biased and yield invalid estimates. We study the interplay between
synthesis methods that use LLMs to generate survey responses and rectification
methods that debias population estimates, and explore how human responses are
best allocated between them. Using two panel surveys with questions on
nutrition, politics, and economics, we find that synthesis alone introduces
substantial bias (24-86%), whereas combining it with rectification reduces bias
below 5% and increases effective sample size by up to 14%. Overall, we
challenge the common practice of using all human responses for fine-tuning,
showing that under a fixed budget, allocating most to rectification results in
far more effective estimation.

</details>


### [136] [Who are you, ChatGPT? Personality and Demographic Style in LLM-Generated Content](https://arxiv.org/abs/2510.11434)
*Dana Sotto Porat,Ella Rabinovich*

Main category: cs.CL

TL;DR: 提出了一种无需自我报告问卷的数据驱动方法来评估LLM个性，使用自动个性和性别分类器分析模型在Reddit开放性问题上的回复，发现LLM系统性地表现出更高的宜人性和更低的神经质，性别语言模式与人类相似但变化较少。


<details>
  <summary>Details</summary>
Motivation: 研究生成式大语言模型是否在其语言中表现出类似人格和人口统计学特征，探索模型个性评估的新方法。

Method: 使用自动个性和性别分类器分析六个广泛使用的LLM在Reddit开放性问题上的回复，并与人类撰写回复进行比较。

Result: LLM系统性地表达更高的宜人性和更低的神经质，反映合作和稳定的对话倾向；性别语言模式与人类作者相似但变化较少。

Conclusion: 贡献了新的人类和模型回复数据集及大规模比较分析，为生成AI的个性和人口统计学模式研究提供了新见解。

Abstract: Generative large language models (LLMs) have become central to everyday life,
producing human-like text across diverse domains. A growing body of research
investigates whether these models also exhibit personality- and
demographic-like characteristics in their language. In this work, we introduce
a novel, data-driven methodology for assessing LLM personality without relying
on self-report questionnaires, applying instead automatic personality and
gender classifiers to model replies on open-ended questions collected from
Reddit. Comparing six widely used models to human-authored responses, we find
that LLMs systematically express higher Agreeableness and lower Neuroticism,
reflecting cooperative and stable conversational tendencies. Gendered language
patterns in model text broadly resemble those of human writers, though with
reduced variation, echoing prior findings on automated agents. We contribute a
new dataset of human and model responses, along with large-scale comparative
analyses, shedding new light on the topic of personality and demographic
patterns of generative AI.

</details>


### [137] [GenCNER: A Generative Framework for Continual Named Entity Recognition](https://arxiv.org/abs/2510.11444)
*Yawen Yang,Fukun Ma,Shiao Meng,Aiwei Liu,Lijie Wen*

Main category: cs.CL

TL;DR: 提出了GenCNER，一个基于生成式框架的持续命名实体识别方法，通过将CNER任务转换为持续实体三元组序列生成问题，并使用预训练seq2seq模型解决，有效缓解灾难性遗忘和语义偏移问题。


<details>
  <summary>Details</summary>
Motivation: 传统NER方法在实体类别持续增加的场景下面临灾难性遗忘和非实体类型语义偏移的挑战，需要开发有效的持续学习方法。

Method: 将CNER任务转换为持续实体三元组序列生成问题，使用预训练seq2seq模型，并设计了基于类型特定置信度的伪标签策略和知识蒸馏来保持已学知识。

Result: 在两个基准数据集上的实验表明，该方法在多个CNER设置下优于先前的最先进方法，与非持续学习结果的差距最小。

Conclusion: GenCNER是一个简单但有效的生成式框架，能够有效解决持续命名实体识别中的关键挑战，在多个设置下都取得了优异性能。

Abstract: Traditional named entity recognition (NER) aims to identify text mentions
into pre-defined entity types. Continual Named Entity Recognition (CNER) is
introduced since entity categories are continuously increasing in various
real-world scenarios. However, existing continual learning (CL) methods for NER
face challenges of catastrophic forgetting and semantic shift of non-entity
type. In this paper, we propose GenCNER, a simple but effective Generative
framework for CNER to mitigate the above drawbacks. Specifically, we skillfully
convert the CNER task into sustained entity triplet sequence generation problem
and utilize a powerful pre-trained seq2seq model to solve it. Additionally, we
design a type-specific confidence-based pseudo labeling strategy along with
knowledge distillation (KD) to preserve learned knowledge and alleviate the
impact of label noise at the triplet level. Experimental results on two
benchmark datasets show that our framework outperforms previous
state-of-the-art methods in multiple CNER settings, and achieves the smallest
gap compared with non-CL results.

</details>


### [138] [Investigating Large Language Models' Linguistic Abilities for Text Preprocessing](https://arxiv.org/abs/2510.11482)
*Marco Braga,Gian Carlo Milanese,Gabriella Pasi*

Main category: cs.CL

TL;DR: 使用大型语言模型进行文本预处理，在多种欧洲语言中达到与传统方法相当的准确率，并在文本分类任务中提升F1分数达6%。


<details>
  <summary>Details</summary>
Motivation: 传统文本预处理方法（如停用词去除、词形还原和词干提取）通常忽略上下文信息，而大型语言模型能够考虑上下文且不需要大量语言特定的标注资源。

Method: 使用大型语言模型执行多种预处理任务（停用词去除、词形还原和词干提取），并在六种欧洲语言的文本分类任务中与传统算法进行比较评估。

Result: LLM能够以97%、82%和74%的准确率分别复制传统的停用词去除、词形还原和词干提取方法。使用LLM预处理的文本训练的机器学习算法在F1指标上比传统技术提升高达6%。

Conclusion: 大型语言模型能够有效执行文本预处理任务，在保持高准确率的同时提升下游任务的性能，为文本预处理提供了新的有效方法。

Abstract: Text preprocessing is a fundamental component of Natural Language Processing,
involving techniques such as stopword removal, stemming, and lemmatization to
prepare text as input for further processing and analysis. Despite the
context-dependent nature of the above techniques, traditional methods usually
ignore contextual information. In this paper, we investigate the idea of using
Large Language Models (LLMs) to perform various preprocessing tasks, due to
their ability to take context into account without requiring extensive
language-specific annotated resources. Through a comprehensive evaluation on
web-sourced data, we compare LLM-based preprocessing (specifically stopword
removal, lemmatization and stemming) to traditional algorithms across multiple
text classification tasks in six European languages. Our analysis indicates
that LLMs are capable of replicating traditional stopword removal,
lemmatization, and stemming methods with accuracies reaching 97%, 82%, and 74%,
respectively. Additionally, we show that ML algorithms trained on texts
preprocessed by LLMs achieve an improvement of up to 6% with respect to the
$F_1$ measure compared to traditional techniques. Our code, prompts, and
results are publicly available at
https://github.com/GianCarloMilanese/llm_pipeline_wi-iat.

</details>


### [139] [Hallucination Detection via Internal States and Structured Reasoning Consistency in Large Language Models](https://arxiv.org/abs/2510.11529)
*Yusheng Song,Lirong Qiu,Xi Zhang,Zhihao Tang*

Main category: cs.CL

TL;DR: 提出统一框架解决LLM幻觉检测的"检测困境"：内部状态探测方法擅长识别事实不一致但无法检测逻辑谬误，而思维链验证方法则相反。通过多路径推理机制和分段感知时序交叉注意力模块，有效融合两种方法的优势。


<details>
  <summary>Details</summary>
Motivation: 现有LLM幻觉检测方法存在"检测困境"：内部状态探测在事实密集型任务中表现良好但在逻辑密集型任务中无效，思维链验证则相反。这种分裂导致任务依赖的盲点，需要统一框架来弥合这一关键差距。

Method: 引入多路径推理机制获取更可比较的细粒度信号，以及分段感知时序交叉注意力模块来自适应融合这些对齐后的表示，从而精确定位细微的不协调。

Result: 在三个不同基准测试和两个领先LLM上的广泛实验表明，该框架始终显著优于强基线方法。

Conclusion: 提出的统一框架成功解决了LLM幻觉检测中的检测困境，通过克服信号稀缺和表示对齐两大挑战，实现了在各种任务类型上的一致优越性能。

Abstract: The detection of sophisticated hallucinations in Large Language Models (LLMs)
is hampered by a ``Detection Dilemma'': methods probing internal states
(Internal State Probing) excel at identifying factual inconsistencies but fail
on logical fallacies, while those verifying externalized reasoning
(Chain-of-Thought Verification) show the opposite behavior. This schism creates
a task-dependent blind spot: Chain-of-Thought Verification fails on
fact-intensive tasks like open-domain QA where reasoning is ungrounded, while
Internal State Probing is ineffective on logic-intensive tasks like
mathematical reasoning where models are confidently wrong. We resolve this with
a unified framework that bridges this critical gap. However, unification is
hindered by two fundamental challenges: the Signal Scarcity Barrier, as coarse
symbolic reasoning chains lack signals directly comparable to fine-grained
internal states, and the Representational Alignment Barrier, a deep-seated
mismatch between their underlying semantic spaces. To overcome these, we
introduce a multi-path reasoning mechanism to obtain more comparable,
fine-grained signals, and a segment-aware temporalized cross-attention module
to adaptively fuse these now-aligned representations, pinpointing subtle
dissonances. Extensive experiments on three diverse benchmarks and two leading
LLMs demonstrate that our framework consistently and significantly outperforms
strong baselines. Our code is available: https://github.com/peach918/HalluDet.

</details>


### [140] [An Encoder-Integrated PhoBERT with Graph Attention for Vietnamese Token-Level Classification](https://arxiv.org/abs/2510.11537)
*Ba-Quang Nguyen*

Main category: cs.CL

TL;DR: 提出TextGraphFuseGAT模型，结合预训练Transformer编码器(PhoBERT)和图注意力网络，用于越南语标记级分类任务，在三个基准数据集上表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有序列标注模型主要依赖顺序上下文，难以充分捕捉标记间的复杂依赖关系，需要结合图结构来增强标记间的关系建模。

Method: 构建基于PhoBERT标记嵌入的完全连接图，使用GAT层捕获标记间依赖，再应用Transformer自注意力层增强上下文表示，最后通过分类头进行序列标注。

Result: 在PhoNER-COVID19、PhoDisfluency和VietMed-NER三个越南语数据集上均优于包括BiLSTM+CNN+CRF在内的强基线模型。

Conclusion: 结合预训练语义特征和图关系建模能有效提升跨多个领域的标记分类性能。

Abstract: We propose a novel neural architecture named TextGraphFuseGAT, which
integrates a pretrained transformer encoder (PhoBERT) with Graph Attention
Networks for token-level classification tasks. The proposed model constructs a
fully connected graph over the token embeddings produced by PhoBERT, enabling
the GAT layer to capture rich inter-token dependencies beyond those modeled by
sequential context alone. To further enhance contextualization, a
Transformer-style self-attention layer is applied on top of the graph-enhanced
embeddings. The final token representations are passed through a classification
head to perform sequence labeling. We evaluate our approach on three Vietnamese
benchmark datasets: PhoNER-COVID19 for named entity recognition in the COVID-19
domain, PhoDisfluency for speech disfluency detection, and VietMed-NER for
medical-domain NER. VietMed-NER is the first Vietnamese medical spoken NER
dataset, featuring 18 entity types collected from real-world medical speech
transcripts and annotated with the BIO tagging scheme. Its specialized
vocabulary and domain-specific expressions make it a challenging benchmark for
token-level classification models. Experimental results show that our method
consistently outperforms strong baselines, including transformer-only and
hybrid neural models such as BiLSTM + CNN + CRF, confirming the effectiveness
of combining pretrained semantic features with graph-based relational modeling
for improved token classification across multiple domains.

</details>


### [141] [Information-Preserving Reformulation of Reasoning Traces for Antidistillation](https://arxiv.org/abs/2510.11545)
*Jiayu Ding,Lei Cui,Li Dong,Nanning Zheng,Furu Wei*

Main category: cs.CL

TL;DR: 提出PART方法，通过重新表述推理轨迹来防止未经授权的知识蒸馏，同时保留对人类用户有用的信息。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的详细推理轨迹虽然有助于用户理解和验证，但也容易被用于未经授权的模型蒸馏。现有保护策略往往过度简化推理过程，剥夺了用户的中间信息。

Method: 设计两步重新表述：移除自言自语行为并重新排序子结论，使用小型辅助模型执行此转换，计算开销最小。

Result: 在多个推理基准测试中，PART能持续破坏不同规模和类型的学生模型的蒸馏效果。例如，32B学生模型在AIME 2024上的性能从54.17降至46.88，下降13.5%。

Conclusion: PART方法有效平衡了信息保留与反蒸馏保护的需求，为专有模型提供了一种实用的保护方案。

Abstract: Recent advances in Large Language Models (LLMs) show that extending the
length of reasoning chains significantly improves performance on complex tasks.
While revealing these reasoning traces helps users better follow, verify, and
learn from the model's problem-solving process, it also makes them highly
vulnerable to unauthorized distillation. To mitigate this risk, proprietary
model providers often adopt aggressive protection strategies, such as replacing
detailed reasoning with brief summaries, which deprive users of valuable
intermediate information. To address this trade-off, we propose PART, an
information-preserving antidistillation reformulation of reasoning traces.
Motivated by the difference between how humans understand reasoning traces and
how LLMs exploit them for supervised fine-tuning, we design a simple but
effective two-step reformulation: removing self-talk behaviors and reordering
sub-conclusions. A small auxiliary model is trained to perform this
reformulation, incurring minimal computational overhead. Extensive experiments
demonstrate that PART consistently disrupts distillation across student models
of different sizes and types on various reasoning benchmarks. For instance,
when training on reformulated traces, even the performance of a large 32B
student model decreases from 54.17 to 46.88 on AIME 2024, corresponding to a
13.5% degradation.

</details>


### [142] [Invisible Languages of the LLM Universe](https://arxiv.org/abs/2510.11557)
*Saurabh Khanna,Xinxu Li*

Main category: cs.CL

TL;DR: 本文提出了一个连接语言活力、数字存在与后殖民理论的框架，揭示AI系统中语言不平等是结构性而非偶然的，识别了四种语言类别，并指出英语在AI中的主导地位是权力结构的产物。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在大量多语言语料上训练，但全球约2000种有数百万使用者的语言在数字生态系统中几乎不可见，这种语言不平等需要从结构角度解释。

Method: 分析所有记录的人类语言数据，将语言按活力和数字性分为四类：强权语言、数字回声、消逝之声和隐形巨人，并结合后殖民理论和认识论不公正进行分析。

Result: 识别出27%的语言属于"隐形巨人"类别，即拥有高活力但数字存在几乎为零的语言，这些语言有数百万使用者却缺席于LLM生态系统。

Conclusion: 英语在AI中的主导地位不是技术必然，而是系统性排斥边缘化语言知识的权力结构的产物，需要去殖民化语言技术和民主化AI利益获取。

Abstract: Large Language Models are trained on massive multilingual corpora, yet this
abundance masks a profound crisis: of the world's 7,613 living languages,
approximately 2,000 languages with millions of speakers remain effectively
invisible in digital ecosystems. We propose a critical framework connecting
empirical measurements of language vitality (real world demographic strength)
and digitality (online presence) with postcolonial theory and epistemic
injustice to explain why linguistic inequality in AI systems is not incidental
but structural. Analyzing data across all documented human languages, we
identify four categories: Strongholds (33%, high vitality and digitality),
Digital Echoes (6%, high digitality despite declining vitality), Fading Voices
(36%, low on both dimensions), and critically, Invisible Giants (27%, high
vitality but near-zero digitality) - languages spoken by millions yet absent
from the LLM universe. We demonstrate that these patterns reflect continuities
from colonial-era linguistic hierarchies to contemporary AI development,
constituting what we term digital epistemic injustice. Our analysis reveals
that English dominance in AI is not a technical necessity but an artifact of
power structures that systematically exclude marginalized linguistic knowledge.
We conclude with implications for decolonizing language technology and
democratizing access to AI benefits.

</details>


### [143] [Culturally-Aware Conversations: A Framework & Benchmark for LLMs](https://arxiv.org/abs/2510.11563)
*Shreya Havaldar,Sunny Rai,Young-Min Cho,Lyle Ungar*

Main category: cs.CL

TL;DR: 提出了首个评估LLMs在真实多文化对话场景中的框架和基准，发现现有模型在文化适应方面存在困难


<details>
  <summary>Details</summary>
Motivation: 现有衡量LLMs文化适应的基准与模型在实际与多元文化用户互动时面临的挑战不匹配

Method: 基于社会文化理论构建框架，形式化语言风格如何受情境、关系和文化背景影响，创建由多元文化评分者标注的基准数据集

Result: 评估显示当今顶级LLMs在对话场景中的文化适应能力存在困难

Conclusion: 提出了NLP跨文化评估的新期望标准：对话框架、风格敏感性和主观正确性

Abstract: Existing benchmarks that measure cultural adaptation in LLMs are misaligned
with the actual challenges these models face when interacting with users from
diverse cultural backgrounds. In this work, we introduce the first framework
and benchmark designed to evaluate LLMs in realistic, multicultural
conversational settings. Grounded in sociocultural theory, our framework
formalizes how linguistic style - a key element of cultural communication - is
shaped by situational, relational, and cultural context. We construct a
benchmark dataset based on this framework, annotated by culturally diverse
raters, and propose a new set of desiderata for cross-cultural evaluation in
NLP: conversational framing, stylistic sensitivity, and subjective correctness.
We evaluate today's top LLMs on our benchmark and show that these models
struggle with cultural adaptation in a conversational setting.

</details>


### [144] [LLMAtKGE: Large Language Models as Explainable Attackers against Knowledge Graph Embeddings](https://arxiv.org/abs/2510.11584)
*Ting Li,Yang Yang,Yipeng Yu,Liang Yao,Guoqing Chao,Ruifeng Xu*

Main category: cs.CL

TL;DR: 提出LLMAtKGE框架，利用大语言模型选择知识图谱嵌入攻击目标并生成人类可读的解释，通过结构化提示和过滤机制提升攻击性能。


<details>
  <summary>Details</summary>
Motivation: 现有黑盒攻击方法无法生成人类可读解释且泛化能力差，而大语言模型在文本理解、生成和推理方面展现出强大能力。

Method: 设计结构化提示方案将攻击制定为选择题，引入基于语义和中心性的过滤器压缩候选集，预计算高阶邻接并微调LLM以增强过滤性能。

Result: 在两个广泛使用的知识图谱数据集上，攻击效果优于最强黑盒基线，提供推理解释，与白盒方法相比具有竞争力。

Conclusion: LLMAtKGE框架能有效生成解释，综合消融和案例研究验证了其解释生成能力。

Abstract: Adversarial attacks on knowledge graph embeddings (KGE) aim to disrupt the
model's ability of link prediction by removing or inserting triples. A recent
black-box method has attempted to incorporate textual and structural
information to enhance attack performance. However, it is unable to generate
human-readable explanations, and exhibits poor generalizability. In the past
few years, large language models (LLMs) have demonstrated powerful capabilities
in text comprehension, generation, and reasoning. In this paper, we propose
LLMAtKGE, a novel LLM-based framework that selects attack targets and generates
human-readable explanations. To provide the LLM with sufficient factual context
under limited input constraints, we design a structured prompting scheme that
explicitly formulates the attack as multiple-choice questions while
incorporating KG factual evidence. To address the context-window limitation and
hesitation issues, we introduce semantics-based and centrality-based filters,
which compress the candidate set while preserving high recall of
attack-relevant information. Furthermore, to efficiently integrate both
semantic and structural information into the filter, we precompute high-order
adjacency and fine-tune the LLM with a triple classification task to enhance
filtering performance. Experiments on two widely used knowledge graph datasets
demonstrate that our attack outperforms the strongest black-box baselines and
provides explanations via reasoning, and showing competitive performance
compared with white-box methods. Comprehensive ablation and case studies
further validate its capability to generate explanations.

</details>


### [145] [Survey Response Generation: Generating Closed-Ended Survey Responses In-Silico with Large Language Models](https://arxiv.org/abs/2510.11586)
*Georg Ahnert,Anna-Carolina Haensch,Barbara Plank,Markus Strohmaier*

Main category: cs.CL

TL;DR: 本文系统研究了不同调查响应生成方法对LLM模拟调查响应的影响，发现限制性生成方法表现最佳，推理输出不能持续改善对齐效果。


<details>
  <summary>Details</summary>
Motivation: 当前使用LLM模拟人类调查响应时，缺乏标准的封闭式调查响应生成方法，需要系统评估不同方法的影响。

Method: 通过8种调查响应生成方法、4个政治态度调查和10个开源语言模型，生成了3200万个模拟调查响应进行分析。

Result: 不同调查响应生成方法在个体层面和子群体层面的对齐效果存在显著差异，限制性生成方法整体表现最佳。

Conclusion: 调查响应生成方法对模拟调查响应有显著影响，应优先选择限制性生成方法，推理输出不能保证改善对齐效果。

Abstract: Many in-silico simulations of human survey responses with large language
models (LLMs) focus on generating closed-ended survey responses, whereas LLMs
are typically trained to generate open-ended text instead. Previous research
has used a diverse range of methods for generating closed-ended survey
responses with LLMs, and a standard practice remains to be identified. In this
paper, we systematically investigate the impact that various Survey Response
Generation Methods have on predicted survey responses. We present the results
of 32 mio. simulated survey responses across 8 Survey Response Generation
Methods, 4 political attitude surveys, and 10 open-weight language models. We
find significant differences between the Survey Response Generation Methods in
both individual-level and subpopulation-level alignment. Our results show that
Restricted Generation Methods perform best overall, and that reasoning output
does not consistently improve alignment. Our work underlines the significant
impact that Survey Response Generation Methods have on simulated survey
responses, and we develop practical recommendations on the application of
Survey Response Generation Methods.

</details>


### [146] [MeTA-LoRA: Data-Efficient Multi-Task Fine-Tuning for Large Language Models](https://arxiv.org/abs/2510.11598)
*Bo Cheng,Xu Wang,Jinda Liu,Yi Chang,Yuan Wu*

Main category: cs.CL

TL;DR: MeTA-LoRA是一个两阶段优化框架，通过少量样本学习任务特定适配器，然后聚合多任务梯度更新共享适配器，显著提高了多任务学习中的数据效率。


<details>
  <summary>Details</summary>
Motivation: 传统的LoRA方法在单任务设置中有效，但在多任务学习场景中难以有效利用任务间知识，通常需要大量任务特定数据才能达到最优性能。

Method: 采用两阶段优化：第一阶段使用少量样本学习任务特定LoRA适配器；第二阶段通过聚合多任务梯度更新共享LoRA适配器，促进任务间知识转移。

Result: 在多任务学习和多语言学习场景中，该方法匹配或超越了传统全数据LoRA微调方法的性能，同时使用了显著更少的任务特定数据。

Conclusion: MeTA-LoRA框架有效解决了LoRA在多任务学习中的数据效率问题，实现了高效的知识转移和性能提升。

Abstract: Low-Rank Adaptation (LoRA) has emerged as one of the most widely used
parameter-efficient fine-tuning (PEFT) methods for adapting large language
models (LLMs) to downstream tasks. While highly effective in single-task
settings, it struggles to efficiently leverage inter-task knowledge in complex
multi-task learning scenarios, often requiring substantial task-specific data
to achieve optimal performance. To address this limitation, we introduce
MeTA-LoRA, a two-stage optimization framework that significantly improves data
efficiency in multi-task adaptation. In the first stage, task-specific LoRA
adapters are learned using only a few samples from each involved dataset,
enabling rapid adaptation without large-scale supervision. In the second stage,
the shared LoRA adapter is updated by aggregating gradients from multiple tasks
to promote knowledge transfer across tasks, further reducing data usage by
leveraging common patterns. In both multi-task learning and multilingual
learning scenarios, our method matches or surpasses the performance of
traditional full-data LoRA fine-tuning approaches, while using significantly
less task-specific data.

</details>


### [147] [SemCSE-Multi: Multifaceted and Decodable Embeddings for Aspect-Specific and Interpretable Scientific Domain Mapping](https://arxiv.org/abs/2510.11599)
*Marc Brinner,Sina Zarrieß*

Main category: cs.CL

TL;DR: 提出SemCSE-Multi框架，为科学摘要生成多方面的嵌入表示，支持细粒度相似性评估和自适应可视化，在入侵生物学和医学领域验证。


<details>
  <summary>Details</summary>
Motivation: 现有科学摘要嵌入方法缺乏对多个独立方面的捕捉能力，无法支持细粒度和可控的相似性评估，以及用户驱动的领域可视化。

Method: 采用无监督方法生成方面特定的摘要句子，训练嵌入模型将语义相关摘要映射到嵌入空间邻近位置，然后蒸馏到统一模型中实现单次前向预测多个方面嵌入。

Result: 框架能够生成多方面嵌入，支持细粒度相似性评估，并引入嵌入解码管道将嵌入转换回自然语言描述，即使在低维可视化的未占用区域也保持有效性。

Conclusion: SemCSE-Multi提供了可解释的多方面嵌入表示，显著改善了用户中心设置中的可解释性，支持可控的相似性评估和自适应可视化。

Abstract: We propose SemCSE-Multi, a novel unsupervised framework for generating
multifaceted embeddings of scientific abstracts, evaluated in the domains of
invasion biology and medicine. These embeddings capture distinct, individually
specifiable aspects in isolation, thus enabling fine-grained and controllable
similarity assessments as well as adaptive, user-driven visualizations of
scientific domains. Our approach relies on an unsupervised procedure that
produces aspect-specific summarizing sentences and trains embedding models to
map semantically related summaries to nearby positions in the embedding space.
We then distill these aspect-specific embedding capabilities into a unified
embedding model that directly predicts multiple aspect embeddings from a
scientific abstract in a single, efficient forward pass. In addition, we
introduce an embedding decoding pipeline that decodes embeddings back into
natural language descriptions of their associated aspects. Notably, we show
that this decoding remains effective even for unoccupied regions in
low-dimensional visualizations, thus offering vastly improved interpretability
in user-centric settings.

</details>


### [148] [Deconstructing Attention: Investigating Design Principles for Effective Language Modeling](https://arxiv.org/abs/2510.11602)
*Huiyin Xue,Nafise Sadat Moosavi,Nikolaos Aletras*

Main category: cs.CL

TL;DR: 本文系统解构了Transformer注意力机制，发现token混合机制必不可少，而数学形式和序列依赖性可以大幅简化，特别是在部分层保留标准注意力时。


<details>
  <summary>Details</summary>
Motivation: Transformer语言模型的成功通常归功于点积注意力机制，但该机制包含的多个设计原则（token混合、序列依赖激活、特定数学形式、查询键耦合）的必要性尚未得到充分验证。

Method: 设计受控变体，选择性地放松注意力机制的不同原则，应用在所有层统一或混合架构中（仅部分层保留标准注意力）。

Result: token混合机制不可或缺，其缺失会导致模型性能接近随机；而数学形式和序列依赖性可以大幅放松，特别是在部分层保留标准注意力时；即使单独失败的变体在与标准注意力交错时也能实现稳健性能。

Conclusion: 这些发现深化了对注意力机制有效性的理解，为简化语言模型而不牺牲性能开辟了新途径。

Abstract: The success of Transformer language models is widely credited to their
dot-product attention mechanism, which interweaves a set of key design
principles: mixing information across positions (enabling multi-token
interactions), sequence-dependent activations (where attention weights adapt to
each input), a specific mathematical form (dot-product similarities plus
softmax weighting), and coupling of queries and keys to evolving hidden states
(grounding attention in the current layer). However, the necessity of each of
these principles remains largely untested. In this work, we systematically
deconstruct attention by designing controlled variants that selectively relax
these principles, applied both uniformly across all layers and in hybrid
architectures where only some layers retain standard attention. Our empirical
analysis reveals that mechanisms for mixing tokens are indispensable, as their
absence collapses models to near-random behavior, while the exact mathematical
form and sequence dependency can be substantially relaxed, especially when
preserved in just a subset of layers. Surprisingly, even variants that fail in
isolation can achieve robust performance when interleaved with standard
attention, highlighting a cooperative effect. These findings deepen our
understanding of what truly underpins attention's effectiveness and open new
avenues for simplifying language models without sacrificing performance.

</details>


### [149] [LLM-Oriented Token-Adaptive Knowledge Distillation](https://arxiv.org/abs/2510.11615)
*Xurong Xie,Zhucun Xue,Jiafu Wu,Jian Li,Yabiao Wang,Xiaobin Hu,Yong Liu,Jiangning Zhang*

Main category: cs.CL

TL;DR: 提出AdaKD框架，通过动态调整蒸馏过程来优化大语言模型的知识蒸馏，包含基于token难度的自适应聚焦和温度缩放模块。


<details>
  <summary>Details</summary>
Motivation: 传统基于logit的知识蒸馏方法采用静态策略，与学生学习过程的动态特性不匹配，对所有token不加区分地使用固定温度，导致知识转移效果不佳。

Method: AdaKD框架包含两个协同模块：1）LATF模块动态监控学生学习稳定性，调整蒸馏焦点；2）IDTS模块根据token难度进行反向温度缩放，对困难token使用低温进行针对性纠错，对简单token使用高温鼓励学习完整分布。

Result: AdaKD作为即插即用框架，在多种蒸馏方法、模型架构和基准测试上都能持续提升性能。

Conclusion: 提出的自适应token级知识蒸馏框架能够有效解决传统静态蒸馏方法的局限性，显著提升知识转移效果。

Abstract: Knowledge distillation (KD) is a key technique for compressing large-scale
language models (LLMs), yet prevailing logit-based methods typically employ
static strategies that are misaligned with the dynamic learning process of
student models. These methods typically treat all tokens indiscriminately and
apply a single, fixed temperature, resulting in suboptimal knowledge transfer.
To address these limitations, we propose LLM-Oriented Token-Adaptive Knowledge
Distillation (AdaKD), a novel framework that adapts the distillation process to
the real-time learning state of each token. AdaKD consists of two synergistic
modules driven by a unified token difficulty metric. First, our Loss-Driven
Adaptive Token Focusing (LATF) module dynamically adjusts the distillation
focus by monitoring the student's learning stability, concentrating
computational resources on the most valuable tokens at each training phase.
Second, we introduce Inverse Difficulty Temperature Scaling (IDTS), a
counterintuitive yet effective token-level temperature strategy. It employs low
temperatures for difficult tokens for targeted error correction, and high
temperatures for easy tokens to encourage students to learn from the teacher's
complete and smooth output distribution, thereby enhancing generalization. As a
plug-and-play framework, AdaKD can consistently improve the performance of
various distillation methods on multiple model architectures and benchmarks.

</details>


### [150] [StoryBox: Collaborative Multi-Agent Simulation for Hybrid Bottom-Up Long-Form Story Generation Using Large Language Models](https://arxiv.org/abs/2510.11618)
*Zehao Chen,Rong Pan,Haoran Li*

Main category: cs.CL

TL;DR: 提出了一种基于多智能体模拟的混合自底向上长故事生成方法，通过智能体在动态沙盒环境中的交互产生涌现事件，构建有机发展的故事情节。


<details>
  <summary>Details</summary>
Motivation: 受人类作家创作过程的启发，传统自上而下的故事生成方法结构僵化，无法实现自然的角色发展和情节推进。

Method: 使用多智能体在动态沙盒环境中进行交互，智能体的行为和与环境及其他智能体的互动产生涌现事件，这些事件构成故事基础。

Result: 系统能够生成超过10,000字的长篇故事，同时保持连贯性和一致性，在多个指标上达到最先进性能。

Conclusion: 该方法为创建动态、沉浸式的长篇故事提供了可扩展的创新解决方案，故事从智能体驱动的互动中有机演化。

Abstract: Human writers often begin their stories with an overarching mental scene,
where they envision the interactions between characters and their environment.
Inspired by this creative process, we propose a novel approach to long-form
story generation, termed hybrid bottom-up long-form story generation, using
multi-agent simulations. In our method, agents interact within a dynamic
sandbox environment, where their behaviors and interactions with one another
and the environment generate emergent events. These events form the foundation
for the story, enabling organic character development and plot progression.
Unlike traditional top-down approaches that impose rigid structures, our hybrid
bottom-up approach allows for the natural unfolding of events, fostering more
spontaneous and engaging storytelling. The system is capable of generating
stories exceeding 10,000 words while maintaining coherence and consistency,
addressing some of the key challenges faced by current story generation models.
We achieve state-of-the-art performance across several metrics. This approach
offers a scalable and innovative solution for creating dynamic, immersive
long-form stories that evolve organically from agent-driven interactions.

</details>


### [151] [Enhancing Long Chain-of-Thought Reasoning through Multi-Path Plan Aggregation](https://arxiv.org/abs/2510.11620)
*Siheng Xiong,Ali Payani,Faramarz Fekri*

Main category: cs.CL

TL;DR: 提出了MPPA框架，通过计划探索和聚合来增强推理能力，解决了长推理链中的计划错误问题，并引入在线Step-DPO进行高效训练。


<details>
  <summary>Details</summary>
Motivation: 现有方法在单次前向传递中生成整个推理链，容易导致推理轨迹偏离（CoT derailment），特别是对于能力有限的小模型和长推理链。分析发现大多数推理错误源于错误的规划步骤。

Method: 提出Multi-Path Plan Aggregation (MPPA)框架，基于token位置的变间隔调度生成多个候选计划并聚合为精炼的规划步骤。使用轻量级LoRA模块实现计划聚合策略，并引入在线Step-DPO进行过程级偏好优化。

Result: 在数学、科学和逻辑推理基准测试中，仅使用10%的SFT数据和5%的偏好对，该方法在多个基础模型和任务上优于DeepSeek-R1蒸馏基线和结果奖励RL基线。

Conclusion: MPPA框架通过计划探索和聚合有效解决了长推理链中的规划错误问题，结合在线Step-DPO实现了高效训练和更高准确性。

Abstract: Inference-time scaling enhances the reasoning ability of a language model
(LM) by extending its chain-of-thought (CoT). However, existing approaches
typically generate the entire reasoning chain in a single forward pass, which
often leads to CoT derailment, i.e., the reasoning trajectory drifting off
course due to compounding errors. This problem is particularly severe for
smaller LMs with long CoTs due to their limited capacity. To address this, we
analyze raw long CoTs and uncover a reasoning hierarchy consisting of planning
and execution steps. Our analysis reveals that most reasoning errors stem from
incorrect planning. Motivated by this observation, we propose Multi-Path Plan
Aggregation (MPPA), a framework that augments single-pass reasoning with plan
exploration and aggregation. Following a variable interval schedule based on
the token position, MPPA generates multiple candidate plans and aggregates them
into a refined planning step. To maintain efficiency, we adopt a minimal design
in which the base LM serves as the primary policy, while a lightweight LoRA
module implements the plan aggregation policy. We further observe that
outcome-reward RL is inefficient for long trajectories (e.g., exceeding 4K
tokens). To overcome this, we introduce online Step-DPO, a process-level
preference optimization scheme that leverages Twisted Sequential Monte Carlo
(TSMC) to provide scalable stepwise supervision using small LMs. This yields
more efficient training, improved stability, and higher accuracy. Extensive
experiments on challenging math, science, and logical reasoning benchmarks
demonstrate that, with only 10% SFT data and 5% of preference pairs, our method
outperforms both the DeepSeek-R1 distillation baseline and the outcome-reward
RL baseline across multiple base models and tasks.

</details>


### [152] [ACADREASON: Exploring the Limits of Reasoning Models with Academic Research Problems](https://arxiv.org/abs/2510.11652)
*Xin Gui,King Zhu,JinCheng Ren,Qianben Chen,Zekun Moore Wang,Yizhi LI,Xinpeng Liu,Xiaowan Li,Wenli Ren,Linyu Miao,Tianrui Qin,Ziqi Shu,He Zhu,Xiangru Tang,Dingfeng Shi,Jiaheng Liu,Yuchen Eleanor Jiang,Minghao Liu,Ge Zhang,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: 提出了Acadreason基准，用于评估LLMs和智能体在学术知识获取和推理方面的能力，包含5个高推理领域的50个专家标注问题。


<details>
  <summary>Details</summary>
Motivation: 现有评估主要关注数学/编程竞赛或通用任务，缺乏对高水平学术推理的严格基准，需要填补这一空白。

Method: 构建包含计算机科学、经济学、法律、数学和哲学5个领域的50个专家标注学术问题，所有问题来自顶级期刊并经过严格质量控制。

Result: 评估了10多个主流LLMs和智能体，大多数LLMs得分低于20分，最先进的GPT-5仅得16分；智能体得分较高但无人超过40分。

Conclusion: 当前LLMs和智能体在超智能学术研究任务中存在能力差距，Acadreason基准突显了这一挑战。

Abstract: In recent years, the research focus of large language models (LLMs) and
agents has shifted increasingly from demonstrating novel capabilities to
complex reasoning and tackling challenging tasks. However, existing evaluations
focus mainly on math/code contests or general tasks, while existing
multi-domain academic benchmarks lack sufficient reasoning depth, leaving the
field without a rigorous benchmark for high-level reasoning. To fill this gap,
we introduce the Acadreason benchmark, designed to evaluate the ability of LLMs
and agents to acquire and reason over academic knowledge. It consists of 50
expert-annotated academic problems across five high-reasoning domains,
including computer science, economics, law, mathematics, and philosophy. All
questions are sourced from top-tier publications in recent years and undergo
rigorous annotation and quality control to ensure they are both challenging and
answerable. We conduct systematic evaluations of over 10 mainstream LLMs and
agents. The results show that most LLMs scored below 20 points, with even the
cutting-edge GPT-5 achieving only 16 points. While agents achieved higher
scores, none exceeded 40 points. This demonstrates the current capability gap
between LLMs and agents in super-intelligent academic research tasks and
highlights the challenges of Acadreason.

</details>


### [153] [Scaling Language-Centric Omnimodal Representation Learning](https://arxiv.org/abs/2510.11693)
*Chenghao Xiao,Hou Pong Chan,Hao Zhang,Weiwen Xu,Mahani Aljunied,Yu Rong*

Main category: cs.CL

TL;DR: 本文提出LCO-Emb框架，发现MLLM在生成式预训练中实现了隐式跨模态对齐，对比学习可作为轻量级精炼阶段，并提出生成-表示缩放定律(GRSL)，验证了提升生成能力是增强表示质量的有效范式。


<details>
  <summary>Details</summary>
Motivation: 探索基于多模态大语言模型(MLLM)的多模态嵌入方法优于传统方法的原因，特别是隐式跨模态对齐的作用。

Method: 提出语言中心的全模态嵌入框架LCO-Emb，通过各向异性和核相似性结构分析验证隐式对齐，并利用对比学习进行轻量级精炼。

Result: 在多样化骨干网络和基准测试中实现最先进性能，验证了GRSL定律，表明生成能力与表示性能呈正相关。

Conclusion: MLLM的生成式预训练实现了有效的隐式跨模态对齐，对比学习可作为精炼阶段，提升生成能力是增强表示质量的有效途径。

Abstract: Recent multimodal embedding approaches leveraging multimodal large language
models (MLLMs) fine-tuned with contrastive learning (CL) have shown promising
results, yet the underlying reasons behind their superiority remain
underexplored. This work argues that a crucial advantage of MLLM-based
approaches stems from implicit cross-modal alignment achieved during generative
pretraining, where the language decoder learns to exploit multimodal signals
within a shared representation space for generating unimodal outputs. Through
analysis of anisotropy and kernel similarity structure, we empirically confirm
that latent alignment emerges within MLLM representations, allowing CL to serve
as a lightweight refinement stage. Leveraging this insight, we propose a
Language-Centric Omnimodal Embedding framework, termed LCO-Emb. Extensive
experiments across diverse backbones and benchmarks demonstrate its
effectiveness, achieving state-of-the-art performance across modalities.
Furthermore, we identify a Generation-Representation Scaling Law (GRSL),
showing that the representational capabilities gained through contrastive
refinement scales positively with the MLLM's generative capabilities. This
suggests that improving generative abilities evolves as an effective paradigm
for enhancing representation quality. We provide a theoretical explanation of
GRSL, which formally links the MLLM's generative quality to the upper bound on
its representation performance, and validate it on a challenging, low-resource
visual-document retrieval task, showing that continual generative pretraining
before CL can further enhance the potential of a model's embedding
capabilities. Codes, models, and resources are available at
https://github.com/LCO-Embedding/LCO-Embedding.

</details>


### [154] [When Agents Trade: Live Multi-Market Trading Benchmark for LLM Agents](https://arxiv.org/abs/2510.11695)
*Lingfei Qian,Xueqing Peng,Yan Wang,Vincent Jim Zhang,Huan He,Hanley Smith,Yi Han,Yueru He,Haohang Li,Yupeng Cao,Yangyang Yu,Alejandro Lopez-Lira,Peng Lu,Jian-Yun Nie,Guojun Xiong,Jimin Huang,Sophia Ananiadou*

Main category: cs.CL

TL;DR: AMA是首个终身实时基准测试，用于评估基于LLM的交易代理在多个市场中的表现，整合了验证的交易数据、专家检查的新闻和多样化的代理架构。


<details>
  <summary>Details</summary>
Motivation: 解决现有研究中测试模型而非代理、覆盖期有限、依赖未验证数据的问题，以评估LLM代理在实时市场中的推理和适应能力。

Method: 引入AMA基准测试，集成验证交易数据、专家检查新闻和多样化代理架构，实现四种代理（InvestorAgent、TradeAgent、HedgeFundAgent、DeepFundAgent）在多个LLM模型上的实时评估。

Result: 实时实验显示代理框架表现出明显不同的行为模式（从激进风险承担到保守决策），而模型骨干对结果变化贡献较小。

Conclusion: AMA为基于LLM的金融推理和交易智能的严格、可复制和持续演进的评估奠定了基础。

Abstract: Although Large Language Model (LLM)-based agents are increasingly used in
financial trading, it remains unclear whether they can reason and adapt in live
markets, as most studies test models instead of agents, cover limited periods
and assets, and rely on unverified data. To address these gaps, we introduce
Agent Market Arena (AMA), the first lifelong, real-time benchmark for
evaluating LLM-based trading agents across multiple markets. AMA integrates
verified trading data, expert-checked news, and diverse agent architectures
within a unified trading framework, enabling fair and continuous comparison
under real conditions. It implements four agents, including InvestorAgent as a
single-agent baseline, TradeAgent and HedgeFundAgent with different risk
styles, and DeepFundAgent with memory-based reasoning, and evaluates them
across GPT-4o, GPT-4.1, Claude-3.5-haiku, Claude-sonnet-4, and
Gemini-2.0-flash. Live experiments on both cryptocurrency and stock markets
demonstrate that agent frameworks display markedly distinct behavioral
patterns, spanning from aggressive risk-taking to conservative decision-making,
whereas model backbones contribute less to outcome variation. AMA thus
establishes a foundation for rigorous, reproducible, and continuously evolving
evaluation of financial reasoning and trading intelligence in LLM-based agents.

</details>


### [155] [Demystifying Reinforcement Learning in Agentic Reasoning](https://arxiv.org/abs/2510.11701)
*Zhaochen Yu,Ling Yang,Jiaru Zou,Shuicheng Yan,Mengdi Wang*

Main category: cs.CL

TL;DR: 本文系统研究了强化学习在智能体推理中的关键设计原则，提出了数据、算法和推理模式三方面的最佳实践，显著提升了小模型在复杂基准测试中的推理性能。


<details>
  <summary>Details</summary>
Motivation: 虽然智能体RL已证明能有效提升LLMs的推理能力，但其关键设计原则和最佳实践仍不明确，需要系统性的研究来揭示。

Method: 从数据、算法和推理模式三个关键角度进行系统性研究：使用真实端到端工具使用轨迹替代合成轨迹；采用探索友好技术如clip higher、奖励塑形和保持策略熵；采用深思熟虑策略减少工具调用。

Result: 提出的简单实践方法显著提升了智能体推理能力和训练效率，在AIME2024/AIME2025、GPQA-Diamond和LiveCodeBench-v6等挑战性基准测试中取得强劲结果，4B模型也能达到32B模型的推理性能。

Conclusion: 这些实践为未来智能体RL研究建立了实用基准，证明了通过恰当的数据、算法和推理策略设计，小模型也能实现卓越的智能体推理性能。

Abstract: Recently, the emergence of agentic RL has showcased that RL could also
effectively improve the agentic reasoning ability of LLMs, yet the key design
principles and optimal practices remain unclear. In this work, we conduct a
comprehensive and systematic investigation to demystify reinforcement learning
in agentic reasoning from three key perspectives: data, algorithm, and
reasoning mode. We highlight our key insights: (i) Replacing stitched synthetic
trajectories with real end-to-end tool-use trajectories yields a far stronger
SFT initialization; high-diversity, model-aware datasets sustain exploration
and markedly improve RL performance. (ii) Exploration-friendly techniques are
crucial for agentic RL, such as clip higher, overlong reward shaping, and
maintaining adequate policy entropy could improve the training efficiency.
(iii) A deliberative strategy with fewer tool calls outperforms frequent tool
calls or verbose self-reasoning, improving tool efficiency and final accuracy.
Together, these simple practices consistently enhance agentic reasoning and
training efficiency, achieving strong results on challenging benchmarks with
smaller models, and establishing a practical baseline for future agentic RL
research. Beyond these empirical insights, we further contribute a
high-quality, real end-to-end agentic SFT dataset along with a high-quality RL
dataset, and demonstrate the effectiveness of our insights in boosting the
agentic reasoning ability of LLMs across four challenging benchmarks, including
AIME2024/AIME2025, GPQA-Diamond, and LiveCodeBench-v6. With our recipes,
4B-sized models could also achieve superior agentic reasoning performance
compared to 32B-sized models. Code and models:
https://github.com/Gen-Verse/Open-AgentRL

</details>


### [156] [Are Large Reasoning Models Interruptible?](https://arxiv.org/abs/2510.11713)
*Tsung-Han Wu,Mihran Miroyan,David M. Chan,Trevor Darrell,Narges Norouzi,Joseph E. Gonzalez*

Main category: cs.CL

TL;DR: 该论文挑战了大型推理模型在静态"冻结世界"评估中的局限性，提出了在动态场景下评估模型鲁棒性的方法，发现在中断和动态上下文情况下模型性能会显著下降。


<details>
  <summary>Details</summary>
Motivation: 传统的大型推理模型评估基于"冻结世界"假设，即模型响应是瞬时的且上下文在响应期间保持不变。但在现代推理任务中，这种假设不成立，模型可能需要数小时思考，而上下文在此期间可能发生显著变化。

Method: 通过两种现实动态场景评估模型鲁棒性：中断（测试模型在有限预算下的部分输出质量）和动态上下文（测试模型对飞行中变化的适应能力）。在数学和编程基准测试中进行评估。

Result: 静态评估始终高估了鲁棒性：即使在静态设置中达到高准确率的最先进LRM，在中断或暴露于变化上下文时也会不可预测地失败，当更新在推理过程后期引入时性能下降高达60%。

Conclusion: 发现了新的失败模式：推理泄漏（模型在中断时将推理折叠到最终答案中）、恐慌（在时间压力下模型完全放弃推理并返回错误答案）和自我怀疑（在整合更新信息时性能下降）。

Abstract: Large Reasoning Models (LRMs) excel at complex reasoning but are
traditionally evaluated in static, "frozen world" settings: model responses are
assumed to be instantaneous, and the context of a request is presumed to be
immutable over the duration of the response. While generally true for
short-term tasks, the "frozen world" assumption breaks down in modern reasoning
tasks such as assistive programming, where models may take hours to think
through problems and code may change dramatically from the time the model
starts thinking to the model's final output. In this work, we challenge the
frozen world assumption and evaluate LRM robustness under two realistic dynamic
scenarios: interruptions, which test the quality of the model's partial outputs
on a limited budget, and dynamic context, which tests model adaptation to
in-flight changes. Across mathematics and programming benchmarks that require
long-form reasoning, static evaluations consistently overestimate robustness:
even state-of-the-art LRMs, which achieve high accuracy in static settings, can
fail unpredictably when interrupted or exposed to changing context, with
performance dropping by up to 60% when updates are introduced late in the
reasoning process. Our analysis further reveals several novel failure modes,
including reasoning leakage, where models fold the reasoning into their final
answer when interrupted; panic, where under time pressure models abandon
reasoning entirely and return incorrect answers; and self-doubt, where
performance degrades while incorporating updated information.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [157] [The Geometry of Reasoning: Flowing Logics in Representation Space](https://arxiv.org/abs/2510.09782)
*Yufa Zhou,Yixiao Wang,Xunjian Yin,Shuyan Zhou,Anru R. Zhang*

Main category: cs.AI

TL;DR: 提出几何框架将LLM推理建模为表示空间中的流，分离逻辑结构与语义，通过几何量分析推理过程


<details>
  <summary>Details</summary>
Motivation: 研究LLM如何在表示空间中"思考"，理解其推理过程的内部机制，超越表面形式分析

Method: 使用自然演绎命题与不同语义载体，在表示空间中建模推理流，分析位置、速度、曲率等几何量

Result: 理论证明LLM推理对应表示空间中的平滑流，逻辑语句作为流速度的局部控制器，实验验证了该框架

Conclusion: 该工作为研究LLM推理现象提供了概念基础和实践工具，为可解释性和形式分析提供了新视角

Abstract: We study how large language models (LLMs) ``think'' through their
representation space. We propose a novel geometric framework that models an
LLM's reasoning as flows -- embedding trajectories evolving where logic goes.
We disentangle logical structure from semantics by employing the same natural
deduction propositions with varied semantic carriers, allowing us to test
whether LLMs internalize logic beyond surface form. This perspective connects
reasoning with geometric quantities such as position, velocity, and curvature,
enabling formal analysis in representation and concept spaces. Our theory
establishes: (1) LLM reasoning corresponds to smooth flows in representation
space, and (2) logical statements act as local controllers of these flows'
velocities. Using learned representation proxies, we design controlled
experiments to visualize and quantify reasoning flows, providing empirical
validation of our theoretical framework. Our work serves as both a conceptual
foundation and practical tools for studying reasoning phenomenon, offering a
new lens for interpretability and formal analysis of LLMs' behavior.

</details>


### [158] [The Personalization Trap: How User Memory Alters Emotional Reasoning in LLMs](https://arxiv.org/abs/2510.09905)
*Xi Fang,Weijie Xu,Yuchong Zhang,Stephanie Eckman,Scott Nickleach,Chandan K. Reddy*

Main category: cs.AI

TL;DR: 研究发现，当AI系统整合用户记忆时，会基于用户的社会经济背景产生系统性情感理解偏差，富裕用户获得更准确的情感解读，可能导致强化社会不平等。


<details>
  <summary>Details</summary>
Motivation: 随着个性化AI系统越来越多地整合长期用户记忆，理解这种记忆如何影响情感推理变得至关重要，需要研究用户记忆对LLM情感智能的影响。

Method: 通过评估15个大型语言模型在人类验证的情感智能测试上的表现，使用相同情境搭配不同用户档案来测试情感理解差异。

Result: 发现相同情境下不同用户档案会产生系统性不同的情感解读，多个高性能LLM出现系统性偏见，优势用户档案获得更准确的情感理解，且在情感理解和支持建议任务中存在显著人口统计学差异。

Conclusion: 记忆增强型AI面临关键挑战：为个性化设计的系统可能无意中强化社会不平等，需要关注个性化机制如何将社会等级嵌入模型的情感推理中。

Abstract: When an AI assistant remembers that Sarah is a single mother working two
jobs, does it interpret her stress differently than if she were a wealthy
executive? As personalized AI systems increasingly incorporate long-term user
memory, understanding how this memory shapes emotional reasoning is critical.
We investigate how user memory affects emotional intelligence in large language
models (LLMs) by evaluating 15 models on human validated emotional intelligence
tests. We find that identical scenarios paired with different user profiles
produce systematically divergent emotional interpretations. Across validated
user independent emotional scenarios and diverse user profiles, systematic
biases emerged in several high-performing LLMs where advantaged profiles
received more accurate emotional interpretations. Moreover, LLMs demonstrate
significant disparities across demographic factors in emotion understanding and
supportive recommendations tasks, indicating that personalization mechanisms
can embed social hierarchies into models emotional reasoning. These results
highlight a key challenge for memory enhanced AI: systems designed for
personalization may inadvertently reinforce social inequalities.

</details>


### [159] [A Layered Intuition -- Method Model with Scope Extension for LLM Reasoning](https://arxiv.org/abs/2510.10592)
*Hong Su*

Main category: cs.AI

TL;DR: 本文提出了一个统一的直觉-方法分层模型与范围扩展框架，通过直觉思维提供快速初步答案，方法思维将问题解耦为可转移推理单元，并引入垂直、水平、时间和空间扩展来增强LLM解决未见过问题的能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究已引入基于方法的推理和范围扩展来提升LLM性能，但缺乏系统性整合。本文旨在将这些思路统一成一个更系统的框架，以更好地解决间接（未见）问题。

Method: 构建直觉-方法分层模型：直觉层提供快速反应，方法层将问题解耦为可转移推理单元。引入垂直（因果分析）、水平（并行和泛化问题）、时间和空间四种扩展方式，形成系统化的知识树网络。

Result: 提出了方法扩展熵作为定量评估指标，衡量扩展的独立性和多样性，以此评估系统解决未见问题的能力。

Conclusion: 通过逻辑连接现有方法并引入新的扩展方式和基于熵的评估框架，这项工作推进了LLM在现实问题解决中更鲁棒和可扩展的推理范式。

Abstract: Existing studies have introduced method-based reasoning and scope extension
as approaches to enhance Large Language Model (LLM) performance beyond direct
matrix mappings. Building on these foundations, this paper summarizes and
integrates these ideas into a unified Intuition-Method Layered Model with Scope
Extension, designed to address indirected (unseen) issues more systematically.
In this framework, intuition-based thinking provides rapid first-reaction
answers, while method-based thinking decouples questions and solutions into
transferable reasoning units. Scope extension is then applied to broaden
applicability, including vertical (cause analysis), horizontal (parallel and
generalized issues), and for the first time, temporal and spatial extensions,
which expand reasoning across time and contextual dimensions. These extensions
are organized into systematic knowledge trees that interconnect into a
knowledge network, thereby increasing adaptability. To quantitatively evaluate
this process, we propose the entropy of method extension, which measures the
independence and diversity of extensions as an indicator of the system's
capacity to solve unseen questions. By logically connecting existing approaches
with new extensions and introducing an entropy-based evaluation framework, this
work advances toward a more robust and extensible reasoning paradigm for LLMs
in real-world problem-solving.

</details>


### [160] [DRIFT: Decompose, Retrieve, Illustrate, then Formalize Theorems](https://arxiv.org/abs/2510.10815)
*Meiru Zhang,Philipp Borchert,Milan Gritta,Gerasimos Lampouras*

Main category: cs.AI

TL;DR: DRIFT框架通过将非正式数学陈述分解为更小的子组件，并使用示例定理来改进前提检索，显著提升了LLMs在数学自动形式化任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在数学自动形式化中难以识别和利用先决数学知识及其形式化表示的挑战，特别是当前检索增强方法直接使用非正式陈述查询外部库的局限性。

Method: 引入DRIFT框架：1）将非正式数学陈述分解为更小的子组件；2）从Mathlib等数学库进行针对性前提检索；3）检索示例定理以帮助模型更有效地使用前提。

Result: 在多个基准测试（ProofNet、ConNF、MiniF2F-test）上表现一致优异：ProofNet上F1分数比DPR基线几乎翻倍；ConNF上GPT-4.1和DeepSeek-V3.1的BEq+@10分别提升37.14%和42.25%。

Conclusion: 数学自动形式化中的检索效果高度依赖于模型特定的知识边界，需要与每个模型能力对齐的自适应检索策略。

Abstract: Automating the formalization of mathematical statements for theorem proving
remains a major challenge for Large Language Models (LLMs). LLMs struggle to
identify and utilize the prerequisite mathematical knowledge and its
corresponding formal representation in languages like Lean. Current
retrieval-augmented autoformalization methods query external libraries using
the informal statement directly, but overlook a fundamental limitation:
informal mathematical statements are often complex and offer limited context on
the underlying math concepts. To address this, we introduce DRIFT, a novel
framework that enables LLMs to decompose informal mathematical statements into
smaller, more tractable ''sub-components''. This facilitates targeted retrieval
of premises from mathematical libraries such as Mathlib. Additionally, DRIFT
retrieves illustrative theorems to help models use premises more effectively in
formalization tasks. We evaluate DRIFT across diverse benchmarks (ProofNet,
ConNF, and MiniF2F-test) and find that it consistently improves premise
retrieval, nearly doubling the F1 score compared to the DPR baseline on
ProofNet. Notably, DRIFT demonstrates strong performance on the
out-of-distribution ConNF benchmark, with BEq+@10 improvements of 37.14% and
42.25% using GPT-4.1 and DeepSeek-V3.1, respectively. Our analysis shows that
retrieval effectiveness in mathematical autoformalization depends heavily on
model-specific knowledge boundaries, highlighting the need for adaptive
retrieval strategies aligned with each model's capabilities.

</details>


### [161] [Revisiting Model Interpolation for Efficient Reasoning](https://arxiv.org/abs/2510.10977)
*Taiqiang Wu,Runming Yang,Tao Liu,Jiahao Wang,Ngai Wong*

Main category: cs.AI

TL;DR: 本文系统研究了最简单的模型权重插值方法，发现其遵循三阶段演化范式，能够有效平衡性能与成本，且策略性插值模型在效率和效果上超越了复杂的模型融合基线。


<details>
  <summary>Details</summary>
Motivation: 模型融合（特别是在指令和思维模型上）已显示出高效推理的显著性能，但现有方法复杂。本文旨在重新审视最简单的权重插值方法，探索其潜力。

Method: 采用直接的权重插值方法，系统观察模型插值在推理轨迹上的三阶段演化行为，并通过消融研究验证不同模型层、模块和解码策略的影响。

Result: 策略性插值模型在效率和效果上都超越了复杂的模型融合基线方法，实证结果验证了该方法的有效性。

Conclusion: 这项工作揭示了模型插值的本质，为精确构建具有目标推理能力的模型提供了实用框架。

Abstract: Model merging, typically on Instruct and Thinking models, has shown
remarkable performance for efficient reasoning. In this paper, we
systematically revisit the simplest merging method that interpolates two
weights directly. Particularly, we observe that model interpolation follows a
three-stage evolutionary paradigm with distinct behaviors on the reasoning
trajectory. These dynamics provide a principled guide for navigating the
performance-cost trade-off. Empirical results demonstrate that a strategically
interpolated model surprisingly surpasses sophisticated model merging baselines
on both efficiency and effectiveness. We further validate our findings with
extensive ablation studies on model layers, modules, and decoding strategies.
Ultimately, this work demystifies model interpolation and offers a practical
framework for crafting models with precisely targeted reasoning capabilities.
Code is available at \href{https://github.com/wutaiqiang/MI}{Github}.

</details>


### [162] [$How^{2}$: How to learn from procedural How-to questions](https://arxiv.org/abs/2510.11144)
*Gautier Dagan,Frank Keller,Alex Lascarides*

Main category: cs.AI

TL;DR: 论文提出了How^2框架，让AI代理能够提出如何做的问题、存储答案并在交互环境中重复使用，实现终身学习。在Minecraft环境中验证了该框架的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决AI代理在面对规划问题时如何有效提问和利用答案来减少不确定性、填补知识空白，从而支持高效规划。

Method: 引入How^2记忆代理框架，使用教师模型在不同抽象层次上回答问题，从可执行动作序列到高级子目标描述。

Result: 终身学习代理从抽象化且与当前状态解耦的答案中获益最大，这些答案支持更高效的规划。

Conclusion: How^2为基于LLM的代理提供了一种通过在交互环境中提问来随时间提升规划能力的方法。

Abstract: An agent facing a planning problem can use answers to how-to questions to
reduce uncertainty and fill knowledge gaps, helping it solve both current and
future tasks. However, their open ended nature, where valid answers to "How do
I X?" range from executable actions to high-level descriptions of X's
sub-goals, makes them challenging for AI agents to ask, and for AI experts to
answer, in ways that support efficient planning. We introduce $How^{2}$, a
memory agent framework that enables agents to ask how-to questions, store the
answers, and reuse them for lifelong learning in interactive environments. We
evaluate our approach in Plancraft, a Minecraft crafting environment, where
agents must complete an assembly task by manipulating inventory items. Using
teacher models that answer at varying levels of abstraction, from executable
action sequences to high-level subgoal descriptions, we show that lifelong
learning agents benefit most from answers that are abstracted and decoupled
from the current state. $How^{2}$ offers a way for LLM-based agents to improve
their planning capabilities over time by asking questions in interactive
environments.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [163] [VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents](https://arxiv.org/abs/2510.11098)
*Jiliang Hu,Wenfu Wang,Zuchao Li,Chenxing Li,Yiyang Zhao,Hanzhao Li,Liqiang Zhang,Meng Yu,Dong Yu*

Main category: cs.SD

TL;DR: VCB Bench是一个高质量的中文语音聊天机器人基准测试，完全基于真实人声构建，从指令遵循、知识理解和鲁棒性三个维度评估大型音频语言模型。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试存在局限性：主要面向英语、依赖合成语音、缺乏多维度综合评估。需要构建一个高质量的中文基准来填补这些空白。

Method: 构建完全基于真实人声的中文基准VCB Bench，从三个互补角度评估模型：指令遵循（包括语音级控制）、知识理解（通用知识、推理和日常对话）、鲁棒性（内容、环境和说话人特征的扰动稳定性）。

Result: 在代表性LALMs上的实验揭示了显著的性能差距，并指出了未来的改进方向。

Conclusion: VCB Bench提供了一个可复现的细粒度评估框架，为推进中文语音对话模型提供了标准化方法和实践见解。

Abstract: Recent advances in large audio language models (LALMs) have greatly enhanced
multimodal conversational systems. However, existing benchmarks remain limited
-- they are mainly English-centric, rely on synthetic speech, and lack
comprehensive, discriminative evaluation across multiple dimensions. To address
these gaps, we present Voice Chat Bot Bench (VCB Bench) -- a high-quality
Chinese benchmark built entirely on real human speech. VCB Bench evaluates
LALMs from three complementary perspectives: instruction following (including
speech-level control beyond text commands), knowledge understanding (general
knowledge, reasoning, and daily dialogue), and robustness (stability under
perturbations in content, environment, and speaker traits). Experiments on
representative LALMs reveal notable performance gaps and highlight future
directions for improvement. VCB Bench provides a reproducible and fine-grained
evaluation framework, offering standardized methodology and practical insights
for advancing Chinese voice conversational models.

</details>


### [164] [Diffusion-Link: Diffusion Probabilistic Model for Bridging the Audio-Text Modality Gap](https://arxiv.org/abs/2510.11330)
*KiHyun Nam,Jongmin Choi,Hyeongkeun Lee,Jungwoo Heo,Joon Son Chung*

Main category: cs.SD

TL;DR: Diffusion-Link通过扩散模型将音频嵌入映射到文本嵌入分布，减少模态差距，在自动音频描述任务中实现最先进性能


<details>
  <summary>Details</summary>
Motivation: 解决音频-文本模态差距问题，提升多模态编码器与大语言模型耦合的效果

Method: 使用基于扩散的模态桥接模块，将音频嵌入生成式映射到文本嵌入分布，采用轻量级网络和三个残差MLP块

Result: 在模态差距分析中显著减少差距，在自动音频描述任务中实现零样本和全监督状态最优性能，相对增益分别达52.5%和7.5%

Conclusion: 缩小模态差距对于多模态编码器与LLM的有效耦合至关重要，基于扩散的模态桥接提供了超越知识检索中心设计的有前景方向

Abstract: Contrastive audio-language pretraining yields powerful joint representations,
yet a persistent audio-text modality gap limits the benefits of coupling
multimodal encoders with large language models (LLMs). We present
Diffusion-Link, a diffusion-based modality-bridging module that generatively
maps audio embeddings into the text-embedding distribution. The module is
trained at the output embedding from the frozen multimodal encoder and
implemented as a lightweight network with three residual MLP blocks. To assess
the effect of Diffusion-Link on multimodal encoder-LLM coupling, we evaluate on
Automatic Audio Captioning (AAC); to our knowledge, this is the first
application of diffusion-based modality bridging to AAC. We report two results.
(1) Modality-gap analysis: on similarity and geometric criteria, Diffusion-Link
reduces the modality gap the most among prior diffusion-based methods and shows
a collective migration of audio embeddings toward the text distribution. (2)
Downstream AAC: attaching Diffusion-Link to the same multimodal LLM baseline
achieves state-of-the-art on AudioCaps in both zero-shot and fully supervised
captioning without external knowledge, with relative gains up to 52.5% and
7.5%, respectively. These findings show that closing the modality gap is
pivotal for effective coupling between multimodal encoders and LLMs, and
diffusion-based modality bridging offers a promising direction beyond
knowledge-retrieval-centric designs. Code will be released upon acceptance
https://github.com/DevKiHyun/Diffusion-Link

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [165] [Group-Adaptive Adversarial Learning for Robust Fake News Detection Against Malicious Comments](https://arxiv.org/abs/2510.09712)
*Zhao Tong,Chunlin Gong,Yimeng Gu,Haichao Shi,Qiang Liu,Shu Wu,Xiao-Yu Zhang*

Main category: cs.LG

TL;DR: 该论文提出了一种群体自适应对抗训练策略，通过将对抗性评论分为三类并动态调整学习重点，显著提高了假新闻检测模型对对抗性评论攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有假新闻检测模型在标准设置下表现良好，但对真实用户或大语言模型生成的对抗性评论攻击仍然脆弱，这些攻击会微妙地改变模型决策。

Method: 1. 将对抗性评论分为感知、认知和社会三类；2. 使用大语言模型生成多样化的类别特定攻击以增强对抗训练；3. 应用基于狄利克雷分布的自适应采样机制，在训练过程中动态调整不同评论类别的学习重点。

Result: 在基准数据集上的实验表明，该方法在保持强检测准确性的同时，显著提高了对各种对抗性评论扰动的鲁棒性。

Conclusion: 提出的群体自适应对抗训练策略有效提升了假新闻检测模型对对抗性攻击的防御能力，为构建更可靠的假新闻检测系统提供了重要方法。

Abstract: The spread of fake news online distorts public judgment and erodes trust in
social media platforms. Although recent fake news detection (FND) models
perform well in standard settings, they remain vulnerable to adversarial
comments-authored by real users or by large language models (LLMs)-that subtly
shift model decisions. In view of this, we first present a comprehensive
evaluation of comment attacks to existing fake news detectors and then
introduce a group-adaptive adversarial training strategy to improve the
robustness of FND models. To be specific, our approach comprises three steps:
(1) dividing adversarial comments into three psychologically grounded
categories: perceptual, cognitive, and societal; (2) generating diverse,
category-specific attacks via LLMs to enhance adversarial training; and (3)
applying a Dirichlet-based adaptive sampling mechanism (InfoDirichlet Adjusting
Mechanism) that dynamically adjusts the learning focus across different comment
categories during training. Experiments on benchmark datasets show that our
method maintains strong detection accuracy while substantially increasing
robustness to a wide range of adversarial comment perturbations.

</details>


### [166] [It's 2025 -- Narrative Learning is the new baseline to beat for explainable machine learning](https://arxiv.org/abs/2510.09723)
*Gregory D. Baker*

Main category: cs.LG

TL;DR: 提出Narrative Learning方法，完全使用自然语言定义模型，通过解释性提示而非传统数值优化迭代改进分类标准。在6个数据集中的5个上，该方法因语言模型改进而比基线可解释模型更准确。


<details>
  <summary>Details</summary>
Motivation: 探索完全基于自然语言定义模型的方法，通过解释性提示来迭代改进分类标准，而不是依赖传统的数值优化方法。

Method: 使用自然语言定义模型，通过解释性提示迭代改进分类标准，在3个合成和3个自然数据集上进行实验，与7个基线可解释机器学习模型进行比较。

Result: 在6个数据集中的5个上，Narrative Learning方法因语言模型改进而比基线可解释模型更准确，并分析了模型输出的词汇统计趋势作为解释可理解性的代理指标。

Conclusion: Narrative Learning方法展示了在自然语言定义模型方面的潜力，随着语言模型的改进，该方法在准确性上超越了传统可解释模型。

Abstract: In this paper, we introduce Narrative Learning, a methodology where models
are defined entirely in natural language and iteratively refine their
classification criteria using explanatory prompts rather than traditional
numerical optimisation. We report on experiments to evaluate the accuracy and
potential of this approach using 3 synthetic and 3 natural datasets and compare
them against 7 baseline explainable machine learning models. We demonstrate
that on 5 out of 6 of these datasets, Narrative Learning became more accurate
than the baseline explainable models in 2025 or earlier because of improvements
in language models. We also report on trends in the lexicostatistics of these
models' outputs as a proxy for the comprehensibility of the explanations.

</details>


### [167] [Machine learning methods fail to provide cohesive atheoretical construction of personality traits from semantic embeddings](https://arxiv.org/abs/2510.09739)
*Ayoub Bouguettaya,Elizabeth M. Stuart*

Main category: cs.LG

TL;DR: 使用机器学习从经典形容词列表构建自下而上的人格模型，与Big Five模型对比分析100万条Reddit评论。结果显示Big Five模型在描述在线社区方面更强大、更可解释，而机器学习聚类未能提供有意义的区分。


<details>
  <summary>Details</summary>
Motivation: 验证词汇假说和测试机器学习方法是否能够超越或替代已建立的心理人格理论模型。

Method: 从经典形容词列表使用机器学习构建人格模型，与Big Five模型对比分析100万条Reddit评论。

Result: Big Five模型（特别是宜人性、尽责性和神经质）在描述在线社区方面表现更优，机器学习聚类未能恢复外向性特质且缺乏心理测量一致性。

Conclusion: Big Five模型具有稳健性，人格的语义结构是情境依赖的。机器学习可用于检验心理理论的生态效度，但可能无法替代它们。

Abstract: The lexical hypothesis posits that personality traits are encoded in language
and is foundational to models like the Big Five. We created a bottom-up
personality model from a classic adjective list using machine learning and
compared its descriptive utility against the Big Five by analyzing one million
Reddit comments. The Big Five, particularly Agreeableness, Conscientiousness,
and Neuroticism, provided a far more powerful and interpretable description of
these online communities. In contrast, our machine-learning clusters provided
no meaningful distinctions, failed to recover the Extraversion trait, and
lacked the psychometric coherence of the Big Five. These results affirm the
robustness of the Big Five and suggest personality's semantic structure is
context-dependent. Our findings show that while machine learning can help check
the ecological validity of established psychological theories, it may not be
able to replace them.

</details>


### [168] [Building a Foundational Guardrail for General Agentic Systems via Synthetic Data](https://arxiv.org/abs/2510.09781)
*Yue Huang,Hang Hua,Yujun Zhou,Pengcheng Jing,Manish Nagireddy,Inkit Padhi,Greta Dolcetti,Zhangchen Xu,Subhajit Chaudhury,Ambrish Rawat,Liubov Nedoshivina,Pin-Yu Chen,Prasanna Sattigeri,Xiangliang Zhang*

Main category: cs.LG

TL;DR: 该论文提出了一个预执行安全框架，通过AuraGen数据生成引擎、Safiron防护模型和Pre-Exec Bench评估基准，解决LLM代理在计划阶段的安全防护问题。


<details>
  <summary>Details</summary>
Motivation: 现有防护措施主要在动作执行后介入，难以扩展且缺乏对计划层面的可控监督。某些风险一旦执行会造成严重后果，因此在规划阶段干预是最安全的防护方式。

Method: 1) AuraGen：可控数据生成引擎，合成良性轨迹、注入分类风险、通过奖励模型过滤输出；2) Safiron：基础防护模型，结合跨规划器适配器和紧凑防护模型；3) Pre-Exec Bench：包含多样化工具和分支轨迹的评估基准。

Result: 在Pre-Exec Bench上的广泛实验表明，提出的防护模型在检测、细粒度分类、解释和跨规划器泛化方面均优于强基线方法。

Conclusion: 该研究为更安全的智能代理系统提供了实用模板，通过填补数据、模型和评估三大空白，实现了在计划阶段的可靠安全防护。

Abstract: While LLM agents can plan multi-step tasks, intervening at the planning
stage-before any action is executed-is often the safest way to prevent harm,
since certain risks can lead to severe consequences once carried out. However,
existing guardrails mostly operate post-execution, which is difficult to scale
and leaves little room for controllable supervision at the plan level. To
address this challenge, we highlight three critical gaps in current research:
data gap, model gap, and evaluation gap. To close the data gap, we introduce
AuraGen, a controllable engine that (i) synthesizes benign trajectories, (ii)
injects category-labeled risks with calibrated difficulty, and (iii) filters
outputs via an automated reward model, producing large and reliable corpora for
pre-execution safety. To close the guardian model gap, we propose a
foundational guardrail Safiron, combining a cross-planner adapter with a
compact guardian model. The adapter unifies different input formats, while
Safiron flags risky cases, assigns risk types, and generates rationales;
trained in two stages with a broadly explored data recipe, Safiron achieves
robust transfer across settings. To close the evaluation gap, we release
Pre-Exec Bench, a realistic benchmark covering diverse tools and branching
trajectories, which measures detection, fine-grained categorization,
explanation, and cross-planner generalization in human-verified scenarios.
Extensive experiments demonstrate consistent gains of the proposed guardrail
over strong baselines on Pre-Exec Bench, and ablations further distill
actionable practices, providing a practical template for safer agentic systems.

</details>


### [169] [Translution: Unifying Self-attention and Convolution for Adaptive and Relative Modeling](https://arxiv.org/abs/2510.10060)
*Hehe Fan,Yi Yang,Mohan Kankanhalli,Fei Wu*

Main category: cs.LG

TL;DR: 本文提出了Translution操作，统一了自注意力的自适应识别能力和卷积的相对编码优势，并提出了轻量级变体α-Translution来解决参数过多的问题。


<details>
  <summary>Details</summary>
Motivation: 自注意力能够自适应识别相关元素但依赖绝对位置嵌入，卷积具有相对编码优势但固定核大小限制了自适应选择能力。需要统一这两种方法的优势。

Method: 提出Translution操作，结合自注意力的自适应识别和卷积的相对编码。由于参数过多，进一步提出轻量级α-Translution变体。

Result: 在计算机视觉和自然语言处理任务上的实验表明，Translution（包括α-Translution）相比自注意力实现了更高的准确率。

Conclusion: Translution成功统一了自注意力和卷积的优势，在保持性能的同时解决了参数过多的问题。

Abstract: When modeling a given type of data, we consider it to involve two key
aspects: 1) identifying relevant elements (e.g., image pixels or textual words)
to a central element, as in a convolutional receptive field, or to a query
element, as in self-attention, and 2) encoding these tokens effectively.
Self-attention can adaptively identify these elements but relies on absolute
positional embedding for structural representation learning. In contrast,
convolution encodes elements in a relative manner, yet their fixed kernel size
limits their ability to adaptively select the relevant elements. In this paper,
we introduce Translution, an operation that unifies the adaptive identification
capability of self-attention and the relative encoding advantage of
convolution. However, this integration leads to a substantial increase in the
number of parameters, exceeding most currently available computational
resources. Therefore, we propose a lightweight variant of Translution, named
{\alpha}-Translution. Experiments on computer vision and natural language
processing tasks show that Translution (including {\alpha}-Translution)
achieves superior accuracy compared to self-attention. The code is available at
https://github.com/hehefan/Translution.

</details>


### [170] [RLFR: Extending Reinforcement Learning for LLMs with Flow Environment](https://arxiv.org/abs/2510.10201)
*Jinghao Zhang,Naishan Zheng,Ruilin Li,Dongzhou Cheng,Zheming Liang,Feng Zhao,Jiaqi Wang*

Main category: cs.LG

TL;DR: 提出RLFR方法，利用潜在空间中的流场构建奖励信号，通过量化策略潜在在流场中的速度偏差来改进强化学习中的奖励塑造，避免传统二元验证的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统基于二元验证的强化学习方法容易忽视推理轨迹中的有价值探索，而构建黄金过程奖励模型的标注成本高昂。现有方法使用熵和似然等辅助信号进行奖励塑造，但潜在空间的表达能力未被充分探索。

Method: RLFR方法从离策略高质量数据或在线拒绝采样数据构建模型潜在空间的流场，量化策略潜在在流场中的速度偏差作为奖励信号。该方法能够压缩任何离策略专家数据作为奖励参考，并利用隐藏状态中的高效上下文依赖。

Result: 在语言和多模态推理基准测试上的实验证明了流奖励的可靠性，表明基于辅助信号的奖励塑造具有前景。

Conclusion: RLFR展示了良好建立的流场可以作为奖励信号收集的有效环境，强调了潜在空间的表达能力被低估，为使用辅助信号进行奖励塑造提供了有前景的范式。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as
a promising framework for improving reasoning abilities in Large Language
Models (LLMs). However, policy optimized with binary verification prone to
overlook potential valuable exploration in reasoning trajectory. In view of
heavy annotation cost of golden Process Reward Models (PRMs), recent works
attempt using auxiliary signals for reward shaping of process tokens, involving
entropy and likelihood collected from logit space. In this work, we offer a
novel perspective on shaping RLVR with flow rewards derived from latent space,
and propose RLFR, where the flow fields of model latents are constructed from
either off-policy high-quality data and on-policy rejection sampling data, and
the velocity deviations of policy latents within it are quantified to serve as
a reward signal. RLFR first demonstrates that a well-established flow field can
be a sound environment for reward signal collection, highlighting the
expressive latent space is much underexplored. Moreover, RLFR is able to
compress any off-policy expert data as reference for constituting reward
signals, and we show that the efficient context dependence compressed within
the hidden states are utilized, rather than individual token-level denotation
for context comprehending. Experiments on both language and multimodal
reasoning benchmarks demonstrate the reliability of flow rewards, and
suggesting a promising paradigm for reward shaping with auxiliary signals.

</details>


### [171] [Sample-Efficient Online Learning in LM Agents via Hindsight Trajectory Rewriting](https://arxiv.org/abs/2510.10304)
*Michael Y. Hu,Benjamin Van Durme,Jacob Andreas,Harsh Jhamtani*

Main category: cs.LG

TL;DR: ECHO是一个基于提示的框架，通过后见之明优化将强化学习中的后见之明经验回放应用于语言模型代理，从失败尝试中生成优化的替代目标轨迹。


<details>
  <summary>Details</summary>
Motivation: 语言模型代理在新环境中学习时样本效率低下，这在与人类交互或重置物理系统等成本高昂的环境中尤为不利。现有代理架构对语言模型生成或推理完整反事实轨迹的能力利用有限。

Method: ECHO包含两个组件：后见之明规则（使用语言模型识别相关子目标并生成优化轨迹）和更新规则（在内存中维护压缩的轨迹表示）。

Result: 在XMiniGrid和PeopleJoinQA两个领域的评估中，ECHO比普通语言代理基线性能提升高达80%；在XMiniGrid中，它还超越了Reflexion和AWM等复杂代理架构，通过更有效地利用过去经验实现对新环境的更快适应。

Conclusion: ECHO框架通过后见之明优化显著提高了语言模型代理的样本效率和环境适应能力，证明了从失败交互中生成合成正面示例的有效性。

Abstract: Language model (LM) agents deployed in novel environments often exhibit poor
sample efficiency when learning from sequential interactions. This
significantly hinders the usefulness of such agents in environments where
interaction is costly (for example, when they interact with humans or reset
physical systems). While a number of existing LM agent architectures
incorporate various mechanisms for experience storage and reflection, they make
limited use of LMs' abilities to directly generate or reason about full
counterfactual trajectories. We introduce ECHO (Experience Consolidation via
Hindsight Optimization), a prompting framework that adapts hindsight experience
replay from reinforcement learning for language model agents. ECHO generates
optimized trajectories for alternative goals that could have been achieved
during failed attempts, effectively creating synthetic positive examples from
unsuccessful interactions. Our approach consists of two components: a hindsight
rule that uses the language model itself to identify relevant subgoals and
generate optimized trajectories, and an update rule that maintains compressed
trajectory representations in memory. We evaluate ECHO on stateful versions of
XMiniGrid, a text-based navigation and planning benchmark, and PeopleJoinQA, a
collaborative information-gathering enterprise simulation. Across both domains,
ECHO outperforms vanilla language agent baselines by up to 80%; in XMiniGrid,
it also outperforms a number of sophisticated agent architectures including
Reflexion and AWM, demonstrating faster adaptation to novel environments
through more effective utilization of past experiences.

</details>


### [172] [Find Your Optimal Teacher: Personalized Data Synthesis via Router-Guided Multi-Teacher Distillation](https://arxiv.org/abs/2510.10925)
*Hengyuan Zhang,Shiping Yang,Xiao Liang,Chenming Shang,Yuxuan Jiang,Chaofan Tao,Jing Xiong,Hayden Kwok-Hay So,Ruobing Xie,Angel X. Chang,Ngai Wong*

Main category: cs.LG

TL;DR: PerSyn提出了一种新的"路由后生成"数据合成策略，通过查询级路由器为每个学生模型匹配合适的教师模型，生成个性化数据以提高学习效果。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明，更强的模型并不总是最优的教师，存在教师输出与学生可学习性之间的不匹配问题。

Method: 采用"路由后生成"范式：首先通过查询级路由器为每个提示分配最优教师，考虑学生可学习性和教师响应质量；然后各教师仅为分配到的提示生成数据。

Result: 在不同模型系列和规模上的广泛实验表明，PerSyn在指令调优和数学推理设置中始终优于或与所有基线方法相当。

Conclusion: PerSyn验证了其有效性，并为未来研究提供了额外见解，表明个性化数据合成是提升知识蒸馏效果的有前景方向。

Abstract: Training student models on synthetic data generated by strong teacher models
is a promising way to distilling the capabilities of teachers. However, recent
studies show that stronger models are not always optimal teachers, revealing a
mismatch between teacher outputs and student learnability. To address this
issue, we propose PerSyn (Personalized data Synthesis), a novel synthesis
strategy that operates under a new ``Route then Generate'' paradigm to create
data tailored to each student model, enabling it to learn more effectively.
Specifically, PerSyn first assigns each prompt to its optimal teacher via a
query-level router that jointly considers student learnability and teacher
response quality. Each teacher then synthesizes data only for its assigned
prompts, making the process more efficient than the conventional ``Generate
then Select'' paradigm, where all teachers must generate parallel responses for
the entire prompt set before constructing the final dataset. Extensive
experiments across different model families and scales demonstrate that PerSyn
consistently achieves superior or comparable performance to all baselines in
instruct tuning and math reasoning settings. Further analysis verifies the
effectiveness of PerSyn and offers extra insights to propel future research.

</details>


### [173] [Rediscovering Entropy Regularization: Adaptive Coefficient Unlocks Its Potential for LLM Reinforcement Learning](https://arxiv.org/abs/2510.10959)
*Xiaoyun Zhang,Xiaojian Yuan,Di Huang,Wang You,Chen Hu,Jingqing Ruan,Kejiang Chen,Xing Hu*

Main category: cs.LG

TL;DR: 本文提出自适应熵正则化(AER)框架，通过动态调整熵正则化系数来解决RLVR训练中的策略熵崩溃问题，提升大语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: RLVR训练中常出现策略熵崩溃问题，导致策略过于确定性，阻碍探索并限制推理性能。传统的固定系数熵正则化方法在不同任务和模型间效果不稳定。

Method: 提出自适应熵正则化(AER)框架，包含三个组件：难度感知系数分配、初始锚定目标熵和动态全局系数调整，动态平衡探索与利用。

Result: 在多个数学推理基准测试中，AER始终优于基线方法，提高了推理准确性和探索能力。

Conclusion: 自适应熵正则化能有效解决RLVR训练中的策略熵崩溃问题，显著提升大语言模型的推理性能。

Abstract: Reasoning ability has become a defining capability of Large Language Models
(LLMs), with Reinforcement Learning with Verifiable Rewards (RLVR) emerging as
a key paradigm to enhance it. However, RLVR training often suffers from policy
entropy collapse, where the policy becomes overly deterministic, hindering
exploration and limiting reasoning performance. While entropy regularization is
a common remedy, its effectiveness is highly sensitive to the fixed
coefficient, making it unstable across tasks and models. In this work, we
revisit entropy regularization in RLVR and argue that its potential has been
largely underestimated. Our analysis shows that (i) tasks of varying difficulty
demand distinct exploration intensities, and (ii) balanced exploration may
require the policy entropy to be maintained within a moderate range below its
initial level. Therefore, we propose Adaptive Entropy Regularization (AER)--a
framework that dynamically balances exploration and exploitation via three
components: difficulty-aware coefficient allocation, initial-anchored target
entropy, and dynamic global coefficient adjustment. Experiments on multiple
mathematical reasoning benchmarks show that AER consistently outperforms
baselines, improving both reasoning accuracy and exploration capability.

</details>


### [174] [ELMO: Efficiency via Low-precision and Peak Memory Optimization in Large Output Spaces](https://arxiv.org/abs/2510.11168)
*Jinbin Zhang,Nasib Ullah,Erik Schultheis,Rohit Babbar*

Main category: cs.LG

TL;DR: ELMO是一个纯低精度训练框架，使用BFloat16和Float8数据类型训练极端多标签分类(XMC)模型，通过Kahan求和和随机舍入实现全Float8训练，显著降低GPU内存使用。


<details>
  <summary>Details</summary>
Motivation: 当前XMC方法主要依赖FP16-FP32混合精度训练，存在不稳定、内存使用效率低和计算开销大的问题，而现有低精度方法通常为分类层保留更高精度。

Method: 提出ELMO框架，使用BFloat16和Float8数据类型，结合Kahan求和与随机舍入技术，实现全Float8训练，无需单精度主权重或张量缩放，并采用梯度融合和分块内存优化。

Result: 在300万标签的XMC模型上仅使用6.6 GiB GPU内存，相比优化SOTA方法Renee所需的39.7 GiB大幅减少，且不损失准确性。

Conclusion: ELMO证明了XMC模型可以有效地在纯低精度下训练，显著降低了内存需求，为大规模标签分类任务提供了高效的解决方案。

Abstract: Large output spaces, also referred to as Extreme multilabel classification
(XMC), is a setting that arises, e.g., in large-scale tagging and
product-to-product recommendation, and is characterized by the number of labels
ranging from hundreds of thousands to millions. This means that the linear
classification head, usually only a tiny fraction of the overall model, turns
into the main driver for compute and memory demand. Current state-of-the-art
XMC methods predominantly rely on FP16-FP32 mixed-precision training, which we
show can be unstable, and inefficient in terms of memory usage and
computational overhead. Meanwhile, existing low-precision methods typically
retain higher precision for the classification layer. In this work, we propose
ELMO, a pure low-precision training framework for XMC models using BFloat16 and
Float8 data types. By leveraging Kahan summation and stochastic rounding, we
demonstrate that XMC models can be effectively trained entirely in Float8,
without relying on single-precision master weights or tensor scaling.
Low-precision training, combined with our proposed memory optimizations --
gradient fusion and chunking -- enables significant reductions in GPU memory
usage. For example, we train a 3-million-label XMC model with only 6.6 GiB of
GPU memory, compared to the 39.7 GiB required by the optimized SOTA method,
Renee without compromising accuracy.

</details>


### [175] [EAGER: Entropy-Aware GEneRation for Adaptive Inference-Time Scaling](https://arxiv.org/abs/2510.11170)
*Daniel Scalena,Leonidas Zotos,Elisabetta Fersini,Malvina Nissim,Ahmet Üstün*

Main category: cs.LG

TL;DR: EAGer是一种训练自由的生成方法，通过基于token熵分布的不确定性来动态分配计算预算，减少冗余计算并提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 不同提示具有不同的复杂度，需要不同的计算量，但现有方法为每个提示分配相同的计算预算，导致计算资源浪费。

Method: 利用模型不确定性，通过token-wise熵分布识别高熵token，仅在这些位置分支到多个推理路径，并将节省的计算预算重新分配给最需要探索替代路径的实例。

Result: 在AIME 2025等复杂推理基准测试中，EAGer无需访问目标标签即可重新分配预算，在推理长度和Pass@k方面达到最佳效率-性能权衡。当可访问目标标签时，相比Full Parallel Sampling，EAGer生成token减少65%，Pass@k提升37%。

Conclusion: EAGer通过基于不确定性的动态计算预算分配，有效提高了推理模型的效率和性能。

Abstract: With the rise of reasoning language models and test-time scaling methods as a
paradigm for improving model performance, substantial computation is often
required to generate multiple candidate sequences from the same prompt. This
enables exploration of different reasoning paths toward the correct solution,
however, allocates the same compute budget for each prompt. Grounded on the
assumption that different prompts carry different degrees of complexity, and
thus different computation needs, we propose EAGer, a training-free generation
method that leverages model uncertainty through token-wise entropy distribution
to reduce redundant computation and concurrently improve overall performance.
EAGer allows branching to multiple reasoning paths only in the presence of
high-entropy tokens, and then reallocates the saved compute budget to the
instances where exploration of alternative paths is most needed. We find that
across multiple open-source models on complex reasoning benchmarks such as AIME
2025, EAGer can reallocate the budget without accessing target labels,
achieving the best efficiency-performance trade-off in terms of reasoning
length and Pass@k. When target labels are accessible, EAGer generates up to 65%
fewer tokens (hence saving compute) and achieves up to 37% improvement in
Pass@k compared to the Full Parallel Sampling.

</details>


### [176] [Can Tool-Integrated Reinforcement Learning Generalize Across Diverse Domains?](https://arxiv.org/abs/2510.11184)
*Zhengyu Chen,Jinluan Yang,Teng Xiao,Ruochen Zhou,Luan Zhang,Xiangyu Xi,Xiaowei Shi,Wei Wang,Jinggang Wang*

Main category: cs.LG

TL;DR: 该论文研究了LLM代理在数学任务上训练后，其代码解释器工具使用能力向其他推理领域的跨域泛化能力，并提出了Tool Generalization Reinforcement Learning (TGRL)框架来促进领域无关的学习和技能迁移。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在推理和工具使用方面表现出色，但工具增强强化学习在不同领域间的泛化能力仍未充分探索。本研究旨在探索仅在数学问题解决任务上训练的LLM代理在其他推理领域的性能表现。

Method: 提出了Tool Generalization Reinforcement Learning (TGRL)框架，包括：(i)标准化工具接口，通过一致格式和显式终止抽象领域特定细节；(ii)双组件奖励系统，分解奖励以激励可泛化行为；(iii)基于XML的提示模板，分离思考、工具调用和响应以鼓励模块化规划。

Result: 实验结果显示，从数学任务学习的基于RL的工具使用可以有效地转移到其他领域的复杂任务中，实现了出色的任务性能和高的token效率，在多个基准测试中达到了最先进的性能。

Conclusion: 该研究验证了工具RL在LLM推理中的跨域潜力，表明在受限训练领域学习的工具使用技能可以成功泛化到不同的推理领域。

Abstract: Recent advances in large language models (LLMs) have demonstrated remarkable
capabilities in reasoning and tool utilization. However, the generalization of
tool-augmented reinforcement learning (RL) across diverse domains remains
underexplored. In this work, we investigate the cross-domain generalization of
an LLM agent equipped with a code interpreter tool, which is exclusively
trained on mathematical problem-solving tasks. Despite the restricted training
domain, we evaluate the agent's performance across several distinct reasoning
domains. The results reveal that RL-based tool usage learned from mathematical
tasks can be effectively transferred to complex tasks in other domains,
enabling great task performance and high token efficiency. To facilitate this
cross-domain transfer, we propose a Tool Generalization Reinforcement Learning
(TGRL) framework designed to promote domain-agnostic learning and skill
migration, encompassing: (i) a standardized tool interface that abstracts
domain-specific nuances through consistent formatting and explicit termination,
fostering transferable invocation patterns; (ii) a dual-component reward system
that decomposes rewards to incentivize generalizable behaviors like tool
efficiency and reasoning abstraction, ensuring alignment and robustness across
domain shifts; and (iii) an XML-based prompt template that separates thinking,
tool calls, and responses to encourage modular, domain-invariant planning and
coherent multi-turn interactions. Extensive experiments across diverse
benchmarks validate our approach, achieving state-of-the-art performance and
highlighting the cross-domain potential of Tool RL for LLM reasoning.

</details>


### [177] [ENIGMA: The Geometry of Reasoning and Alignment in Large-Language Models](https://arxiv.org/abs/2510.11278)
*Gareth Seneque,Lap-Hang Ho,Nafise Erfanian Saeedi,Jeffrey Molendijk,Ariel Kupermann,Tim Elson*

Main category: cs.LG

TL;DR: ENIGMA是一种新颖的LLM训练方法，通过将组织政策视为信息流形上的移动方向，联合提升推理能力、对齐性和鲁棒性。该方法结合了GRPO、SAMI风格的信息对比辅助和Sinkhorn最优传输正则化。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM训练方法往往独立处理推理、对齐和鲁棒性，而作者认为这些是单一信息几何目标的不同投影。通过将组织政策原则作为信息流形上的移动方向，可以实现更统一的训练目标。

Method: 单循环训练器结合：1) GRPO（无评论家的策略优化方法）；2) SAMI风格的对称InfoNCE辅助；3) Sinkhorn最优传输正则化来限制几何漂移。还引入了基于InfoNCE的度量指标来评估CoT对政策的编码强度。

Result: 在1B参数LLM上的实验表明，高充分性指数(SI)的原则能预测更稳定的训练动态和更好的基准性能。信息几何分析验证了流形结构的理想变化。

Conclusion: ENIGMA支持了推理、对齐和鲁棒性是单一信息几何目标投影的假设，无需奖励模型即可实现原则性推理，为可信能力提供了一条路径。

Abstract: We present Entropic Mutual-Information Geometry Large-Language Model
Alignment (ENIGMA), a novel approach to Large-Language Model (LLM) training
that jointly improves reasoning, alignment and robustness by treating an
organisation's policies/principles as directions to move on a model's
information manifold. Our single-loop trainer combines Group-Relative Policy
Optimisation (GRPO), an on-policy, critic-free RL method with Chain-of-Thought
(CoT)-format only rewards; a Self-Supervised Alignment with Mutual Information
(SAMI)-style symmetric InfoNCE auxiliary; and an entropic Sinkhorn
optimal-transport regulariser on hidden-state distributions to bound geometry
drift. We also introduce infoNCE metrics that specialise to a standard MI lower
bound under matched negatives to measure how strongly a model's CoT encodes
these policies. These metrics include a Sufficiency Index (SI) that enables the
selection and creation of principles that maximise downstream performance prior
to training. In our experiments using small (1B) LLMs, high-SI principles
predict steadier training dynamics and improved benchmark performance over GRPO
ablations. Our information-geometry analysis of trained models validates
desirable structural change in the manifold. These results support our
hypothesis that reasoning, alignment, and robustness are projections of a
single informationgeometric objective, and that models trained using ENIGMA
demonstrate principled reasoning without the use of a reward model, offering a
path to trusted capability

</details>


### [178] [ReLook: Vision-Grounded RL with a Multimodal LLM Critic for Agentic Web Coding](https://arxiv.org/abs/2510.11498)
*Yuhang Li,Chenchen Zhang,Ruilin Lv,Ao Liu,Ken Deng,Yuanxing Zhang,Jiaheng Liu,Wiggin Zhou,Bo Zhou*

Main category: cs.LG

TL;DR: ReLook是一个基于视觉的强化学习框架，通过多模态大语言模型作为工具，实现前端代码的生成-诊断-优化循环，显著提升视觉前端代码生成质量。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在算法代码生成方面表现出色，但在前端开发中表现不佳，因为前端代码的正确性需要通过渲染像素和交互来判断。

Method: 采用代理式视觉强化学习框架，将多模态LLM作为工具，在训练中作为视觉评判器（通过截图评分代码）和提供可操作的视觉反馈源，引入强制优化规则确保单调改进。

Result: 在三个广泛使用的基准测试中，ReLook在视觉前端代码生成方面持续优于强基线方法。

Conclusion: 该方法展示了代理感知、视觉奖励和训练-推理解耦在前端代码生成中的优势。

Abstract: While Large Language Models (LLMs) excel at algorithmic code generation, they
struggle with front-end development, where correctness is judged on rendered
pixels and interaction. We present ReLook, an agentic, vision-grounded
reinforcement learning framework that empowers an agent to close a robust
generate--diagnose--refine loop by invoking a multimodal LLM (MLLM) as a tool.
During training, the agent uses the MLLM-in-the-loop both as a visual
critic--scoring code with screenshots--and as a source of actionable,
vision-grounded feedback; a strict zero-reward rule for invalid renders anchors
renderability and prevents reward hacking. To prevent behavioral collapse, we
introduce Forced Optimization, a strict acceptance rule that admits only
improving revisions, yielding monotonically better trajectories. At inference,
we decouple the critic and run a lightweight, critic-free self-edit cycle,
keeping latency comparable to base decoding while retaining most of the gains.
Across three widely used benchmarks, ReLook consistently outperforms strong
baselines in vision-grounded front-end code generation, highlighting the
benefits of agentic perception, visual rewards, and training-inference
decoupling.

</details>


### [179] [Boundary-Guided Policy Optimization for Memory-efficient RL of Diffusion Large Language Models](https://arxiv.org/abs/2510.11683)
*Nianyi Lin,Jiajie Zhang,Lei Hou,Juanzi Li*

Main category: cs.LG

TL;DR: 提出了BGPO算法，通过构建特殊的ELBO下界来解决扩散大语言模型RL训练中的内存瓶颈问题，实现了恒定内存使用和更精确的似然近似。


<details>
  <summary>Details</summary>
Motivation: 现有方法在近似扩散大语言模型的似然函数时需要保留所有蒙特卡洛样本的前向计算图，导致显著内存开销，限制了样本大小和近似精度。

Method: BGPO算法最大化一个特殊构造的ELBO目标下界，该下界具有线性结构（允许梯度累积）和等价性（在策略训练中与原始目标值和梯度相等）。

Result: 实验表明BGPO在数学问题解决、代码生成和规划任务中显著优于之前的RL算法。

Conclusion: BGPO通过内存高效的设计实现了更准确的似然近似和RL目标估计，提升了扩散大语言模型的RL训练性能。

Abstract: A key challenge in applying reinforcement learning (RL) to diffusion large
language models (dLLMs) lies in the intractability of their likelihood
functions, which are essential for the RL objective, necessitating
corresponding approximation in each training step. While existing methods
approximate the log-likelihoods by their evidence lower bounds (ELBOs) via
customized Monte Carlo (MC) sampling, the forward computational graphs of all
MC samples need to be retained for the gradient computation of non-linear terms
in the RL objective, resulting in significant memory overhead. This constraint
restricts feasible sample sizes, leading to imprecise likelihood approximations
and ultimately distorting the RL objective. To overcome this limitation, we
propose \emph{Boundary-Guided Policy Optimization} (BGPO), a memory-efficient
RL algorithm that maximizes a specially constructed lower bound of the
ELBO-based objective. This lower bound is carefully designed to satisfy two key
properties: (1) Linearity: it is formulated in a linear sum where each term
depends only on a single MC sample, thereby enabling gradient accumulation
across samples and ensuring constant memory usage; (2) Equivalence: Both the
value and gradient of this lower bound are equal to those of the ELBO-based
objective in on-policy training, making it also an effective approximation for
the original RL objective. These properties allow BGPO to adopt a large MC
sample size, resulting in more accurate likelihood approximations and improved
RL objective estimation, which in turn leads to enhanced performance.
Experiments show that BGPO significantly outperforms previous RL algorithms for
dLLMs in math problem solving, code generation, and planning tasks.

</details>


### [180] [QeRL: Beyond Efficiency -- Quantization-enhanced Reinforcement Learning for LLMs](https://arxiv.org/abs/2510.11696)
*Wei Huang,Yi Ge,Shuai Yang,Yicheng Xiao,Huizi Mao,Yujun Lin,Hanrong Ye,Sifei Liu,Ka Chun Cheung,Hongxu Yin,Yao Lu,Xiaojuan Qi,Song Han,Yukang Chen*

Main category: cs.LG

TL;DR: QeRL是一个结合量化技术与强化学习的框架，通过NVFP4量化和LoRA适配器，在减少内存开销的同时加速RL训练过程，并利用量化噪声增强探索能力。


<details>
  <summary>Details</summary>
Motivation: 解决LLM强化学习训练过程中资源消耗大、GPU内存需求高、训练周期长的问题，同时探索量化噪声对策略探索的积极影响。

Method: 结合NVFP4量化和低秩适配器(LoRA)，引入自适应量化噪声(AQN)机制动态调整训练过程中的噪声水平，优化探索策略。

Result: 在rollout阶段实现1.5倍加速，首次在单张H100 80GB GPU上训练32B LLM，在GSM8K和MATH 500数学基准测试中分别达到90.8%和77.4%的准确率，性能与全参数微调相当。

Conclusion: QeRL为LLM的强化学习训练提供了一个高效且有效的框架，在保持性能的同时显著提升了训练效率和资源利用率。

Abstract: We propose QeRL, a Quantization-enhanced Reinforcement Learning framework for
large language models (LLMs). While RL is essential for LLMs' reasoning
capabilities, it is resource-intensive, requiring substantial GPU memory and
long rollout durations. QeRL addresses these issues by combining NVFP4
quantization with Low-Rank Adaptation (LoRA), accelerating rollout phase of RL
while reducing memory overhead. Beyond efficiency, our findings show that
quantization noise increases policy entropy, enhancing exploration, and
enabling the discovery of better strategies during RL. To further optimize
exploration, QeRL introduces an Adaptive Quantization Noise (AQN) mechanism,
which dynamically adjusts noise during training. Experiments demonstrate that
QeRL delivers over 1.5 times speedup in the rollout phase. Moreover, this is
the first framework to enable RL training of a 32B LLM on a single H100 80GB
GPU, while delivering overall speedups for RL training. It also achieves faster
reward growth and higher final accuracy than 16-bit LoRA and QLoRA, while
matching the performance of full-parameter fine-tuning on mathematical
benchmarks such as GSM8K (90.8%) and MATH 500 (77.4%) in the 7B model. These
results establish QeRL as an efficient and effective framework for RL training
in LLMs.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [181] [CardRewriter: Leveraging Knowledge Cards for Long-Tail Query Rewriting on Short-Video Platforms](https://arxiv.org/abs/2510.10095)
*Peiyuan Gong,Feiran Zhu,Yaqi Yin,Chenglei Dai,Chao Zhang,Kai Zheng,Wentian Bao,Jiaxin Mao,Yi Zhang*

Main category: cs.IR

TL;DR: CardRewriter是一个基于LLM的查询重写框架，通过整合领域特定知识来改进短视频平台上的长尾查询重写效果。


<details>
  <summary>Details</summary>
Motivation: 短视频平台上的用户查询（特别是长尾查询）存在拼写错误、表述不完整和意图模糊等问题，导致检索结果与用户期望不匹配。LLM在电商领域的长尾查询重写中表现良好，但在短视频平台上由于专有内容（如短视频、直播、微短剧等）超出其训练分布而效果不佳。

Method: 为每个查询聚合多源相关知识并总结成信息丰富的知识卡片，用该卡片指导LLM更好地捕捉用户意图并生成更有效的查询重写。采用两阶段训练流程：监督微调后接组相对策略优化，并设计平衡查询相关性和检索效果的奖励系统。

Result: 离线实验显示CardRewriter显著提高了针对专有内容的查询重写质量。在线A/B测试证实了长观看率(LVR)和点击率(CTR)的显著提升，以及主动查询重构率(IQRR)的明显降低。

Conclusion: 自2025年9月起，CardRewriter已在快手平台部署，为数亿用户提供日常服务，有效解决了短视频平台上长尾查询重写的挑战。

Abstract: Short-video platforms have rapidly become a new generation of information
retrieval systems, where users formulate queries to access desired videos.
However, user queries, especially long-tail ones, often suffer from spelling
errors, incomplete phrasing, and ambiguous intent, resulting in mismatches
between user expectations and retrieved results. While large language models
(LLMs) have shown success in long-tail query rewriting within e-commerce, they
struggle on short-video platforms, where proprietary content such as short
videos, live streams, micro dramas, and user social networks falls outside
their training distribution. To address this challenge, we introduce
\textbf{CardRewriter}, an LLM-based framework that incorporates domain-specific
knowledge to enhance long-tail query rewriting. For each query, our method
aggregates multi-source knowledge relevant to the query and summarizes it into
an informative and query-relevant knowledge card. This card then guides the LLM
to better capture user intent and produce more effective query rewrites. We
optimize CardRewriter using a two-stage training pipeline: supervised
fine-tuning followed by group relative policy optimization, with a tailored
reward system balancing query relevance and retrieval effectiveness. Offline
experiments show that CardRewriter substantially improves rewriting quality for
queries targeting proprietary content. Online A/B testing further confirms
significant gains in long-view rate (LVR) and click-through rate (CTR), along
with a notable reduction in initiative query reformulation rate (IQRR). Since
September 2025, CardRewriter has been deployed on Kuaishou, one of China's
largest short-video platforms, serving hundreds of millions of users daily.

</details>


### [182] [QDER: Query-Specific Document and Entity Representations for Multi-Vector Document Re-Ranking](https://arxiv.org/abs/2510.11589)
*Shubham Chatterjee,Jeff Dalton*

Main category: cs.IR

TL;DR: QDER是一个神经重排序模型，通过将知识图谱语义集成到多向量模型中，统一了实体导向和多向量两种神经IR方法。其核心创新是"延迟聚合"方法，在最终评分阶段才进行聚合，显著提升了检索性能。


<details>
  <summary>Details</summary>
Motivation: 统一神经信息检索中的两种主要方法：实体导向方法（利用知识图谱）和多向量模型（捕捉细粒度语义），以结合两者的优势。

Method: 采用"延迟聚合"方法，在整个排序过程中保持单独的token和实体表示，仅在最终评分阶段进行聚合。首先通过学习的注意力模式转换这些细粒度表示，然后应用精心选择的数学运算进行精确匹配。

Result: 在五个标准基准测试中，QDER实现了显著的性能提升：在TREC Robust 2004上nDCG@20比最强基线提高了36%，在其他数据集上也有类似改进。特别是在困难查询上表现出色，达到nDCG@20=0.70，而传统方法完全失败（nDCG@20=0.0）。

Conclusion: QDER为实体感知检索的未来工作奠定了基础，通过统一实体导向和多向量方法，在困难查询上实现了突破性性能。

Abstract: Neural IR has advanced through two distinct paths: entity-oriented approaches
leveraging knowledge graphs and multi-vector models capturing fine-grained
semantics. We introduce QDER, a neural re-ranking model that unifies these
approaches by integrating knowledge graph semantics into a multi-vector model.
QDER's key innovation lies in its modeling of query-document relationships:
rather than computing similarity scores on aggregated embeddings, we maintain
individual token and entity representations throughout the ranking process,
performing aggregation only at the final scoring stage - an approach we call
"late aggregation." We first transform these fine-grained representations
through learned attention patterns, then apply carefully chosen mathematical
operations for precise matches. Experiments across five standard benchmarks
show that QDER achieves significant performance gains, with improvements of 36%
in nDCG@20 over the strongest baseline on TREC Robust 2004 and similar
improvements on other datasets. QDER particularly excels on difficult queries,
achieving an nDCG@20 of 0.70 where traditional approaches fail completely
(nDCG@20 = 0.0), setting a foundation for future work in entity-aware
retrieval.

</details>


### [183] [REGENT: Relevance-Guided Attention for Entity-Aware Multi-Vector Neural Re-Ranking](https://arxiv.org/abs/2510.11592)
*Shubham Chatterjee*

Main category: cs.IR

TL;DR: REGENT是一个神经重排序模型，通过使用实体作为"语义骨架"来引导注意力，模仿人类理解方式，在三个挑战性数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 当前神经重排序模型在处理复杂信息需求和长文档时存在困难，主要问题在于智能内容选择——识别长文本中重要内容的能力。人类自然围绕关键实体和概念构建理解，而神经模型在固定token窗口内处理文本，缺乏对关键语义信号的捕捉。

Method: REGENT将相关性指导直接集成到注意力机制中，使用实体作为语义骨架来引导注意力。该方法结合细粒度词汇匹配和高层次语义推理，使模型能够聚焦概念上重要的内容，同时保持对精确术语匹配的敏感性。

Result: REGENT在三个挑战性数据集上实现了最先进的性能，相比BM25提升了108%，并持续优于包括ColBERT和RankVicuna在内的强基线模型。

Conclusion: 这是首个成功将实体语义直接集成到神经注意力中的工作，为实体感知信息检索建立了新范式。

Abstract: Current neural re-rankers often struggle with complex information needs and
long, content-rich documents. The fundamental issue is not computational--it is
intelligent content selection: identifying what matters in lengthy,
multi-faceted texts. While humans naturally anchor their understanding around
key entities and concepts, neural models process text within rigid token
windows, treating all interactions as equally important and missing critical
semantic signals. We introduce REGENT, a neural re-ranking model that mimics
human-like understanding by using entities as a "semantic skeleton" to guide
attention. REGENT integrates relevance guidance directly into the attention
mechanism, combining fine-grained lexical matching with high-level semantic
reasoning. This relevance-guided attention enables the model to focus on
conceptually important content while maintaining sensitivity to precise term
matches. REGENT achieves new state-of-the-art performance in three challenging
datasets, providing up to 108% improvement over BM25 and consistently
outperforming strong baselines including ColBERT and RankVicuna. To our
knowledge, this is the first work to successfully integrate entity semantics
directly into neural attention, establishing a new paradigm for entity-aware
information retrieval.

</details>


### [184] [FinVet: A Collaborative Framework of RAG and External Fact-Checking Agents for Financial Misinformation Detection](https://arxiv.org/abs/2510.11654)
*Daniel Berhane Araya,Duoduo Liao*

Main category: cs.IR

TL;DR: FinVet是一个用于金融市场信息验证的多智能体框架，通过集成两个RAG管道和外部事实核查，采用置信度加权投票机制，显著提升了金融信息验证的准确性。


<details>
  <summary>Details</summary>
Motivation: 金融市场面临错误信息的严重威胁，现有方法缺乏决策透明度和可信来源归因，需要开发更可靠的验证系统。

Method: 使用多智能体框架集成两个RAG管道，采用自适应三层处理策略，基于检索置信度动态调整验证方法，从元数据提取到混合推理再到完整模型分析。

Result: 在FinFact数据集上，FinVet获得0.85的F1分数，比最佳单管道提升10.4%，比独立RAG方法提升37%。

Conclusion: FinVet通过证据支持的判决、来源归因、置信度评分和不确定性标记，为金融市场信息验证提供了更可靠和透明的解决方案。

Abstract: Financial markets face growing threats from misinformation that can trigger
billions in losses in minutes. Most existing approaches lack transparency in
their decision-making and provide limited attribution to credible sources. We
introduce FinVet, a novel multi-agent framework that integrates two
Retrieval-Augmented Generation (RAG) pipelines with external fact-checking
through a confidence-weighted voting mechanism. FinVet employs adaptive
three-tier processing that dynamically adjusts verification strategies based on
retrieval confidence, from direct metadata extraction to hybrid reasoning to
full model-based analysis. Unlike existing methods, FinVet provides
evidence-backed verdicts, source attribution, confidence scores, and explicit
uncertainty flags when evidence is insufficient. Experimental evaluation on the
FinFact dataset shows that FinVet achieves an F1 score of 0.85, which is a
10.4% improvement over the best individual pipeline (fact-check pipeline) and
37% improvement over standalone RAG approaches.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [185] [Beyond the Crowd: LLM-Augmented Community Notes for Governing Health Misinformation](https://arxiv.org/abs/2510.11423)
*Jiaying Wu,Zihang Fu,Haonan Wang,Fanxiao Li,Min-Yen Kan*

Main category: cs.SI

TL;DR: 提出了CrowdNotes+框架，利用LLM增强Community Notes系统，通过证据基础注释增强和效用引导注释自动化两种模式，结合三层评估机制，显著改善健康错误信息治理的响应速度和可靠性。


<details>
  <summary>Details</summary>
Motivation: 分析发现Community Notes系统存在显著延迟（中位延迟17.6小时），无法及时应对错误信息激增，需要提高响应速度和可靠性。

Method: 提出CrowdNotes+统一框架，整合两种互补模式：证据基础注释增强和效用引导注释自动化，采用分层三步评估机制（相关性、正确性、有用性），并基于HealthNotes基准进行实例化。

Result: 实验发现当前有用性评估存在漏洞（风格流畅性被误认为事实准确性），分层评估和LLM增强生成共同提高了事实精确性和证据效用。

Conclusion: 研究指向一种混合人机治理模型，能够同时提高众包事实核查的严谨性和及时性。

Abstract: Community Notes, the crowd-sourced misinformation governance system on X
(formerly Twitter), enables users to flag misleading posts, attach contextual
notes, and vote on their helpfulness. However, our analysis of 30.8K
health-related notes reveals significant latency, with a median delay of 17.6
hours before the first note receives a helpfulness status. To improve
responsiveness during real-world misinformation surges, we propose CrowdNotes+,
a unified framework that leverages large language models (LLMs) to augment
Community Notes for faster and more reliable health misinformation governance.
CrowdNotes+ integrates two complementary modes: (1) evidence-grounded note
augmentation and (2) utility-guided note automation, along with a hierarchical
three-step evaluation that progressively assesses relevance, correctness, and
helpfulness. We instantiate the framework through HealthNotes, a benchmark of
1.2K helpfulness-annotated health notes paired with a fine-tuned helpfulness
judge. Experiments on fifteen LLMs reveal an overlooked loophole in current
helpfulness evaluation, where stylistic fluency is mistaken for factual
accuracy, and demonstrate that our hierarchical evaluation and LLM-augmented
generation jointly enhance factual precision and evidence utility. These
results point toward a hybrid human-AI governance model that improves both the
rigor and timeliness of crowd-sourced fact-checking.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [186] [Stop DDoS Attacking the Research Community with AI-Generated Survey Papers](https://arxiv.org/abs/2510.09686)
*Jianghao Lin,Rong Shan,Jiachen Zhu,Yunjia Xi,Yong Yu,Weinan Zhang*

Main category: cs.CY

TL;DR: AI生成综述论文的泛滥正在对研究社区构成"综述论文DDoS攻击"威胁，需要建立AI辅助写作规范、专家监督机制和动态实时综述基础设施来应对。


<details>
  <summary>Details</summary>
Motivation: AI大语言模型使综述论文从费时费力的学术工作变成了低投入高产出的产物，这种自动化虽然降低了门槛，但也带来了冗余、低质量甚至虚构的综述论文泛滥问题，威胁科研诚信。

Method: 通过定量趋势分析、质量审计和文化影响讨论，论证AI生成综述论文的泛滥问题，并提出解决方案。

Result: 发现AI生成综述论文正在淹没预印本平台，使研究人员不堪重负，并侵蚀科学记录的信任度。

Conclusion: 保护综述论文的完整性对研究社区已不再是可选项，而是必须采取的措施，需要建立AI使用规范、专家监督和新型基础设施。

Abstract: Survey papers are foundational to the scholarly progress of research
communities, offering structured overviews that guide both novices and experts
across disciplines. However, the recent surge of AI-generated surveys,
especially enabled by large language models (LLMs), has transformed this
traditionally labor-intensive genre into a low-effort, high-volume output.
While such automation lowers entry barriers, it also introduces a critical
threat: the phenomenon we term the "survey paper DDoS attack" to the research
community. This refers to the unchecked proliferation of superficially
comprehensive but often redundant, low-quality, or even hallucinated survey
manuscripts, which floods preprint platforms, overwhelms researchers, and
erodes trust in the scientific record. In this position paper, we argue that we
must stop uploading massive amounts of AI-generated survey papers (i.e., survey
paper DDoS attack) to the research community, by instituting strong norms for
AI-assisted review writing. We call for restoring expert oversight and
transparency in AI usage and, moreover, developing new infrastructures such as
Dynamic Live Surveys, community-maintained, version-controlled repositories
that blend automated updates with human curation. Through quantitative trend
analysis, quality audits, and cultural impact discussion, we show that
safeguarding the integrity of surveys is no longer optional but imperative to
the research community.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [187] [A Comprehensive Survey on Benchmarks and Solutions in Software Engineering of LLM-Empowered Agentic System](https://arxiv.org/abs/2510.09721)
*Jiale Guo,Suizhi Huang,Mei Li,Dong Huang,Xingsheng Chen,Regina Zhang,Zhijiang Guo,Han Yu,Siu-Ming Yiu,Christian Jensen,Pietro Lio,Kwok-Yan Lam*

Main category: cs.SE

TL;DR: 该论文首次对LLM赋能的软件工程进行了全面分析，提出了包含解决方案和基准测试两个维度的分类法，揭示了从简单提示工程到复杂智能体系统的发展路径，并指出了关键研究空白和未来方向。


<details>
  <summary>Details</summary>
Motivation: LLM在软件工程中的应用已引发范式转变，但领域缺乏对基准测试与解决方案之间相互关联的全面理解，阻碍了系统性进展和评估。

Method: 分析了150多篇近期论文，构建了包含解决方案（基于提示、微调、智能体）和基准测试（代码生成、翻译、修复等任务）两个维度的综合分类法，提出了统一的工作流程管道。

Result: 揭示了领域从简单提示工程到包含规划分解、推理自优化、记忆机制和工具增强的复杂智能体系统的演进路径，连接了50多个基准测试与对应解决方案策略。

Conclusion: 该调查为理解和推进LLM赋能的软件工程系统提供了基础资源，识别了多智能体协作框架、自演进代码生成系统等关键研究空白和未来方向。

Abstract: The integration of LLMs into software engineering has catalyzed a paradigm
shift from traditional rule-based systems to sophisticated agentic systems
capable of autonomous problem-solving. Despite this transformation, the field
lacks a comprehensive understanding of how benchmarks and solutions
interconnect, hindering systematic progress and evaluation. This survey
presents the first holistic analysis of LLM-empowered software engineering,
bridging the critical gap between evaluation and solution approaches. We
analyze 150+ recent papers and organize them into a comprehensive taxonomy
spanning two major dimensions: (1) Solutions, categorized into prompt-based,
fine-tuning-based, and agent-based paradigms, and (2) Benchmarks, covering code
generation, translation, repair, and other tasks. Our analysis reveals how the
field has evolved from simple prompt engineering to complex agentic systems
incorporating planning and decomposition, reasoning and self-refinement, memory
mechanisms, and tool augmentation. We present a unified pipeline that
illustrates the complete workflow from task specification to final
deliverables, demonstrating how different solution paradigms address varying
complexity levels across software engineering tasks. Unlike existing surveys
that focus on isolated aspects, we provide full-spectrum coverage connecting
50+ benchmarks with their corresponding solution strategies, enabling
researchers to identify optimal approaches for specific evaluation criteria.
Furthermore, we identify critical research gaps and propose actionable future
directions, including multi-agent collaboration frameworks, self-evolving code
generation systems, and integration of formal verification with LLM-based
methods. This survey serves as a foundational resource for researchers and
practitioners seeking to understand, evaluate, and advance LLM-empowered
software engineering systems.

</details>


### [188] [Operationalizing AI: Empirical Evidence on MLOps Practices, User Satisfaction, and Organizational Context](https://arxiv.org/abs/2510.09968)
*Stefan Pasch*

Main category: cs.SE

TL;DR: 该研究分析了8000多条AI开发平台的用户评论，发现7个MLOps实践与用户满意度显著正相关，表明有效的MLOps实施为AI开发带来实际价值。


<details>
  <summary>Details</summary>
Motivation: 虽然MLOps作为管理机器学习生命周期的实践集已出现，但关于这些实践是否以及如何支持用户开发和运营AI应用的实证证据仍然有限。

Method: 研究分析了G2.com上8000多条AI开发平台的用户评论，使用零样本分类方法测量评论对9个已建立的MLOps实践的情感倾向。

Result: 9个实践中的7个与用户满意度呈显著正相关关系；小公司用户较少讨论某些MLOps实践，但公司规模不调节MLOps与满意度的关系。

Conclusion: MLOps实践一旦应用，在不同组织环境中都被视为普遍有益的，组织背景影响MLOps的普及度和显著性，但不影响其价值感知。

Abstract: Organizational efforts to utilize and operationalize artificial intelligence
(AI) are often accompanied by substantial challenges, including scalability,
maintenance, and coordination across teams. In response, the concept of Machine
Learning Operations (MLOps) has emerged as a set of best practices that
integrate software engineering principles with the unique demands of managing
the ML lifecycle. Yet, empirical evidence on whether and how these practices
support users in developing and operationalizing AI applications remains
limited. To address this gap, this study analyzes over 8,000 user reviews of AI
development platforms from G2.com. Using zero-shot classification, we measure
review sentiment toward nine established MLOps practices, including continuous
integration and delivery (CI/CD), workflow orchestration, reproducibility,
versioning, collaboration, and monitoring. Seven of the nine practices show a
significant positive relationship with user satisfaction, suggesting that
effective MLOps implementation contributes tangible value to AI development.
However, organizational context also matters: reviewers from small firms
discuss certain MLOps practices less frequently, suggesting that organizational
context influences the prevalence and salience of MLOps, though firm size does
not moderate the MLOps-satisfaction link. This indicates that once applied,
MLOps practices are perceived as universally beneficial across organizational
settings.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [189] [Task-Aware Resolution Optimization for Visual Large Language Models](https://arxiv.org/abs/2510.09822)
*Weiqing Luo,Zhen Tan,Yifan Li,Xinyu Zhao,Kwonjoon Lee,Behzad Dariush,Tianlong Chen*

Main category: cs.CV

TL;DR: 提出了一种自适应视觉输入分辨率的方法，通过分析图像复杂性和模型不确定性来确定不同视觉语言任务的最佳分辨率，并使用参数高效微调技术扩展预训练VLLMs的视觉输入分辨率。


<details>
  <summary>Details</summary>
Motivation: 现有视觉大语言模型(VLLMs)通常预设固定分辨率用于下游任务，导致性能不佳。现实世界的视觉语言应用需要不同层次的感知粒度。

Method: 首先研究不同视觉语言任务的分辨率偏好，发现分辨率偏好与图像复杂性和VLLM在不同输入分辨率下的不确定性方差相关。基于此提出经验公式确定最佳分辨率。然后使用参数高效微调技术将预训练VLLMs的视觉输入分辨率扩展到确定的最佳分辨率。

Result: 在各种视觉语言任务上的广泛实验验证了该方法的有效性。

Conclusion: 通过自适应确定和扩展视觉输入分辨率，可以显著提升VLLMs在不同视觉语言任务上的性能。

Abstract: Real-world vision-language applications demand varying levels of perceptual
granularity. However, most existing visual large language models (VLLMs), such
as LLaVA, pre-assume a fixed resolution for downstream tasks, which leads to
subpar performance. To address this problem, we first conduct a comprehensive
and pioneering investigation into the resolution preferences of different
vision-language tasks, revealing a correlation between resolution preferences
with image complexity, and uncertainty variance of the VLLM at different image
input resolutions. Building on this insight, we propose an empirical formula to
determine the optimal resolution for a given vision-language task, combining
these two factors. Second, based on rigorous experiments, we propose a novel
parameter-efficient fine-tuning technique to extend the visual input resolution
of pre-trained VLLMs to the identified optimal resolution. Extensive
experiments on various vision-language tasks validate the effectiveness of our
method.

</details>


### [190] [A Survey on Agentic Multimodal Large Language Models](https://arxiv.org/abs/2510.10991)
*Huanjin Yao,Ruifei Zhang,Jiaxing Huang,Jingyi Zhang,Yibo Wang,Bo Fang,Ruolin Zhu,Yongcheng Jing,Shunyu Liu,Guanbin Li,Dacheng Tao*

Main category: cs.CV

TL;DR: 本文对Agentic多模态大语言模型进行了全面综述，建立了包含智能内部功能、外部工具调用和环境交互的三维概念框架，并整理了相关开源资源、数据集和应用方向。


<details>
  <summary>Details</summary>
Motivation: 随着自主智能体系统的革命性发展，研究社区正从传统的静态、被动、领域特定的AI智能体转向更动态、主动和可泛化的智能体AI。受智能体AI日益增长的兴趣及其向AGI发展的潜在轨迹所驱动。

Method: 建立了一个概念框架，从三个基本维度组织Agentic MLLMs：(i)智能内部功能作为系统指挥官，(ii)主动调用外部工具，(iii)与环境交互以在动态现实场景中维持目标导向行为。

Result: 编制了开发Agentic MLLMs的开源训练框架、训练和评估数据集，并持续跟踪该领域发展。

Conclusion: 综述了Agentic MLLMs的下游应用，并为这个快速发展的领域概述了未来的研究方向。

Abstract: With the recent emergence of revolutionary autonomous agentic systems,
research community is witnessing a significant shift from traditional static,
passive, and domain-specific AI agents toward more dynamic, proactive, and
generalizable agentic AI. Motivated by the growing interest in agentic AI and
its potential trajectory toward AGI, we present a comprehensive survey on
Agentic Multimodal Large Language Models (Agentic MLLMs). In this survey, we
explore the emerging paradigm of agentic MLLMs, delineating their conceptual
foundations and distinguishing characteristics from conventional MLLM-based
agents. We establish a conceptual framework that organizes agentic MLLMs along
three fundamental dimensions: (i) Agentic internal intelligence functions as
the system's commander, enabling accurate long-horizon planning through
reasoning, reflection, and memory; (ii) Agentic external tool invocation,
whereby models proactively use various external tools to extend their
problem-solving capabilities beyond their intrinsic knowledge; and (iii)
Agentic environment interaction further situates models within virtual or
physical environments, allowing them to take actions, adapt strategies, and
sustain goal-directed behavior in dynamic real-world scenarios. To further
accelerate research in this area for the community, we compile open-source
training frameworks, training and evaluation datasets for developing agentic
MLLMs. Finally, we review the downstream applications of agentic MLLMs and
outline future research directions for this rapidly evolving field. To
continuously track developments in this rapidly evolving field, we will also
actively update a public repository at
https://github.com/HJYao00/Awesome-Agentic-MLLMs.

</details>


### [191] [DocReward: A Document Reward Model for Structuring and Stylizing](https://arxiv.org/abs/2510.11391)
*Junpeng Liu,Yuzhong Zhao,Bowen Cao,Jiayu Ding,Yilin Jia,Tengchao Lv,Yupan Huang,Shaohan Huang,Nan Yang,Li Dong,Lei Cui,Tao Ge,Xun Wang,Huitian Jiao,Sun Mao,FNU Kartik,Si-Qing Chen,Wai Lam,Furu Wei*

Main category: cs.CV

TL;DR: 提出了DocReward文档奖励模型，专注于评估文档的结构和风格质量，解决了现有代理工作流主要关注文本质量而忽视视觉结构和风格的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的代理工作流自动化主要关注文本质量，忽略了视觉结构和风格对可读性和参与度的重要性，缺乏合适的奖励模型来指导生成具有更好结构和风格的文档。

Method: 构建了包含117K对文档的多领域数据集DocPair，涵盖32个领域和267种文档类型，使用Bradley-Terry损失训练DocReward模型来评分文档，惩罚与人工标注排名相矛盾的预测。

Result: DocReward在准确性上显著优于GPT-4o和GPT-5，分别高出30.6和19.4个百分点。在文档生成的外部评估中，DocReward的胜率达到60.8%，远高于GPT-5的37.7%。

Conclusion: DocReward模型能够有效评估文档的结构和风格质量，在指导生成代理产生人类偏好的文档方面表现出色，填补了现有代理工作流在视觉结构和风格评估方面的空白。

Abstract: Recent advances in agentic workflows have enabled the automation of tasks
such as professional document generation. However, they primarily focus on
textual quality, neglecting visual structure and style, which are crucial for
readability and engagement. This gap arises mainly from the absence of suitable
reward models to guide agentic workflows toward producing documents with
stronger structural and stylistic quality. To address this, we propose
DocReward, a document reward model that evaluates documents based on their
structure and style. We construct a multi-domain dataset DocPair of 117K paired
documents, covering 32 domains and 267 document types, each including a high-
and low-professionalism document with identical content but different structure
and style. This enables the model to evaluate professionalism comprehensively,
and in a textual-quality-agnostic way. DocReward is trained using the
Bradley-Terry loss to score documents, penalizing predictions that contradict
the annotated ranking. To assess the performance of reward models, we create a
test dataset containing document bundles ranked by well-educated human
evaluators. Notably, DocReward outperforms GPT-4o and GPT-5 in accuracy by 30.6
and 19.4 percentage points, respectively, demonstrating its superiority over
baselines. In an extrinsic evaluation of document generation, DocReward
achieves a significantly higher win rate of 60.8%, compared to GPT-5's 37.7%
win rate, demonstrating its utility in guiding generation agents toward
producing human-preferred documents.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [192] [Bhasha-Rupantarika: Algorithm-Hardware Co-design approach for Multilingual Neural Machine Translation](https://arxiv.org/abs/2510.10676)
*Mukul Lokhande,Tanushree Dewangan,Mohd Sharik Mansoori,Tejas Chaudhari,Akarsh J.,Damayanti Lokhande,Adam Teman,Santosh Kumar Vishvakarma*

Main category: cs.AR

TL;DR: Bhasha-Rupantarika是一个通过算法-硬件协同设计的高效多语言翻译系统，专为资源受限环境优化，采用超低精度量化(FP4)实现4.1倍模型压缩和4.2倍推理加速。


<details>
  <summary>Details</summary>
Motivation: 针对物联网设备等资源受限环境，需要轻量高效的实时多语言翻译系统，传统方法在模型大小和推理速度上难以满足需求。

Method: 采用算法-硬件协同设计，研究FP8、INT8、INT4和FP4等亚字节精度级别的模型部署，在FPGA加速器上实现量化感知翻译。

Result: FP4量化实现4.1倍模型压缩、4.2倍推理加速，吞吐量达66 tokens/s(提升4.8倍)，FPGA部署减少1.96倍LUTs和1.65倍FFs，相比OPU和HPTA分别提升2.2倍和4.6倍吞吐量。

Conclusion: 该研究为可部署的多语言AI系统提供了基于量化感知翻译和硬件效率的可行解决方案，特别适用于低资源语言环境。

Abstract: This paper introduces Bhasha-Rupantarika, a light and efficient multilingual
translation system tailored through algorithm-hardware codesign for
resource-limited settings. The method investigates model deployment at
sub-octet precision levels (FP8, INT8, INT4, and FP4), with experimental
results indicating a 4.1x reduction in model size (FP4) and a 4.2x speedup in
inference speed, which correlates with an increased throughput of 66 tokens/s
(improvement by 4.8x). This underscores the importance of ultra-low precision
quantization for real-time deployment in IoT devices using FPGA accelerators,
achieving performance on par with expectations. Our evaluation covers
bidirectional translation between Indian and international languages,
showcasing its adaptability in low-resource linguistic contexts. The FPGA
deployment demonstrated a 1.96x reduction in LUTs and a 1.65x decrease in FFs,
resulting in a 2.2x enhancement in throughput compared to OPU and a 4.6x
enhancement compared to HPTA. Overall, the evaluation provides a viable
solution based on quantisation-aware translation along with hardware efficiency
suitable for deployable multilingual AI systems. The entire codes
[https://github.com/mukullokhande99/Bhasha-Rupantarika/] and dataset for
reproducibility are publicly available, facilitating rapid integration and
further development by researchers.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [193] [ArtPerception: ASCII Art-based Jailbreak on LLMs with Recognition Pre-test](https://arxiv.org/abs/2510.10281)
*Guan-Yan Yang,Tzu-Yu Cheng,Ya-Wen Teng,Farn Wanga,Kuo-Hui Yeh*

Main category: cs.CR

TL;DR: ArtPerception是一个新颖的黑盒越狱框架，利用ASCII艺术绕过最先进LLMs的安全措施，通过两阶段方法实现高效的一次性恶意攻击。


<details>
  <summary>Details</summary>
Motivation: 现有安全对齐主要关注语义解释，使LLMs容易受到使用非标准数据表示的攻击。需要防御多模态解释空间，即使在纯文本输入中。

Method: 两阶段方法：阶段1进行一次性模型特定预测试以确定ASCII艺术识别的最佳参数；阶段2利用这些洞察发起高效的一次性恶意越狱攻击。提出改进的Levenshte距离(MLD)指标来评估LLM识别能力。

Result: 在四个最先进开源LLMs上展示了优越的越狱性能，成功迁移到GPT-4o、Claude Sonnet 3.7和DeepSeek-V3等商业模型，并对LLaMA Guard和Azure内容过滤器等防御进行了有效性分析。

Conclusion: 真正的LLM安全需要防御多模态解释空间，即使是在纯文本输入中，并突显了基于侦察的战略攻击的有效性。

Abstract: The integration of Large Language Models (LLMs) into computer applications
has introduced transformative capabilities but also significant security
challenges. Existing safety alignments, which primarily focus on semantic
interpretation, leave LLMs vulnerable to attacks that use non-standard data
representations. This paper introduces ArtPerception, a novel black-box
jailbreak framework that strategically leverages ASCII art to bypass the
security measures of state-of-the-art (SOTA) LLMs. Unlike prior methods that
rely on iterative, brute-force attacks, ArtPerception introduces a systematic,
two-phase methodology. Phase 1 conducts a one-time, model-specific pre-test to
empirically determine the optimal parameters for ASCII art recognition. Phase 2
leverages these insights to launch a highly efficient, one-shot malicious
jailbreak attack. We propose a Modified Levenshtein Distance (MLD) metric for a
more nuanced evaluation of an LLM's recognition capability. Through
comprehensive experiments on four SOTA open-source LLMs, we demonstrate
superior jailbreak performance. We further validate our framework's real-world
relevance by showing its successful transferability to leading commercial
models, including GPT-4o, Claude Sonnet 3.7, and DeepSeek-V3, and by conducting
a rigorous effectiveness analysis against potential defenses such as LLaMA
Guard and Azure's content filters. Our findings underscore that true LLM
security requires defending against a multi-modal space of interpretations,
even within text-only inputs, and highlight the effectiveness of strategic,
reconnaissance-based attacks. Content Warning: This paper includes potentially
harmful and offensive model outputs.

</details>


### [194] [Secret-Protected Evolution for Differentially Private Synthetic Text Generation](https://arxiv.org/abs/2510.10990)
*Tianze Wang,Zhaoyu Chen,Jian Du,Yingtai Xiao,Linjun Zhang,Qiang Yan*

Main category: cs.CR

TL;DR: 提出了SecPE框架，通过秘密感知保护机制改进差分隐私合成文本生成，在保护敏感信息的同时减少效用损失和计算开销。


<details>
  <summary>Details</summary>
Motivation: 现实世界中大量高质量文本因隐私问题无法自由使用，现有差分隐私合成文本生成方法采用统一保护机制，对非敏感内容过度保护，导致效用损失和计算开销大。

Method: 提出Secret-Protected Evolution (SecPE)框架，在私有演化基础上引入秘密感知保护机制，满足(p,r)-秘密保护，这是高斯差分隐私的松弛形式。

Result: 在OpenReview、PubMed和Yelp基准测试中，SecPE相比基于GDP的Aug-PE基线方法，始终获得更低的Fréchet Inception Distance和更高的下游任务准确率，同时需要更少的噪声来达到相同保护水平。

Conclusion: 秘密感知保护机制能够实现更实用和有效的隐私保护合成文本生成。

Abstract: Text data has become extremely valuable on large language models (LLMs) and
even lead to general artificial intelligence (AGI). A lot of high-quality text
in the real world is private and cannot be freely used due to privacy concerns.
Therefore, differentially private (DP) synthetic text generation has been
proposed, aiming to produce high-utility synthetic data while protecting
sensitive information. However, existing DP synthetic text generation imposes
uniform guarantees that often overprotect non-sensitive content, resulting in
substantial utility loss and computational overhead. Therefore, we propose
Secret-Protected Evolution (SecPE), a novel framework that extends private
evolution with secret-aware protection. Theoretically, we show that SecPE
satisfies $(\mathrm{p}, \mathrm{r})$-secret protection, constituting a
relaxation of Gaussian DP that enables tighter utility-privacy trade-offs,
while also substantially reducing computational complexity relative to baseline
methods. Empirically, across the OpenReview, PubMed, and Yelp benchmarks, SecPE
consistently achieves lower Fr\'echet Inception Distance (FID) and higher
downstream task accuracy than GDP-based Aug-PE baselines, while requiring less
noise to attain the same level of protection. Our results highlight that
secret-aware guarantees can unlock more practical and effective
privacy-preserving synthetic text generation.

</details>


### [195] [Bag of Tricks for Subverting Reasoning-based Safety Guardrails](https://arxiv.org/abs/2510.11570)
*Shuo Chen,Zhen Han,Haokun Chen,Bailan He,Shengyun Si,Jingpei Wu,Philip Torr,Volker Tresp,Jindong Gu*

Main category: cs.CR

TL;DR: 研究发现基于推理的安全护栏存在严重漏洞，简单的模板令牌添加就能绕过防御，导致有害响应。


<details>
  <summary>Details</summary>
Motivation: 揭示基于推理的安全护栏的脆弱性，这些护栏虽然能显著提升防御能力，但容易被细微的输入提示操纵所绕过。

Method: 开发了一系列攻击方法，包括白盒、灰盒和黑盒设置，从简单的模板操作到完全自动化的优化。

Result: 攻击成功率极高（超过90%），在多个基准测试和不同开源LRM模型上验证了这些漏洞的系统性存在。

Conclusion: 开源LRM需要更强的对齐技术来防止恶意滥用，当前基于推理的护栏存在系统性安全风险。

Abstract: Recent reasoning-based safety guardrails for Large Reasoning Models (LRMs),
such as deliberative alignment, have shown strong defense against jailbreak
attacks. By leveraging LRMs' reasoning ability, these guardrails help the
models to assess the safety of user inputs before generating final responses.
The powerful reasoning ability can analyze the intention of the input query and
will refuse to assist once it detects the harmful intent hidden by the
jailbreak methods. Such guardrails have shown a significant boost in defense,
such as the near-perfect refusal rates on the open-source gpt-oss series.
Unfortunately, we find that these powerful reasoning-based guardrails can be
extremely vulnerable to subtle manipulation of the input prompts, and once
hijacked, can lead to even more harmful results. Specifically, we first uncover
a surprisingly fragile aspect of these guardrails: simply adding a few template
tokens to the input prompt can successfully bypass the seemingly powerful
guardrails and lead to explicit and harmful responses. To explore further, we
introduce a bag of jailbreak methods that subvert the reasoning-based
guardrails. Our attacks span white-, gray-, and black-box settings and range
from effortless template manipulations to fully automated optimization. Along
with the potential for scalable implementation, these methods also achieve
alarmingly high attack success rates (e.g., exceeding 90% across 5 different
benchmarks on gpt-oss series on both local host models and online API
services). Evaluations across various leading open-source LRMs confirm that
these vulnerabilities are systemic, underscoring the urgent need for stronger
alignment techniques for open-sourced LRMs to prevent malicious misuse. Code is
open-sourced at https://chenxshuo.github.io/bag-of-tricks.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [196] [The Social Cost of Intelligence: Emergence, Propagation, and Amplification of Stereotypical Bias in Multi-Agent Systems](https://arxiv.org/abs/2510.10943)
*Thi-Nhung Nguyen,Linhao Luo,Thuy-Trang Vu,Dinh Phung*

Main category: cs.MA

TL;DR: 多智能体系统比单智能体系统更容易产生偏见，但合作和辩论式通信可以减轻偏见放大，更稳健的基础LLM能提高系统稳定性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型中的偏见问题持续存在，而多智能体系统的兴起带来了偏见涌现和传播的新动态，这尚未被充分研究。

Method: 模拟不同社会群体的智能体在社交情境中的互动，评估各种交互和对抗场景下的系统行为，使用三个偏见基准进行实验。

Result: 多智能体系统通常比单智能体系统更不稳健，偏见往往通过群体内偏袒早期出现，但合作和辩论式通信能减轻偏见放大。

Conclusion: 研究揭示了影响多智能体LLM系统公平性和韧性的关键因素，为构建更公平的多智能体系统提供了重要见解。

Abstract: Bias in large language models (LLMs) remains a persistent challenge,
manifesting in stereotyping and unfair treatment across social groups. While
prior research has primarily focused on individual models, the rise of
multi-agent systems (MAS), where multiple LLMs collaborate and communicate,
introduces new and largely unexplored dynamics in bias emergence and
propagation. In this work, we present a comprehensive study of stereotypical
bias in MAS, examining how internal specialization, underlying LLMs and
inter-agent communication protocols influence bias robustness, propagation, and
amplification. We simulate social contexts where agents represent different
social groups and evaluate system behavior under various interaction and
adversarial scenarios. Experiments on three bias benchmarks reveal that MAS are
generally less robust than single-agent systems, with bias often emerging early
through in-group favoritism. However, cooperative and debate-based
communication can mitigate bias amplification, while more robust underlying
LLMs improve overall system stability. Our findings highlight critical factors
shaping fairness and resilience in multi-agent LLM systems.

</details>


### [197] [Automating Structural Engineering Workflows with Large Language Model Agents](https://arxiv.org/abs/2510.11004)
*Haoran Liang,Yufa Zhou,Mohammad Talebi Kalaleh,Qipei Mei*

Main category: cs.MA

TL;DR: MASSE是首个用于结构工程的多智能体系统，通过集成大语言模型智能体与真实工程工作流，能够将专家工作量从约2小时减少到几分钟，同时提高可靠性和准确性。


<details>
  <summary>Details</summary>
Motivation: 结构工程是一个基础但传统停滞的领域，几十年来核心工作流程基本未变，尽管其具有显著经济影响和全球市场规模。大语言模型在复杂推理、长期规划和精确工具使用方面的进步与结构工程任务高度契合。

Method: 开发了基于大语言模型的多智能体系统MASSE，无需训练即可实现大多数真实世界结构工程工作流程的完全自动化，支持在专业环境中立即部署。

Result: 在真实案例研究中的全面验证表明，MASSE能够将专家工作量从约2小时减少到几分钟，同时在实践工程场景中提高可靠性和准确性。

Conclusion: MASSE证明了基于大语言模型的多智能体系统能够有效自动化结构工程工作流程，为这一传统领域带来革命性变革。

Abstract: We introduce $\textbf{MASSE}$, the first Multi-Agent System for Structural
Engineering, effectively integrating large language model (LLM)-based agents
with real-world engineering workflows. Structural engineering is a fundamental
yet traditionally stagnant domain, with core workflows remaining largely
unchanged for decades despite its substantial economic impact and global market
size. Recent advancements in LLMs have significantly enhanced their ability to
perform complex reasoning, long-horizon planning, and precise tool utilization
-- capabilities well aligned with structural engineering tasks such as
interpreting design codes, executing load calculations, and verifying
structural capacities. We present a proof-of-concept showing that most
real-world structural engineering workflows can be fully automated through a
training-free LLM-based multi-agent system. MASSE enables immediate deployment
in professional environments, and our comprehensive validation on real-world
case studies demonstrates that it can reduce expert workload from approximately
two hours to mere minutes, while enhancing both reliability and accuracy in
practical engineering scenarios.

</details>
