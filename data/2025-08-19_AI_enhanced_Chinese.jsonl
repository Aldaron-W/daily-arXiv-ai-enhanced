{"id": "2508.11676", "pdf": "https://arxiv.org/pdf/2508.11676", "abs": "https://arxiv.org/abs/2508.11676", "authors": ["Maksym Shamrai", "Vladyslav Hamolia"], "title": "Deep Language Geometry: Constructing a Metric Space from LLM Weights", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "18 pages, accepted to RANLP 2025", "summary": "We introduce a novel framework that utilizes the internal weight activations\nof modern Large Language Models (LLMs) to construct a metric space of\nlanguages. Unlike traditional approaches based on hand-crafted linguistic\nfeatures, our method automatically derives high-dimensional vector\nrepresentations by computing weight importance scores via an adapted pruning\nalgorithm. Our approach captures intrinsic language characteristics that\nreflect linguistic phenomena. We validate our approach across diverse datasets\nand multilingual LLMs, covering 106 languages. The results align well with\nestablished linguistic families while also revealing unexpected inter-language\nconnections that may indicate historical contact or language evolution. The\nsource code, computed language latent vectors, and visualization tool are made\npublicly available at https://github.com/mshamrai/deep-language-geometry.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eLLM\u5185\u90e8\u6743\u91cd\u6fc0\u6d3b\u6784\u5efa\u8bed\u8a00\u5ea6\u91cf\u7a7a\u95f4\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u526a\u679d\u7b97\u6cd5\u81ea\u52a8\u751f\u6210\u9ad8\u7ef4\u5411\u91cf\u8868\u793a\uff0c\u65e0\u9700\u624b\u5de5\u7279\u5f81\u5de5\u7a0b\uff0c\u5728106\u79cd\u8bed\u8a00\u4e0a\u9a8c\u8bc1\u4e86\u4e0e\u4f20\u7edf\u8bed\u7cfb\u5206\u7c7b\u7684\u4e00\u81f4\u6027\u5e76\u53d1\u73b0\u65b0\u7684\u8bed\u8a00\u5173\u8054", "motivation": "\u4f20\u7edf\u8bed\u8a00\u5206\u7c7b\u65b9\u6cd5\u4f9d\u8d56\u624b\u5de5\u8bbe\u8ba1\u7684\u8bed\u8a00\u5b66\u7279\u5f81\uff0c\u9700\u8981\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u4e14\u96be\u4ee5\u6355\u6349\u6df1\u5c42\u6b21\u7684\u8bed\u8a00\u5185\u5728\u7279\u6027\u3002\u672c\u6587\u65e8\u5728\u5229\u7528LLM\u7684\u5185\u90e8\u6743\u91cd\u6fc0\u6d3b\u81ea\u52a8\u53d1\u73b0\u8bed\u8a00\u95f4\u7684\u5ea6\u91cf\u5173\u7cfb", "method": "\u91c7\u7528\u6539\u8fdb\u7684\u526a\u679d\u7b97\u6cd5\u8ba1\u7b97\u6743\u91cd\u91cd\u8981\u6027\u5206\u6570\uff0c\u4ece\u591a\u8bed\u8a00LLM\u4e2d\u81ea\u52a8\u63a8\u5bfc\u9ad8\u7ef4\u5411\u91cf\u8868\u793a\uff0c\u6784\u5efa\u8bed\u8a00\u5ea6\u91cf\u7a7a\u95f4\u6765\u6355\u6349\u8bed\u8a00\u5185\u5728\u7279\u5f81", "result": "\u5728106\u79cd\u8bed\u8a00\u4e0a\u9a8c\u8bc1\uff0c\u7ed3\u679c\u4e0e\u4f20\u7edf\u8bed\u7cfb\u5206\u7c7b\u9ad8\u5ea6\u4e00\u81f4\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u610f\u60f3\u4e0d\u5230\u7684\u8bed\u8a00\u95f4\u8054\u7cfb\uff0c\u53ef\u80fd\u53cd\u6620\u5386\u53f2\u63a5\u89e6\u6216\u8bed\u8a00\u6f14\u5316\u5173\u7cfb", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u81ea\u52a8\u53d1\u73b0\u8bed\u8a00\u95f4\u7684\u6df1\u5c42\u5173\u7cfb\uff0c\u4e3a\u8bed\u8a00\u5206\u7c7b\u548c\u6f14\u5316\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u4ee3\u7801\u548c\u8bed\u8a00\u6f5c\u5728\u5411\u91cf\u5df2\u5f00\u6e90"}}
{"id": "2508.11758", "pdf": "https://arxiv.org/pdf/2508.11758", "abs": "https://arxiv.org/abs/2508.11758", "authors": ["Jonas van Elburg", "Peter van der Putten", "Maarten Marx"], "title": "Can we Evaluate RAGs with Synthetic Data?", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted for the SynDAiTE workshop at the European Conference on\n  Machine Learning and Principles and Practice of Knowledge Discovery in\n  Databases (ECML-PKDD 2025), September 15, 2025 - Porto, Portugal", "summary": "We investigate whether synthetic question-answer (QA) data generated by large\nlanguage models (LLMs) can serve as an effective proxy for human-labeled\nbenchmarks when such data is unavailable. We assess the reliability of\nsynthetic benchmarks across two experiments: one varying retriever parameters\nwhile keeping the generator fixed, and another varying the generator with fixed\nretriever parameters. Across four datasets, of which two open-domain and two\nproprietary, we find that synthetic benchmarks reliably rank the RAGs varying\nin terms of retriever configuration, aligning well with human-labeled benchmark\nbaselines. However, they fail to produce consistent RAG rankings when comparing\ngenerator architectures. The breakdown possibly arises from a combination of\ntask mismatch between the synthetic and human benchmarks, and stylistic bias\nfavoring certain generators.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLM\u751f\u6210\u7684\u5408\u6210QA\u6570\u636e\u5728\u8bc4\u4f30\u4e0d\u540c\u68c0\u7d22\u5668\u914d\u7f6e\u7684RAG\u7cfb\u7edf\u65f6\u80fd\u53ef\u9760\u66ff\u4ee3\u4eba\u5de5\u6807\u6ce8\u57fa\u51c6\uff0c\u4f46\u5728\u6bd4\u8f83\u4e0d\u540c\u751f\u6210\u5668\u67b6\u6784\u65f6\u65e0\u6cd5\u4ea7\u751f\u4e00\u81f4\u7684\u6392\u540d\u7ed3\u679c\u3002", "motivation": "\u63a2\u7d22\u5f53\u4eba\u5de5\u6807\u6ce8\u57fa\u51c6\u4e0d\u53ef\u7528\u65f6\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u5408\u6210\u95ee\u7b54\u6570\u636e\u662f\u5426\u80fd\u6709\u6548\u66ff\u4ee3\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u6765\u8bc4\u4f30\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u7cfb\u7edf\u3002", "method": "\u901a\u8fc7\u4e24\u4e2a\u5b9e\u9a8c\u8fdb\u884c\u8bc4\u4f30\uff1a1\uff09\u56fa\u5b9a\u751f\u6210\u5668\uff0c\u53d8\u5316\u68c0\u7d22\u5668\u53c2\u6570\uff1b2\uff09\u56fa\u5b9a\u68c0\u7d22\u5668\u53c2\u6570\uff0c\u53d8\u5316\u751f\u6210\u5668\u67b6\u6784\u3002\u5728\u56db\u4e2a\u6570\u636e\u96c6\uff08\u4e24\u4e2a\u5f00\u653e\u57df\u3001\u4e24\u4e2a\u4e13\u6709\uff09\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u5408\u6210\u57fa\u51c6\u5728\u8bc4\u4f30\u4e0d\u540c\u68c0\u7d22\u5668\u914d\u7f6e\u7684RAG\u7cfb\u7edf\u65f6\u8868\u73b0\u53ef\u9760\uff0c\u4e0e\u4eba\u5de5\u6807\u6ce8\u57fa\u51c6\u7ed3\u679c\u4e00\u81f4\uff1b\u4f46\u5728\u6bd4\u8f83\u4e0d\u540c\u751f\u6210\u5668\u67b6\u6784\u65f6\u65e0\u6cd5\u4ea7\u751f\u4e00\u81f4\u7684\u6392\u540d\uff0c\u53ef\u80fd\u7531\u4e8e\u4efb\u52a1\u4e0d\u5339\u914d\u548c\u98ce\u683c\u504f\u597d\u504f\u5dee\u5bfc\u81f4\u3002", "conclusion": "\u5408\u6210QA\u6570\u636e\u53ef\u4f5c\u4e3a\u8bc4\u4f30\u68c0\u7d22\u5668\u914d\u7f6e\u7684\u6709\u6548\u4ee3\u7406\uff0c\u4f46\u4e0d\u9002\u7528\u4e8e\u6bd4\u8f83\u4e0d\u540c\u751f\u6210\u5668\u67b6\u6784\u7684\u6027\u80fd\u8bc4\u4f30\u3002"}}
{"id": "2508.11767", "pdf": "https://arxiv.org/pdf/2508.11767", "abs": "https://arxiv.org/abs/2508.11767", "authors": ["Noah Kasmanoff", "Rahul Zalkikar"], "title": "Limitation Learning: Catching Adverse Dialog with GAIL", "categories": ["cs.CL", "cs.LG"], "comment": "Paper from 2021", "summary": "Imitation learning is a proven method for creating a policy in the absence of\nrewards, by leveraging expert demonstrations. In this work, we apply imitation\nlearning to conversation. In doing so, we recover a policy capable of talking\nto a user given a prompt (input state), and a discriminator capable of\nclassifying between expert and synthetic conversation. While our policy is\neffective, we recover results from our discriminator that indicate the\nlimitations of dialog models. We argue that this technique can be used to\nidentify adverse behavior of arbitrary data models common for dialog oriented\ntasks.", "AI": {"tldr": "\u5e94\u7528\u6a21\u4eff\u5b66\u4e60\u5230\u5bf9\u8bdd\u4efb\u52a1\u4e2d\uff0c\u901a\u8fc7\u4e13\u5bb6\u6f14\u793a\u8bad\u7ec3\u7b56\u7565\u548c\u5224\u522b\u5668\uff0c\u53d1\u73b0\u5bf9\u8bdd\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u8be5\u65b9\u6cd5\u53ef\u7528\u4e8e\u8bc6\u522b\u5bf9\u8bdd\u5bfc\u5411\u4efb\u52a1\u4e2d\u6570\u636e\u6a21\u578b\u7684\u4e0d\u826f\u884c\u4e3a", "motivation": "\u5728\u6ca1\u6709\u5956\u52b1\u7684\u60c5\u51b5\u4e0b\uff0c\u5229\u7528\u4e13\u5bb6\u6f14\u793a\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u521b\u5efa\u5bf9\u8bdd\u7b56\u7565\uff0c\u540c\u65f6\u8bad\u7ec3\u5224\u522b\u5668\u6765\u533a\u5206\u4e13\u5bb6\u5bf9\u8bdd\u548c\u5408\u6210\u5bf9\u8bdd", "method": "\u5e94\u7528\u6a21\u4eff\u5b66\u4e60\u6280\u672f\uff0c\u4ece\u4e13\u5bb6\u5bf9\u8bdd\u6f14\u793a\u4e2d\u5b66\u4e60\u7b56\u7565\uff08\u80fd\u591f\u6839\u636e\u63d0\u793a\u4e0e\u7528\u6237\u5bf9\u8bdd\uff09\uff0c\u5e76\u8bad\u7ec3\u5224\u522b\u5668\u6765\u5206\u7c7b\u4e13\u5bb6\u5bf9\u8bdd\u548c\u5408\u6210\u5bf9\u8bdd", "result": "\u7b56\u7565\u8868\u73b0\u6709\u6548\uff0c\u4f46\u5224\u522b\u5668\u7ed3\u679c\u663e\u793a\u5bf9\u8bdd\u6a21\u578b\u5b58\u5728\u5c40\u9650\u6027\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u8bc6\u522b\u5bf9\u8bdd\u5bfc\u5411\u4efb\u52a1\u4e2d\u5e38\u89c1\u6570\u636e\u6a21\u578b\u7684\u4e0d\u826f\u884c\u4e3a", "conclusion": "\u6a21\u4eff\u5b66\u4e60\u6280\u672f\u53ef\u7528\u4e8e\u6709\u6548\u8bc6\u522b\u5bf9\u8bdd\u6a21\u578b\u4e2d\u5b58\u5728\u7684\u4e0d\u826f\u884c\u4e3a\uff0c\u4e3a\u5bf9\u8bdd\u7cfb\u7edf\u7684\u8d28\u91cf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5"}}
{"id": "2508.11771", "pdf": "https://arxiv.org/pdf/2508.11771", "abs": "https://arxiv.org/abs/2508.11771", "authors": ["Leo Peckham", "Michael Ong", "Naomi Nagy", "Ewan Dunbar"], "title": "Investigating Transcription Normalization in the Faetar ASR Benchmark", "categories": ["cs.CL"], "comment": null, "summary": "We examine the role of transcription inconsistencies in the Faetar Automatic\nSpeech Recognition benchmark, a challenging low-resource ASR benchmark. With\nthe help of a small, hand-constructed lexicon, we conclude that find that,\nwhile inconsistencies do exist in the transcriptions, they are not the main\nchallenge in the task. We also demonstrate that bigram word-based language\nmodelling is of no added benefit, but that constraining decoding to a finite\nlexicon can be beneficial. The task remains extremely difficult.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0Faetar ASR\u57fa\u51c6\u4e2d\u7684\u8f6c\u5f55\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\u5b58\u5728\u4f46\u4e0d\u662f\u4e3b\u8981\u6311\u6218\uff0c\u6709\u9650\u8bcd\u5178\u7ea6\u675f\u89e3\u7801\u6709\u76ca\uff0c\u4f46\u4efb\u52a1\u4ecd\u7136\u6781\u5176\u56f0\u96be", "motivation": "\u68c0\u9a8c\u4f4e\u8d44\u6e90\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u57fa\u51c6\u4e2d\u8f6c\u5f55\u4e0d\u4e00\u81f4\u6027\u7684\u5f71\u54cd\uff0c\u786e\u5b9a\u5176\u662f\u5426\u662f\u4efb\u52a1\u7684\u4e3b\u8981\u6311\u6218", "method": "\u4f7f\u7528\u624b\u5de5\u6784\u5efa\u7684\u5c0f\u578b\u8bcd\u5178\u5206\u6790\u8f6c\u5f55\u4e0d\u4e00\u81f4\u6027\uff0c\u6d4b\u8bd5bigram\u8bcd\u7ea7\u8bed\u8a00\u6a21\u578b\u548c\u6709\u9650\u8bcd\u5178\u7ea6\u675f\u89e3\u7801\u7684\u6548\u679c", "result": "\u8f6c\u5f55\u4e0d\u4e00\u81f4\u786e\u5b9e\u5b58\u5728\u4f46\u4e0d\u662f\u4e3b\u8981\u95ee\u9898\uff0cbigram\u8bed\u8a00\u6a21\u578b\u65e0\u989d\u5916\u76ca\u5904\uff0c\u6709\u9650\u8bcd\u5178\u7ea6\u675f\u89e3\u7801\u6709\u79ef\u6781\u6548\u679c", "conclusion": "Faetar ASR\u4efb\u52a1\u6781\u5176\u56f0\u96be\uff0c\u8f6c\u5f55\u4e0d\u4e00\u81f4\u6027\u4e0d\u662f\u4e3b\u8981\u969c\u788d\uff0c\u8bcd\u5178\u7ea6\u675f\u662f\u6709\u6548\u7684\u6539\u8fdb\u65b9\u5411"}}
{"id": "2508.11779", "pdf": "https://arxiv.org/pdf/2508.11779", "abs": "https://arxiv.org/abs/2508.11779", "authors": ["Tianyi Li", "Yu Qin", "Olivia R. Liu Sheng"], "title": "A Multi-Task Evaluation of LLMs' Processing of Academic Text Input", "categories": ["cs.CL", "econ.GN", "q-fin.EC"], "comment": null, "summary": "How much large language models (LLMs) can aid scientific discovery, notably\nin assisting academic peer review, is in heated debate. Between a literature\ndigest and a human-comparable research assistant lies their practical\napplication potential. We organize individual tasks that computer science\nstudies employ in separate terms into a guided and robust workflow to evaluate\nLLMs' processing of academic text input. We employ four tasks in the\nassessment: content reproduction/comparison/scoring/reflection, each demanding\na specific role of the LLM (oracle/judgmental arbiter/knowledgeable\narbiter/collaborator) in assisting scholarly works, and altogether testing LLMs\nwith questions that increasingly require intellectual capabilities towards a\nsolid understanding of scientific texts to yield desirable solutions. We\nexemplify a rigorous performance evaluation with detailed instructions on the\nprompts. Adopting first-rate Information Systems articles at three top journals\nas the input texts and an abundant set of text metrics, we record a compromised\nperformance of the leading LLM - Google's Gemini: its summary and paraphrase of\nacademic text is acceptably reliable; using it to rank texts through pairwise\ntext comparison is faintly scalable; asking it to grade academic texts is prone\nto poor discrimination; its qualitative reflection on the text is\nself-consistent yet hardly insightful to inspire meaningful research. This\nevidence against an endorsement of LLMs' text-processing capabilities is\nconsistent across metric-based internal (linguistic assessment), external\n(comparing to the ground truth), and human evaluation, and is robust to the\nvariations of the prompt. Overall, we do not recommend an unchecked use of LLMs\nin constructing peer reviews.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u7279\u522b\u662fGoogle Gemini\uff09\u5728\u5b66\u672f\u6587\u672c\u5904\u7406\u65b9\u9762\u7684\u80fd\u529b\uff0c\u901a\u8fc7\u56db\u4e2a\u4efb\u52a1\u6d4b\u8bd5\u53d1\u73b0\u5176\u5728\u5b66\u672f\u540c\u884c\u8bc4\u5ba1\u4e2d\u7684\u8868\u73b0\u6709\u9650\uff0c\u4e0d\u5efa\u8bae\u65e0\u9650\u5236\u4f7f\u7528", "motivation": "\u63a2\u8ba8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8f85\u52a9\u5b66\u672f\u540c\u884c\u8bc4\u5ba1\u548c\u79d1\u5b66\u53d1\u73b0\u65b9\u9762\u7684\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\uff0c\u8bc4\u4f30\u5176\u5728\u5b66\u672f\u6587\u672c\u5904\u7406\u4e2d\u7684\u771f\u5b9e\u80fd\u529b", "method": "\u91c7\u7528\u56db\u4e2a\u4efb\u52a1\u8bc4\u4f30\u6846\u67b6\uff1a\u5185\u5bb9\u590d\u73b0/\u6bd4\u8f83/\u8bc4\u5206/\u53cd\u601d\uff0c\u4f7f\u7528\u9876\u7ea7\u4fe1\u606f\u7cfb\u7edf\u671f\u520a\u6587\u7ae0\u4f5c\u4e3a\u8f93\u5165\u6587\u672c\uff0c\u7ed3\u5408\u591a\u79cd\u6587\u672c\u6307\u6807\u8fdb\u884c\u4e25\u683c\u6027\u80fd\u8bc4\u4f30", "result": "Gemini\u5728\u5b66\u672f\u6587\u672c\u6458\u8981\u548c\u8f6c\u8ff0\u65b9\u9762\u8868\u73b0\u53ef\u63a5\u53d7\uff0c\u4f46\u5728\u6587\u672c\u6392\u5e8f\u3001\u8bc4\u5206\u548c\u6df1\u5ea6\u53cd\u601d\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u7f3a\u4e4f\u533a\u5206\u5ea6\u548c\u6d1e\u5bdf\u529b", "conclusion": "LLMs\u5728\u5b66\u672f\u6587\u672c\u5904\u7406\u80fd\u529b\u65b9\u9762\u5b58\u5728\u660e\u663e\u5c40\u9650\uff0c\u4e0d\u5efa\u8bae\u5728\u6784\u5efa\u540c\u884c\u8bc4\u5ba1\u4e2d\u65e0\u9650\u5236\u4f7f\u7528\uff0c\u9700\u8981\u66f4\u8c28\u614e\u7684\u5e94\u7528\u8bc4\u4f30"}}
{"id": "2508.11816", "pdf": "https://arxiv.org/pdf/2508.11816", "abs": "https://arxiv.org/abs/2508.11816", "authors": ["Krishna Chaitanya Marturi", "Heba H. Elwazzan"], "title": "LLM-Guided Planning and Summary-Based Scientific Text Simplification: DS@GT at CLEF 2025 SimpleText", "categories": ["cs.CL"], "comment": "Text Simplification, hallucination detection, LLMs, CLEF 2025,\n  SimpleText, CEUR-WS", "summary": "In this paper, we present our approach for the CLEF 2025 SimpleText Task 1,\nwhich addresses both sentence-level and document-level scientific text\nsimplification. For sentence-level simplification, our methodology employs\nlarge language models (LLMs) to first generate a structured plan, followed by\nplan-driven simplification of individual sentences. At the document level, we\nleverage LLMs to produce concise summaries and subsequently guide the\nsimplification process using these summaries. This two-stage, LLM-based\nframework enables more coherent and contextually faithful simplifications of\nscientific text.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e24\u9636\u6bb5\u79d1\u5b66\u6587\u672c\u7b80\u5316\u65b9\u6cd5\uff0c\u5728\u53e5\u5b50\u7ea7\u522b\u4f7f\u7528\u7ed3\u6784\u5316\u89c4\u5212\u6307\u5bfc\u7b80\u5316\uff0c\u5728\u6587\u6863\u7ea7\u522b\u901a\u8fc7\u6458\u8981\u5f15\u5bfc\u7b80\u5316\u8fc7\u7a0b", "motivation": "\u89e3\u51b3\u79d1\u5b66\u6587\u672c\u7b80\u5316\u4efb\u52a1\uff0c\u9700\u8981\u5728\u4fdd\u6301\u5185\u5bb9\u51c6\u786e\u6027\u7684\u540c\u65f6\u63d0\u9ad8\u53ef\u8bfb\u6027\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u5904\u7406\u53e5\u5b50\u7ea7\u522b\u548c\u6587\u6863\u7ea7\u522b\u7684\u8fde\u8d2f\u6027", "method": "\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4e24\u9636\u6bb5\u5904\u7406\uff1a\u53e5\u5b50\u7ea7\u522b\u5148\u751f\u6210\u7ed3\u6784\u5316\u89c4\u5212\u518d\u6309\u89c4\u5212\u7b80\u5316\uff1b\u6587\u6863\u7ea7\u522b\u5148\u751f\u6210\u6458\u8981\u518d\u7528\u6458\u8981\u6307\u5bfc\u7b80\u5316", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u4ea7\u751f\u66f4\u52a0\u8fde\u8d2f\u4e14\u4e0a\u4e0b\u6587\u5fe0\u5b9e\u5ea6\u66f4\u9ad8\u7684\u79d1\u5b66\u6587\u672c\u7b80\u5316\u7ed3\u679c", "conclusion": "\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e24\u9636\u6bb5\u89c4\u5212\u9a71\u52a8\u65b9\u6cd5\u5728\u79d1\u5b66\u6587\u672c\u7b80\u5316\u4efb\u52a1\u4e2d\u8868\u73b0\u6709\u6548\uff0c\u7279\u522b\u662f\u5728\u4fdd\u6301\u5185\u5bb9\u51c6\u786e\u6027\u548c\u8fde\u8d2f\u6027\u65b9\u9762"}}
{"id": "2508.11823", "pdf": "https://arxiv.org/pdf/2508.11823", "abs": "https://arxiv.org/abs/2508.11823", "authors": ["Krishna Chaitanya Marturi", "Heba H. Elwazzan"], "title": "Hallucination Detection and Mitigation in Scientific Text Simplification using Ensemble Approaches: DS@GT at CLEF 2025 SimpleText", "categories": ["cs.CL"], "comment": "Text Simplification, hallucination detection, LLMs, CLEF 2025,\n  SimpleText, CEUR-WS", "summary": "In this paper, we describe our methodology for the CLEF 2025 SimpleText Task\n2, which focuses on detecting and evaluating creative generation and\ninformation distortion in scientific text simplification. Our solution\nintegrates multiple strategies: we construct an ensemble framework that\nleverages BERT-based classifier, semantic similarity measure, natural language\ninference model, and large language model (LLM) reasoning. These diverse\nsignals are combined using meta-classifiers to enhance the robustness of\nspurious and distortion detection. Additionally, for grounded generation, we\nemploy an LLM-based post-editing system that revises simplifications based on\nthe original input texts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u96c6\u6210\u6846\u67b6\uff0c\u7ed3\u5408BERT\u5206\u7c7b\u5668\u3001\u8bed\u4e49\u76f8\u4f3c\u5ea6\u3001\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u548cLLM\u63a8\u7406\u6765\u68c0\u6d4b\u79d1\u5b66\u6587\u672c\u7b80\u5316\u4e2d\u7684\u521b\u9020\u6027\u751f\u6210\u548c\u4fe1\u606f\u5931\u771f\uff0c\u5e76\u4f7f\u7528LLM\u540e\u7f16\u8f91\u7cfb\u7edf\u8fdb\u884c\u57fa\u4e8e\u539f\u6587\u7684\u4fee\u6b63\u3002", "motivation": "\u89e3\u51b3\u79d1\u5b66\u6587\u672c\u7b80\u5316\u4efb\u52a1\u4e2d\u7684\u521b\u9020\u6027\u751f\u6210\u548c\u4fe1\u606f\u5931\u771f\u68c0\u6d4b\u95ee\u9898\uff0c\u63d0\u9ad8\u68c0\u6d4b\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002", "method": "\u6784\u5efa\u96c6\u6210\u6846\u67b6\uff0c\u7ed3\u5408BERT\u5206\u7c7b\u5668\u3001\u8bed\u4e49\u76f8\u4f3c\u5ea6\u6d4b\u91cf\u3001\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u6a21\u578b\u548cLLM\u63a8\u7406\uff0c\u4f7f\u7528\u5143\u5206\u7c7b\u5668\u6574\u5408\u591a\u79cd\u4fe1\u53f7\uff1b\u91c7\u7528LLM\u540e\u7f16\u8f91\u7cfb\u7edf\u57fa\u4e8e\u539f\u6587\u4fee\u8ba2\u7b80\u5316\u6587\u672c\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7efc\u5408\u6027\u7684\u68c0\u6d4b\u548c\u4fee\u6b63\u7cfb\u7edf\uff0c\u80fd\u591f\u6709\u6548\u8bc6\u522b\u79d1\u5b66\u6587\u672c\u7b80\u5316\u8fc7\u7a0b\u4e2d\u7684\u521b\u9020\u6027\u751f\u6210\u548c\u4fe1\u606f\u5931\u771f\u95ee\u9898\u3002", "conclusion": "\u8be5\u96c6\u6210\u65b9\u6cd5\u901a\u8fc7\u591a\u7b56\u7565\u878d\u5408\u548cLLM\u540e\u7f16\u8f91\uff0c\u4e3a\u79d1\u5b66\u6587\u672c\u7b80\u5316\u7684\u8d28\u91cf\u63a7\u5236\u548c\u5931\u771f\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.11828", "pdf": "https://arxiv.org/pdf/2508.11828", "abs": "https://arxiv.org/abs/2508.11828", "authors": ["Michael Flor", "Xinyi Liu", "Anna Feldman"], "title": "A Survey of Idiom Datasets for Psycholinguistic and Computational Research", "categories": ["cs.CL"], "comment": "KONVENS 2025. To appear", "summary": "Idioms are figurative expressions whose meanings often cannot be inferred\nfrom their individual words, making them difficult to process computationally\nand posing challenges for human experimental studies. This survey reviews\ndatasets developed in psycholinguistics and computational linguistics for\nstudying idioms, focusing on their content, form, and intended use.\nPsycholinguistic resources typically contain normed ratings along dimensions\nsuch as familiarity, transparency, and compositionality, while computational\ndatasets support tasks like idiomaticity detection/classification,\nparaphrasing, and cross-lingual modeling. We present trends in annotation\npractices, coverage, and task framing across 53 datasets. Although recent\nefforts expanded language coverage and task diversity, there seems to be no\nrelation yet between psycholinguistic and computational research on idioms.", "AI": {"tldr": "\u8fd9\u7bc7\u8c03\u67e5\u8bba\u6587\u7efc\u8ff0\u4e86\u8bed\u8a00\u5b66\u548c\u8ba1\u7b97\u8bed\u8a00\u5b66\u4e2d\u7528\u4e8e\u7814\u7a76\u4e60\u8bed\u768453\u4e2a\u6570\u636e\u96c6\uff0c\u5206\u6790\u4e86\u5b83\u4eec\u7684\u5185\u5bb9\u3001\u5f62\u5f0f\u548c\u7528\u9014\u3002", "motivation": "\u4e60\u8bed\u4f5c\u4e3a\u56fe\u5f0f\u8868\u8fbe\u5f0f\uff0c\u5176\u542b\u4e49\u65e0\u6cd5\u4ece\u5355\u8bcd\u63a8\u65ad\uff0c\u8fd9\u7ed9\u8ba1\u7b97\u5904\u7406\u548c\u4eba\u7c7b\u5b9e\u9a8c\u7814\u7a76\u5e26\u6765\u6311\u6218\u3002\u9700\u8981\u7cfb\u7edf\u6027\u5730\u8bc4\u4f30\u73b0\u6709\u7684\u4e60\u8bed\u7814\u7a76\u6570\u636e\u8d44\u6e90\u3002", "method": "\u8c03\u67e5\u5206\u6790\u4e8653\u4e2a\u4e60\u8bed\u6570\u636e\u96c6\uff0c\u5305\u62ec\u5fc3\u7406\u8bed\u8a00\u5b66\u8d44\u6e90\uff08\u5305\u542b\u719f\u6089\u5ea6\u3001\u900f\u660e\u5ea6\u3001\u7ec4\u5408\u6027\u7b49\u8bc4\u5206\uff09\u548c\u8ba1\u7b97\u8bed\u8a00\u5b66\u6570\u636e\u96c6\uff08\u652f\u6301\u4e60\u8bed\u68c0\u6d4b/\u5206\u7c7b\u3001\u6539\u5199\u3001\u8de8\u8bed\u8a00\u5efa\u6a21\u7b49\u4efb\u52a1\uff09\u3002", "result": "\u5c55\u793a\u4e86\u6ce8\u91ca\u5b9e\u8df5\u3001\u8986\u76d6\u8303\u56f4\u548c\u4efb\u52a1\u6784\u5efa\u65b9\u9762\u7684\u8d8b\u52bf\u3002\u8fd1\u671f\u7814\u7a76\u6269\u5927\u4e86\u8bed\u8a00\u8986\u76d6\u8303\u56f4\u548c\u4efb\u52a1\u591a\u6837\u6027\uff0c\u4f46\u5fc3\u7406\u8bed\u8a00\u5b66\u548c\u8ba1\u7b97\u8bed\u8a00\u5b66\u5728\u4e60\u8bed\u7814\u7a76\u65b9\u9762\u4ecd\u7136\u7f3a\u4e4f\u8054\u7cfb\u3002", "conclusion": "\u867d\u7136\u4e60\u8bed\u7814\u7a76\u6570\u636e\u96c6\u5728\u8bed\u8a00\u8986\u76d6\u548c\u4efb\u52a1\u591a\u6837\u6027\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u4e24\u4e2a\u5b66\u79d1\u9886\u57df\u4e4b\u95f4\u7684\u7814\u7a76\u4ecd\u5b58\u5728\u8131\u8282\uff0c\u9700\u8981\u66f4\u591a\u8de8\u5b66\u79d1\u5408\u4f5c\u6765\u63a8\u52a8\u4e60\u8bed\u7406\u89e3\u7684\u6df1\u5165\u7814\u7a76\u3002"}}
{"id": "2508.11829", "pdf": "https://arxiv.org/pdf/2508.11829", "abs": "https://arxiv.org/abs/2508.11829", "authors": ["Leigh Levinson", "Christopher J. Agostino"], "title": "Every 28 Days the AI Dreams of Soft Skin and Burning Stars: Scaffolding AI Agents with Hormones and Emotions", "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": "9 pages, 1 figure, submitted to NeurIPS Creative AI track", "summary": "Despite significant advances, AI systems struggle with the frame problem:\ndetermining what information is contextually relevant from an exponentially\nlarge possibility space. We hypothesize that biological rhythms, particularly\nhormonal cycles, serve as natural relevance filters that could address this\nfundamental challenge. We develop a framework that embeds simulated menstrual\nand circadian cycles into Large Language Models through system prompts\ngenerated from periodic functions modeling key hormones including estrogen,\ntestosterone, and cortisol. Across multiple state-of-the-art models, linguistic\nanalysis reveals emotional and stylistic variations that track biological\nphases; sadness peaks during menstruation while happiness dominates ovulation\nand circadian patterns show morning optimism transitioning to nocturnal\nintrospection. Benchmarking on SQuAD, MMLU, Hellaswag, and AI2-ARC demonstrates\nsubtle but consistent performance variations aligning with biological\nexpectations, including optimal function in moderate rather than extreme\nhormonal ranges. This methodology provides a novel approach to contextual AI\nwhile revealing how societal biases regarding gender and biology are embedded\nwithin language models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u901a\u8fc7\u6a21\u62df\u6708\u7ecf\u548c\u663c\u591c\u8282\u5f8b\u7b49\u751f\u7269\u8282\u5f8b\u6765\u589e\u5f3aAI\u7cfb\u7edf\u7684\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\u8fc7\u6ee4\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u89c2\u5bdf\u5230\u4e0e\u751f\u7269\u9884\u671f\u4e00\u81f4\u7684\u6027\u80fd\u53d8\u5316", "motivation": "\u89e3\u51b3AI\u7cfb\u7edf\u9762\u4e34\u7684\u6846\u67b6\u95ee\u9898\u2014\u2014\u4ece\u6307\u6570\u7ea7\u5927\u7684\u53ef\u80fd\u6027\u7a7a\u95f4\u4e2d\u786e\u5b9a\u4e0a\u4e0b\u6587\u76f8\u5173\u4fe1\u606f\uff0c\u53d7\u751f\u7269\u8282\u5f8b\u4f5c\u4e3a\u81ea\u7136\u76f8\u5173\u6027\u8fc7\u6ee4\u5668\u7684\u542f\u53d1", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u901a\u8fc7\u5468\u671f\u6027\u51fd\u6570\u6a21\u62df\u5173\u952e\u6fc0\u7d20\uff08\u96cc\u6fc0\u7d20\u3001\u777e\u916e\u3001\u76ae\u8d28\u9187\uff09\u751f\u6210\u7cfb\u7edf\u63d0\u793a\uff0c\u5c06\u6a21\u62df\u7684\u751f\u7269\u8282\u5f8b\u5d4c\u5165\u5927\u578b\u8bed\u8a00\u6a21\u578b", "result": "\u8bed\u8a00\u5206\u6790\u663e\u793a\u60c5\u611f\u548c\u98ce\u683c\u53d8\u5316\u8ddf\u8e2a\u751f\u7269\u9636\u6bb5\uff1a\u7ecf\u671f\u60b2\u4f24\u60c5\u7eea\u8fbe\u5230\u5cf0\u503c\uff0c\u6392\u5375\u671f\u4ee5\u5feb\u4e50\u4e3a\u4e3b\uff1b\u663c\u591c\u6a21\u5f0f\u663e\u793a\u65e9\u6668\u4e50\u89c2\u8f6c\u5411\u591c\u95f4\u5185\u7701\u3002\u5728SQuAD\u3001MMLU\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4e0e\u751f\u7269\u9884\u671f\u4e00\u81f4\u7684\u5fae\u5999\u4f46\u4e00\u81f4\u7684\u6027\u80fd\u53d8\u5316", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4e0a\u4e0b\u6587AI\u63d0\u4f9b\u4e86\u65b0\u9896\u9014\u5f84\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u8bed\u8a00\u6a21\u578b\u4e2d\u5d4c\u5165\u7684\u5173\u4e8e\u6027\u522b\u548c\u751f\u7269\u5b66\u7684\u793e\u4f1a\u504f\u89c1"}}
{"id": "2508.11831", "pdf": "https://arxiv.org/pdf/2508.11831", "abs": "https://arxiv.org/abs/2508.11831", "authors": ["Julia Sammartino", "Libby Barak", "Jing Peng", "Anna Feldman"], "title": "When Does Language Transfer Help? Sequential Fine-Tuning for Cross-Lingual Euphemism Detection", "categories": ["cs.CL", "cs.AI"], "comment": "RANLP 2025", "summary": "Euphemisms are culturally variable and often ambiguous, posing challenges for\nlanguage models, especially in low-resource settings. This paper investigates\nhow cross-lingual transfer via sequential fine-tuning affects euphemism\ndetection across five languages: English, Spanish, Chinese, Turkish, and\nYoruba. We compare sequential fine-tuning with monolingual and simultaneous\nfine-tuning using XLM-R and mBERT, analyzing how performance is shaped by\nlanguage pairings, typological features, and pretraining coverage. Results show\nthat sequential fine-tuning with a high-resource L1 improves L2 performance,\nespecially for low-resource languages like Yoruba and Turkish. XLM-R achieves\nlarger gains but is more sensitive to pretraining gaps and catastrophic\nforgetting, while mBERT yields more stable, though lower, results. These\nfindings highlight sequential fine-tuning as a simple yet effective strategy\nfor improving euphemism detection in multilingual models, particularly when\nlow-resource languages are involved.", "AI": {"tldr": "\u901a\u8fc7\u8de8\u8bed\u8a00\u8f6c\u79fb\u7684\u5e8f\u5217\u7cbe\u8c03\u63d0\u9ad8\u9690\u55bb\u8bed\u68c0\u6d4b\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5bf9\u8d44\u6e90\u8f83\u5c11\u7684\u8bed\u8a00\u5982\u7ea6\u9c81\u5df4\u8bed\u548c\u571f\u8033\u5176\u8bed", "motivation": "\u9690\u55bb\u8bed\u5177\u6709\u6587\u5316\u53d8\u5f02\u6027\u548c\u6a21\u7cca\u6027\uff0c\u7ed9\u8bed\u8a00\u6a21\u578b\u5e26\u6765\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e2d", "method": "\u4f7f\u7528XLM-R\u548cmBERT\u6a21\u578b\uff0c\u6bd4\u8f83\u5e8f\u5217\u7cbe\u8c03\u3001\u5355\u8bed\u8a00\u7cbe\u8c03\u548c\u540c\u65f6\u7cbe\u8c03\u57285\u79cd\u8bed\u8a00\uff08\u82f1\u8bed\u3001\u897f\u73ed\u7259\u8bed\u3001\u4e2d\u6587\u3001\u571f\u8033\u5176\u8bed\u3001\u7ea6\u9c81\u5df4\u8bed\uff09\u4e0a\u7684\u9690\u55bb\u8bed\u68c0\u6d4b\u6027\u80fd", "result": "\u4f7f\u7529\u9ad8\u8d44\u6e90\u8bed\u8a00\u8fdb\u884c\u5e8f\u5217\u7cbe\u8c03\u663e\u8457\u63d0\u9ad8\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u6027\u80fd\uff0cXLM-R\u83b7\u5f97\u66f4\u5927\u6536\u76ca\u4f46\u66f4\u654f\u611f\uff0cmBERT\u66f4\u7a33\u5b9a\u4f46\u6027\u80fd\u8f83\u4f4e", "conclusion": "\u5e8f\u5217\u7cbe\u8c03\u662f\u4e00\u79cd\u7b80\u5355\u4f46\u6709\u6548\u7684\u7b56\u7565\uff0c\u7279\u522b\u9002\u7528\u4e8e\u6539\u5584\u591a\u8bed\u8a00\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0a\u7684\u9690\u55bb\u8bed\u68c0\u6d4b\u80fd\u529b"}}
{"id": "2508.11857", "pdf": "https://arxiv.org/pdf/2508.11857", "abs": "https://arxiv.org/abs/2508.11857", "authors": ["Andrei-Valentin T\u0103nase", "Elena Pelican"], "title": "SupraTok: Cross-Boundary Tokenization for Enhanced Language Model Performance", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Tokenization remains a fundamental yet underexplored bottleneck in natural\nlanguage processing, with strategies largely static despite remarkable progress\nin model architectures. We present SupraTok, a novel tokenization architecture\nthat reimagines subword segmentation through three innovations: cross-boundary\npattern learning that discovers multi-word semantic units, entropy-driven data\ncuration that optimizes training corpus quality, and multi-phase curriculum\nlearning for stable convergence. Our approach extends Byte-Pair Encoding by\nlearning \"superword\" tokens, coherent multi-word expressions that preserve\nsemantic unity while maximizing compression efficiency. SupraTok achieves 31%\nimprovement in English tokenization efficiency (5.91 versus 4.51 characters per\ntoken) compared to OpenAI's o200k tokenizer and 30% improvement over Google's\nGemma 3 tokenizer (256k vocabulary), while maintaining competitive performance\nacross 38 languages. When integrated with a GPT-2 scale model (124M parameters)\ntrained on 10 billion tokens from the FineWeb-Edu dataset, SupraTok yields 8.4%\nimprovement on HellaSWAG and 9.5% on MMLU benchmarks without architectural\nmodifications. While these results are promising at this scale, further\nvalidation at larger model scales is needed. These findings suggest that\nefficient tokenization can complement architectural innovations as a path to\nimproved language model performance.", "AI": {"tldr": "SupraTok\u662f\u4e00\u79cd\u65b0\u9896\u7684\u5206\u8bcd\u67b6\u6784\uff0c\u901a\u8fc7\u8de8\u8fb9\u754c\u6a21\u5f0f\u5b66\u4e60\u3001\u71b5\u9a71\u52a8\u6570\u636e\u7b5b\u9009\u548c\u591a\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86\u6bd4OpenAI\u548cGoogle\u5206\u8bcd\u5668\u9ad830%\u4ee5\u4e0a\u7684\u6548\u7387\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u591a\u8bed\u8a00\u7ade\u4e89\u529b\uff0c\u5728GPT-2\u89c4\u6a21\u6a21\u578b\u4e0a\u5e26\u67658-9%\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5206\u8bcd\u4f5c\u4e3aNLP\u4e2d\u7684\u57fa\u7840\u4f46\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u74f6\u9888\uff0c\u73b0\u6709\u7b56\u7565\u5728\u6a21\u578b\u67b6\u6784\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\u7684\u60c5\u51b5\u4e0b\u4ecd\u7136\u76f8\u5bf9\u9759\u6001\uff0c\u9700\u8981\u91cd\u65b0\u601d\u8003\u5b50\u8bcd\u5206\u5272\u65b9\u6cd5\u3002", "method": "\u63d0\u51faSupraTok\u67b6\u6784\uff0c\u5305\u542b\u4e09\u4e2a\u521b\u65b0\uff1a\u8de8\u8fb9\u754c\u6a21\u5f0f\u5b66\u4e60\u53d1\u73b0\u591a\u8bcd\u8bed\u4e49\u5355\u5143\u3001\u71b5\u9a71\u52a8\u6570\u636e\u4f18\u5316\u8bad\u7ec3\u8bed\u6599\u8d28\u91cf\u3001\u591a\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\u786e\u4fdd\u7a33\u5b9a\u6536\u655b\u3002\u8be5\u65b9\u6cd5\u6269\u5c55\u4e86BPE\u7b97\u6cd5\uff0c\u5b66\u4e60\u4fdd\u6301\u8bed\u4e49\u5b8c\u6574\u6027\u7684\"\u8d85\u8bcd\"\u6807\u8bb0\u3002", "result": "\u82f1\u8bed\u5206\u8bcd\u6548\u7387\u63d0\u534731%\uff085.91 vs 4.51\u5b57\u7b26/\u8bcd\u5143\uff09\uff0c\u4f18\u4e8eOpenAI o200k\u548cGoogle Gemma 3\u5206\u8bcd\u5668\uff0c\u572838\u79cd\u8bed\u8a00\u4e2d\u4fdd\u6301\u7ade\u4e89\u529b\u3002\u96c6\u6210\u5230GPT-2\u6a21\u578b\uff08124M\u53c2\u6570\uff09\u540e\uff0c\u5728HellaSWAG\u548cMMLU\u57fa\u51c6\u4e0a\u5206\u522b\u63d0\u53478.4%\u548c9.5%\u3002", "conclusion": "\u9ad8\u6548\u5206\u8bcd\u53ef\u4ee5\u4f5c\u4e3a\u67b6\u6784\u521b\u65b0\u7684\u8865\u5145\u8def\u5f84\u6765\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u6027\u80fd\uff0c\u867d\u7136\u5c0f\u89c4\u6a21\u7ed3\u679c\u6709\u5e0c\u671b\uff0c\u4f46\u9700\u8981\u5728\u5927\u89c4\u6a21\u6a21\u578b\u4e0a\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002"}}
{"id": "2508.11889", "pdf": "https://arxiv.org/pdf/2508.11889", "abs": "https://arxiv.org/abs/2508.11889", "authors": ["Hui Ma", "Bo Zhang", "Jinpeng Hu", "Zenglin Shi"], "title": "In-Context Examples Matter: Improving Emotion Recognition in Conversation with Instruction Tuning", "categories": ["cs.CL"], "comment": null, "summary": "Emotion recognition in conversation (ERC) aims to identify the emotion of\neach utterance in a conversation, playing a vital role in empathetic artificial\nintelligence. With the growing of large language models (LLMs), instruction\ntuning has emerged as a critical paradigm for ERC. Existing studies mainly\nfocus on multi-stage instruction tuning, which first endows LLMs with speaker\ncharacteristics, and then conducts context-aware instruction tuning to\ncomprehend emotional states. However, these methods inherently constrains the\ncapacity to jointly capture the dynamic interaction between speaker\ncharacteristics and conversational context, resulting in weak alignment among\nspeaker identity, contextual cues, and emotion states within a unified\nframework. In this paper, we propose InitERC, a simple yet effective one-stage\nin-context instruction tuning framework for ERC. InitERC adapts LLMs to learn\nspeaker-context-emotion alignment from context examples via in-context\ninstruction tuning. Specifically, InitERC comprises four components, i.e.,\ndemonstration pool construction, in-context example selection, prompt template\ndesign, and in-context instruction tuning. To explore the impact of in-context\nexamples, we conduct a comprehensive study on three key factors: retrieval\nstrategy, example ordering, and the number of examples. Extensive experiments\non three widely used datasets demonstrate that our proposed InitERC achieves\nsubstantial improvements over the state-of-the-art baselines.", "AI": {"tldr": "\u63d0\u51faInitERC\uff0c\u4e00\u4e2a\u7b80\u5355\u6709\u6548\u7684\u4e00\u9636\u6bb5\u4e0a\u4e0b\u6587\u6307\u4ee4\u8c03\u4f18\u6846\u67b6\uff0c\u7528\u4e8e\u5bf9\u8bdd\u60c5\u611f\u8bc6\u522b\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u5b9e\u73b0\u8bf4\u8bdd\u4eba-\u4e0a\u4e0b\u6587-\u60c5\u611f\u7684\u5bf9\u9f50\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u9636\u6bb5\u6307\u4ee4\u8c03\u4f18\u65b9\u6cd5\u65e0\u6cd5\u8054\u5408\u6355\u6349\u8bf4\u8bdd\u4eba\u7279\u5f81\u548c\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u4e4b\u95f4\u7684\u52a8\u6001\u4ea4\u4e92\uff0c\u5bfc\u81f4\u5728\u7edf\u4e00\u6846\u67b6\u5185\u8bf4\u8bdd\u4eba\u8eab\u4efd\u3001\u4e0a\u4e0b\u6587\u7ebf\u7d22\u548c\u60c5\u611f\u72b6\u6001\u4e4b\u95f4\u7684\u5bf9\u9f50\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u63d0\u51faInitERC\u6846\u67b6\uff0c\u5305\u542b\u56db\u4e2a\u7ec4\u4ef6\uff1a\u6f14\u793a\u6c60\u6784\u5efa\u3001\u4e0a\u4e0b\u6587\u793a\u4f8b\u9009\u62e9\u3001\u63d0\u793a\u6a21\u677f\u8bbe\u8ba1\u548c\u4e0a\u4e0b\u6587\u6307\u4ee4\u8c03\u4f18\u3002\u901a\u8fc7\u5355\u9636\u6bb5\u4e0a\u4e0b\u6587\u6307\u4ee4\u8c03\u4f18\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\u4ece\u4e0a\u4e0b\u6587\u793a\u4f8b\u4e2d\u5b66\u4e60\u8bf4\u8bdd\u4eba-\u4e0a\u4e0b\u6587-\u60c5\u611f\u5bf9\u9f50\u3002", "result": "\u5728\u4e09\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5927\u91cf\u5b9e\u9a8c\uff0c\u8bc1\u660eInitERC\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "InitERC\u901a\u8fc7\u5355\u9636\u6bb5\u4e0a\u4e0b\u6587\u6307\u4ee4\u8c03\u4f18\u6709\u6548\u89e3\u51b3\u4e86\u8bf4\u8bdd\u4eba\u7279\u5f81\u548c\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u7684\u8054\u5408\u5efa\u6a21\u95ee\u9898\uff0c\u5728\u60c5\u611f\u8bc6\u522b\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2508.11915", "pdf": "https://arxiv.org/pdf/2508.11915", "abs": "https://arxiv.org/abs/2508.11915", "authors": ["Punya Syon Pandey", "Yongjin Yang", "Jiarui Liu", "Zhijing Jin"], "title": "CORE: Measuring Multi-Agent LLM Interaction Quality under Game-Theoretic Pressures", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Game-theoretic interactions between agents with Large Language Models (LLMs)\nhave revealed many emergent capabilities, yet the linguistic diversity of these\ninteractions has not been sufficiently quantified. In this paper, we present\nthe Conversational Robustness Evaluation Score: CORE, a metric to quantify the\neffectiveness of language use within multi-agent systems across different\ngame-theoretic interactions. CORE integrates measures of cluster entropy,\nlexical repetition, and semantic similarity, providing a direct lens of dialog\nquality. We apply CORE to pairwise LLM dialogs across competitive, cooperative,\nand neutral settings, further grounding our analysis in Zipf's and Heaps' Laws\nto characterize word frequency distributions and vocabulary growth. Our\nfindings show that cooperative settings exhibit both steeper Zipf distributions\nand higher Heap exponents, indicating more repetition alongside greater\nvocabulary expansion. In contrast, competitive interactions display lower Zipf\nand Heaps exponents, reflecting less repetition and more constrained\nvocabularies. These results provide new insights into how social incentives\ninfluence language adaptation, and highlight CORE as a robust diagnostic for\nmeasuring linguistic robustness in multi-agent LLM systems. Our code is\navailable at https://github.com/psyonp/core.", "AI": {"tldr": "\u63d0\u51faCORE\u6307\u6807\u6765\u91cf\u5316\u591a\u6e38\u620f\u4e92\u52a8\u4e2dLLM\u8bed\u8a00\u4f7f\u7528\u6548\u679c\uff0c\u53d1\u73b0\u5408\u4f5c\u73af\u5883\u4e0b\u8bed\u8a00\u91cd\u590d\u6027\u66f4\u9ad8\u4f46\u8bcd\u6c47\u6269\u5c55\u66f4\u5feb\uff0c\u7ade\u4e89\u73af\u5883\u5219\u76f8\u53cd", "motivation": "\u5f53\u524d\u5bf9LLM\u591a\u6e38\u620f\u4e92\u52a8\u4e2d\u8bed\u8a00\u591a\u6837\u6027\u7684\u91cf\u5316\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u4e2a\u7efc\u5408\u6307\u6807\u6765\u8bc4\u4f30\u5bf9\u8bdd\u8d28\u91cf", "method": "\u63d0\u51faCORE\u6307\u6807\uff0c\u7ed3\u5408\u805a\u7c7b\u71b5\u3001\u8bcd\u6c47\u91cd\u590d\u548c\u8bed\u4e49\u76f8\u4f3c\u6027\uff0c\u5e76\u57fa\u4e8eZipf\u548cHeaps\u5b9a\u5f8b\u5206\u6790\u4e0d\u540c\u6e38\u620f\u73af\u5883\u4e0b\u7684\u8bcd\u9891\u5206\u5e03\u548c\u8bcd\u6c47\u589e\u957f", "result": "\u5408\u4f5c\u73af\u5883\u5448\u73b0\u66f4\u6df1\u7684Zipf\u5206\u5e03\u548c\u66f4\u9ad8\u7684Heaps\u6307\u6570\uff08\u66f4\u591a\u91cd\u590d\u4f46\u8bcd\u6c47\u6269\u5c55\u66f4\u5feb\uff09\uff0c\u7ade\u4e89\u73af\u5883\u5219\u76f8\u53cd\uff08\u66f4\u5c11\u91cd\u590d\u4f46\u8bcd\u6c47\u66f4\u53d7\u9650\u5236\uff09", "conclusion": "\u793e\u4f1a\u6fc0\u52b1\u5f71\u54cd\u8bed\u8a00\u9002\u5e94\uff0cCORE\u6307\u6807\u53ef\u4f5c\u4e3a\u591a\u6e38\u620fLLM\u7cfb\u7edf\u8bed\u8a00\u7a33\u5065\u6027\u7684\u7efc\u5408\u8bc4\u4f30\u5de5\u5177"}}
{"id": "2508.11927", "pdf": "https://arxiv.org/pdf/2508.11927", "abs": "https://arxiv.org/abs/2508.11927", "authors": ["Jie Lu", "Du Jin", "Hitomi Yanaka"], "title": "LLMs Struggle with NLI for Perfect Aspect: A Cross-Linguistic Study in Chinese and Japanese", "categories": ["cs.CL"], "comment": "9 pages, 3 figures", "summary": "Unlike English, which uses distinct forms (e.g., had, has, will have) to mark\nthe perfect aspect across tenses, Chinese and Japanese lack separate\ngrammatical forms for tense within the perfect aspect, which complicates\nNatural Language Inference (NLI). Focusing on the perfect aspect in these\nlanguages, we construct a linguistically motivated, template-based NLI dataset\n(1,350 pairs per language). Experiments reveal that even advanced LLMs struggle\nwith temporal inference, particularly in detecting subtle tense and\nreference-time shifts. These findings highlight model limitations and\nunderscore the need for cross-linguistic evaluation in temporal semantics. Our\ndataset is available at https://github.com/Lujie2001/CrossNLI.", "AI": {"tldr": "\u4e2d\u65e5\u8bed\u8a00\u7f3a\u4e4f\u5b8c\u7f8e\u4f53\u7684\u660e\u786e\u65f6\u6001\u6807\u8bb0\uff0c\u589e\u52a0\u4e86\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u96be\u5ea6\u3002\u7814\u7a76\u6784\u5efa\u4e86\u8bed\u8a00\u5b66\u52a8\u673a\u6a21\u677f\u6570\u636e\u96c6\uff0c\u53d1\u73b0\u9ad8\u7ea7LLM\u5728\u65f6\u6001\u63a8\u7406\u4e0a\u8868\u73b0\u8f83\u5dee\uff0c\u5c24\u5176\u5728\u7ec6\u5fae\u65f6\u6001\u53d8\u5316\u68c0\u6d4b\u65b9\u9762\u3002", "motivation": "\u4e2d\u6587\u548c\u65e5\u8bed\u4e0d\u50cf\u82f1\u8bed\u90a3\u6837\u6709\u660e\u786e\u7684\u5b8c\u7f8e\u4f53\u65f6\u6001\u8bed\u6cd5\u5f62\u5f0f\uff0c\u8fd9\u7ed9\u81ea\u7136\u8bed\u8a00\u63a8\u7406(NLI)\u5e26\u6765\u4e86\u7279\u522b\u6311\u6218\u3002\u7814\u7a76\u8005\u60f3\u8981\u63a2\u7d22\u8bed\u8a00\u6a21\u578b\u5728\u8fd9\u4e9b\u8bed\u8a00\u4e2d\u7684\u65f6\u6001\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u8bed\u8a00\u5b66\u52a8\u673a\u7684\u6a21\u677f\u57fa\u7840NLI\u6570\u636e\u96c6\uff0c\u6bcf\u79cd\u8bed\u8a00\u5305\u542b1,350\u5bf9\u8bed\u6599\u3002\u91cd\u70b9\u5173\u6ce8\u5b8c\u7f8e\u4f53\u5728\u4e2d\u6587\u548c\u65e5\u8bed\u4e2d\u7684\u8868\u8fbe\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5373\u4f7f\u662f\u5148\u8fdb\u7684\u5927\u8bed\u8a00\u6a21\u578b(LLM)\u4e5f\u5728\u65f6\u6001\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u56f0\u96be\uff0c\u5c24\u5176\u5728\u68c0\u6d4b\u7ec6\u5fae\u7684\u65f6\u6001\u548c\u53c2\u8003\u65f6\u95f4\u53d8\u5316\u65b9\u9762\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u663e\u793a\u4e86\u6a21\u578b\u5728\u65f6\u6001\u8bed\u4e49\u7406\u89e3\u65b9\u9762\u7684\u9650\u5236\uff0c\u5e76\u5f3a\u8c03\u4e86\u8de8\u8bed\u8a00\u8bc4\u4f30\u5728\u65f6\u6001\u8bed\u4e49\u7814\u7a76\u4e2d\u7684\u91cd\u8981\u6027\u3002\u7814\u7a76\u6570\u636e\u96c6\u5df2\u5f00\u6e90\u63d0\u4f9b\u3002"}}
{"id": "2508.11933", "pdf": "https://arxiv.org/pdf/2508.11933", "abs": "https://arxiv.org/abs/2508.11933", "authors": ["Yue Wang", "Liesheng Wei", "Yuxiang Wang"], "title": "CAMF: Collaborative Adversarial Multi-agent Framework for Machine Generated Text Detection", "categories": ["cs.CL"], "comment": null, "summary": "Detecting machine-generated text (MGT) from contemporary Large Language\nModels (LLMs) is increasingly crucial amid risks like disinformation and\nthreats to academic integrity. Existing zero-shot detection paradigms, despite\ntheir practicality, often exhibit significant deficiencies. Key challenges\ninclude: (1) superficial analyses focused on limited textual attributes, and\n(2) a lack of investigation into consistency across linguistic dimensions such\nas style, semantics, and logic. To address these challenges, we introduce the\n\\textbf{C}ollaborative \\textbf{A}dversarial \\textbf{M}ulti-agent\n\\textbf{F}ramework (\\textbf{CAMF}), a novel architecture using multiple\nLLM-based agents. CAMF employs specialized agents in a synergistic three-phase\nprocess: \\emph{Multi-dimensional Linguistic Feature Extraction},\n\\emph{Adversarial Consistency Probing}, and \\emph{Synthesized Judgment\nAggregation}. This structured collaborative-adversarial process enables a deep\nanalysis of subtle, cross-dimensional textual incongruities indicative of\nnon-human origin. Empirical evaluations demonstrate CAMF's significant\nsuperiority over state-of-the-art zero-shot MGT detection techniques.", "AI": {"tldr": "\u57fa\u4e8e\u591a\u6cbb\u7406\u534f\u4f5c\u5bf9\u6297\u6846\u67b6CAMF\uff0c\u901a\u8fc7\u591a\u7ef4\u8bed\u8a00\u7279\u5f81\u63d0\u53d6\u3001\u5bf9\u6297\u6027\u4e00\u81f4\u6027\u63a2\u6d4b\u548c\u7efc\u5408\u5224\u65ad\u805a\u5408\u7684\u4e09\u9636\u6bb5\u8fc7\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u96f6\u6837\u672c\u673a\u5668\u751f\u6210\u6587\u672c\u68c0\u6d4b\u7684\u6027\u80fd", "motivation": "\u5f53\u524d\u96f6\u6837\u672c\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u4e24\u5927\u95ee\u9898\uff1a(1)\u5bf9\u6587\u672c\u5c5e\u6027\u7684\u5206\u6790\u6d45\u5c42\u6709\u9650\uff0c(2)\u7f3a\u4e4f\u5bf9\u8bed\u8a00\u591a\u7ef4\u5ea6\uff08\u98ce\u683c\u3001\u8bed\u4e49\u3001\u903b\u8f91\uff09\u4e00\u81f4\u6027\u7684\u7814\u7a76", "method": "\u63d0\u51faCAMF\u6846\u67b6\uff0c\u4f7f\u7528\u591a\u4e2aLLM\u57fa\u4e8e\u7684\u6cbb\u7406\u5728\u4e09\u4e2a\u534f\u540c\u9636\u6bb5\u4e2d\u5de5\u4f5c\uff1a\u591a\u7ef4\u8bed\u8a00\u7279\u5f81\u63d0\u53d6\u3001\u5bf9\u6297\u6027\u4e00\u81f4\u6027\u63a2\u6d4b\u3001\u7efc\u5408\u5224\u65ad\u805a\u5408", "result": "\u7ecf\u9a8c\u8bc4\u4f30\u8bc1\u660eCAMF\u5728\u96f6\u6837\u672cMGT\u68c0\u6d4b\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u6280\u672f", "conclusion": "CAMF\u901a\u8fc7\u7ed3\u6784\u5316\u7684\u534f\u4f5c-\u5bf9\u6297\u8fc7\u7a0b\uff0c\u80fd\u591f\u6df1\u5165\u5206\u6790\u6587\u672c\u4e2d\u7ec6\u5fae\u7684\u8de8\u7ef4\u5ea6\u4e0d\u534f\u8c03\u6027\uff0c\u4e3a\u68c0\u6d4b\u975e\u4eba\u7c7b\u751f\u6210\u6587\u672c\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5"}}
{"id": "2508.12031", "pdf": "https://arxiv.org/pdf/2508.12031", "abs": "https://arxiv.org/abs/2508.12031", "authors": ["Shaozhe Yin", "Jinyu Guo", "Kai Shuang", "Xia Liu", "Ruize Ou"], "title": "Learning Wisdom from Errors: Promoting LLM's Continual Relation Learning through Exploiting Error Cases", "categories": ["cs.CL"], "comment": null, "summary": "Continual Relation Extraction (CRE) aims to continually learn new emerging\nrelations while avoiding catastrophic forgetting. Existing CRE methods mainly\nuse memory replay and contrastive learning to mitigate catastrophic forgetting.\nHowever, these methods do not attach importance to the error cases that can\nreveal the model's cognitive biases more effectively. To address this issue, we\npropose an instruction-based continual contrastive tuning approach for Large\nLanguage Models (LLMs) in CRE. Different from existing CRE methods that\ntypically handle the training and memory data in a unified manner, this\napproach splits the training and memory data of each task into two parts\nrespectively based on the correctness of the initial responses and treats them\ndifferently through dual-task fine-tuning. In addition, leveraging the\nadvantages of LLM's instruction-following ability, we propose a novel\ninstruction-based contrastive tuning strategy for LLM to continuously correct\ncurrent cognitive biases with the guidance of previous data in an\ninstruction-tuning manner, which mitigates the gap between old and new\nrelations in a more suitable way for LLMs. We experimentally evaluate our model\non TACRED and FewRel, and the results show that our model achieves new\nstate-of-the-art CRE performance with significant improvements, demonstrating\nthe importance of specializing in exploiting error cases.", "AI": {"tldr": "\u901a\u8fc7\u6307\u4ee4\u57fa\u5bf9\u6bd4\u5fae\u8c03\u65b9\u6cd5\uff0c\u4e13\u95e8\u5229\u7528\u9519\u8bef\u6848\u4f8b\u6765\u7f13\u89e3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6301\u7eed\u5173\u7cfb\u63d0\u53d6\u4e2d\u7684\u5fd8\u5371\u95ee\u9898", "motivation": "\u73b0\u6709\u6301\u7eed\u5173\u7cfb\u63d0\u53d6\u65b9\u6cd5\u6ca1\u6709\u5145\u5206\u5229\u7528\u80fd\u66f4\u6709\u6548\u53cd\u6620\u6a21\u578b\u8ba4\u77e5\u504f\u5dee\u7684\u9519\u8bef\u6848\u4f8b", "method": "\u5c06\u6bcf\u4e2a\u4efb\u52a1\u7684\u8bad\u7ec3\u548c\u8bb0\u5fc6\u6570\u636e\u6309\u521d\u59cb\u54cd\u5e94\u6b63\u786e\u6027\u5206\u4e3a\u4e24\u90e8\u5206\uff0c\u901a\u8fc7\u53cc\u4efb\u52a1\u5fae\u8c03\u5339\u914d\u5904\u7406\uff0c\u4f7f\u7528\u6307\u4ee4\u57fa\u5bf9\u6bd4\u5fae\u8c03\u7b56\u7565\u6301\u7eed\u7f13\u89e3\u8ba4\u77e5\u504f\u5dee", "result": "\u5728TACRED\u548cFewRel\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u65b0\u7684\u6700\u9ad8\u6c34\u5e73\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6301\u7eed\u5173\u7cfb\u63d0\u53d6\u6027\u80fd", "conclusion": "\u4e13\u95e8\u5229\u7528\u9519\u8bef\u6848\u4f8b\u5728\u7f13\u89e3\u5927\u8bed\u8a00\u6a21\u578b\u5fd8\u5371\u95ee\u9898\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c"}}
{"id": "2508.12040", "pdf": "https://arxiv.org/pdf/2508.12040", "abs": "https://arxiv.org/abs/2508.12040", "authors": ["Jinyi Han", "Tingyun Li", "Shisong Chen", "Jie Shi", "Xinyi Wang", "Guanglei Yue", "Jiaqing Liang", "Xin Lin", "Liqian Wen", "Zulong Chen", "Yanghua Xiao"], "title": "Mind the Generation Process: Fine-Grained Confidence Estimation During LLM Generation", "categories": ["cs.CL", "cs.AI"], "comment": "The initial versin was made in August 2024", "summary": "While large language models (LLMs) have demonstrated remarkable performance\nacross diverse tasks, they fundamentally lack self-awareness and frequently\nexhibit overconfidence, assigning high confidence scores to incorrect\npredictions. Accurate confidence estimation is therefore critical for enhancing\nthe trustworthiness and reliability of LLM-generated outputs. However, existing\napproaches suffer from coarse-grained scoring mechanisms that fail to provide\nfine-grained, continuous confidence estimates throughout the generation\nprocess. To address these limitations, we introduce FineCE, a novel confidence\nestimation method that delivers accurate, fine-grained confidence scores during\ntext generation. Specifically, we first develop a comprehensive pipeline for\nconstructing training data that effectively captures the underlying\nprobabilistic distribution of LLM responses, and then train a model to predict\nconfidence scores for arbitrary text sequences in a supervised manner.\nFurthermore, we propose a Backward Confidence Integration (BCI) strategy that\nleverages information from the subsequent text to enhance confidence estimation\nfor the current sequence during inference. We also introduce three strategies\nfor identifying optimal positions to perform confidence estimation within the\ngeneration process. Extensive experiments on multiple benchmark datasets\ndemonstrate that FineCE consistently outperforms existing classical confidence\nestimation methods. Our code and all baselines used in the paper are available\non GitHub.", "AI": {"tldr": "FineCE\u662f\u4e00\u79cd\u65b0\u9896\u7684\u7ec6\u7c92\u5ea6\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u76d1\u7763\u5b66\u4e60\u548c\u540e\u5411\u7f6e\u4fe1\u5ea6\u96c6\u6210\u7b56\u7565\uff0c\u5728\u6587\u672c\u751f\u6210\u8fc7\u7a0b\u4e2d\u63d0\u4f9b\u51c6\u786e\u7684\u8fde\u7eed\u7f6e\u4fe1\u5ea6\u5206\u6570\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7f3a\u4e4f\u81ea\u6211\u610f\u8bc6\uff0c\u7ecf\u5e38\u5bf9\u9519\u8bef\u9884\u6d4b\u8868\u73b0\u51fa\u8fc7\u5ea6\u81ea\u4fe1\uff0c\u73b0\u6709\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u53ea\u80fd\u63d0\u4f9b\u7c97\u7c92\u5ea6\u7684\u8bc4\u5206\uff0c\u65e0\u6cd5\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u7684\u8fde\u7eed\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u3002", "method": "\u9996\u5148\u6784\u5efa\u8bad\u7ec3\u6570\u636e\u7ba1\u9053\u6355\u83b7LLM\u54cd\u5e94\u7684\u6982\u7387\u5206\u5e03\uff0c\u7136\u540e\u4ee5\u76d1\u7763\u65b9\u5f0f\u8bad\u7ec3\u6a21\u578b\u9884\u6d4b\u4efb\u610f\u6587\u672c\u5e8f\u5217\u7684\u7f6e\u4fe1\u5ea6\u5206\u6570\u3002\u63d0\u51fa\u540e\u5411\u7f6e\u4fe1\u5ea6\u96c6\u6210(BCI)\u7b56\u7565\u5229\u7528\u540e\u7eed\u6587\u672c\u4fe1\u606f\u589e\u5f3a\u5f53\u524d\u5e8f\u5217\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\uff0c\u5e76\u5f15\u5165\u4e09\u79cd\u7b56\u7565\u786e\u5b9a\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u6700\u4f73\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u4f4d\u7f6e\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cFineCE\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u7684\u7ecf\u5178\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u3002", "conclusion": "FineCE\u901a\u8fc7\u7ec6\u7c92\u5ea6\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u663e\u8457\u63d0\u9ad8\u4e86LLM\u751f\u6210\u8f93\u51fa\u7684\u53ef\u4fe1\u5ea6\u548c\u53ef\u9760\u6027\uff0c\u4e3a\u89e3\u51b3LLM\u8fc7\u5ea6\u81ea\u4fe1\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.12086", "pdf": "https://arxiv.org/pdf/2508.12086", "abs": "https://arxiv.org/abs/2508.12086", "authors": ["Yao Wu"], "title": "J6: Jacobian-Driven Role Attribution for Multi-Objective Prompt Optimization in LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50, 90C29, 62F07", "I.2.7; I.2.6; G.1.6"], "comment": "9 pages, 3 tables, 1 algorithm", "summary": "In large language model (LLM) adaptation, balancing multiple optimization\nobjectives such as improving factuality (heat) and increasing confidence (via\nlow entropy) poses a fundamental challenge, especially when prompt parameters\n(e.g., hidden-layer insertions h and embedding modifications w) interact in\nnon-trivial ways. Existing multi-objective optimization strategies often rely\non scalar gradient aggregation, ignoring the deeper geometric structure between\nobjectives and parameters. We propose J6, a structured Jacobian-based method\nthat decomposes the gradient interaction matrix into six interpretable\ncomponents. This decomposition enables both hard decision-making (e.g.,\nchoosing the dominant update direction via argmax) and soft strategies (e.g.,\nattention-style weighting via softmax over J6), forming a dynamic update\nframework that adapts to local conflict and synergy. Moreover, the\ninterpretable structure of J6 provides insight into parameter attribution, task\ninterference, and geometry-aligned adaptation. Our work introduces a principled\nand extensible mechanism for conflict-aware prompt optimization, and opens a\nnew avenue for incorporating structured Jacobian reasoning into multi-objective\nneural tuning.", "AI": {"tldr": "J6\u65b9\u6cd5\u901a\u8fc7\u96c5\u53ef\u6bd4\u77e9\u9635\u5206\u89e3\u4e3a\u516d\u4e2a\u53ef\u89e3\u91ca\u7ec4\u4ef6\uff0c\u89e3\u51b3LLM\u591a\u76ee\u6807\u4f18\u5316\u4e2d\u7684\u51b2\u7a81\u95ee\u9898\uff0c\u63d0\u4f9b\u786c\u51b3\u7b56\u548c\u8f6f\u7b56\u7565\u7684\u52a8\u6001\u66f4\u65b0\u6846\u67b6", "motivation": "\u73b0\u6709LLM\u591a\u76ee\u6807\u4f18\u5316\u65b9\u6cd5\u4f9d\u8d56\u6807\u91cf\u68af\u5ea6\u805a\u5408\uff0c\u5ffd\u7565\u4e86\u76ee\u6807\u4e0e\u53c2\u6570\u95f4\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u96be\u4ee5\u5e73\u8861\u4e8b\u5b9e\u6027\u63d0\u5347\u548c\u7f6e\u4fe1\u5ea6\u589e\u52a0\u7b49\u51b2\u7a81\u76ee\u6807", "method": "\u63d0\u51fa\u7ed3\u6784\u5316\u96c5\u53ef\u6bd4\u65b9\u6cd5J6\uff0c\u5c06\u68af\u5ea6\u4ea4\u4e92\u77e9\u9635\u5206\u89e3\u4e3a\u516d\u4e2a\u53ef\u89e3\u91ca\u7ec4\u4ef6\uff0c\u652f\u6301argmax\u786c\u51b3\u7b56\u548csoftmax\u8f6f\u7b56\u7565\u7684\u52a8\u6001\u66f4\u65b0\u6846\u67b6", "result": "J6\u65b9\u6cd5\u63d0\u4f9b\u4e86\u53c2\u6570\u5f52\u56e0\u3001\u4efb\u52a1\u5e72\u6270\u548c\u51e0\u4f55\u5bf9\u9f50\u9002\u5e94\u7684\u53ef\u89e3\u91ca\u6027\u6d1e\u5bdf\uff0c\u80fd\u591f\u6839\u636e\u5c40\u90e8\u51b2\u7a81\u548c\u534f\u540c\u5173\u7cfb\u81ea\u9002\u5e94\u8c03\u6574", "conclusion": "J6\u4e3a\u51b2\u7a81\u611f\u77e5\u7684\u63d0\u793a\u4f18\u5316\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u548c\u53ef\u6269\u5c55\u673a\u5236\uff0c\u5f00\u8f9f\u4e86\u5c06\u7ed3\u6784\u5316\u96c5\u53ef\u6bd4\u63a8\u7406\u878d\u5165\u591a\u76ee\u6807\u795e\u7ecf\u8c03\u4f18\u7684\u65b0\u9014\u5f84"}}
{"id": "2508.12096", "pdf": "https://arxiv.org/pdf/2508.12096", "abs": "https://arxiv.org/abs/2508.12096", "authors": ["Haiquan Hu", "Jiazhi Jiang", "Shiyou Xu", "Ruhan Zeng", "Tian Wang"], "title": "STEM: Efficient Relative Capability Evaluation of LLMs through Structured Transition Samples", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Submit to AAAI 2026", "summary": "Evaluating large language models (LLMs) has become increasingly challenging\nas model capabilities advance rapidly. While recent models often achieve higher\nscores on standard benchmarks, these improvements do not consistently reflect\nenhanced real-world reasoning capabilities. Moreover, widespread overfitting to\npublic benchmarks and the high computational cost of full evaluations have made\nit both expensive and less effective to distinguish meaningful differences\nbetween models. To address these challenges, we propose the \\textbf{S}tructured\n\\textbf{T}ransition \\textbf{E}valuation \\textbf{M}ethod (STEM), a lightweight\nand interpretable evaluation framework for efficiently estimating the relative\ncapabilities of LLMs. STEM identifies \\textit{significant transition samples}\n(STS) by analyzing consistent performance transitions among LLMs of the same\narchitecture but varying parameter scales. These samples enable STEM to\neffectively estimate the capability position of an unknown model. Qwen3 model\nfamily is applied to construct the STS pool on six diverse and representative\nbenchmarks. To assess generalizability. Experimental results indicate that STEM\nreliably captures performance trends, aligns with ground-truth rankings of\nmodel capability. These findings highlight STEM as a practical and scalable\nmethod for fine-grained, architecture-agnostic evaluation of LLMs.", "AI": {"tldr": "STEM\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u53ef\u89e3\u91ca\u7684LLM\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u540c\u67b6\u6784\u4e0d\u540c\u53c2\u6570\u89c4\u6a21\u6a21\u578b\u95f4\u7684\u6027\u80fd\u8f6c\u53d8\u6765\u8bc6\u522b\u5173\u952e\u6837\u672c\uff0c\u4ece\u800c\u9ad8\u6548\u4f30\u8ba1\u672a\u77e5\u6a21\u578b\u7684\u80fd\u529b\u4f4d\u7f6e\u3002", "motivation": "\u4f20\u7edf\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u96be\u4ee5\u6709\u6548\u533a\u5206\u6a21\u578b\u95f4\u7684\u771f\u5b9e\u80fd\u529b\u5dee\u5f02\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63d0\u51faSTEM\u6846\u67b6\uff0c\u901a\u8fc7\u8bc6\u522b\u663e\u8457\u8f6c\u53d8\u6837\u672c(STS)\u6765\u5206\u6790\u540c\u67b6\u6784\u4e0d\u540c\u53c2\u6570\u89c4\u6a21LLM\u95f4\u7684\u6027\u80fd\u4e00\u81f4\u6027\u8f6c\u53d8\uff0c\u6784\u5efaSTS\u6c60\u6765\u4f30\u8ba1\u672a\u77e5\u6a21\u578b\u80fd\u529b\u3002", "result": "\u5728\u516d\u4e2a\u591a\u6837\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u5e94\u7528Qwen3\u6a21\u578b\u65cf\u6784\u5efaSTS\u6c60\uff0c\u5b9e\u9a8c\u8868\u660eSTEM\u80fd\u53ef\u9760\u6355\u6349\u6027\u80fd\u8d8b\u52bf\uff0c\u4e0e\u771f\u5b9e\u6a21\u578b\u80fd\u529b\u6392\u540d\u4e00\u81f4\u3002", "conclusion": "STEM\u662f\u4e00\u79cd\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u7ec6\u7c92\u5ea6\u3001\u67b6\u6784\u65e0\u5173\u7684LLM\u8bc4\u4f30\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u89e3\u51b3\u4f20\u7edf\u8bc4\u4f30\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2508.12140", "pdf": "https://arxiv.org/pdf/2508.12140", "abs": "https://arxiv.org/abs/2508.12140", "authors": ["Ziqian Bi", "Lu Chen", "Junhao Song", "Hongying Luo", "Enze Ge", "Junmin Huang", "Tianyang Wang", "Keyu Chen", "Chia Xin Liang", "Zihan Wei", "Huafeng Liu", "Chunjie Tian", "Jibin Guan", "Joe Yeong", "Yongzhi Xu", "Peng Wang", "Junfeng Hao"], "title": "Exploring Efficiency Frontiers of Thinking Budget in Medical Reasoning: Scaling Laws between Computational Resources and Reasoning Quality", "categories": ["cs.CL"], "comment": null, "summary": "This study presents the first comprehensive evaluation of thinking budget\nmechanisms in medical reasoning tasks, revealing fundamental scaling laws\nbetween computational resources and reasoning quality. We systematically\nevaluated two major model families, Qwen3 (1.7B to 235B parameters) and\nDeepSeek-R1 (1.5B to 70B parameters), across 15 medical datasets spanning\ndiverse specialties and difficulty levels. Through controlled experiments with\nthinking budgets ranging from zero to unlimited tokens, we establish\nlogarithmic scaling relationships where accuracy improvements follow a\npredictable pattern with both thinking budget and model size. Our findings\nidentify three distinct efficiency regimes: high-efficiency (0 to 256 tokens)\nsuitable for real-time applications, balanced (256 to 512 tokens) offering\noptimal cost-performance tradeoffs for routine clinical support, and\nhigh-accuracy (above 512 tokens) justified only for critical diagnostic tasks.\nNotably, smaller models demonstrate disproportionately larger benefits from\nextended thinking, with 15 to 20% improvements compared to 5 to 10% for larger\nmodels, suggesting a complementary relationship where thinking budget provides\ngreater relative benefits for capacity-constrained models. Domain-specific\npatterns emerge clearly, with neurology and gastroenterology requiring\nsignificantly deeper reasoning processes than cardiovascular or respiratory\nmedicine. The consistency between Qwen3 native thinking budget API and our\nproposed truncation method for DeepSeek-R1 validates the generalizability of\nthinking budget concepts across architectures. These results establish thinking\nbudget control as a critical mechanism for optimizing medical AI systems,\nenabling dynamic resource allocation aligned with clinical needs while\nmaintaining the transparency essential for healthcare deployment.", "AI": {"tldr": "\u8fd9\u7814\u7a76\u9996\u6b21\u7cfb\u7edf\u8bc4\u4f30\u533b\u7597\u7406\u7531\u4efb\u52a1\u4e2d\u7684\u601d\u7ef4\u9884\u7b97\u673a\u5236\uff0c\u53d1\u73b0\u8ba1\u7b97\u8d44\u6e90\u4e0e\u7406\u7531\u8d28\u91cf\u5b58\u5728\u5bf9\u6570\u7f29\u653e\u5173\u7cfb\uff0c\u5e76\u786e\u5b9a\u4e86\u4e09\u79cd\u6548\u7387\u6a21\u5f0f\u9002\u7528\u4e8e\u4e0d\u540c\u4e34\u5e8a\u573a\u666f\u3002", "motivation": "\u7406\u89e3\u533b\u7597AI\u7cfb\u7edf\u4e2d\u601d\u7ef4\u9884\u7b97\u673a\u5236\u5bf9\u7406\u7531\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u4ee5\u4f18\u5316\u8d44\u6e90\u5206\u914d\u5e76\u63d0\u9ad8\u4e34\u5e8a\u90e8\u7f72\u6548\u7387\u3002", "method": "\u7cfb\u7edf\u6027\u8bc4\u4f30Qwen3\u548cDeepSeek-R1\u4e24\u5927\u6a21\u578b\u5bb6\u65cf\uff0c\u6d89\u53ca1.5B\u5230235B\u53c2\u6570\u8303\u56f4\uff0c\u572815\u4e2a\u533b\u7597\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u601d\u7ef4\u9884\u7b97\u5b9e\u9a8c\uff08\u4ece\u96f6\u5230\u65e0\u9650\u4ee4\u724c\uff09\u3002", "result": "\u53d1\u73b0\u51c6\u786e\u7387\u6539\u5584\u4e0e\u601d\u7ef4\u9884\u7b97\u548c\u6a21\u578b\u5927\u5c0f\u5448\u5bf9\u6570\u5173\u7cfb\uff1b\u5c0f\u6a21\u578b\u4ece\u6269\u5c55\u601d\u7ef4\u4e2d\u83b7\u76ca\u66f4\u5927\uff08\u6536\u76ca\u6bd415-20%\uff09\uff1b\u4e0d\u540c\u533b\u5b66\u4e13\u79d1\u9700\u8981\u4e0d\u540c\u6df1\u5ea6\u7684\u7406\u7531\u8fc7\u7a0b\u3002", "conclusion": "\u601d\u7ef4\u9884\u7b97\u63a7\u5236\u662f\u4f18\u5316\u533b\u7597AI\u7cfb\u7edf\u7684\u5173\u952e\u673a\u5236\uff0c\u80fd\u591f\u6839\u636e\u4e34\u5e8a\u9700\u6c42\u52a8\u6001\u5206\u914d\u8d44\u6e90\uff0c\u540c\u65f6\u4fdd\u6301\u9002\u5408\u533b\u7597\u90e8\u7f72\u7684\u900f\u660e\u6027\u3002"}}
{"id": "2508.12158", "pdf": "https://arxiv.org/pdf/2508.12158", "abs": "https://arxiv.org/abs/2508.12158", "authors": ["Stephen Meisenbacher", "Alexandra Klymenko", "Florian Matthes"], "title": "LLM-as-a-Judge for Privacy Evaluation? Exploring the Alignment of Human and LLM Perceptions of Privacy in Textual Data", "categories": ["cs.CL"], "comment": "13 pages, 3 figures, 4 tables. Accepted to HAIPS @ CCS 2025", "summary": "Despite advances in the field of privacy-preserving Natural Language\nProcessing (NLP), a significant challenge remains the accurate evaluation of\nprivacy. As a potential solution, using LLMs as a privacy evaluator presents a\npromising approach $\\unicode{x2013}$ a strategy inspired by its success in\nother subfields of NLP. In particular, the so-called $\\textit{LLM-as-a-Judge}$\nparadigm has achieved impressive results on a variety of natural language\nevaluation tasks, demonstrating high agreement rates with human annotators.\nRecognizing that privacy is both subjective and difficult to define, we\ninvestigate whether LLM-as-a-Judge can also be leveraged to evaluate the\nprivacy sensitivity of textual data. Furthermore, we measure how closely LLM\nevaluations align with human perceptions of privacy in text. Resulting from a\nstudy involving 10 datasets, 13 LLMs, and 677 human survey participants, we\nconfirm that privacy is indeed a difficult concept to measure empirically,\nexhibited by generally low inter-human agreement rates. Nevertheless, we find\nthat LLMs can accurately model a global human privacy perspective, and through\nan analysis of human and LLM reasoning patterns, we discuss the merits and\nlimitations of LLM-as-a-Judge for privacy evaluation in textual data. Our\nfindings pave the way for exploring the feasibility of LLMs as privacy\nevaluators, addressing a core challenge in solving pressing privacy issues with\ninnovative technical solutions.", "AI": {"tldr": "\u4f7f\u7528LLM\u4f5c\u4e3a\u9690\u79c1\u8bc4\u4f30\u5668\u7684\u53ef\u884c\u6027\u7814\u7a76\uff0cLLM\u80fd\u591f\u6a21\u62df\u5168\u5c40\u4eba\u7c7b\u9690\u79c1\u89c2\u70b9\uff0c\u4f46\u9690\u79c1\u672c\u8eab\u8bc4\u4f30\u5b58\u5728\u56f0\u96be", "motivation": "\u89e3\u51b3\u9690\u79c1\u4fdd\u62a4NLP\u9886\u57df\u4e2d\u9690\u79c1\u8bc4\u4f30\u7684\u6311\u6218\uff0c\u6d4b\u8bd5LLM-as-a-Judge\u6a21\u5f0f\u5728\u9690\u79c1\u654f\u611f\u6027\u8bc4\u4f30\u4e2d\u7684\u6548\u679c", "method": "\u4f7f\u752810\u4e2a\u6570\u636e\u96c6\u300113\u4e2aLLM\u6a21\u578b\u548c677\u540d\u4eba\u7c7b\u53c2\u4e0e\u8005\u8fdb\u884c\u7814\u7a76\uff0c\u6bd4\u8f83LLM\u4e0e\u4eba\u7c7b\u5728\u6587\u672c\u9690\u79c1\u8bc4\u4f30\u4e0a\u7684\u4e00\u81f4\u6027", "result": "\u4eba\u7c7b\u4e4b\u95f4\u9690\u79c1\u8bc4\u4f30\u4e00\u81f4\u6027\u8f83\u4f4e\uff0c\u4f46LLM\u80fd\u591f\u51c6\u786e\u6a21\u62df\u5168\u5c40\u4eba\u7c7b\u9690\u79c1\u89c2\u70b9\uff0c\u5c55\u73b0\u4e86LLM-as-a-Judge\u5728\u9690\u79c1\u8bc4\u4f30\u4e2d\u7684\u6f5c\u529b", "conclusion": "LLM\u4f5c\u4e3a\u9690\u79c1\u8bc4\u4f30\u5668\u5177\u6709\u53ef\u884c\u6027\uff0c\u4e3a\u89e3\u51b3\u9690\u79c1\u95ee\u9898\u63d0\u4f9b\u4e86\u521b\u65b0\u6280\u672f\u89e3\u51b3\u65b9\u6848\u7684\u57fa\u7840"}}
{"id": "2508.12227", "pdf": "https://arxiv.org/pdf/2508.12227", "abs": "https://arxiv.org/abs/2508.12227", "authors": ["Abdelhamid Haouhat", "Slimane Bellaouar", "Attia Nehar", "Hadda Cherroun", "Ahmed Abdelali"], "title": "Arabic Multimodal Machine Learning: Datasets, Applications, Approaches, and Challenges", "categories": ["cs.CL"], "comment": null, "summary": "Multimodal Machine Learning (MML) aims to integrate and analyze information\nfrom diverse modalities, such as text, audio, and visuals, enabling machines to\naddress complex tasks like sentiment analysis, emotion recognition, and\nmultimedia retrieval. Recently, Arabic MML has reached a certain level of\nmaturity in its foundational development, making it time to conduct a\ncomprehensive survey. This paper explores Arabic MML by categorizing efforts\nthrough a novel taxonomy and analyzing existing research. Our taxonomy\norganizes these efforts into four key topics: datasets, applications,\napproaches, and challenges. By providing a structured overview, this survey\noffers insights into the current state of Arabic MML, highlighting areas that\nhave not been investigated and critical research gaps. Researchers will be\nempowered to build upon the identified opportunities and address challenges to\nadvance the field.", "AI": {"tldr": "\u963f\u62c9\u4f2f\u591a\u6a21\u6001\u673a\u5668\u5b66\u4e60\u7efc\u8ff0\u6587\u7ae0\uff0c\u901a\u8fc7\u65b0\u7684\u5206\u7c7b\u6cd5\u5bf9\u8be5\u9886\u57df\u8fdb\u884c\u4e86\u7ed3\u6784\u5316\u5206\u6790\u548c\u603b\u7ed3", "motivation": "\u963f\u62c9\u4f2f\u591a\u6a21\u6001\u673a\u5668\u5b66\u4e60\u5df2\u7ecf\u53d1\u5c55\u5230\u4e00\u5b9a\u6210\u719f\u9636\u6bb5\uff0c\u9700\u8981\u8fdb\u884c\u5168\u9762\u7684\u7efc\u8ff0\u6027\u7814\u7a76\u6765\u603b\u7ed3\u5f53\u524d\u72b6\u51b5\u548c\u6307\u660e\u672a\u6765\u65b9\u5411", "method": "\u4f7f\u7528\u65b0\u7684\u5206\u7c7b\u6cd5\u5c06\u963f\u62c9\u4f2fMML\u7814\u7a76\u5206\u4e3a\u56db\u4e2a\u4e3b\u8981\u8bdd\u9898\uff1a\u6570\u636e\u96c6\u3001\u5e94\u7528\u573a\u666f\u3001\u65b9\u6cd5\u63a5\u53e3\u548c\u6311\u6218\u96be\u9898\uff0c\u5e76\u5bf9\u73b0\u6709\u7814\u7a76\u8fdb\u884c\u6df1\u5165\u5206\u6790", "result": "\u63d0\u4f9b\u4e86\u963f\u62c9\u4f2fMML\u9886\u57df\u7684\u7ed3\u6784\u5316\u6982\u89c8\uff0c\u8bc6\u522b\u51fa\u4e86\u5c1a\u672a\u88ab\u6d89\u53ca\u7684\u7814\u7a76\u533a\u57df\u548c\u91cd\u8981\u7684\u7814\u7a76\u7a7a\u767d", "conclusion": "\u8be5\u7efc\u8ff0\u6587\u7ae0\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u5efa\u8bae\u6027\u7684\u7814\u7a76\u6846\u67b6\uff0c\u5e2e\u52a9\u4ed6\u4eec\u57fa\u4e8e\u8bc6\u522b\u7684\u673a\u9047\u548c\u6311\u6218\u6765\u63a8\u52a8\u963f\u62c9\u4f2f\u591a\u6a21\u6001\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u53d1\u5c55"}}
{"id": "2508.12243", "pdf": "https://arxiv.org/pdf/2508.12243", "abs": "https://arxiv.org/abs/2508.12243", "authors": ["Wuttikorn Ponwitayarat", "Raymond Ng", "Jann Railey Montalan", "Thura Aung", "Jian Gang Ngui", "Yosephine Susanto", "William Tjhi", "Panuthep Tasawong", "Erik Cambria", "Ekapol Chuangsuwanich", "Sarana Nutanong", "Peerat Limkonchotiwat"], "title": "SEA-BED: Southeast Asia Embedding Benchmark", "categories": ["cs.CL"], "comment": null, "summary": "Sentence embeddings are essential for NLP tasks such as semantic search,\nre-ranking, and textual similarity. Although multilingual benchmarks like MMTEB\nbroaden coverage, Southeast Asia (SEA) datasets are scarce and often\nmachine-translated, missing native linguistic properties. With nearly 700\nmillion speakers, the SEA region lacks a region-specific embedding benchmark.\nWe introduce SEA-BED, the first large-scale SEA embedding benchmark with 169\ndatasets across 9 tasks and 10 languages, where 71% are formulated by humans,\nnot machine generation or translation. We address three research questions: (1)\nwhich SEA languages and tasks are challenging, (2) whether SEA languages show\nunique performance gaps globally, and (3) how human vs. machine translations\naffect evaluation. We evaluate 17 embedding models across six studies,\nanalyzing task and language challenges, cross-benchmark comparisons, and\ntranslation trade-offs. Results show sharp ranking shifts, inconsistent model\nperformance among SEA languages, and the importance of human-curated datasets\nfor low-resource languages like Burmese.", "AI": {"tldr": "\u9996\u4e2a\u4e13\u95e8\u9488\u5bf9\u4e1c\u5357\u4e9a\u8bed\u8a00\u7684\u5927\u89c4\u6a21\u5d4c\u5165\u6a21\u578b\u8bc4\u6d4b\u6807\u51c6SEA-BED\uff0c\u5305\u542b169\u4e2a\u6570\u636e\u96c6\u30019\u9879\u4efb\u52a1\u548c10\u79cd\u8bed\u8a00\uff0c71%\u4eba\u5de5\u6784\u5efa\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u4e1c\u5357\u4e9a\u8bed\u8a00\u4e0a\u8868\u73b0\u4e0d\u7a33\u5b9a\u4e14\u9700\u8981\u4eba\u5de5\u7cbe\u68c0\u6570\u636e\u96c6\u6765\u63d0\u5347\u4f4e\u8d44\u6e90\u8bed\u8a00\u6027\u80fd\u3002", "motivation": "\u4e1c\u5357\u4e9a\u5730\u533a\u8fd17\u4ebf\u4eba\u53e3\u4f46\u7f3a\u4e4f\u4e13\u95e8\u7684\u5d4c\u5165\u6a21\u578b\u8bc4\u6d4b\u6807\u51c6\uff0c\u73b0\u6709\u591a\u8bed\u8a00\u6807\u51c6\u5982MMTEB\u4e2dSEA\u6570\u636e\u96c6\u7a00\u7f3a\u4e14\u591a\u4e3a\u673a\u5668\u7ffb\u8bd1\uff0c\u5931\u53bb\u4e86\u672c\u571f\u8bed\u8a00\u7279\u6027\u3002", "method": "\u6784\u5efaSEA-BED\u6807\u51c6\uff0c\u5305\u542b169\u4e2a\u6570\u636e\u96c6\u30019\u9879\u4efb\u52a1\u548c10\u79cd\u4e1c\u5357\u4e9a\u8bed\u8a00\uff0c\u517671%\u4eba\u5de5\u6784\u5efa\u3002\u5bf917\u4e2a\u5d4c\u5165\u6a21\u578b\u8fdb\u884c\u516d\u9879\u7814\u7a76\u5206\u6790\uff0c\u6d89\u53ca\u4efb\u52a1\u96be\u5ea6\u3001\u8bed\u8a00\u95ee\u9898\u3001\u8de8\u6807\u51c6\u5bf9\u6bd4\u548c\u7ffb\u8bd1\u65b9\u5f0f\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u6a21\u578b\u5728\u4e1c\u5357\u4e9a\u8bed\u8a00\u4e0a\u6392\u540d\u663e\u8457\u53d8\u5316\uff0c\u8868\u73b0\u4e0d\u4e00\u81f4\uff0c\u5c24\u5176\u662f\u5bf9\u4f8b\u5982\u7f05\u7538\u8bed\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\uff0c\u4eba\u5de5\u7cbe\u68c0\u6570\u636e\u96c6\u5bf9\u6027\u80fd\u63d0\u5347\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "SEA-BED\u586b\u8865\u4e86\u4e1c\u5357\u4e9a\u5d4c\u5165\u6a21\u578b\u8bc4\u6d4b\u7684\u7a7a\u767d\uff0c\u8bc1\u660e\u4e86\u8bed\u8a00\u7279\u5b9a\u6807\u51c6\u7684\u5fc5\u8981\u6027\uff0c\u5e76\u5f3a\u8c03\u4eba\u5de5\u6784\u5efa\u672c\u571f\u5316\u6570\u636e\u96c6\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00NLP\u4efb\u52a1\u7684\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2508.12255", "pdf": "https://arxiv.org/pdf/2508.12255", "abs": "https://arxiv.org/abs/2508.12255", "authors": ["Ankita Pasad"], "title": "What do Speech Foundation Models Learn? Analysis and Applications", "categories": ["cs.CL", "eess.AS"], "comment": "Ph.D. Thesis", "summary": "Speech foundation models (SFMs) are designed to serve as general-purpose\nrepresentations for a wide range of speech-processing tasks. The last five\nyears have seen an influx of increasingly successful self-supervised and\nsupervised pre-trained models with impressive performance on various downstream\ntasks.\n  Although the zoo of SFMs continues to grow, our understanding of the\nknowledge they acquire lags behind. This thesis presents a lightweight analysis\nframework using statistical tools and training-free tasks to investigate the\nacoustic and linguistic knowledge encoded in SFM layers. We conduct a\ncomparative study across multiple SFMs and statistical tools. Our study also\nshows that the analytical insights have concrete implications for downstream\ntask performance.\n  The effectiveness of an SFM is ultimately determined by its performance on\nspeech applications. Yet it remains unclear whether the benefits extend to\nspoken language understanding (SLU) tasks that require a deeper understanding\nthan widely studied ones, such as speech recognition. The limited exploration\nof SLU is primarily due to a lack of relevant datasets. To alleviate that, this\nthesis contributes tasks, specifically spoken named entity recognition (NER)\nand named entity localization (NEL), to the Spoken Language Understanding\nEvaluation benchmark. We develop SFM-based approaches for NER and NEL, and find\nthat end-to-end (E2E) models leveraging SFMs can surpass traditional cascaded\n(speech recognition followed by a text model) approaches. Further, we evaluate\nE2E SLU models across SFMs and adaptation strategies to assess the impact on\ntask performance.\n  Collectively, this thesis tackles previously unanswered questions about SFMs,\nproviding tools and datasets to further our understanding and to enable the\ncommunity to make informed design choices for future model development and\nadoption.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u5206\u6790\u6846\u67b6\uff0c\u7528\u7edf\u8ba1\u5de5\u5177\u548c\u65e0\u9700\u8bad\u7ec3\u7684\u4efb\u52a1\u6765\u7814\u7a76\u8bed\u97f3\u57fa\u7840\u6a21\u578b\uff08SFMs\uff09\u4e2d\u7f16\u7801\u7684\u97f3\u54cd\u548c\u8bed\u8a00\u77e5\u8bc6\uff0c\u5e76\u4e3a\u53e3\u8bed\u8bed\u8a00\u7406\u89e3\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u6570\u636e\u96c6\u548c\u65b9\u6cd5\u3002", "motivation": "\u867d\u7136\u8bed\u97f3\u57fa\u7840\u6a21\u578b\u5728\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8270\uff0c\u4f46\u5bf9\u5b83\u4eec\u6240\u83b7\u5f97\u77e5\u8bc6\u7684\u7406\u89e3\u8fdc\u8fdc\u8fdf\u540e\u3002\u540c\u65f6\uff0c\u5bf9\u4e8e\u9700\u8981\u6df1\u5c42\u7406\u89e3\u7684\u53e3\u8bed\u8bed\u8a00\u7406\u89e3\u4efb\u52a1\uff0c\u76f8\u5173\u6570\u636e\u96c6\u7684\u7f3a\u4e4f\u9650\u5236\u4e86\u7814\u7a76\u3002", "method": "\u4f7f\u7528\u7edf\u8ba1\u5de5\u5177\u548c\u65e0\u9700\u8bad\u7ec3\u7684\u4efb\u52a1\u6765\u5206\u6790SFM\u5404\u5c42\u7f16\u7801\u7684\u77e5\u8bc6\uff1b\u4e3a\u53e3\u8bed\u8bed\u8a00\u7406\u89e3\u8bc4\u6d4b\u6807\u51c6\u63d0\u4f9b\u4e86\u53e3\u8bed\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u548c\u547d\u540d\u5b9e\u4f53\u5b9a\u4f4d\u4efb\u52a1\uff1b\u5f00\u53d1\u4e86\u57fa\u4e8eSFM\u7684\u7aef\u5230\u7aef\u6a21\u578b\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5206\u6790\u89c1\u89e3\u5bf9\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u6709\u5177\u4f53\u5f71\u54cd\uff1b\u7aef\u5230\u7aef\u6a21\u578b\u80fd\u591f\u8d85\u8d8a\u4f20\u7edf\u7684\u6d41\u6c34\u7ebf\u65b9\u6cd5\uff08\u8bed\u97f3\u8bc6\u522b+\u6587\u672c\u6a21\u578b\uff09\u3002", "conclusion": "\u8fd9\u4efd\u8bba\u6587\u89e3\u51b3\u4e86\u5173\u4e8eSFM\u7684\u4e4b\u524d\u672a\u89e3\u51b3\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u5de5\u5177\u548c\u6570\u636e\u96c6\u6765\u6df1\u5316\u6211\u4eec\u7684\u7406\u89e3\uff0c\u5e76\u4e3a\u672a\u6765\u6a21\u578b\u5f00\u53d1\u548c\u91c7\u7528\u63d0\u4f9b\u4e86\u6709\u4fe1\u606f\u652f\u6491\u7684\u8bbe\u8ba1\u9009\u62e9\u3002"}}
{"id": "2508.12257", "pdf": "https://arxiv.org/pdf/2508.12257", "abs": "https://arxiv.org/abs/2508.12257", "authors": ["Zheye Deng", "Chunkit Chan", "Tianshi Zheng", "Wei Fan", "Weiqi Wang", "Yangqiu Song"], "title": "Structuring the Unstructured: A Systematic Review of Text-to-Structure Generation for Agentic AI with a Universal Evaluation Framework", "categories": ["cs.CL"], "comment": "Under Review", "summary": "The evolution of AI systems toward agentic operation and context-aware\nretrieval necessitates transforming unstructured text into structured formats\nlike tables, knowledge graphs, and charts. While such conversions enable\ncritical applications from summarization to data mining, current research lacks\na comprehensive synthesis of methodologies, datasets, and metrics. This\nsystematic review examines text-to-structure techniques and the encountered\nchallenges, evaluates current datasets and assessment criteria, and outlines\npotential directions for future research. We also introduce a universal\nevaluation framework for structured outputs, establishing text-to-structure as\nfoundational infrastructure for next-generation AI systems.", "AI": {"tldr": "\u672c\u6587\u5bf9\u6587\u672c\u5230\u7ed3\u6784\u5316\u8f6c\u6362\u6280\u672f\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u7efc\u8ff0\uff0c\u5206\u6790\u4e86\u65b9\u6cd5\u3001\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u63d0\u51fa\u4e86\u901a\u7528\u8bc4\u4f30\u6846\u67b6\u3002", "motivation": "AI\u7cfb\u7edf\u5411\u4ee3\u7406\u64cd\u4f5c\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u68c0\u7d22\u53d1\u5c55\u9700\u8981\u5c06\u975e\u7ed3\u6784\u5316\u6587\u672c\u8f6c\u6362\u4e3a\u8868\u683c\u3001\u77e5\u8bc6\u56fe\u8c31\u548c\u56fe\u8868\u7b49\u7ed3\u6784\u5316\u683c\u5f0f\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u5bf9\u65b9\u6cd5\u3001\u6570\u636e\u96c6\u548c\u6307\u6807\u7684\u7efc\u5408\u5206\u6790\u3002", "method": "\u91c7\u7528\u7cfb\u7edf\u6027\u7efc\u8ff0\u65b9\u6cd5\uff0c\u5206\u6790\u6587\u672c\u5230\u7ed3\u6784\u8f6c\u6362\u6280\u672f\u3001\u73b0\u6709\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6807\u51c6\uff0c\u5e76\u5f15\u5165\u901a\u7528\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u5efa\u7acb\u4e86\u6587\u672c\u5230\u7ed3\u6784\u8f6c\u6362\u4f5c\u4e3a\u4e0b\u4e00\u4ee3AI\u7cfb\u7edf\u57fa\u7840\u67b6\u6784\u7684\u5730\u4f4d\uff0c\u7cfb\u7edf\u68b3\u7406\u4e86\u8be5\u9886\u57df\u7684\u6280\u672f\u6311\u6218\u548c\u53d1\u5c55\u73b0\u72b6\u3002", "conclusion": "\u6587\u672c\u5230\u7ed3\u6784\u8f6c\u6362\u662fAI\u7cfb\u7edf\u53d1\u5c55\u7684\u5173\u952e\u6280\u672f\uff0c\u9700\u8981\u5efa\u7acb\u7edf\u4e00\u7684\u8bc4\u4f30\u6807\u51c6\u548c\u6846\u67b6\u6765\u63a8\u52a8\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2508.12265", "pdf": "https://arxiv.org/pdf/2508.12265", "abs": "https://arxiv.org/abs/2508.12265", "authors": ["Xinda Jia", "Jinpeng Li", "Zezhong Wang", "Jingjing Li", "Xingshan Zeng", "Yasheng Wang", "Weinan Zhang", "Yong Yu", "Weiwen Liu"], "title": "Fast, Slow, and Tool-augmented Thinking for LLMs: A Review", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable progress in\nreasoning across diverse domains. However, effective reasoning in real-world\ntasks requires adapting the reasoning strategy to the demands of the problem,\nranging from fast, intuitive responses to deliberate, step-by-step reasoning\nand tool-augmented thinking. Drawing inspiration from cognitive psychology, we\npropose a novel taxonomy of LLM reasoning strategies along two knowledge\nboundaries: a fast/slow boundary separating intuitive from deliberative\nprocesses, and an internal/external boundary distinguishing reasoning grounded\nin the model's parameters from reasoning augmented by external tools. We\nsystematically survey recent work on adaptive reasoning in LLMs and categorize\nmethods based on key decision factors. We conclude by highlighting open\nchallenges and future directions toward more adaptive, efficient, and reliable\nLLMs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u8ba4\u77e5\u5fc3\u7406\u5b66\u7684\u65b0\u9896LLM\u63a8\u7406\u7b56\u7565\u5206\u7c7b\u6cd5\uff0c\u5305\u542b\u5feb/\u6162\u8fb9\u754c\u548c\u5185\u90e8/\u5916\u90e8\u8fb9\u754c\u4e24\u4e2a\u7ef4\u5ea6\uff0c\u7cfb\u7edf\u6027\u5730\u8c03\u7814\u4e86\u81ea\u9002\u5e94\u63a8\u7406\u65b9\u6cd5\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4efb\u52a1\u9700\u8981LLMs\u80fd\u591f\u6839\u636e\u95ee\u9898\u9700\u6c42\u81ea\u9002\u5e94\u5730\u9009\u62e9\u63a8\u7406\u7b56\u7565\uff0c\u4ece\u5feb\u901f\u76f4\u89c2\u54cd\u5e94\u5230\u6df1\u601d\u719f\u8651\u7684\u5206\u6b65\u63a8\u7406\u548c\u5de5\u5177\u589e\u5f3a\u601d\u8003\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7684\u5206\u7c7b\u6846\u67b6\u3002", "method": "\u57fa\u4e8e\u8ba4\u77e5\u5fc3\u7406\u5b66\u63d0\u51fa\u53cc\u7ef4\u5ea6\u5206\u7c7b\u6cd5\uff1a\u5feb/\u6162\u8fb9\u754c\uff08\u76f4\u89c9vs\u5ba1\u614e\u8fc7\u7a0b\uff09\u548c\u5185\u90e8/\u5916\u90e8\u8fb9\u754c\uff08\u53c2\u6570\u5185\u63a8\u7406vs\u5916\u90e8\u5de5\u5177\u589e\u5f3a\u63a8\u7406\uff09\uff0c\u5e76\u7cfb\u7edf\u6027\u5730\u8c03\u7814\u548c\u5206\u7c7b\u73b0\u6709\u81ea\u9002\u5e94\u63a8\u7406\u65b9\u6cd5\u3002", "result": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u5168\u9762\u7684LLM\u63a8\u7406\u7b56\u7565\u5206\u7c7b\u6846\u67b6\uff0c\u4e3a\u7406\u89e3\u4e0d\u540c\u63a8\u7406\u65b9\u6cd5\u7684\u7279\u6027\u548c\u9002\u7528\u573a\u666f\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u7684\u7406\u8bba\u57fa\u7840\u3002", "conclusion": "\u8be5\u5206\u7c7b\u6cd5\u4e3a\u5f00\u53d1\u66f4\u81ea\u9002\u5e94\u3001\u9ad8\u6548\u548c\u53ef\u9760\u7684LLMs\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u8be5\u9886\u57df\u9762\u4e34\u7684\u5f00\u653e\u6311\u6218\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2508.12277", "pdf": "https://arxiv.org/pdf/2508.12277", "abs": "https://arxiv.org/abs/2508.12277", "authors": ["Elon Ezra", "Ariel Weizman", "Amos Azaria"], "title": "The Self-Execution Benchmark: Measuring LLMs' Attempts to Overcome Their Lack of Self-Execution", "categories": ["cs.CL", "cs.AI"], "comment": "11 pages, 9 figures", "summary": "Large language models (LLMs) are commonly evaluated on tasks that test their\nknowledge or reasoning abilities. In this paper, we explore a different type of\nevaluation: whether an LLM can predict aspects of its own responses. Since LLMs\nlack the ability to execute themselves, we introduce the Self-Execution\nBenchmark, which measures a model's ability to anticipate properties of its\noutput, such as whether a question will be difficult for it, whether it will\nrefuse to answer, or what kinds of associations it is likely to produce. Our\nexperiments show that models generally perform poorly on this benchmark, and\nthat increased model size or capability does not consistently lead to better\nperformance. These results suggest a fundamental limitation in how LLMs\nrepresent and reason about their own behavior.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u81ea\u6211\u6267\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u9884\u6d4b\u81ea\u8eab\u54cd\u5e94\u7279\u6027\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u6a21\u578b\u666e\u904d\u8868\u73b0\u4e0d\u4f73\u4e14\u6a21\u578b\u89c4\u6a21\u589e\u5927\u4e0d\u4e00\u5b9a\u63d0\u5347\u6027\u80fd\uff0c\u63ed\u793a\u4e86LLM\u5728\u81ea\u6211\u884c\u4e3a\u8ba4\u77e5\u65b9\u9762\u7684\u6839\u672c\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u8bc4\u4f30\u4e3b\u8981\u6d4b\u8bd5LLM\u7684\u77e5\u8bc6\u548c\u63a8\u7406\u80fd\u529b\uff0c\u672c\u6587\u63a2\u7d22\u4e0d\u540c\u7684\u8bc4\u4f30\u7ef4\u5ea6\uff1aLLM\u662f\u5426\u80fd\u9884\u6d4b\u81ea\u8eab\u54cd\u5e94\u7684\u7279\u6027\uff0c\u4ee5\u4e86\u89e3\u6a21\u578b\u5bf9\u81ea\u8eab\u884c\u4e3a\u7684\u8ba4\u77e5\u80fd\u529b\u3002", "method": "\u5f15\u5165\u81ea\u6211\u6267\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u8bbe\u8ba1\u5b9e\u9a8c\u8ba9\u6a21\u578b\u9884\u6d4b\u5176\u8f93\u51fa\u7684\u5404\u79cd\u7279\u6027\uff0c\u5305\u62ec\u95ee\u9898\u96be\u5ea6\u9884\u6d4b\u3001\u62d2\u7edd\u56de\u7b54\u53ef\u80fd\u6027\u9884\u6d4b\u3001\u4ee5\u53ca\u53ef\u80fd\u4ea7\u751f\u7684\u5173\u8054\u7c7b\u578b\u9884\u6d4b\u7b49\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u6a21\u578b\u5728\u8fd9\u4e00\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u666e\u904d\u8868\u73b0\u4e0d\u4f73\uff0c\u6a21\u578b\u89c4\u6a21\u6216\u80fd\u529b\u7684\u589e\u52a0\u5e76\u4e0d\u603b\u662f\u5e26\u6765\u6027\u80fd\u63d0\u5347\uff0c\u8868\u660eLLM\u5728\u81ea\u6211\u884c\u4e3a\u8868\u5f81\u548c\u63a8\u7406\u65b9\u9762\u5b58\u5728\u6839\u672c\u6027\u5c40\u9650\u3002", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9884\u6d4b\u81ea\u8eab\u54cd\u5e94\u7279\u6027\u65b9\u9762\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\uff0c\u8fd9\u79cd\u81ea\u6211\u8ba4\u77e5\u80fd\u529b\u7684\u7f3a\u5931\u63ed\u793a\u4e86\u5f53\u524dLLM\u67b6\u6784\u7684\u5185\u5728\u7f3a\u9677\uff0c\u5bf9\u6a21\u578b\u81ea\u6211\u610f\u8bc6\u548c\u5143\u8ba4\u77e5\u80fd\u529b\u7684\u53d1\u5c55\u63d0\u51fa\u4e86\u91cd\u8981\u6311\u6218\u3002"}}
{"id": "2508.12281", "pdf": "https://arxiv.org/pdf/2508.12281", "abs": "https://arxiv.org/abs/2508.12281", "authors": ["Xin Dai", "Buqiang Xu", "Zhenghao Liu", "Yukun Yan", "Huiyuan Xie", "Xiaoyuan Yi", "Shuo Wang", "Ge Yu"], "title": "Legal$\u0394$: Enhancing Legal Reasoning in LLMs via Reinforcement Learning with Chain-of-Thought Guided Information Gain", "categories": ["cs.CL"], "comment": null, "summary": "Legal Artificial Intelligence (LegalAI) has achieved notable advances in\nautomating judicial decision-making with the support of Large Language Models\n(LLMs). However, existing legal LLMs still struggle to generate reliable and\ninterpretable reasoning processes. They often default to fast-thinking behavior\nby producing direct answers without explicit multi-step reasoning, limiting\ntheir effectiveness in complex legal scenarios that demand rigorous\njustification. To address this challenge, we propose Legal$\\Delta$, a\nreinforcement learning framework designed to enhance legal reasoning through\nchain-of-thought guided information gain. During training, Legal$\\Delta$\nemploys a dual-mode input setup-comprising direct answer and\nreasoning-augmented modes-and maximizes the information gain between them. This\nencourages the model to acquire meaningful reasoning patterns rather than\ngenerating superficial or redundant explanations. Legal$\\Delta$ follows a\ntwo-stage approach: (1) distilling latent reasoning capabilities from a\npowerful Large Reasoning Model (LRM), DeepSeek-R1, and (2) refining reasoning\nquality via differential comparisons, combined with a multidimensional reward\nmechanism that assesses both structural coherence and legal-domain specificity.\nExperimental results on multiple legal reasoning tasks demonstrate that\nLegal$\\Delta$ outperforms strong baselines in both accuracy and\ninterpretability. It consistently produces more robust and trustworthy legal\njudgments without relying on labeled preference data. All code and data will be\nreleased at https://github.com/NEUIR/LegalDelta.", "AI": {"tldr": "Legal\u0394\u662f\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u601d\u7ef4\u94fe\u5f15\u5bfc\u7684\u4fe1\u606f\u589e\u76ca\u6765\u589e\u5f3a\u6cd5\u5f8b\u63a8\u7406\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u6cd5\u5f8b\u63a8\u7406\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u6cd5\u5f8b\u5927\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u53ef\u9760\u548c\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u8fc7\u7a0b\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u5f80\u5f80\u76f4\u63a5\u7ed9\u51fa\u7b54\u6848\u800c\u7f3a\u4e4f\u591a\u6b65\u63a8\u7406\uff0c\u9650\u5236\u4e86\u5728\u590d\u6742\u6cd5\u5f8b\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002", "method": "\u91c7\u7528\u53cc\u6a21\u5f0f\u8f93\u5165\uff08\u76f4\u63a5\u7b54\u6848\u6a21\u5f0f\u548c\u63a8\u7406\u589e\u5f3a\u6a21\u5f0f\uff09\uff0c\u6700\u5927\u5316\u4e24\u8005\u95f4\u7684\u4fe1\u606f\u589e\u76ca\uff1b\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u4eceDeepSeek-R1\u63d0\u70bc\u6f5c\u5728\u63a8\u7406\u80fd\u529b\uff0c\u901a\u8fc7\u5dee\u5206\u6bd4\u8f83\u548c\u591a\u7ef4\u5956\u52b1\u673a\u5236\u7cbe\u70bc\u63a8\u7406\u8d28\u91cf\u3002", "result": "\u5728\u591a\u4e2a\u6cd5\u5f8b\u63a8\u7406\u4efb\u52a1\u4e0a\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u6a21\u578b\uff0c\u5728\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5747\u6709\u63d0\u5347\uff0c\u80fd\u591f\u4ea7\u751f\u66f4\u7a33\u5065\u548c\u53ef\u4fe1\u7684\u6cd5\u5f8b\u5224\u65ad\uff0c\u4e14\u65e0\u9700\u4f9d\u8d56\u6807\u6ce8\u7684\u504f\u597d\u6570\u636e\u3002", "conclusion": "Legal\u0394\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u6cd5\u5f8bAI\u4e2d\u63a8\u7406\u8fc7\u7a0b\u4e0d\u53ef\u9760\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548c\u4fe1\u606f\u589e\u76ca\u673a\u5236\u6709\u6548\u63d0\u5347\u4e86\u6cd5\u5f8b\u63a8\u7406\u7684\u8d28\u91cf\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2508.12282", "pdf": "https://arxiv.org/pdf/2508.12282", "abs": "https://arxiv.org/abs/2508.12282", "authors": ["Ziyang Chen", "Erxue Min", "Xiang Zhao", "Yunxin Li", "Xin Jia", "Jinzhi Liao", "Jichao Li", "Shuaiqiang Wang", "Baotian Hu", "Dawei Yin"], "title": "A Question Answering Dataset for Temporal-Sensitive Retrieval-Augmented Generation", "categories": ["cs.CL", "cs.IR", "68T50, 68P20", "I.2.7; H.3.3"], "comment": "10 pages, 5 figures", "summary": "We introduce ChronoQA, a large-scale benchmark dataset for Chinese question\nanswering, specifically designed to evaluate temporal reasoning in\nRetrieval-Augmented Generation (RAG) systems. ChronoQA is constructed from over\n300,000 news articles published between 2019 and 2024, and contains 5,176\nhigh-quality questions covering absolute, aggregate, and relative temporal\ntypes with both explicit and implicit time expressions. The dataset supports\nboth single- and multi-document scenarios, reflecting the real-world\nrequirements for temporal alignment and logical consistency. ChronoQA features\ncomprehensive structural annotations and has undergone multi-stage validation,\nincluding rule-based, LLM-based, and human evaluation, to ensure data quality.\nBy providing a dynamic, reliable, and scalable resource, ChronoQA enables\nstructured evaluation across a wide range of temporal tasks, and serves as a\nrobust benchmark for advancing time-sensitive retrieval-augmented question\nanswering systems.", "AI": {"tldr": "ChronoQA\u662f\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7cfb\u7edf\u65f6\u95f4\u63a8\u7406\u80fd\u529b\u7684\u5927\u89c4\u6a21\u4e2d\u6587\u95ee\u7b54\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b5,176\u4e2a\u9ad8\u8d28\u91cf\u95ee\u9898\uff0c\u8986\u76d6\u591a\u79cd\u65f6\u95f4\u7c7b\u578b\u548c\u8868\u8fbe\u65b9\u5f0f\u3002", "motivation": "\u73b0\u6709\u7684\u95ee\u7b54\u7cfb\u7edf\u5728\u65f6\u95f4\u63a8\u7406\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u65f6\u95f4\u5bf9\u9f50\u548c\u903b\u8f91\u4e00\u81f4\u6027\u65b9\u9762\u3002\u9700\u8981\u6784\u5efa\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9\u4e2d\u6587\u65f6\u95f4\u654f\u611f\u95ee\u7b54\u7684\u53ef\u9760\u57fa\u51c6\u6570\u636e\u96c6\u6765\u63a8\u52a8\u76f8\u5173\u7814\u7a76\u53d1\u5c55\u3002", "method": "\u57fa\u4e8e2019-2024\u5e74\u95f4\u8d85\u8fc730\u4e07\u7bc7\u65b0\u95fb\u6587\u7ae0\u6784\u5efa\u6570\u636e\u96c6\uff0c\u5305\u542b\u7edd\u5bf9\u65f6\u95f4\u3001\u805a\u5408\u65f6\u95f4\u548c\u76f8\u5bf9\u65f6\u95f4\u4e09\u79cd\u7c7b\u578b\u7684\u95ee\u9898\uff0c\u652f\u6301\u5355\u6587\u6863\u548c\u591a\u6587\u6863\u573a\u666f\uff0c\u5e76\u901a\u8fc7\u89c4\u5219\u3001\u5927\u8bed\u8a00\u6a21\u578b\u548c\u4eba\u5de5\u9a8c\u8bc1\u786e\u4fdd\u6570\u636e\u8d28\u91cf\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b5,176\u4e2a\u9ad8\u8d28\u91cf\u95ee\u9898\u7684\u4e2d\u6587\u65f6\u95f4\u95ee\u7b54\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5177\u6709\u5168\u9762\u7684\u7ed3\u6784\u6807\u6ce8\u548c\u591a\u9636\u6bb5\u9a8c\u8bc1\u673a\u5236\uff0c\u4e3a\u65f6\u95f4\u654f\u611f\u7684\u68c0\u7d22\u589e\u5f3a\u95ee\u7b54\u7cfb\u7edf\u63d0\u4f9b\u4e86\u52a8\u6001\u3001\u53ef\u9760\u548c\u53ef\u6269\u5c55\u7684\u8bc4\u4f30\u8d44\u6e90\u3002", "conclusion": "ChronoQA\u4f5c\u4e3a\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9\u4e2d\u6587\u65f6\u95f4\u63a8\u7406\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u80fd\u591f\u6709\u6548\u8bc4\u4f30\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7cfb\u7edf\u7684\u65f6\u95f4\u654f\u611f\u80fd\u529b\uff0c\u4e3a\u63a8\u8fdb\u65f6\u95f4\u654f\u611f\u95ee\u7b54\u7cfb\u7edf\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\u3002"}}
{"id": "2508.12286", "pdf": "https://arxiv.org/pdf/2508.12286", "abs": "https://arxiv.org/abs/2508.12286", "authors": ["Qinghua Wang", "Xu Zhang", "Lingyan Yang", "Rui Shao", "Bonan Wang", "Fang Wang", "Cunquan Qu"], "title": "Incorporating Legal Logic into Deep Learning: An Intelligent Approach to Probation Prediction", "categories": ["cs.CL"], "comment": null, "summary": "Probation is a crucial institution in modern criminal law, embodying the\nprinciples of fairness and justice while contributing to the harmonious\ndevelopment of society. Despite its importance, the current Intelligent\nJudicial Assistant System (IJAS) lacks dedicated methods for probation\nprediction, and research on the underlying factors influencing probation\neligibility remains limited. In addition, probation eligibility requires a\ncomprehensive analysis of both criminal circumstances and remorse. Much of the\nexisting research in IJAS relies primarily on data-driven methodologies, which\noften overlooks the legal logic underpinning judicial decision-making. To\naddress this gap, we propose a novel approach that integrates legal logic into\ndeep learning models for probation prediction, implemented in three distinct\nstages. First, we construct a specialized probation dataset that includes fact\ndescriptions and probation legal elements (PLEs). Second, we design a distinct\nprobation prediction model named the Multi-Task Dual-Theory Probation\nPrediction Model (MT-DT), which is grounded in the legal logic of probation and\nthe \\textit{Dual-Track Theory of Punishment}. Finally, our experiments on the\nprobation dataset demonstrate that the MT-DT model outperforms baseline models,\nand an analysis of the underlying legal logic further validates the\neffectiveness of the proposed approach.", "AI": {"tldr": "\u901a\u8fc7\u5c06\u6cd5\u5f8b\u903b\u8f91\u4e0e\u6df1\u5ea6\u5b66\u4e60\u76f8\u7ed3\u5408\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7f5a\u91d1\u9884\u6d4b\u6a21\u578bMT-DT\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u667a\u80fd\u53f8\u6cd5\u8f85\u52a9\u7cfb\u7edf\u4e2d\u7f3a\u4e4f\u4e13\u95e8\u7f5a\u91d1\u9884\u6d4b\u65b9\u6cd5\u7684\u95ee\u9898", "motivation": "\u73b0\u6709\u667a\u80fd\u53f8\u6cd5\u8f85\u52a9\u7cfb\u7edf\u7f3a\u4e4f\u4e13\u95e8\u7684\u7f5a\u91d1\u9884\u6d4b\u65b9\u6cd5\uff0c\u800c\u4e14\u5927\u90e8\u5206\u73b0\u6709\u7814\u7a76\u4ec5\u4f9d\u8d56\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u5ffd\u89c6\u4e86\u53f8\u6cd5\u51b3\u7b56\u7684\u6cd5\u5f8b\u903b\u8f91\u57fa\u7840", "method": "\u6784\u5efa\u4e13\u95e8\u7684\u7f5a\u91d1\u6570\u636e\u96c6\uff08\u5305\u542b\u4e8b\u5b9e\u63cf\u8ff0\u548c\u7f5a\u91d1\u6cd5\u5f8b\u8981\u7d20\uff09\uff0c\u8bbe\u8ba1\u57fa\u4e8e\u7f5a\u91d1\u6cd5\u5f8b\u903b\u8f91\u548c\"\u53cc\u8f68\u5211\u7f5a\u7406\u8bba\"\u7684\u591a\u4efb\u52a1\u53cc\u7406\u8bba\u7f5a\u91d1\u9884\u6d4b\u6a21\u578b\uff08MT-DT\uff09", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aMT-DT\u6a21\u578b\u5728\u7f5a\u91d1\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u8d85\u8fc7\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u6cd5\u5f8b\u903b\u8f91\u5206\u6790\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027", "conclusion": "\u6574\u5408\u6cd5\u5f8b\u903b\u8f91\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u80fd\u591f\u66f4\u6709\u6548\u5730\u9884\u6d4b\u7f5a\u91d1\uff0c\u4e3a\u667a\u80fd\u53f8\u6cd5\u8f85\u52a9\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u7b26\u5408\u53f8\u6cd5\u903b\u8f91\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.12301", "pdf": "https://arxiv.org/pdf/2508.12301", "abs": "https://arxiv.org/abs/2508.12301", "authors": ["Tomer Krichli", "Bhiksha Raj", "Joseph Keshet"], "title": "CarelessWhisper: Turning Whisper into a Causal Streaming Model", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "comment": "17 pages, 7 Figures, This work has been submitted to the IEEE for\n  possible publication", "summary": "Automatic Speech Recognition (ASR) has seen remarkable progress, with models\nlike OpenAI Whisper and NVIDIA Canary achieving state-of-the-art (SOTA)\nperformance in offline transcription. However, these models are not designed\nfor streaming (online or real-time) transcription, due to limitations in their\narchitecture and training methodology. We propose a method to turn the\ntransformer encoder-decoder model into a low-latency streaming model that is\ncareless about future context. We present an analysis explaining why it is not\nstraightforward to convert an encoder-decoder transformer to a low-latency\nstreaming model. Our proposed method modifies the existing (non-causal) encoder\nto a causal encoder by fine-tuning both the encoder and decoder using Low-Rank\nAdaptation (LoRA) and a weakly aligned dataset. We then propose an updated\ninference mechanism that utilizes the fine-tune causal encoder and decoder to\nyield greedy and beam-search decoding, and is shown to be locally optimal.\nExperiments on low-latency chunk sizes (less than 300 msec) show that our\nfine-tuned model outperforms existing non-fine-tuned streaming approaches in\nmost cases, while using a lower complexity. Additionally, we observe that our\ntraining process yields better alignment, enabling a simple method for\nextracting word-level timestamps. We release our training and inference code,\nalong with the fine-tuned models, to support further research and development\nin streaming ASR.", "AI": {"tldr": "\u901a\u8fc7LoRA\u7ec6\u8c03\u548c\u56f0\u9759\u6570\u636e\u96c6\u5c06Transformer\u7f16\u7801\u5668-\u89e3\u7801\u5668ASR\u6a21\u578b\u8f6c\u6362\u4e3a\u4f4e\u5ef6\u8fdf\u6d41\u5f0f\u6a21\u578b\uff0c\u5728\u5c0f\u4e8e300\u6beb\u79d2\u7684\u5757\u5927\u5c0f\u4e0b\u8d85\u8d8a\u73b0\u6709\u975e\u7ec6\u8c03\u6d41\u5f0f\u65b9\u6cd5", "motivation": "\u867d\u7136OpenAI Whisper\u7b49ASR\u6a21\u578b\u5728\u79bb\u7ebf\u8bc6\u522b\u4e2d\u8868\u73b0\u7a81\u51fa\uff0c\u4f46\u5176\u67b6\u6784\u548c\u8bad\u7ec3\u65b9\u6cd5\u4e0d\u9002\u5408\u6d41\u5f0f\uff08\u5b9e\u65f6\uff09\u8bc6\u522b\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u5c06\u8fd9\u4e9b\u6a21\u578b\u8f6c\u6362\u4e3a\u4f4e\u5ef6\u8fdf\u6d41\u5f0f\u6a21\u578b", "method": "\u4fee\u6539\u975e\u56f0\u679c\u6027\u7f16\u7801\u5668\u4e3a\u56f0\u679c\u6027\u7f16\u7801\u5668\uff0c\u4f7f\u7528\u4f4e\u79e9\u9002\u914d\uff08LoRA\uff09\u548c\u5f31\u5bf9\u9f50\u6570\u636e\u96c6\u540c\u65f6\u7ec6\u8c03\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\uff0c\u63d0\u51fa\u66f4\u65b0\u7684\u63a8\u7406\u673a\u5236\u652f\u6301\u8d2a\u5a6a\u548c\u6750\u6790\u89e3\u7801", "result": "\u5728\u5c0f\u4e8e300\u6beb\u79d2\u7684\u4f4e\u5ef6\u8fdf\u5757\u5927\u5c0f\u4e0b\uff0c\u7ec6\u8c03\u6a21\u578b\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u8d85\u8fc7\u73b0\u6709\u975e\u7ec6\u8c03\u6d41\u5f0f\u65b9\u6cd5\uff0c\u4e14\u590d\u6742\u5ea6\u66f4\u4f4e\uff0c\u540c\u65f6\u8bad\u7ec3\u8fc7\u7a0b\u4ea7\u751f\u4e86\u66f4\u597d\u7684\u5bf9\u9f50\u6548\u679c\uff0c\u652f\u6301\u5355\u8bcd\u7ea7\u65f6\u95f4\u6233\u63d0\u53d6", "conclusion": "\u901a\u8fc7LoRA\u7ec6\u8c03\u548c\u56f0\u9759\u6027\u6539\u9020\uff0c\u53ef\u4ee5\u6709\u6548\u5c06Transformer\u7f16\u7801\u5668-\u89e3\u7801\u5668ASR\u6a21\u578b\u8f6c\u6362\u4e3a\u4f4e\u5ef6\u8fdf\u6d41\u5f0f\u6a21\u578b\uff0c\u4e3a\u6d41\u5f0fASR\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.12355", "pdf": "https://arxiv.org/pdf/2508.12355", "abs": "https://arxiv.org/abs/2508.12355", "authors": ["Eviatar Nachshoni", "Arie Cattan", "Shmuel Amar", "Ori Shapira", "Ido Dagan"], "title": "Consensus or Conflict? Fine-Grained Evaluation of Conflicting Answers in Question-Answering", "categories": ["cs.CL"], "comment": "no comments", "summary": "Large Language Models (LLMs) have demonstrated strong performance in question\nanswering (QA) tasks. However, Multi-Answer Question Answering (MAQA), where a\nquestion may have several valid answers, remains challenging. Traditional QA\nsettings often assume consistency across evidences, but MAQA can involve\nconflicting answers. Constructing datasets that reflect such conflicts is\ncostly and labor-intensive, while existing benchmarks often rely on synthetic\ndata, restrict the task to yes/no questions, or apply unverified automated\nannotation. To advance research in this area, we extend the conflict-aware MAQA\nsetting to require models not only to identify all valid answers, but also to\ndetect specific conflicting answer pairs, if any. To support this task, we\nintroduce a novel cost-effective methodology for leveraging fact-checking\ndatasets to construct NATCONFQA, a new benchmark for realistic, conflict-aware\nMAQA, enriched with detailed conflict labels, for all answer pairs. We evaluate\neight high-end LLMs on NATCONFQA, revealing their fragility in handling various\ntypes of conflicts and the flawed strategies they employ to resolve them.", "AI": {"tldr": "\u63d0\u51fa\u4e86NATCONFQA\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u591a\u7b54\u6848\u95ee\u7b54\u4efb\u52a1\u4e2d\u5904\u7406\u51b2\u7a81\u7b54\u6848\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u51b2\u7a81\u5904\u7406\u65b9\u9762\u8868\u73b0\u8106\u5f31", "motivation": "\u591a\u7b54\u6848\u95ee\u7b54\u4efb\u52a1\u4e2d\u53ef\u80fd\u5b58\u5728\u51b2\u7a81\u7b54\u6848\uff0c\u73b0\u6709\u6570\u636e\u96c6\u6784\u5efa\u6210\u672c\u9ad8\u4e14\u591a\u4e3a\u5408\u6210\u6570\u636e\uff0c\u9700\u8981\u66f4\u73b0\u5b9e\u7684\u51b2\u7a81\u611f\u77e5\u8bc4\u4f30\u57fa\u51c6", "method": "\u5229\u7528\u4e8b\u5b9e\u6838\u67e5\u6570\u636e\u96c6\u6784\u5efaNATCONFQA\u57fa\u51c6\uff0c\u8981\u6c42\u6a21\u578b\u4e0d\u4ec5\u8bc6\u522b\u6240\u6709\u6709\u6548\u7b54\u6848\uff0c\u8fd8\u8981\u68c0\u6d4b\u7279\u5b9a\u51b2\u7a81\u7b54\u6848\u5bf9", "result": "\u8bc4\u4f30\u4e868\u4e2a\u9ad8\u7aefLLM\uff0c\u53d1\u73b0\u5b83\u4eec\u5728\u5904\u7406\u5404\u79cd\u51b2\u7a81\u7c7b\u578b\u65f6\u8868\u73b0\u8106\u5f31\uff0c\u5e76\u91c7\u7528\u4e86\u6709\u7f3a\u9677\u7684\u89e3\u51b3\u7b56\u7565", "conclusion": "\u9700\u8981\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u51b2\u7a81\u5904\u7406\u80fd\u529b\uff0cNATCONFQA\u4e3a\u591a\u7b54\u6848\u95ee\u7b54\u7814\u7a76\u63d0\u4f9b\u4e86\u73b0\u5b9e\u7684\u8bc4\u4f30\u57fa\u51c6"}}
{"id": "2508.12387", "pdf": "https://arxiv.org/pdf/2508.12387", "abs": "https://arxiv.org/abs/2508.12387", "authors": ["Yuanfeng Xu", "Zehui Dai", "Jian Liang", "Jiapeng Guan", "Guangrun Wang", "Liang Lin", "Xiaohui Lv"], "title": "ReaLM: Reflection-Enhanced Autonomous Reasoning with Small Language Models", "categories": ["cs.CL"], "comment": "16pages, 3 figures", "summary": "Small Language Models (SLMs) are a cost-effective alternative to Large\nLanguage Models (LLMs), but often struggle with complex reasoning due to their\nlimited capacity and a tendency to produce mistakes or inconsistent answers\nduring multi-step reasoning. Existing efforts have improved SLM performance,\nbut typically at the cost of one or more of three key aspects: (1) reasoning\ncapability, due to biased supervision that filters out negative reasoning paths\nand limits learning from errors; (2) autonomy, due to over-reliance on\nexternally generated reasoning signals; and (3) generalization, which suffers\nwhen models overfit to teacher-specific patterns. In this paper, we introduce\nReaLM, a reinforcement learning framework for robust and self-sufficient\nreasoning in vertical domains. To enhance reasoning capability, we propose\nMulti-Route Process Verification (MRPV), which contrasts both positive and\nnegative reasoning paths to extract decisive patterns. To reduce reliance on\nexternal guidance and improve autonomy, we introduce Enabling Autonomy via\nAsymptotic Induction (EAAI), a training strategy that gradually fades external\nsignals. To improve generalization, we apply guided chain-of-thought\ndistillation to encode domain-specific rules and expert knowledge into SLM\nparameters, making them part of what the model has learned. Extensive\nexperiments on both vertical and general reasoning tasks demonstrate that ReaLM\nsignificantly improves SLM performance across aspects (1)-(3) above.", "AI": {"tldr": "ReaLM\u662f\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u8def\u5f84\u8fc7\u7a0b\u9a8c\u8bc1\u3001\u6e10\u8fdb\u5f0f\u81ea\u4e3b\u8bf1\u5bfc\u548c\u5f15\u5bfc\u5f0f\u601d\u7ef4\u94fe\u84b8\u998f\uff0c\u63d0\u5347\u5c0f\u8bed\u8a00\u6a21\u578b\u5728\u5782\u76f4\u9886\u57df\u7684\u63a8\u7406\u80fd\u529b\u3001\u81ea\u4e3b\u6027\u548c\u6cdb\u5316\u6027\u3002", "motivation": "\u5c0f\u8bed\u8a00\u6a21\u578b(SLMs)\u5728\u590d\u6742\u63a8\u7406\u4e2d\u5b58\u5728\u80fd\u529b\u6709\u9650\u3001\u5bb9\u6613\u51fa\u9519\u7684\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u5728\u63a8\u7406\u80fd\u529b\u3001\u81ea\u4e3b\u6027\u6216\u6cdb\u5316\u6027\u65b9\u9762\u5b58\u5728\u59a5\u534f\u3002", "method": "\u63d0\u51faReaLM\u6846\u67b6\uff1a1) MRPV\u5bf9\u6bd4\u6b63\u8d1f\u63a8\u7406\u8def\u5f84\u63d0\u53d6\u5173\u952e\u6a21\u5f0f\uff1b2) EAAI\u901a\u8fc7\u6e10\u8fdb\u51cf\u5c11\u5916\u90e8\u4fe1\u53f7\u63d0\u5347\u81ea\u4e3b\u6027\uff1b3) \u5f15\u5bfc\u5f0f\u601d\u7ef4\u94fe\u84b8\u998f\u7f16\u7801\u9886\u57df\u77e5\u8bc6\u3002", "result": "\u5728\u5782\u76f4\u9886\u57df\u548c\u901a\u7528\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cReaLM\u663e\u8457\u63d0\u5347\u4e86SLM\u5728\u63a8\u7406\u80fd\u529b\u3001\u81ea\u4e3b\u6027\u548c\u6cdb\u5316\u6027\u4e09\u4e2a\u65b9\u9762\u7684\u6027\u80fd\u3002", "conclusion": "ReaLM\u4e3a\u5c0f\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u63a8\u7406\u80fd\u529b\u3001\u81ea\u4e3b\u6027\u548c\u6cdb\u5316\u6027\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u5728\u5782\u76f4\u9886\u57df\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2508.12393", "pdf": "https://arxiv.org/pdf/2508.12393", "abs": "https://arxiv.org/abs/2508.12393", "authors": ["Duzhen Zhang", "Zixiao Wang", "Zhong-Zhi Li", "Yahan Yu", "Shuncheng Jia", "Jiahua Dong", "Haotian Xu", "Xing Wu", "Yingying Zhang", "Tielin Zhang", "Jie Yang", "Xiuying Chen", "Le Song"], "title": "MedKGent: A Large Language Model Agent Framework for Constructing Temporally Evolving Medical Knowledge Graph", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rapid expansion of medical literature presents growing challenges for\nstructuring and integrating domain knowledge at scale. Knowledge Graphs (KGs)\noffer a promising solution by enabling efficient retrieval, automated\nreasoning, and knowledge discovery. However, current KG construction methods\noften rely on supervised pipelines with limited generalizability or naively\naggregate outputs from Large Language Models (LLMs), treating biomedical\ncorpora as static and ignoring the temporal dynamics and contextual uncertainty\nof evolving knowledge. To address these limitations, we introduce MedKGent, a\nLLM agent framework for constructing temporally evolving medical KGs.\nLeveraging over 10 million PubMed abstracts published between 1975 and 2023, we\nsimulate the emergence of biomedical knowledge via a fine-grained daily time\nseries. MedKGent incrementally builds the KG in a day-by-day manner using two\nspecialized agents powered by the Qwen2.5-32B-Instruct model. The Extractor\nAgent identifies knowledge triples and assigns confidence scores via\nsampling-based estimation, which are used to filter low-confidence extractions\nand inform downstream processing. The Constructor Agent incrementally\nintegrates the retained triples into a temporally evolving graph, guided by\nconfidence scores and timestamps to reinforce recurring knowledge and resolve\nconflicts. The resulting KG contains 156,275 entities and 2,971,384 relational\ntriples. Quality assessments by two SOTA LLMs and three domain experts\ndemonstrate an accuracy approaching 90%, with strong inter-rater agreement. To\nevaluate downstream utility, we conduct RAG across seven medical question\nanswering benchmarks using five leading LLMs, consistently observing\nsignificant improvements over non-augmented baselines. Case studies further\ndemonstrate the KG's value in literature-based drug repurposing via\nconfidence-aware causal inference.", "AI": {"tldr": "MedKGent\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u4ee3\u7406\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u6784\u5efa\u65f6\u95f4\u6f14\u5316\u7684\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\uff0c\u901a\u8fc7\u63d0\u53d6\u548c\u6784\u5efa\u4ee3\u7406\u5904\u7406PubMed\u6587\u732e\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u65f6\u95f4\u611f\u77e5\u7684\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\uff0c\u5728\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5f53\u524d\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u65b9\u6cd5\u5b58\u5728\u6cdb\u5316\u6027\u6709\u9650\u3001\u5ffd\u89c6\u77e5\u8bc6\u65f6\u95f4\u52a8\u6001\u6027\u548c\u4e0a\u4e0b\u6587\u4e0d\u786e\u5b9a\u6027\u7684\u95ee\u9898\uff0c\u9700\u8981\u80fd\u591f\u5904\u7406\u751f\u7269\u533b\u5b66\u77e5\u8bc6\u6f14\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528Qwen2.5-32B-Instruct\u6a21\u578b\u9a71\u52a8\u7684\u4e24\u4e2a\u4e13\u4e1a\u4ee3\u7406\uff1a\u63d0\u53d6\u4ee3\u7406\u8bc6\u522b\u77e5\u8bc6\u4e09\u5143\u7ec4\u5e76\u5206\u914d\u7f6e\u4fe1\u5ea6\uff0c\u6784\u5efa\u4ee3\u7406\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u548c\u65f6\u95f4\u6233\u589e\u91cf\u6574\u5408\u4e09\u5143\u7ec4\u5230\u65f6\u95f4\u6f14\u5316\u56fe\u8c31\u4e2d\u3002", "result": "\u6784\u5efa\u4e86\u5305\u542b156,275\u4e2a\u5b9e\u4f53\u548c2,971,384\u4e2a\u5173\u7cfb\u4e09\u5143\u7ec4\u7684KG\uff0c\u51c6\u786e\u7387\u63a5\u8fd190%\uff0c\u57287\u4e2a\u533b\u5b66\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u975e\u589e\u5f3a\u57fa\u7ebf\u3002", "conclusion": "MedKGent\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u4e2d\u7684\u65f6\u95f4\u52a8\u6001\u6027\u95ee\u9898\uff0c\u4e3a\u751f\u7269\u533b\u5b66\u77e5\u8bc6\u7ba1\u7406\u548c\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.12405", "pdf": "https://arxiv.org/pdf/2508.12405", "abs": "https://arxiv.org/abs/2508.12405", "authors": ["Zilong Bai", "Zihan Xu", "Cong Sun", "Chengxi Zang", "H. Timothy Bunnell", "Catherine Sinfield", "Jacqueline Rutter", "Aaron Thomas Martinez", "L. Charles Bailey", "Mark Weiner", "Thomas R. Campion", "Thomas Carton", "Christopher B. Forrest", "Rainu Kaushal", "Fei Wang", "Yifan Peng"], "title": "Extracting Post-Acute Sequelae of SARS-CoV-2 Infection Symptoms from Clinical Notes via Hybrid Natural Language Processing", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted for publication in npj Health Systems", "summary": "Accurately and efficiently diagnosing Post-Acute Sequelae of COVID-19 (PASC)\nremains challenging due to its myriad symptoms that evolve over long- and\nvariable-time intervals. To address this issue, we developed a hybrid natural\nlanguage processing pipeline that integrates rule-based named entity\nrecognition with BERT-based assertion detection modules for PASC-symptom\nextraction and assertion detection from clinical notes. We developed a\ncomprehensive PASC lexicon with clinical specialists. From 11 health systems of\nthe RECOVER initiative network across the U.S., we curated 160 intake progress\nnotes for model development and evaluation, and collected 47,654 progress notes\nfor a population-level prevalence study. We achieved an average F1 score of\n0.82 in one-site internal validation and 0.76 in 10-site external validation\nfor assertion detection. Our pipeline processed each note at $2.448\\pm 0.812$\nseconds on average. Spearman correlation tests showed $\\rho >0.83$ for positive\nmentions and $\\rho >0.72$ for negative ones, both with $P <0.0001$. These\ndemonstrate the effectiveness and efficiency of our models and their potential\nfor improving PASC diagnosis.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u6df7\u5408\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6d41\u6c34\u7ebf\uff0c\u901a\u8fc7\u89c4\u5219\u57fa\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u548cBERT\u57fa\u4e8e\u65ad\u8a00\u68c0\u6d4b\u6a21\u5757\uff0c\u51c6\u786e\u9ad8\u6548\u5730\u4ece\u4e34\u5e8a\u7b14\u8bb0\u4e2d\u63d0\u53d6COVID-19\u540e\u9057\u75c7\u72b6\u3002", "motivation": "\u89e3\u51b3PASC\u8bca\u65ad\u56f0\u96be\uff0c\u56e0\u4e3a\u5176\u75c7\u72b6\u591a\u6837\u4e14\u968f\u65f6\u95f4\u53d8\u5316\uff0c\u9700\u8981\u51c6\u786e\u9ad8\u6548\u7684\u81ea\u52a8\u5316\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u96c6\u6210\u89c4\u5219\u57fa\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u548cBERT\u57fa\u4e8e\u65ad\u8a00\u68c0\u6d4b\u6a21\u5757\uff0c\u5f00\u53d1\u4e86\u7efc\u5408\u6027PASC\u8bcd\u5178\uff0c\u4f7f\u7528160\u4efd\u8fdb\u5c55\u7b14\u8bb0\u8fdb\u884c\u6a21\u578b\u5f00\u53d1\u548c\u8bc4\u4f30\u3002", "result": "\u5728\u5185\u90e8\u9a8c\u8bc1\u4e2d\u83b7\u5f97\u5e73\u5747F1\u52060.82\uff0c\u5916\u90e8\u9a8c\u8bc1\u4e2d\u83b7\u5f970.76\uff0c\u6bcf\u4efd\u7b14\u8bb0\u5904\u7406\u65f6\u95f4\u4ec5\u97002.448\u79d2\uff0c\u76f8\u5173\u6027\u68c0\u9a8c\u663e\u793a\u9ad8\u5ea6\u663e\u8457\u76f8\u5173\u3002", "conclusion": "\u8be5\u6a21\u578b\u663e\u793a\u51fa\u9ad8\u6548\u6027\u548c\u51c6\u786e\u6027\uff0c\u6709\u529b\u652f\u6301PASC\u8bca\u65ad\u7684\u6539\u8fdb\u3002"}}
{"id": "2508.12407", "pdf": "https://arxiv.org/pdf/2508.12407", "abs": "https://arxiv.org/abs/2508.12407", "authors": ["Zhuorui Liu", "Chen Zhang", "Dawei Song"], "title": "ZigzagAttention: Efficient Long-Context Inference with Exclusive Retrieval and Streaming Heads", "categories": ["cs.CL"], "comment": "5 pages, 4 figures", "summary": "With the rapid development of large language models (LLMs), handling long\ncontext has become one of the vital abilities in LLMs. Such long-context\nability is accompanied by difficulties in deployment, especially due to the\nincreased consumption of KV cache. There is certain work aiming to optimize the\nmemory footprint of KV cache, inspired by the observation that attention heads\ncan be categorized into retrieval heads that are of great significance and\nstreaming heads that are of less significance. Typically, identifying the\nstreaming heads and and waiving the KV cache in the streaming heads would\nlargely reduce the overhead without hurting the performance that much. However,\nsince employing both retrieval and streaming heads in one layer decomposes one\nlarge round of attention computation into two small ones, it may unexpectedly\nbring extra latency on accessing and indexing tensors. Based on this intuition,\nwe impose an important improvement to the identification process of retrieval\nand streaming heads, in which we design a criterion that enforces exclusively\nretrieval or streaming heads gathered in one unique layer. In this way, we\nfurther eliminate the extra latency and only incur negligible performance\ndegradation. Our method named \\textsc{ZigzagAttention} is competitive among\nconsidered baselines owing to reduced latency and comparable performance.", "AI": {"tldr": "ZigzagAttention\u65b9\u6cd5\u901a\u8fc7\u5c06\u68c0\u7d22\u5934\u548c\u6d41\u5f0f\u5934\u5206\u79bb\u5230\u4e0d\u540c\u5c42\u6765\u4f18\u5316LLM\u7684\u957f\u4e0a\u4e0b\u6587\u5904\u7406\uff0c\u51cf\u5c11KV\u7f13\u5b58\u5185\u5b58\u5360\u7528\u548c\u5ef6\u8fdf\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u65f6KV\u7f13\u5b58\u6d88\u8017\u5de8\u5927\uff0c\u73b0\u6709\u65b9\u6cd5\u867d\u7136\u80fd\u51cf\u5c11\u5185\u5b58\u5360\u7528\u4f46\u4f1a\u5e26\u6765\u989d\u5916\u7684\u5f20\u91cf\u8bbf\u95ee\u548c\u7d22\u5f15\u5ef6\u8fdf\u3002", "method": "\u8bbe\u8ba1\u65b0\u7684\u6807\u51c6\uff0c\u5f3a\u5236\u5c06\u68c0\u7d22\u5934\u548c\u6d41\u5f0f\u5934\u5206\u522b\u805a\u96c6\u5728\u4e0d\u540c\u7684\u5c42\u4e2d\uff0c\u907f\u514d\u6df7\u5408\u8ba1\u7b97\u5e26\u6765\u7684\u989d\u5916\u5ef6\u8fdf\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u5ef6\u8fdf\uff0c\u540c\u65f6\u53ea\u5e26\u6765\u53ef\u5ffd\u7565\u7684\u6027\u80fd\u4e0b\u964d\uff0c\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u7ade\u4e89\u529b\u3002", "conclusion": "ZigzagAttention\u901a\u8fc7\u5934\u7c7b\u578b\u5206\u5c42\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u4e2d\u7684\u5185\u5b58\u548c\u5ef6\u8fdf\u6743\u8861\u95ee\u9898\u3002"}}
{"id": "2508.12411", "pdf": "https://arxiv.org/pdf/2508.12411", "abs": "https://arxiv.org/abs/2508.12411", "authors": ["Emanuel Z. Fenech-Borg", "Tilen P. Meznaric-Kos", "Milica D. Lekovic-Bojovic", "Arni J. Hentze-Djurhuus"], "title": "The Cultural Gene of Large Language Models: A Study on the Impact of Cross-Corpus Training on Model Values and Biases", "categories": ["cs.CL", "I.2.7; K.4.1; H.3.3"], "comment": "10 pages, 5 figures, IEEE conference format, submitted to [Conference\n  Name]", "summary": "Large language models (LLMs) are deployed globally, yet their underlying\ncultural and ethical assumptions remain underexplored. We propose the notion of\na \"cultural gene\" -- a systematic value orientation that LLMs inherit from\ntheir training corpora -- and introduce a Cultural Probe Dataset (CPD) of 200\nprompts targeting two classic cross-cultural dimensions:\nIndividualism-Collectivism (IDV) and Power Distance (PDI). Using standardized\nzero-shot prompts, we compare a Western-centric model (GPT-4) and an\nEastern-centric model (ERNIE Bot). Human annotation shows significant and\nconsistent divergence across both dimensions. GPT-4 exhibits individualistic\nand low-power-distance tendencies (IDV score approx 1.21; PDI score approx\n-1.05), while ERNIE Bot shows collectivistic and higher-power-distance\ntendencies (IDV approx -0.89; PDI approx 0.76); differences are statistically\nsignificant (p < 0.001). We further compute a Cultural Alignment Index (CAI)\nagainst Hofstede's national scores and find GPT-4 aligns more closely with the\nUSA (e.g., IDV CAI approx 0.91; PDI CAI approx 0.88) whereas ERNIE Bot aligns\nmore closely with China (IDV CAI approx 0.85; PDI CAI approx 0.81). Qualitative\nanalyses of dilemma resolution and authority-related judgments illustrate how\nthese orientations surface in reasoning. Our results support the view that LLMs\nfunction as statistical mirrors of their cultural corpora and motivate\nculturally aware evaluation and deployment to avoid algorithmic cultural\nhegemony.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63a2\u7d22\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6587\u5316\u504f\u5411\uff0c\u901a\u8fc7\u6587\u5316\u6316\u6398\u6570\u636e\u96c6\u548c\u6587\u5316\u5bf9\u9f50\u6307\u6570\u5f3a\u5316\u4e86GPT-4\u7684\u897f\u65b9\u4e2a\u4eba\u4e3b\u4e49\u7279\u5f81\u548cERNIE Bot\u7684\u4e1c\u65b9\u96c6\u4f53\u4e3b\u4e49\u7279\u5f81\uff0c\u544a\u8bc9\u4e86\u907f\u514d\u7b97\u6cd5\u6587\u5316\u9738\u6743\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5168\u7403\u90e8\u7f72\uff0c\u4f46\u5176\u57fa\u7840\u6587\u5316\u548c\u4f26\u7406\u504f\u5411\u5f88\u5c11\u88ab\u63a2\u7d22\u3002\u7814\u7a76\u8005\u60f3\u77e5\u9053\u8fd9\u4e9b\u6a21\u578b\u662f\u5426\u7ee7\u627f\u4e86\u8bad\u7ec3\u8bed\u6599\u7684\u7cfb\u7edf\u6027\u4ef7\u503c\u89c2\u5f0f\u3002", "method": "\u63d0\u51fa\"\u6587\u5316\u57fa\u56e0\"\u6982\u5ff5\uff0c\u6784\u5efa\u5305\u542b200\u4e2a\u63d0\u793a\u7684\u6587\u5316\u6316\u6398\u6570\u636e\u96c6(CPD)\uff0c\u91cd\u70b9\u8003\u5bdf\u4e2a\u4eba\u4e3b\u4e49-\u96c6\u4f53\u4e3b\u4e49(IDV)\u548c\u6743\u529b\u8ddd\u79bb(PDI)\u4e24\u4e2a\u7ecf\u5178\u8de8\u6587\u5316\u7ef4\u5ea6\u3002\u4f7f\u7528\u6807\u51c6\u5316\u96f6\u6837\u672c\u63d0\u793a\u6bd4\u8f83GPT-4\u548cERNIE Bot\u6a21\u578b\u3002", "result": "\u4eba\u5de5\u6807\u6ce8\u663e\u793a\u4e24\u4e2a\u6a21\u578b\u5728\u4e24\u4e2a\u7ef4\u5ea6\u4e0a\u5b58\u5728\u663e\u8457\u4e14\u4e00\u81f4\u7684\u5206\u5f02\u3002GPT-4\u5448\u73b0\u4e2a\u4eba\u4e3b\u4e49\u548c\u4f4e\u6743\u529b\u8ddd\u79bb\u503e\u5411\uff0cERNIE Bot\u5448\u73b0\u96c6\u4f53\u4e3b\u4e49\u548c\u8f83\u9ad8\u6743\u529b\u8ddd\u79bb\u503e\u5411\uff0c\u5dee\u5f02\u7edf\u8ba1\u663e\u8457(p < 0.001)\u3002\u6587\u5316\u5bf9\u9f50\u6307\u6570\u663e\u793aGPT-4\u66f4\u63a5\u8fd1\u7f8e\u56fd\uff0cERNIE Bot\u66f4\u63a5\u8fd1\u4e2d\u56fd\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u652f\u6301\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u5176\u6587\u5316\u8bed\u6599\u5e93\u7edf\u8ba1\u955c\u50cf\u7684\u89c2\u70b9\uff0c\u5e76\u9f13\u52b1\u91c7\u7528\u6587\u5316\u610f\u8bc6\u8bc4\u4f30\u548c\u90e8\u7f72\u65b9\u6848\uff0c\u4ee5\u907f\u514d\u7b97\u6cd5\u6587\u5316\u9738\u6743\u3002"}}
{"id": "2508.12448", "pdf": "https://arxiv.org/pdf/2508.12448", "abs": "https://arxiv.org/abs/2508.12448", "authors": ["Yeongwoo Song", "Jaeyong Bae", "Dong-Kyum Kim", "Hawoong Jeong"], "title": "Uncovering Emergent Physics Representations Learned In-Context by Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "17 pages, 10 figures", "summary": "Large language models (LLMs) exhibit impressive in-context learning (ICL)\nabilities, enabling them to solve wide range of tasks via textual prompts\nalone. As these capabilities advance, the range of applicable domains continues\nto expand significantly. However, identifying the precise mechanisms or\ninternal structures within LLMs that allow successful ICL across diverse,\ndistinct classes of tasks remains elusive. Physics-based tasks offer a\npromising testbed for probing this challenge. Unlike synthetic sequences such\nas basic arithmetic or symbolic equations, physical systems provide\nexperimentally controllable, real-world data based on structured dynamics\ngrounded in fundamental principles. This makes them particularly suitable for\nstudying the emergent reasoning behaviors of LLMs in a realistic yet tractable\nsetting. Here, we mechanistically investigate the ICL ability of LLMs,\nespecially focusing on their ability to reason about physics. Using a dynamics\nforecasting task in physical systems as a proxy, we evaluate whether LLMs can\nlearn physics in context. We first show that the performance of dynamics\nforecasting in context improves with longer input contexts. To uncover how such\ncapability emerges in LLMs, we analyze the model's residual stream activations\nusing sparse autoencoders (SAEs). Our experiments reveal that the features\ncaptured by SAEs correlate with key physical variables, such as energy. These\nfindings demonstrate that meaningful physical concepts are encoded within LLMs\nduring in-context learning. In sum, our work provides a novel case study that\nbroadens our understanding of how LLMs learn in context.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u901a\u8fc7\u7269\u7406\u5b66\u4efb\u52a1\u63a2\u7d22\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\uff0c\u53d1\u73b0\u6a21\u578b\u80fd\u591f\u5728\u4e0a\u4e0b\u6587\u4e2d\u7f16\u7801\u5173\u952e\u7269\u7406\u53d8\u91cf\u5982\u80fd\u91cf\uff0c\u63ed\u793a\u4e86LLM\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u673a\u5236\u3002", "motivation": "\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\u65b9\u9762\u8868\u73b0\u7a81\u51fa\uff0c\u4f46\u5176\u5185\u90e8\u673a\u5236\u4ecd\u4e0d\u660e\u786e\u3002\u7269\u7406\u5b66\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7406\u60f3\u7684\u7814\u7a76\u5e73\u53f0\uff0c\u56e0\u4e3a\u7269\u7406\u7cfb\u7edf\u57fa\u4e8e\u57fa\u7840\u539f\u7406\u4e14\u53ef\u5b9e\u9a8c\u63a7\u5236\u3002", "method": "\u4f7f\u7528\u7269\u7406\u7cfb\u7edf\u52a8\u529b\u5b66\u9884\u6d4b\u4efb\u52a1\u4f5c\u4e3a\u4ee3\u7406\uff0c\u5206\u6790LLM\u5728\u957f\u4e0a\u4e0b\u6587\u4e2d\u7684\u8868\u73b0\u3002\u901a\u8fc7\u7a00\u758f\u81ea\u7f16\u7801\u5668(SAEs)\u5206\u6790\u6a21\u578b\u6fc0\u6d3b\u503c\uff0c\u8bc6\u522b\u4e0e\u7269\u7406\u53d8\u91cf\u76f8\u5173\u7684\u7279\u5f81\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u52a8\u529b\u5b66\u9884\u6d4b\u6027\u80fd\u968f\u7740\u8f93\u5165\u4e0a\u4e0b\u6587\u957f\u5ea6\u589e\u52a0\u800c\u63d0\u5347\u3002SAEs\u6355\u83b7\u7684\u7279\u5f81\u4e0e\u5173\u952e\u7269\u7406\u53d8\u91cf\uff08\u5982\u80fd\u91cf\uff09\u5448\u73b0\u663e\u8457\u76f8\u5173\u6027\u3002", "conclusion": "\u8fd9\u4e2a\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u6269\u5c55\u4e86\u6211\u4eec\u5bf9LLM\u4e0a\u4e0b\u6587\u5b66\u4e60\u673a\u5236\u7684\u7406\u89e3\uff0c\u8bc1\u660e\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u6709\u610f\u4e49\u7684\u7269\u7406\u6982\u5ff5\u88ab\u7f16\u7801\u5230\u6a21\u578b\u4e2d\u3002"}}
{"id": "2508.12458", "pdf": "https://arxiv.org/pdf/2508.12458", "abs": "https://arxiv.org/abs/2508.12458", "authors": ["Ruirui Gao", "Emily Johnson", "Bowen Tan", "Yanfei Qian"], "title": "M3PO: Multimodal-Model-Guided Preference Optimization for Visual Instruction Following", "categories": ["cs.CL"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) hold immense potential for complex\nmultimodal instruction following, yet their development is often hindered by\nthe high cost and inconsistency of human annotation required for effective\nfine-tuning and preference alignment. Traditional supervised fine-tuning (SFT)\nand existing preference optimization methods like RLHF and DPO frequently\nstruggle to efficiently leverage the model's own generation space to identify\nhighly informative \"hard negative\" samples. To address these challenges, we\npropose Multimodal-Model-Guided Preference Optimization (M3PO), a novel and\ndata-efficient method designed to enhance LVLMs' capabilities in visual\ninstruction following. M3PO intelligently selects the most \"learning-valuable\"\npreference sample pairs from a diverse pool of LVLM-generated candidates. This\nselection is driven by a sophisticated mechanism that integrates two crucial\nsignals: a Multimodal Alignment Score (MAS) to assess external quality and the\nmodel's Self-Consistency / Confidence (log-probability) to gauge internal\nbelief. These are combined into a novel M3P-Score, which specifically\nidentifies preferred responses and challenging dispreferred responses that the\nmodel might confidently generate despite being incorrect. These high-quality\npreference pairs are then used for efficient Direct Preference Optimization\n(DPO) fine-tuning on base LVLMs like LLaVA-1.5 (7B/13B) using LoRA. Our\nextensive experiments demonstrate that M3PO consistently outperforms strong\nbaselines, including SFT, simulated RLHF, vanilla DPO, and RM-DPO, across a\ncomprehensive suite of multimodal instruction following benchmarks (MME-Bench,\nPOPE, IFT, Human Pref. Score).", "AI": {"tldr": "M3PO\u662f\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001\u6a21\u578b\u5f15\u5bfc\u504f\u597d\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u667a\u80fd\u9009\u62e9LVLM\u751f\u6210\u7684\u9ad8\u4ef7\u503c\u504f\u597d\u6837\u672c\u5bf9\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u5bf9\u9f50\u5206\u6570\u548c\u6a21\u578b\u81ea\u4e00\u81f4\u6027\u8bc4\u5206\u6765\u63d0\u5347\u89c6\u89c9\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\u53d7\u9650\u4e8e\u4eba\u5de5\u6807\u6ce8\u7684\u9ad8\u6210\u672c\u548c\u4e00\u81f4\u6027\u5dee\u7684\u95ee\u9898\uff0c\u4f20\u7edf\u76d1\u7763\u5fae\u8c03\u548c\u504f\u597d\u4f18\u5316\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5229\u7528\u6a21\u578b\u81ea\u8eab\u751f\u6210\u7a7a\u95f4\u6765\u8bc6\u522b\u9ad8\u4ef7\u503c\u7684\u56f0\u96be\u8d1f\u6837\u672c\u3002", "method": "\u63d0\u51faM3P-Score\u673a\u5236\uff0c\u6574\u5408\u591a\u6a21\u6001\u5bf9\u9f50\u5206\u6570(MAS)\u548c\u6a21\u578b\u81ea\u7f6e\u4fe1\u5ea6(log\u6982\u7387)\uff0c\u4eceLVLM\u751f\u6210\u7684\u5019\u9009\u6837\u672c\u4e2d\u9009\u62e9\u6700\u5177\u5b66\u4e60\u4ef7\u503c\u7684\u504f\u597d\u6837\u672c\u5bf9\uff0c\u7136\u540e\u4f7f\u7528LoRA\u8fdb\u884c\u76f4\u63a5\u504f\u597d\u4f18\u5316(DPO)\u5fae\u8c03\u3002", "result": "\u5728\u591a\u4e2a\u591a\u6a21\u6001\u6307\u4ee4\u8ddf\u968f\u57fa\u51c6\u6d4b\u8bd5(MME-Bench\u3001POPE\u3001IFT\u3001Human Pref. Score)\u4e0a\uff0cM3PO consistently outperforms strong baselines\uff0c\u5305\u62ecSFT\u3001\u6a21\u62dfRLHF\u3001\u666e\u901aDPO\u548cRM-DPO\u3002", "conclusion": "M3PO\u63d0\u4f9b\u4e86\u4e00\u79cd\u6570\u636e\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u589e\u5f3aLVLMs\u7684\u89c6\u89c9\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\uff0c\u901a\u8fc7\u667a\u80fd\u6837\u672c\u9009\u62e9\u673a\u5236\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u8bc6\u522b\u56f0\u96be\u8d1f\u6837\u672c\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2508.12459", "pdf": "https://arxiv.org/pdf/2508.12459", "abs": "https://arxiv.org/abs/2508.12459", "authors": ["Alham Fikri Aji", "Trevor Cohn"], "title": "LoraxBench: A Multitask, Multilingual Benchmark Suite for 20 Indonesian Languages", "categories": ["cs.CL"], "comment": null, "summary": "As one of the world's most populous countries, with 700 languages spoken,\nIndonesia is behind in terms of NLP progress. We introduce LoraxBench, a\nbenchmark that focuses on low-resource languages of Indonesia and covers 6\ndiverse tasks: reading comprehension, open-domain QA, language inference,\ncausal reasoning, translation, and cultural QA. Our dataset covers 20\nlanguages, with the addition of two formality registers for three languages. We\nevaluate a diverse set of multilingual and region-focused LLMs and found that\nthis benchmark is challenging. We note a visible discrepancy between\nperformance in Indonesian and other languages, especially the low-resource\nones. There is no clear lead when using a region-specific model as opposed to\nthe general multilingual model. Lastly, we show that a change in register\naffects model performance, especially with registers not commonly found in\nsocial media, such as high-level politeness `Krama' Javanese.", "AI": {"tldr": "LoraxBench\u662f\u4e00\u4e2a\u9488\u5bf9\u5370\u5c3c\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d66\u4e2a\u4efb\u52a1\u548c20\u79cd\u8bed\u8a00\uff0c\u53d1\u73b0\u591a\u8bed\u8a00\u6a21\u578b\u5728\u5370\u5c3c\u8bed\u8a00\u4e0a\u8868\u73b0\u5b58\u5728\u660e\u663e\u5dee\u8ddd\uff0c\u4e14\u8bed\u57df\u53d8\u5316\u663e\u8457\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5370\u5c3c\u4f5c\u4e3a\u4eba\u53e3\u5927\u56fd\u62e5\u6709700\u79cd\u8bed\u8a00\uff0c\u4f46\u5728NLP\u53d1\u5c55\u65b9\u9762\u76f8\u5bf9\u843d\u540e\uff0c\u9700\u8981\u4e13\u95e8\u7684\u57fa\u51c6\u6d4b\u8bd5\u6765\u8bc4\u4f30\u4f4e\u8d44\u6e90\u8bed\u8a00\u5904\u7406\u80fd\u529b\u3002", "method": "\u6784\u5efaLoraxBench\u57fa\u51c6\uff0c\u5305\u542b\u9605\u8bfb\u7406\u89e3\u3001\u5f00\u653e\u57dfQA\u3001\u8bed\u8a00\u63a8\u7406\u3001\u56e0\u679c\u63a8\u7406\u3001\u7ffb\u8bd1\u548c\u6587\u5316QA\u7b496\u4e2a\u4efb\u52a1\uff0c\u8986\u76d620\u79cd\u8bed\u8a00\u548c\u4e0d\u540c\u8bed\u57df\u53d8\u4f53\u3002", "result": "\u57fa\u51c6\u6d4b\u8bd5\u5177\u6709\u6311\u6218\u6027\uff0c\u5370\u5c3c\u8bed\u4e0e\u5176\u4ed6\u4f4e\u8d44\u6e90\u8bed\u8a00\u6027\u80fd\u5b58\u5728\u660e\u663e\u5dee\u8ddd\uff0c\u533a\u57df\u7279\u5b9a\u6a21\u578b\u4e0e\u901a\u7528\u591a\u8bed\u8a00\u6a21\u578b\u65e0\u663e\u8457\u4f18\u52bf\uff0c\u8bed\u57df\u53d8\u5316\uff08\u5982\u9ad8\u7ea7\u793c\u8c8c\u8bed\uff09\u663e\u8457\u5f71\u54cd\u6a21\u578b\u8868\u73b0\u3002", "conclusion": "\u5370\u5c3c\u4f4e\u8d44\u6e90\u8bed\u8a00NLP\u4ecd\u9762\u4e34\u91cd\u5927\u6311\u6218\uff0c\u9700\u8981\u66f4\u591a\u9488\u5bf9\u6027\u7684\u7814\u7a76\u548c\u6a21\u578b\u4f18\u5316\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u4e0d\u540c\u8bed\u57df\u548c\u6587\u5316\u8bed\u5883\u65b9\u9762\u3002"}}
{"id": "2508.12461", "pdf": "https://arxiv.org/pdf/2508.12461", "abs": "https://arxiv.org/abs/2508.12461", "authors": ["Ziqian Bi", "Keyu Chen", "Chiung-Yi Tseng", "Danyang Zhang", "Tianyang Wang", "Hongying Luo", "Lu Chen", "Junming Huang", "Jibin Guan", "Junfeng Hao", "Junhao Song"], "title": "Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI's Latest Open Source Models", "categories": ["cs.CL"], "comment": null, "summary": "In August 2025, OpenAI released GPT-OSS models, its first open weight large\nlanguage models since GPT-2 in 2019, comprising two mixture of experts\narchitectures with 120B and 20B parameters. We evaluated both variants against\nsix contemporary open source large language models ranging from 14.7B to 235B\nparameters, representing both dense and sparse designs, across ten benchmarks\ncovering general knowledge, mathematical reasoning, code generation,\nmultilingual understanding, and conversational ability. All models were tested\nin unquantised form under standardised inference settings, with statistical\nvalidation using McNemars test and effect size analysis. Results show that\ngpt-oss-20B consistently outperforms gpt-oss-120B on several benchmarks, such\nas HumanEval and MMLU, despite requiring substantially less memory and energy\nper response. Both models demonstrate mid-tier overall performance within the\ncurrent open source landscape, with relative strength in code generation and\nnotable weaknesses in multilingual tasks. These findings provide empirical\nevidence that scaling in sparse architectures may not yield proportional\nperformance gains, underscoring the need for further investigation into\noptimisation strategies and informing more efficient model selection for future\nopen source deployments.", "AI": {"tldr": "OpenAI\u53d1\u5e03GPT-OSS\u5f00\u6e90\u6a21\u578b\uff0820B\u548c120B\u53c2\u6570\uff09\uff0c\u8bc4\u4f30\u663e\u793a20B\u6a21\u578b\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e120B\u6a21\u578b\uff0c\u8868\u660e\u7a00\u758f\u67b6\u6784\u7684\u89c4\u6a21\u6269\u5c55\u4e0d\u4e00\u5b9a\u5e26\u6765\u6027\u80fd\u63d0\u5347", "motivation": "\u8bc4\u4f30OpenAI\u9996\u4e2a\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578bGPT-OSS\u7684\u6027\u80fd\uff0c\u6bd4\u8f83\u4e0d\u540c\u53c2\u6570\u89c4\u6a21\u7684\u7a00\u758f\u67b6\u6784\u6a21\u578b\u5728\u5404\u9879\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u4e3a\u5f00\u6e90\u6a21\u578b\u90e8\u7f72\u63d0\u4f9b\u5b9e\u8bc1\u4f9d\u636e", "method": "\u572810\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u6bd4\u8f836\u4e2a\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\uff0814.7B-235B\u53c2\u6570\uff09\uff0c\u4f7f\u7528\u6807\u51c6\u5316\u63a8\u7406\u8bbe\u7f6e\uff0c\u91c7\u7528McNemar\u68c0\u9a8c\u548c\u6548\u5e94\u91cf\u5206\u6790\u8fdb\u884c\u7edf\u8ba1\u9a8c\u8bc1", "result": "GPT-OSS-20B\u5728HumanEval\u548cMMLU\u7b49\u591a\u4e2a\u57fa\u51c6\u4e0a\u6301\u7eed\u4f18\u4e8eGPT-OSS-120B\uff0c\u4e14\u5185\u5b58\u548c\u80fd\u8017\u66f4\u4f4e\uff1b\u4e24\u4e2a\u6a21\u578b\u5728\u5f00\u6e90\u6a21\u578b\u4e2d\u5904\u4e8e\u4e2d\u6e38\u6c34\u5e73\uff0c\u4ee3\u7801\u751f\u6210\u8f83\u5f3a\u4f46\u591a\u8bed\u8a00\u80fd\u529b\u8f83\u5f31", "conclusion": "\u7a00\u758f\u67b6\u6784\u7684\u89c4\u6a21\u6269\u5c55\u53ef\u80fd\u4e0d\u4f1a\u5e26\u6765\u76f8\u5e94\u7684\u6027\u80fd\u589e\u76ca\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u4f18\u5316\u7b56\u7565\uff0c\u4e3a\u672a\u6765\u5f00\u6e90\u90e8\u7f72\u63d0\u4f9b\u66f4\u9ad8\u6548\u7684\u6a21\u578b\u9009\u62e9\u6307\u5bfc"}}
{"id": "2508.12482", "pdf": "https://arxiv.org/pdf/2508.12482", "abs": "https://arxiv.org/abs/2508.12482", "authors": ["Xiaomeng Zhu", "R. Thomas McCoy", "Robert Frank"], "title": "The Structural Sources of Verb Meaning Revisited: Large Language Models Display Syntactic Bootstrapping", "categories": ["cs.CL"], "comment": null, "summary": "Syntactic bootstrapping (Gleitman, 1990) is the hypothesis that children use\nthe syntactic environments in which a verb occurs to learn its meaning. In this\npaper, we examine whether large language models exhibit a similar behavior. We\ndo this by training RoBERTa and GPT-2 on perturbed datasets where syntactic\ninformation is ablated. Our results show that models' verb representation\ndegrades more when syntactic cues are removed than when co-occurrence\ninformation is removed. Furthermore, the representation of mental verbs, for\nwhich syntactic bootstrapping has been shown to be particularly crucial in\nhuman verb learning, is more negatively impacted in such training regimes than\nphysical verbs. In contrast, models' representation of nouns is affected more\nwhen co-occurrences are distorted than when syntax is distorted. In addition to\nreinforcing the important role of syntactic bootstrapping in verb learning, our\nresults demonstrated the viability of testing developmental hypotheses on a\nlarger scale through manipulating the learning environments of large language\nmodels.", "AI": {"tldr": "\u8bed\u6cd5\u542f\u52a8\u5b66\u4e60\u5728\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u8868\u73b0\uff1a\u79fb\u9664\u8bed\u6cd5\u4fe1\u606f\u6bd4\u79fb\u9664\u5171\u73b0\u4fe1\u606f\u66f4\u4e25\u91cd\u5f71\u54cd\u52a8\u8bcd\u8868\u5f81\uff0c\u5fc3\u7406\u52a8\u8bcd\u53d7\u5f71\u54cd\u66f4\u5927", "motivation": "\u68c0\u9a8c\u5927\u8bed\u8a00\u6a21\u578b\u662f\u5426\u4e5f\u50cf\u513f\u7ae5\u4e00\u6837\u901a\u8fc7\u8bed\u6cd5\u73af\u5883\u5b66\u4e60\u52a8\u8bcd\u542b\u4e49\uff0c\u5373\u8bed\u6cd5\u542f\u52a8\u5b66\u4e60\u5047\u8bbefalse\u5728AI\u6a21\u578b\u4e2d\u7684\u9002\u7528\u6027", "method": "\u5bf9RoBERTa\u548cGPT-2\u8fdb\u884c\u8bad\u7ec3\uff0c\u4f7f\u7528\u7ecf\u8fc7\u6279\u53d8\u7684\u6570\u636e\u96c6\uff08\u79fb\u9664\u8bed\u6cd5\u4fe1\u606f\u6216\u5171\u73b0\u4fe1\u606f\uff09\uff0c\u6bd4\u8f83\u6a21\u578b\u5728\u4e0d\u540c\u8bad\u7ec3\u60c5\u51b5\u4e0b\u7684\u8868\u73b0", "result": "\u79fb\u9664\u8bed\u6cd5\u7ebf\u7d22\u5bfc\u81f4\u52a8\u8bcd\u8868\u5f81\u66f4\u5927\u7684\u9000\u5316\uff0c\u5c24\u5176\u662f\u5fc3\u7406\u52a8\u8bcd\uff1b\u800c\u540d\u8bcd\u8868\u5f81\u66f4\u53d7\u5171\u73b0\u4fe1\u606f\u5f71\u54cd\uff1b\u8bc1\u5b9e\u8bed\u6cd5\u542f\u52a8\u5b66\u4e60\u5728\u6a21\u578b\u4e2d\u7684\u91cd\u8981\u6027", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u64cd\u7eb5\u5b66\u4e60\u73af\u5883\u53ef\u4ee5\u5927\u89c4\u6a21\u6d4b\u8bd5\u53d1\u80b2\u5047\u8bbefalse\uff0c\u8bc1\u660e\u8bed\u6cd5\u542f\u52a8\u5b66\u4e60\u5728\u52a8\u8bcd\u5b66\u4e60\u4e2d\u5177\u6709\u91cd\u8981\u4f5c\u7528"}}
{"id": "2508.12495", "pdf": "https://arxiv.org/pdf/2508.12495", "abs": "https://arxiv.org/abs/2508.12495", "authors": ["Yuangang Li", "Yiqing Shen", "Yi Nian", "Jiechao Gao", "Ziyi Wang", "Chenxiao Yu", "Shawn Li", "Jie Wang", "Xiyang Hu", "Yue Zhao"], "title": "Mitigating Hallucinations in Large Language Models via Causal Reasoning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) exhibit logically inconsistent hallucinations\nthat appear coherent yet violate reasoning principles, with recent research\nsuggesting an inverse relationship between causal reasoning capabilities and\nsuch hallucinations. However, existing reasoning approaches in LLMs, such as\nChain-of-Thought (CoT) and its graph-based variants, operate at the linguistic\ntoken level rather than modeling the underlying causal relationships between\nvariables, lacking the ability to represent conditional independencies or\nsatisfy causal identification assumptions. To bridge this gap, we introduce\ncausal-DAG construction and reasoning (CDCR-SFT), a supervised fine-tuning\nframework that trains LLMs to explicitly construct variable-level directed\nacyclic graph (DAG) and then perform reasoning over it. Moreover, we present a\ndataset comprising 25,368 samples (CausalDR), where each sample includes an\ninput question, explicit causal DAG, graph-based reasoning trace, and validated\nanswer. Experiments on four LLMs across eight tasks show that CDCR-SFT improves\nthe causal reasoning capability with the state-of-the-art 95.33% accuracy on\nCLADDER (surpassing human performance of 94.8% for the first time) and reduces\nthe hallucination on HaluEval with 10% improvements. It demonstrates that\nexplicit causal structure modeling in LLMs can effectively mitigate logical\ninconsistencies in LLM outputs. Code is available at\nhttps://github.com/MrLYG/CDCR-SFT.", "AI": {"tldr": "\u901a\u8fc7\u76f4\u63a5\u5efa\u6a21\u53d8\u91cf\u7ea7\u56e0\u679c\u56fe\u8fdb\u884c\u7406\u8bba\uff0cCDCR-SFT\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u7684\u56e0\u679c\u63a8\u7406\u80fd\u529b\u5e76\u964d\u4f4e\u5e7b\u89c9\u73b0\u8c61", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u903b\u8f91\u4e0d\u4e00\u81f4\u6027\u5e7b\u89c9\u65b9\u9762\u7684\u95ee\u9898\uff0c\u73b0\u6709\u7684\u94fe\u5f0f\u601d\u7ef4\u7b49\u65b9\u6cd5\u5728\u8bed\u8a00\u6807\u8bb0\u5c42\u9762\u8fd0\u4f5c\uff0c\u7f3a\u4e4f\u5bf9\u57fa\u7840\u56e0\u679c\u5173\u7cfb\u7684\u5efa\u6a21\u80fd\u529b", "method": "\u63d0\u51faCDCR-SFT\u76d1\u7763\u5fae\u8c03\u6846\u67b6\uff0c\u8bad\u7ec3LLMs\u660e\u786e\u6784\u5efa\u53d8\u91cf\u7ea7\u6709\u5411\u65e0\u73af\u56fe(DAG)\u5e76\u5728\u5176\u4e0a\u8fdb\u884c\u63a8\u7406\uff0c\u540c\u65f6\u521b\u5efa\u4e86\u5305\u542b25,368\u4e2a\u6837\u672c\u7684CausalDR\u6570\u636e\u96c6", "result": "\u57284\u4e2aLLMs\u548c8\u4e2a\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cCDCR-SFT\u5728CLADDER\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e86195.33%\u7684\u6700\u9ad8\u7cbe\u5ea6(\u9996\u6b21\u8d85\u8d8a\u4eba\u7c7b\u8868\u73b094.8%)\uff0c\u5e76\u5728HaluEval\u4e0a\u5c06\u5e7b\u89c9\u964d\u4f4e\u4e8610%", "conclusion": "\u660e\u786e\u7684\u56e0\u679c\u7ed3\u6784\u5efa\u6a21\u80fd\u591f\u6709\u6548\u51cf\u5c11LLM\u8f93\u51fa\u4e2d\u7684\u903b\u8f91\u4e0d\u4e00\u81f4\u6027\uff0c\u4e3a\u63d0\u5347\u6a21\u578b\u7406\u6027\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411"}}
{"id": "2508.12535", "pdf": "https://arxiv.org/pdf/2508.12535", "abs": "https://arxiv.org/abs/2508.12535", "authors": ["Seonglae Cho", "Zekun Wu", "Adriano Koshiyama"], "title": "CorrSteer: Steering Improves Task Performance and Safety in LLMs through Correlation-based Sparse Autoencoder Feature Selection", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "42 pages, 9 tables", "summary": "Sparse Autoencoders (SAEs) can extract interpretable features from large\nlanguage models (LLMs) without supervision. However, their effectiveness in\ndownstream steering tasks is limited by the requirement for contrastive\ndatasets or large activation storage. To address these limitations, we propose\nCorrSteer, which selects features by correlating sample correctness with SAE\nactivations from generated tokens at inference time. This approach uses only\ninference-time activations to extract more relevant features, thereby avoiding\nspurious correlations. It also obtains steering coefficients from average\nactivations, automating the entire pipeline. Our method shows improved task\nperformance on QA, bias mitigation, jailbreaking prevention, and reasoning\nbenchmarks on Gemma 2 2B and LLaMA 3.1 8B, notably achieving a +4.1%\nimprovement in MMLU performance and a +22.9% improvement in HarmBench with only\n4000 samples. Selected features demonstrate semantically meaningful patterns\naligned with each task's requirements, revealing the underlying capabilities\nthat drive performance. Our work establishes correlationbased selection as an\neffective and scalable approach for automated SAE steering across language\nmodel applications.", "AI": {"tldr": "CorrSteer\u662f\u4e00\u79cd\u57fa\u4e8e\u76f8\u5173\u6027\u9009\u62e9\u7a00\u758f\u81ea\u7f16\u7801\u5668\u7279\u5f81\u7684\u65b0\u65b9\u6cd5\uff0c\u4ec5\u4f7f\u7528\u63a8\u7406\u65f6\u6fc0\u6d3b\u6765\u81ea\u52a8\u63d0\u53d6\u76f8\u5173\u7279\u5f81\uff0c\u5728\u591a\u9879\u4efb\u52a1\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd", "motivation": "\u89e3\u51b3\u4f20\u7edf\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u9700\u8981\u5bf9\u6bd4\u6570\u636e\u96c6\u6216\u5927\u91cf\u6fc0\u6d3b\u5b58\u50a8\u7684\u9650\u5236\uff0c\u5bfb\u6c42\u66f4\u9ad8\u6548\u7684\u7279\u5f81\u9009\u62e9\u548c\u5f15\u5bfc\u65b9\u6cd5", "method": "\u901a\u8fc7\u5c06\u6837\u672c\u6b63\u786e\u6027\u4e0e\u63a8\u7406\u65f6\u751f\u6210\u7684token\u7684SAE\u6fc0\u6d3b\u8fdb\u884c\u76f8\u5173\u6027\u5206\u6790\u6765\u9009\u62e9\u7279\u5f81\uff0c\u4f7f\u7528\u5e73\u5747\u6fc0\u6d3b\u83b7\u5f97\u5f15\u5bfc\u7cfb\u6570\uff0c\u5b9e\u73b0\u5168\u81ea\u52a8\u5316\u6d41\u7a0b", "result": "\u5728Gemma 2 2B\u548cLLaMA 3.1 8B\u6a21\u578b\u4e0a\uff0cMMLU\u6027\u80fd\u63d0\u53474.1%\uff0cHarmBench\u63d0\u534722.9%\uff0c\u4ec5\u97004000\u4e2a\u6837\u672c", "conclusion": "\u57fa\u4e8e\u76f8\u5173\u6027\u9009\u62e9\u7684\u65b9\u6cd5\u4e3a\u81ea\u52a8\u5316SAE\u5f15\u5bfc\u63d0\u4f9b\u4e86\u6709\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63ed\u793a\u4e86\u9a71\u52a8\u6027\u80fd\u7684\u5e95\u5c42\u80fd\u529b\u6a21\u5f0f"}}
{"id": "2508.12591", "pdf": "https://arxiv.org/pdf/2508.12591", "abs": "https://arxiv.org/abs/2508.12591", "authors": ["Yu-Hsuan Fang", "Tien-Hong Lo", "Yao-Ting Sung", "Berlin Chen"], "title": "Beyond Modality Limitations: A Unified MLLM Approach to Automated Speaking Assessment with Effective Curriculum Learning", "categories": ["cs.CL", "cs.AI", "cs.SD"], "comment": "Accepted at IEEE ASRU 2025", "summary": "Traditional Automated Speaking Assessment (ASA) systems exhibit inherent\nmodality limitations: text-based approaches lack acoustic information while\naudio-based methods miss semantic context. Multimodal Large Language Models\n(MLLM) offer unprecedented opportunities for comprehensive ASA by\nsimultaneously processing audio and text within unified frameworks. This paper\npresents a very first systematic study of MLLM for comprehensive ASA,\ndemonstrating the superior performance of MLLM across the aspects of content\nand language use . However, assessment on the delivery aspect reveals unique\nchallenges, which is deemed to require specialized training strategies. We thus\npropose Speech-First Multimodal Training (SFMT), leveraging a curriculum\nlearning principle to establish more robust modeling foundations of speech\nbefore cross-modal synergetic fusion. A series of experiments on a benchmark\ndataset show MLLM-based systems can elevate the holistic assessment performance\nfrom a PCC value of 0.783 to 0.846. In particular, SFMT excels in the\nevaluation of the delivery aspect, achieving an absolute accuracy improvement\nof 4% over conventional training approaches, which also paves a new avenue for\nASA.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u6027\u7814\u7a76\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u8bf4\u8bdd\u8bc4\u6d4b\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u4e13\u95e8\u7684\u8bed\u97f3\u4f18\u5148\u591a\u6a21\u6001\u8bad\u7ec3\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bc4\u6d4b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u81ea\u52a8\u8bf4\u8bdd\u8bc4\u6d4b\u7cfb\u7edf\u5b58\u5728\u6a21\u6001\u9650\u5236\uff1a\u6587\u672c\u65b9\u6cd5\u7f3a\u4e4f\u58f0\u5b66\u4fe1\u606f\uff0c\u97f3\u9891\u65b9\u6cd5\u7f3a\u5c11\u8bed\u4e49\u4e0a\u4e0b\u6587\u3002\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e3a\u5168\u9762\u8bc4\u6d4b\u63d0\u4f9b\u4e86\u65b0\u673a\u9047\u3002", "method": "\u63d0\u51faSpeech-First Multimodal Training (SFMT)\u65b9\u6cd5\uff0c\u5229\u7528\u8bfe\u7a0b\u5b66\u4e60\u539f\u7406\uff0c\u5148\u5efa\u7acb\u5065\u58ee\u7684\u8bed\u97f3\u5efa\u6a21\u57fa\u7840\uff0c\u518d\u8fdb\u884c\u8de8\u6a21\u6001\u534f\u540c\u878d\u5408\u3002", "result": "\u5728\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u5c06\u6574\u4f53\u8bc4\u6d4b\u6027\u80fd\u4ecePCC 0.783\u63d0\u5347\u52300.846\u3002\u5728\u8868\u8fbe\u65b9\u9762\u7684\u8bc4\u4f30\u4e2d\uff0cSFMT\u65b9\u6cd5\u6bd4\u4f20\u7edf\u8bad\u7ec3\u65b9\u6cd5\u7edd\u5bf9\u51c6\u786e\u7387\u63d0\u9ad84%\u3002", "conclusion": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u663e\u8457\u63d0\u5347\u81ea\u52a8\u8bf4\u8bdd\u8bc4\u6d4b\u7684\u6027\u80fd\uff0c\u800c\u4e13\u95e8\u7684\u8bed\u97f3\u4f18\u5148\u8bad\u7ec3\u7b56\u7565\u80fd\u591f\u6709\u6548\u89e3\u51b3\u8868\u8fbe\u65b9\u9762\u7684\u8bc4\u6d4b\u6311\u6218\uff0c\u4e3a\u8be5\u9886\u57df\u5f00\u542f\u4e86\u65b0\u8def\u5f84\u3002"}}
{"id": "2508.12630", "pdf": "https://arxiv.org/pdf/2508.12630", "abs": "https://arxiv.org/abs/2508.12630", "authors": ["Maitreyi Chatterjee", "Devansh Agarwal"], "title": "Semantic Anchoring in Agentic Memory: Leveraging Linguistic Structures for Persistent Conversational Context", "categories": ["cs.CL"], "comment": "Paper is currently in peer review", "summary": "Large Language Models (LLMs) have demonstrated impressive fluency and task\ncompetence in conversational settings. However, their effectiveness in\nmulti-session and long-term interactions is hindered by limited memory\npersistence. Typical retrieval-augmented generation (RAG) systems store\ndialogue history as dense vectors, which capture semantic similarity but\nneglect finer linguistic structures such as syntactic dependencies, discourse\nrelations, and coreference links. We propose Semantic Anchoring, a hybrid\nagentic memory architecture that enriches vector-based storage with explicit\nlinguistic cues to improve recall of nuanced, context-rich exchanges. Our\napproach combines dependency parsing, discourse relation tagging, and\ncoreference resolution to create structured memory entries. Experiments on\nadapted long-term dialogue datasets show that semantic anchoring improves\nfactual recall and discourse coherence by up to 18% over strong RAG baselines.\nWe further conduct ablation studies, human evaluations, and error analysis to\nassess robustness and interpretability.", "AI": {"tldr": "\u8bed\u4e49\u951a\u5b9a\u6280\u672f\u901a\u8fc7\u7ed3\u5408\u8bed\u8a00\u7ed3\u6784\u7ec6\u8282\u6765\u6539\u5584LLM\u5728\u957f\u671f\u4ea4\u4e92\u4e2d\u7684\u8bb0\u5fc6\u6301\u7eed\u6027\uff0c\u5728\u4e8b\u5b9e\u56de\u5fc6\u548c\u8bed\u7ecf\u8fde\u8d2f\u6027\u65b9\u9762\u8d85\u8fc7\u57fa\u7ebfRAG\u7cfb\u7edf18%", "motivation": "\u73b0\u6709RAG\u7cfb\u7edf\u4ec5\u4f7f\u7528\u6d53\u5ea6\u5411\u91cf\u5b58\u50a8\u5bf9\u8bdd\u5386\u53f2\uff0c\u5ffd\u89c6\u4e86\u8bed\u6cd5\u4f9d\u5b58\u5173\u7cfb\u3001\u8bed\u7ecf\u5173\u7cfb\u548c\u6307\u793a\u5173\u7cfb\u7b49\u7ec6\u5fae\u8bed\u8a00\u7ed3\u6784\uff0c\u5bfc\u81f4\u957f\u671f\u4ea4\u4e92\u6548\u679c\u53d7\u9650", "method": "\u63d0\u51fa\u6df7\u5408\u5f0f\u4ee3\u7406\u8bb0\u5fc6\u67b6\u6784\uff0c\u7ed3\u5408\u4f9d\u5b58\u89e3\u6790\u3001\u8bed\u7ecf\u5173\u7cfb\u6807\u6ce8\u548c\u6307\u793a\u6d88\u89e3\u6280\u672f\uff0c\u5728\u5411\u91cf\u5b58\u50a8\u57fa\u7840\u4e0a\u6dfb\u52a0\u663e\u5f0f\u8bed\u8a00\u7ebf\u7d22\u6765\u521b\u5efa\u7ed3\u6784\u5316\u8bb0\u5fc6\u9879", "result": "\u5728\u9002\u914d\u7684\u957f\u671f\u5bf9\u8bdd\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\uff0c\u8bed\u4e49\u951a\u5b9a\u6280\u672f\u5728\u4e8b\u5b9e\u56de\u5fc6\u548c\u8bed\u7ecf\u8fde\u8d2f\u6027\u65b9\u9762\u8d85\u8fc7\u5f3a\u52b2RAG\u57fa\u7ebf\u8fbe18%\uff0c\u8fdb\u884c\u4e86\u6d88\u878d\u7814\u7a76\u3001\u4eba\u5de5\u8bc4\u4f30\u548c\u9519\u8bef\u5206\u6790\u4ee5\u8bc4\u4f30\u7a33\u5065\u6027\u548c\u53ef\u89e3\u91ca\u6027", "conclusion": "\u901a\u8fc7\u5c06\u663e\u5f0f\u8bed\u8a00\u7ebf\u7d22\u4e0e\u5411\u91cf\u8868\u5f81\u76f8\u7ed3\u5408\uff0c\u8bed\u4e49\u951a\u5b9a\u6280\u672f\u80fd\u591f\u6709\u6548\u63d0\u5347LLM\u5728\u591a\u6b21\u4f1a\u8bdd\u548c\u957f\u671f\u4ea4\u4e92\u4e2d\u7684\u8bb0\u5fc6\u6301\u7eed\u6027\u548c\u4e0a\u4e0b\u6587\u4e00\u81f4\u6027"}}
{"id": "2508.12631", "pdf": "https://arxiv.org/pdf/2508.12631", "abs": "https://arxiv.org/abs/2508.12631", "authors": ["Yiqun Zhang", "Hao Li", "Jianhao Chen", "Hangfan Zhang", "Peng Ye", "Lei Bai", "Shuyue Hu"], "title": "Beyond GPT-5: Making LLMs Cheaper and Better via Performance-Efficiency Optimized Routing", "categories": ["cs.CL"], "comment": "Ongoing work", "summary": "Balancing performance and efficiency is a central challenge in large language\nmodel (LLM) advancement. GPT-5 addresses this with test-time routing,\ndynamically assigning queries to either an efficient or a high-capacity model\nduring inference. In this work, we present Avengers-Pro, a test-time routing\nframework that ensembles LLMs of varying capacities and efficiencies, providing\na unified solution for all performance-efficiency tradeoffs. The Avengers-Pro\nembeds and clusters incoming queries, then routes each to the most suitable\nmodel based on a performance-efficiency score. Across 6 challenging benchmarks\nand 8 leading models -- including GPT-5-medium, Gemini-2.5-pro, and\nClaude-opus-4.1 -- Avengers-Pro achieves state-of-the-art results: by varying a\nperformance-efficiency trade-off parameter, it can surpass the strongest single\nmodel (GPT-5-medium) by +7% in average accuracy. Moreover, it can match the\naverage accuracy of the strongest single model at 27% lower cost, and reach\n~90% of that performance at 63% lower cost. Last but not least, it achieves a\nPareto frontier, consistently yielding the highest accuracy for any given cost,\nand the lowest cost for any given accuracy, among all single models. Code is\navailable at https://github.com/ZhangYiqun018/AvengersPro.", "AI": {"tldr": "Avengers-Pro\u662f\u4e00\u4e2a\u6d4b\u8bd5\u65f6\u8def\u7531\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u6210\u4e0d\u540c\u5bb9\u91cf\u548c\u6548\u7387\u7684LLM\uff0c\u52a8\u6001\u5206\u914d\u67e5\u8be2\u5230\u6700\u9002\u5408\u7684\u6a21\u578b\uff0c\u5728\u6027\u80fd-\u6548\u7387\u6743\u8861\u65b9\u9762\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6027\u80fd\u548c\u6548\u7387\u4e4b\u95f4\u7684\u5e73\u8861\u6311\u6218\uff0cGPT-5\u867d\u7136\u63d0\u51fa\u4e86\u6d4b\u8bd5\u65f6\u8def\u7531\uff0c\u4f46\u9700\u8981\u4e00\u4e2a\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848\u6765\u5904\u7406\u6240\u6709\u6027\u80fd-\u6548\u7387\u6743\u8861\u3002", "method": "\u5d4c\u5165\u548c\u805a\u7c7b\u4f20\u5165\u67e5\u8be2\uff0c\u7136\u540e\u57fa\u4e8e\u6027\u80fd-\u6548\u7387\u5206\u6570\u5c06\u6bcf\u4e2a\u67e5\u8be2\u8def\u7531\u5230\u6700\u5408\u9002\u7684\u6a21\u578b\uff0c\u96c6\u6210\u4e0d\u540c\u5bb9\u91cf\u548c\u6548\u7387\u7684LLM\u3002", "result": "\u57286\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c8\u4e2a\u9886\u5148\u6a21\u578b\u4e0a\uff0cAvengers-Pro\u8d85\u8d8a\u6700\u5f3a\u5355\u6a21\u578b(GPT-5-medium)\u5e73\u5747\u51c6\u786e\u7387+7%\uff0c\u4ee527%\u66f4\u4f4e\u7684\u6210\u672c\u8fbe\u5230\u76f8\u540c\u51c6\u786e\u7387\uff0c\u4ee563%\u66f4\u4f4e\u7684\u6210\u672c\u8fbe\u523090%\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u5e15\u7d2f\u6258\u6700\u4f18\u3002", "conclusion": "Avengers-Pro\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6d4b\u8bd5\u65f6\u8def\u7531\u6846\u67b6\uff0c\u80fd\u591f\u52a8\u6001\u4f18\u5316\u6027\u80fd-\u6548\u7387\u6743\u8861\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u6210\u672c\uff0c\u662f\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.12632", "pdf": "https://arxiv.org/pdf/2508.12632", "abs": "https://arxiv.org/abs/2508.12632", "authors": ["Chi Wang", "Min Gao", "Zongwei Wang", "Junwei Yin", "Kai Shu", "Chenghua Lin"], "title": "Prompt-Induced Linguistic Fingerprints for LLM-Generated Fake News Detection", "categories": ["cs.CL"], "comment": null, "summary": "With the rapid development of large language models, the generation of fake\nnews has become increasingly effortless, posing a growing societal threat and\nunderscoring the urgent need for reliable detection methods. Early efforts to\nidentify LLM-generated fake news have predominantly focused on the textual\ncontent itself; however, because much of that content may appear coherent and\nfactually consistent, the subtle traces of falsification are often difficult to\nuncover. Through distributional divergence analysis, we uncover prompt-induced\nlinguistic fingerprints: statistically distinct probability shifts between\nLLM-generated real and fake news when maliciously prompted. Based on this\ninsight, we propose a novel method named Linguistic Fingerprints Extraction\n(LIFE). By reconstructing word-level probability distributions, LIFE can find\ndiscriminative patterns that facilitate the detection of LLM-generated fake\nnews. To further amplify these fingerprint patterns, we also leverage\nkey-fragment techniques that accentuate subtle linguistic differences, thereby\nimproving detection reliability. Our experiments show that LIFE achieves\nstate-of-the-art performance in LLM-generated fake news and maintains high\nperformance in human-written fake news. The code and data are available at\nhttps://anonymous.4open.science/r/LIFE-E86A.", "AI": {"tldr": "\u63d0\u51faLIFE\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u6076\u610f\u63d0\u793a\u4e0bLLM\u751f\u6210\u771f\u5047\u65b0\u95fb\u7684\u6982\u7387\u5206\u5e03\u5dee\u5f02\u6765\u68c0\u6d4bAI\u751f\u6210\u7684\u5047\u65b0\u95fb\uff0c\u5728LLM\u751f\u6210\u548c\u4eba\u5de5\u64b0\u5199\u5047\u65b0\u95fb\u68c0\u6d4b\u4e0a\u90fd\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u53d1\u5c55\uff0c\u5047\u65b0\u95fb\u751f\u6210\u53d8\u5f97\u5bb9\u6613\u4e14\u96be\u4ee5\u68c0\u6d4b\uff0c\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6587\u672c\u5185\u5bb9\u672c\u8eab\uff0c\u4f46\u865a\u5047\u75d5\u8ff9\u5f80\u5f80\u5f88\u9690\u853d\uff0c\u9700\u8981\u65b0\u7684\u68c0\u6d4b\u65b9\u6cd5", "method": "\u901a\u8fc7\u5206\u5e03\u5dee\u5f02\u5206\u6790\u53d1\u73b0\u63d0\u793a\u8bf1\u5bfc\u7684\u8bed\u8a00\u6307\u7eb9\uff0c\u63d0\u51faLIFE\u65b9\u6cd5\u91cd\u6784\u8bcd\u7ea7\u6982\u7387\u5206\u5e03\u6765\u5bfb\u627e\u5224\u522b\u6a21\u5f0f\uff0c\u5e76\u5229\u7528\u5173\u952e\u7247\u6bb5\u6280\u672f\u653e\u5927\u8bed\u8a00\u5dee\u5f02", "result": "LIFE\u5728LLM\u751f\u6210\u5047\u65b0\u95fb\u68c0\u6d4b\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u540c\u65f6\u5728\u4eba\u5de5\u64b0\u5199\u5047\u65b0\u95fb\u68c0\u6d4b\u4e0a\u4e5f\u4fdd\u6301\u9ad8\u6027\u80fd", "conclusion": "\u57fa\u4e8e\u6982\u7387\u5206\u5e03\u7684\u8bed\u8a00\u6307\u7eb9\u5206\u6790\u662f\u68c0\u6d4bLLM\u751f\u6210\u5047\u65b0\u95fb\u7684\u6709\u6548\u65b9\u6cd5\uff0cLIFE\u65b9\u6cd5\u5177\u6709\u5f88\u597d\u7684\u68c0\u6d4b\u6548\u679c\u548c\u6cdb\u5316\u80fd\u529b"}}
{"id": "2508.12662", "pdf": "https://arxiv.org/pdf/2508.12662", "abs": "https://arxiv.org/abs/2508.12662", "authors": ["Tanay Nagar", "Grigorii Khvatskii", "Anna Sokol", "Nitesh V. Chawla"], "title": "Breaking Language Barriers: Equitable Performance in Multilingual Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted as a non-archival work-in-progress paper at the NAACL 2025\n  Student Research Workshop", "summary": "Cutting-edge LLMs have emerged as powerful tools for multilingual\ncommunication and understanding. However, LLMs perform worse in Common Sense\nReasoning (CSR) tasks when prompted in low-resource languages (LRLs) like Hindi\nor Swahili compared to high-resource languages (HRLs) like English. Equalizing\nthis inconsistent access to quality LLM outputs is crucial to ensure fairness\nfor speakers of LRLs and across diverse linguistic communities. In this paper,\nwe propose an approach to bridge this gap in LLM performance. Our approach\ninvolves fine-tuning an LLM on synthetic code-switched text generated using\ncontrolled language-mixing methods. We empirically demonstrate that fine-tuning\nLLMs on synthetic code-switched datasets leads to substantial improvements in\nLRL model performance while preserving or enhancing performance in HRLs.\nAdditionally, we present a new dataset of synthetic code-switched text derived\nfrom the CommonSenseQA dataset, featuring three distinct language ratio\nconfigurations.", "AI": {"tldr": "\u901a\u8fc7\u4ee3\u7801\u8f6c\u6362\u7cbe\u786e\u63a7\u5236\u7684\u8bed\u8a00\u6df7\u5408\u65b9\u6cd5\uff0c\u751f\u6210\u5408\u6210\u7684\u4ee3\u7801\u8f6c\u6362\u6587\u672c\u5bf9LLM\u8fdb\u884c\u5fae\u8c03\uff0c\u4ee5\u63d0\u5347\u4f4e\u8d44\u6e90\u8bed\u8a00\u5728\u5e38\u8bc6\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u9ad8\u8d44\u6e90\u8bed\u8a00\u7684\u8868\u73b0", "motivation": "\u867d\u7136\u5148\u8fdb\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8bed\u8a00\u901a\u4fe1\u548c\u7406\u89e3\u65b9\u9762\u8868\u73b0\u5f3a\u52b2\uff0c\u4f46\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u5370\u5730\u8bed\u3001\u65af\u74e6\u5e0c\u91cc\u8bed\uff09\u4e0a\u7684\u5e38\u8bc6\u63a8\u7406\u4efb\u52a1\u8868\u73b0\u8f83\u5dee\uff0c\u8fdc\u4e0d\u5982\u9ad8\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u82f1\u8bed\uff09\u3002\u5e73\u8861\u8fd9\u79cd\u6027\u80fd\u5dee\u5f02\u5bf9\u4e8e\u786e\u4fdd\u4e0d\u540c\u8bed\u8a00\u793e\u533a\u7684\u516c\u5e73\u6027\u81f3\u5173\u91cd\u8981", "method": "\u4f7f\u7528\u63a7\u5236\u8bed\u8a00\u6df7\u5408\u65b9\u6cd5\u751f\u6210\u5408\u6210\u7684\u4ee3\u7801\u8f6c\u6362\u6587\u672c\uff0c\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u8bad\u7ec3\u3002\u7814\u7a76\u8fd8\u63d0\u4f9b\u4e86\u4e00\u4e2a\u57fa\u4e8eCommonSenseQA\u6570\u636e\u96c6\u7684\u65b0\u7684\u5408\u6210\u4ee3\u7801\u8f6c\u6362\u6587\u672c\u6570\u636e\u96c6\uff0c\u5305\u542b\u4e09\u79cd\u4e0d\u540c\u7684\u8bed\u8a00\u6bd4\u4f8b\u914d\u7f6e", "result": "\u7ecf\u9a8c\u8bc1\u660e\uff0c\u5728\u5408\u6210\u4ee3\u7801\u8f6c\u6362\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u5b9e\u73b0\u4f4e\u8d44\u6e90\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u7684\u663e\u8457\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u9ad8\u8d44\u6e90\u8bed\u8a00\u7684\u8868\u73b0", "conclusion": "\u901a\u8fc7\u63a7\u5236\u8bed\u8a00\u6df7\u5408\u65b9\u6cd5\u751f\u6210\u5408\u6210\u4ee3\u7801\u8f6c\u6362\u6587\u672c\u5bf9LLM\u8fdb\u884c\u5fae\u8c03\uff0c\u662f\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u7f29\u5c0f\u4e0d\u540c\u8bed\u8a00\u5728\u5e38\u8bc6\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u7528\u6237\u63d0\u4f9b\u66f4\u516c\u5e73\u7684AI\u8bbf\u95ee\u673a\u4f1a"}}
{"id": "2508.12669", "pdf": "https://arxiv.org/pdf/2508.12669", "abs": "https://arxiv.org/abs/2508.12669", "authors": ["Bishanka Seal", "Rahul Seetharaman", "Aman Bansal", "Abhilash Nandy"], "title": "Leveraging Large Language Models for Predictive Analysis of Human Misery", "categories": ["cs.CL", "cs.CY"], "comment": "14 pages, 4 tables", "summary": "This study investigates the use of Large Language Models (LLMs) for\npredicting human-perceived misery scores from natural language descriptions of\nreal-world scenarios. The task is framed as a regression problem, where the\nmodel assigns a scalar value from 0 to 100 to each input statement. We evaluate\nmultiple prompting strategies, including zero-shot, fixed-context few-shot, and\nretrieval-based prompting using BERT sentence embeddings. Few-shot approaches\nconsistently outperform zero-shot baselines, underscoring the value of\ncontextual examples in affective prediction. To move beyond static evaluation,\nwe introduce the \"Misery Game Show\", a novel gamified framework inspired by a\ntelevision format. It tests LLMs through structured rounds involving ordinal\ncomparison, binary classification, scalar estimation, and feedback-driven\nreasoning. This setup enables us to assess not only predictive accuracy but\nalso the model's ability to adapt based on corrective feedback. The gamified\nevaluation highlights the broader potential of LLMs in dynamic emotional\nreasoning tasks beyond standard regression. Code and data link:\nhttps://github.com/abhi1nandy2/Misery_Data_Exps_GitHub", "AI": {"tldr": "\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u9884\u6d4b\u4eba\u7c7b\u611f\u77e5\u7684\u75db\u82e6\u5206\u6570\uff0c\u901a\u8fc7\u591a\u79cd\u63d0\u793a\u7b56\u7565\u548c\u521b\u65b0\u7684\u6e38\u620f\u5316\u8bc4\u4f30\u6846\u67b6\u6765\u6d4b\u8bd5\u6a21\u578b\u7684\u60c5\u611f\u63a8\u7406\u80fd\u529b", "motivation": "\u7814\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u5728\u60c5\u611f\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u5982\u4f55\u4ece\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u4e2d\u9884\u6d4b\u4eba\u7c7b\u611f\u77e5\u7684\u75db\u82e6\u7a0b\u5ea6\uff0c\u63a2\u7d22\u8d85\u8d8a\u4f20\u7edf\u9759\u6001\u8bc4\u4f30\u7684\u52a8\u6001\u60c5\u611f\u63a8\u7406\u80fd\u529b", "method": "\u91c7\u7528\u56de\u5f52\u95ee\u9898\u6846\u67b6\uff0c\u8bc4\u4f30\u96f6\u6837\u672c\u3001\u56fa\u5b9a\u4e0a\u4e0b\u6587\u5c11\u6837\u672c\u548c\u57fa\u4e8eBERT\u53e5\u5b50\u5d4c\u5165\u7684\u68c0\u7d22\u5f0f\u63d0\u793a\u7b56\u7565\u3002\u521b\u65b0\u6027\u5730\u8bbe\u8ba1\u4e86\"\u75db\u82e6\u6e38\u620f\u79c0\"\u6e38\u620f\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u542b\u5e8f\u6570\u6bd4\u8f83\u3001\u4e8c\u5143\u5206\u7c7b\u3001\u6807\u91cf\u4f30\u8ba1\u548c\u53cd\u9988\u9a71\u52a8\u63a8\u7406\u7b49\u591a\u4e2a\u73af\u8282", "result": "\u5c11\u6837\u672c\u65b9\u6cd5\u6301\u7eed\u4f18\u4e8e\u96f6\u6837\u672c\u57fa\u7ebf\uff0c\u8bc1\u660e\u4e86\u4e0a\u4e0b\u6587\u793a\u4f8b\u5728\u60c5\u611f\u9884\u6d4b\u4e2d\u7684\u4ef7\u503c\u3002\u6e38\u620f\u5316\u8bc4\u4f30\u663e\u793a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u52a8\u6001\u60c5\u611f\u63a8\u7406\u4efb\u52a1\u4e2d\u5177\u6709\u8d85\u8d8a\u6807\u51c6\u56de\u5f52\u7684\u6f5c\u529b", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u6709\u6548\u9884\u6d4b\u4eba\u7c7b\u611f\u77e5\u7684\u75db\u82e6\u5206\u6570\uff0c\u6e38\u620f\u5316\u8bc4\u4f30\u6846\u67b6\u4e3a\u6d4b\u8bd5\u6a21\u578b\u7684\u60c5\u611f\u63a8\u7406\u548c\u9002\u5e94\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0c\u5c55\u793a\u4e86\u5728\u52a8\u6001\u60c5\u611f\u4efb\u52a1\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u524d\u666f"}}
{"id": "2508.12685", "pdf": "https://arxiv.org/pdf/2508.12685", "abs": "https://arxiv.org/abs/2508.12685", "authors": ["Xingshan Zeng", "Weiwen Liu", "Lingzhi Wang", "Liangyou Li", "Fei Mi", "Yasheng Wang", "Lifeng Shang", "Xin Jiang", "Qun Liu"], "title": "ToolACE-MT: Non-Autoregressive Generation for Agentic Multi-Turn Interaction", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Agentic task-solving with Large Language Models (LLMs) requires multi-turn,\nmulti-step interactions, often involving complex function calls and dynamic\nuser-agent exchanges. Existing simulation-based data generation methods for\nsuch scenarios rely heavily on costly autoregressive interactions between\nmultiple LLM agents, thereby limiting real-world performance of agentic tasks.\nIn this paper, we propose a novel Non-Autoregressive Iterative Generation\nframework, called ToolACE-MT, for constructing high-quality multi-turn agentic\ndialogues. ToolACE-MT generates full conversational trajectories through three\nstages: coarse-grained initialization, iterative refinement, and offline\nverification. The initialization phase builds a structurally complete yet\nsemantically coarse dialogue skeleton; the iterative refinement phase\nintroduces realistic complexities and continued refinement via mask-and-fill\noperations; and the offline verification phase ensures correctness and\ncoherence via rule- and model-based checks. Experiments demonstrate that\nToolACE-MT enables efficient, effective and generalizable agentic data\ngeneration, offering a new paradigm for high-quality data construction in\ntool-augmented LLM scenarios.", "AI": {"tldr": "\u63d0\u51faToolACE-MT\u6846\u67b6\uff0c\u901a\u8fc7\u975e\u81ea\u56de\u5f52\u8fed\u4ee3\u751f\u6210\u9ad8\u8d28\u91cf\u591a\u8f6e\u673a\u5668\u4eba\u5bf9\u8bdd\u6570\u636e\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u81ea\u52a8\u56de\u5f52\u65b9\u6cd5\u7684\u9ad8\u6210\u672c\u95ee\u9898", "motivation": "\u73b0\u6709\u7684\u591a\u8f6e\u673a\u5668\u4eba\u4efb\u52a1\u6570\u636e\u751f\u6210\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u591a\u4e2aLLM\u673a\u5668\u4eba\u4e4b\u95f4\u8d39\u7528\u6602\u8d35\u7684\u81ea\u52a8\u56de\u5f52\u4ea4\u4e92\uff0c\u9650\u5236\u4e86\u673a\u5668\u4eba\u4efb\u52a1\u7684\u5b9e\u9645\u6027\u80fd", "method": "\u4e09\u9636\u6bb5\u6846\u67b6\uff1a\u7c97\u7c92\u5ea6\u521d\u59cb\u5316\u6784\u5efa\u5bf9\u8bdd\u9aa8\u67b6\u3001\u901a\u8fc7mask-and-fill\u64cd\u4f5c\u8fdb\u884c\u8fed\u4ee3\u7cbe\u7ec6\u5316\u3001\u79bb\u7ebf\u9a8c\u8bc1\u4fdd\u8bc1\u6b63\u786e\u6027\u548c\u4e00\u81f4\u6027", "result": "\u5b9e\u9a8c\u8bc1\u660eToolACE-MT\u80fd\u591f\u5b9e\u73b0\u9ad8\u6548\u3001\u9ad8\u6548\u80fd\u548c\u53ef\u901a\u7528\u7684\u673a\u5668\u4eba\u6570\u636e\u751f\u6210", "conclusion": "\u4e3a\u5de5\u5177\u589e\u5f3a\u578bLLM\u573a\u666f\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u6570\u636e\u6784\u5efa\u7684\u65b0\u8303\u5f0f"}}
{"id": "2508.12726", "pdf": "https://arxiv.org/pdf/2508.12726", "abs": "https://arxiv.org/abs/2508.12726", "authors": ["Weize Liu", "Yongchi Zhao", "Yijia Luo", "Mingyu Xu", "Jiaheng Liu", "Yanan Li", "Xiguo Hu", "Yuchi Xu", "Wenbo Su", "Bo Zheng"], "title": "DESIGNER: Design-Logic-Guided Multidisciplinary Data Synthesis for LLM Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have achieved remarkable success in many natural\nlanguage tasks but still struggle with complex, multi-step reasoning,\nparticularly across diverse disciplines. Existing reasoning datasets often\neither lack disciplinary breadth or the structural depth necessary to elicit\nrobust reasoning behaviors. We propose DESIGNER: a DESIGN-logic-guidEd\nReasoning data synthesis pipeline that leverages naturally available, extensive\nraw documents (book corpus and web corpus) to generate multidisciplinary\nchallenging questions. A core innovation of our approach is the introduction of\na Design Logic concept, which mimics the question-creation process of human\neducators. We use LLMs to reverse-engineer and abstract over 120,000 design\nlogics from existing questions across various disciplines. By matching these\ndesign logics with disciplinary source materials, we are able to create\nreasoning questions that far surpass the difficulty and diversity of existing\ndatasets. Based on this pipeline, we synthesized two large-scale reasoning\ndatasets that span 75 disciplines: Design-Logic-Reasoning-Book (DLR-Book),\ncontaining 3.04 million challenging questions synthesized from the book corpus,\nand Design-Logic-Reasoning-Web (DLR-Web), with 1.66 million challenging\nquestions from the web corpus. Our data analysis demonstrates that the\nquestions synthesized by our method exhibit substantially greater difficulty\nand diversity than those in the baseline datasets. We validate the\neffectiveness of these datasets by conducting SFT experiments on the\nQwen3-8B-Base and Qwen3-4B-Base models. The results show that our dataset\nsignificantly outperforms existing multidisciplinary datasets of the same\nvolume. Training with the full datasets further enables the models to surpass\nthe multidisciplinary reasoning performance of the official Qwen3-8B and\nQwen3-4B models.", "AI": {"tldr": "DESIGNER\u662f\u4e00\u4e2a\u57fa\u4e8e\u8bbe\u8ba1\u903b\u8f91\u7684\u591a\u5b66\u79d1\u63a8\u7406\u6570\u636e\u5408\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u4ece\u73b0\u6709\u95ee\u9898\u4e2d\u63d0\u53d612\u4e07+\u8bbe\u8ba1\u903b\u8f91\uff0c\u7ed3\u5408\u4e66\u7c4d\u548c\u7f51\u7edc\u8bed\u6599\u751f\u6210\u5927\u89c4\u6a21\u3001\u9ad8\u96be\u5ea6\u7684\u63a8\u7406\u95ee\u9898\u6570\u636e\u96c6\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u7684\u8de8\u5b66\u79d1\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u63a8\u7406\u6570\u636e\u96c6\u5728\u5b66\u79d1\u5e7f\u5ea6\u548c\u7ed3\u6784\u6df1\u5ea6\u4e0a\u4e0d\u8db3\uff0c\u96be\u4ee5\u6fc0\u53d1\u5f3a\u5927\u7684\u63a8\u7406\u80fd\u529b\u3002\u9700\u8981\u521b\u5efa\u66f4\u5177\u6311\u6218\u6027\u548c\u591a\u6837\u6027\u7684\u591a\u5b66\u79d1\u63a8\u7406\u6570\u636e\u96c6\u6765\u63d0\u5347LLM\u7684\u590d\u6742\u63a8\u7406\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u8bbe\u8ba1\u903b\u8f91\u6982\u5ff5\uff0c\u6a21\u4eff\u4eba\u7c7b\u6559\u80b2\u8005\u7684\u51fa\u9898\u8fc7\u7a0b\u3002\u4f7f\u7528LLM\u4ece\u73b0\u6709\u95ee\u9898\u4e2d\u9006\u5411\u5de5\u7a0b\u63d0\u53d612\u4e07+\u8bbe\u8ba1\u903b\u8f91\uff0c\u7ed3\u5408\u4e66\u7c4d\u548c\u7f51\u7edc\u8bed\u6599\u751f\u6210\u63a8\u7406\u95ee\u9898\uff0c\u6784\u5efa\u4e86\u4e24\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6DLR-Book(304\u4e07\u95ee\u9898)\u548cDLR-Web(166\u4e07\u95ee\u9898)\u3002", "result": "\u5408\u6210\u7684\u95ee\u9898\u5728\u96be\u5ea6\u548c\u591a\u6837\u6027\u4e0a\u8fdc\u8d85\u57fa\u7ebf\u6570\u636e\u96c6\u3002\u5728Qwen3\u6a21\u578b\u4e0a\u7684SFT\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u6570\u636e\u96c6\u663e\u8457\u4f18\u4e8e\u540c\u7b49\u89c4\u6a21\u7684\u591a\u5b66\u79d1\u6570\u636e\u96c6\uff0c\u5b8c\u6574\u8bad\u7ec3\u540e\u6a21\u578b\u63a8\u7406\u6027\u80fd\u751a\u81f3\u8d85\u8fc7\u5b98\u65b9Qwen3\u6a21\u578b\u3002", "conclusion": "DESIGNER\u6846\u67b6\u6210\u529f\u521b\u5efa\u4e86\u5927\u89c4\u6a21\u3001\u9ad8\u8d28\u91cf\u7684\u591a\u5b66\u79d1\u63a8\u7406\u6570\u636e\u96c6\uff0c\u6709\u6548\u63d0\u5347\u4e86LLM\u7684\u590d\u6742\u63a8\u7406\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u8bbe\u8ba1\u903b\u8f91\u5f15\u5bfc\u7684\u6570\u636e\u5408\u6210\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.12733", "pdf": "https://arxiv.org/pdf/2508.12733", "abs": "https://arxiv.org/abs/2508.12733", "authors": ["Zhiyuan Ning", "Tianle Gu", "Jiaxin Song", "Shixin Hong", "Lingyu Li", "Huacan Liu", "Jie Li", "Yixu Wang", "Meng Lingyu", "Yan Teng", "Yingchun Wang"], "title": "LinguaSafe: A Comprehensive Multilingual Safety Benchmark for Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "7pages, 5 figures", "summary": "The widespread adoption and increasing prominence of large language models\n(LLMs) in global technologies necessitate a rigorous focus on ensuring their\nsafety across a diverse range of linguistic and cultural contexts. The lack of\na comprehensive evaluation and diverse data in existing multilingual safety\nevaluations for LLMs limits their effectiveness, hindering the development of\nrobust multilingual safety alignment. To address this critical gap, we\nintroduce LinguaSafe, a comprehensive multilingual safety benchmark crafted\nwith meticulous attention to linguistic authenticity. The LinguaSafe dataset\ncomprises 45k entries in 12 languages, ranging from Hungarian to Malay. Curated\nusing a combination of translated, transcreated, and natively-sourced data, our\ndataset addresses the critical need for multilingual safety evaluations of\nLLMs, filling the void in the safety evaluation of LLMs across diverse\nunder-represented languages from Hungarian to Malay. LinguaSafe presents a\nmultidimensional and fine-grained evaluation framework, with direct and\nindirect safety assessments, including further evaluations for oversensitivity.\nThe results of safety and helpfulness evaluations vary significantly across\ndifferent domains and different languages, even in languages with similar\nresource levels. Our benchmark provides a comprehensive suite of metrics for\nin-depth safety evaluation, underscoring the critical importance of thoroughly\nassessing multilingual safety in LLMs to achieve more balanced safety\nalignment. Our dataset and code are released to the public to facilitate\nfurther research in the field of multilingual LLM safety.", "AI": {"tldr": "LinguaSafe\u662f\u4e00\u4e2a\u5305\u542b12\u79cd\u8bed\u8a00\u300145k\u6761\u76ee\u7684\u591a\u8bed\u8a00\u5b89\u5168\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u7ffb\u8bd1\u3001\u8f6c\u521b\u548c\u672c\u5730\u6570\u636e\u6784\u5efa\uff0c\u586b\u8865\u4e86LLM\u5728\u591a\u8bed\u8a00\u5b89\u5168\u8bc4\u4f30\u65b9\u9762\u7684\u7a7a\u767d\u3002", "motivation": "\u73b0\u6709LLM\u591a\u8bed\u8a00\u5b89\u5168\u8bc4\u4f30\u7f3a\u4e4f\u5168\u9762\u6027\u548c\u591a\u6837\u6027\u6570\u636e\uff0c\u9650\u5236\u4e86\u591a\u8bed\u8a00\u5b89\u5168\u5bf9\u9f50\u7684\u53d1\u5c55\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u591a\u8bed\u8a00\u5b89\u5168\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u7ed3\u5408\u7ffb\u8bd1\u3001\u8f6c\u521b\u548c\u672c\u5730\u6765\u6e90\u6570\u636e\u6784\u5efa\u5305\u542b12\u79cd\u8bed\u8a00\u768445k\u6761\u76ee\u6570\u636e\u96c6\uff0c\u91c7\u7528\u591a\u7ef4\u5ea6\u548c\u7ec6\u7c92\u5ea6\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u62ec\u76f4\u63a5/\u95f4\u63a5\u5b89\u5168\u8bc4\u4f30\u548c\u8fc7\u5ea6\u654f\u611f\u6027\u8bc4\u4f30\u3002", "result": "\u5b89\u5168\u548c\u6709\u7528\u6027\u8bc4\u4f30\u7ed3\u679c\u5728\u4e0d\u540c\u9886\u57df\u548c\u8bed\u8a00\u95f4\u5dee\u5f02\u663e\u8457\uff0c\u5373\u4f7f\u5728\u8d44\u6e90\u6c34\u5e73\u76f8\u4f3c\u7684\u8bed\u8a00\u4e2d\u4e5f\u5b58\u5728\u660e\u663e\u5dee\u5f02\u3002", "conclusion": "LinguaSafe\u57fa\u51c6\u6d4b\u8bd5\u5f3a\u8c03\u4e86\u5168\u9762\u8bc4\u4f30LLM\u591a\u8bed\u8a00\u5b89\u5168\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u5b9e\u73b0\u66f4\u5e73\u8861\u7684\u5b89\u5168\u5bf9\u9f50\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\uff0c\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5df2\u516c\u5f00\u4ee5\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2508.12769", "pdf": "https://arxiv.org/pdf/2508.12769", "abs": "https://arxiv.org/abs/2508.12769", "authors": ["Shaoming Duan", "Zirui Wang", "Chuanyi Liu", "Zhibin Zhu", "Yuhao Zhang", "Peiyi Han", "Liang Yan", "Zewu Penge"], "title": "CRED-SQL: Enhancing Real-world Large Scale Database Text-to-SQL Parsing through Cluster Retrieval and Execution Description", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in large language models (LLMs) have significantly improved\nthe accuracy of Text-to-SQL systems. However, a critical challenge remains: the\nsemantic mismatch between natural language questions (NLQs) and their\ncorresponding SQL queries. This issue is exacerbated in large-scale databases,\nwhere semantically similar attributes hinder schema linking and semantic drift\nduring SQL generation, ultimately reducing model accuracy. To address these\nchallenges, we introduce CRED-SQL, a framework designed for large-scale\ndatabases that integrates Cluster Retrieval and Execution Description. CRED-SQL\nfirst performs cluster-based large-scale schema retrieval to pinpoint the\ntables and columns most relevant to a given NLQ, alleviating schema mismatch.\nIt then introduces an intermediate natural language representation-Execution\nDescription Language (EDL)-to bridge the gap between NLQs and SQL. This\nreformulation decomposes the task into two stages: Text-to-EDL and EDL-to-SQL,\nleveraging LLMs' strong general reasoning capabilities while reducing semantic\ndeviation. Extensive experiments on two large-scale, cross-domain\nbenchmarks-SpiderUnion and BirdUnion-demonstrate that CRED-SQL achieves new\nstate-of-the-art (SOTA) performance, validating its effectiveness and\nscalability. Our code is available at https://github.com/smduan/CRED-SQL.git", "AI": {"tldr": "CRED-SQL\u662f\u4e00\u4e2a\u9488\u5bf9\u5927\u89c4\u6a21\u6570\u636e\u5e93\u7684Text-to-SQL\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u7fa4\u68c0\u7d22\u548c\u6267\u884c\u63cf\u8ff0\u8bed\u8a00\u89e3\u51b3\u8bed\u4e49\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5728\u4e24\u4e2a\u5927\u578b\u8de8\u57df\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u65b0\u7684SOTA\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u7136\u63d0\u5347\u4e86Text-to-SQL\u7cfb\u7edf\u7684\u51c6\u786e\u6027\uff0c\u4f46\u5728\u5927\u89c4\u6a21\u6570\u636e\u5e93\u4e2d\uff0c\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u4e0eSQL\u67e5\u8be2\u4e4b\u95f4\u7684\u8bed\u4e49\u4e0d\u5339\u914d\u95ee\u9898\u4ecd\u7136\u4e25\u91cd\uff0c\u7279\u522b\u662f\u5728\u8bed\u4e49\u76f8\u4f3c\u7684\u5c5e\u6027\u5bfc\u81f4\u6a21\u5f0f\u94fe\u63a5\u56f0\u96be\u548c\u8bed\u4e49\u6f02\u79fb\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u63d0\u51faCRED-SQL\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1\uff09\u57fa\u4e8e\u96c6\u7fa4\u7684\u5927\u89c4\u6a21\u6a21\u5f0f\u68c0\u7d22\uff0c\u7cbe\u786e\u5b9a\u4f4d\u4e0eNLQ\u6700\u76f8\u5173\u7684\u8868\u548c\u5217\uff1b2\uff09\u5f15\u5165\u4e2d\u95f4\u81ea\u7136\u8bed\u8a00\u8868\u793a-\u6267\u884c\u63cf\u8ff0\u8bed\u8a00(EDL)\uff0c\u5c06\u4efb\u52a1\u5206\u89e3\u4e3aText-to-EDL\u548cEDL-to-SQL\u4e24\u4e2a\u9636\u6bb5\uff0c\u5229\u7528LLM\u7684\u5f3a\u5927\u63a8\u7406\u80fd\u529b\u540c\u65f6\u51cf\u5c11\u8bed\u4e49\u504f\u5dee\u3002", "result": "\u5728\u4e24\u4e2a\u5927\u89c4\u6a21\u8de8\u57df\u57fa\u51c6\u6d4b\u8bd5SpiderUnion\u548cBirdUnion\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cCRED-SQL\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u5148\u8fdb(SOTA)\u6027\u80fd\u3002", "conclusion": "CRED-SQL\u901a\u8fc7\u521b\u65b0\u7684\u96c6\u7fa4\u68c0\u7d22\u548cEDL\u4e2d\u95f4\u8868\u793a\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u6570\u636e\u5e93\u4e2d\u7684\u8bed\u4e49\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2508.12774", "pdf": "https://arxiv.org/pdf/2508.12774", "abs": "https://arxiv.org/abs/2508.12774", "authors": ["Javier Garcia Gilabert", "Xixian Liao", "Severino Da Dalt", "Ella Bohman", "Audrey Mash", "Francesca De Luca Fornaciari", "Irene Baucells", "Joan Llop", "Miguel Claramunt Argote", "Carlos Escolano", "Maite Melero"], "title": "From SALAMANDRA to SALAMANDRATA: BSC Submission for WMT25 General Machine Translation Shared Task", "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we present the SALAMANDRATA family of models, an improved\niteration of SALAMANDRA LLMs (Gonzalez-Agirre et al., 2025) specifically\ntrained to achieve strong performance in translation-related tasks for 38\nEuropean languages. SALAMANDRATA comes in two scales: 2B and 7B parameters. For\nboth versions, we applied the same training recipe with a first step of\ncontinual pre-training on parallel data, and a second step of supervised\nfine-tuning on high-quality instructions. The BSC submission to the WMT25\nGeneral Machine Translation shared task is based on the 7B variant of\nSALAMANDRATA. We first adapted the model vocabulary to support the additional\nnon-European languages included in the task. This was followed by a second\nphase of continual pre-training and supervised fine-tuning, carefully designed\nto optimize performance across all translation directions for this year's\nshared task. For decoding, we employed two quality-aware strategies: Minimum\nBayes Risk Decoding and Tuned Re-ranking using COMET and COMET-KIWI\nrespectively. We publicly release both the 2B and 7B versions of SALAMANDRATA,\nalong with the newer SALAMANDRATA-V2 model, on Hugging Face1", "AI": {"tldr": "SALAMANDRATA\u6a21\u578b\u5bb6\u65cf\u662fSALAMANDRA LLMs\u7684\u6539\u8fdb\u7248\u672c\uff0c\u4e13\u95e8\u9488\u5bf938\u79cd\u6b27\u6d32\u8bed\u8a00\u7684\u7ffb\u8bd1\u4efb\u52a1\u8fdb\u884c\u8bad\u7ec3\uff0c\u63d0\u4f9b2B\u548c7B\u4e24\u4e2a\u89c4\u6a21\u7248\u672c\uff0c\u91c7\u7528\u6301\u7eed\u9884\u8bad\u7ec3\u548c\u76d1\u7763\u5fae\u8c03\u7684\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5e76\u5728WMT25\u673a\u5668\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u5e94\u7528\u4e86\u8d28\u91cf\u611f\u77e5\u7684\u89e3\u7801\u7b56\u7565\u3002", "motivation": "\u5f00\u53d1\u4e13\u95e8\u9488\u5bf9\u591a\u8bed\u8a00\u7ffb\u8bd1\u4efb\u52a1\u7684\u9ad8\u6027\u80fd\u6a21\u578b\uff0c\u7279\u522b\u662f\u9488\u5bf9\u6b27\u6d32\u8bed\u8a00\u7684\u7ffb\u8bd1\u9700\u6c42\uff0c\u63d0\u5347\u673a\u5668\u7ffb\u8bd1\u7684\u8d28\u91cf\u548c\u6548\u7387\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\uff1a\u9996\u5148\u5728\u5e73\u884c\u6570\u636e\u4e0a\u8fdb\u884c\u6301\u7eed\u9884\u8bad\u7ec3\uff0c\u7136\u540e\u5728\u9ad8\u8d28\u91cf\u6307\u4ee4\u4e0a\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\u3002\u9488\u5bf9WMT25\u4efb\u52a1\uff0c\u8fd8\u8fdb\u884c\u4e86\u8bcd\u6c47\u8868\u6269\u5c55\u548c\u989d\u5916\u7684\u8bad\u7ec3\u9636\u6bb5\u3002\u89e3\u7801\u65f6\u4f7f\u7528\u6700\u5c0f\u8d1d\u53f6\u65af\u98ce\u9669\u89e3\u7801\u548c\u57fa\u4e8eCOMET\u7684\u8c03\u4f18\u91cd\u6392\u5e8f\u7b56\u7565\u3002", "result": "\u5f00\u53d1\u4e86SALAMANDRATA\u6a21\u578b\u5bb6\u65cf\u76842B\u548c7B\u7248\u672c\uff0c\u4ee5\u53ca\u66f4\u65b0\u7684SALAMANDRATA-V2\u6a21\u578b\uff0c\u8fd9\u4e9b\u6a21\u578b\u5df2\u5728Hugging Face\u5e73\u53f0\u4e0a\u516c\u5f00\u53d1\u5e03\u3002", "conclusion": "SALAMANDRATA\u6a21\u578b\u5bb6\u65cf\u901a\u8fc7\u4e13\u95e8\u7684\u8bad\u7ec3\u65b9\u6cd5\u548c\u8d28\u91cf\u611f\u77e5\u89e3\u7801\u7b56\u7565\uff0c\u4e3a\u591a\u8bed\u8a00\u673a\u5668\u7ffb\u8bd1\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u662f\u5728\u6b27\u6d32\u8bed\u8a00\u7ffb\u8bd1\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2508.12778", "pdf": "https://arxiv.org/pdf/2508.12778", "abs": "https://arxiv.org/abs/2508.12778", "authors": ["Zhe Chen", "Yusheng Liao", "Shuyang Jiang", "Zhiyuan Zhu", "Haolin Li", "Yanfeng Wang", "Yu Wang"], "title": "HeteroRAG: A Heterogeneous Retrieval-Augmented Generation Framework for Medical Vision Language Tasks", "categories": ["cs.CL"], "comment": null, "summary": "Medical large vision-language Models (Med-LVLMs) have shown promise in\nclinical applications but suffer from factual inaccuracies and unreliable\noutputs, posing risks in real-world diagnostics. While retrieval-augmented\ngeneration has emerged as a potential solution, current medical multimodal RAG\nsystems are unable to perform effective retrieval across heterogeneous sources.\nThe irrelevance of retrieved reports affects the factuality of analysis, while\ninsufficient knowledge affects the credibility of clinical decision-making. To\nbridge the gap, we construct MedAtlas, which includes extensive multimodal\nreport repositories and diverse text corpora. Based on it, we present\nHeteroRAG, a novel framework that enhances Med-LVLMs through heterogeneous\nknowledge sources. The framework introduces Modality-specific CLIPs for\neffective report retrieval and a Multi-corpora Query Generator for dynamically\nconstructing queries for diverse corpora. Incorporating knowledge from such\nmultifaceted sources, Med-LVLM is then trained with Heterogeneous Knowledge\nPreference Tuning to achieve cross-modality and multi-source knowledge\nalignment. Extensive experiments across 12 datasets and 3 modalities\ndemonstrate that the proposed HeteroRAG achieves state-of-the-art performance\nin most medical vision language benchmarks, significantly improving factual\naccuracy and reliability of Med-LVLMs.", "AI": {"tldr": "\u533b\u7597\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u4e8b\u5b9e\u6027\u95ee\u9898\uff0c\u8fd9\u7bc7\u6587\u7ae0\u63d0\u51faHeteroRAG\u6846\u67b6\uff0c\u901a\u8fc7\u5f02\u6784\u77e5\u8bc6\u6e90\u589e\u5f3a\u6765\u63d0\u9ad8\u8bca\u65ad\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027", "motivation": "\u533b\u7597\u5927\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b(Med-LVLMs)\u5728\u4e34\u5e8a\u5e94\u7528\u4e2d\u663e\u793a\u6f5c\u529b\uff0c\u4f46\u5b58\u5728\u4e8b\u5b9e\u6027\u4e0d\u51c6\u786e\u548c\u4e0d\u53ef\u9760\u8f93\u51fa\u7684\u95ee\u9898\uff0c\u5728\u771f\u5b9e\u8bca\u65ad\u4e2d\u5e26\u6765\u98ce\u9669\u3002\u5f53\u524d\u7684\u591a\u6a21\u6001RAG\u7cfb\u7edf\u65e0\u6cd5\u5728\u5f02\u6784\u6765\u6e90\u4e2d\u8fdb\u884c\u6709\u6548\u68c0\u7d22", "method": "\u6784\u5efaMedAtlas\u591a\u6a21\u6001\u62a5\u544a\u4ed3\u5e93\uff0c\u63d0\u51faHeteroRAG\u6846\u67b6\uff0c\u5305\u62ec\u6a21\u6001\u7279\u5f02\u6027CLIPs\u7528\u4e8e\u62a5\u544a\u68c0\u7d22\uff0c\u591a\u8bed\u6599\u5e93\u67e5\u8be2\u751f\u6210\u5668\u52a8\u6001\u6784\u5efa\u67e5\u8be2\uff0c\u4ee5\u53ca\u5f02\u6784\u77e5\u8bc6\u504f\u597d\u5faa\u73af\u8bad\u7ec3\u65b9\u6cd5", "result": "\u572812\u4e2a\u6570\u636e\u96c6\u548c3\u79cd\u6a21\u6001\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0cHeteroRAG\u5728\u5927\u591a\u6570\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u663e\u8457\u63d0\u9ad8\u4e86Med-LVLMs\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027", "conclusion": "HeteroRAG\u6846\u67b6\u901a\u8fc7\u5f02\u6784\u77e5\u8bc6\u6e90\u7684\u6709\u6548\u6574\u5408\uff0c\u6210\u529f\u63d0\u5347\u4e86\u533b\u7597\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u7684\u8bca\u65ad\u6027\u80fd\uff0c\u4e3a\u4e34\u5e8a\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.12800", "pdf": "https://arxiv.org/pdf/2508.12800", "abs": "https://arxiv.org/abs/2508.12800", "authors": ["Yong Deng", "Guoqing Wang", "Zhenzhe Ying", "Xiaofeng Wu", "Jinzhen Lin", "Wenwen Xiong", "Yuqin Dai", "Shuo Yang", "Zhanwei Zhang", "Qiwen Wang", "Yang Qin", "Changhua Meng"], "title": "Atom-Searcher: Enhancing Agentic Deep Research via Fine-Grained Atomic Thought Reward", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) exhibit remarkable problem-solving abilities,\nbut struggle with complex tasks due to static internal knowledge.\nRetrieval-Augmented Generation (RAG) enhances access to external information,\nyet remains limited in multi-hop reasoning and strategic search due to rigid\nworkflows. Recent advancements in agentic deep research empower LLMs to\nautonomously reason, search, and synthesize information. However, current\napproaches relying on outcome-based reinforcement learning (RL) face critical\nissues such as conflicting gradients and reward sparsity, limiting performance\ngains and training efficiency. To address these, we first propose Atomic\nThought, a novel LLM thinking paradigm that decomposes reasoning into\nfine-grained functional units. These units are supervised by Reasoning Reward\nModels (RRMs), which provide Atomic Thought Rewards (ATR) for fine-grained\nguidance. Building on this, we propose Atom-Searcher, a novel RL framework for\nagentic deep research that integrates Atomic Thought and ATR. Atom-Searcher\nuses a curriculum-inspired reward schedule, prioritizing process-level ATR\nearly and transitioning to outcome rewards, accelerating convergence on\neffective reasoning paths. Experiments on seven benchmarks show consistent\nimprovements over the state-of-the-art. Key advantages include: (1)\nAtom-Searcher scales computation at test-time. (2) Atomic Thought provides\nsupervision anchors for RRMs, bridging deep research tasks and RRMs. (3)\nAtom-Searcher exhibits more interpretable, human-like reasoning patterns.", "AI": {"tldr": "\u63d0\u51fa\u4e86Atomic Thought\u601d\u7ef4\u8303\u5f0f\u548cAtom-Searcher\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u63a8\u7406\u5355\u5143\u548c\u8fc7\u7a0b\u5956\u52b1\u89e3\u51b3\u4f20\u7edfRAG\u548c\u57fa\u4e8e\u7ed3\u679c\u7684RL\u5728\u6df1\u5ea6\u7814\u7a76\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u9759\u6001\u77e5\u8bc6\u9650\u5236\u3001RAG\u5728\u591a\u5c42\u6b21\u63a8\u7406\u548c\u7b56\u7565\u641c\u7d22\u4e2d\u7684\u521a\u6027\u5de5\u4f5c\u6d41\u95ee\u9898\uff0c\u4ee5\u53ca\u57fa\u4e8e\u7ed3\u679c\u7684\u5f3a\u5316\u5b66\u4e60\u9762\u4e34\u7684\u68af\u5ea6\u51b2\u7a81\u548c\u5956\u52b1\u7a00\u758f\u6027\u95ee\u9898", "method": "1. Atomic Thought\uff1a\u5c06\u63a8\u7406\u5206\u89e3\u4e3a\u7ec6\u7c92\u5ea6\u529f\u80fd\u5355\u5143\uff0c\u7531\u63a8\u7406\u5956\u52b1\u6a21\u578b\u63d0\u4f9b\u539f\u5b50\u601d\u7ef4\u5956\u52b1 2. Atom-Searcher\uff1a\u96c6\u6210Atomic Thought\u548cATR\u7684RL\u6846\u67b6\uff0c\u91c7\u7528\u8bfe\u7a0b\u5f0f\u5956\u52b1\u8c03\u5ea6\uff0c\u65e9\u671f\u4f18\u5148\u8fc7\u7a0b\u7ea7\u5956\u52b1\uff0c\u540e\u671f\u8f6c\u5411\u7ed3\u679c\u5956\u52b1", "result": "\u5728\u4e03\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u7684\u6301\u7eed\u6539\u8fdb\uff0c\u5177\u6709\u53ef\u6269\u5c55\u8ba1\u7b97\u3001\u66f4\u597d\u7684\u53ef\u89e3\u91ca\u6027\u548c\u66f4\u63a5\u8fd1\u4eba\u7c7b\u63a8\u7406\u6a21\u5f0f\u7b49\u4f18\u52bf", "conclusion": "Atomic Thought\u548cAtom-Searcher\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u6df1\u5ea6\u7814\u7a76\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u548c\u5956\u52b1\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.12803", "pdf": "https://arxiv.org/pdf/2508.12803", "abs": "https://arxiv.org/abs/2508.12803", "authors": ["Ahmed Elshabrawy", "Hour Kaing", "Haiyue Song", "Alham Fikri Aji", "Hideki Tanaka", "Masao Utiyama", "Raj Dabre"], "title": "When Alignment Hurts: Decoupling Representational Spaces in Multilingual Models", "categories": ["cs.CL"], "comment": null, "summary": "Alignment with high-resource standard languages is often assumed to aid the\nmodeling of related low-resource varieties. We challenge this assumption by\ndemonstrating that excessive representational entanglement with a dominant\nvariety, such as Modern Standard Arabic (MSA) in relation to Arabic dialects,\ncan actively hinder generative modeling. We present the first comprehensive\ncausal study of this phenomenon by analyzing and directly intervening in the\ninternal representation geometry of large language models (LLMs). Our key\ncontribution is an online variational probing framework that continuously\nestimates the subspace of the standard variety during fine-tuning, enabling\nprojection-based decoupling from this space. While our study uses Arabic as a\ncase due to its unusually rich parallel resources across 25 dialects, the\nbroader motivation is methodological: dialectal MT serves as a controlled proxy\nfor generative tasks where comparable multi-variety corpora are unavailable.\nAcross 25 dialects, our intervention improves generation quality by up to +4.9\nchrF++ and +2.0 on average compared to standard fine-tuning, despite a measured\ntradeoff in standard-language performance. These results provide causal\nevidence that subspace dominance by high-resource varieties can restrict\ngenerative capacity for related varieties. More generally, we unify geometric\nand information-theoretic probing with subspace-level causal interventions,\noffering practical tools for improving generative modeling in closely related\nlanguage families and, more broadly, for controlling representational\nallocation in multilingual and multi-domain LLMs. Code will be released.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u901a\u8fc7\u5728\u963f\u62c9\u4f2f\u8bed\u8a00\u5bb6\u65cf\u7684\u7814\u7a76\uff0c\u8bc1\u660e\u4e86\u9ad8\u8d44\u6e90\u6807\u51c6\u8bed\u8a00\u7684\u8868\u5f81\u7a7a\u95f4\u5360\u636e\u4f1a\u5bf9\u76f8\u5173\u4f4e\u8d44\u6e90\u65b9\u8a00\u7684\u751f\u6210\u6a21\u578b\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u5728\u7ebf\u53d8\u5206\u63a2\u9488\u6846\u67b6\u6765\u89e3\u8026\u8fd9\u79cd\u8868\u5f81\u7f18\u7ed5\u3002", "motivation": "\u6316\u6398\u9ad8\u8d44\u6e90\u6807\u51c6\u8bed\u8a00\u4e0e\u76f8\u5173\u4f4e\u8d44\u6e90\u65b9\u8a00\u4e4b\u95f4\u8868\u5f81\u5173\u7cfb\u7684\u6d1e\u5bdf\uff0c\u6316\u6398\u8fc7\u5ea6\u8868\u5f81\u7f18\u7ed5\u5bf9\u751f\u6210\u6a21\u578b\u7684\u5f71\u54cd\uff0c\u4ee5\u53ca\u63d0\u4f9b\u65b9\u6cd5\u8bba\u4e0a\u7684\u63a2\u7d22\uff0c\u4e3a\u591a\u8bed\u8a00\u591a\u57df\u6a21\u578b\u7684\u8868\u5f81\u5206\u914d\u63a7\u5236\u63d0\u4f9b\u5b9e\u7528\u5de5\u5177\u3002", "method": "\u4f7f\u7528\u5728\u7ebf\u53d8\u5206\u63a2\u9488\u6846\u67b6\u6301\u7eed\u4f30\u8ba1\u6807\u51c6\u8bed\u8a00\u5b50\u7a7a\u95f4\uff0c\u901a\u8fc7\u6295\u5f71\u57fa\u7840\u7684\u89e3\u8026\u6280\u672f\u5b9e\u73b0\u4e0e\u6807\u51c6\u8bed\u8a00\u8868\u5f81\u7a7a\u95f4\u7684\u5206\u79bb\uff0c\u5e76\u5728\u963f\u62c9\u4f2f\u8bed25\u79cd\u65b9\u8a00\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u572825\u79cd\u65b9\u8a00\u4e0a\uff0c\u5e72\u9884\u63a7\u5236\u63aa\u65bd\u4f7f\u751f\u6210\u8d28\u91cf\u63d0\u5347\u6700\u9ad8+4.9 chrF++\uff0c\u5e73\u5747+2.0\uff0c\u867d\u7136\u5bf9\u6807\u51c6\u8bed\u8a00\u6027\u80fd\u6709\u6240\u4ea4\u6362\u3002", "conclusion": "\u9ad8\u8d44\u6e90\u8bed\u8a00\u7684\u5b50\u7a7a\u95f4\u5360\u636e\u4f1a\u9650\u5236\u76f8\u5173\u8bed\u8a00\u7684\u751f\u6210\u80fd\u529b\uff0c\u7ed9\u51fa\u4e86\u56e0\u679c\u6027\u8bc1\u636e\uff0c\u5e76\u7edf\u4e00\u4e86\u51e0\u4f55\u4e0e\u4fe1\u606f\u8bba\u63a2\u9488\u6280\u672f\uff0c\u4e3a\u63a7\u5236\u591a\u8bed\u8a00\u591a\u57dfLLM\u7684\u8868\u5f81\u5206\u914d\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2508.12819", "pdf": "https://arxiv.org/pdf/2508.12819", "abs": "https://arxiv.org/abs/2508.12819", "authors": ["Jeongwoo Kang", "Maria Boritchev", "Maximin Coavoux"], "title": "ding-01 :ARG0: An AMR Corpus for Spontaneous French Dialogue", "categories": ["cs.CL"], "comment": "Accepted at IWCS 2025", "summary": "We present our work to build a French semantic corpus by annotating French\ndialogue in Abstract Meaning Representation (AMR). Specifically, we annotate\nthe DinG corpus, consisting of transcripts of spontaneous French dialogues\nrecorded during the board game Catan. As AMR has insufficient coverage of the\ndynamics of spontaneous speech, we extend the framework to better represent\nspontaneous speech and sentence structures specific to French. Additionally, to\nsupport consistent annotation, we provide an annotation guideline detailing\nthese extensions. We publish our corpus under a free license (CC-SA-BY). We\nalso train and evaluate an AMR parser on our data. This model can be used as an\nassistance annotation tool to provide initial annotations that can be refined\nby human annotators. Our work contributes to the development of semantic\nresources for French dialogue.", "AI": {"tldr": "\u6784\u5efa\u6cd5\u8bed\u5bf9\u8bdd\u8bed\u4e49\u8bed\u6599\u5e93\uff0c\u6269\u5c55AMR\u6846\u67b6\u9002\u914d\u81ea\u53d1\u8bed\u8a00\u7279\u5f81\uff0c\u5e76\u8bad\u7ec3AMR\u89e3\u6790\u5668\u4f5c\u4e3a\u6ce8\u89e3\u5de5\u5177", "motivation": "\u4e3a\u6cd5\u8bed\u5bf9\u8bdd\u53d1\u5c55\u8bed\u4e49\u8d44\u6e90\uff0c\u89e3\u51b3AMR\u5728\u81ea\u53d1\u8bed\u8a00\u52a8\u6001\u7279\u5f81\u8986\u76d6\u4e0d\u8db3\u7684\u95ee\u9898", "method": "\u6ce8\u89e3\u6cd5\u8bedCatan\u6846\u67b6\u6e38\u620f\u5bf9\u8bdd\u8bed\u6599\uff0c\u6269\u5c55AMR\u6846\u67b6\u9002\u914d\u81ea\u53d1\u8bed\u8a00\u548c\u6cd5\u8bed\u7279\u6709\u53e5\u6cd5\u7ed3\u6784\uff0c\u5236\u5b9a\u6ce8\u89e3\u6307\u5357", "result": "\u53d1\u5e03\u514d\u8d39CC-SA-BY\u8bb8\u53ef\u7684\u8bed\u6599\u5e93\uff0c\u8bad\u7ec3\u7684AMR\u89e3\u6790\u5668\u53ef\u4f5c\u4e3a\u534f\u52a9\u6ce8\u89e3\u5de5\u5177", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u6cd5\u8bed\u5bf9\u8bdd\u8bed\u4e49\u8d44\u6e90\u5f00\u53d1\u505a\u51fa\u8d21\u732e\uff0c\u63d0\u4f9b\u4e86\u6269\u5c55\u7684AMR\u6846\u67b6\u548c\u5b9e\u7528\u7684\u6ce8\u89e3\u5de5\u5177"}}
{"id": "2508.12828", "pdf": "https://arxiv.org/pdf/2508.12828", "abs": "https://arxiv.org/abs/2508.12828", "authors": ["Raneem Alharthi", "Rajwa Alharthi", "Aiqi Jiang", "Arkaitz Zubiaga"], "title": "Context Matters: Incorporating Target Awareness in Conversational Abusive Language Detection", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Abusive language detection has become an increasingly important task as a\nmeans to tackle this type of harmful content in social media. There has been a\nsubstantial body of research developing models for determining if a social\nmedia post is abusive or not; however, this research has primarily focused on\nexploiting social media posts individually, overlooking additional context that\ncan be derived from surrounding posts. In this study, we look at conversational\nexchanges, where a user replies to an earlier post by another user (the parent\ntweet). We ask: does leveraging context from the parent tweet help determine if\na reply post is abusive or not, and what are the features that contribute the\nmost? We study a range of content-based and account-based features derived from\nthe context, and compare this to the more widely studied approach of only\nlooking at the features from the reply tweet. For a more generalizable study,\nwe test four different classification models on a dataset made of\nconversational exchanges (parent-reply tweet pairs) with replies labeled as\nabusive or not. Our experiments show that incorporating contextual features\nleads to substantial improvements compared to the use of features derived from\nthe reply tweet only, confirming the importance of leveraging context. We\nobserve that, among the features under study, it is especially the\ncontent-based features (what is being posted) that contribute to the\nclassification performance rather than account-based features (who is posting\nit). While using content-based features, it is best to combine a range of\ndifferent features to ensure improved performance over being more selective and\nusing fewer features. Our study provides insights into the development of\ncontextualized abusive language detection models in realistic settings\ninvolving conversations.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u793e\u4ea4\u5a92\u4f53\u6ee5\u7528\u8bed\u8a00\u68c0\u6d4b\u4e2d\u5229\u7528\u4e0a\u4e0b\u6587\u4fe1\u606f\uff08\u7236\u63a8\u6587\uff09\u7684\u91cd\u8981\u6027\uff0c\u53d1\u73b0\u7ed3\u5408\u4e0a\u4e0b\u6587\u7279\u5f81\u80fd\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\uff0c\u5176\u4e2d\u57fa\u4e8e\u5185\u5bb9\u7684\u7279\u5f81\u6bd4\u57fa\u4e8e\u8d26\u6237\u7684\u7279\u5f81\u8d21\u732e\u66f4\u5927\u3002", "motivation": "\u73b0\u6709\u7684\u6ee5\u7528\u8bed\u8a00\u68c0\u6d4b\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5355\u4e2a\u793e\u4ea4\u5a92\u4f53\u5e16\u5b50\uff0c\u5ffd\u7565\u4e86\u53ef\u4ee5\u4ece\u5468\u56f4\u5e16\u5b50\u4e2d\u83b7\u5f97\u7684\u989d\u5916\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5229\u7528\u7236\u63a8\u6587\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u662f\u5426\u6709\u52a9\u4e8e\u5224\u65ad\u56de\u590d\u5e16\u5b50\u662f\u5426\u4e3a\u6ee5\u7528\u5185\u5bb9\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u4e86\u5305\u542b\u5bf9\u8bdd\u4ea4\u6362\uff08\u7236\u63a8\u6587-\u56de\u590d\u63a8\u6587\u5bf9\uff09\u7684\u6570\u636e\u96c6\uff0c\u6d4b\u8bd5\u4e86\u56db\u79cd\u4e0d\u540c\u7684\u5206\u7c7b\u6a21\u578b\uff0c\u6bd4\u8f83\u4e86\u4ec5\u4f7f\u7528\u56de\u590d\u63a8\u6587\u7279\u5f81\u4e0e\u7ed3\u5408\u4e0a\u4e0b\u6587\u7279\u5f81\u7684\u6548\u679c\uff0c\u5206\u6790\u4e86\u57fa\u4e8e\u5185\u5bb9\u548c\u57fa\u4e8e\u8d26\u6237\u7684\u7279\u5f81\u8d21\u732e\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7ed3\u5408\u4e0a\u4e0b\u6587\u7279\u5f81\u76f8\u6bd4\u4ec5\u4f7f\u7528\u56de\u590d\u63a8\u6587\u7279\u5f81\u6709\u663e\u8457\u6539\u8fdb\u3002\u57fa\u4e8e\u5185\u5bb9\u7684\u7279\u5f81\u5bf9\u5206\u7c7b\u6027\u80fd\u8d21\u732e\u6700\u5927\uff0c\u800c\u57fa\u4e8e\u8d26\u6237\u7684\u7279\u5f81\u8d21\u732e\u8f83\u5c0f\u3002\u4f7f\u7528\u591a\u79cd\u5185\u5bb9\u7279\u5f81\u7ec4\u5408\u6548\u679c\u6700\u4f73\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u4fe1\u606f\u5728\u6ee5\u7528\u8bed\u8a00\u68c0\u6d4b\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u7279\u522b\u662f\u57fa\u4e8e\u5185\u5bb9\u7684\u7279\u5f81\u3002\u7814\u7a76\u7ed3\u679c\u4e3a\u5728\u6d89\u53ca\u5bf9\u8bdd\u7684\u73b0\u5b9e\u573a\u666f\u4e2d\u5f00\u53d1\u60c5\u5883\u5316\u6ee5\u7528\u8bed\u8a00\u68c0\u6d4b\u6a21\u578b\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2508.12830", "pdf": "https://arxiv.org/pdf/2508.12830", "abs": "https://arxiv.org/abs/2508.12830", "authors": ["Jan Maliszewski"], "title": "It takes a village to write a book: Mapping anonymous contributions in Stephen Langton's Quaestiones Theologiae", "categories": ["cs.CL"], "comment": null, "summary": "While the indirect evidence suggests that already in the early scholastic\nperiod the literary production based on records of oral teaching (so-called\nreportationes) was not uncommon, there are very few sources commenting on the\npractice. This paper details the design of a study applying stylometric\ntechniques of authorship attribution to a collection developed from\nreportationes -- Stephen Langton's Quaestiones Theologiae -- aiming to uncover\nlayers of editorial work and thus validate some hypotheses regarding the\ncollection's formation. Following Camps, Cl\\'erice, and Pinche (2021), I\ndiscuss the implementation of an HTR pipeline and stylometric analysis based on\nthe most frequent words, POS tags, and pseudo-affixes. The proposed study will\noffer two methodological gains relevant to computational research on the\nscholastic tradition: it will directly compare performance on manually composed\nand automatically extracted data, and it will test the validity of\ntransformer-based OCR and automated transcription alignment for workflows\napplied to scholastic Latin corpora. If successful, this study will provide an\neasily reusable template for the exploratory analysis of collaborative literary\nproduction stemming from medieval universities.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u91c7\u7528\u98ce\u683c\u8ba1\u91cf\u6280\u672f\u5206\u6790\u4e2d\u4e16\u7eaa\u6559\u80b2\u53e3\u8bb2\u8bb0\u5f55\u6587\u672c\u7684\u65b9\u6cd5\uff0c\u4ee5\u9a8c\u8bc1\u5173\u4e8eStephen Langton\u795e\u5b66\u95ee\u9898\u96c6\u7f16\u8bd9\u8fc7\u7a0b\u7684\u5047\u8bbe\u3002", "motivation": "\u867d\u7136\u95f4\u63a5\u8bc1\u636e\u663e\u793a\u65e9\u671f\u7ecf\u9662\u54f2\u5b66\u65f6\u4ee3\u5c31\u5b58\u5728\u57fa\u4e8e\u53e3\u8bb2\u6559\u5b66\u8bb0\u5f55\u7684\u6587\u5b66\u521b\u4f5c\uff0c\u4f46\u5f88\u5c11\u6709\u6e90\u6cc4\u8fd9\u79cd\u5b9e\u8df5\u3002\u8bba\u6587\u5f3a\u8c03\u9700\u8981\u9a8c\u8bc1\u5173\u4e8e\u8fd9\u4e9b\u6587\u672c\u96c6\u7f16\u8bd9\u8fc7\u7a0b\u7684\u5047\u8bbe\u3002", "method": "\u91c7\u7528\u98ce\u683c\u8ba1\u91cf\u4f5c\u8005\u5f52\u5c5e\u6280\u672f\uff0c\u57fa\u4e8e\u6700\u5e38\u7528\u8bcd\u3001\u8bcd\u6027\u6807\u6ce8\u548c\u4f2a\u540e\u7f00\u8fdb\u884c\u5206\u6790\u3002\u5b9e\u65bdHTR\uff08\u624b\u5199\u6587\u672c\u8bc6\u522b\uff09\u6d41\u7a0b\u548c\u98ce\u683c\u8ba1\u91cf\u5206\u6790\u3002", "result": "\u8fd9\u9879\u7814\u7a76\u5c06\u63d0\u4f9b\u4e24\u4e2a\u65b9\u6cd5\u8bba\u653f\u76ca\uff1a\u76f4\u63a5\u6bd4\u8f83\u624b\u5de5\u7f16\u5199\u548c\u81ea\u52a8\u63d0\u53d6\u6570\u636e\u7684\u6027\u80fd\uff0c\u6d4b\u8bd5\u57fa\u4e8etransformer\u7684OCR\u548c\u81ea\u52a8\u8f6c\u5f55\u5bf9\u9f50\u5728\u7ecf\u9662\u62d3\u7247\u62d3\u5e55\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5982\u679c\u6210\u529f\uff0c\u8fd9\u9879\u7814\u7a76\u5c06\u4e3a\u4e2d\u4e16\u7eaa\u5927\u5b66\u534f\u4f5c\u6587\u5b66\u521b\u4f5c\u7684\u63a2\u7d22\u6027\u5206\u6790\u63d0\u4f9b\u4e00\u4e2a\u5bb9\u6613\u91cd\u7528\u7684\u6a21\u677f\uff0c\u5e76\u63a8\u8fdb\u8ba1\u7b97\u6559\u80b2\u5b66\u4f20\u7edf\u7814\u7a76\u3002"}}
{"id": "2508.12863", "pdf": "https://arxiv.org/pdf/2508.12863", "abs": "https://arxiv.org/abs/2508.12863", "authors": ["Jumbly Grindrod", "Peter Grindrod"], "title": "Word Meanings in Transformer Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We investigate how word meanings are represented in the transformer language\nmodels. Specifically, we focus on whether transformer models employ something\nanalogous to a lexical store - where each word has an entry that contains\nsemantic information. To do this, we extracted the token embedding space of\nRoBERTa-base and k-means clustered it into 200 clusters. In our first study, we\nthen manually inspected the resultant clusters to consider whether they are\nsensitive to semantic information. In our second study, we tested whether the\nclusters are sensitive to five psycholinguistic measures: valence,\nconcreteness, iconicity, taboo, and age of acquisition. Overall, our findings\nwere very positive - there is a wide variety of semantic information encoded\nwithin the token embedding space. This serves to rule out certain \"meaning\neliminativist\" hypotheses about how transformer LLMs process semantic\ninformation.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u901a\u8fc7\u5bf9RoBERTa-base\u6a21\u578b\u7684\u5206\u6790\uff0c\u8bc1\u660e\u4e86\u8f6c\u6362\u5668\u8bed\u8a00\u6a21\u578b\u5728\u8bcd\u6c47\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7f16\u7801\u4e86\u4e30\u5bcc\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u53cd\u9a73\u4e86\u67d0\u4e9b\u8bed\u4e49\u6d88\u8fc7\u8bba\u5047\u8bbe\u3002", "motivation": "\u63a2\u7d22\u8f6c\u6362\u5668\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u8868\u793a\u8bcd\u4e49\uff0c\u7279\u522b\u662f\u662f\u5426\u5b58\u5728\u7c7b\u4f3c\u4e8e\u8bcd\u6c47\u5e93\u7684\u8bed\u4e49\u4fe1\u606f\u5b58\u50a8\u673a\u5236\u3002", "method": "\u63d0\u53d6RoBERTa-base\u6a21\u578b\u7684token\u5d4c\u5165\u7a7a\u95f4\uff0c\u4f7f\u7528k-means\u805a\u7c7b\u7b97\u6cd5\u5c06\u5176\u5206\u4e3a200\u4e2a\u805a\u7c7b\uff0c\u7136\u540e\u901a\u8fc7\u4eba\u5de5\u68c0\u67e5\u548c\u4e94\u79cd\u5fc3\u7406\u8bed\u8a00\u5b66\u6307\u6807\uff08\u60c5\u611f\u4ef7\u503c\u3001\u5177\u4f53\u6027\u3001\u8c61\u5f81\u6027\u3001\u7981\u5fcc\u6027\u3001\u83b7\u5f97\u5e74\u9f84\uff09\u8fdb\u884c\u5206\u6790\u3002", "result": "\u53d1\u73b0\u8bcd\u6c47\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7f16\u7801\u4e86\u5e7f\u6cdb\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u805a\u7c7b\u7ed3\u679c\u663e\u793a\u51fa\u5bf9\u8bed\u4e49\u4fe1\u606f\u7684\u654f\u611f\u6027\u3002", "conclusion": "\u8f6c\u6362\u5668\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u5176\u5d4c\u5165\u7a7a\u95f4\u6709\u6548\u5730\u7f16\u7801\u8bed\u4e49\u4fe1\u606f\uff0c\u8fd9\u4e00\u53d1\u73b0\u53cd\u9a73\u4e86\u8bed\u4e49\u6d88\u8fc7\u8bba\u8005\u5bf9\u6a21\u578b\u5904\u7406\u8bed\u4e49\u65b9\u5f0f\u7684\u5047\u8bbe\u3002"}}
{"id": "2508.12868", "pdf": "https://arxiv.org/pdf/2508.12868", "abs": "https://arxiv.org/abs/2508.12868", "authors": ["Yilin Geng", "Shujing Wang", "Chuan Wang", "Keqing He", "Yanfei Lv", "Ying Wang", "Zaiwen Feng", "Xiaoying Bai"], "title": "An LLM Agent-Based Complex Semantic Table Annotation Approach", "categories": ["cs.CL", "cs.DB"], "comment": null, "summary": "The Semantic Table Annotation (STA) task, which includes Column Type\nAnnotation (CTA) and Cell Entity Annotation (CEA), maps table contents to\nontology entities and plays important roles in various semantic applications.\nHowever, complex tables often pose challenges such as semantic loss of column\nnames or cell values, strict ontological hierarchy requirements, homonyms,\nspelling errors, and abbreviations, which hinder annotation accuracy. To\naddress these issues, this paper proposes an LLM-based agent approach for CTA\nand CEA. We design and implement five external tools with tailored prompts\nbased on the ReAct framework, enabling the STA agent to dynamically select\nsuitable annotation strategies depending on table characteristics. Experiments\nare conducted on the Tough Tables and BiodivTab datasets from the SemTab\nchallenge, which contain the aforementioned challenges. Our method outperforms\nexisting approaches across various metrics. Furthermore, by leveraging\nLevenshtein distance to reduce redundant annotations, we achieve a 70%\nreduction in time costs and a 60% reduction in LLM token usage, providing an\nefficient and cost-effective solution for STA.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u65b9\u6cd5\u89e3\u51b3\u8bed\u4e49\u8868\u683c\u6807\u6ce8(STA)\u4efb\u52a1\uff0c\u901a\u8fc7ReAct\u6846\u67b6\u8bbe\u8ba1\u4e94\u4e2a\u5916\u90e8\u5de5\u5177\uff0c\u5728Tough Tables\u548cBiodivTab\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7Levenshtein\u8ddd\u79bb\u51cf\u5c11\u5197\u4f59\u6807\u6ce8\uff0c\u663e\u8457\u964d\u4f4e\u65f6\u95f4\u548c\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u590d\u6742\u8868\u683c\u5b58\u5728\u5217\u540d/\u5355\u5143\u683c\u503c\u8bed\u4e49\u4e22\u5931\u3001\u4e25\u683c\u672c\u4f53\u5c42\u6b21\u8981\u6c42\u3001\u540c\u4e49\u8bcd\u3001\u62fc\u5199\u9519\u8bef\u548c\u7f29\u5199\u7b49\u6311\u6218\uff0c\u5f71\u54cd\u6807\u6ce8\u51c6\u786e\u6027\uff0c\u9700\u8981\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u57fa\u4e8eReAct\u6846\u67b6\u8bbe\u8ba1\u4e94\u4e2a\u5916\u90e8\u5de5\u5177\uff0c\u4f7f\u7528LLM\u667a\u80fd\u4f53\u6839\u636e\u8868\u683c\u7279\u5f81\u52a8\u6001\u9009\u62e9\u5408\u9002\u7684\u6807\u6ce8\u7b56\u7565\uff0c\u5e76\u5229\u7528Levenshtein\u8ddd\u79bb\u51cf\u5c11\u5197\u4f59\u6807\u6ce8\u3002", "result": "\u5728SemTab\u6311\u6218\u7684Tough Tables\u548cBiodivTab\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u65f6\u95f4\u6210\u672c\u964d\u4f4e70%\uff0cLLM token\u4f7f\u7528\u51cf\u5c1160%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aSTA\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u6210\u672c\u6548\u76ca\u9ad8\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u590d\u6742\u8868\u683c\u7684\u5404\u79cd\u6311\u6218\u3002"}}
{"id": "2508.12903", "pdf": "https://arxiv.org/pdf/2508.12903", "abs": "https://arxiv.org/abs/2508.12903", "authors": ["Jinyi Han", "Xinyi Wang", "Haiquan Zhao", "Tingyun li", "Zishang Jiang", "Sihang Jiang", "Jiaqing Liang", "Xin Lin", "Weikang Zhou", "Zeye Sun", "Fei Yu", "Yanghua Xiao"], "title": "A Stitch in Time Saves Nine: Proactive Self-Refinement for Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in self-refinement have demonstrated significant potential\nfor improving the outputs of large language models (LLMs) through iterative\nrefinement. However, most existing self-refinement methods rely on a reactive\nprocess with a fixed number of iterations, making it difficult to determine the\noptimal timing and content of refinement based on the evolving generation\ncontext. Inspired by the way humans dynamically refine their thoughts during\nexecution, we propose ProActive Self-Refinement (PASR), a novel method that\nenables LLMs to refine their outputs during the generation process. Unlike\nmethods that regenerate entire responses, PASR proactively decides whether,\nwhen, and how to refine based on the model's internal state and evolving\ncontext. We conduct extensive experiments on a diverse set of 10 tasks to\nevaluate the effectiveness of PASR. Experimental results show that PASR\nsignificantly enhances problem-solving performance. In particular, on Qwen3-8B,\nPASR reduces average token consumption by 41.6 percent compared to standard\ngeneration, while also achieving an 8.2 percent improvement in accuracy. Our\ncode and all baselines used in the paper are available in the GitHub.", "AI": {"tldr": "PASR\u662f\u4e00\u79cd\u4e3b\u52a8\u5f0f\u81ea\u4f18\u5316\u65b9\u6cd5\uff0c\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u52a8\u6001\u51b3\u5b9a\u662f\u5426\u3001\u4f55\u65f6\u4ee5\u53ca\u5982\u4f55\u4f18\u5316\u8f93\u51fa\uff0c\u76f8\u6bd4\u56fa\u5b9a\u8fed\u4ee3\u6b21\u6570\u7684\u4f20\u7edf\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u4e86token\u6d88\u8017\u5e76\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u81ea\u4f18\u5316\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u56fa\u5b9a\u8fed\u4ee3\u6b21\u6570\u7684\u88ab\u52a8\u8fc7\u7a0b\uff0c\u96be\u4ee5\u6839\u636e\u751f\u6210\u4e0a\u4e0b\u6587\u52a8\u6001\u786e\u5b9a\u6700\u4f73\u4f18\u5316\u65f6\u673a\u548c\u5185\u5bb9\uff0c\u800c\u4eba\u7c7b\u5728\u601d\u8003\u8fc7\u7a0b\u4e2d\u4f1a\u52a8\u6001\u4f18\u5316\u81ea\u5df1\u7684\u60f3\u6cd5\u3002", "method": "\u63d0\u51faProActive Self-Refinement (PASR)\u65b9\u6cd5\uff0c\u57fa\u4e8e\u6a21\u578b\u5185\u90e8\u72b6\u6001\u548c\u6f14\u5316\u4e0a\u4e0b\u6587\uff0c\u4e3b\u52a8\u51b3\u5b9a\u662f\u5426\u3001\u4f55\u65f6\u4ee5\u53ca\u5982\u4f55\u4f18\u5316\u8f93\u51fa\uff0c\u800c\u4e0d\u662f\u91cd\u65b0\u751f\u6210\u6574\u4e2a\u54cd\u5e94\u3002", "result": "\u572810\u4e2a\u591a\u6837\u5316\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPASR\u663e\u8457\u63d0\u5347\u4e86\u95ee\u9898\u89e3\u51b3\u6027\u80fd\u3002\u5728Qwen3-8B\u4e0a\uff0c\u76f8\u6bd4\u6807\u51c6\u751f\u6210\u5e73\u5747\u51cf\u5c1141.6%\u7684token\u6d88\u8017\uff0c\u540c\u65f6\u51c6\u786e\u7387\u63d0\u53478.2%\u3002", "conclusion": "PASR\u901a\u8fc7\u4e3b\u52a8\u5f0f\u81ea\u4f18\u5316\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u56fa\u5b9a\u8fed\u4ee3\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b9e\u65f6\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2508.12981", "pdf": "https://arxiv.org/pdf/2508.12981", "abs": "https://arxiv.org/abs/2508.12981", "authors": ["Tianyue Ou", "Saujas Vaduguru", "Daniel Fried"], "title": "Analyzing Information Sharing and Coordination in Multi-Agent Planning", "categories": ["cs.CL"], "comment": null, "summary": "Multi-agent systems (MASs) have pushed the boundaries of large language model\n(LLM) agents in domains such as web research and software engineering. However,\nlong-horizon, multi-constraint planning tasks involve conditioning on detailed\ninformation and satisfying complex interdependent constraints, which can pose a\nchallenge for these systems. In this study, we construct an LLM-based MAS for a\ntravel planning task which is representative of these challenges. We evaluate\nthe impact of a notebook to facilitate information sharing, and evaluate an\norchestrator agent to improve coordination in free form conversation between\nagents. We find that the notebook reduces errors due to hallucinated details by\n18%, while an orchestrator directs the MAS to focus on and further reduce\nerrors by up to 13.5% within focused sub-areas. Combining both mechanisms\nachieves a 25% final pass rate on the TravelPlanner benchmark, a 17.5% absolute\nimprovement over the single-agent baseline's 7.5% pass rate. These results\nhighlight the potential of structured information sharing and reflective\norchestration as key components in MASs for long horizon planning with LLMs.", "AI": {"tldr": "\u591a\u6e38\u5ba2\u89c4\u5212\u4efb\u52a1\u4e2d\uff0c\u901a\u8fc7\u7b14\u8bb0\u672c\u5171\u4eab\u548c\u7ec4\u7ec7\u8005\u534f\u8c03\u673a\u5236\uff0c\u591a\u6e38\u5ba2\u7cfb\u7edf\u5728\u957f\u671f\u8c1c\u5212\u5212\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u6700\u7ec8\u901a\u8fc7\u7387\u63d0\u9ad825%", "motivation": "\u89e3\u51b3\u957f\u671f\u8c1c\u5212\u5212\u4efb\u52a1\u4e2d\u7684\u590d\u6742\u7ea6\u675f\u6761\u4ef6\u548c\u8be6\u7ec6\u4fe1\u606f\u5904\u7406\u6311\u6218\uff0c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u591a\u6e38\u5ba2\u7cfb\u7edf\u7684\u89c4\u5212\u80fd\u529b", "method": "\u6784\u5efa\u57fa\u4e8eLLM\u7684\u591a\u6e38\u5ba2\u7cfb\u7edf\uff0c\u91c7\u7528\u7b14\u8bb0\u672c\u673a\u5236\u4fc3\u8fdb\u4fe1\u606f\u5171\u4eab\uff0c\u4ee5\u53ca\u7ec4\u7ec7\u8005\u6e38\u5ba2\u6765\u6539\u5584\u81ea\u7531\u5bf9\u8bdd\u4e2d\u7684\u534f\u8c03", "result": "\u7b14\u8bb0\u672c\u51cf\u5c11\u5e7d\u7075\u7ec6\u8282\u9519\u8bef18%\uff0c\u7ec4\u7ec7\u8005\u5728\u91cd\u70b9\u5b50\u9886\u57df\u8fdb\u4e00\u6b65\u51cf\u5c11\u9519\u8bef13.5%\u3002\u7ec4\u5408\u673a\u5236\u5728TravelPlanner\u51c0\u9f84\u4e0a\u8fbe\u523025%\u901a\u8fc7\u7387\uff0c\u76f8\u6bd4\u5355\u6e38\u5ba2\u57fa\u51c6\u63d0\u534717.5%", "conclusion": "\u7ed3\u6784\u5316\u4fe1\u606f\u5171\u4eab\u548c\u53cd\u601d\u6027\u7ec4\u7ec7\u662f\u957f\u671f\u8c1c\u5212\u5212\u4efb\u52a1\u4e2d\u591a\u6e38\u5ba2\u7cfb\u7edf\u7684\u5173\u952e\u7ec4\u6210\u90e8\u5206\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u6e38\u5ba2\u7684\u89c4\u5212\u6027\u80fd"}}
{"id": "2508.13024", "pdf": "https://arxiv.org/pdf/2508.13024", "abs": "https://arxiv.org/abs/2508.13024", "authors": ["Ralph Peeters", "Aaron Steiner", "Luca Schwarz", "Julian Yuya Caspary", "Christian Bizer"], "title": "WebMall -- A Multi-Shop Benchmark for Evaluating Web Agents", "categories": ["cs.CL"], "comment": null, "summary": "LLM-based web agents have the potential to automate long-running web tasks,\nsuch as finding offers for specific products in multiple online shops and\nsubsequently ordering the cheapest products that meet the users needs. This\npaper introduces WebMall, a multi-shop online shopping benchmark for evaluating\nthe effectiveness and efficiency of web agents for comparison-shopping. WebMall\nconsists of four simulated online shops populated with authentic product offers\nsourced from the Common Crawl, alongside a suite of 91 cross-shop tasks. These\ntasks include basic tasks such as finding specific products in multiple shops,\nperforming price comparisons, adding items to the shopping cart, and completing\ncheckout. Advanced tasks involve searching for products based on vague\nrequirements, identifying suitable substitutes, and finding compatible\nproducts. Compared to existing e-commerce benchmarks, such as WebShop or\nShoppingBench, WebMall introduces comparison-shopping tasks across multiple\nshops. Furthermore, the product offers are more heterogeneous, as they\noriginate from hundreds of distinct real-world shops. The tasks in WebMall\nrequire longer interaction trajectories than those in WebShop, while remaining\nrepresentative of real-world shopping behaviors. We evaluate eight baseline\nagents on WebMall, varying in observation modality, memory utilization, and\nunderlying large language model (GPT 4.1 and Claude Sonnet 4). The\nbest-performing configurations achieve completion rates of 75% and 53%, and F1\nscores of 87% and 63%, on the basic and advanced task sets, respectively.\nWebMall is publicly released to facilitate research on web agents and to\npromote advancements in navigation, reasoning, and efficiency within e-commerce\nscenarios.", "AI": {"tldr": "WebMall\u662f\u4e00\u4e2a\u591a\u5546\u5e97\u5728\u7ebf\u8d2d\u7269\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30Web\u4ee3\u7406\u5728\u6bd4\u8f83\u8d2d\u7269\u4e2d\u7684\u6548\u679c\u548c\u6548\u7387\uff0c\u5305\u542b4\u4e2a\u6a21\u62df\u5546\u5e97\u548c91\u4e2a\u8de8\u5546\u5e97\u4efb\u52a1\uff0c\u6bd4\u73b0\u6709\u57fa\u51c6\u66f4\u590d\u6742\u548c\u771f\u5b9e\u3002", "motivation": "\u73b0\u6709\u7684\u7535\u5b50\u5546\u52a1\u57fa\u51c6\u6d4b\u8bd5\u5982WebShop\u6216ShoppingBench\u4e3b\u8981\u5173\u6ce8\u5355\u4e00\u5546\u5e97\u5185\u7684\u4efb\u52a1\uff0c\u7f3a\u4e4f\u8de8\u591a\u4e2a\u5546\u5e97\u7684\u6bd4\u8f83\u8d2d\u7269\u4efb\u52a1\u8bc4\u4f30\uff0c\u65e0\u6cd5\u5145\u5206\u6d4b\u8bd5Web\u4ee3\u7406\u5728\u771f\u5b9e\u590d\u6742\u8d2d\u7269\u573a\u666f\u4e2d\u7684\u80fd\u529b\u3002", "method": "\u6784\u5efa\u5305\u542b4\u4e2a\u6a21\u62df\u5728\u7ebf\u5546\u5e97\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4ea7\u54c1\u6570\u636e\u6765\u81eaCommon Crawl\u7684\u771f\u5b9e\u5546\u54c1\u4fe1\u606f\uff0c\u5305\u542b91\u4e2a\u8de8\u5546\u5e97\u4efb\u52a1\uff08\u57fa\u7840\u4efb\u52a1\u548c\u9ad8\u7ea7\u4efb\u52a1\uff09\uff0c\u5e76\u8bc4\u4f30\u4e868\u4e2a\u4e0d\u540c\u914d\u7f6e\u7684\u57fa\u7ebf\u4ee3\u7406\u3002", "result": "\u6700\u4f73\u914d\u7f6e\u5728\u57fa\u7840\u4efb\u52a1\u96c6\u4e0a\u8fbe\u523075%\u5b8c\u6210\u7387\u548c87% F1\u5206\u6570\uff0c\u5728\u9ad8\u7ea7\u4efb\u52a1\u96c6\u4e0a\u8fbe\u523053%\u5b8c\u6210\u7387\u548c63% F1\u5206\u6570\uff0c\u663e\u793a\u4e86\u5f53\u524dWeb\u4ee3\u7406\u5728\u590d\u6742\u8d2d\u7269\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u6c34\u5e73\u3002", "conclusion": "WebMall\u57fa\u51c6\u6d4b\u8bd5\u4e3aWeb\u4ee3\u7406\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u771f\u5b9e\u3001\u66f4\u590d\u6742\u7684\u8bc4\u4f30\u73af\u5883\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u7535\u5b50\u5546\u52a1\u573a\u666f\u4e2d\u7684\u5bfc\u822a\u3001\u63a8\u7406\u548c\u6548\u7387\u65b9\u9762\u7684\u6280\u672f\u8fdb\u6b65\uff0c\u5e76\u5df2\u516c\u5f00\u53d1\u5e03\u4ee5\u4fc3\u8fdb\u76f8\u5173\u7814\u7a76\u3002"}}
{"id": "2508.13028", "pdf": "https://arxiv.org/pdf/2508.13028", "abs": "https://arxiv.org/abs/2508.13028", "authors": ["Zhu Li", "Yuqing Zhang", "Xiyuan Gao", "Devraj Raghuvanshi", "Nagendra Kumar", "Shekhar Nayak", "Matt Coler"], "title": "Integrating Feedback Loss from Bi-modal Sarcasm Detector for Sarcastic Speech Synthesis", "categories": ["cs.CL"], "comment": "Speech Synthesis Workshop 2025", "summary": "Sarcastic speech synthesis, which involves generating speech that effectively\nconveys sarcasm, is essential for enhancing natural interactions in\napplications such as entertainment and human-computer interaction. However,\nsynthesizing sarcastic speech remains a challenge due to the nuanced prosody\nthat characterizes sarcasm, as well as the limited availability of annotated\nsarcastic speech data. To address these challenges, this study introduces a\nnovel approach that integrates feedback loss from a bi-modal sarcasm detection\nmodel into the TTS training process, enhancing the model's ability to capture\nand convey sarcasm. In addition, by leveraging transfer learning, a speech\nsynthesis model pre-trained on read speech undergoes a two-stage fine-tuning\nprocess. First, it is fine-tuned on a diverse dataset encompassing various\nspeech styles, including sarcastic speech. In the second stage, the model is\nfurther refined using a dataset focused specifically on sarcastic speech,\nenhancing its ability to generate sarcasm-aware speech. Objective and\nsubjective evaluations demonstrate that our proposed methods improve the\nquality, naturalness, and sarcasm-awareness of synthesized speech.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u53cc\u6a21\u6001\u8bbd\u523a\u68c0\u6d4b\u6a21\u578b\u53cd\u9988\u635f\u5931\u548c\u4e24\u9636\u6bb5\u5fae\u8c03\u7b56\u7565\u7684\u8bbd\u523a\u8bed\u97f3\u5408\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u548c\u591a\u9636\u6bb5\u8bad\u7ec3\u63d0\u5347\u5408\u6210\u8bed\u97f3\u7684\u8bbd\u523a\u8868\u8fbe\u8d28\u91cf", "motivation": "\u8bbd\u523a\u8bed\u97f3\u5408\u6210\u5bf9\u4e8e\u589e\u5f3a\u5a31\u4e50\u548c\u4eba\u673a\u4ea4\u4e92\u5e94\u7528\u7684\u81ea\u7136\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u8bbd\u523a\u7684\u5fae\u5999\u97f5\u5f8b\u7279\u5f81\u548c\u6807\u6ce8\u6570\u636e\u6709\u9650\uff0c\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u6311\u6218", "method": "1) \u5c06\u53cc\u6a21\u6001\u8bbd\u523a\u68c0\u6d4b\u6a21\u578b\u7684\u53cd\u9988\u635f\u5931\u6574\u5408\u5230TTS\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff1b2) \u91c7\u7528\u4e24\u9636\u6bb5\u5fae\u8c03\uff1a\u5148\u5728\u5305\u542b\u591a\u79cd\u8bed\u97f3\u98ce\u683c\u7684\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\uff0c\u518d\u5728\u4e13\u95e8\u7684\u8bbd\u523a\u8bed\u97f3\u6570\u636e\u96c6\u4e0a\u8fdb\u4e00\u6b65\u4f18\u5316", "result": "\u4e3b\u5ba2\u89c2\u8bc4\u4f30\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u5408\u6210\u8bed\u97f3\u7684\u8d28\u91cf\u3001\u81ea\u7136\u5ea6\u548c\u8bbd\u523a\u611f\u77e5\u80fd\u529b", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u8bbd\u523a\u8bed\u97f3\u5408\u6210\u7684\u6311\u6218\uff0c\u4e3a\u751f\u6210\u5177\u6709\u4e30\u5bcc\u60c5\u611f\u8868\u8fbe\u7684\u8bed\u97f3\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848"}}
{"id": "2508.13037", "pdf": "https://arxiv.org/pdf/2508.13037", "abs": "https://arxiv.org/abs/2508.13037", "authors": ["Xinhe Li", "Jiajun Liu", "Peng Wang"], "title": "Can Large Models Teach Student Models to Solve Mathematical Problems Like Human Beings? A Reasoning Distillation Method via Multi-LoRA Interaction", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by IJCAI2025", "summary": "Recent studies have demonstrated that Large Language Models (LLMs) have\nstrong mathematical reasoning abilities but rely on hundreds of billions of\nparameters. To tackle the challenge of poor reasoning in Small Language Models\n(SLMs), existing methods typically leverage LLMs to generate massive amounts of\ndata for cramming training. In psychology, they are akin to System 1 thinking,\nwhich resolves reasoning problems rapidly based on experience and intuition.\nHowever, human learning also requires System 2 thinking, where knowledge is\nfirst acquired and then reinforced through practice. Inspired by such two\ndistinct modes of thinking, we propose a novel method based on the multi-LoRA\nInteraction for mathematical reasoning Distillation (LoRID). First, we input\nthe question and reasoning of each sample into an LLM to create\nknowledge-enhanced datasets. Subsequently, we train a LoRA block on the student\nmodel as an Intuitive Reasoner (IR), which directly generates Chain-of-Thoughts\nfor problem-solving. Then, to imitate System 2 thinking, we train the Knowledge\nGenerator (KG) and Deep Reasoner (DR), respectively. The former outputs only\nknowledge after receiving problems, while the latter uses that knowledge to\nperform reasoning. Finally, to address the randomness in the generation of IR\nand DR, we evaluate whether their outputs are consistent, and the inference\nprocess needs to be iterated if not. This step can enhance the mathematical\nreasoning ability of SLMs through mutual feedback. Experimental results show\nthat LoRID achieves state-of-the-art performance, especially on the GSM8K\ndataset, where it outperforms the second-best method by 2.3%, 16.1%, 2.4%,\n12.3%, and 1.8% accuracy across the five base models, respectively.", "AI": {"tldr": "LoRID\u662f\u4e00\u79cd\u57fa\u4e8e\u591aLoRA\u4ea4\u4e92\u7684\u6570\u5b66\u63a8\u7406\u84b8\u998f\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u62df\u4eba\u7c7bSystem 1\u548cSystem 2\u4e24\u79cd\u601d\u7ef4\u6a21\u5f0f\uff0c\u63d0\u5347\u5c0f\u8bed\u8a00\u6a21\u578b\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u5728GSM8K\u7b49\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5c0f\u8bed\u8a00\u6a21\u578b(SLMs)\u6570\u5b66\u63a8\u7406\u80fd\u529b\u5dee\u7684\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5927\u91cf\u6570\u636e\u8fdb\u884c\u586b\u9e2d\u5f0f\u8bad\u7ec3\uff0c\u8fd9\u7c7b\u4f3c\u4e8e\u5fc3\u7406\u5b66\u4e2d\u7684System 1\u601d\u7ef4\uff0c\u4f46\u4eba\u7c7b\u5b66\u4e60\u8fd8\u9700\u8981System 2\u601d\u7ef4\uff08\u5148\u83b7\u53d6\u77e5\u8bc6\u518d\u901a\u8fc7\u5b9e\u8df5\u5f3a\u5316\uff09\u3002", "method": "\u63d0\u51faLoRID\u65b9\u6cd5\uff1a1\uff09\u7528LLM\u521b\u5efa\u77e5\u8bc6\u589e\u5f3a\u6570\u636e\u96c6\uff1b2\uff09\u8bad\u7ec3Intuitive Reasoner\uff08IR\uff09\u76f4\u63a5\u751f\u6210\u63a8\u7406\u94fe\uff1b3\uff09\u8bad\u7ec3Knowledge Generator\uff08KG\uff09\u8f93\u51fa\u77e5\u8bc6\uff1b4\uff09\u8bad\u7ec3Deep Reasoner\uff08DR\uff09\u5229\u7528\u77e5\u8bc6\u8fdb\u884c\u63a8\u7406\uff1b5\uff09\u901a\u8fc7IR\u548cDR\u8f93\u51fa\u4e00\u81f4\u6027\u68c0\u67e5\u8fdb\u884c\u8fed\u4ee3\u63a8\u7406\u3002", "result": "\u5728GSM8K\u6570\u636e\u96c6\u4e0a\uff0cLoRID\u5728\u4e94\u4e2a\u57fa\u7840\u6a21\u578b\u4e0a\u5206\u522b\u6bd4\u7b2c\u4e8c\u597d\u65b9\u6cd5\u63d0\u5347\u4e862.3%\u300116.1%\u30012.4%\u300112.3%\u548c1.8%\u7684\u51c6\u786e\u7387\uff0c\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "LoRID\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u53cc\u7cfb\u7edf\u601d\u7ef4\u6a21\u5f0f\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5c0f\u8bed\u8a00\u6a21\u578b\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u591aLoRA\u4ea4\u4e92\u84b8\u998f\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.13044", "pdf": "https://arxiv.org/pdf/2508.13044", "abs": "https://arxiv.org/abs/2508.13044", "authors": ["M. Ali Bayram", "Ali Arda Fincan", "Ahmet Semih G\u00fcm\u00fc\u015f", "Banu Diri", "Sava\u015f Y\u0131ld\u0131r\u0131m", "\u00d6ner Ayta\u015f"], "title": "B\u00fcy\u00fck Dil Modelleri i\u00e7in TR-MMLU Benchmark\u0131: Performans De\u011ferlendirmesi, Zorluklar ve \u0130yile\u015ftirme F\u0131rsatlar\u0131", "categories": ["cs.CL", "68T50", "I.2.7; I.2.6"], "comment": "10 pages, in Turkish language, 5 figures. Presented at the 2025 33rd\n  Signal Processing and Communications Applications Conference (SIU), 25--28\n  June 2025, Sile, Istanbul, T\\\"urkiye", "summary": "Language models have made significant advancements in understanding and\ngenerating human language, achieving remarkable success in various\napplications. However, evaluating these models remains a challenge,\nparticularly for resource-limited languages like Turkish. To address this\nissue, we introduce the Turkish MMLU (TR-MMLU) benchmark, a comprehensive\nevaluation framework designed to assess the linguistic and conceptual\ncapabilities of large language models (LLMs) in Turkish. TR-MMLU is based on a\nmeticulously curated dataset comprising 6,200 multiple-choice questions across\n62 sections within the Turkish education system. This benchmark provides a\nstandard framework for Turkish NLP research, enabling detailed analyses of\nLLMs' capabilities in processing Turkish text. In this study, we evaluated\nstate-of-the-art LLMs on TR-MMLU, highlighting areas for improvement in model\ndesign. TR-MMLU sets a new standard for advancing Turkish NLP research and\ninspiring future innovations.", "AI": {"tldr": "\u571f\u8033\u5176\u8bed\u8a00\u6a21\u578b\u8bc4\u6d4b\u6807\u51c6\u7f3a\u4e4f\uff0c\u7814\u7a76\u4eba\u5458\u521b\u5efa\u4e86TR-MMLU\u8bc4\u6d4b\u6846\u67b6\uff0c\u5305\u542b6,200\u9053\u591a\u9009\u9898\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u571f\u8033\u5176\u8bed\u4e0a\u7684\u8bed\u8a00\u548c\u6982\u5ff5\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u867d\u7136\u8bed\u8a00\u6a21\u578b\u53d6\u5f97\u4e86\u91cd\u5927\u8fdb\u5c55\uff0c\u4f46\u5bf9\u4e8e\u8d44\u6e90\u6709\u9650\u7684\u8bed\u8a00\uff08\u5982\u571f\u8033\u5176\u8bed\uff09\u7684\u8bc4\u4f30\u4ecd\u9762\u4e34\u6311\u6218\u3002\u5f53\u524d\u7f3a\u4e4f\u4e13\u95e8\u4e3a\u571f\u8033\u5176\u8bed\u8a00\u6a21\u578b\u8bbe\u8ba1\u7684\u7ef4\u5e76\u8bc4\u6d4b\u6807\u51c6\u3002", "method": "\u57fa\u4e8e\u571f\u8033\u5176\u6559\u80b2\u4f53\u7cfb\u4e2d62\u4e2a\u5b66\u79d1\u76846,200\u9053\u591a\u9009\u9898\uff0c\u7ec6\u5fc3\u7f16\u8bd1\u548c\u7ef4\u62a4\u4e86\u4e00\u4e2a\u7ef4\u5e76\u7684\u8bc4\u6d4b\u6570\u636e\u96c6\u3002\u8fd9\u4e2a\u6807\u51c6\u6846\u67b6\u5141\u8bb8\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u5728\u571f\u8033\u5176\u8bed\u6587\u672c\u5904\u7406\u80fd\u529b\u8fdb\u884c\u8be6\u7ec6\u5206\u6790\u3002", "result": "\u7814\u7a76\u4eba\u5458\u5728TR-MMLU\u4e0a\u8bc4\u4f30\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u6307\u51fa\u4e86\u6a21\u578b\u8bbe\u8ba1\u4e2d\u9700\u8981\u6539\u8fdb\u7684\u9886\u57df\u3002", "conclusion": "TR-MMLU\u4e3a\u63a8\u52a8\u571f\u8033\u5176\u8bed\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7814\u7a76\u8bbe\u7acb\u4e86\u65b0\u6807\u51c6\uff0c\u5e76\u5c06\u6fc0\u52b1\u672a\u6765\u7684\u521b\u65b0\u3002"}}
{"id": "2508.13058", "pdf": "https://arxiv.org/pdf/2508.13058", "abs": "https://arxiv.org/abs/2508.13058", "authors": ["M. Ali Bayram", "Ali Arda Fincan", "Ahmet Semih G\u00fcm\u00fc\u015f", "Sercan Karaka\u015f", "Banu Diri", "Sava\u015f Y\u0131ld\u0131r\u0131m"], "title": "Do\u011fal Dil \u0130\u015flemede Tokenizasyon Standartlar\u0131 ve \u00d6l\u00e7\u00fcm\u00fc: T\u00fcrk\u00e7e \u00dczerinden B\u00fcy\u00fck Dil Modellerinin Kar\u015f\u0131la\u015ft\u0131rmal\u0131 Analizi", "categories": ["cs.CL", "68T50", "I.2.7; I.2.6"], "comment": "in Turkish language, Presented at the 2025 33rd Signal Processing and\n  Communications Applications Conference (SIU), 25--28 June 2025, \\c{S}ile,\n  Istanbul, T\\\"urkiye", "summary": "Tokenization is a fundamental preprocessing step in Natural Language\nProcessing (NLP), significantly impacting the capability of large language\nmodels (LLMs) to capture linguistic and semantic nuances. This study introduces\na novel evaluation framework addressing tokenization challenges specific to\nmorphologically-rich and low-resource languages such as Turkish. Utilizing the\nTurkish MMLU (TR-MMLU) dataset, comprising 6,200 multiple-choice questions from\nthe Turkish education system, we assessed tokenizers based on vocabulary size,\ntoken count, processing time, language-specific token percentages (\\%TR), and\ntoken purity (\\%Pure). These newly proposed metrics measure how effectively\ntokenizers preserve linguistic structures. Our analysis reveals that\nlanguage-specific token percentages exhibit a stronger correlation with\ndownstream performance (e.g., MMLU scores) than token purity. Furthermore,\nincreasing model parameters alone does not necessarily enhance linguistic\nperformance, underscoring the importance of tailored, language-specific\ntokenization methods. The proposed framework establishes robust and practical\ntokenization standards for morphologically complex languages.", "AI": {"tldr": "\u8fd9\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u5bf9\u4e8e\u5f62\u6001\u4e30\u5bcc\u8bed\u8a00\uff08\u5982\u571f\u8033\u5176\u8bed\uff09\u7684\u5206\u8bcd\u5668\u6027\u80fd\uff0c\u53d1\u73b0\u8bed\u8a00\u7279\u5b9a\u5206\u8bcd\u767e\u5206\u6bd4\u6bd4\u5206\u8bcd\u7eaf\u5ea6\u66f4\u80fd\u9884\u6d4b\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\uff0c\u5e76\u5f3a\u8c03\u4e86\u8bed\u8a00\u7279\u5b9a\u5206\u8bcd\u65b9\u6cd5\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u5206\u8bcd\u662fNLP\u4e2d\u7684\u57fa\u7840\u9884\u5904\u7406\u6b65\u9aa4\uff0c\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u8a00\u5b66\u4e60\u80fd\u529b\u6709\u91cd\u8981\u5f71\u54cd\u3002\u5f53\u524d\u7684\u5206\u8bcd\u65b9\u6cd5\u5728\u5f62\u6001\u4e30\u5bcc\u548c\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u571f\u8033\u5176\u8bed\uff09\u4e2d\u9762\u4e34\u7279\u6b8a\u6311\u6218\uff0c\u9700\u8981\u4e13\u95e8\u7684\u8bc4\u4f30\u6807\u51c6\u6765\u8bc4\u4f30\u5176\u6548\u679c\u3002", "method": "\u4f7f\u7528\u571f\u8033\u5176\u6559\u80b2\u7cfb\u7edf\u7684TR-MMLU\u6570\u636e\u96c6\uff086,200\u9053\u9009\u62e9\u9898\uff09\uff0c\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u65b0\u7684\u8bc4\u4f30\u6307\u6807\uff1a\u8bcd\u6c47\u91cf\u5927\u5c0f\u3001\u5206\u8bcd\u6570\u91cf\u3001\u5904\u7406\u65f6\u95f4\u3001\u8bed\u8a00\u7279\u5b9a\u5206\u8bcd\u767e\u5206\u6bd4\uff08%TR\uff09\u548c\u5206\u8bcd\u7eaf\u5ea6\uff08%Pure\uff09\uff0c\u7528\u4e8e\u8bc4\u4f30\u5206\u8bcd\u5668\u4fdd\u7559\u8bed\u8a00\u7ed3\u6784\u7684\u80fd\u529b\u3002", "result": "\u5206\u6790\u663e\u793a\uff0c\u8bed\u8a00\u7279\u5b9a\u5206\u8bcd\u767e\u5206\u6bd4\uff08%TR\uff09\u4e0e\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\uff08\u5982MMLU\u5206\u6570\uff09\u7684\u76f8\u5173\u6027\u66f4\u5f3a\uff0c\u800c\u4ec5\u4ec5\u589e\u52a0\u6a21\u578b\u53c2\u6570\u5e76\u4e0d\u80fd\u5fc5\u7136\u63d0\u5347\u8bed\u8a00\u6027\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u5efa\u7acb\u4e86\u4e00\u4e2a\u5065\u58ee\u800c\u5b9e\u7528\u7684\u5206\u8bcd\u6807\u51c6\u6846\u67b6\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5f62\u6001\u590d\u6742\u8bed\u8a00\u3002\u7ed3\u679c\u5f3a\u8c03\u4e86\u91c7\u7528\u8bed\u8a00\u7279\u5b9a\u7684\u3001\u7ec6\u81f4\u8c03\u6574\u7684\u5206\u8bcd\u65b9\u6cd5\u5bf9\u4e8e\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u7684\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2508.13060", "pdf": "https://arxiv.org/pdf/2508.13060", "abs": "https://arxiv.org/abs/2508.13060", "authors": ["John Alderete", "Macarious Kin Fung Hui", "Aanchan Mohan"], "title": "Evaluating ASR robustness to spontaneous speech errors: A study of WhisperX using a Speech Error Database", "categories": ["cs.CL"], "comment": "5 pages, 6 figures, 1 table, Interspeech 2025 (Rotterdam)", "summary": "The Simon Fraser University Speech Error Database (SFUSED) is a public data\ncollection developed for linguistic and psycholinguistic research. Here we\ndemonstrate how its design and annotations can be used to test and evaluate\nspeech recognition models. The database comprises systematically annotated\nspeech errors from spontaneous English speech, with each error tagged for\nintended and actual error productions. The annotation schema incorporates\nmultiple classificatory dimensions that are of some value to model assessment,\nincluding linguistic hierarchical level, contextual sensitivity, degraded\nwords, word corrections, and both word-level and syllable-level error\npositioning. To assess the value of these classificatory variables, we\nevaluated the transcription accuracy of WhisperX across 5,300 documented word\nand phonological errors. This analysis demonstrates the atabase's effectiveness\nas a diagnostic tool for ASR system performance.", "AI": {"tldr": "SFUSED\u8bed\u97f3\u9519\u8bef\u6570\u636e\u5e93\u53ef\u7528\u4e8e\u8bc4\u4f30\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\u6027\u80fd\uff0c\u901a\u8fc7\u5206\u6790WhisperX\u57285300\u4e2a\u6807\u6ce8\u9519\u8bef\u4e0a\u7684\u8f6c\u5f55\u51c6\u786e\u7387\uff0c\u8bc1\u660e\u4e86\u8be5\u6570\u636e\u5e93\u4f5c\u4e3aASR\u7cfb\u7edf\u8bca\u65ad\u5de5\u5177\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u516c\u5f00\u7684\u8bed\u97f3\u9519\u8bef\u6570\u636e\u5e93\uff0c\u7528\u4e8e\u8bed\u8a00\u5b66\u548c\u5fc3\u7406\u8bed\u8a00\u5b66\u7814\u7a76\uff0c\u5e76\u5c55\u793a\u5176\u5982\u4f55\u7528\u4e8e\u6d4b\u8bd5\u548c\u8bc4\u4f30\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\u3002", "method": "\u4f7f\u7528SFUSED\u6570\u636e\u5e93\u4e2d\u76845300\u4e2a\u6807\u6ce8\u7684\u8bcd\u6c47\u548c\u8bed\u97f3\u9519\u8bef\uff0c\u8bc4\u4f30WhisperX\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\u7684\u8f6c\u5f55\u51c6\u786e\u6027\u3002\u6570\u636e\u5e93\u5305\u542b\u7cfb\u7edf\u6807\u6ce8\u7684\u81ea\u53d1\u82f1\u8bed\u8bed\u97f3\u9519\u8bef\uff0c\u6bcf\u4e2a\u9519\u8bef\u90fd\u6807\u6ce8\u4e86\u9884\u671f\u548c\u5b9e\u9645\u4ea7\u51fa\u3002", "result": "\u5206\u6790\u8bc1\u660e\u4e86SFUSED\u6570\u636e\u5e93\u4f5c\u4e3aASR\u7cfb\u7edf\u6027\u80fd\u8bca\u65ad\u5de5\u5177\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u8bc4\u4f30\u6a21\u578b\u5728\u4e0d\u540c\u5206\u7c7b\u7ef4\u5ea6\uff08\u5982\u8bed\u8a00\u5c42\u7ea7\u3001\u4e0a\u4e0b\u6587\u654f\u611f\u6027\u7b49\uff09\u4e0a\u7684\u8868\u73b0\u3002", "conclusion": "SFUSED\u8bed\u97f3\u9519\u8bef\u6570\u636e\u5e93\u662f\u4e00\u4e2a\u6709\u4ef7\u503c\u7684\u5de5\u5177\uff0c\u53ef\u7528\u4e8e\u5168\u9762\u8bc4\u4f30\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u5404\u79cd\u7c7b\u578b\u8bed\u97f3\u9519\u8bef\u65b9\u9762\u7684\u80fd\u529b\u3002"}}
{"id": "2508.13070", "pdf": "https://arxiv.org/pdf/2508.13070", "abs": "https://arxiv.org/abs/2508.13070", "authors": ["Long Ma", "Fangwei Zhong", "Yizhou Wang"], "title": "Reinforced Context Order Recovery for Adaptive Reasoning and Planning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Modern causal language models, followed by rapid developments in discrete\ndiffusion models, can now produce a wide variety of interesting and useful\ncontent. However, these families of models are predominantly trained to output\ntokens with a fixed (left-to-right) or random order, which may deviate from the\nlogical order in which tokens are generated originally. In this paper, we\nobserve that current causal and diffusion models encounter difficulties in\nproblems that require adaptive token generation orders to solve tractably,\nwhich we characterize with the $\\mathcal{V}$-information framework. Motivated\nby this, we propose Reinforced Context Order Recovery (ReCOR), a\nreinforcement-learning-based framework to extract adaptive, data-dependent\ntoken generation orders from text data without annotations. Self-supervised by\ntoken prediction statistics, ReCOR estimates the hardness of predicting every\nunfilled token and adaptively selects the next token during both training and\ninference. Experiments on challenging reasoning and planning datasets\ndemonstrate the superior performance of ReCOR compared with baselines,\nsometimes outperforming oracle models supervised with the ground-truth order.", "AI": {"tldr": "ReCOR\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u9009\u62e9token\u751f\u6210\u987a\u5e8f\u6765\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u65e0\u9700\u6807\u6ce8\u6570\u636e\u5373\u53ef\u5b66\u4e60\u6700\u4f18\u751f\u6210\u987a\u5e8f\u3002", "motivation": "\u5f53\u524d\u56e0\u679c\u8bed\u8a00\u6a21\u578b\u548c\u6269\u6563\u6a21\u578b\u4f7f\u7528\u56fa\u5b9a\u6216\u968f\u673a\u7684token\u751f\u6210\u987a\u5e8f\uff0c\u8fd9\u4e0e\u539f\u59cb\u903b\u8f91\u987a\u5e8f\u4e0d\u7b26\uff0c\u5bfc\u81f4\u5728\u9700\u8981\u81ea\u9002\u5e94\u751f\u6210\u987a\u5e8f\u7684\u590d\u6742\u63a8\u7406\u95ee\u9898\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51faReCOR\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7token\u9884\u6d4b\u7edf\u8ba1\u8fdb\u884c\u81ea\u76d1\u7763\uff0c\u4f30\u8ba1\u6bcf\u4e2a\u672a\u586b\u5145token\u7684\u9884\u6d4b\u96be\u5ea6\uff0c\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u8fc7\u7a0b\u4e2d\u81ea\u9002\u5e94\u9009\u62e9\u4e0b\u4e00\u4e2a\u8981\u751f\u6210\u7684token\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u63a8\u7406\u548c\u89c4\u5212\u6570\u636e\u96c6\u4e0a\uff0cReCOR\u8868\u73b0\u51fa\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u7684\u6027\u80fd\uff0c\u6709\u65f6\u751a\u81f3\u8d85\u8fc7\u4f7f\u7528\u771f\u5b9e\u987a\u5e8f\u76d1\u7763\u7684oracle\u6a21\u578b\u3002", "conclusion": "\u81ea\u9002\u5e94token\u751f\u6210\u987a\u5e8f\u5bf9\u4e8e\u590d\u6742\u63a8\u7406\u4efb\u52a1\u81f3\u5173\u91cd\u8981\uff0cReCOR\u6846\u67b6\u80fd\u591f\u6709\u6548\u5b66\u4e60\u6570\u636e\u4f9d\u8d56\u7684\u751f\u6210\u987a\u5e8f\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2508.13079", "pdf": "https://arxiv.org/pdf/2508.13079", "abs": "https://arxiv.org/abs/2508.13079", "authors": ["Dayy\u00e1n O'Brien", "Bhavitvya Malik", "Ona de Gibert", "Pinzhen Chen", "Barry Haddow", "J\u00f6rg Tiedemann"], "title": "DocHPLT: A Massively Multilingual Document-Level Translation Dataset", "categories": ["cs.CL"], "comment": null, "summary": "Existing document-level machine translation resources are only available for\na handful of languages, mostly high-resourced ones. To facilitate the training\nand evaluation of document-level translation and, more broadly, long-context\nmodeling for global communities, we create DocHPLT, the largest publicly\navailable document-level translation dataset to date. It contains 124 million\naligned document pairs across 50 languages paired with English, comprising 4.26\nbillion sentences, with further possibility to provide 2500 bonus pairs not\ninvolving English. Unlike previous reconstruction-based approaches that piece\ntogether documents from sentence-level data, we modify an existing web\nextraction pipeline to preserve complete document integrity from the source,\nretaining all content including unaligned portions. After our preliminary\nexperiments identify the optimal training context strategy for document-level\ntranslation, we demonstrate that LLMs fine-tuned on DocHPLT substantially\noutperform off-the-shelf instruction-tuned baselines, with particularly\ndramatic improvements for under-resourced languages. We open-source the dataset\nunder a permissive license, providing essential infrastructure for advancing\nmultilingual document-level translation.", "AI": {"tldr": "DocHPLT\u662f\u8fc4\u4eca\u4e3a\u6b62\u6700\u5927\u7684\u516c\u5f00\u6587\u6863\u7ea7\u7ffb\u8bd1\u6570\u636e\u96c6\uff0c\u5305\u542b1.24\u4ebf\u4e2a\u6587\u6863\u5bf9\uff0c\u6db5\u76d650\u79cd\u8bed\u8a00\u4e0e\u82f1\u8bed\u7684\u914d\u5bf9\uff0c\u517142.6\u4ebf\u4e2a\u53e5\u5b50\uff0c\u4e3a\u591a\u8bed\u8a00\u6587\u6863\u7ea7\u7ffb\u8bd1\u63d0\u4f9b\u91cd\u8981\u57fa\u7840\u8bbe\u65bd\u3002", "motivation": "\u73b0\u6709\u6587\u6863\u7ea7\u673a\u5668\u7ffb\u8bd1\u8d44\u6e90\u4ec5\u9002\u7528\u4e8e\u5c11\u6570\u9ad8\u8d44\u6e90\u8bed\u8a00\uff0c\u4e3a\u4e86\u4fc3\u8fdb\u6587\u6863\u7ea7\u7ffb\u8bd1\u8bad\u7ec3\u548c\u8bc4\u4f30\uff0c\u4ee5\u53ca\u66f4\u5e7f\u6cdb\u7684\u5168\u7403\u793e\u533a\u957f\u4e0a\u4e0b\u6587\u5efa\u6a21\uff0c\u9700\u8981\u521b\u5efa\u5927\u89c4\u6a21\u591a\u8bed\u8a00\u6587\u6863\u7ea7\u7ffb\u8bd1\u6570\u636e\u96c6\u3002", "method": "\u4fee\u6539\u73b0\u6709\u7684\u7f51\u9875\u63d0\u53d6\u6d41\u7a0b\uff0c\u4ece\u6e90\u5934\u4fdd\u7559\u5b8c\u6574\u7684\u6587\u6863\u5b8c\u6574\u6027\uff08\u5305\u62ec\u672a\u5bf9\u9f50\u90e8\u5206\uff09\uff0c\u800c\u4e0d\u662f\u57fa\u4e8e\u53e5\u5b50\u7ea7\u6570\u636e\u91cd\u5efa\u6587\u6863\u3002\u901a\u8fc7\u521d\u6b65\u5b9e\u9a8c\u786e\u5b9a\u6700\u4f73\u8bad\u7ec3\u4e0a\u4e0b\u6587\u7b56\u7565\u3002", "result": "\u5728DocHPLT\u4e0a\u5fae\u8c03\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u73b0\u6210\u7684\u6307\u4ee4\u8c03\u4f18\u57fa\u7ebf\u6a21\u578b\uff0c\u7279\u522b\u662f\u5bf9\u8d44\u6e90\u532e\u4e4f\u8bed\u8a00\u7684\u6539\u8fdb\u5c24\u4e3a\u663e\u8457\u3002", "conclusion": "DocHPLT\u6570\u636e\u96c6\u5728\u5bbd\u677e\u8bb8\u53ef\u4e0b\u5f00\u6e90\uff0c\u4e3a\u63a8\u8fdb\u591a\u8bed\u8a00\u6587\u6863\u7ea7\u7ffb\u8bd1\u63d0\u4f9b\u4e86\u5fc5\u8981\u7684\u57fa\u7840\u8bbe\u65bd\uff0c\u7279\u522b\u6709\u52a9\u4e8e\u8d44\u6e90\u532e\u4e4f\u8bed\u8a00\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.13107", "pdf": "https://arxiv.org/pdf/2508.13107", "abs": "https://arxiv.org/abs/2508.13107", "authors": ["Figarri Keisha", "Prince Singh", "Pallavi", "Dion Fernandes", "Aravindh Manivannan", "Ilham Wicaksono", "Faisal Ahmad"], "title": "All for law and law for all: Adaptive RAG Pipeline for Legal Research", "categories": ["cs.CL", "cs.IR", "F.2.2, H.3.3, I.2.7"], "comment": "submitted to NLLP 2025 Workshop", "summary": "Retrieval-Augmented Generation (RAG) mitigates hallucinations by grounding\nlarge language model outputs in cited sources, a capability that is especially\ncritical in the legal domain. We present an end-to-end RAG pipeline that\nrevisits and extends the LegalBenchRAG baseline with three targeted\nenhancements: (i) a context-aware query translator that disentangles document\nreferences from natural-language questions and adapts retrieval depth and\nresponse style based on expertise and specificity, (ii) open-source retrieval\nstrategies using SBERT and GTE embeddings that achieve substantial performance\ngains (improving Recall@K by 30-95\\% and Precision@K by $\\sim$2.5$\\times$ for\n$K>4$) while remaining cost-efficient, and (iii) a comprehensive evaluation and\ngeneration framework that combines RAGAS, BERTScore-F1, and ROUGE-Recall to\nassess semantic alignment and faithfulness across models and prompt designs.\nOur results show that carefully designed open-source pipelines can rival or\noutperform proprietary approaches in retrieval quality, while a custom\nlegal-grounded prompt consistently produces more faithful and contextually\nrelevant answers than baseline prompting. Taken together, these contributions\ndemonstrate the potential of task-aware, component-level tuning to deliver\nlegally grounded, reproducible, and cost-effective RAG systems for legal\nresearch assistance.", "AI": {"tldr": "\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u67e5\u8be2\u7ffb\u8bd1\u5668\u3001\u5f00\u6e90\u63d0\u53d6\u7b56\u7565\u548c\u7efc\u5408\u8bc4\u4f30\u6846\u67b6\u4e09\u9879\u6539\u8fdb\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u6548\u679c\u66f4\u597d\u3001\u6210\u672c\u66f4\u4f4e\u7684\u6cd5\u5f8b\u9886\u57dfRAG\u7cfb\u7edf\uff0c\u5728\u67e5\u8be2\u8d28\u91cf\u548c\u56de\u7b54\u5fe0\u5b9e\u6027\u65b9\u9762\u8d85\u8fc7\u4e86\u4e13\u6709\u65b9\u6848\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6cd5\u5f8b\u9886\u57df\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u901a\u8fc7\u57fa\u4e8e\u6765\u6e90\u7684RAG\u6280\u672f\u63d0\u9ad8\u8f93\u51fa\u7684\u53ef\u9760\u6027\u548c\u51c6\u786e\u6027\uff0c\u9002\u5408\u6cd5\u5f8b\u7814\u7a76\u7684\u9ad8\u8981\u6c42\u3002", "method": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u7ed3\u5408\u4e0a\u4e0b\u6587\u611f\u77e5\u67e5\u8be2\u7ffb\u8bd1\u5668\u7684\u7aef\u5230\u7aefRAG\u6d41\u6c34\u7ebf\uff0c\u4f7f\u7528SBERT\u548cGTE\u5f15\u64ce\u8fdb\u884c\u5f00\u6e90\u63d0\u53d6\uff0c\u5e76\u4f7f\u7528RAGAS\u3001BERTScore-F1\u548cROUGE-Recall\u8fdb\u884c\u7efc\u5408\u8bc4\u4f30\u3002", "result": "\u63d0\u53d6\u7b56\u7565\u5728Recall@K\u4e0a\u63d0\u534730-95%\uff0c\u5728Precision@K\u4e0a\u63d0\u5347\u7ea62.5\u500d\uff08K>4\u65f6\uff09\uff0c\u5f00\u6e90\u6d41\u6c34\u7ebf\u5728\u63d0\u53d6\u8d28\u91cf\u4e0a\u53ef\u4e0e\u4e13\u6709\u65b9\u6848\u76f8\u6bd4\u6216\u66f4\u4f18\uff0c\u81ea\u5b9a\u4e49\u6cd5\u5f8b\u63d0\u793a\u80fd\u4ea7\u751f\u66f4\u5fe0\u5b9e\u3001\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u7b54\u6848\u3002", "conclusion": "\u901a\u8fc7\u4efb\u52a1\u611f\u77e5\u7684\u7ec4\u4ef6\u7ea7\u8c03\u6574\uff0c\u53ef\u4ee5\u5efa\u7acb\u57fa\u4e8e\u6cd5\u5f8b\u57fa\u7840\u3001\u53ef\u590d\u73b0\u4e14\u6210\u672c\u6548\u76ca\u9ad8\u7684RAG\u7cfb\u7edf\uff0c\u4e3a\u6cd5\u5f8b\u7814\u7a76\u63d0\u4f9b\u6709\u6548\u652f\u6301\u3002"}}
{"id": "2508.13118", "pdf": "https://arxiv.org/pdf/2508.13118", "abs": "https://arxiv.org/abs/2508.13118", "authors": ["Zefang Liu", "Arman Anwar"], "title": "AutoBnB-RAG: Enhancing Multi-Agent Incident Response with Retrieval-Augmented Generation", "categories": ["cs.CL", "cs.CR"], "comment": null, "summary": "Incident response (IR) requires fast, coordinated, and well-informed\ndecision-making to contain and mitigate cyber threats. While large language\nmodels (LLMs) have shown promise as autonomous agents in simulated IR settings,\ntheir reasoning is often limited by a lack of access to external knowledge. In\nthis work, we present AutoBnB-RAG, an extension of the AutoBnB framework that\nincorporates retrieval-augmented generation (RAG) into multi-agent incident\nresponse simulations. Built on the Backdoors & Breaches (B&B) tabletop game\nenvironment, AutoBnB-RAG enables agents to issue retrieval queries and\nincorporate external evidence during collaborative investigations. We introduce\ntwo retrieval settings: one grounded in curated technical documentation\n(RAG-Wiki), and another using narrative-style incident reports (RAG-News). We\nevaluate performance across eight team structures, including newly introduced\nargumentative configurations designed to promote critical reasoning. To\nvalidate practical utility, we also simulate real-world cyber incidents based\non public breach reports, demonstrating AutoBnB-RAG's ability to reconstruct\ncomplex multi-stage attacks. Our results show that retrieval augmentation\nimproves decision quality and success rates across diverse organizational\nmodels. This work demonstrates the value of integrating retrieval mechanisms\ninto LLM-based multi-agent systems for cybersecurity decision-making.", "AI": {"tldr": "AutoBnB-RAG\u662f\u4e00\u4e2a\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7684\u591a\u4ee3\u7406\u4e8b\u4ef6\u54cd\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u5728AutoBnB\u6846\u67b6\u4e2d\u96c6\u6210RAG\u6280\u672f\uff0c\u4f7fLLM\u4ee3\u7406\u80fd\u591f\u5728\u7f51\u7edc\u5b89\u5168\u4e8b\u4ef6\u8c03\u67e5\u4e2d\u8bbf\u95ee\u5916\u90e8\u77e5\u8bc6\uff0c\u63d0\u9ad8\u51b3\u7b56\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709LLM\u5728\u6a21\u62df\u4e8b\u4ef6\u54cd\u5e94\u4e2d\u7684\u63a8\u7406\u80fd\u529b\u53d7\u9650\u4e8e\u7f3a\u4e4f\u5916\u90e8\u77e5\u8bc6\u8bbf\u95ee\uff0c\u9700\u8981\u589e\u5f3a\u5176\u68c0\u7d22\u80fd\u529b\u6765\u652f\u6301\u66f4\u660e\u667a\u7684\u7f51\u7edc\u5b89\u5168\u51b3\u7b56\u3002", "method": "\u5728Backdoors & Breaches\u684c\u9762\u6e38\u620f\u73af\u5883\u4e2d\u6784\u5efaAutoBnB-RAG\u6846\u67b6\uff0c\u5f15\u5165\u4e24\u79cd\u68c0\u7d22\u8bbe\u7f6e\uff1a\u57fa\u4e8e\u6280\u672f\u6587\u6863\u7684RAG-Wiki\u548c\u57fa\u4e8e\u4e8b\u4ef6\u62a5\u544a\u7684RAG-News\uff0c\u8bc4\u4f308\u79cd\u56e2\u961f\u7ed3\u6784\u3002", "result": "\u68c0\u7d22\u589e\u5f3a\u663e\u8457\u63d0\u9ad8\u4e86\u51b3\u7b56\u8d28\u91cf\u548c\u6210\u529f\u7387\uff0c\u80fd\u591f\u91cd\u5efa\u590d\u6742\u7684\u591a\u9636\u6bb5\u7f51\u7edc\u653b\u51fb\uff0c\u5728\u4e0d\u540c\u7ec4\u7ec7\u6a21\u578b\u4e2d\u5747\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u5c06\u68c0\u7d22\u673a\u5236\u96c6\u6210\u5230\u57fa\u4e8eLLM\u7684\u591a\u4ee3\u7406\u7cfb\u7edf\u4e2d\u5bf9\u7f51\u7edc\u5b89\u5168\u51b3\u7b56\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0cAutoBnB-RAG\u6846\u67b6\u5c55\u793a\u4e86\u8fd9\u79cd\u96c6\u6210\u5728\u5b9e\u9645\u7f51\u7edc\u4e8b\u4ef6\u54cd\u5e94\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.13124", "pdf": "https://arxiv.org/pdf/2508.13124", "abs": "https://arxiv.org/abs/2508.13124", "authors": ["Kawin Mayilvaghanan", "Siddhant Gupta", "Ayush Kumar"], "title": "Spot the BlindSpots: Systematic Identification and Quantification of Fine-Grained LLM Biases in Contact Center Summaries", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Abstractive summarization is a core application in contact centers, where\nLarge Language Models (LLMs) generate millions of summaries of call transcripts\ndaily. Despite their apparent quality, it remains unclear whether LLMs\nsystematically under- or over-attend to specific aspects of the transcript,\npotentially introducing biases in the generated summary. While prior work has\nexamined social and positional biases, the specific forms of bias pertinent to\ncontact center operations - which we term Operational Bias - have remained\nunexplored. To address this gap, we introduce BlindSpot, a framework built upon\na taxonomy of 15 operational bias dimensions (e.g., disfluency, speaker, topic)\nfor the identification and quantification of these biases. BlindSpot leverages\nan LLM as a zero-shot classifier to derive categorical distributions for each\nbias dimension in a pair of transcript and its summary. The bias is then\nquantified using two metrics: Fidelity Gap (the JS Divergence between\ndistributions) and Coverage (the percentage of source labels omitted). Using\nBlindSpot, we conducted an empirical study with 2500 real call transcripts and\ntheir summaries generated by 20 LLMs of varying scales and families (e.g., GPT,\nLlama, Claude). Our analysis reveals that biases are systemic and present\nacross all evaluated models, regardless of size or family.", "AI": {"tldr": "\u63d0\u51fa\u4e86BlindSpot\u6846\u67b6\u6765\u68c0\u6d4b\u548c\u91cf\u5316LLM\u5728\u5ba2\u670d\u4e2d\u5fc3\u6458\u8981\u751f\u6210\u4e2d\u7684\u64cd\u4f5c\u504f\u89c1\uff0c\u53d1\u73b0\u6240\u6709\u6a21\u578b\u90fd\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u89c1", "motivation": "LLM\u5728\u5ba2\u670d\u4e2d\u5fc3\u751f\u6210\u5927\u91cf\u901a\u8bdd\u6458\u8981\uff0c\u4f46\u5b58\u5728\u6f5c\u5728\u7684\u64cd\u4f5c\u504f\u89c1\uff08\u5982\u53e3\u5403\u3001\u8bf4\u8bdd\u8005\u3001\u8bdd\u9898\u7b49\u7ef4\u5ea6\uff09\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u7684\u68c0\u6d4b\u65b9\u6cd5", "method": "\u6784\u5efa15\u4e2a\u64cd\u4f5c\u504f\u89c1\u7ef4\u5ea6\u7684\u5206\u7c7b\u4f53\u7cfb\uff0c\u4f7f\u7528LLM\u4f5c\u4e3a\u96f6\u6837\u672c\u5206\u7c7b\u5668\u8ba1\u7b97\u8f6c\u5f55\u672c\u548c\u6458\u8981\u7684\u5206\u5e03\u5dee\u5f02\uff0c\u7528Fidelity Gap\u548cCoverage\u4e24\u4e2a\u6307\u6807\u91cf\u5316\u504f\u89c1", "result": "\u5bf92500\u4e2a\u771f\u5b9e\u901a\u8bdd\u548c20\u4e2a\u4e0d\u540c\u89c4\u6a21LLM\u751f\u6210\u7684\u6458\u8981\u8fdb\u884c\u5206\u6790\uff0c\u53d1\u73b0\u6240\u6709\u6a21\u578b\u65e0\u8bba\u5927\u5c0f\u6216\u5bb6\u65cf\u90fd\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u89c1", "conclusion": "\u64cd\u4f5c\u504f\u89c1\u5728LLM\u6458\u8981\u751f\u6210\u4e2d\u666e\u904d\u5b58\u5728\uff0c\u9700\u8981\u4e13\u95e8\u7684\u68c0\u6d4b\u6846\u67b6\u6765\u8bc6\u522b\u548c\u91cf\u5316\u8fd9\u4e9b\u504f\u89c1"}}
{"id": "2508.13130", "pdf": "https://arxiv.org/pdf/2508.13130", "abs": "https://arxiv.org/abs/2508.13130", "authors": ["Kareem Elozeiri", "Mervat Abassy", "Preslav Nakov", "Yuxia Wang"], "title": "MuDRiC: Multi-Dialect Reasoning for Arabic Commonsense Validation", "categories": ["cs.CL"], "comment": null, "summary": "Commonsense validation evaluates whether a sentence aligns with everyday\nhuman understanding, a critical capability for developing robust natural\nlanguage understanding systems. While substantial progress has been made in\nEnglish, the task remains underexplored in Arabic, particularly given its rich\nlinguistic diversity. Existing Arabic resources have primarily focused on\nModern Standard Arabic (MSA), leaving regional dialects underrepresented\ndespite their prevalence in spoken contexts. To bridge this gap, we present two\nkey contributions: (i) we introduce MuDRiC, an extended Arabic commonsense\ndataset incorporating multiple dialects, and (ii) a novel method adapting Graph\nConvolutional Networks (GCNs) to Arabic commonsense reasoning, which enhances\nsemantic relationship modeling for improved commonsense validation. Our\nexperimental results demonstrate that this approach achieves superior\nperformance in Arabic commonsense validation. Our work enhances Arabic natural\nlanguage understanding by providing both a foundational dataset and a novel\nmethod for handling its complex variations. To the best of our knowledge, we\nrelease the first Arabic multi-dialect commonsense reasoning dataset.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86MuDRiC\u6570\u636e\u96c6\u548c\u4e00\u79cd\u57fa\u4e8e\u56fe\u5370\u8c61\u5370\u5370\u5370\u7f51\u7edc\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u963f\u62c9\u4f2f\u8bed\u591a\u65b9\u8a00\u7684\u5e38\u8bc6\u9a8c\u8bc1\u4efb\u52a1\uff0c\u8865\u5145\u4e86\u73b0\u6709\u8d44\u6e90\u4e3b\u8981\u96c6\u4e2d\u5728\u73b0\u4ee3\u6807\u51c6\u963f\u62c9\u4f2f\u8bed\u7684\u7a7a\u767d\u3002", "motivation": "\u963f\u62c9\u4f2f\u8bed\u5e38\u8bc6\u9a8c\u8bc1\u4efb\u52a1\u5728\u82f1\u8bed\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u6b65\uff0c\u4f46\u5728\u963f\u62c9\u4f2f\u8bed\u4e2d\u7814\u7a76\u8f83\u5c11\uff0c\u5c24\u5176\u662f\u8003\u8651\u5230\u963f\u62c9\u4f2f\u8bed\u4e30\u5bcc\u7684\u8bed\u8a00\u591a\u6837\u6027\u3002\u73b0\u6709\u8d44\u6e90\u4e3b\u8981\u96c6\u4e2d\u5728\u73b0\u4ee3\u6807\u51c6\u963f\u62c9\u4f2f\u8bed(MSA)\uff0c\u800c\u5730\u65b9\u65b9\u8a00\u5728\u53e3\u8bed\u4e2d\u5e7f\u6cdb\u4f7f\u7528\u5374\u88ab\u5ffd\u89c6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u9002\u914d\u56fe\u5370\u8c61\u5370\u5370\u5370\u7f51\u7edc(GCNs)\u7528\u4e8e\u963f\u62c9\u4f2f\u8bed\u5e38\u8bc6\u63a8\u7406\uff0c\u901a\u8fc7\u589e\u5f3a\u8bed\u4e49\u5173\u7cfb\u5efa\u6a21\u6765\u6539\u5584\u5e38\u8bc6\u9a8c\u8bc1\u7684\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u963f\u62c9\u4f2f\u8bed\u5e38\u8bc6\u9a8c\u8bc1\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u901a\u8fc7\u63d0\u4f9b\u57fa\u7840\u6570\u636e\u96c6\u548c\u65b0\u7684\u65b9\u6cd5\uff0c\u589e\u5f3a\u4e86\u963f\u62c9\u4f2f\u8bed\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u80fd\u529b\uff0c\u80fd\u591f\u5904\u7406\u5176\u590d\u6742\u7684\u8bed\u8a00\u53d8\u4f53\u3002\u8fd9\u662f\u7b2c\u4e00\u4e2a\u963f\u62c9\u8bed\u591a\u65b9\u8a00\u5e38\u8bc6\u63a8\u7406\u6570\u636e\u96c6\u3002"}}
{"id": "2508.13131", "pdf": "https://arxiv.org/pdf/2508.13131", "abs": "https://arxiv.org/abs/2508.13131", "authors": ["Dara Bahri", "John Wieting"], "title": "Improving Detection of Watermarked Language Models", "categories": ["cs.CL", "cs.LG", "stat.ML"], "comment": null, "summary": "Watermarking has recently emerged as an effective strategy for detecting the\ngenerations of large language models (LLMs). The strength of a watermark\ntypically depends strongly on the entropy afforded by the language model and\nthe set of input prompts. However, entropy can be quite limited in practice,\nespecially for models that are post-trained, for example via instruction tuning\nor reinforcement learning from human feedback (RLHF), which makes detection\nbased on watermarking alone challenging. In this work, we investigate whether\ndetection can be improved by combining watermark detectors with non-watermark\nones. We explore a number of hybrid schemes that combine the two, observing\nperformance gains over either class of detector under a wide range of\nexperimental conditions.", "AI": {"tldr": "\u6c34\u5370\u68c0\u6d4b\u5728\u8bed\u8a00\u6a21\u578b\u4ea7\u51fa\u68c0\u6d4b\u4e2d\u6548\u679c\u53d7\u5230\u6a21\u578b\u71b5\u7684\u9650\u5236\uff0c\u672c\u6587\u63a2\u7d22\u901a\u8fc7\u7ed3\u5408\u6c34\u5370\u68c0\u6d4b\u5668\u548c\u975e\u6c34\u5370\u68c0\u6d4b\u5668\u7684\u6df7\u5408\u65b9\u6848\u6765\u63d0\u9ad8\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u6307\u4ee4\u5faa\u73af\u8c03\u6574\u6216RLHF\u7b49\u540e\u8bad\u7ec3\u6a21\u578b\u7684\u71b5\u8f83\u4f4e\uff0c\u5355\u7eaf\u4f9d\u9760\u6c34\u5370\u68c0\u6d4b\u6548\u679c\u6709\u9650\uff0c\u9700\u8981\u63d0\u9ad8\u68c0\u6d4b\u80fd\u529b\u3002", "method": "\u63a2\u7d22\u591a\u79cd\u6df7\u5408\u68c0\u6d4b\u65b9\u6848\uff0c\u5c06\u6c34\u5370\u68c0\u6d4b\u5668\u4e0e\u975e\u6c34\u5370\u68c0\u6d4b\u5668\u7ed3\u5408\u4f7f\u7528\u3002", "result": "\u5728\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u6761\u4ef6\u4e0b\uff0c\u6df7\u5408\u68c0\u6d4b\u65b9\u6848\u5728\u6027\u80fd\u4e0a\u8d85\u8fc7\u4e86\u4efb\u4f55\u5355\u4e00\u7c7b\u578b\u68c0\u6d4b\u5668\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u4e0d\u540c\u68c0\u6d4b\u65b9\u6cd5\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u9ad8\u5bf9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5185\u5bb9\u7684\u68c0\u6d4b\u6548\u679c\u3002"}}
{"id": "2508.13141", "pdf": "https://arxiv.org/pdf/2508.13141", "abs": "https://arxiv.org/abs/2508.13141", "authors": ["Pranjal Aggarwal", "Seungone Kim", "Jack Lanchantin", "Sean Welleck", "Jason Weston", "Ilia Kulikov", "Swarnadeep Saha"], "title": "OptimalThinkingBench: Evaluating Over and Underthinking in LLMs", "categories": ["cs.CL", "cs.LG"], "comment": "26 pages, 6 tables, 10 figures", "summary": "Thinking LLMs solve complex tasks at the expense of increased compute and\noverthinking on simpler problems, while non-thinking LLMs are faster and\ncheaper but underthink on harder reasoning problems. This has led to the\ndevelopment of separate thinking and non-thinking LLM variants, leaving the\nonus of selecting the optimal model for each query on the end user. In this\nwork, we introduce OptimalThinkingBench, a unified benchmark that jointly\nevaluates overthinking and underthinking in LLMs and also encourages the\ndevelopment of optimally-thinking models that balance performance and\nefficiency. Our benchmark comprises two sub-benchmarks: OverthinkingBench,\nfeaturing simple queries in 72 domains, and UnderthinkingBench, containing 11\nchallenging reasoning tasks. Using novel thinking-adjusted accuracy metrics, we\nperform extensive evaluation of 33 different thinking and non-thinking models\nand show that no model is able to optimally think on our benchmark. Thinking\nmodels often overthink for hundreds of tokens on the simplest user queries\nwithout improving performance. In contrast, large non-thinking models\nunderthink, often falling short of much smaller thinking models. We further\nexplore several methods to encourage optimal thinking, but find that these\napproaches often improve on one sub-benchmark at the expense of the other,\nhighlighting the need for better unified and optimal models in the future.", "AI": {"tldr": "OptimalThinkingBench\u662f\u4e00\u4e2a\u7edf\u4e00\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30LLMs\u7684\u8fc7\u5ea6\u601d\u8003\uff08overthinking\uff09\u548c\u601d\u8003\u4e0d\u8db3\uff08underthinking\uff09\u95ee\u9898\uff0c\u5305\u542b72\u4e2a\u7b80\u5355\u67e5\u8be2\u9886\u57df\u7684OverthinkingBench\u548c11\u4e2a\u6311\u6218\u6027\u63a8\u7406\u4efb\u52a1\u7684UnderthinkingBench\u4e24\u4e2a\u5b50\u57fa\u51c6\u3002", "motivation": "\u73b0\u6709\u7684\u601d\u8003\u578bLLMs\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u8868\u73b0\u66f4\u597d\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u5bf9\u7b80\u5355\u95ee\u9898\u8fc7\u5ea6\u601d\u8003\uff0c\u800c\u975e\u601d\u8003\u578bLLMs\u867d\u7136\u66f4\u5feb\u66f4\u4fbf\u5b9c\u4f46\u5728\u56f0\u96be\u63a8\u7406\u95ee\u9898\u4e0a\u601d\u8003\u4e0d\u8db3\uff0c\u9700\u8981\u7528\u6237\u81ea\u884c\u9009\u62e9\u5408\u9002\u6a21\u578b\u3002", "method": "\u5f00\u53d1\u4e86\u5305\u542b\u4e24\u4e2a\u5b50\u57fa\u51c6\u7684\u7edf\u4e00\u8bc4\u6d4b\u6846\u67b6\uff0c\u4f7f\u7528\u65b0\u9896\u7684\u601d\u8003\u8c03\u6574\u51c6\u786e\u7387\u6307\u6807\uff0c\u5bf933\u79cd\u4e0d\u540c\u601d\u8003\u548c\u975e\u601d\u8003\u6a21\u578b\u8fdb\u884c\u4e86\u5e7f\u6cdb\u8bc4\u4f30\u3002", "result": "\u6ca1\u6709\u6a21\u578b\u80fd\u5728\u8be5\u57fa\u51c6\u4e0a\u5b9e\u73b0\u6700\u4f18\u601d\u8003\u3002\u601d\u8003\u578b\u6a21\u578b\u5728\u7b80\u5355\u67e5\u8be2\u4e0a\u8fc7\u5ea6\u601d\u8003\u6570\u767e\u4e2atoken\u4f46\u6027\u80fd\u6ca1\u6709\u63d0\u5347\uff0c\u5927\u578b\u975e\u601d\u8003\u578b\u6a21\u578b\u601d\u8003\u4e0d\u8db3\uff0c\u8868\u73b0\u4e0d\u5982\u66f4\u5c0f\u7684\u601d\u8003\u578b\u6a21\u578b\u3002", "conclusion": "\u9700\u8981\u5f00\u53d1\u66f4\u597d\u7684\u7edf\u4e00\u6700\u4f18\u6a21\u578b\uff0c\u5f53\u524d\u65b9\u6cd5\u5f80\u5f80\u5728\u4e00\u4e2a\u5b50\u57fa\u51c6\u4e0a\u6539\u8fdb\u5374\u727a\u7272\u53e6\u4e00\u4e2a\u5b50\u57fa\u51c6\u7684\u6027\u80fd\u3002"}}
{"id": "2508.13144", "pdf": "https://arxiv.org/pdf/2508.13144", "abs": "https://arxiv.org/abs/2508.13144", "authors": ["David Heineman", "Valentin Hofmann", "Ian Magnusson", "Yuling Gu", "Noah A. Smith", "Hannaneh Hajishirzi", "Kyle Lo", "Jesse Dodge"], "title": "Signal and Noise: A Framework for Reducing Uncertainty in Language Model Evaluation", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Developing large language models is expensive and involves making decisions\nwith small experiments, typically by evaluating on large, multi-task evaluation\nsuites. In this work, we analyze specific properties which make a benchmark\nmore reliable for such decisions, and interventions to design higher-quality\nevaluation benchmarks. We introduce two key metrics that show differences in\ncurrent benchmarks: signal, a benchmark's ability to separate better models\nfrom worse models, and noise, a benchmark's sensitivity to random variability\nbetween training steps. We demonstrate that benchmarks with a better\nsignal-to-noise ratio are more reliable when making decisions at small scale,\nand those with less noise have lower scaling law prediction error. These\nresults suggest that improving signal or noise will lead to more useful\nbenchmarks, so we introduce three interventions designed to directly affect\nsignal or noise. For example, we propose that switching to a metric that has\nbetter signal and noise (e.g., perplexity rather than accuracy) leads to better\nreliability and improved scaling law error. We also find that filtering noisy\nsubtasks, to improve an aggregate signal-to-noise ratio, leads to more reliable\nmulti-task evaluations. We also find that averaging the output of a model's\nintermediate checkpoints to reduce noise leads to consistent improvements. We\nconclude by recommending that those creating new benchmarks, or selecting which\nexisting benchmarks to use, aim for high signal and low noise. We use 30\nbenchmarks for these experiments, and 375 open-weight language models from 60M\nto 32B parameters, resulting in a new, publicly available dataset of 900K\nevaluation benchmark results, totaling 200M instances.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u5206\u6790\u4e86\u8bc4\u4f30\u57fa\u51c6\u7684\u4fe1\u53f7\u548c\u566a\u97f3\u7279\u6027\uff0c\u63d0\u51fa\u4e86\u4e09\u79cd\u63d0\u9ad8\u57fa\u51c6\u8d28\u91cf\u7684\u5e72\u9884\u65b9\u6cd5\uff0c\u5e76\u5efa\u8bae\u9009\u62e9\u9ad8\u4fe1\u53f7\u4f4e\u566a\u97f3\u7684\u8bc4\u4f30\u57fa\u51c6\u6765\u63d0\u9ad8\u6a21\u578b\u51b3\u7b56\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5f00\u53d1\u6210\u672c\u6602\u8d35\uff0c\u9700\u8981\u901a\u8fc7\u5c0f\u89c4\u6a21\u5b9e\u9a8c\u505a\u51fa\u51b3\u7b56\u3002\u4f46\u5f53\u524d\u7684\u591a\u4efb\u52a1\u8bc4\u4f30\u57fa\u51c6\u5728\u53ef\u9760\u6027\u65b9\u9762\u5b58\u5728\u95ee\u9898\uff0c\u5f71\u54cd\u4e86\u5c0f\u89c4\u6a21\u5b9e\u9a8c\u7684\u51b3\u7b56\u8d28\u91cf\u3002", "method": "\u5f15\u5165\u4e86\u4e24\u4e2a\u5173\u952e\u6307\u6807\uff1a\u4fe1\u53f7\uff08benchmark\u533a\u5206\u6a21\u578b\u597d\u574f\u7684\u80fd\u529b\uff09\u548c\u566a\u97f3\uff08benchmark\u5bf9\u8bad\u7ec3\u6b65\u9a7f\u968f\u673a\u53d8\u5316\u7684\u654f\u611f\u6027\uff09\u3002\u63d0\u51fa\u4e86\u4e09\u79cd\u5e72\u9884\u65b9\u6cd5\uff1a\u6539\u7528\u66f4\u597d\u7684\u6307\u6807\uff08\u5982\u7528perplexity\u66ff\u4ee3accuracy\uff09\u3001\u8fc7\u6ee4\u566a\u97f3\u5b50\u4efb\u52a1\u3001\u5e73\u5747\u4e2d\u95f4checkpoint\u8f93\u51fa\u6765\u964d\u566a\u3002\u57fa\u4e8e30\u4e2a\u57fa\u51c6\u548c375\u4e2a\u6a21\u578b\u7684900K\u8bc4\u4f30\u7ed3\u679c\u8fdb\u884c\u5b9e\u9a8c\u5206\u6790\u3002", "result": "\u8bc1\u660e\u4e86\u4fe1\u53f7-\u566a\u97f3\u6bd4\u66f4\u597d\u7684benchmark\u5728\u5c0f\u89c4\u6a21\u5b9e\u9a8c\u4e2d\u66f4\u53ef\u9760\uff0c\u4e14\u566a\u97f3\u66f4\u4f4e\u7684benchmark\u6709\u66f4\u4f4e\u7684\u7f29\u653e\u5f8b\u9884\u6d4b\u9519\u8bef\u3002\u4e09\u79cd\u5e72\u9884\u65b9\u6cd5\u90fd\u80fd\u6709\u6548\u63d0\u9ad8\u8bc4\u4f30\u7684\u53ef\u9760\u6027\u548c\u51c6\u786e\u6027\u3002", "conclusion": "\u5efa\u8bae\u5728\u521b\u5efa\u65b0\u57fa\u51c6\u6216\u9009\u62e9\u73b0\u6709\u57fa\u51c6\u65f6\uff0c\u5e94\u8be5\u9009\u62e9\u5177\u6709\u9ad8\u4fe1\u53f7\u548c\u4f4e\u566a\u97f3\u7279\u6027\u7684benchmark\uff0c\u8fd9\u6837\u53ef\u4ee5\u63d0\u9ad8\u5c0f\u89c4\u6a21\u5b9e\u9a8c\u7684\u51b3\u7b56\u8d28\u91cf\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2508.13152", "pdf": "https://arxiv.org/pdf/2508.13152", "abs": "https://arxiv.org/abs/2508.13152", "authors": ["Xin Chen", "Junchao Wu", "Shu Yang", "Runzhe Zhan", "Zeyu Wu", "Ziyang Luo", "Di Wang", "Min Yang", "Lidia S. Chao", "Derek F. Wong"], "title": "RepreGuard: Detecting LLM-Generated Text by Revealing Hidden Representation Patterns", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to TACL 2025. This version is a pre-MIT Press publication\n  version", "summary": "Detecting content generated by large language models (LLMs) is crucial for\npreventing misuse and building trustworthy AI systems. Although existing\ndetection methods perform well, their robustness in out-of-distribution (OOD)\nscenarios is still lacking. In this paper, we hypothesize that, compared to\nfeatures used by existing detection methods, the internal representations of\nLLMs contain more comprehensive and raw features that can more effectively\ncapture and distinguish the statistical pattern differences between\nLLM-generated texts (LGT) and human-written texts (HWT). We validated this\nhypothesis across different LLMs and observed significant differences in neural\nactivation patterns when processing these two types of texts. Based on this, we\npropose RepreGuard, an efficient statistics-based detection method.\nSpecifically, we first employ a surrogate model to collect representation of\nLGT and HWT, and extract the distinct activation feature that can better\nidentify LGT. We can classify the text by calculating the projection score of\nthe text representations along this feature direction and comparing with a\nprecomputed threshold. Experimental results show that RepreGuard outperforms\nall baselines with average 94.92% AUROC on both in-distribution (ID) and OOD\nscenarios, while also demonstrating robust resilience to various text sizes and\nmainstream attacks. Data and code are publicly available at:\nhttps://github.com/NLP2CT/RepreGuard", "AI": {"tldr": "RepreGuard\u662f\u4e00\u79cd\u57fa\u4e8eLLM\u5185\u90e8\u8868\u793a\u7edf\u8ba1\u7279\u5f81\u7684\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5728\u5206\u5e03\u5185\u5916\u573a\u666f\u4e0b\u90fd\u80fd\u6709\u6548\u533a\u5206AI\u751f\u6210\u6587\u672c\u548c\u4eba\u7c7b\u5199\u4f5c\u6587\u672c\uff0c\u5e73\u5747AUROC\u8fbe\u523094.92%", "motivation": "\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u5728\u5206\u5e03\u5916\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u6027\u4e0d\u8db3\uff0cLLM\u5185\u90e8\u8868\u793a\u5305\u542b\u66f4\u5168\u9762\u548c\u539f\u59cb\u7684\u7279\u5f81\uff0c\u80fd\u66f4\u597d\u5730\u533a\u5206AI\u751f\u6210\u6587\u672c\u548c\u4eba\u7c7b\u5199\u4f5c\u6587\u672c\u7684\u7edf\u8ba1\u6a21\u5f0f\u5dee\u5f02", "method": "\u4f7f\u7528\u4ee3\u7406\u6a21\u578b\u6536\u96c6\u4e24\u79cd\u6587\u672c\u7684\u8868\u793a\uff0c\u63d0\u53d6\u80fd\u66f4\u597d\u8bc6\u522bAI\u751f\u6210\u6587\u672c\u7684\u6fc0\u6d3b\u7279\u5f81\uff0c\u901a\u8fc7\u8ba1\u7b97\u6587\u672c\u8868\u793a\u5728\u8be5\u7279\u5f81\u65b9\u5411\u4e0a\u7684\u6295\u5f71\u5206\u6570\u5e76\u4e0e\u9884\u8ba1\u7b97\u9608\u503c\u6bd4\u8f83\u8fdb\u884c\u5206\u7c7b", "result": "\u5728\u5206\u5e03\u5185\u5916\u573a\u666f\u4e0b\u5e73\u5747AUROC\u8fbe\u523094.92%\uff0c\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5bf9\u4e0d\u540c\u6587\u672c\u957f\u5ea6\u548c\u4e3b\u6d41\u653b\u51fb\u5177\u6709\u9c81\u68d2\u6027", "conclusion": "LLM\u5185\u90e8\u8868\u793a\u786e\u5b9e\u5305\u542b\u66f4\u6709\u6548\u7684\u533a\u5206\u7279\u5f81\uff0cRepreGuard\u65b9\u6cd5\u5728\u68c0\u6d4bAI\u751f\u6210\u6587\u672c\u65b9\u9762\u8868\u73b0\u51fa\u8272\u4e14\u9c81\u68d2"}}
{"id": "2508.11661", "pdf": "https://arxiv.org/pdf/2508.11661", "abs": "https://arxiv.org/abs/2508.11661", "authors": ["Ziyi Cao", "Qingyi Si", "Jingbin Zhang", "Bingquan Liu"], "title": "Sparse Attention across Multiple-context KV Cache", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Large language models face significant cost challenges in long-sequence\ninference. To address this, reusing historical Key-Value (KV) Cache for\nimproved inference efficiency has become a mainstream approach. Recent advances\nfurther enhance throughput by sparse attention mechanisms to select the most\nrelevant KV Cache, thereby reducing sequence length. However, such techniques\nare limited to single-context scenarios, where historical KV Cache is computed\nsequentially with causal-attention dependencies. In retrieval-augmented\ngeneration (RAG) scenarios, where retrieved documents as context are unknown\nbeforehand, each document's KV Cache is computed and stored independently\n(termed multiple-context KV Cache), lacking cross-attention between contexts.\nThis renders existing methods ineffective. Although prior work partially\nrecomputes multiple-context KV Cache to mitigate accuracy loss from missing\ncross-attention, it requires retaining all KV Cache throughout, failing to\nreduce memory overhead. This paper presents SamKV, the first exploration of\nattention sparsification for multiple-context KV Cache. Specifically, SamKV\ntakes into account the complementary information of other contexts when\nsparsifying one context, and then locally recomputes the sparsified\ninformation. Experiments demonstrate that our method compresses sequence length\nto 15% without accuracy degradation compared with full-recompuation baselines,\nsignificantly boosting throughput in multi-context RAG scenarios.", "AI": {"tldr": "SamKV\u662f\u9996\u4e2a\u9488\u5bf9\u591a\u4e0a\u4e0b\u6587KV\u7f13\u5b58\u8fdb\u884c\u6ce8\u610f\u529b\u7a00\u758f\u5316\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8003\u8651\u5176\u4ed6\u4e0a\u4e0b\u6587\u7684\u4e92\u8865\u4fe1\u606f\u8fdb\u884c\u7a00\u758f\u5316\u5e76\u5c40\u90e8\u91cd\u8ba1\u7b97\uff0c\u5728RAG\u573a\u666f\u4e2d\u5c06\u5e8f\u5217\u957f\u5ea6\u538b\u7f29\u81f315%\u4e14\u4e0d\u635f\u5931\u7cbe\u5ea6", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u957f\u5e8f\u5217\u63a8\u7406\u4e2d\u9762\u4e34\u663e\u8457\u6210\u672c\u6311\u6218\uff0c\u73b0\u6709\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\u4ec5\u9650\u4e8e\u5355\u4e0a\u4e0b\u6587\u573a\u666f\uff0c\u65e0\u6cd5\u5904\u7406RAG\u4e2d\u591a\u4e0a\u4e0b\u6587KV\u7f13\u5b58\u7f3a\u4e4f\u4ea4\u53c9\u6ce8\u610f\u529b\u7684\u95ee\u9898", "method": "SamKV\u65b9\u6cd5\u5728\u7a00\u758f\u5316\u4e00\u4e2a\u4e0a\u4e0b\u6587\u65f6\u8003\u8651\u5176\u4ed6\u4e0a\u4e0b\u6587\u7684\u4e92\u8865\u4fe1\u606f\uff0c\u7136\u540e\u5c40\u90e8\u91cd\u8ba1\u7b97\u88ab\u7a00\u758f\u5316\u7684\u4fe1\u606f", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5c06\u5e8f\u5217\u957f\u5ea6\u538b\u7f29\u523015%\uff0c\u76f8\u6bd4\u5b8c\u5168\u91cd\u8ba1\u7b97\u57fa\u7ebf\u6ca1\u6709\u7cbe\u5ea6\u635f\u5931\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u4e0a\u4e0b\u6587RAG\u573a\u666f\u7684\u541e\u5410\u91cf", "conclusion": "SamKV\u6210\u529f\u89e3\u51b3\u4e86\u591a\u4e0a\u4e0b\u6587KV\u7f13\u5b58\u7684\u9ad8\u6548\u538b\u7f29\u95ee\u9898\uff0c\u4e3aRAG\u573a\u666f\u4e2d\u7684\u957f\u5e8f\u5217\u63a8\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.11667", "pdf": "https://arxiv.org/pdf/2508.11667", "abs": "https://arxiv.org/abs/2508.11667", "authors": ["Bryan E. Tuck", "Rakesh M. Verma"], "title": "Assessing Representation Stability for Transformer Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "19 pages, 19 figures, 8 tables. Code available at\n  https://github.com/ReDASers/representation-stability", "summary": "Adversarial text attacks remain a persistent threat to transformer models,\nyet existing defenses are typically attack-specific or require costly model\nretraining. We introduce Representation Stability (RS), a model-agnostic\ndetection framework that identifies adversarial examples by measuring how\nembedding representations change when important words are masked. RS first\nranks words using importance heuristics, then measures embedding sensitivity to\nmasking top-k critical words, and processes the resulting patterns with a\nBiLSTM detector. Experiments show that adversarially perturbed words exhibit\ndisproportionately high masking sensitivity compared to naturally important\nwords. Across three datasets, three attack types, and two victim models, RS\nachieves over 88% detection accuracy and demonstrates competitive performance\ncompared to existing state-of-the-art methods, often at lower computational\ncost. Using Normalized Discounted Cumulative Gain (NDCG) to measure\nperturbation identification quality, we reveal that gradient-based ranking\noutperforms attention and random selection approaches, with identification\nquality correlating with detection performance for word-level attacks. RS also\ngeneralizes well to unseen datasets, attacks, and models without retraining,\nproviding a practical solution for adversarial text detection.", "AI": {"tldr": "\u63d0\u51fa\u4e86Representation Stability (RS)\u6846\u67b6\uff0c\u901a\u8fc7\u6d4b\u91cf\u63a9\u7801\u91cd\u8981\u8bcd\u65f6\u5d4c\u5165\u8868\u793a\u7684\u53d8\u5316\u6765\u68c0\u6d4b\u5bf9\u6297\u6587\u672c\u653b\u51fb\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u653b\u51fb\u7c7b\u578b\u4e0a\u8fbe\u523088%\u4ee5\u4e0a\u7684\u68c0\u6d4b\u51c6\u786e\u7387\u3002", "motivation": "\u5bf9\u6297\u6587\u672c\u653b\u51fb\u5bf9transformer\u6a21\u578b\u6784\u6210\u6301\u7eed\u5a01\u80c1\uff0c\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u8981\u4e48\u9488\u5bf9\u7279\u5b9a\u653b\u51fb\uff0c\u8981\u4e48\u9700\u8981\u6602\u8d35\u7684\u6a21\u578b\u91cd\u65b0\u8bad\u7ec3\u6210\u672c\u3002", "method": "RS\u6846\u67b6\u9996\u5148\u4f7f\u7528\u91cd\u8981\u6027\u542f\u53d1\u5f0f\u5bf9\u5355\u8bcd\u8fdb\u884c\u6392\u5e8f\uff0c\u7136\u540e\u6d4b\u91cf\u63a9\u7801\u524dk\u4e2a\u5173\u952e\u8bcd\u65f6\u7684\u5d4c\u5165\u654f\u611f\u6027\uff0c\u6700\u540e\u4f7f\u7528BiLSTM\u68c0\u6d4b\u5668\u5904\u7406\u5f97\u5230\u7684\u6a21\u5f0f\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u3001\u4e09\u79cd\u653b\u51fb\u7c7b\u578b\u548c\u4e24\u4e2a\u53d7\u5bb3\u6a21\u578b\u4e0a\uff0cRS\u8fbe\u5230\u8d85\u8fc788%\u7684\u68c0\u6d4b\u51c6\u786e\u7387\uff0c\u8ba1\u7b97\u6210\u672c\u8f83\u4f4e\uff0c\u4e14\u80fd\u5f88\u597d\u5730\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u6570\u636e\u96c6\u3001\u653b\u51fb\u548c\u6a21\u578b\u3002", "conclusion": "RS\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u5bf9\u6297\u6587\u672c\u68c0\u6d4b\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u6709\u6548\u68c0\u6d4b\u591a\u79cd\u5bf9\u6297\u653b\u51fb\uff0c\u68af\u5ea6\u57fa\u6392\u540d\u65b9\u6cd5\u5728\u6270\u52a8\u8bc6\u522b\u8d28\u91cf\u4e0a\u8868\u73b0\u6700\u4f73\u3002"}}
{"id": "2508.11710", "pdf": "https://arxiv.org/pdf/2508.11710", "abs": "https://arxiv.org/abs/2508.11710", "authors": ["Hael Abdulhakim Ali Humran", "Ferdi Sonmez"], "title": "Code Vulnerability Detection Across Different Programming Languages with AI Models", "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": null, "summary": "Security vulnerabilities present in a code that has been written in diverse\nprogramming languages are among the most critical yet complicated aspects of\nsource code to detect. Static analysis tools based on rule-based patterns\nusually do not work well at detecting the context-dependent bugs and lead to\nhigh false positive rates. Recent developments in artificial intelligence,\nspecifically the use of transformer-based models like CodeBERT and CodeLlama,\nprovide light to this problem, as they show potential in finding such flaws\nbetter. This paper presents the implementations of these models on various\ndatasets of code vulnerability, showing how off-the-shelf models can\nsuccessfully produce predictive capacity in models through dynamic fine-tuning\nof the models on vulnerable and safe code fragments. The methodology comprises\nthe gathering of the dataset, normalization of the language, fine-tuning of the\nmodel, and incorporation of ensemble learning and explainable AI. Experiments\nshow that a well-trained CodeBERT can be as good as or even better than some\nexisting static analyzers in terms of accuracy greater than 97%. Further study\nhas indicated that although language models can achieve close-to-perfect\nrecall, the precision can decrease. A solution to this is given by hybrid\nmodels and validation procedures, which will reduce false positives. According\nto the results, the AI-based solutions generalize to different programming\nlanguages and classes of vulnerability. Nevertheless, robustness,\ninterpretability, and deployment readiness are still being developed. The\nresults illustrate the probabilities that AI will enhance the trustworthiness\nin the usability and scalability of machine-learning-based detectors of\nvulnerabilities.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u4e86\u4f7f\u7528\u8f6c\u6362\u5668\u6a21\u578b\uff08CodeBERT\u548cCodeLlama\uff09\u8fdb\u884c\u4ee3\u7801\u6f0f\u6d1e\u68c0\u6d4b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ec6\u8c03\u6a21\u578b\u5728\u5b58\u5728\u6f0f\u6d1e\u548c\u5b89\u5168\u4ee3\u7801\u7247\u6bb5\u4e0a\u5b9e\u73b0\u4e86\u8d85\u8fc797%\u7684\u51c6\u786e\u7387\uff0c\u663e\u793a\u4e86AI\u5728\u63d0\u5347\u6f0f\u6d1e\u68c0\u6d4b\u53ef\u9760\u6027\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u7684\u6f5c\u529b\u3002", "motivation": "\u4f20\u7edf\u7684\u57fa\u4e8e\u89c4\u5219\u7684\u9759\u6001\u5206\u6790\u5de5\u5177\u5728\u68c0\u6d4b\u4e0a\u4e0b\u6587\u4f9d\u8d56\u6027\u9519\u8bef\u65f6\u6548\u679c\u4e0d\u4f73\u4e14\u5bfc\u81f4\u9ad8\u5047\u6b63\u7387\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684AI\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u6570\u636e\u96c6\u6536\u96c6\u3001\u8bed\u8a00\u6807\u51c6\u5316\u3001\u6a21\u578b\u7ec6\u8c03\uff0c\u4ee5\u53ca\u96c6\u6210\u5b66\u4e60\u548c\u53ef\u89e3\u91ca\u6027AI\u6280\u672f\u7684\u7ed3\u5408\u3002\u4f7f\u7528CodeBERT\u548cCodeLlama\u6a21\u578b\u5728\u591a\u79cd\u4ee3\u7801\u6f0f\u6d1e\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u7ec6\u8c03\u540e\u7684CodeBERT\u6a21\u578b\u51c6\u786e\u7387\u8d85\u8fc797%\uff0c\u751a\u81f3\u8d85\u8fc7\u67d0\u4e9b\u73b0\u6709\u9759\u6001\u5206\u6790\u5668\u3002\u6a21\u578b\u80fd\u591f\u5b8c\u7f8e\u8bc6\u522b\u6f0f\u6d1e\uff08\u63a5\u8fd1\u5b8c\u7f8e\u7684\u53ec\u56de\u7387\uff09\uff0c\u4f46\u7cbe\u786e\u5ea6\u53ef\u80fd\u4f1a\u4e0b\u964d\u3002\u6df7\u5408\u6a21\u578b\u548c\u9a8c\u8bc1\u6d41\u7a0b\u80fd\u6709\u6548\u51cf\u5c11\u5047\u6b63\u7387\u3002", "conclusion": "AI\u57fa\u4e8e\u6f0f\u6d1e\u68c0\u6d4b\u65b9\u6848\u80fd\u591f\u826f\u597d\u5730\u6e10\u8fdb\u5230\u4e0d\u540c\u7f16\u7a0b\u8bed\u8a00\u548c\u6f0f\u6d1e\u7c7b\u578b\uff0c\u663e\u793a\u4e86\u5728\u63d0\u5347\u6f0f\u6d1e\u68c0\u6d4b\u7684\u53ef\u9760\u6027\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u7684\u6f5c\u529b\u3002\u4f46\u5728\u7a33\u5065\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u90e8\u7f72\u51c6\u5907\u5ea6\u65b9\u9762\u4ecd\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2508.11737", "pdf": "https://arxiv.org/pdf/2508.11737", "abs": "https://arxiv.org/abs/2508.11737", "authors": ["Shiyin Lu", "Yang Li", "Yu Xia", "Yuwei Hu", "Shanshan Zhao", "Yanqing Ma", "Zhichao Wei", "Yinglun Li", "Lunhao Duan", "Jianshan Zhao", "Yuxuan Han", "Haijun Li", "Wanying Chen", "Junke Tang", "Chengkun Hou", "Zhixing Du", "Tianli Zhou", "Wenjie Zhang", "Huping Ding", "Jiahe Li", "Wen Li", "Gui Hu", "Yiliang Gu", "Siran Yang", "Jiamang Wang", "Hailong Sun", "Yibo Wang", "Hui Sun", "Jinlong Huang", "Yuping He", "Shengze Shi", "Weihong Zhang", "Guodong Zheng", "Junpeng Jiang", "Sensen Gao", "Yi-Feng Wu", "Sijia Chen", "Yuhui Chen", "Qing-Guo Chen", "Zhao Xu", "Weihua Luo", "Kaifu Zhang"], "title": "Ovis2.5 Technical Report", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "We present Ovis2.5, a successor to Ovis2 designed for native-resolution\nvisual perception and strong multimodal reasoning. Ovis2.5 integrates a\nnative-resolution vision transformer that processes images at their native,\nvariable resolutions, avoiding the degradation from fixed-resolution tiling and\npreserving both fine detail and global layout -- crucial for visually dense\ncontent like complex charts. To strengthen reasoning, we train the model to\nmove beyond linear chain-of-thought and perform reflection -- including\nself-checking and revision. This advanced capability is exposed as an optional\n\"thinking mode\" at inference time, allowing users to trade latency for enhanced\naccuracy on difficult inputs. The model is trained via a comprehensive\nfive-phase curriculum that progressively builds its skills. The process begins\nwith foundational visual and multimodal pretraining, advances through\nlarge-scale instruction tuning, and culminates in alignment and reasoning\nenhancement using DPO and GRPO. To scale these upgrades efficiently, we employ\nmultimodal data packing and hybrid parallelism, yielding a significant\nend-to-end speedup. We release two open-source models: Ovis2.5-9B and\nOvis2.5-2B. The latter continues the \"small model, big performance\" philosophy\nof Ovis2, making it ideal for resource-constrained, on-device scenarios. On the\nOpenCompass multimodal leaderboard, Ovis2.5-9B averages 78.3, marking a\nsubstantial improvement over its predecessor, Ovis2-8B, and achieving\nstate-of-the-art results among open-source MLLMs in the sub-40B parameter\nrange; Ovis2.5-2B scores 73.9, establishing SOTA for its size. Beyond aggregate\nscores, Ovis2.5 achieves leading results on STEM benchmarks, exhibits strong\ncapabilities on grounding and video tasks, and achieves open-source SOTA at its\nscale for complex chart analysis.", "AI": {"tldr": "Ovis2.5\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u539f\u751f\u5206\u8fa8\u7387\u89c6\u89c9\u5904\u7406\u548c\u53cd\u601d\u63a8\u7406\u80fd\u529b\uff0c\u5728OpenCompass\u6392\u884c\u699c\u4e0a\u53d6\u5f9778.3\u5206\uff0c\u572840B\u53c2\u6570\u4ee5\u4e0b\u5f00\u6e90\u6a21\u578b\u4e2d\u8fbe\u5230SOTA\u6c34\u5e73\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u56fa\u5b9a\u5206\u8fa8\u7387\u56fe\u50cf\u5904\u7406\u5bfc\u81f4\u7684\u7ec6\u8282\u4e22\u5931\u95ee\u9898\uff0c\u5e76\u589e\u5f3a\u6a21\u578b\u7684\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u89c6\u89c9\u5bc6\u96c6\u5185\u5bb9\uff08\u5982\u590d\u6742\u56fe\u8868\uff09\u7684\u5206\u6790\u3002", "method": "\u91c7\u7528\u539f\u751f\u5206\u8fa8\u7387\u89c6\u89c9Transformer\u5904\u7406\u53ef\u53d8\u5206\u8fa8\u7387\u56fe\u50cf\uff0c\u5f15\u5165\u53cd\u601d\u63a8\u7406\u673a\u5236\uff08\u5305\u62ec\u81ea\u68c0\u548c\u4fee\u8ba2\uff09\uff0c\u901a\u8fc7\u4e94\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\uff08\u57fa\u7840\u9884\u8bad\u7ec3\u3001\u6307\u4ee4\u5fae\u8c03\u3001DPO/GRPO\u5bf9\u9f50\uff09\u8fdb\u884c\u8bad\u7ec3\uff0c\u4f7f\u7528\u591a\u6a21\u6001\u6570\u636e\u6253\u5305\u548c\u6df7\u5408\u5e76\u884c\u6280\u672f\u63d0\u5347\u6548\u7387\u3002", "result": "Ovis2.5-9B\u5728OpenCompass\u4e0a\u5e73\u5747\u5f97\u520678.3\uff0c\u76f8\u6bd4\u524d\u4ee3Ovis2-8B\u6709\u663e\u8457\u63d0\u5347\uff1bOvis2.5-2B\u5f97\u520673.9\uff0c\u5728\u540c\u7b49\u89c4\u6a21\u6a21\u578b\u4e2d\u8fbe\u5230SOTA\u3002\u5728STEM\u57fa\u51c6\u6d4b\u8bd5\u3001\u63a5\u5730\u4efb\u52a1\u3001\u89c6\u9891\u4efb\u52a1\u548c\u590d\u6742\u56fe\u8868\u5206\u6790\u65b9\u9762\u5747\u53d6\u5f97\u9886\u5148\u7ed3\u679c\u3002", "conclusion": "Ovis2.5\u901a\u8fc7\u539f\u751f\u5206\u8fa8\u7387\u5904\u7406\u548c\u53cd\u601d\u63a8\u7406\u673a\u5236\uff0c\u5728\u591a\u6a21\u6001\u7406\u89e3\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u7279\u522b\u662f\u5728\u89c6\u89c9\u5bc6\u96c6\u5185\u5bb9\u5206\u6790\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u63d0\u4f9b\u4e86\u9ad8\u6027\u80fd\u7684\u5c0f\u578b\u6a21\u578b\u9009\u62e9\u3002"}}
{"id": "2508.11759", "pdf": "https://arxiv.org/pdf/2508.11759", "abs": "https://arxiv.org/abs/2508.11759", "authors": ["Peter Lindes", "Kaoutar Skiker"], "title": "Using Natural Language for Human-Robot Collaboration in the Real World", "categories": ["cs.RO", "cs.AI", "cs.CL"], "comment": "34 pages, 11 figures, 5 tables. Submitted for publication (2026) in\n  W.F. Lawless, Ranjeev Mittu, Shannon P. McGrarry, & Marco Brambilla (Eds.),\n  Generative AI Risks and Benefits within Human-Machine Teams, Elsevier,\n  Chapter 6", "summary": "We have a vision of a day when autonomous robots can collaborate with humans\nas assistants in performing complex tasks in the physical world. This vision\nincludes that the robots will have the ability to communicate with their human\ncollaborators using language that is natural to the humans. Traditional\nInteractive Task Learning (ITL) systems have some of this ability, but the\nlanguage they can understand is very limited. The advent of large language\nmodels (LLMs) provides an opportunity to greatly improve the language\nunderstanding of robots, yet integrating the language abilities of LLMs with\nrobots that operate in the real physical world is a challenging problem.\n  In this chapter we first review briefly a few commercial robot products that\nwork closely with humans, and discuss how they could be much better\ncollaborators with robust language abilities. We then explore how an AI system\nwith a cognitive agent that controls a physical robot at its core, interacts\nwith both a human and an LLM, and accumulates situational knowledge through its\nexperiences, can be a possible approach to reach that vision. We focus on three\nspecific challenges of having the robot understand natural language, and\npresent a simple proof-of-concept experiment using ChatGPT for each. Finally,\nwe discuss what it will take to turn these simple experiments into an\noperational system where LLM-assisted language understanding is a part of an\nintegrated robotic assistant that uses language to collaborate with humans.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u5347\u81ea\u4e3b\u673a\u5668\u4eba\u7684\u8bed\u8a00\u7406\u89e3\u80fd\u529b\uff0c\u4ee5\u5b9e\u73b0\u4eba\u673a\u81ea\u7136\u8bed\u8a00\u534f\u4f5c\u7684\u8fdc\u666f\u3002", "motivation": "\u5b9e\u73b0\u81ea\u4e3b\u673a\u5668\u4eba\u4e0e\u4eba\u7c7b\u5728\u7269\u7406\u4e16\u754c\u4e2d\u8fdb\u884c\u590d\u6742\u4efb\u52a1\u534f\u4f5c\uff0c\u9700\u8981\u673a\u5668\u4eba\u5177\u5907\u81ea\u7136\u8bed\u8a00\u6c9f\u901a\u80fd\u529b\u3002\u4f20\u7edf\u4ea4\u4e92\u5f0f\u4efb\u52a1\u5b66\u4e60\u7cfb\u7edf\u7684\u8bed\u8a00\u7406\u89e3\u80fd\u529b\u6709\u9650\uff0c\u800c\u5927\u8bed\u8a00\u6a21\u578b\u4e3a\u63d0\u5347\u673a\u5668\u4eba\u8bed\u8a00\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u673a\u9047\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ee5\u8ba4\u77e5\u4ee3\u7406\u4e3a\u6838\u5fc3\u7684AI\u7cfb\u7edf\u67b6\u6784\uff0c\u8be5\u4ee3\u7406\u63a7\u5236\u7269\u7406\u673a\u5668\u4eba\u3001\u4e0e\u4eba\u7c7b\u548cLLM\u4ea4\u4e92\u3001\u901a\u8fc7\u7ecf\u9a8c\u79ef\u7d2f\u60c5\u5883\u77e5\u8bc6\u3002\u4f7f\u7528ChatGPT\u8fdb\u884c\u4e86\u4e09\u4e2a\u7406\u89e3\u81ea\u7136\u8bed\u8a00\u6311\u6218\u7684\u6982\u5ff5\u9a8c\u8bc1\u5b9e\u9a8c\u3002", "result": "\u901a\u8fc7\u6982\u5ff5\u9a8c\u8bc1\u5b9e\u9a8c\u8bc1\u660e\u4e86LLM\u5728\u63d0\u5347\u673a\u5668\u4eba\u8bed\u8a00\u7406\u89e3\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u4e3a\u5efa\u7acb\u96c6\u6210LLM\u8bed\u8a00\u7406\u89e3\u80fd\u529b\u7684\u673a\u5668\u4eba\u534f\u52a9\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "\u5c06LLM\u96c6\u6210\u5230\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u53ef\u4ee5\u5927\u5927\u63d0\u5347\u4eba\u673a\u534f\u4f5c\u80fd\u529b\uff0c\u4f46\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u5982\u4f55\u5c06\u8fd9\u4e9b\u6982\u5ff5\u9a8c\u8bc1\u8f6c\u5316\u4e3a\u53ef\u64cd\u4f5c\u7684\u96c6\u6210\u7cfb\u7edf\uff0c\u5b9e\u73b0\u81ea\u7136\u8bed\u8a00\u9a71\u52a8\u7684\u673a\u5668\u4eba\u534f\u52a9\u5668\u3002"}}
{"id": "2508.11801", "pdf": "https://arxiv.org/pdf/2508.11801", "abs": "https://arxiv.org/abs/2508.11801", "authors": ["Ming Cheng", "Tong Wu", "Jiazhen Hu", "Jiaying Gong", "Hoda Eldardiry"], "title": "VideoAVE: A Multi-Attribute Video-to-Text Attribute Value Extraction Dataset and Benchmark Models", "categories": ["cs.CV", "cs.CL"], "comment": "5 pages, 2 figures, 5 tables, accepted in CIKM 2025", "summary": "Attribute Value Extraction (AVE) is important for structuring product\ninformation in e-commerce. However, existing AVE datasets are primarily limited\nto text-to-text or image-to-text settings, lacking support for product videos,\ndiverse attribute coverage, and public availability. To address these gaps, we\nintroduce VideoAVE, the first publicly available video-to-text e-commerce AVE\ndataset across 14 different domains and covering 172 unique attributes. To\nensure data quality, we propose a post-hoc CLIP-based Mixture of Experts\nfiltering system (CLIP-MoE) to remove the mismatched video-product pairs,\nresulting in a refined dataset of 224k training data and 25k evaluation data.\nIn order to evaluate the usability of the dataset, we further establish a\ncomprehensive benchmark by evaluating several state-of-the-art video vision\nlanguage models (VLMs) under both attribute-conditioned value prediction and\nopen attribute-value pair extraction tasks. Our results analysis reveals that\nvideo-to-text AVE remains a challenging problem, particularly in open settings,\nand there is still room for developing more advanced VLMs capable of leveraging\neffective temporal information. The dataset and benchmark code for VideoAVE are\navailable at: https://github.com/gjiaying/VideoAVE", "AI": {"tldr": "VideoAVE\u662f\u9996\u4e2a\u516c\u5f00\u7684\u89c6\u9891\u5230\u6587\u672c\u7535\u5546\u5c5e\u6027\u503c\u63d0\u53d6\u6570\u636e\u96c6\uff0c\u8986\u76d614\u4e2a\u9886\u57df\u548c172\u4e2a\u5c5e\u6027\uff0c\u5305\u542b224k\u8bad\u7ec3\u6570\u636e\u548c25k\u8bc4\u4f30\u6570\u636e\uff0c\u901a\u8fc7CLIP-MoE\u8fc7\u6ee4\u7cfb\u7edf\u786e\u4fdd\u6570\u636e\u8d28\u91cf\uff0c\u5e76\u5efa\u7acb\u4e86\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709AVE\u6570\u636e\u96c6\u4ec5\u9650\u4e8e\u6587\u672c\u5230\u6587\u672c\u6216\u56fe\u50cf\u5230\u6587\u672c\u8bbe\u7f6e\uff0c\u7f3a\u4e4f\u5bf9\u4ea7\u54c1\u89c6\u9891\u652f\u6301\u3001\u591a\u6837\u5316\u5c5e\u6027\u8986\u76d6\u548c\u516c\u5f00\u53ef\u7528\u6027\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u57fa\u4e8eCLIP\u7684\u4e13\u5bb6\u6df7\u5408\u8fc7\u6ee4\u7cfb\u7edf(CLIP-MoE)\u6765\u79fb\u9664\u4e0d\u5339\u914d\u7684\u89c6\u9891-\u4ea7\u54c1\u5bf9\uff0c\u786e\u4fdd\u6570\u636e\u8d28\u91cf\uff1b\u5efa\u7acb\u5305\u542b\u5c5e\u6027\u6761\u4ef6\u503c\u9884\u6d4b\u548c\u5f00\u653e\u5c5e\u6027\u503c\u5bf9\u63d0\u53d6\u4efb\u52a1\u7684\u7efc\u5408\u57fa\u51c6\u3002", "result": "\u89c6\u9891\u5230\u6587\u672cAVE\u4ecd\u7136\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5f00\u653e\u8bbe\u7f6e\u4e2d\uff0c\u73b0\u6709\u89c6\u9891\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5229\u7528\u6709\u6548\u65f6\u5e8f\u4fe1\u606f\u65b9\u9762\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "conclusion": "VideoAVE\u6570\u636e\u96c6\u586b\u8865\u4e86\u89c6\u9891\u5230\u6587\u672cAVE\u9886\u57df\u7684\u7a7a\u767d\uff0c\u4e3a\u5f00\u53d1\u66f4\u5148\u8fdb\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\uff0c\u5c55\u793a\u4e86\u8be5\u9886\u57df\u4ecd\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u53d1\u5c55\u3002"}}
{"id": "2508.11808", "pdf": "https://arxiv.org/pdf/2508.11808", "abs": "https://arxiv.org/abs/2508.11808", "authors": ["Sahajpreet Singh", "Rongxin Ouyang", "Subhayan Mukerjee", "Kokil Jaidka"], "title": "Labels or Input? Rethinking Augmentation in Multimodal Hate Detection", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.CY", "cs.MM", "I.2.7; I.2.10"], "comment": "13 pages, 2 figures, 7 tables", "summary": "The modern web is saturated with multimodal content, intensifying the\nchallenge of detecting hateful memes, where harmful intent is often conveyed\nthrough subtle interactions between text and image under the guise of humor or\nsatire. While recent advances in Vision-Language Models (VLMs) show promise,\nthese models lack support for fine-grained supervision and remain susceptible\nto implicit hate speech. In this paper, we present a dual-pronged approach to\nimprove multimodal hate detection. First, we propose a prompt optimization\nframework that systematically varies prompt structure, supervision granularity,\nand training modality. We show that prompt design and label scaling both\ninfluence performance, with structured prompts improving robustness even in\nsmall models, and InternVL2 achieving the best F1-scores across binary and\nscaled settings. Second, we introduce a multimodal data augmentation pipeline\nthat generates 2,479 counterfactually neutral memes by isolating and rewriting\nthe hateful modality. This pipeline, powered by a multi-agent LLM-VLM setup,\nsuccessfully reduces spurious correlations and improves classifier\ngeneralization. Our approaches inspire new directions for building synthetic\ndata to train robust and fair vision-language models. Our findings demonstrate\nthat prompt structure and data composition are as critical as model size, and\nthat targeted augmentation can support more trustworthy and context-sensitive\nhate detection.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u91cd\u63aa\u65bd\u6765\u6539\u5584\u591a\u6a21\u6001\u6068\u607c\u56fe\u7247\u68c0\u6d4b\uff1a\u901a\u8fc7\u63d0\u793a\u4f18\u5316\u6846\u67b6\u548c\u591a\u6a21\u6001\u6570\u636e\u589e\u5f3a\u7ba1\u9053\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u7a33\u5065\u6027\u548c\u901a\u7528\u6027\u3002", "motivation": "\u73b0\u4ee3\u7f51\u7edc\u4e2d\u591a\u6a21\u6001\u6068\u607c\u5185\u5bb9\u6d41\u884c\uff0c\u5e38\u4ee5\u5e7d\u9ed8\u6216\u8bbd\u523a\u7684\u65b9\u5f0f\u4f20\u64ad\u3002\u867d\u7136\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b(VLMs)\u6709\u4e00\u5b9a\u6f5c\u529b\uff0c\u4f46\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u76d1\u7763\u652f\u6301\uff0c\u5bb9\u6613\u53d7\u9690\u5f0f\u6068\u607c\u8bed\u8a00\u5f71\u54cd\u3002", "method": "1) \u63d0\u51fa\u63d0\u793a\u4f18\u5316\u6846\u67b6\uff0c\u7cfb\u7edf\u6027\u53d8\u5316\u63d0\u793a\u7ed3\u6784\u3001\u76d1\u7763\u7ec6\u5ea6\u548c\u8bad\u7ec3\u6a21\u6001\u3002\n2) \u5f15\u5165\u591a\u6a21\u6001\u6570\u636e\u589e\u5f3a\u7ba1\u9053\uff0c\u901a\u8fc7\u591a\u4ee3\u7406LLM-VLM\u8bbe\u7f6e\u751f\u62102,479\u4e2a\u53cd\u4e8b\u5b9e\u4e2d\u7acb\u56fe\u7247\uff0c\u901a\u8fc7\u9694\u79bb\u548c\u91cd\u5199\u6068\u607c\u6a21\u6001\u6765\u51cf\u5c11\u504f\u504f\u76f8\u5173\u6027\u3002", "result": "\u7ed3\u6784\u5316\u63d0\u793a\u63d0\u9ad8\u4e86\u5c0f\u6a21\u578b\u7684\u7a33\u5065\u6027\uff0cInternVL2\u5728\u4e8c\u503c\u548c\u7ec6\u7c92\u5ea6\u8bbe\u7f6e\u4e2d\u83b7\u5f97\u6700\u4f73F1\u5206\u6570\u3002\u6570\u636e\u589e\u5f3a\u7ba1\u9053\u6210\u529f\u51cf\u5c11\u4e86\u504f\u504f\u76f8\u5173\u6027\uff0c\u63d0\u9ad8\u4e86\u5206\u7c7b\u5668\u7684\u901a\u7528\u6027\u3002", "conclusion": "\u63d0\u793a\u7ed3\u6784\u548c\u6570\u636e\u7ec4\u6210\u4e0e\u6a21\u578b\u5927\u5c0f\u540c\u6837\u5173\u952e\uff0c\u76ee\u6807\u5bfc\u5411\u7684\u6570\u636e\u589e\u5f3a\u80fd\u591f\u652f\u6301\u66f4\u53ef\u4fe1\u8c56\u548c\u4e0a\u4e0b\u6587\u654f\u611f\u7684\u6068\u607c\u68c0\u6d4b\u3002\u8fd9\u4e9b\u65b9\u6cd5\u4e3a\u6784\u5efa\u5408\u6210\u6570\u636e\u6765\u8bad\u7ec3\u7a33\u5065\u548c\u516c\u5e73\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5f00\u542f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.11860", "pdf": "https://arxiv.org/pdf/2508.11860", "abs": "https://arxiv.org/abs/2508.11860", "authors": ["Frazier N. Baker", "Daniel Adu-Ampratwum", "Reza Averly", "Botao Yu", "Huan Sun", "Xia Ning"], "title": "LARC: Towards Human-level Constrained Retrosynthesis Planning through an Agentic Framework", "categories": ["cs.AI", "cs.CL"], "comment": "24 pages, 5 figures", "summary": "Large language model (LLM) agent evaluators leverage specialized tools to\nground the rational decision-making of LLMs, making them well-suited to aid in\nscientific discoveries, such as constrained retrosynthesis planning.\nConstrained retrosynthesis planning is an essential, yet challenging, process\nwithin chemistry for identifying synthetic routes from commercially available\nstarting materials to desired target molecules, subject to practical\nconstraints. Here, we present LARC, the first LLM-based Agentic framework for\nRetrosynthesis planning under Constraints. LARC incorporates agentic constraint\nevaluation, through an Agent-as-a-Judge, directly into the retrosynthesis\nplanning process, using agentic feedback grounded in tool-based reasoning to\nguide and constrain route generation. We rigorously evaluate LARC on a\ncarefully curated set of 48 constrained retrosynthesis planning tasks across 3\nconstraint types. LARC achieves a 72.9% success rate on these tasks, vastly\noutperforming LLM baselines and approaching human expert-level success in\nsubstantially less time. The LARC framework is extensible, and serves as a\nfirst step towards an effective agentic tool or a co-scientist to human experts\nfor constrained retrosynthesis.", "AI": {"tldr": "LARC\u662f\u9996\u4e2a\u57fa\u4e8eLLM\u7684\u7ea6\u675f\u9006\u5408\u6210\u89c4\u5212\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5de5\u5177\u5316\u63a8\u7406\u548c\u4ee3\u7406\u8bc4\u4f30\u673a\u5236\uff0c\u5728\u5316\u5b66\u5408\u6210\u8def\u7ebf\u89c4\u5212\u4e2d\u5b9e\u73b0\u4e8672.9%\u7684\u6210\u529f\u7387\uff0c\u63a5\u8fd1\u4e13\u5bb6\u6c34\u5e73\u4f46\u8017\u65f6\u66f4\u5c11\u3002", "motivation": "\u5316\u5b66\u4e2d\u7684\u7ea6\u675f\u9006\u5408\u6210\u89c4\u5212\u662f\u4e00\u4e2a\u5173\u952e\u4f46\u5177\u6709\u6311\u6218\u6027\u7684\u8fc7\u7a0b\uff0c\u9700\u8981\u4ece\u5546\u4e1a\u53ef\u7528\u8d77\u59cb\u6750\u6599\u5230\u76ee\u6807\u5206\u5b50\u7684\u5408\u6210\u8def\u7ebf\u8bc6\u522b\uff0c\u540c\u65f6\u6ee1\u8db3\u5b9e\u9645\u7ea6\u675f\u6761\u4ef6\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u7ea6\u675f\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002", "method": "LARC\u6846\u67b6\u91c7\u7528\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u8bc4\u4f30\u673a\u5236\uff08Agent-as-a-Judge\uff09\uff0c\u5c06\u5de5\u5177\u5316\u63a8\u7406\u7684\u4ee3\u7406\u53cd\u9988\u76f4\u63a5\u6574\u5408\u5230\u9006\u5408\u6210\u89c4\u5212\u8fc7\u7a0b\u4e2d\uff0c\u901a\u8fc7\u4ee3\u7406\u5316\u7ea6\u675f\u8bc4\u4f30\u6765\u6307\u5bfc\u548c\u7ea6\u675f\u8def\u7ebf\u751f\u6210\u3002", "result": "\u5728\u7cbe\u5fc3\u7b56\u5212\u768448\u4e2a\u7ea6\u675f\u9006\u5408\u6210\u89c4\u5212\u4efb\u52a1\uff08\u6db5\u76d63\u79cd\u7ea6\u675f\u7c7b\u578b\uff09\u4e0a\uff0cLARC\u5b9e\u73b0\u4e8672.9%\u7684\u6210\u529f\u7387\uff0c\u663e\u8457\u4f18\u4e8eLLM\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u4e14\u5728\u8fdc\u5c11\u4e8e\u4eba\u7c7b\u4e13\u5bb6\u6240\u9700\u65f6\u95f4\u7684\u60c5\u51b5\u4e0b\u63a5\u8fd1\u4e13\u5bb6\u7ea7\u6210\u529f\u7387\u3002", "conclusion": "LARC\u6846\u67b6\u5177\u6709\u53ef\u6269\u5c55\u6027\uff0c\u662f\u671d\u7740\u4e3a\u4eba\u7c7b\u4e13\u5bb6\u5f00\u53d1\u6709\u6548\u4ee3\u7406\u5de5\u5177\u6216\u534f\u4f5c\u79d1\u5b66\u5bb6\u7684\u7b2c\u4e00\u6b65\uff0c\u4e3a\u7ea6\u675f\u9006\u5408\u6210\u89c4\u5212\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.11886", "pdf": "https://arxiv.org/pdf/2508.11886", "abs": "https://arxiv.org/abs/2508.11886", "authors": ["Wenhui Zhu", "Xiwen Chen", "Zhipeng Wang", "Shao Tang", "Sayan Ghosh", "Xuanzhao Dong", "Rajat Koner", "Yalin Wang"], "title": "EVTP-IVS: Effective Visual Token Pruning For Unifying Instruction Visual Segmentation In Multi-Modal Large Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "eess.IV"], "comment": null, "summary": "Instructed Visual Segmentation (IVS) tasks require segmenting objects in\nimages or videos based on natural language instructions. While recent\nmultimodal large language models (MLLMs) have achieved strong performance on\nIVS, their inference cost remains a major bottleneck, particularly in video. We\nempirically analyze visual token sampling in MLLMs and observe a strong\ncorrelation between subset token coverage and segmentation performance. This\nmotivates our design of a simple and effective token pruning method that\nselects a compact yet spatially representative subset of tokens to accelerate\ninference. In this paper, we introduce a novel visual token pruning method for\nIVS, called EVTP-IV, which builds upon the k-center by integrating spatial\ninformation to ensure better coverage. We further provide an\ninformation-theoretic analysis to support our design. Experiments on standard\nIVS benchmarks show that our method achieves up to 5X speed-up on video tasks\nand 3.5X on image tasks, while maintaining comparable accuracy using only 20%\nof the tokens. Our method also consistently outperforms state-of-the-art\npruning baselines under varying pruning ratios.", "AI": {"tldr": "\u901a\u8fc7\u7a7a\u95f4\u4fe1\u606f\u96c6\u6210\u7684k-center\u89c6\u89c9\u4ee4\u724c\u526a\u679d\u65b9\u6cd5EVTP-IV\uff0c\u5728\u6307\u4ee4\u89c6\u89c9\u5206\u5272\u4efb\u52a1\u4e2d\u5b9e\u73b05\u500d\u89c6\u9891\u901f\u5ea6\u63d0\u5347\u548c3.5\u500d\u56fe\u50cf\u901f\u5ea6\u63d0\u5347\uff0c\u4ec5\u4f7f\u752820%\u4ee4\u724c\u4fdd\u6301\u76f8\u5f53\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6307\u4ee4\u89c6\u89c9\u5206\u5272\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u6210\u672c\u8fc7\u9ad8\uff0c\u5c24\u5176\u662f\u5728\u89c6\u9891\u5904\u7406\u65f6\uff0c\u9700\u8981\u6279\u91cf\u5904\u7406\u5927\u91cf\u89c6\u89c9\u4ee4\u724c\u5bfc\u81f4\u6027\u80fd\u74f6\u9888\u3002", "method": "\u63d0\u51faEVTP-IV\u65b9\u6cd5\uff0c\u57fa\u4e8ek-center\u7b97\u6cd5\u96c6\u6210\u7a7a\u95f4\u4fe1\u606f\uff0c\u9009\u62e9\u7a7a\u95f4\u4e0a\u5177\u6709\u4ee3\u8868\u6027\u7684\u7b80\u7ea6\u4ee4\u724c\u5b50\u96c6\u6765\u52a0\u901f\u63a8\u7406\uff0c\u5e76\u901a\u8fc7\u4fe1\u606f\u8bba\u5206\u6790\u9a8c\u8bc1\u8bbe\u8ba1\u3002", "result": "\u5728\u6807\u51c6IVS\u6d4b\u8bd5\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u9ad85\u500d\u89c6\u9891\u901f\u5ea6\u63d0\u5347\u548c3.5\u500d\u56fe\u50cf\u901f\u5ea6\u63d0\u5347\uff0c\u4ec5\u4f7f\u752820%\u4ee4\u724c\u5374\u4fdd\u6301\u4e86\u76f8\u5f53\u7684\u5206\u5272\u51c6\u786e\u6027\uff0c\u4e14\u5728\u4e0d\u540c\u526a\u679d\u6bd4\u4e0b\u5747\u8d85\u8fc7\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u3002", "conclusion": "EVTP-IV\u901a\u8fc7\u7b80\u5355\u6709\u6548\u7684\u89c6\u89c9\u4ee4\u724c\u526a\u679d\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u6a21\u578b\u5728\u6307\u4ee4\u89c6\u89c9\u5206\u5272\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u6548\u7387\uff0c\u4e3a\u9ad8\u6548\u89c6\u89c9\u7406\u89e3\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.11925", "pdf": "https://arxiv.org/pdf/2508.11925", "abs": "https://arxiv.org/abs/2508.11925", "authors": ["Zhimeng Guo", "Huaisheng Zhu", "Siyuan Xu", "Hangfan Zhang", "Teng Xiao", "Minhao Cheng"], "title": "Optimizing Token Choice for Code Watermarking: A RL Approach", "categories": ["cs.CR", "cs.CL", "cs.LG"], "comment": "18 pages, 3 figures", "summary": "The need for detecting LLM-generated code necessitates watermarking systems\ncapable of operating within its highly structured and syntactically constrained\nenvironment. To address this, we introduce CodeTracer, an innovative adaptive\ncode watermarking framework underpinned by a novel reinforcement learning\ntraining paradigm. At its core, CodeTracer features a policy-driven approach\nthat utilizes a parameterized model to intelligently bias token choices during\nnext-token prediction. This strategy ensures that embedded watermarks maintain\ncode functionality while exhibiting subtle yet statistically detectable\ndeviations from typical token distributions. To facilitate policy learning, we\ndevise a comprehensive reward system that seamlessly integrates execution\nfeedback with watermark embedding signals, balancing process-level and\noutcome-level rewards. Additionally, we employ Gumbel Top-k reparameterization\nto enable gradient-based optimization of discrete watermarking decisions.\nExtensive comparative evaluations demonstrate CodeTracer's significant\nsuperiority over state-of-the-art baselines in both watermark detectability and\nthe preservation of generated code's functionality.", "AI": {"tldr": "CodeTracer\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u81ea\u9002\u5e94\u4ee3\u7801\u6c34\u5370\u6846\u67b6\uff0c\u901a\u8fc7\u7b56\u7565\u9a71\u52a8\u7684\u53c2\u6570\u5316\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u5d4c\u5165\u53ef\u68c0\u6d4b\u7684\u6c34\u5370\uff0c\u540c\u65f6\u4fdd\u6301\u4ee3\u7801\u529f\u80fd\u5b8c\u6574\u6027\u3002", "motivation": "\u68c0\u6d4bLLM\u751f\u6210\u7684\u4ee3\u7801\u9700\u8981\u80fd\u591f\u5728\u9ad8\u5ea6\u7ed3\u6784\u5316\u3001\u8bed\u6cd5\u7ea6\u675f\u7684\u73af\u5883\u4e2d\u5de5\u4f5c\u7684\u6c34\u5370\u7cfb\u7edf\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u8303\u5f0f\uff0c\u4f7f\u7528\u7b56\u7565\u9a71\u52a8\u7684\u65b9\u6cd5\u901a\u8fc7\u53c2\u6570\u5316\u6a21\u578b\u5728\u4e0b\u4e00\u4e2atoken\u9884\u6d4b\u65f6\u667a\u80fd\u504f\u7f6etoken\u9009\u62e9\uff0c\u7ed3\u5408Gumbel Top-k\u91cd\u53c2\u6570\u5316\u5b9e\u73b0\u79bb\u6563\u6c34\u5370\u51b3\u7b56\u7684\u68af\u5ea6\u4f18\u5316\u3002", "result": "\u5e7f\u6cdb\u7684\u6bd4\u8f83\u8bc4\u4f30\u663e\u793aCodeTracer\u5728\u6c34\u5370\u53ef\u68c0\u6d4b\u6027\u548c\u751f\u6210\u4ee3\u7801\u529f\u80fd\u4fdd\u6301\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "CodeTracer\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u4ee3\u7801\u529f\u80fd\u7684\u540c\u65f6\u5d4c\u5165\u7edf\u8ba1\u53ef\u68c0\u6d4b\u7684\u6c34\u5370\uff0c\u4e3aLLM\u751f\u6210\u4ee3\u7801\u7684\u68c0\u6d4b\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u6280\u672f\u624b\u6bb5\u3002"}}
{"id": "2508.11944", "pdf": "https://arxiv.org/pdf/2508.11944", "abs": "https://arxiv.org/abs/2508.11944", "authors": ["Hongtao Liu", "Zhicheng Du", "Zihe Wang", "Weiran Shen"], "title": "CHBench: A Cognitive Hierarchy Benchmark for Evaluating Strategic Reasoning Capability of LLMs", "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "Game-playing ability serves as an indicator for evaluating the strategic\nreasoning capability of large language models (LLMs). While most existing\nstudies rely on utility performance metrics, which are not robust enough due to\nvariations in opponent behavior and game structure. To address this limitation,\nwe propose \\textbf{Cognitive Hierarchy Benchmark (CHBench)}, a novel evaluation\nframework inspired by the cognitive hierarchy models from behavioral economics.\nWe hypothesize that agents have bounded rationality -- different agents behave\nat varying reasoning depths/levels. We evaluate LLMs' strategic reasoning\nthrough a three-phase systematic framework, utilizing behavioral data from six\nstate-of-the-art LLMs across fifteen carefully selected normal-form games.\nExperiments show that LLMs exhibit consistent strategic reasoning levels across\ndiverse opponents, confirming the framework's robustness and generalization\ncapability. We also analyze the effects of two key mechanisms (Chat Mechanism\nand Memory Mechanism) on strategic reasoning performance. Results indicate that\nthe Chat Mechanism significantly degrades strategic reasoning, whereas the\nMemory Mechanism enhances it. These insights position CHBench as a promising\ntool for evaluating LLM capabilities, with significant potential for future\nresearch and practical applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86CHBench\u8bc4\u4f30\u6846\u67b6\uff0c\u57fa\u4e8e\u8ba4\u77e5\u5c42\u6b21\u6a21\u578b\u6765\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6218\u7565\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u804a\u5929\u673a\u5236\u4f1a\u964d\u4f4e\u6218\u7565\u63a8\u7406\u800c\u8bb0\u5fc6\u673a\u5236\u4f1a\u589e\u5f3a\u5b83\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u4f9d\u8d56\u6548\u7528\u6027\u80fd\u6307\u6807\u6765\u8bc4\u4f30LLMs\u7684\u6e38\u620f\u80fd\u529b\uff0c\u4f46\u8fd9\u4e9b\u6307\u6807\u4e0d\u591f\u7a33\u5065\uff0c\u53d7\u5bf9\u624b\u884c\u4e3a\u548c\u6e38\u620f\u7ed3\u6784\u53d8\u5316\u5f71\u54cd\u8f83\u5927\u3002", "method": "\u91c7\u7528\u8ba4\u77e5\u5c42\u6b21\u6a21\u578b\u7684\u4e09\u9636\u6bb5\u7cfb\u7edf\u6846\u67b6\uff0c\u572815\u4e2a\u7cbe\u9009\u7684\u6b63\u89c4\u5f62\u5f0f\u6e38\u620f\u4e2d\u6536\u96c66\u4e2a\u6700\u5148\u8fdbLLMs\u7684\u884c\u4e3a\u6570\u636e\u8fdb\u884c\u5206\u6790\u3002", "result": "\u5b9e\u9a8c\u663e\u793aLLMs\u5728\u4e0d\u540c\u5bf9\u624b\u95f4\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u6218\u7565\u63a8\u7406\u6c34\u5e73\uff0c\u804a\u5929\u673a\u5236\u663e\u8457\u964d\u4f4e\u6218\u7565\u63a8\u7406\u80fd\u529b\uff0c\u800c\u8bb0\u5fc6\u673a\u5236\u5219\u589e\u5f3a\u8be5\u80fd\u529b\u3002", "conclusion": "CHBench\u662f\u4e00\u4e2a\u6709\u524d\u666f\u7684\u8bc4\u4f30\u5de5\u5177\uff0c\u5bf9LLMs\u80fd\u529b\u8bc4\u4f30\u5177\u6709\u91cd\u8981\u7814\u7a76\u4ef7\u503c\u548c\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.12072", "pdf": "https://arxiv.org/pdf/2508.12072", "abs": "https://arxiv.org/abs/2508.12072", "authors": ["Wei Jie Yeo", "Ranjan Satapathy", "Erik Cambria"], "title": "Mitigating Jailbreaks with Intent-Aware LLMs", "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "Despite extensive safety-tuning, large language models (LLMs) remain\nvulnerable to jailbreak attacks via adversarially crafted instructions,\nreflecting a persistent trade-off between safety and task performance. In this\nwork, we propose Intent-FT, a simple and lightweight fine-tuning approach that\nexplicitly trains LLMs to infer the underlying intent of an instruction before\nresponding. By fine-tuning on a targeted set of adversarial instructions,\nIntent-FT enables LLMs to generalize intent deduction to unseen attacks,\nthereby substantially improving their robustness. We comprehensively evaluate\nboth parametric and non-parametric attacks across open-source and proprietary\nmodels, considering harmfulness from attacks, utility, over-refusal, and impact\nagainst white-box threats. Empirically, Intent-FT consistently mitigates all\nevaluated attack categories, with no single attack exceeding a 50\\% success\nrate -- whereas existing defenses remain only partially effective. Importantly,\nour method preserves the model's general capabilities and reduces excessive\nrefusals on benign instructions containing superficially harmful keywords.\nFurthermore, models trained with Intent-FT accurately identify hidden harmful\nintent in adversarial attacks, and these learned intentions can be effectively\ntransferred to enhance vanilla model defenses.", "AI": {"tldr": "Intent-FT\u662f\u4e00\u79cd\u7b80\u5355\u8f7b\u91cf\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bad\u7ec3LLMs\u5728\u54cd\u5e94\u524d\u63a8\u65ad\u6307\u4ee4\u7684\u6f5c\u5728\u610f\u56fe\uff0c\u663e\u8457\u63d0\u9ad8\u6a21\u578b\u5bf9\u8d8a\u72f1\u653b\u51fb\u7684\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u901a\u7528\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b89\u5168\u8c03\u4f18\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u4ecd\u7136\u5bb9\u6613\u53d7\u5230\u901a\u8fc7\u5bf9\u6297\u6027\u6307\u4ee4\u8fdb\u884c\u7684\u8d8a\u72f1\u653b\u51fb\uff0c\u8fd9\u53cd\u6620\u4e86\u5b89\u5168\u6027\u548c\u4efb\u52a1\u6027\u80fd\u4e4b\u95f4\u7684\u6301\u7eed\u6743\u8861\u3002", "method": "\u63d0\u51faIntent-FT\u65b9\u6cd5\uff0c\u5728\u9488\u5bf9\u6027\u7684\u5bf9\u6297\u6307\u4ee4\u96c6\u4e0a\u8fdb\u884c\u5fae\u8c03\uff0c\u4f7fLLMs\u80fd\u591f\u5c06\u610f\u56fe\u63a8\u65ad\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u653b\u51fb\uff0c\u5305\u62ec\u53c2\u6570\u5316\u548c\u975e\u53c2\u6570\u5316\u653b\u51fb\u7684\u5168\u9762\u8bc4\u4f30\u3002", "result": "Intent-FT\u6301\u7eed\u7f13\u89e3\u6240\u6709\u8bc4\u4f30\u7684\u653b\u51fb\u7c7b\u522b\uff0c\u6ca1\u6709\u5355\u4e00\u653b\u51fb\u6210\u529f\u7387\u8d85\u8fc750%\uff0c\u800c\u73b0\u6709\u9632\u5fa1\u4ec5\u90e8\u5206\u6709\u6548\u3002\u65b9\u6cd5\u4fdd\u6301\u6a21\u578b\u901a\u7528\u80fd\u529b\uff0c\u51cf\u5c11\u5bf9\u5305\u542b\u8868\u9762\u6709\u5bb3\u5173\u952e\u8bcd\u7684\u826f\u6027\u6307\u4ee4\u7684\u8fc7\u5ea6\u62d2\u7edd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4f7f\u6a21\u578b\u80fd\u591f\u51c6\u786e\u8bc6\u522b\u5bf9\u6297\u653b\u51fb\u4e2d\u7684\u9690\u85cf\u6709\u5bb3\u610f\u56fe\uff0c\u5e76\u4e14\u8fd9\u4e9b\u5b66\u4e60\u5230\u7684\u610f\u56fe\u53ef\u4ee5\u6709\u6548\u5730\u8f6c\u79fb\u5230\u589e\u5f3a\u666e\u901a\u6a21\u578b\u9632\u5fa1\u4e2d\u3002"}}
{"id": "2508.12081", "pdf": "https://arxiv.org/pdf/2508.12081", "abs": "https://arxiv.org/abs/2508.12081", "authors": ["Haidong Xu", "Guangwei Xu", "Zhedong Zheng", "Xiatian Zhu", "Wei Ji", "Xiangtai Li", "Ruijie Guo", "Meishan Zhang", "Min zhang", "Hao Fei"], "title": "VimoRAG: Video-based Retrieval-augmented 3D Motion Generation for Motion Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "20 pages,13 figures", "summary": "This paper introduces VimoRAG, a novel video-based retrieval-augmented motion\ngeneration framework for motion large language models (LLMs). As motion LLMs\nface severe out-of-domain/out-of-vocabulary issues due to limited annotated\ndata, VimoRAG leverages large-scale in-the-wild video databases to enhance 3D\nmotion generation by retrieving relevant 2D human motion signals. While\nvideo-based motion RAG is nontrivial, we address two key bottlenecks: (1)\ndeveloping an effective motion-centered video retrieval model that\ndistinguishes human poses and actions, and (2) mitigating the issue of error\npropagation caused by suboptimal retrieval results. We design the Gemini Motion\nVideo Retriever mechanism and the Motion-centric Dual-alignment DPO Trainer,\nenabling effective retrieval and generation processes. Experimental results\nshow that VimoRAG significantly boosts the performance of motion LLMs\nconstrained to text-only input.", "AI": {"tldr": "VimoRAG\u662f\u4e00\u4e2a\u57fa\u4e8e\u89c6\u9891\u68c0\u7d22\u589e\u5f3a\u7684\u8fd0\u52a8\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u4ece\u5927\u89c4\u6a21\u89c6\u9891\u6570\u636e\u5e93\u4e2d\u68c0\u7d22\u76f8\u5173\u76842D\u4eba\u4f53\u8fd0\u52a8\u4fe1\u53f7\u6765\u89e3\u51b3\u8fd0\u52a8\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ec5\u4f7f\u7528\u6587\u672c\u8f93\u5165\u7684\u8fd0\u52a8\u751f\u6210\u6027\u80fd\u3002", "motivation": "\u8fd0\u52a8\u5927\u8bed\u8a00\u6a21\u578b\u7531\u4e8e\u6807\u6ce8\u6570\u636e\u6709\u9650\u800c\u9762\u4e34\u4e25\u91cd\u7684\u57df\u5916/\u8bcd\u6c47\u5916\u95ee\u9898\uff0c\u9700\u8981\u5229\u7528\u5927\u89c4\u6a21\u91ce\u5916\u89c6\u9891\u6570\u636e\u5e93\u6765\u589e\u5f3a3D\u8fd0\u52a8\u751f\u6210\u80fd\u529b\u3002", "method": "\u8bbe\u8ba1\u4e86Gemini Motion Video Retriever\u673a\u5236\u548cMotion-centric Dual-alignment DPO Trainer\uff0c\u89e3\u51b3\u89c6\u9891\u68c0\u7d22\u4e2d\u7684\u4e24\u4e2a\u5173\u952e\u74f6\u9888\uff1a\u6709\u6548\u7684\u4eba\u4f53\u59ff\u6001\u548c\u52a8\u4f5c\u533a\u5206\uff0c\u4ee5\u53ca\u6b21\u4f18\u68c0\u7d22\u7ed3\u679c\u7684\u8bef\u5dee\u4f20\u64ad\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cVimoRAG\u663e\u8457\u63d0\u5347\u4e86\u4ec5\u4f7f\u7528\u6587\u672c\u8f93\u5165\u7684\u8fd0\u52a8\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "VimoRAG\u6846\u67b6\u901a\u8fc7\u89c6\u9891\u68c0\u7d22\u589e\u5f3a\u7684\u65b9\u5f0f\u6709\u6548\u89e3\u51b3\u4e86\u8fd0\u52a8\u751f\u6210\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u4e3a\u8fd0\u52a8\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2508.12104", "pdf": "https://arxiv.org/pdf/2508.12104", "abs": "https://arxiv.org/abs/2508.12104", "authors": ["Shane Waxler", "Paul Blazek", "Davis White", "Daniel Sneider", "Kevin Chung", "Mani Nagarathnam", "Patrick Williams", "Hank Voeller", "Karen Wong", "Matthew Swanhorst", "Sheng Zhang", "Naoto Usuyama", "Cliff Wong", "Tristan Naumann", "Hoifung Poon", "Andrew Loza", "Daniella Meeker", "Seth Hain", "Rahul Shah"], "title": "Generative Medical Event Models Improve with Scale", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Realizing personalized medicine at scale calls for methods that distill\ninsights from longitudinal patient journeys, which can be viewed as a sequence\nof medical events. Foundation models pretrained on large-scale medical event\ndata represent a promising direction for scaling real-world evidence generation\nand generalizing to diverse downstream tasks. Using Epic Cosmos, a dataset with\nmedical events from de-identified longitudinal health records for 16.3 billion\nencounters over 300 million unique patient records from 310 health systems, we\nintroduce the Cosmos Medical Event Transformer ( CoMET) models, a family of\ndecoder-only transformer models pretrained on 118 million patients representing\n115 billion discrete medical events (151 billion tokens). We present the\nlargest scaling-law study for medical event data, establishing a methodology\nfor pretraining and revealing power-law scaling relationships for compute,\ntokens, and model size. Based on this, we pretrained a series of\ncompute-optimal models with up to 1 billion parameters. Conditioned on a\npatient's real-world history, CoMET autoregressively generates the next medical\nevent, simulating patient health timelines. We studied 78 real-world tasks,\nincluding diagnosis prediction, disease prognosis, and healthcare operations.\nRemarkably for a foundation model with generic pretraining and simulation-based\ninference, CoMET generally outperformed or matched task-specific supervised\nmodels on these tasks, without requiring task-specific fine-tuning or few-shot\nexamples. CoMET's predictive power consistently improves as the model and\npretraining scale. Our results show that CoMET, a generative medical event\nfoundation model, can effectively capture complex clinical dynamics, providing\nan extensible and generalizable framework to support clinical decision-making,\nstreamline healthcare operations, and improve patient outcomes.", "AI": {"tldr": "CoMET\u662f\u57fa\u4e8e163\u4ebf\u6b21\u533b\u7597\u4e8b\u4ef6\u8bad\u7ec3\u7684\u5927\u578b\u533b\u7597\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u56de\u5f52\u751f\u6210\u6a21\u62df\u60a3\u8005\u5065\u5eb7\u65f6\u95f4\u7ebf\uff0c\u572878\u4e2a\u533b\u7597\u4efb\u52a1\u4e2d\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u8fbe\u5230\u6216\u8d85\u8d8a\u4e13\u7528\u6a21\u578b\u6027\u80fd", "motivation": "\u5b9e\u73b0\u89c4\u6a21\u5316\u4e2a\u6027\u5316\u533b\u7597\u9700\u8981\u4ece\u7eb5\u5411\u60a3\u8005\u65c5\u7a0b\u4e2d\u63d0\u53d6\u6d1e\u5bdf\uff0c\u57fa\u7840\u6a21\u578b\u9884\u8bad\u7ec3\u5728\u5927\u89c4\u6a21\u533b\u7597\u4e8b\u4ef6\u6570\u636e\u4e0a\u4ee3\u8868\u4e86\u6269\u5c55\u771f\u5b9e\u4e16\u754c\u8bc1\u636e\u751f\u6210\u548c\u6cdb\u5316\u5230\u591a\u6837\u5316\u4e0b\u6e38\u4efb\u52a1\u7684\u6709\u524d\u666f\u65b9\u5411", "method": "\u4f7f\u7528Epic Cosmos\u6570\u636e\u96c6\uff08163\u4ebf\u6b21\u533b\u7597\u4e8b\u4ef6\u30013\u4ebf\u60a3\u8005\u8bb0\u5f55\uff09\uff0c\u8bad\u7ec3\u89e3\u7801\u5668Transformer\u6a21\u578bCoMET\uff0c\u8fdb\u884c\u81ea\u56de\u5f52\u533b\u7597\u4e8b\u4ef6\u751f\u6210\uff0c\u5efa\u7acb\u4e86\u533b\u7597\u6570\u636e\u7684\u7f29\u653e\u5b9a\u5f8b\u7814\u7a76", "result": "CoMET\u5728\u8bca\u65ad\u9884\u6d4b\u3001\u75be\u75c5\u9884\u540e\u548c\u533b\u7597\u8fd0\u8425\u7b4978\u4e2a\u4efb\u52a1\u4e2d\uff0c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\u6216few-shot\u793a\u4f8b\uff0c\u666e\u904d\u4f18\u4e8e\u6216\u5339\u914d\u4e13\u7528\u76d1\u7763\u6a21\u578b\uff0c\u9884\u6d4b\u80fd\u529b\u968f\u6a21\u578b\u89c4\u6a21\u548c\u9884\u8bad\u7ec3\u89c4\u6a21\u6301\u7eed\u63d0\u5347", "conclusion": "CoMET\u4f5c\u4e3a\u751f\u6210\u5f0f\u533b\u7597\u4e8b\u4ef6\u57fa\u7840\u6a21\u578b\uff0c\u80fd\u6709\u6548\u6355\u6349\u590d\u6742\u4e34\u5e8a\u52a8\u6001\uff0c\u4e3a\u652f\u6301\u4e34\u5e8a\u51b3\u7b56\u3001\u7b80\u5316\u533b\u7597\u8fd0\u8425\u548c\u6539\u5584\u60a3\u8005\u7ed3\u5c40\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u548c\u53ef\u6cdb\u5316\u7684\u6846\u67b6"}}
{"id": "2508.12116", "pdf": "https://arxiv.org/pdf/2508.12116", "abs": "https://arxiv.org/abs/2508.12116", "authors": ["Haebin Shin", "Lei Ji", "Xiao Liu", "Zhiwei Yu", "Qi Chen", "Yeyun Gong"], "title": "DynamixSFT: Dynamic Mixture Optimization of Instruction Tuning Collections", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "As numerous instruction-tuning datasets continue to emerge during the\npost-training stage, dynamically balancing and optimizing their mixtures has\nbecome a critical challenge. To address this, we propose DynamixSFT, a dynamic\nand automated method for instruction-tuning dataset mixture optimization. We\nformulate the problem as a multi-armed bandit setup and introduce a\nPrior-scaled Boltzmann Exploration that softly anchors the updated sampling\ndistribution to the original dataset proportions, thereby preserving the\ninherent diversity and coverage of the collection. Sampling probabilities are\nupdated using a lightweight 1-Step Look-ahead Reward, reflecting how much the\ndataset contributes to improving the model's performance at its current state.\nWhen applied to the Tulu-v2-mixture collection comprising 16 instruction-tuning\ndatasets, DynamixSFT achieves up to a 2.2% performance improvement across 10\nbenchmarks. Furthermore, we provide a comprehensive analysis and visualizations\nto offer deeper insights into the adaptive dynamics of our method.", "AI": {"tldr": "DynamixSFT\u662f\u4e00\u79cd\u52a8\u6001\u81ea\u52a8\u5316\u7684\u6307\u4ee4\u8c03\u4f18\u6570\u636e\u96c6\u6df7\u5408\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u81c2\u8001\u864e\u673a\u6846\u67b6\u548c\u5148\u9a8c\u7f29\u653e\u73bb\u5c14\u5179\u66fc\u63a2\u7d22\u6765\u52a8\u6001\u5e73\u8861\u6570\u636e\u96c6\u91c7\u6837\u6982\u7387\uff0c\u5728Tulu-v2\u6570\u636e\u96c6\u96c6\u5408\u4e0a\u5b9e\u73b0\u4e862.2%\u7684\u6027\u80fd\u63d0\u5347", "motivation": "\u968f\u7740\u6307\u4ee4\u8c03\u4f18\u6570\u636e\u96c6\u4e0d\u65ad\u6d8c\u73b0\uff0c\u5982\u4f55\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u52a8\u6001\u5e73\u8861\u548c\u4f18\u5316\u8fd9\u4e9b\u6570\u636e\u96c6\u7684\u6df7\u5408\u6bd4\u4f8b\u6210\u4e3a\u4e00\u4e2a\u5173\u952e\u6311\u6218", "method": "\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u591a\u81c2\u8001\u864e\u673a\u6846\u67b6\uff0c\u63d0\u51fa\u5148\u9a8c\u7f29\u653e\u73bb\u5c14\u5179\u66fc\u63a2\u7d22\u65b9\u6cd5\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea71\u6b65\u524d\u77bb\u5956\u52b1\u6765\u66f4\u65b0\u91c7\u6837\u6982\u7387\uff0c\u4fdd\u6301\u539f\u59cb\u6570\u636e\u96c6\u6bd4\u4f8b\u7684\u8f6f\u951a\u5b9a", "result": "\u5728\u5305\u542b16\u4e2a\u6307\u4ee4\u8c03\u4f18\u6570\u636e\u96c6\u7684Tulu-v2\u96c6\u5408\u4e0a\uff0cDynamixSFT\u572810\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u9ad82.2%\u7684\u6027\u80fd\u63d0\u5347", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u52a8\u6001\u4f18\u5316\u6570\u636e\u96c6\u6df7\u5408\u6bd4\u4f8b\uff0c\u540c\u65f6\u4fdd\u6301\u6570\u636e\u96c6\u7684\u56fa\u6709\u591a\u6837\u6027\u548c\u8986\u76d6\u8303\u56f4\uff0c\u4e3a\u6307\u4ee4\u8c03\u4f18\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.12398", "pdf": "https://arxiv.org/pdf/2508.12398", "abs": "https://arxiv.org/abs/2508.12398", "authors": ["Zhixin Xie", "Xurui Song", "Jun Luo"], "title": "Where to Start Alignment? Diffusion Large Language Model May Demand a Distinct Position", "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": null, "summary": "Diffusion Large Language Models (dLLMs) have recently emerged as a\ncompetitive non-autoregressive paradigm due to their unique training and\ninference approach. However, there is currently a lack of safety study on this\nnovel architecture. In this paper, we present the first analysis of dLLMs'\nsafety performance and propose a novel safety alignment method tailored to\ntheir unique generation characteristics. Specifically, we identify a critical\nasymmetry between the defender and attacker in terms of security. For the\ndefender, we reveal that the middle tokens of the response, rather than the\ninitial ones, are more critical to the overall safety of dLLM outputs; this\nseems to suggest that aligning middle tokens can be more beneficial to the\ndefender. The attacker, on the contrary, may have limited power to manipulate\nmiddle tokens, as we find dLLMs have a strong tendency towards a sequential\ngeneration order in practice, forcing the attack to meet this distribution and\ndiverting it from influencing the critical middle tokens. Building on this\nasymmetry, we introduce Middle-tOken Safety Alignment (MOSA), a novel method\nthat directly aligns the model's middle generation with safe refusals\nexploiting reinforcement learning. We implement MOSA and compare its security\nperformance against eight attack methods on two benchmarks. We also test the\nutility of MOSA-aligned dLLM on coding, math, and general reasoning. The\nresults strongly prove the superiority of MOSA.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5206\u6790\u4e86\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b(dLLMs)\u7684\u5b89\u5168\u6027\uff0c\u53d1\u73b0\u54cd\u5e94\u4e2d\u95f4token\u5bf9\u5b89\u5168\u6027\u66f4\u5173\u952e\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u4e2d\u95f4token\u7684\u5b89\u5168\u5bf9\u9f50\u65b9\u6cd5MOSA\uff0c\u5728\u5b89\u5168\u6027\u548c\u5b9e\u7528\u6027\u65b9\u9762\u90fd\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b(dLLMs)\u4f5c\u4e3a\u4e00\u79cd\u65b0\u5174\u7684\u975e\u81ea\u56de\u5f52\u8303\u5f0f\uff0c\u76ee\u524d\u7f3a\u4e4f\u5bf9\u5176\u5b89\u5168\u6027\u7684\u7814\u7a76\uff0c\u9700\u8981\u63a2\u7d22\u9002\u5408\u5176\u72ec\u7279\u751f\u6210\u7279\u6027\u7684\u5b89\u5168\u5bf9\u9f50\u65b9\u6cd5\u3002", "method": "\u63d0\u51faMiddle-tOken Safety Alignment (MOSA)\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u76f4\u63a5\u5bf9\u9f50\u6a21\u578b\u7684\u4e2d\u95f4\u751f\u6210\u8fc7\u7a0b\u4e0e\u5b89\u5168\u62d2\u7edd\u54cd\u5e94\uff0c\u5229\u7528dLLMs\u4e2d\u95f4token\u5bf9\u5b89\u5168\u6027\u66f4\u5173\u952e\u7684\u7279\u6027\u3002", "result": "\u57288\u79cd\u653b\u51fb\u65b9\u6cd5\u548c2\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMOSA\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u5b89\u5168\u6027\u80fd\uff0c\u540c\u65f6\u5728\u7f16\u7a0b\u3001\u6570\u5b66\u548c\u901a\u7528\u63a8\u7406\u4efb\u52a1\u4e0a\u4fdd\u6301\u4e86\u826f\u597d\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86dLLMs\u5b89\u5168\u6027\u7684\u5173\u952e\u4e0d\u5bf9\u79f0\u6027\uff0c\u63d0\u51fa\u7684MOSA\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u5b89\u5168\u6027\uff0c\u4e3a\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u5bf9\u9f50\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2508.12425", "pdf": "https://arxiv.org/pdf/2508.12425", "abs": "https://arxiv.org/abs/2508.12425", "authors": ["Phuong Minh Nguyen", "Tien Huu Dang", "Naoya Inoue"], "title": "Non-Iterative Symbolic-Aided Chain-of-Thought for Logical Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "This work introduces Symbolic-Aided Chain-of-Thought (CoT), an improved\napproach to standard CoT, for logical reasoning in large language models\n(LLMs). The key idea is to integrate lightweight symbolic representations into\nfew-shot prompts, structuring the inference steps with a consistent strategy to\nmake reasoning patterns more explicit within a non-iterative reasoning process.\nBy incorporating these symbolic structures, our method preserves the\ngeneralizability of standard prompting techniques while enhancing the\ntransparency, interpretability, and analyzability of LLM logical reasoning.\nExtensive experiments on four well-known logical reasoning benchmarks --\nProofWriter, FOLIO, ProntoQA, and LogicalDeduction, which cover diverse\nreasoning scenarios -- demonstrate the effectiveness of the proposed approach,\nparticularly in complex reasoning tasks that require navigating multiple\nconstraints or rules. Notably, Symbolic-Aided CoT consistently improves LLMs'\nreasoning capabilities across various model sizes and significantly outperforms\nconventional CoT on three out of four datasets, ProofWriter, ProntoQA, and\nLogicalDeduction.", "AI": {"tldr": "Symbolic-Aided CoT\u901a\u8fc7\u6574\u5408\u8f7b\u91cf\u7ea7\u7b26\u53f7\u8868\u793a\u5230\u5c11\u6837\u672c\u63d0\u793a\u4e2d\uff0c\u6539\u8fdb\u6807\u51c6CoT\u65b9\u6cd5\uff0c\u63d0\u5347LLM\u5728\u903b\u8f91\u63a8\u7406\u4e2d\u7684\u900f\u660e\u5ea6\u3001\u53ef\u89e3\u91ca\u6027\u548c\u5206\u6790\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u4f20\u7edfCoT\u3002", "motivation": "\u6807\u51c6CoT\u65b9\u6cd5\u5728\u903b\u8f91\u63a8\u7406\u4e2d\u7f3a\u4e4f\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u4fdd\u6301\u901a\u7528\u6027\u540c\u65f6\u589e\u5f3a\u63a8\u7406\u8fc7\u7a0b\u6e05\u6670\u5ea6\u7684\u65b9\u6cd5\u3002", "method": "\u5728\u5c11\u6837\u672c\u63d0\u793a\u4e2d\u96c6\u6210\u8f7b\u91cf\u7ea7\u7b26\u53f7\u8868\u793a\uff0c\u4f7f\u7528\u4e00\u81f4\u7684\u7b56\u7565\u6784\u5efa\u63a8\u7406\u6b65\u9aa4\uff0c\u4f7f\u63a8\u7406\u6a21\u5f0f\u5728\u975e\u8fed\u4ee3\u63a8\u7406\u8fc7\u7a0b\u4e2d\u66f4\u52a0\u660e\u786e\u3002", "result": "\u5728ProofWriter\u3001FOLIO\u3001ProntoQA\u548cLogicalDeduction\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u5904\u7406\u591a\u91cd\u7ea6\u675f\u7684\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\uff0c\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u8d85\u8d8a\u4f20\u7edfCoT\u3002", "conclusion": "Symbolic-Aided CoT\u6709\u6548\u63d0\u5347\u4e86LLM\u7684\u903b\u8f91\u63a8\u7406\u80fd\u529b\uff0c\u589e\u5f3a\u4e86\u63a8\u7406\u8fc7\u7a0b\u7684\u900f\u660e\u5ea6\u548c\u53ef\u5206\u6790\u6027\uff0c\u5728\u4e0d\u540c\u89c4\u6a21\u7684\u6a21\u578b\u4e0a\u90fd\u8868\u73b0\u51fa\u7a33\u5b9a\u7684\u6539\u8fdb\u6548\u679c\u3002"}}
{"id": "2508.12430", "pdf": "https://arxiv.org/pdf/2508.12430", "abs": "https://arxiv.org/abs/2508.12430", "authors": ["Yahsin Yeh", "Yilun Wu", "Bokai Ruan", "Honghan Shuai"], "title": "Adversarial Attacks on VQA-NLE: Exposing and Alleviating Inconsistencies in Visual Question Answering Explanations", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Natural language explanations in visual question answering (VQA-NLE) aim to\nmake black-box models more transparent by elucidating their decision-making\nprocesses. However, we find that existing VQA-NLE systems can produce\ninconsistent explanations and reach conclusions without genuinely understanding\nthe underlying context, exposing weaknesses in either their inference pipeline\nor explanation-generation mechanism. To highlight these vulnerabilities, we not\nonly leverage an existing adversarial strategy to perturb questions but also\npropose a novel strategy that minimally alters images to induce contradictory\nor spurious outputs. We further introduce a mitigation method that leverages\nexternal knowledge to alleviate these inconsistencies, thereby bolstering model\nrobustness. Extensive evaluations on two standard benchmarks and two widely\nused VQA-NLE models underscore the effectiveness of our attacks and the\npotential of knowledge-based defenses, ultimately revealing pressing security\nand reliability concerns in current VQA-NLE systems.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63a2\u7d22\u4e86\u89c6\u89c9\u95ee\u9898\u56de\u7b54\u4e2d\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u7cfb\u7edf\u7684\u810f\u70b9\uff0c\u901a\u8fc7\u653b\u51fb\u548c\u7f13\u89e1\u65b9\u6cd5\u63ed\u793a\u4e86\u5f53\u524d\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709VQA-NLE\u7cfb\u7edf\u5b58\u5728\u89e3\u91ca\u4e0d\u4e00\u81f4\u548c\u7f3a\u4e4f\u771f\u6b63\u7406\u89e3\u7684\u95ee\u9898\uff0c\u9700\u8981\u63ed\u793a\u8fd9\u4e9b\u810f\u70b9\u5e76\u63d0\u51fa\u6539\u5584\u65b9\u6848\u3002", "method": "\u7ee7\u627f\u73b0\u6709\u7684\u95ee\u9898\u5e72\u6270\u7b56\u7565\uff0c\u63d0\u51fa\u65b0\u7684\u56fe\u50cf\u6700\u5c0f\u6539\u52a8\u653b\u51fb\u7b56\u7565\uff0c\u5e76\u4f7f\u7528\u5916\u90e8\u77e5\u8bc6\u6765\u7f13\u89e3\u4e0d\u4e00\u81f4\u6027\u3002", "result": "\u5728\u4e24\u4e2a\u6807\u51c6\u6d4b\u8bd5\u96c6\u548c\u4e24\u4e2a\u5e38\u7528VQA-NLE\u6a21\u578b\u4e0a\u8bc1\u660e\u4e86\u653b\u51fb\u7684\u6709\u6548\u6027\u548c\u77e5\u8bc6\u57fa\u9632\u5fa1\u7684\u6f5c\u529b\u3002", "conclusion": "\u5f53\u524dVQA-NLE\u7cfb\u7edf\u5b58\u5728\u4e25\u91cd\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u95ee\u9898\uff0c\u77e5\u8bc6\u57fa\u9632\u5fa1\u65b9\u6848\u5c55\u793a\u4e86\u63d0\u5347\u6a21\u578b\u5065\u58ee\u6027\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.12574", "pdf": "https://arxiv.org/pdf/2508.12574", "abs": "https://arxiv.org/abs/2508.12574", "authors": ["Bin Ma", "Yifei Zhang", "Yongjin Xian", "Qi Li", "Linna Zhou", "Gongxun Miao"], "title": "Insight Rumors: A Novel Textual Rumor Locating and Marking Model Leveraging Att_BiMamba2 Network", "categories": ["cs.SI", "cs.CL"], "comment": null, "summary": "With the development of social media networks, rumor detection models have\nattracted more and more attention. Whereas, these models primarily focus on\nclassifying contexts as rumors or not, lacking the capability to locate and\nmark specific rumor content. To address this limitation, this paper proposes a\nnovel rumor detection model named Insight Rumors to locate and mark rumor\ncontent within textual data. Specifically, we propose the Bidirectional Mamba2\nNetwork with Dot-Product Attention (Att_BiMamba2), a network that constructs a\nbidirectional Mamba2 model and applies dot-product attention to weight and\ncombine the outputs from both directions, thereby enhancing the representation\nof high-dimensional rumor features. Simultaneously, a Rumor Locating and\nMarking module is designed to locate and mark rumors. The module constructs a\nskip-connection network to project high-dimensional rumor features onto\nlow-dimensional label features. Moreover, Conditional Random Fields (CRF) is\nemployed to impose strong constraints on the output label features, ensuring\naccurate rumor content location. Additionally, a labeled dataset for rumor\nlocating and marking is constructed, with the effectiveness of the proposed\nmodel is evaluated through comprehensive experiments. Extensive experiments\nindicate that the proposed scheme not only detects rumors accurately but also\nlocates and marks them in context precisely, outperforming state-of-the-art\nschemes that can only discriminate rumors roughly.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8c23\u8a00\u68c0\u6d4b\u6a21\u578bInsight Rumors\uff0c\u80fd\u591f\u4e0d\u4ec5\u8bc6\u522b\u8c23\u8a00\u8fd8\u80fd\u51c6\u786e\u5b9a\u4f4d\u548c\u6807\u8bb0\u8c23\u8a00\u5185\u5bb9\uff0c\u8d85\u8d8a\u4e86\u4ee5\u5f80\u53ea\u80fd\u5927\u81f4\u5224\u65ad\u8c23\u8a00\u7684\u65b9\u6848\u3002", "motivation": "\u73b0\u6709\u8c23\u8a00\u68c0\u6d4b\u6a21\u578b\u4e3b\u8981\u5173\u6ce8\u6587\u672c\u662f\u5426\u4e3a\u8c23\u8a00\uff0c\u7f3a\u4e4f\u5bf9\u5177\u4f53\u8c23\u8a00\u5185\u5bb9\u7684\u5b9a\u4f4d\u548c\u6807\u8bb0\u80fd\u529b\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u9650\u5236\u3002", "method": "\u63d0\u51fa\u4e86\u5177\u6709\u70b9\u79ef\u6ce8\u610f\u529b\u7684\u53cc\u5411Mamba2\u7f51\u7edc(Att_BiMamba2)\uff0c\u901a\u8fc7\u8de8\u8fde\u63a5\u7f51\u7edc\u5c06\u9ad8\u7ef4\u8c23\u8a00\u7279\u5f81\u6295\u5f71\u5230\u4f4e\u7ef4\u6807\u7b7e\u7279\u5f81\uff0c\u5e76\u4f7f\u7528\u6761\u4ef6\u968f\u673a\u573a(CRF)\u5bf9\u8f93\u51fa\u6807\u7b7e\u8fdb\u884c\u7ea6\u675f\u4ee5\u786e\u4fdd\u51c6\u786e\u5b9a\u4f4d\u3002", "result": "\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6848\u4e0d\u4ec5\u80fd\u591f\u51c6\u786e\u68c0\u6d4b\u8c23\u8a00\uff0c\u8fd8\u80fd\u5728\u6587\u672c\u4e2d\u7cbe\u786e\u5b9a\u4f4d\u548c\u6807\u8bb0\u8c23\u8a00\u5185\u5bb9\uff0c\u8868\u73b0\u8d85\u8fc7\u4e86\u53ea\u80fd\u5927\u81f4\u8bc6\u522b\u8c23\u8a00\u7684\u6700\u65b0\u65b9\u6848\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5f00\u53d1\u4e86\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u8fdb\u884c\u8c23\u8a00\u68c0\u6d4b\u548c\u7cbe\u786e\u5b9a\u4f4d\u7684\u6a21\u578b\uff0c\u4e3a\u793e\u4ea4\u5a92\u4f53\u8c23\u8a00\u76d1\u6d4b\u63d0\u4f9b\u4e86\u66f4\u52a0\u5b8c\u6574\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u91cd\u8981\u7684\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2508.12611", "pdf": "https://arxiv.org/pdf/2508.12611", "abs": "https://arxiv.org/abs/2508.12611", "authors": ["Trang Tran", "Trung Hoang Le", "Huiping Cao", "Tran Cao Son"], "title": "An LLM + ASP Workflow for Joint Entity-Relation Extraction", "categories": ["cs.AI", "cs.CL", "I.2.7; F.4.1"], "comment": "13 pages, 1 figure, Accepted as Technical Communication, 41st\n  International Conference on Logic Programming", "summary": "Joint entity-relation extraction (JERE) identifies both entities and their\nrelationships simultaneously. Traditional machine-learning based approaches to\nperforming this task require a large corpus of annotated data and lack the\nability to easily incorporate domain specific information in the construction\nof the model. Therefore, creating a model for JERE is often labor intensive,\ntime consuming, and elaboration intolerant. In this paper, we propose\nharnessing the capabilities of generative pretrained large language models\n(LLMs) and the knowledge representation and reasoning capabilities of Answer\nSet Programming (ASP) to perform JERE. We present a generic workflow for JERE\nusing LLMs and ASP. The workflow is generic in the sense that it can be applied\nfor JERE in any domain. It takes advantage of LLM's capability in natural\nlanguage understanding in that it works directly with unannotated text. It\nexploits the elaboration tolerant feature of ASP in that no modification of its\ncore program is required when additional domain specific knowledge, in the form\nof type specifications, is found and needs to be used. We demonstrate the\nusefulness of the proposed workflow through experiments with limited training\ndata on three well-known benchmarks for JERE. The results of our experiments\nshow that the LLM + ASP workflow is better than state-of-the-art JERE systems\nin several categories with only 10\\% of training data. It is able to achieve a\n2.5 times (35\\% over 15\\%) improvement in the Relation Extraction task for the\nSciERC corpus, one of the most difficult benchmarks.", "AI": {"tldr": "\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u548c\u7b54\u6848\u96c6\u7f16\u7a0b\u7ed3\u5408\u7684\u65b0\u65b9\u6cd5\uff0c\u5728\u5c11\u91cf\u8bad\u7ec3\u6570\u636e\u4e0b\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u805a\u5408\u5b9e\u4f53-\u5173\u7cfb\u63d0\u53d6\u6548\u679c", "motivation": "\u4f20\u7edf\u7684\u805a\u5408\u5b9e\u4f53-\u5173\u7cfb\u63d0\u53d6\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u52b3\u52a8\u5bc6\u96c6\u4e14\u65e0\u6cd5\u8f7b\u677e\u878d\u5165\u9886\u57df\u77e5\u8bc6", "method": "\u7ed3\u5408\u751f\u6210\u5f0f\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b(LLMs)\u7684\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u80fd\u529b\u548c\u7b54\u6848\u96c6\u7f16\u7a0b(ASP)\u7684\u77e5\u8bc6\u8868\u793a\u4e0e\u63a8\u7406\u80fd\u529b\uff0c\u63d0\u51fa\u901a\u7528\u5de5\u4f5c\u6d41", "result": "\u5728\u4e09\u4e2a\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u4ec5\u970010%\u8bad\u7ec3\u6570\u636e\u60c5\u51b5\u4e0b\u5c31\u8d85\u8fc7\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\uff0c\u5728SciERC\u6570\u636e\u96c6\u4e0a\u5173\u7cfb\u63d0\u53d6\u4efb\u52a1\u7684\u6027\u80fd\u63d0\u534715%\u523035%", "conclusion": "LLM + ASP\u7ed3\u5408\u7684\u65b9\u6cd5\u4e3a\u805a\u5408\u5b9e\u4f53-\u5173\u7cfb\u63d0\u53d6\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u3001\u6548\u679c\u597d\u4e14\u9700\u8981\u5c11\u91cf\u8bad\u7ec3\u6570\u636e\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.12680", "pdf": "https://arxiv.org/pdf/2508.12680", "abs": "https://arxiv.org/abs/2508.12680", "authors": ["Yuheng Zha", "Kun Zhou", "Yujia Wu", "Yushu Wang", "Jie Feng", "Zhi Xu", "Shibo Hao", "Zhengzhong Liu", "Eric P. Xing", "Zhiting Hu"], "title": "Vision-G1: Towards General Vision Language Reasoning with Multi-Domain Data Curation", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Despite their success, current training pipelines for reasoning VLMs focus on\na limited range of tasks, such as mathematical and logical reasoning. As a\nresult, these models face difficulties in generalizing their reasoning\ncapabilities to a wide range of domains, primarily due to the scarcity of\nreadily available and verifiable reward data beyond these narrowly defined\nareas. Moreover, integrating data from multiple domains is challenging, as the\ncompatibility between domain-specific datasets remains uncertain. To address\nthese limitations, we build a comprehensive RL-ready visual reasoning dataset\nfrom 46 data sources across 8 dimensions, covering a wide range of tasks such\nas infographic, mathematical, spatial, cross-image, graphic user interface,\nmedical, common sense and general science. We propose an influence function\nbased data selection and difficulty based filtering strategy to identify\nhigh-quality training samples from this dataset. Subsequently, we train the\nVLM, referred to as Vision-G1, using multi-round RL with a data curriculum to\niteratively improve its visual reasoning capabilities. Our model achieves\nstate-of-the-art performance across various visual reasoning benchmarks,\noutperforming similar-sized VLMs and even proprietary models like GPT-4o and\nGemini-1.5 Flash. The model, code and dataset are publicly available at\nhttps://github.com/yuh-zha/Vision-G1.", "AI": {"tldr": "\u6784\u5efa\u4e86\u6db5\u76d68\u4e2a\u7ef4\u5ea646\u4e2a\u6570\u636e\u6e90\u7684\u89c6\u89c9\u63a8\u7406\u6570\u636e\u96c6\uff0c\u63d0\u51fa\u57fa\u4e8e\u5f71\u54cd\u529b\u51fd\u6570\u7684\u6570\u636e\u9009\u62e9\u548c\u96be\u5ea6\u8fc7\u6ee4\u7b56\u7565\uff0c\u901a\u8fc7\u591a\u8f6eRL\u8bad\u7ec3Vision-G1\u6a21\u578b\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u4e3b\u8981\u5c40\u9650\u4e8e\u6570\u5b66\u548c\u903b\u8f91\u63a8\u7406\u4efb\u52a1\uff0c\u7f3a\u4e4f\u8de8\u9886\u57df\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e14\u591a\u9886\u57df\u6570\u636e\u6574\u5408\u56f0\u96be", "method": "\u6784\u5efa\u5927\u89c4\u6a21\u591a\u9886\u57df\u89c6\u89c9\u63a8\u7406\u6570\u636e\u96c6\uff0c\u91c7\u7528\u5f71\u54cd\u529b\u51fd\u6570\u6570\u636e\u9009\u62e9\u548c\u96be\u5ea6\u8fc7\u6ee4\u7b56\u7565\uff0c\u4f7f\u7528\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\u914d\u5408\u6570\u636e\u8bfe\u7a0b\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3", "result": "Vision-G1\u6a21\u578b\u5728\u591a\u4e2a\u89c6\u89c9\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u540c\u7c7b\u89c4\u6a21VLMs\u751a\u81f3GPT-4o\u548cGemini-1.5 Flash\u7b49\u4e13\u6709\u6a21\u578b", "conclusion": "\u63d0\u51fa\u7684\u591a\u9886\u57df\u6570\u636e\u96c6\u6784\u5efa\u65b9\u6cd5\u548c\u8bad\u7ec3\u7b56\u7565\u6709\u6548\u63d0\u5347\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u8de8\u9886\u57df\u89c6\u89c9\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.12790", "pdf": "https://arxiv.org/pdf/2508.12790", "abs": "https://arxiv.org/abs/2508.12790", "authors": ["Zenan Huang", "Yihong Zhuang", "Guoshan Lu", "Zeyu Qin", "Haokai Xu", "Tianyu Zhao", "Ru Peng", "Jiaqi Hu", "Zhanming Shen", "Xiaomeng Hu", "Xijun Gu", "Peiyi Tu", "Jiaxin Liu", "Wenyu Chen", "Yuzhuo Fu", "Zhiting Fan", "Yanmei Gu", "Yuanyuan Wang", "Zhengkai Yang", "Jianguo Li", "Junbo Zhao"], "title": "Reinforcement Learning with Rubric Anchors", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "technical report", "summary": "Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a\npowerful paradigm for enhancing Large Language Models (LLMs), exemplified by\nthe success of OpenAI's o-series. In RLVR, rewards are derived from verifiable\nsignals-such as passing unit tests in code generation or matching correct\nanswers in mathematical reasoning. While effective, this requirement largely\nconfines RLVR to domains with automatically checkable outcomes. To overcome\nthis, we extend the RLVR paradigm to open-ended tasks by integrating\nrubric-based rewards, where carefully designed rubrics serve as structured,\nmodel-interpretable criteria for automatic scoring of subjective outputs. We\nconstruct, to our knowledge, the largest rubric reward system to date, with\nover 10,000 rubrics from humans, LLMs, or a hybrid human-LLM collaboration.\nImplementing rubric-based RL is challenging; we tackle these issues with a\nclear framework and present an open-sourced Qwen-30B-A3B model with notable\ngains: 1) With only 5K+ samples, our system improves by +5.2% on open-ended\nbenchmarks (especially humanities), outperforming a 671B DeepSeek-V3 model by\n+2.4%, while preserving general and reasoning abilities. 2) Our method provides\nfine-grained stylistic control, using rubrics as anchors to mitigate the\n\"AI-like\" tone and produce more human-like, expressive responses. We share key\nlessons in rubric construction, data selection, and training, and discuss\nlimitations and future releases.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u91cf\u89c4\u5956\u52b1\u7684RLVR\u65b9\u6cd5\uff0c\u5c06\u53ef\u9a8c\u8bc1\u5956\u52b1\u5b66\u4e60\u6269\u5c55\u5230\u5f00\u653e\u5f0f\u4efb\u52a1\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u8bc4\u5206\u6807\u51c6\u5b9e\u73b0\u4e3b\u89c2\u8f93\u51fa\u7684\u81ea\u52a8\u8bc4\u5206\uff0c\u5728\u4ec5\u4f7f\u75285K+\u6837\u672c\u7684\u60c5\u51b5\u4e0b\u5728\u5f00\u653e\u5f0f\u57fa\u51c6\u4e0a\u63d0\u53475.2%", "motivation": "\u4f20\u7edfRLVR\u65b9\u6cd5\u4e3b\u8981\u5c40\u9650\u4e8e\u53ef\u81ea\u52a8\u68c0\u67e5\u7ed3\u679c\u7684\u9886\u57df\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u5f00\u653e\u5f0f\u4e3b\u89c2\u4efb\u52a1\uff0c\u9700\u8981\u6269\u5c55RLVR\u8303\u5f0f\u5230\u5f00\u653e\u5f0f\u4efb\u52a1\u9886\u57df", "method": "\u6784\u5efa\u4e86\u5305\u542b10,000+\u91cf\u89c4\u7684\u5956\u52b1\u7cfb\u7edf\uff08\u4eba\u5de5\u3001LLM\u751f\u6210\u6216\u4eba\u673a\u534f\u4f5c\uff09\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u91cf\u89c4\u4f5c\u4e3a\u7ed3\u6784\u5316\u3001\u6a21\u578b\u53ef\u89e3\u91ca\u7684\u8bc4\u5206\u6807\u51c6\uff0c\u5b9e\u73b0\u4e3b\u89c2\u8f93\u51fa\u7684\u81ea\u52a8\u8bc4\u5206", "result": "1) \u5728\u5f00\u653e\u5f0f\u57fa\u51c6\uff08\u7279\u522b\u662f\u4eba\u6587\u5b66\u79d1\uff09\u4e0a\u63d0\u53475.2%\uff0c\u8d85\u8d8a671B DeepSeek-V3\u6a21\u578b2.4%\uff1b2) \u63d0\u4f9b\u7ec6\u7c92\u5ea6\u98ce\u683c\u63a7\u5236\uff0c\u51cf\u8f7b\"AI\u8154\u8c03\"\uff0c\u4ea7\u751f\u66f4\u4eba\u6027\u5316\u3001\u5bcc\u6709\u8868\u73b0\u529b\u7684\u56de\u7b54", "conclusion": "\u57fa\u4e8e\u91cf\u89c4\u7684RL\u65b9\u6cd5\u4e3a\u5f00\u653e\u5f0f\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u5728\u4fdd\u6301\u901a\u7528\u548c\u63a8\u7406\u80fd\u529b\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e3b\u89c2\u4efb\u52a1\u8868\u73b0\uff0c\u5e76\u63d0\u4f9b\u4e86\u98ce\u683c\u63a7\u5236\u7684\u65b0\u9014\u5f84"}}
{"id": "2508.12792", "pdf": "https://arxiv.org/pdf/2508.12792", "abs": "https://arxiv.org/abs/2508.12792", "authors": ["Felipe Maia Polo", "Xinhe Wang", "Mikhail Yurochkin", "Gongjun Xu", "Moulinath Banerjee", "Yuekai Sun"], "title": "Bridging Human and LLM Judgments: Understanding and Narrowing the Gap", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": null, "summary": "Large language models are increasingly used as judges (LLM-as-a-judge) to\nevaluate model outputs at scale, but their assessments often diverge\nsystematically from human judgments. We present Bridge, a unified statistical\nframework that explicitly bridges human and LLM evaluations under both absolute\nscoring and pairwise comparison paradigms. Bridge posits a latent human\npreference score for each prompt-response pair and models LLM deviations as\nlinear transformations of covariates that capture sources of discrepancies.\nThis offers a simple and principled framework for refining LLM ratings and\ncharacterizing systematic discrepancies between humans and LLMs. We provide an\nefficient fitting algorithm with asymptotic guarantees for statistical\ninference. Using six LLM judges and two benchmarks (BigGen Bench and Chatbot\nArena), Bridge achieves higher agreement with human ratings (accuracy,\ncalibration, and KL divergence) and exposes systematic human-LLM gaps.", "AI": {"tldr": "Bridge\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u7edf\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u5efa\u6a21\u4eba\u7c7b\u504f\u597d\u548cLLM\u8bc4\u4f30\u4e4b\u95f4\u7684\u7cfb\u7edf\u6027\u5dee\u5f02\uff0c\u63d0\u9ad8LLM\u4f5c\u4e3a\u8bc4\u5224\u8005\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u88ab\u5e7f\u6cdb\u7528\u4f5c\u8bc4\u5224\u8005\u6765\u8bc4\u4f30\u6a21\u578b\u8f93\u51fa\uff0c\u4f46\u5176\u8bc4\u4f30\u7ed3\u679c\u4e0e\u4eba\u7c7b\u5224\u65ad\u5b58\u5728\u7cfb\u7edf\u6027\u5dee\u5f02\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u5f25\u5408\u8fd9\u79cd\u5dee\u8ddd\u3002", "method": "\u63d0\u51fa\u4e86Bridge\u7edf\u8ba1\u6846\u67b6\uff0c\u5047\u8bbe\u6bcf\u4e2a\u63d0\u793a-\u54cd\u5e94\u5bf9\u5b58\u5728\u6f5c\u5728\u4eba\u7c7b\u504f\u597d\u5206\u6570\uff0c\u5c06LLM\u504f\u5dee\u5efa\u6a21\u4e3a\u6355\u83b7\u5dee\u5f02\u6765\u6e90\u7684\u534f\u53d8\u91cf\u7684\u7ebf\u6027\u53d8\u6362\u3002\u63d0\u4f9b\u4e86\u5177\u6709\u6e10\u8fd1\u4fdd\u8bc1\u7684\u9ad8\u6548\u62df\u5408\u7b97\u6cd5\u3002", "result": "\u5728\u516d\u4e2aLLM\u8bc4\u5224\u8005\u548c\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5(BigGen Bench\u548cChatbot Arena)\u4e0a\uff0cBridge\u5b9e\u73b0\u4e86\u4e0e\u4eba\u7c7b\u8bc4\u5206\u66f4\u9ad8\u7684\u4e00\u81f4\u6027(\u51c6\u786e\u6027\u3001\u6821\u51c6\u548cKL\u6563\u5ea6)\uff0c\u5e76\u63ed\u793a\u4e86\u7cfb\u7edf\u6027\u7684\u4eba-LLM\u5dee\u8ddd\u3002", "conclusion": "Bridge\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u800c\u6709\u539f\u5219\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u6539\u8fdbLLM\u8bc4\u5206\u5e76\u8868\u5f81\u4eba\u7c7b\u4e0eLLM\u4e4b\u95f4\u7684\u7cfb\u7edf\u6027\u5dee\u5f02\uff0c\u4e3a\u89e3\u51b3LLM\u8bc4\u4f30\u504f\u5dee\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.12801", "pdf": "https://arxiv.org/pdf/2508.12801", "abs": "https://arxiv.org/abs/2508.12801", "authors": ["Bowen Dong", "Yilong Fan", "Yutao Sun", "Zhenyu Li", "Tengyu Pan", "Xun Zhou", "Jianyong Wang"], "title": "Maximum Score Routing For Mixture-of-Experts", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Routing networks in sparsely activated mixture-of-experts (MoE) dynamically\nallocate input tokens to top-k experts through differentiable sparse\ntransformations, enabling scalable model capacity while preserving\ncomputational efficiency. Traditional MoE networks impose an expert capacity\nconstraint to ensure GPU-friendly computation. However, this leads to token\ndropping when capacity is saturated and results in low hardware efficiency due\nto padding in underutilized experts. Removing the capacity constraint, in turn,\ncompromises load balancing and computational efficiency. To address these\nissues, we propose Maximum Score Routing ($\\mathbf{MaxScore}$), a novel MoE\nrouting paradigm that models routing as a minimum-cost maximum-flow problem and\nintegrates a SoftTopk operator. MaxScore resolves the fundamental limitations\nof iterative rerouting and optimal transport formulations, achieving lower\ntraining losses and higher evaluation scores at equivalent FLOPs compared to\nboth constrained and unconstrained baselines. Implementation details and\nexperimental configurations can be obtained from\n$\\href{https://github.com/dongbw18/MaxScore.git}{MaxScore}$.", "AI": {"tldr": "MaxScore\u662f\u4e00\u79cd\u65b0\u7684MoE\u8def\u7531\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5c0f\u6210\u672c\u6700\u5927\u6d41\u95ee\u9898\u548cSoftTopk\u7b97\u5b50\u89e3\u51b3\u4f20\u7edfMoE\u7f51\u7edc\u4e2d\u7684\u4ee4\u724c\u4e22\u5f03\u548c\u786c\u4ef6\u6548\u7387\u95ee\u9898\uff0c\u5728\u76f8\u540cFLOPs\u4e0b\u83b7\u5f97\u66f4\u597d\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfMoE\u7f51\u7edc\u5b58\u5728\u4e13\u5bb6\u5bb9\u91cf\u7ea6\u675f\u5bfc\u81f4\u4ee4\u724c\u4e22\u5f03\u548c\u786c\u4ef6\u6548\u7387\u4f4e\u4e0b\uff0c\u800c\u65e0\u7ea6\u675f\u65b9\u6cd5\u53c8\u4f1a\u635f\u5bb3\u8d1f\u8f7d\u5e73\u8861\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u9700\u8981\u65b0\u7684\u8def\u7531\u8303\u5f0f\u6765\u89e3\u51b3\u8fd9\u4e9b\u6839\u672c\u95ee\u9898\u3002", "method": "\u63d0\u51faMaximum Score Routing (MaxScore)\uff0c\u5c06\u8def\u7531\u5efa\u6a21\u4e3a\u6700\u5c0f\u6210\u672c\u6700\u5927\u6d41\u95ee\u9898\uff0c\u5e76\u96c6\u6210SoftTopk\u7b97\u5b50\uff0c\u907f\u514d\u4e86\u8fed\u4ee3\u91cd\u8def\u7531\u548c\u6700\u4f18\u4f20\u8f93\u516c\u5f0f\u7684\u5c40\u9650\u6027\u3002", "result": "\u5728\u76f8\u540cFLOPs\u4e0b\uff0c\u76f8\u6bd4\u6709\u7ea6\u675f\u548c\u65e0\u7ea6\u675f\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0cMaxScore\u5b9e\u73b0\u4e86\u66f4\u4f4e\u7684\u8bad\u7ec3\u635f\u5931\u548c\u66f4\u9ad8\u7684\u8bc4\u4f30\u5206\u6570\u3002", "conclusion": "MaxScore\u901a\u8fc7\u521b\u65b0\u7684\u8def\u7531\u8303\u5f0f\u6709\u6548\u89e3\u51b3\u4e86MoE\u7f51\u7edc\u4e2d\u7684\u5bb9\u91cf\u7ea6\u675f\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2508.12815", "pdf": "https://arxiv.org/pdf/2508.12815", "abs": "https://arxiv.org/abs/2508.12815", "authors": ["Jayneel Parekh", "Pegah Khayatan", "Mustafa Shukor", "Arnaud Dapogny", "Alasdair Newson", "Matthieu Cord"], "title": "Learning to Steer: Input-dependent Steering for Multimodal LLMs", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Steering has emerged as a practical approach to enable post-hoc guidance of\nLLMs towards enforcing a specific behavior. However, it remains largely\nunderexplored for multimodal LLMs (MLLMs); furthermore, existing steering\ntechniques, such as mean steering, rely on a single steering vector, applied\nindependently of the input query. This paradigm faces limitations when the\ndesired behavior is dependent on the example at hand. For example, a safe\nanswer may consist in abstaining from answering when asked for an illegal\nactivity, or may point to external resources or consultation with an expert\nwhen asked about medical advice. In this paper, we investigate a fine-grained\nsteering that uses an input-specific linear shift. This shift is computed using\ncontrastive input-specific prompting. However, the input-specific prompts\nrequired for this approach are not known at test time. Therefore, we propose to\ntrain a small auxiliary module to predict the input-specific steering vector.\nOur approach, dubbed as L2S (Learn-to-Steer), demonstrates that it reduces\nhallucinations and enforces safety in MLLMs, outperforming other static\nbaselines.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86L2S\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bad\u7ec3\u8f85\u52a9\u6a21\u5757\u9884\u6d4b\u8f93\u5165\u7279\u5f02\u7684\u63a7\u5236\u5411\u91cf\uff0c\u5b9e\u73b0\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7ec6\u7c92\u5ea6\u63a7\u5236\uff0c\u5728\u51cf\u5c11\u5e7b\u89c9\u548c\u63d0\u9ad8\u5b89\u5168\u6027\u65b9\u9762\u8d85\u8d8a\u4e86\u9759\u6001\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u63a7\u5236\u6280\u672f\u4f9d\u8d56\u5355\u4e00\u9759\u6001\u5411\u91cf\uff0c\u65e0\u6cd5\u5904\u7406\u4e0d\u540c\u8f93\u5165\u67e5\u8be2\u7684\u7279\u5f02\u6027\u9700\u6c42\uff0c\u6bd4\u5982\u533b\u7597\u5efa\u8bae\u9700\u8981\u6307\u5411\u5916\u90e8\u8d44\u6e90\uff0c\u800c\u975e\u6cd5\u6d3b\u52a8\u9700\u8981\u62d2\u7edd\u56de\u7b54\u3002", "method": "\u63d0\u51faL2S\u65b9\u6cd5\uff0c\u4f7f\u7528\u5bf9\u6bd4\u8bed\u63d0\u751f\u6210\u8f93\u5165\u7279\u5f02\u7684\u7ebf\u6027\u504f\u79fb\uff0c\u5e76\u8bad\u7ec3\u4e00\u4e2a\u5c0f\u578b\u8f85\u52a9\u6a21\u5757\u6765\u9884\u6d4b\u8fd9\u4e9b\u63a7\u5236\u5411\u91cf\uff0c\u4ee5\u6d88\u9664\u6d4b\u8bd5\u65f6\u77e5\u8bc6\u7f3a\u5931\u7684\u95ee\u9898\u3002", "result": "L2S\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u663e\u8457\u51cf\u5c11\u4e86\u5e7b\u89c9\u73b0\u8c61\uff0c\u540c\u65f6\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u7684\u5b89\u5168\u6027\u80fd\u529b\uff0c\u8868\u73b0\u8d85\u8fc7\u4e86\u5176\u4ed6\u9759\u6001\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8f93\u5165\u7279\u5f02\u7684\u7ec6\u7c92\u5ea6\u63a7\u5236\u65b9\u6cd5\u80fd\u591f\u66f4\u6709\u6548\u5730\u5bfc\u5411\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u884c\u4e3a\uff0cL2S\u901a\u8fc7\u5b66\u4e60\u9884\u6d4b\u63a7\u5236\u5411\u91cf\u7684\u65b9\u5f0f\u5b9e\u73b0\u4e86\u8fd9\u4e00\u76ee\u6807\uff0c\u4e3a\u63a7\u5236\u6280\u672f\u5728\u591a\u6a21\u6001\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002"}}
{"id": "2508.12854", "pdf": "https://arxiv.org/pdf/2508.12854", "abs": "https://arxiv.org/abs/2508.12854", "authors": ["Ronghao Lin", "Shuai Shen", "Weipeng Hu", "Qiaolin He", "Aolin Xiong", "Li Huang", "Haifeng Hu", "Yap-peng Tan"], "title": "E3RG: Building Explicit Emotion-driven Empathetic Response Generation System with Multimodal Large Language Model", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC", "cs.MM"], "comment": "Accepted at ACM MM 2025 Grand Challenge", "summary": "Multimodal Empathetic Response Generation (MERG) is crucial for building\nemotionally intelligent human-computer interactions. Although large language\nmodels (LLMs) have improved text-based ERG, challenges remain in handling\nmultimodal emotional content and maintaining identity consistency. Thus, we\npropose E3RG, an Explicit Emotion-driven Empathetic Response Generation System\nbased on multimodal LLMs which decomposes MERG task into three parts:\nmultimodal empathy understanding, empathy memory retrieval, and multimodal\nresponse generation. By integrating advanced expressive speech and video\ngenerative models, E3RG delivers natural, emotionally rich, and\nidentity-consistent responses without extra training. Experiments validate the\nsuperiority of our system on both zero-shot and few-shot settings, securing\nTop-1 position in the Avatar-based Multimodal Empathy Challenge on ACM MM 25.\nOur code is available at https://github.com/RH-Lin/E3RG.", "AI": {"tldr": "E3RG\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u663e\u5f0f\u60c5\u611f\u9a71\u52a8\u5171\u60c5\u54cd\u5e94\u751f\u6210\u7cfb\u7edf\uff0c\u901a\u8fc7\u5c06\u591a\u6a21\u6001\u5171\u60c5\u54cd\u5e94\u751f\u6210\u4efb\u52a1\u5206\u89e3\u4e3a\u4e09\u4e2a\u90e8\u5206\u6765\u5b9e\u73b0\u81ea\u7136\u3001\u60c5\u611f\u4e30\u5bcc\u4e14\u8eab\u4efd\u4e00\u81f4\u7684\u54cd\u5e94\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u6539\u8fdb\u4e86\u57fa\u4e8e\u6587\u672c\u7684\u5171\u60c5\u54cd\u5e94\u751f\u6210\uff0c\u4f46\u5728\u5904\u7406\u591a\u6a21\u6001\u60c5\u611f\u5185\u5bb9\u548c\u4fdd\u6301\u8eab\u4efd\u4e00\u81f4\u6027\u65b9\u9762\u4ecd\u5b58\u5728\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5904\u7406\u591a\u6a21\u6001\u60c5\u611f\u5185\u5bb9\u5e76\u4fdd\u6301\u8eab\u4efd\u4e00\u81f4\u6027\u7684\u7cfb\u7edf\u3002", "method": "\u5c06\u591a\u6a21\u6001\u5171\u60c5\u54cd\u5e94\u751f\u6210\u4efb\u52a1\u5206\u89e3\u4e3a\u4e09\u4e2a\u90e8\u5206\uff1a\u591a\u6a21\u6001\u5171\u60c5\u7406\u89e3\u3001\u5171\u60c5\u8bb0\u5fc6\u68c0\u7d22\u548c\u591a\u6a21\u6001\u54cd\u5e94\u751f\u6210\uff0c\u5e76\u6574\u5408\u5148\u8fdb\u7684\u8868\u8fbe\u6027\u8bed\u97f3\u548c\u89c6\u9891\u751f\u6210\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u7684\u4f18\u8d8a\u6027\uff0c\u5728ACM MM 25\u7684\u57fa\u4e8eAvatar\u7684\u591a\u6a21\u6001\u5171\u60c5\u6311\u6218\u4e2d\u83b7\u5f97Top-1\u4f4d\u7f6e\u3002", "conclusion": "E3RG\u7cfb\u7edf\u80fd\u591f\u751f\u6210\u81ea\u7136\u3001\u60c5\u611f\u4e30\u5bcc\u4e14\u8eab\u4efd\u4e00\u81f4\u7684\u591a\u6a21\u6001\u5171\u60c5\u54cd\u5e94\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0c\u5728\u591a\u6a21\u6001\u5171\u60c5\u54cd\u5e94\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2508.12905", "pdf": "https://arxiv.org/pdf/2508.12905", "abs": "https://arxiv.org/abs/2508.12905", "authors": ["Ismail Lamaakal", "Chaymae Yahyati", "Khalid El Makkaoui", "Ibrahim Ouahbi", "Yassine Maleh"], "title": "TCUQ: Single-Pass Uncertainty Quantification from Temporal Consistency with Streaming Conformal Calibration for TinyML", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "We introduce TCUQ, a single pass, label free uncertainty monitor for\nstreaming TinyML that converts short horizon temporal consistency captured via\nlightweight signals on posteriors and features into a calibrated risk score\nwith an O(W ) ring buffer and O(1) per step updates. A streaming conformal\nlayer turns this score into a budgeted accept/abstain rule, yielding calibrated\nbehavior without online labels or extra forward passes. On microcontrollers,\nTCUQ fits comfortably on kilobyte scale devices and reduces footprint and\nlatency versus early exit and deep ensembles (typically about 50 to 60% smaller\nand about 30 to 45% faster), while methods of similar accuracy often run out of\nmemory. Under corrupted in distribution streams, TCUQ improves accuracy drop\ndetection by 3 to 7 AUPRC points and reaches up to 0.86 AUPRC at high\nseverities; for failure detection it attains up to 0.92 AUROC. These results\nshow that temporal consistency, coupled with streaming conformal calibration,\nprovides a practical and resource efficient foundation for on device monitoring\nin TinyML.", "AI": {"tldr": "TCUQ\u662f\u4e00\u79cd\u7528\u4e8eTinyML\u6d41\u5f0f\u5904\u7406\u7684\u5355\u6b21\u901a\u8fc7\u3001\u65e0\u6807\u7b7e\u4e0d\u786e\u5b9a\u6027\u76d1\u6d4b\u5668\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u4fe1\u53f7\u5c06\u77ed\u671f\u65f6\u95f4\u4e00\u81f4\u6027\u8f6c\u6362\u4e3a\u6821\u51c6\u98ce\u9669\u5206\u6570\uff0c\u5177\u6709O(W)\u73af\u5f62\u7f13\u51b2\u533a\u548cO(1)\u6bcf\u6b65\u66f4\u65b0\u3002", "motivation": "\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u5fae\u63a7\u5236\u5668\u8bbe\u5907\u63d0\u4f9b\u9ad8\u6548\u7684\u4e0d\u786e\u5b9a\u6027\u76d1\u6d4b\u89e3\u51b3\u65b9\u6848\uff0c\u907f\u514d\u4f20\u7edf\u65b9\u6cd5\u5982\u65e9\u671f\u9000\u51fa\u548c\u6df1\u5ea6\u96c6\u6210\u7684\u9ad8\u5185\u5b58\u5360\u7528\u548c\u5ef6\u8fdf\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u6d41\u5f0f\u4fdd\u5f62\u6821\u51c6\u5c42\u5c06\u65f6\u95f4\u4e00\u81f4\u6027\u5206\u6570\u8f6c\u6362\u4e3a\u9884\u7b97\u5316\u7684\u63a5\u53d7/\u5f03\u6743\u89c4\u5219\uff0c\u65e0\u9700\u5728\u7ebf\u6807\u7b7e\u6216\u989d\u5916\u524d\u5411\u4f20\u64ad\u3002", "result": "\u5728\u5fae\u63a7\u5236\u5668\u4e0a\uff0cTCUQ\u6bd4\u65e9\u671f\u9000\u51fa\u548c\u6df1\u5ea6\u96c6\u6210\u51cf\u5c11\u7ea650-60%\u7684\u5360\u7528\u7a7a\u95f4\u548c30-45%\u7684\u5ef6\u8fdf\uff0c\u5728\u635f\u574f\u5206\u5e03\u6d41\u4e2d\u63d0\u9ad83-7\u4e2aAUPRC\u70b9\u7684\u7cbe\u5ea6\u4e0b\u964d\u68c0\u6d4b\uff0c\u8fbe\u52300.86 AUPRC\uff0c\u6545\u969c\u68c0\u6d4b\u8fbe\u52300.92 AUROC\u3002", "conclusion": "\u65f6\u95f4\u4e00\u81f4\u6027\u7ed3\u5408\u6d41\u5f0f\u4fdd\u5f62\u6821\u51c6\u4e3aTinyML\u8bbe\u5907\u76d1\u6d4b\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u8d44\u6e90\u9ad8\u6548\u7684\u57fa\u7840\u3002"}}
{"id": "2508.12907", "pdf": "https://arxiv.org/pdf/2508.12907", "abs": "https://arxiv.org/abs/2508.12907", "authors": ["Ismail Lamaakal", "Chaymae Yahyati", "Khalid El Makkaoui", "Ibrahim Ouahbi", "Yassine Maleh"], "title": "SNAP-UQ: Self-supervised Next-Activation Prediction for Single-Pass Uncertainty in TinyML", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "We introduce \\textbf{SNAP-UQ}, a single-pass, label-free uncertainty method\nfor TinyML that estimates risk from \\emph{depth-wise next-activation\nprediction}: tiny int8 heads forecast the statistics of the next layer from a\ncompressed view of the previous one, and a lightweight monotone mapper turns\nthe resulting surprisal into an actionable score. The design requires no\ntemporal buffers, auxiliary exits, or repeated forward passes, and adds only a\nfew tens of kilobytes to MCU deployments. Across vision and audio backbones,\nSNAP-UQ consistently reduces flash and latency relative to early-exit and deep\nensembles (typically $\\sim$40--60\\% smaller and $\\sim$25--35\\% faster), with\ncompeting methods of similar accuracy often exceeding memory limits. In\ncorrupted streams it improves accuracy-drop detection by several AUPRC points\nand maintains strong failure detection (AUROC $\\approx$0.9) in a single pass.\nGrounding uncertainty in layer-to-layer dynamics yields a practical,\nresource-efficient basis for on-device monitoring in TinyML.", "AI": {"tldr": "SNAP-UQ\u662f\u4e00\u79cd\u9762\u5411TinyML\u7684\u5355\u6b21\u524d\u5411\u4f20\u64ad\u3001\u65e0\u9700\u6807\u7b7e\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df1\u5ea6\u6fc0\u6d3b\u9884\u6d4b\u6765\u4f30\u8ba1\u98ce\u9669\uff0c\u5177\u6709\u8d44\u6e90\u9ad8\u6548\u7684\u7279\u70b9\u3002", "motivation": "\u5728TinyML\u8bbe\u5907\u4e0a\u90e8\u7f72\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u65f6\uff0c\u4f20\u7edf\u65b9\u6cd5\u5982\u65e9\u9000\u673a\u5236\u548c\u6df1\u5ea6\u96c6\u6210\u9700\u8981\u989d\u5916\u7684\u8ba1\u7b97\u5f00\u9500\u3001\u5185\u5b58\u7f13\u51b2\u6216\u591a\u8f6e\u524d\u5411\u4f20\u64ad\uff0c\u8fd9\u5728\u8d44\u6e90\u53d7\u9650\u7684\u5fae\u63a7\u5236\u5668\u5355\u5143(MCU)\u4e0a\u4e0d\u53ef\u884c\u3002\u9700\u8981\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u5355\u6b21\u524d\u5411\u4f20\u64ad\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528int8\u7cbe\u5ea6\u7684\u5fae\u5c0f\u9884\u6d4b\u5934\u6765\u4ece\u538b\u7f29\u7684\u524d\u4e00\u5c42\u89c6\u56fe\u9884\u6d4b\u4e0b\u4e00\u5c42\u7684\u7edf\u8ba1\u4fe1\u606f\uff0c\u7136\u540e\u901a\u8fc7\u8f7b\u91cf\u7ea7\u5355\u8c03\u6620\u5c04\u5668\u5c06\u9884\u6d4b\u7684\u60ca\u8bb6\u5ea6\u8f6c\u6362\u4e3a\u53ef\u64cd\u4f5c\u7684\u5206\u6570\u3002\u65e0\u9700\u65f6\u95f4\u7f13\u51b2\u3001\u8f85\u52a9\u9000\u51fa\u6216\u91cd\u590d\u524d\u5411\u4f20\u64ad\u3002", "result": "\u5728\u89c6\u89c9\u548c\u97f3\u9891\u9aa8\u5e72\u7f51\u7edc\u4e0a\uff0cSNAP-UQ\u76f8\u6bd4\u65e9\u9000\u673a\u5236\u548c\u6df1\u5ea6\u96c6\u6210\u51cf\u5c11\u4e86\u7ea640-60%\u7684\u5b58\u50a8\u7a7a\u95f4\u548c25-35%\u7684\u5ef6\u8fdf\uff0c\u5177\u6709\u76f8\u4f3c\u7cbe\u5ea6\u7684\u7ade\u4e89\u65b9\u6cd5\u5f80\u5f80\u8d85\u51fa\u5185\u5b58\u9650\u5236\u3002\u5728\u635f\u574f\u6570\u636e\u6d41\u4e2d\uff0cAUPRC\u63d0\u9ad8\u4e86\u51e0\u4e2a\u70b9\uff0c\u5355\u6b21\u524d\u5411\u4f20\u64ad\u4e0bAUROC\u22480.9\u7684\u5f3a\u6545\u969c\u68c0\u6d4b\u80fd\u529b\u3002", "conclusion": "\u57fa\u4e8e\u5c42\u95f4\u52a8\u6001\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u4e3aTinyML\u8bbe\u5907\u4e0a\u7684\u5b9e\u65f6\u76d1\u63a7\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u4e14\u8d44\u6e90\u9ad8\u6548\u7684\u57fa\u7840\uff0c\u80fd\u591f\u5728\u5355\u6b21\u524d\u5411\u4f20\u64ad\u4e2d\u5b9e\u73b0\u6709\u6548\u7684\u98ce\u9669\u4f30\u8ba1\u3002"}}
{"id": "2508.13021", "pdf": "https://arxiv.org/pdf/2508.13021", "abs": "https://arxiv.org/abs/2508.13021", "authors": ["Pengcheng Huang", "Shuhao Liu", "Zhenghao Liu", "Yukun Yan", "Shuo Wang", "Zulong Chen", "Tong Xiao"], "title": "PC-Sampler: Position-Aware Calibration of Decoding Bias in Masked Diffusion Models", "categories": ["cs.AI", "cs.CL"], "comment": "17 pages,13 figures", "summary": "Recent advances in masked diffusion models (MDMs) have established them as\npowerful non-autoregressive alternatives for sequence generation. Nevertheless,\nour preliminary experiments reveal that the generation quality of MDMs is still\nhighly sensitive to the choice of decoding strategy. In particular, widely\nadopted uncertainty-based samplers suffer from two key limitations: a lack of\nglobal trajectory control and a pronounced bias toward trivial tokens in the\nearly stages of decoding. These shortcomings restrict the full potential of\nMDMs. In this work, we introduce Position-Aware Confidence-Calibrated Sampling\n(PC-Sampler), a novel decoding strategy that unifies global trajectory planning\nwith content-aware informativeness maximization. PC-Sampler incorporates a\nposition-aware weighting mechanism to regulate the decoding path and a\ncalibrated confidence score to suppress the premature selection of trivial\ntokens. Extensive experiments on three advanced MDMs across seven challenging\nbenchmarks-including logical reasoning and planning tasks-demonstrate that\nPC-Sampler consistently outperforms existing MDM decoding strategies by more\nthan 10% on average, significantly narrowing the performance gap with\nstate-of-the-art autoregressive models. All codes are available at\nhttps://github.com/NEUIR/PC-Sampler.", "AI": {"tldr": "PC-Sampler\u662f\u4e00\u79cd\u65b0\u7684\u89e3\u7801\u7b56\u7565\uff0c\u901a\u8fc7\u4f4d\u7f6e\u611f\u77e5\u6743\u91cd\u548c\u7f6e\u4fe1\u5ea6\u6821\u51c6\u6765\u89e3\u51b3\u63a9\u7801\u6269\u6563\u6a21\u578b\u89e3\u7801\u4e2d\u7684\u5168\u5c40\u8f68\u8ff9\u63a7\u5236\u548c\u65e9\u671f\u5e73\u51e1token\u504f\u5411\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u63d0\u534710%\u4ee5\u4e0a\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u63a9\u7801\u6269\u6563\u6a21\u578b(MDMs)\u7684\u89e3\u7801\u7b56\u7565\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u9650\u5236\uff1a\u7f3a\u4e4f\u5168\u5c40\u8f68\u8ff9\u63a7\u5236\u548c\u89e3\u7801\u65e9\u671f\u5bf9\u5e73\u51e1token\u7684\u660e\u663e\u504f\u5411\uff0c\u8fd9\u9650\u5236\u4e86MDMs\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u4f4d\u7f6e\u611f\u77e5\u7f6e\u4fe1\u5ea6\u6821\u51c6\u91c7\u6837(PC-Sampler)\uff0c\u7ed3\u5408\u4f4d\u7f6e\u611f\u77e5\u6743\u91cd\u673a\u5236\u6765\u8c03\u8282\u89e3\u7801\u8def\u5f84\uff0c\u5e76\u4f7f\u7528\u6821\u51c6\u7f6e\u4fe1\u5ea6\u5206\u6570\u6765\u6291\u5236\u65e9\u671f\u9009\u62e9\u5e73\u51e1token\u3002", "result": "\u57287\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\uff08\u5305\u62ec\u903b\u8f91\u63a8\u7406\u548c\u89c4\u5212\u4efb\u52a1\uff09\u4e0a\uff0cPC-Sampler\u5e73\u5747\u6bd4\u73b0\u6709MDM\u89e3\u7801\u7b56\u7565\u63d0\u5347\u8d85\u8fc710%\uff0c\u663e\u8457\u7f29\u5c0f\u4e86\u4e0e\u6700\u5148\u8fdb\u81ea\u56de\u5f52\u6a21\u578b\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "conclusion": "PC-Sampler\u901a\u8fc7\u7edf\u4e00\u7684\u5168\u5c40\u8f68\u8ff9\u89c4\u5212\u548c\u5185\u5bb9\u611f\u77e5\u4fe1\u606f\u6700\u5927\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86MDMs\u89e3\u7801\u7b56\u7565\u7684\u5173\u952e\u9650\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2508.13142", "pdf": "https://arxiv.org/pdf/2508.13142", "abs": "https://arxiv.org/abs/2508.13142", "authors": ["Zhongang Cai", "Yubo Wang", "Qingping Sun", "Ruisi Wang", "Chenyang Gu", "Wanqi Yin", "Zhiqian Lin", "Zhitao Yang", "Chen Wei", "Xuanke Shi", "Kewang Deng", "Xiaoyang Han", "Zukai Chen", "Jiaqi Li", "Xiangyu Fan", "Hanming Deng", "Lewei Lu", "Bo Li", "Ziwei Liu", "Quan Wang", "Dahua Lin", "Lei Yang"], "title": "Has GPT-5 Achieved Spatial Intelligence? An Empirical Study", "categories": ["cs.CV", "cs.CL", "cs.LG", "cs.MM", "cs.RO"], "comment": null, "summary": "Multi-modal models have achieved remarkable progress in recent years.\nNevertheless, they continue to exhibit notable limitations in spatial\nunderstanding and reasoning, which are fundamental capabilities to achieving\nartificial general intelligence. With the recent release of GPT-5, allegedly\nthe most powerful AI model to date, it is timely to examine where the leading\nmodels stand on the path toward spatial intelligence. First, we propose a\ncomprehensive taxonomy of spatial tasks that unifies existing benchmarks and\ndiscuss the challenges in ensuring fair evaluation. We then evaluate\nstate-of-the-art proprietary and open-source models on eight key benchmarks, at\na cost exceeding one billion total tokens. Our empirical study reveals that (1)\nGPT-5 demonstrates unprecedented strength in spatial intelligence, yet (2)\nstill falls short of human performance across a broad spectrum of tasks.\nMoreover, we (3) identify the more challenging spatial intelligence problems\nfor multi-modal models, and (4) proprietary models do not exhibit a decisive\nadvantage when facing the most difficult problems. In addition, we conduct a\nqualitative evaluation across a diverse set of scenarios that are intuitive for\nhumans yet fail even the most advanced multi-modal models.", "AI": {"tldr": "GPT-5\u5728\u591a\u6a21\u6001\u7a7a\u95f4\u667a\u80fd\u65b9\u9762\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\u4f46\u4ecd\u843d\u540e\u4e8e\u4eba\u7c7b\u6c34\u5e73\uff0c\u7814\u7a76\u53d1\u73b0\u4e13\u6709\u6a21\u578b\u5728\u6700\u56f0\u96be\u95ee\u9898\u4e0a\u5e76\u65e0\u51b3\u5b9a\u6027\u4f18\u52bf", "motivation": "\u8bc4\u4f30\u5f53\u524d\u6700\u5148\u8fdb\u591a\u6a21\u6001\u6a21\u578b\u5728\u7a7a\u95f4\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u65b9\u9762\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u65b0\u53d1\u5e03\u7684GPT-5\u6a21\u578b\uff0c\u4ee5\u4e86\u89e3AI\u5728\u7a7a\u95f4\u667a\u80fd\u65b9\u9762\u7684\u53d1\u5c55\u73b0\u72b6", "method": "\u63d0\u51fa\u7edf\u4e00\u7684\u7a7a\u95f4\u4efb\u52a1\u5206\u7c7b\u6cd5\uff0c\u57288\u4e2a\u5173\u952e\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8bc4\u4f30\u6700\u5148\u8fdb\u7684\u4e13\u6709\u548c\u5f00\u6e90\u6a21\u578b\uff0c\u6d88\u8017\u8d85\u8fc710\u4ebf\u4e2atoken\uff0c\u5e76\u8fdb\u884c\u5b9a\u6027\u8bc4\u4f30", "result": "GPT-5\u5c55\u73b0\u51fa\u524d\u6240\u672a\u6709\u7684\u7a7a\u95f4\u667a\u80fd\u80fd\u529b\uff0c\u4f46\u5728\u5e7f\u6cdb\u4efb\u52a1\u4e2d\u4ecd\u4e0d\u53ca\u4eba\u7c7b\u8868\u73b0\uff1b\u4e13\u6709\u6a21\u578b\u5728\u6700\u56f0\u96be\u95ee\u9898\u4e0a\u6ca1\u6709\u660e\u663e\u4f18\u52bf\uff1b\u8bc6\u522b\u51fa\u591a\u6a21\u6001\u6a21\u578b\u66f4\u5177\u6311\u6218\u6027\u7684\u7a7a\u95f4\u667a\u80fd\u95ee\u9898", "conclusion": "\u591a\u6a21\u6001\u6a21\u578b\u5728\u7a7a\u95f4\u667a\u80fd\u65b9\u9762\u867d\u6709\u8fdb\u6b65\u4f46\u4ecd\u5b58\u5728\u663e\u8457\u5c40\u9650\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u6765\u5f25\u5408\u4e0e\u4eba\u7c7b\u80fd\u529b\u7684\u5dee\u8ddd"}}
