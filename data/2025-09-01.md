<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 34]
- [cs.AI](#cs.AI) [Total: 2]
- [physics.ed-ph](#physics.ed-ph) [Total: 1]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.HC](#cs.HC) [Total: 2]
- [cs.LG](#cs.LG) [Total: 4]
- [cs.CV](#cs.CV) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [CoBA: Counterbias Text Augmentation for Mitigating Various Spurious Correlations via Semantic Triples](https://arxiv.org/abs/2508.21083)
*Kyohoon Jin,Juhwan Choi,Jungmin Yun,Junho Lee,Soojin Jang,Youngbin Kim*

Main category: cs.CL

TL;DR: 提出CoBA框架，通过语义三元组分解和选择性修改来生成反偏见数据，解决深度学习模型对虚假相关性的依赖问题


<details>
  <summary>Details</summary>
Motivation: 深度学习模型经常学习和利用训练数据中的虚假相关性，使用非目标特征进行预测，导致性能下降和泛化能力差

Method: CoBA反偏见增强框架：在语义三元组层面操作，先分解文本为主谓宾三元组，然后选择性修改这些三元组来破坏虚假相关性，最后从调整后的三元组重构文本生成反偏见数据

Result: 实验表明CoBA不仅提高了下游任务性能，还能有效减少偏见并增强分布外鲁棒性

Conclusion: CoBA提供了一个通用且强大的解决方案，能够同时处理多种偏见并增强模型对虚假相关性挑战的鲁棒性

Abstract: Deep learning models often learn and exploit spurious correlations in
training data, using these non-target features to inform their predictions.
Such reliance leads to performance degradation and poor generalization on
unseen data. To address these limitations, we introduce a more general form of
counterfactual data augmentation, termed counterbias data augmentation, which
simultaneously tackles multiple biases (e.g., gender bias, simplicity bias) and
enhances out-of-distribution robustness. We present CoBA: CounterBias
Augmentation, a unified framework that operates at the semantic triple level:
first decomposing text into subject-predicate-object triples, then selectively
modifying these triples to disrupt spurious correlations. By reconstructing the
text from these adjusted triples, CoBA generates counterbias data that
mitigates spurious patterns. Through extensive experiments, we demonstrate that
CoBA not only improves downstream task performance, but also effectively
reduces biases and strengthens out-of-distribution resilience, offering a
versatile and robust solution to the challenges posed by spurious correlations.

</details>


### [2] [Mapping Toxic Comments Across Demographics: A Dataset from German Public Broadcasting](https://arxiv.org/abs/2508.21084)
*Jan Fillies,Michael Peter Hoffmann,Rebecca Reichel,Roman Salzwedel,Sven Bodemer,Adrian Paschke*

Main category: cs.CL

TL;DR: 首个大规模德语数据集，包含毒性标注和平台提供的年龄估计，揭示不同年龄段用户的网络毒性言论模式差异


<details>
  <summary>Details</summary>
Motivation: 现有毒性言论数据集缺乏人口统计背景，限制了我们对不同年龄段在线交流方式的理解

Method: 与德国公共服务内容网络funk合作，收集Instagram、TikTok和YouTube的匿名评论，使用预定义毒性关键词筛选，结合人工标注和LLM标注的混合标注流程

Result: 数据集包含3,024条人工标注和30,024条LLM标注评论，16.7%被标记为有问题，发现年轻用户偏好表达性语言，年长用户更多参与虚假信息和贬低行为

Conclusion: 该资源为研究跨人口统计的语言变异提供了新机会，支持开发更公平和年龄感知的内容审核系统

Abstract: A lack of demographic context in existing toxic speech datasets limits our
understanding of how different age groups communicate online. In collaboration
with funk, a German public service content network, this research introduces
the first large-scale German dataset annotated for toxicity and enriched with
platform-provided age estimates. The dataset includes 3,024 human-annotated and
30,024 LLM-annotated anonymized comments from Instagram, TikTok, and YouTube.
To ensure relevance, comments were consolidated using predefined toxic
keywords, resulting in 16.7\% labeled as problematic. The annotation pipeline
combined human expertise with state-of-the-art language models, identifying key
categories such as insults, disinformation, and criticism of broadcasting fees.
The dataset reveals age-based differences in toxic speech patterns, with
younger users favoring expressive language and older users more often engaging
in disinformation and devaluation. This resource provides new opportunities for
studying linguistic variation across demographics and supports the development
of more equitable and age-aware content moderation systems.

</details>


### [3] [Granite Embedding R2 Models](https://arxiv.org/abs/2508.21085)
*Parul Awasthy,Aashka Trivedi,Yulong Li,Meet Doshi,Riyaz Bhat,Vignesh P,Vishwajeet Kumar,Yushu Yang,Bhavani Iyer,Abraham Daniels,Rudra Murthy,Ken Barker,Martin Franz,Madison Lee,Todd Ward,Salim Roukos,David Cox,Luis Lastras,Jaydeep Sen,Radu Florian*

Main category: cs.CL

TL;DR: Granite Embedding R2模型是IBM推出的第二代高性能英文编码器嵌入模型，支持8192 tokens上下文长度，在检索速度上比竞品快19-44%，同时保持高精度，适用于企业级密集检索应用。


<details>
  <summary>Details</summary>
Motivation: 企业级密集检索应用对检索速度和准确性要求越来越高，需要能够处理多样化检索场景（文本、代码、长文档、多轮对话、表格数据）的高性能嵌入模型。

Method: 基于第一代模型改进，采用bi-encoder和cross-encoder架构，包含22层检索模型和高效的12层对应版本，以及高质量的重排序模型，全部使用企业适用数据训练并接受全面治理监督。

Result: 在标准基准测试、IBM开发的评估套件和实际企业用例中表现出卓越的多功能性，为开源嵌入模型设立了新的性能标准，检索速度比领先竞品快19-44%。

Conclusion: Granite R2模型提供了前沿性能、企业级就绪许可和透明数据来源的完美组合，适合关键任务部署，所有模型都在Apache 2.0许可下公开可用。

Abstract: We introduce the Granite Embedding R2 models, a comprehensive family of
high-performance English encoder-based embedding models engineered for
enterprise-scale dense retrieval applications. Building upon our
first-generation release, these models deliver substantial improvements,
including 16x expanded context length (8,192 tokens), state-of-the-art
performance across diverse retrieval domains - text, code, long-document
search, multi-turn conversational, and tabular data - and measurable speed
advantages of 19-44\% over leading competitors while maintaining superior
accuracy. Our release encompasses both bi-encoder and cross-encoder
architectures, featuring a highly effective 22-layer retriever model and its
efficient 12-layer counterpart, alongside a high-quality reranker model, all
trained exclusively on enterprise-appropriate data with comprehensive
governance oversight. The models demonstrate exceptional versatility across
standard benchmarks, IBM-developed evaluation suites, and real-world enterprise
use cases, establishing new performance standards for open-source embedding
models. In an era where retrieval speed and accuracy are paramount for
competitive advantage, the Granite R2 models deliver a compelling combination
of cutting-edge performance, enterprise-ready licensing, and transparent data
provenance that organizations require for mission-critical deployments. All
models are publicly available under the Apache 2.0 license at
https://huggingface.co/collections/ibm-granite, enabling unrestricted research
and commercial use.

</details>


### [4] [TrInk: Ink Generation with Transformer Network](https://arxiv.org/abs/2508.21098)
*Zezhong Jin,Shubhang Desai,Xu Chen,Biyi Fang,Zhuoyi Huang,Zhe Li,Chong-Xin Gan,Xiao Tu,Man-Wai Mak,Yan Lu,Shujie Liu*

Main category: cs.CL

TL;DR: TrInk是基于Transformer的笔迹生成模型，通过缩放位置嵌入和高斯记忆掩码改进文本-笔画对齐，在IAM-OnDB数据集上CER降低35.56%，WER降低29.66%


<details>
  <summary>Details</summary>
Motivation: 解决笔迹生成中全局依赖关系捕捉和文本-笔画对齐的问题

Method: 使用Transformer架构，引入缩放位置嵌入和交叉注意力模块中的高斯记忆掩码，设计主客观评估流程

Result: 在IAM-OnDB数据集上，字符错误率降低35.56%，词错误率降低29.66%，优于先前方法

Conclusion: TrInk模型能有效生成清晰且风格一致的笔迹，在笔迹生成任务上表现优异

Abstract: In this paper, we propose TrInk, a Transformer-based model for ink
generation, which effectively captures global dependencies. To better
facilitate the alignment between the input text and generated stroke points, we
introduce scaled positional embeddings and a Gaussian memory mask in the
cross-attention module. Additionally, we design both subjective and objective
evaluation pipelines to comprehensively assess the legibility and style
consistency of the generated handwriting. Experiments demonstrate that our
Transformer-based model achieves a 35.56\% reduction in character error rate
(CER) and an 29.66% reduction in word error rate (WER) on the IAM-OnDB dataset
compared to previous methods. We provide an demo page with handwriting samples
from TrInk and baseline models at: https://akahello-a11y.github.io/trink-demo/

</details>


### [5] [How Does Cognitive Bias Affect Large Language Models? A Case Study on the Anchoring Effect in Price Negotiation Simulations](https://arxiv.org/abs/2508.21137)
*Yoshiki Takenami,Yin Jou Huang,Yugo Murawaki,Chenhui Chu*

Main category: cs.CL

TL;DR: 研究发现LLMs在价格谈判中存在锚定效应，推理模型能减轻该效应，但人格特质与锚定效应敏感性无显著相关


<details>
  <summary>Details</summary>
Motivation: 研究LLMs中的认知偏见（特别是锚定效应）对现实应用可靠性的影响，探索其在价格谈判中的表现

Method: 指令卖家LLM代理应用锚定效应，使用客观和主观指标评估谈判，分析推理和人格因素与锚定效应的关系

Result: LLMs像人类一样受锚定效应影响，推理模型较不易受影响（长思维链减轻效应），人格特质与锚定效应敏感性无显著相关性

Conclusion: 这些发现有助于深入理解LLMs中的认知偏见，促进LLMs在社会中安全负责任的应用

Abstract: Cognitive biases, well-studied in humans, can also be observed in LLMs,
affecting their reliability in real-world applications. This paper investigates
the anchoring effect in LLM-driven price negotiations. To this end, we
instructed seller LLM agents to apply the anchoring effect and evaluated
negotiations using not only an objective metric but also a subjective metric.
Experimental results show that LLMs are influenced by the anchoring effect like
humans. Additionally, we investigated the relationship between the anchoring
effect and factors such as reasoning and personality. It was shown that
reasoning models are less prone to the anchoring effect, suggesting that the
long chain of thought mitigates the effect. However, we found no significant
correlation between personality traits and susceptibility to the anchoring
effect. These findings contribute to a deeper understanding of cognitive biases
in LLMs and to the realization of safe and responsible application of LLMs in
society.

</details>


### [6] [Can Multimodal LLMs Solve the Basic Perception Problems of Percept-V?](https://arxiv.org/abs/2508.21143)
*Samrajnee Ghosh,Naman Agarwal,Hemanshu Garg,Chinmay Mittal,Mausam,Parag Singla*

Main category: cs.CL

TL;DR: Percept-V数据集包含7200张程序生成的图像，测试MLLMs在基本视觉感知任务上的表现，发现即使是最先进的模型在简单任务上也存在显著性能下降。


<details>
  <summary>Details</summary>
Motivation: 评估多模态大语言模型在基本视觉感知任务上的表现，现有研究主要关注复杂任务而忽略了简单感知能力的测试。

Method: 创建包含30个类别、7200张程序生成图像的Percept-V数据集，测试GPT-4o、Gemini、Claude等先进MLLMs和LRMs模型。

Result: 实验显示所有模型随着问题复杂度增加性能显著下降，不同模型在各类别中表现出相似的趋势，某些认知技能比其他技能更难。

Conclusion: MLLMs在复杂任务上表现出色，但在基本视觉感知任务上存在明显短板，需要进一步改进模型的底层感知能力。

Abstract: The reasoning abilities of Multimodal Large Language Models (MLLMs) have
garnered a lot of attention in recent times, with advances made in frontiers
like coding, mathematics, and science. However, very limited experiments have
been done to assess their performance in simple perception tasks performed over
uncontaminated, generated images containing basic shapes and structures. To
address this issue, the paper introduces a dataset, Percept-V, containing a
total of 7200 program-generated images equally divided into 30 categories, each
testing a combination of visual perception skills. Unlike previously proposed
datasets, Percept-V comprises very basic tasks of varying complexity that test
the perception abilities of MLLMs. This dataset is then tested on
state-of-the-art MLLMs like GPT-4o, Gemini, and Claude as well as Large
Reasoning Models (LRMs) like OpenAI o4-mini and DeepSeek R1 to gauge their
performance. Contrary to the evidence that MLLMs excel in many complex tasks,
our experiments show a significant drop in the models' performance with
increasing problem complexity across all categories. An analysis of the
performances also reveals that the tested MLLMs exhibit a similar trend in
accuracy across categories, testing a particular cognitive skill and find some
skills to be more difficult than others.

</details>


### [7] [A Survey of Scientific Large Language Models: From Data Foundations to Agent Frontiers](https://arxiv.org/abs/2508.21148)
*Ming Hu,Chenglong Ma,Wei Li,Wanghan Xu,Jiamin Wu,Jucheng Hu,Tianbin Li,Guohang Zhuang,Jiaqi Liu,Yingzhou Lu,Ying Chen,Chaoyang Zhang,Cheng Tan,Jie Ying,Guocheng Wu,Shujian Gao,Pengcheng Chen,Jiashi Lin,Haitao Wu,Lulu Chen,Fengxiang Wang,Yuanyuan Zhang,Xiangyu Zhao,Feilong Tang,Encheng Su,Junzhi Ning,Xinyao Liu,Ye Du,Changkai Ji,Cheng Tang,Huihui Xu,Ziyang Chen,Ziyan Huang,Jiyao Liu,Pengfei Jiang,Yizhou Wang,Chen Tang,Jianyu Wu,Yuchen Ren,Siyuan Yan,Zhonghua Wang,Zhongxing Xu,Shiyan Su,Shangquan Sun,Runkai Zhao,Zhisheng Zhang,Yu Liu,Fudi Wang,Yuanfeng Ji,Yanzhou Su,Hongming Shan,Chunmei Feng,Jiahao Xu,Jiangtao Yan,Wenhao Tang,Diping Song,Lihao Liu,Yanyan Huang,Lequan Yu,Bin Fu,Shujun Wang,Xiaomeng Li,Xiaowei Hu,Yun Gu,Ben Fei,Zhongying Deng,Benyou Wang,Yuewen Cao,Minjie Shen,Haodong Duan,Jie Xu,Yirong Chen,Fang Yan,Hongxia Hao,Jielan Li,Jiajun Du,Yanbo Wang,Imran Razzak,Chi Zhang,Lijun Wu,Conghui He,Zhaohui Lu,Jinhai Huang,Yihao Liu,Fenghua Ling,Yuqiang Li,Aoran Wang,Qihao Zheng,Nanqing Dong,Tianfan Fu,Dongzhan Zhou,Yan Lu,Wenlong Zhang,Jin Ye,Jianfei Cai,Wanli Ouyang,Yu Qiao,Zongyuan Ge,Shixiang Tang,Junjun He,Chunfeng Song,Lei Bai,Bowen Zhou*

Main category: cs.CL

TL;DR: 科学大语言模型(Sci-LLMs)的数据中心综述，从模型与数据的共同进化角度探讨科学AI系统的发展路径和挑战。


<details>
  <summary>Details</summary>
Motivation: 科学数据的复杂性（多模态、跨尺度、领域特定）使得Sci-LLMs的发展面临独特挑战，需要整合数据与模型的共同进化研究。

Method: 构建统一的科学数据分类法和科学知识层次模型，系统评估270+训练数据集和190+测试数据集，分析半自动注释流程和专家验证方案。

Result: 识别了科学数据的异质性、多尺度性和不确定性特征，追踪了评估方法从静态考试向过程和发现导向的转变。

Conclusion: 提出向闭环系统的范式转移，共同构建可信赖、持续进化的AI科学发现合作伙伴。

Abstract: Scientific Large Language Models (Sci-LLMs) are transforming how knowledge is
represented, integrated, and applied in scientific research, yet their progress
is shaped by the complex nature of scientific data. This survey presents a
comprehensive, data-centric synthesis that reframes the development of Sci-LLMs
as a co-evolution between models and their underlying data substrate. We
formulate a unified taxonomy of scientific data and a hierarchical model of
scientific knowledge, emphasizing the multimodal, cross-scale, and
domain-specific challenges that differentiate scientific corpora from general
natural language processing datasets. We systematically review recent Sci-LLMs,
from general-purpose foundations to specialized models across diverse
scientific disciplines, alongside an extensive analysis of over 270
pre-/post-training datasets, showing why Sci-LLMs pose distinct demands --
heterogeneous, multi-scale, uncertainty-laden corpora that require
representations preserving domain invariance and enabling cross-modal
reasoning. On evaluation, we examine over 190 benchmark datasets and trace a
shift from static exams toward process- and discovery-oriented assessments with
advanced evaluation protocols. These data-centric analyses highlight persistent
issues in scientific data development and discuss emerging solutions involving
semi-automated annotation pipelines and expert validation. Finally, we outline
a paradigm shift toward closed-loop systems where autonomous agents based on
Sci-LLMs actively experiment, validate, and contribute to a living, evolving
knowledge base. Collectively, this work provides a roadmap for building
trustworthy, continually evolving artificial intelligence (AI) systems that
function as a true partner in accelerating scientific discovery.

</details>


### [8] [Quantifying Label-Induced Bias in Large Language Model Self- and Cross-Evaluations](https://arxiv.org/abs/2508.21164)
*Muskan Saraf,Sajjad Rezvani Boroujeni,Justin Beaudry,Hossein Abedi,Tom Bush*

Main category: cs.CL

TL;DR: 研究发现LLM评估存在显著标签偏见，Claude标签总是提高评分，Gemini标签总是降低评分，错误标签可导致排名反转和50个百分点的偏好投票变化。


<details>
  <summary>Details</summary>
Motivation: 随着LLM越来越多地用于评估输出，需要研究其判断是否受到模型标签的影响，以确保评估的公平性。

Method: 使用ChatGPT、Gemini和Claude三种模型，在无标签、真实标签和两种错误标签条件下，通过整体偏好投票和三个质量维度（连贯性、信息性、简洁性）的百分比评分来评估博客文章。

Result: 发现明显的评估不对称性：Claude标签始终提升分数，Gemini标签始终降低分数；错误标签经常导致排名反转，偏好投票变化达50个百分点，质量评分变化达12个百分点。

Conclusion: 感知到的模型身份会严重扭曲高级判断并微妙影响详细质量评分，强调需要盲审或多模型评估协议来确保LLM基准测试的公平性。

Abstract: Large language models (LLMs) are increasingly used to evaluate outputs, yet
their judgments may be influenced. This study examines bias in self- and
cross-model evaluations by ChatGPT, Gemini, and Claude under four conditions:
no labels, true labels, and two false-label scenarios. Blog posts authored by
each model were evaluated by all three using both overall preference voting and
quality ratings for Coherence, Informativeness, and Conciseness, with all
scores expressed as percentages for direct comparison. Results reveal striking
asymmetries: the "Claude" label consistently boosts scores, while the "Gemini"
label consistently depresses them, regardless of actual content. False labels
frequently reversed rankings, producing shifts of up to 50 percentage points in
preference votes and up to 12 percentage points in converted quality ratings.
Gemini's self-scores collapsed under true labels, while Claude's
self-preference intensified. These findings show that perceived model identity
can heavily distort high-level judgments and subtly influence detailed quality
ratings, underscoring the need for blind or multimodel evaluation protocols to
ensure fairness in LLM benchmarking.

</details>


### [9] [BED-LLM: Intelligent Information Gathering with LLMs and Bayesian Experimental Design](https://arxiv.org/abs/2508.21184)
*Deepro Choudhury,Sinead Williamson,Adam Goliński,Ning Miao,Freddie Bickford Smith,Michael Kirchhof,Yizhe Zhang,Tom Rainforth*

Main category: cs.CL

TL;DR: BED-LLM方法通过贝叶斯实验设计框架，让大语言模型能够智能地选择最大化信息增益的问题，显著提升了多轮对话中的信息收集能力


<details>
  <summary>Details</summary>
Motivation: 提升大语言模型在对话中智能收集信息的能力，使其能够作为有效的多轮对话代理与外部环境交互

Method: 基于序列贝叶斯实验设计框架，迭代选择最大化期望信息增益的问题，使用基于LLM信念分布的概率模型，并设计了专门的EIG估计器和候选查询策略

Result: 在20个问题游戏和用户偏好推断测试中，相比直接提示和其他自适应策略，BED-LLM取得了显著的性能提升

Conclusion: BED-LLM为大语言模型提供了原则性的信息收集框架，通过贝叶斯实验设计显著提升了模型在交互式任务中的表现

Abstract: We propose a general-purpose approach for improving the ability of Large
Language Models (LLMs) to intelligently and adaptively gather information from
a user or other external source using the framework of sequential Bayesian
experimental design (BED). This enables LLMs to act as effective multi-turn
conversational agents and interactively interface with external environments.
Our approach, which we call BED-LLM (Bayesian Experimental Design with Large
Language Models), is based on iteratively choosing questions or queries that
maximize the expected information gain (EIG) about the task of interest given
the responses gathered previously. We show how this EIG can be formulated in a
principled way using a probabilistic model derived from the LLM's belief
distribution and provide detailed insights into key decisions in its
construction. Further key to the success of BED-LLM are a number of specific
innovations, such as a carefully designed estimator for the EIG, not solely
relying on in-context updates for conditioning on previous responses, and a
targeted strategy for proposing candidate queries. We find that BED-LLM
achieves substantial gains in performance across a wide range of tests based on
the 20-questions game and using the LLM to actively infer user preferences,
compared to direct prompting of the LLM and other adaptive design strategies.

</details>


### [10] [Improving Aviation Safety Analysis: Automated HFACS Classification Using Reinforcement Learning with Group Relative Policy Optimization](https://arxiv.org/abs/2508.21201)
*Arash Ahmadi,Sarah Sharif,Yaser Banad*

Main category: cs.CL

TL;DR: 使用强化学习和GRPO优化的Llama-3.1 8B模型自动化HFACS分析，在航空安全分析中实现了准确率显著提升，超越了GPT-5-mini等领先模型


<details>
  <summary>Details</summary>
Motivation: 传统HFACS分析方法存在扩展性和一致性问题，需要更高效、可扩展的自动化分析方案来预防航空事故

Method: 使用强化学习组相对策略优化(GRPO)对Llama-3.1 8B语言模型进行微调，构建专门为航空安全分析设计的多组件奖励系统，并集成合成数据生成来解决数据集类别不平衡问题

Result: 模型表现显著提升：精确匹配准确率从0.0400提升到0.1800(增长350%)，部分匹配准确率达到0.8800，超越了GPT-5-mini和Gemini-2.5-flash等领先模型

Conclusion: 领域优化的小型模型能够提供计算效率更高、性能更好的安全分析方案，为资源受限的边缘设备部署提供了可行性

Abstract: Analyzing the human factors behind aviation accidents is crucial for
preventing future incidents, yet traditional methods using the Human Factors
Analysis and Classification System (HFACS) are limited by scalability and
consistency. To address this, we introduce an automated HFACS classification
framework for aviation safety analysis that utilizes Reinforcement Learning
with Group Relative Policy Optimization (GRPO) to fine-tune a Llama-3.1 8B
language model. Our approach incorporates a multi-component reward system
tailored for aviation safety analysis and integrates synthetic data generation
to overcome class imbalance in accident datasets. The resulting GRPO-optimized
model achieved noticeable performance gains, including a 350% increase in exact
match accuracy (from 0.0400 to 0.1800) and an improved partial match accuracy
of 0.8800. Significantly, our specialized model outperforms state-of-the-art
LLMs (Large Language Models), including GPT-5-mini and Gemini-2.5-fiash, on key
metrics. This research also proposes exact match accuracy in multi-label HFACS
classification problem as a new benchmarking methodology to evaluate the
advanced reasoning capabilities of language models. Ultimately, our work
validates that smaller, domain-optimized models can provide a computationally
efficient and better solution for critical safety analysis. This approach makes
powerful, low-latency deployment on resource-constrained edge devices feasible.

</details>


### [11] [Enhancing Robustness of Autoregressive Language Models against Orthographic Attacks via Pixel-based Approach](https://arxiv.org/abs/2508.21206)
*Han Yang,Jian Lan,Yihong Liu,Hinrich Schütze,Thomas Seidl*

Main category: cs.CL

TL;DR: 提出基于像素的生成语言模型，通过将单词渲染为图像来解决自回归语言模型对多语言字符攻击的脆弱性问题


<details>
  <summary>Details</summary>
Motivation: 自回归语言模型容易受到多语言字符的正字法攻击，导致性能显著下降，这主要源于子词分词器及其嵌入的词汇表外问题

Method: 用基于像素的表示替代基于文本的嵌入，将单词渲染为单个图像，从而增强对噪声输入的鲁棒性并扩展对多语言文本的兼容性

Result: 在多语言LAMBADA数据集、WMT24数据集和SST-2基准测试中验证了该方法对正字法噪声的鲁棒性和在多语言环境中的有效性

Conclusion: 基于像素的表示方法为语言模型提供了更强的抗噪声能力和多语言兼容性，有效解决了传统文本嵌入的词汇表外问题

Abstract: Autoregressive language models are vulnerable to orthographic attacks, where
input text is perturbed with characters from multilingual alphabets, leading to
substantial performance degradation. This vulnerability primarily stems from
the out-of-vocabulary issue inherent in subword tokenizers and their
embeddings. To address this limitation, we propose a pixel-based generative
language model that replaces the text-based embeddings with pixel-based
representations by rendering words as individual images. This design provides
stronger robustness to noisy inputs, while an extension of compatibility to
multilingual text across diverse writing systems. We evaluate the proposed
method on the multilingual LAMBADA dataset, WMT24 dataset and the SST-2
benchmark, demonstrating both its resilience to orthographic noise and its
effectiveness in multilingual settings.

</details>


### [12] [Do Self-Supervised Speech Models Exhibit the Critical Period Effects in Language Acquisition?](https://arxiv.org/abs/2508.21210)
*Yurie Koga,Shunsuke Kando,Yusuke Miyao*

Main category: cs.CL

TL;DR: 本研究探讨自监督语音模型是否表现出人类语言习得中的关键期效应，发现这些模型在语音习得方面并未显示明确的关键期效应证据。


<details>
  <summary>Details</summary>
Motivation: 关键期效应在人类语言习得中表现为第二语言(L2)接触越晚习得越困难，第一语言(L1)接触越晚保持越好。先前研究主要基于文本语言模型，而语音在人类语言习得中的核心作用使得研究语音模型中的这些效应具有重要意义。

Method: 在儿童导向语音数据上训练自监督语音模型，设置不同的L2训练起始时间和L1训练结束时间，评估模型的音位辨别性能。

Result: 自监督语音模型在语音习得方面未显示明确的关键期效应证据。延迟L2接触起始的模型在L2上表现更好，延迟L1接触结束导致L1遗忘。

Conclusion: 自监督语音模型与人类语言习得中的关键期效应模式不同，表明这些模型可能采用与人类不同的语言学习机制。

Abstract: This paper investigates whether the Critical Period (CP) effects in human
language acquisition are observed in self-supervised speech models (S3Ms). CP
effects refer to greater difficulty in acquiring a second language (L2) with
delayed L2 exposure onset, and greater retention of their first language (L1)
with delayed L1 exposure offset. While previous work has studied these effects
using textual language models, their presence in speech models remains
underexplored despite the central role of spoken language in human language
acquisition. We train S3Ms with varying L2 training onsets and L1 training
offsets on child-directed speech and evaluate their phone discrimination
performance. We find that S3Ms do not exhibit clear evidence of either CP
effects in terms of phonological acquisition. Notably, models with delayed L2
exposure onset tend to perform better on L2 and delayed L1 exposure offset
leads to L1 forgetting.

</details>


### [13] [Decoding Memories: An Efficient Pipeline for Self-Consistency Hallucination Detection](https://arxiv.org/abs/2508.21228)
*Weizhi Gao,Xiaorui Liu,Feiyi Wang,Dan Lu,Junqi Yin*

Main category: cs.CL

TL;DR: 这篇论文提出了一种新的解码内存管道(DMP)方法，通过识别自一致性方法中的冗余共享前缀汇性，在不牺牲AUROC性能的情况下实现了达到3倍的生成速度提升。


<details>
  <summary>Details</summary>
Motivation: 当前的语句级幻觉检测方法表现较差，而自一致性方法虽然有效但计算成本高很多，需要重复生成多个响应。

Method: 识别自一致性方法中的冗余共享前缀汇性，发现非准确答案汇性对语义内容贡献最小，提出解码内存管道(DMP)通过选择性推理和逆变温度解码来加速生成。

Result: 大量实验表明，该方法在不牺牲AUROC性能的情况下实现了达到3倍的速度提升，而且无需依赖模型、数据集、解码策略或自一致性基准。

Conclusion: DMP方法能够持续改善多响应生成的效率，并有望扩展到对齐和推理任务中。

Abstract: Large language models (LLMs) have demonstrated impressive performance in both
research and real-world applications, but they still struggle with
hallucination. Existing hallucination detection methods often perform poorly on
sentence-level generation or rely heavily on domain-specific knowledge. While
self-consistency approaches help address these limitations, they incur high
computational costs due to repeated generation. In this paper, we conduct the
first study on identifying redundancy in self-consistency methods, manifested
as shared prefix tokens across generations, and observe that non-exact-answer
tokens contribute minimally to the semantic content. Based on these insights,
we propose a novel Decoding Memory Pipeline (DMP) that accelerates generation
through selective inference and annealed decoding. Being orthogonal to the
model, dataset, decoding strategy, and self-consistency baseline, our DMP
consistently improves the efficiency of multi-response generation and holds
promise for extension to alignment and reasoning tasks. Extensive experiments
show that our method achieves up to a 3x speedup without sacrificing AUROC
performance.

</details>


### [14] [Efficient Code Embeddings from Code Generation Models](https://arxiv.org/abs/2508.21290)
*Daria Kryvosheieva,Saba Sturua,Michael Günther,Scott Martens,Han Xiao*

Main category: cs.CL

TL;DR: jina-code-embeddings是一个创新的代码嵌入模型套件，使用自回归主干网络和最后token池化技术，在小模型规模下实现了最先进的代码检索性能


<details>
  <summary>Details</summary>
Motivation: 开发一个能够从自然语言查询检索代码、进行技术问答以及跨编程语言识别语义相似代码片段的代码嵌入模型

Method: 使用在文本和代码上预训练的自回归主干网络，通过最后token池化(last-token pooling)生成嵌入向量

Result: 尽管模型规模相对较小，但在代码嵌入任务上实现了最先进的性能表现

Conclusion: 验证了这种代码嵌入模型构建方法的有效性，为代码检索和语义相似性识别提供了高效解决方案

Abstract: jina-code-embeddings is a novel code embedding model suite designed to
retrieve code from natural language queries, perform technical
question-answering, and identify semantically similar code snippets across
programming languages. It makes innovative use of an autoregressive backbone
pre-trained on both text and code, generating embeddings via last-token
pooling. We outline the training recipe and demonstrate state-of-the-art
performance despite the relatively small size of the models, validating this
approach to code embedding model construction.

</details>


### [15] [BLUEX Revisited: Enhancing Benchmark Coverage with Automatic Captioning](https://arxiv.org/abs/2508.21294)
*João Guilherme Alves Santos,Giovana Kerche Bonás,Thales Sales Almeida*

Main category: cs.CL

TL;DR: BLUEX数据集更新版本，包含2024-2025考试内容和AI生成的图像标注，使文本模型可访问性提升40%以上，生成1422个可用问题，比原版翻倍。评估了商业和开源LLM利用视觉上下文的能力。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能力增强，需要更强大的评估方法，特别是在多语言和非英语环境中，以研究LLM预训练中的数据污染问题。

Method: 更新BLUEX数据集，加入2024-2025考试内容，使用最先进模型自动生成图像标注，通过标注策略提升文本模型的可访问性。

Result: 标注策略使文本模型可访问性提升40%以上，生成1422个可用问题（比原版翻倍），评估了商业和开源LLM利用视觉上下文的能力。

Conclusion: 更新的BLUEX数据集为LLM评估提供了更全面的基准，特别是在多语言和视觉上下文利用方面，有助于数据污染研究。

Abstract: With the growing capabilities of Large Language Models (LLMs), there is an
increasing need for robust evaluation methods, especially in multilingual and
non-English contexts. We present an updated version of the BLUEX dataset, now
including 2024-2025 exams and automatically generated image captions using
state-of-the-art models, enhancing its relevance for data contamination studies
in LLM pretraining. Captioning strategies increase accessibility to text-only
models by more than 40%, producing 1,422 usable questions, more than doubling
the number in the original BLUEX. We evaluated commercial and open-source LLMs
and their ability to leverage visual context through captions.

</details>


### [16] [Challenges and Applications of Large Language Models: A Comparison of GPT and DeepSeek family of models](https://arxiv.org/abs/2508.21377)
*Shubham Sharma,Sneha Tuli,Narendra Badam*

Main category: cs.CL

TL;DR: 本论文通过对比OpenAI闭源GPT-4o和DeepSeek开源MoE模型，分析了16个LLM开发部署关键挑战，探讨了闭源与开源模型在安全性、可靠性、效率和适配性方面的特点与适用场景。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在各行业的广泛应用，开发和部署的复杂性成为重要挑战。本文旨在通过对比分析最新的闭源和开源模型，为AI研究人员、开发者和决策者提供对当前LLM能力、限制和最佳实践的全面理解。

Method: 采用调查研究方法，系统性评估16个LLM建设与使用的关键挑战。通过对比分析两种不同路径的先进模型：OpenAI的闭源GPT-4o(2024年5月)和DeepSeek-V3-0324(2025年3月)开源混合专家模型，评估各自在不同领域的适用性。

Result: 研究显示闭源模型(如GPT-4o)在安全性和可靠性方面具有优势，而开源模型(如DeepSeek-V3)在效率和适配性方面更具竞争力。不同应用领域(聊天机器人、编码工具、医疗健康、教育等)对模型特性有不同的需求优势。

Conclusion: 闭源与开源LLM模型各有优势，选择取决于具体应用场景的需求。本研究为AI行业参与者提供了重要的决策参考，帮助他们根据实际需求选择最合适的模型策略，以更有效地应用大语言模型技术。

Abstract: Large Language Models (LLMs) are transforming AI across industries, but their
development and deployment remain complex. This survey reviews 16 key
challenges in building and using LLMs and examines how these challenges are
addressed by two state-of-the-art models with unique approaches: OpenAI's
closed source GPT-4o (May 2024 update) and DeepSeek-V3-0324 (March 2025), a
large open source Mixture-of-Experts model. Through this comparison, we
showcase the trade-offs between closed source models (robust safety, fine-tuned
reliability) and open source models (efficiency, adaptability). We also explore
LLM applications across different domains (from chatbots and coding tools to
healthcare and education), highlighting which model attributes are best suited
for each use case. This article aims to guide AI researchers, developers, and
decision-makers in understanding current LLM capabilities, limitations, and
best practices.

</details>


### [17] [Normality and the Turing Test](https://arxiv.org/abs/2508.21382)
*Alexandre Kabbach*

Main category: cs.CL

TL;DR: 本文通过'正常性'概念重新审视图灵测试，认为图灵测试针对的是正常/平均人类智能而非卓越智能，需要机器像普通人一样犯错。同时图灵测试是统计测试，需要多名法官的集体判断。


<details>
  <summary>Details</summary>
Motivation: 重新解读图灵测试的哲学基础，探讨其对人工智能和人类认知理解的真正意义，特别是针对当前大型语言模型的发展现状。

Method: 通过概念分析和哲学论证，从统计学角度重新解释图灵测试中的'正常性'概念，包括测试对象的正常性和评判者的正常性。

Result: 提出图灵测试实际上是测试正常智能而非卓越智能；认为ChatGPT等模型追求的是人工聪明而非真正的人工智能；指出图灵测试对认知科学的价值取决于人类心智是否可简化为平均心智。

Conclusion: 图灵测试的核心价值在于测试机器是否能模拟正常人类行为，而非卓越智能。当前大型语言模型因追求完美表现而难以通过真正的图灵测试，且测试的认知科学价值取决于对'正常心智'概念的根本性质疑。

Abstract: This paper proposes to revisit the Turing test through the concept of
normality. Its core argument is that the statistical interpretation of the
normal--understood as the average both in the normative and mathematical sense
of the term--proves useful for understanding the Turing test in at least two
ways. First, in the sense that the Turing test targets normal/average rather
than exceptional human intelligence, so that successfully passing the test
requires building machines that "make mistakes" and display imperfect behavior
just like normal/average humans. Second, in the sense that the Turing test is a
statistical test where judgments of intelligence are never carried out by a
single "average" judge (understood as non-expert) but always by a full jury. As
such, the notion of "average human interrogator" that Turing talks about in his
original paper should be understood primarily as referring to a mathematical
abstraction made of the normalized aggregate of individual judgments of
multiple judges. In short, this paper argues that the Turing test is a test of
normal intelligence as assessed by a normal judge characterizing the average
judgment of a pool of human interrogators. Its conclusions are twofold. First,
it argues that large language models such as ChatGPT are unlikely to pass the
Turing test as those models precisely target exceptional rather than
normal/average human intelligence. As such, they constitute models of what it
proposes to call artificial smartness rather than artificial intelligence per
se. Second, it argues that the core question of whether the Turing test can
contribute anything to the understanding of human cognition is that of whether
the human mind is really reducible to the normal/average mind--a question which
largely extends beyond the Turing test itself and questions the conceptual
underpinnings of the normalist paradigm it belongs to.

</details>


### [18] [AllSummedUp: un framework open-source pour comparer les metriques d'evaluation de resume](https://arxiv.org/abs/2508.21389)
*Tanguy Herserant,Vincent Guigue*

Main category: cs.CL

TL;DR: 本文研究发现文本摘要自动评估存在严重的可复现性问题，六种主流评估指标在实验中出现与文献报告显著不同的性能表现，揭示了评估效果与计算成本之间的结构性权衡。


<details>
  <summary>Details</summary>
Motivation: 调查文本摘要自动评估指标的可复现性挑战，发现现有文献报告的性能与实际实验存在显著差异，需要建立公平透明的比较框架。

Method: 基于SummEval数据集构建统一的开源评估框架，对六种代表性指标（从ROUGE到最新的LLM方法如G-Eval、SEval-Ex）进行系统性实验比较。

Result: 发现评估指标存在结构性权衡：与人类判断最一致的指标往往计算密集且运行稳定性较差；LLM评估方法存在随机性、技术依赖性和有限可复现性等问题。

Conclusion: 呼吁建立更稳健的评估协议，包括详尽文档记录和方法标准化，以确保自动摘要评估的更高可靠性。

Abstract: This paper investigates reproducibility challenges in automatic text
summarization evaluation. Based on experiments conducted across six
representative metrics ranging from classical approaches like ROUGE to recent
LLM-based methods (G-Eval, SEval-Ex), we highlight significant discrepancies
between reported performances in the literature and those observed in our
experimental setting. We introduce a unified, open-source framework, applied to
the SummEval dataset and designed to support fair and transparent comparison of
evaluation metrics. Our results reveal a structural trade-off: metrics with the
highest alignment with human judgments tend to be computationally intensive and
less stable across runs. Beyond comparative analysis, this study highlights key
concerns about relying on LLMs for evaluation, stressing their randomness,
technical dependencies, and limited reproducibility. We advocate for more
robust evaluation protocols including exhaustive documentation and
methodological standardization to ensure greater reliability in automatic
summarization assessment.

</details>


### [19] [Automatic Reviewers Fail to Detect Faulty Reasoning in Research Papers: A New Counterfactual Evaluation Framework](https://arxiv.org/abs/2508.21422)
*Nils Dycke,Iryna Gurevych*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型作为自动审稿生成器在检测研究逻辑缺陷方面的能力，发现即使存在逻辑错误，LLM生成的审稿意见也没有显著差异，并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 随着LLM越来越多地被用作自动审稿生成器，需要了解其在检测研究逻辑缺陷这一核心审稿技能上的具体能力和局限性，以避免对科学完整性造成风险。

Method: 提出了一个完全自动化的反事实评估框架，在受控条件下测试各种ARG方法检测研究逻辑一致性的能力。

Result: 研究发现，与预期相反，研究逻辑中的缺陷对LLM生成的审稿输出没有显著影响。

Conclusion: 基于研究结果提出了三个可操作的建议，并公开了反事实数据集和评估框架，以促进未来研究。

Abstract: Large Language Models (LLMs) have great potential to accelerate and support
scholarly peer review and are increasingly used as fully automatic review
generators (ARGs). However, potential biases and systematic errors may pose
significant risks to scientific integrity; understanding the specific
capabilities and limitations of state-of-the-art ARGs is essential. We focus on
a core reviewing skill that underpins high-quality peer review: detecting
faulty research logic. This involves evaluating the internal consistency
between a paper's results, interpretations, and claims. We present a fully
automated counterfactual evaluation framework that isolates and tests this
skill under controlled conditions. Testing a range of ARG approaches, we find
that, contrary to expectation, flaws in research logic have no significant
effect on their output reviews. Based on our findings, we derive three
actionable recommendations for future work and release our counterfactual
dataset and evaluation framework publicly.

</details>


### [20] [Med-RewardBench: Benchmarking Reward Models and Judges for Medical Multimodal Large Language Models](https://arxiv.org/abs/2508.21430)
*Meidan Ding,Jipeng Zhang,Wenxuan Wang,Cheng-Yi Li,Wei-Chieh Fang,Hsin-Yu Wu,Haiqin Zhong,Wenting Chen,Linlin Shen*

Main category: cs.CL

TL;DR: 提出了Med-RewardBench，首个专门评估医疗奖励模型和评判器的基准，包含1026个专家标注的多模态医疗案例，涵盖13个器官系统和8个临床科室。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注通用MLLM能力或模型作为求解器的评估，忽略了医疗场景中诊断准确性和临床相关性等关键维度，缺乏专门的医疗奖励模型评估基准。

Method: 构建包含1026个专家标注案例的多模态数据集，采用三步严格流程确保数据质量，评估32个最先进的MLLM模型，并开发了通过微调显著提升性能的基线模型。

Result: 评估发现现有模型在输出与专家判断对齐方面存在重大挑战，基线模型通过微调实现了显著的性能提升。

Conclusion: Med-RewardBench填补了医疗奖励模型评估的空白，为医疗AI系统的可靠性和安全性提供了重要评估工具。

Abstract: Multimodal large language models (MLLMs) hold significant potential in
medical applications, including disease diagnosis and clinical decision-making.
However, these tasks require highly accurate, context-sensitive, and
professionally aligned responses, making reliable reward models and judges
critical. Despite their importance, medical reward models (MRMs) and judges
remain underexplored, with no dedicated benchmarks addressing clinical
requirements. Existing benchmarks focus on general MLLM capabilities or
evaluate models as solvers, neglecting essential evaluation dimensions like
diagnostic accuracy and clinical relevance. To address this, we introduce
Med-RewardBench, the first benchmark specifically designed to evaluate MRMs and
judges in medical scenarios. Med-RewardBench features a multimodal dataset
spanning 13 organ systems and 8 clinical departments, with 1,026
expert-annotated cases. A rigorous three-step process ensures high-quality
evaluation data across six clinically critical dimensions. We evaluate 32
state-of-the-art MLLMs, including open-source, proprietary, and
medical-specific models, revealing substantial challenges in aligning outputs
with expert judgment. Additionally, we develop baseline models that demonstrate
substantial performance improvements through fine-tuning.

</details>


### [21] [Discovering Semantic Subdimensions through Disentangled Conceptual Representations](https://arxiv.org/abs/2508.21436)
*Yunhao Zhang,Shaonan Wang,Nan Lin,Xinyi Dong,Chong Li,Chengqing Zong*

Main category: cs.CL

TL;DR: 本文提出了一个解耦连续语义表示模型(DCSRM)，从大语言模型词嵌入中分解出可解释的语义子维度，并通过脑成像验证其神经合理性。


<details>
  <summary>Details</summary>
Motivation: 现有语义维度研究方法往往依赖预定义的粗粒度维度，忽略了更精细的概念区分，需要开发能够揭示语义子维度结构的新方法。

Method: 提出DCSRM模型，将大语言模型的词嵌入分解为多个子嵌入，每个编码特定语义信息；使用体素编码模型将这些子维度映射到大脑激活模式。

Result: 识别出一组可解释的语义子维度，发现极性是驱动维度分解的关键因素，神经相关性分析支持了这些子维度的认知和神经科学合理性。

Conclusion: 该框架提供了更细粒度的可解释语义子维度，揭示了语义维度按不同原则组织的结构特征，为理解语言和大脑中的意义组织提供了新见解。

Abstract: Understanding the core dimensions of conceptual semantics is fundamental to
uncovering how meaning is organized in language and the brain. Existing
approaches often rely on predefined semantic dimensions that offer only broad
representations, overlooking finer conceptual distinctions. This paper proposes
a novel framework to investigate the subdimensions underlying coarse-grained
semantic dimensions. Specifically, we introduce a Disentangled Continuous
Semantic Representation Model (DCSRM) that decomposes word embeddings from
large language models into multiple sub-embeddings, each encoding specific
semantic information. Using these sub-embeddings, we identify a set of
interpretable semantic subdimensions. To assess their neural plausibility, we
apply voxel-wise encoding models to map these subdimensions to brain
activation. Our work offers more fine-grained interpretable semantic
subdimensions of conceptual meaning. Further analyses reveal that semantic
dimensions are structured according to distinct principles, with polarity
emerging as a key factor driving their decomposition into subdimensions. The
neural correlates of the identified subdimensions support their cognitive and
neuroscientific plausibility.

</details>


### [22] [Beyond the Surface: Probing the Ideological Depth of Large Language Models](https://arxiv.org/abs/2508.21448)
*Shariar Kabir,Kevin Esterling,Yue Dong*

Main category: cs.CL

TL;DR: 该研究提出"意识形态深度"概念来衡量LLMs内部政治表征的稳健性和复杂性，通过可操纵性测试和稀疏自编码器分析发现不同模型在意识形态深度上存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型表现出明显的意识形态倾向，但这些立场的稳定性和深度尚不清楚，表面响应容易被提示工程操控，需要探究其是否反映连贯的底层意识形态。

Method: 采用双重方法：1)通过指令提示和激活引导测量开源LLMs的可操纵性；2)使用稀疏自编码器(SAEs)探测模型内部机制，分析意识形态特征的分布和抽象程度。

Result: 发现可操纵性较低的模型拥有更独特和抽象的意识形态特征，其中一个模型比同等规模模型多7.3倍的政治特征。对深度模型核心政治特征的靶向消融能导致相关主题推理的一致逻辑转变，而浅层模型则增加拒绝输出。

Conclusion: 意识形态深度是LLMs的可量化属性，可操纵性可作为窥探其潜在政治架构的重要窗口，不同模型在意识形态深度上存在显著差异。

Abstract: Large Language Models (LLMs) have demonstrated pronounced ideological
leanings, yet the stability and depth of these positions remain poorly
understood. Surface-level responses can often be manipulated through simple
prompt engineering, calling into question whether they reflect a coherent
underlying ideology. This paper investigates the concept of "ideological depth"
in LLMs, defined as the robustness and complexity of their internal political
representations. We employ a dual approach: first, we measure the
"steerability" of two well-known open-source LLMs using instruction prompting
and activation steering. We find that while some models can easily switch
between liberal and conservative viewpoints, others exhibit resistance or an
increased rate of refusal, suggesting a more entrenched ideological structure.
Second, we probe the internal mechanisms of these models using Sparse
Autoencoders (SAEs). Preliminary analysis reveals that models with lower
steerability possess more distinct and abstract ideological features. Our
evaluations reveal that one model can contain 7.3x more political features than
another model of similar size. This allows targeted ablation of a core
political feature in an ideologically "deep" model, leading to consistent,
logical shifts in its reasoning across related topics, whereas the same
intervention in a "shallow" model results in an increase in refusal outputs.
Our findings suggest that ideological depth is a quantifiable property of LLMs
and that steerability serves as a valuable window into their latent political
architecture.

</details>


### [23] [Igniting Creative Writing in Small Language Models: LLM-as-a-Judge versus Multi-Agent Refined Rewards](https://arxiv.org/abs/2508.21476)
*Xiaolong Wei,Bo Lu,Xingyu Zhang,Zhejun Zhao,Dongdong Shen,Long Xia,Dawei Yin*

Main category: cs.CL

TL;DR: 本文提出两种AI驱动的奖励策略（基于偏好模型的RLAIF和基于原则指导的LLM-as-a-Judge），在7B参数小语言模型上显著提升中文问候语创作的创造性和质量，后者在训练效率和减少人工标注依赖方面更具优势。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然创意写作能力强但计算需求大，小语言模型是替代方案但现有方法（如SFT）缺乏新颖性，RLHF成本高昂，需要更高效的方法来激发小语言模型的创造性写作能力。

Method: 采用RLAIF框架，探索两种AI奖励策略：1）基于多智能体拒绝采样框架训练的偏好模型；2）通过对抗训练和反思机制优化的原则指导LLM-as-a-Judge直接提供奖励信号。

Result: 两种方法都显著超越了基线模型的创造性输出，其中原则指导的LLM-as-a-Judge方法在生成质量上表现更优，训练效率更高，减少了对人工标注数据的依赖，自动化评估方法与人类判断高度一致。

Conclusion: 原则指导的LLM-as-a-Judge方法为创造性小语言模型提供了更可扩展和有效的路径，在保持高质量生成的同时提升了训练效率和减少了人工标注需求。

Abstract: Large Language Models (LLMs) have demonstrated remarkable creative writing
capabilities, yet their substantial computational demands hinder widespread
use. Enhancing Small Language Models (SLMs) offers a promising alternative, but
current methods like Supervised Fine-Tuning (SFT) struggle with novelty, and
Reinforcement Learning from Human Feedback (RLHF) is costly. This paper
explores two distinct AI-driven reward strategies within a Reinforcement
Learning from AI Feedback (RLAIF) framework to ignite the creative writing of a
7B-parameter SLM, specifically for generating Chinese greetings. The first
strategy employs a RM trained on high-quality preference data curated by a
novel multi-agent rejection sampling framework designed for creative tasks. The
second, more novel strategy utilizes a principle-guided LLM-as-a-Judge, whose
reward function is optimized via an adversarial training scheme with a
reflection mechanism, to directly provide reward signals. Comprehensive
experiments reveal that while both approaches significantly enhance creative
output over baselines, the principle-guided LLM-as-a-Judge demonstrably yields
superior generation quality. Furthermore, it offers notable advantages in
training efficiency and reduced dependency on human-annotated data, presenting
a more scalable and effective path towards creative SLMs. Our automated
evaluation methods also exhibit strong alignment with human judgments. Our code
and data are publicly available at
https://github.com/weixiaolong94-hub/Igniting-Creative-Writing-in-Small-Language-Models.

</details>


### [24] [HSFN: Hierarchical Selection for Fake News Detection building Heterogeneous Ensemble](https://arxiv.org/abs/2508.21482)
*Sara B. Coutinho,Rafael M. O. Cruz,Francimaria R. S. Nascimento,George D. C. Cavalcanti*

Main category: cs.CL

TL;DR: 通过层次聚类和多样性优先的自动分类器选择方法，提升了对假新闻识别的集成学习效果


<details>
  <summary>Details</summary>
Motivation: 解决集成学习中分类器多样性选择的挑战，避免模型学习重复模式导致集成效果不佳

Method: 采用成对多样性计算+层次聚类，逐层选择不同多样性水平的分类器池，结合性能指标确保集成效果

Result: 在6个不同领域数据集上，该方法在2个数据集上达到最高准确率，超越Elbow偏好法和其他基线方法

Conclusion: 通过系统化的多样性选择机制，能够有效提升假新闻识别集成学习的性能和稳健性

Abstract: Psychological biases, such as confirmation bias, make individuals
particularly vulnerable to believing and spreading fake news on social media,
leading to significant consequences in domains such as public health and
politics. Machine learning-based fact-checking systems have been widely studied
to mitigate this problem. Among them, ensemble methods are particularly
effective in combining multiple classifiers to improve robustness. However,
their performance heavily depends on the diversity of the constituent
classifiers-selecting genuinely diverse models remains a key challenge,
especially when models tend to learn redundant patterns. In this work, we
propose a novel automatic classifier selection approach that prioritizes
diversity, also extended by performance. The method first computes pairwise
diversity between classifiers and applies hierarchical clustering to organize
them into groups at different levels of granularity. A HierarchySelect then
explores these hierarchical levels to select one pool of classifiers per level,
each representing a distinct intra-pool diversity. The most diverse pool is
identified and selected for ensemble construction from these. The selection
process incorporates an evaluation metric reflecting each classifiers's
performance to ensure the ensemble also generalises well. We conduct
experiments with 40 heterogeneous classifiers across six datasets from
different application domains and with varying numbers of classes. Our method
is compared against the Elbow heuristic and state-of-the-art baselines. Results
show that our approach achieves the highest accuracy on two of six datasets.
The implementation details are available on the project's repository:
https://github.com/SaraBCoutinho/HSFN .

</details>


### [25] [L3Cube-MahaSTS: A Marathi Sentence Similarity Dataset and Models](https://arxiv.org/abs/2508.21569)
*Aishwarya Mirashi,Ananya Joshi,Raviraj Joshi*

Main category: cs.CL

TL;DR: 提出了MahaSTS马拉地语句子相似度数据集和MahaSBERT-STS-v2模型，包含16,860个标注句子对，在0-5分范围内均匀分布，用于低资源语言的句子相似度任务。


<details>
  <summary>Details</summary>
Motivation: 解决马拉地语等低资源语言缺乏高质量句子相似度标注数据的问题，为自然语言处理任务提供基础资源。

Method: 构建人工标注的句子相似度数据集，采用6个分数桶均匀分布策略减少标签偏差，基于MahaSBERT模型进行回归式相似度评分微调。

Result: MahaSBERT-STS-v2模型在马拉地语句子相似度任务上表现优于MahaBERT、MuRIL、IndicBERT和IndicSBERT等基准模型。

Conclusion: 人工标注数据、针对性微调和结构化监督在低资源语言环境中对句子相似度任务具有重要影响，公开的数据集和模型推动了马拉地语NLP发展。

Abstract: We present MahaSTS, a human-annotated Sentence Textual Similarity (STS)
dataset for Marathi, along with MahaSBERT-STS-v2, a fine-tuned Sentence-BERT
model optimized for regression-based similarity scoring. The MahaSTS dataset
consists of 16,860 Marathi sentence pairs labeled with continuous similarity
scores in the range of 0-5. To ensure balanced supervision, the dataset is
uniformly distributed across six score-based buckets spanning the full 0-5
range, thus reducing label bias and enhancing model stability. We fine-tune the
MahaSBERT model on this dataset and benchmark its performance against other
alternatives like MahaBERT, MuRIL, IndicBERT, and IndicSBERT. Our experiments
demonstrate that MahaSTS enables effective training for sentence similarity
tasks in Marathi, highlighting the impact of human-curated annotations,
targeted fine-tuning, and structured supervision in low-resource settings. The
dataset and model are publicly shared at
https://github.com/l3cube-pune/MarathiNLP

</details>


### [26] [A Survey on Current Trends and Recent Advances in Text Anonymization](https://arxiv.org/abs/2508.21587)
*Tobias Deußer,Lorenz Sparrenberg,Armin Berger,Max Hahnbück,Christian Bauckhage,Rafet Sifa*

Main category: cs.CL

TL;DR: 本论文统计了文本匿名化技术的最新进展，包括基础方法、大语言模型的双重作用、领域特定挑战、形式隐私模型和评估框架等多个方面。


<details>
  <summary>Details</summary>
Motivation: 各行业文本数据中含有大量敏感个人信息，需要符合监管要求的健壮匿名化技术来保护隐私，同时保持数据的可用性。

Method: 通过综述性调查方法，从基础命名实体识别技术到大语言模型的双重作用，深入分析各领域特定解决方案、形式隐私模型、风险感知框架以及作者匿名化等多种技术方法。

Result: 本论文统计了当前文本匿名化领域的知识状态，识别了新兴趋势和持续挑战，包括隐私-效用性平衡、几乎识别符处理、大语言模型能力影响等关键问题。

Conclusion: 该综述为学术界和业界实践者提供了全面的文本匿名化技术指南，明确了未来研究方向，对于推动该领域的发展具有重要指导意义。

Abstract: The proliferation of textual data containing sensitive personal information
across various domains requires robust anonymization techniques to protect
privacy and comply with regulations, while preserving data usability for
diverse and crucial downstream tasks. This survey provides a comprehensive
overview of current trends and recent advances in text anonymization
techniques. We begin by discussing foundational approaches, primarily centered
on Named Entity Recognition, before examining the transformative impact of
Large Language Models, detailing their dual role as sophisticated anonymizers
and potent de-anonymization threats. The survey further explores
domain-specific challenges and tailored solutions in critical sectors such as
healthcare, law, finance, and education. We investigate advanced methodologies
incorporating formal privacy models and risk-aware frameworks, and address the
specialized subfield of authorship anonymization. Additionally, we review
evaluation frameworks, comprehensive metrics, benchmarks, and practical
toolkits for real-world deployment of anonymization solutions. This review
consolidates current knowledge, identifies emerging trends and persistent
challenges, including the evolving privacy-utility trade-off, the need to
address quasi-identifiers, and the implications of LLM capabilities, and aims
to guide future research directions for both academics and practitioners in
this field.

</details>


### [27] [Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning](https://arxiv.org/abs/2508.21589)
*Zinan Tang,Xin Gao,Qizhi Pei,Zhuoshi Pan,Mengzhang Cai,Jiang Wu,Conghui He,Lijun Wu*

Main category: cs.CL

TL;DR: Middo是一个自演化的模型驱动动态数据优化框架，通过三轴模型信号诊断和自适应优化引擎，持续提升SFT训练数据质量，平均提高LLM准确率7.15%


<details>
  <summary>Details</summary>
Motivation: 现有数据选择和数据合成方法存在静态数据集构建的局限性，无法适应模型能力的动态演化，需要建立闭环优化系统来持续提升数据质量

Method: 提出三轴模型信号诊断（损失模式、嵌入聚类动态、自对齐分数）来识别次优样本，然后通过自适应优化引擎在保持语义完整性的同时将次优样本转化为有价值的训练点

Result: 在多个基准测试中，该方法持续提升种子数据质量，平均提高LLM准确率7.15%，同时保持原始数据集规模

Conclusion: 这项工作通过数据和模型的动态人机协同演化，为可持续的LLM训练建立了新范式

Abstract: Supervised Fine-Tuning (SFT) Large Language Models (LLM) fundamentally rely
on high-quality training data. While data selection and data synthesis are two
common strategies to improve data quality, existing approaches often face
limitations in static dataset curation that fail to adapt to evolving model
capabilities. In this paper, we introduce Middo, a self-evolving Model-informed
dynamic data optimization framework that uses model-aware data selection and
context-preserving data refinement. Unlike conventional one-off
filtering/synthesis methods, our framework establishes a closed-loop
optimization system: (1) A self-referential diagnostic module proactively
identifies suboptimal samples through tri-axial model signals - loss patterns
(complexity), embedding cluster dynamics (diversity), and self-alignment scores
(quality); (2) An adaptive optimization engine then transforms suboptimal
samples into pedagogically valuable training points while preserving semantic
integrity; (3) This optimization process continuously evolves with model
capability through dynamic learning principles. Experiments on multiple
benchmarks demonstrate that our \method consistently enhances the quality of
seed data and boosts LLM's performance with improving accuracy by 7.15% on
average while maintaining the original dataset scale. This work establishes a
new paradigm for sustainable LLM training through dynamic human-AI co-evolution
of data and models. Our datasets, models, and code are coming soon.

</details>


### [28] [Personality Matters: User Traits Predict LLM Preferences in Multi-Turn Collaborative Tasks](https://arxiv.org/abs/2508.21628)
*Sarfaroz Yunusov,Kaige Chen,Kazi Nishat Anwar,Ali Emami*

Main category: cs.CL

TL;DR: 研究发现不同性格类型用户对GPT-4和Claude 3.5有显著偏好差异，理性类型偏好GPT-4，理想主义者偏好Claude 3.5


<details>
  <summary>Details</summary>
Motivation: 探索不同性格特质的用户是否会系统地偏好某些大语言模型

Method: 将32名参与者按Keirsey性格类型均匀分组，在数据分析、创意写作、信息检索和写作辅助任务中评估GPT-4和Claude 3.5

Result: 理性类型强烈偏好GPT-4（尤其目标导向任务），理想主义者偏好Claude 3.5（尤其创造性和分析性任务），其他类型任务依赖性偏好

Conclusion: 性格基础的分析可以揭示传统评估方法没有发现的大语言模型差异

Abstract: As Large Language Models (LLMs) increasingly integrate into everyday
workflows, where users shape outcomes through multi-turn collaboration, a
critical question emerges: do users with different personality traits
systematically prefer certain LLMs over others? We conducted a study with 32
participants evenly distributed across four Keirsey personality types,
evaluating their interactions with GPT-4 and Claude 3.5 across four
collaborative tasks: data analysis, creative writing, information retrieval,
and writing assistance. Results revealed significant personality-driven
preferences: Rationals strongly preferred GPT-4, particularly for goal-oriented
tasks, while idealists favored Claude 3.5, especially for creative and
analytical tasks. Other personality types showed task-dependent preferences.
Sentiment analysis of qualitative feedback confirmed these patterns. Notably,
aggregate helpfulness ratings were similar across models, showing how
personality-based analysis reveals LLM differences that traditional evaluations
miss.

</details>


### [29] [QZhou-Embedding Technical Report](https://arxiv.org/abs/2508.21632)
*Peng Yu,En Xu,Bin Chen,Haibiao Chen,Yinfei Xu*

Main category: cs.CL

TL;DR: QZhou-Embedding是基于Qwen2.5-7B-Instruct构建的通用文本嵌入模型，通过多任务框架、数据合成和两阶段训练策略，在MTEB和CMTEB基准测试中取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 开发具有卓越文本表示能力的通用上下文文本嵌入模型，通过高质量多样化数据提升检索模型性能。

Method: 采用统一多任务框架，包含专门的数据转换和训练策略；使用LLM API进行数据合成（释义、增强、硬负例生成）；实施两阶段训练策略（检索预训练+全任务微调）。

Result: 在MTEB和CMTEB基准测试中排名第一（2025年8月27日），同时在重排序、聚类等任务上达到SOTA性能。

Conclusion: 高质量多样化数据对提升检索模型性能至关重要，利用LLM生成能力可以进一步优化数据质量以实现嵌入模型突破。

Abstract: We present QZhou-Embedding, a general-purpose contextual text embedding model
with exceptional text representation capabilities. Built upon the
Qwen2.5-7B-Instruct foundation model, we designed a unified multi-task
framework comprising specialized data transformation and training strategies.
The data transformation scheme enables the incorporation of more diverse
textual training datasets, while the task-specific training strategies enhance
model learning efficiency. We developed a data synthesis pipeline leveraging
LLM API, incorporating techniques such as paraphrasing, augmentation, and hard
negative example generation to improve the semantic richness and sample
difficulty of the training set. Additionally, we employ a two-stage training
strategy, comprising initial retrieval-focused pretraining followed by
full-task fine-tuning, enabling the embedding model to extend its capabilities
based on robust retrieval performance. Our model achieves state-of-the-art
results on the MTEB and CMTEB benchmarks, ranking first on both leaderboards
(August 27 2025), and simultaneously achieves state-of-the-art performance on
tasks including reranking, clustering, etc. Our findings demonstrate that
higher-quality, more diverse data is crucial for advancing retrieval model
performance, and that leveraging LLMs generative capabilities can further
optimize data quality for embedding model breakthroughs. Our model weights are
released on HuggingFace under Apache 2.0 license. For reproducibility, we
provide evaluation code and instructions on GitHub.

</details>


### [30] [Is this chart lying to me? Automating the detection of misleading visualizations](https://arxiv.org/abs/2508.21675)
*Jonathan Tonglet,Jan Zimny,Tinne Tuytelaars,Iryna Gurevych*

Main category: cs.CL

TL;DR: 提出了Misviz基准数据集和Misviz-synth合成数据集，用于检测误导性可视化图表并识别其违反的设计规则，评估显示当前MLLMs在此任务上仍面临重大挑战


<details>
  <summary>Details</summary>
Motivation: 误导性可视化是社交媒体和网络上错误信息的重要来源，违反图表设计原则会扭曲数据并导致读者得出错误结论。现有AI模型训练和评估缺乏大型、多样化和公开可用的数据集

Method: 构建了包含2,604个真实世界可视化图表的Misviz基准数据集（标注12种误导类型），以及基于Matplotlib和真实数据表生成的81,814个合成可视化图表的Misviz-synth数据集。使用最先进的MLLMs、基于规则的系统和高精度分类器进行综合评估

Result: 任务仍然极具挑战性，当前模型在检测误导性可视化方面表现有限

Conclusion: 发布了Misviz、Misviz-synth数据集及相关代码，为检测误导性可视化图表和减少错误信息传播提供了重要资源，但该领域仍需进一步研究

Abstract: Misleading visualizations are a potent driver of misinformation on social
media and the web. By violating chart design principles, they distort data and
lead readers to draw inaccurate conclusions. Prior work has shown that both
humans and multimodal large language models (MLLMs) are frequently deceived by
such visualizations. Automatically detecting misleading visualizations and
identifying the specific design rules they violate could help protect readers
and reduce the spread of misinformation. However, the training and evaluation
of AI models has been limited by the absence of large, diverse, and openly
available datasets. In this work, we introduce Misviz, a benchmark of 2,604
real-world visualizations annotated with 12 types of misleaders. To support
model training, we also release Misviz-synth, a synthetic dataset of 81,814
visualizations generated using Matplotlib and based on real-world data tables.
We perform a comprehensive evaluation on both datasets using state-of-the-art
MLLMs, rule-based systems, and fine-tuned classifiers. Our results reveal that
the task remains highly challenging. We release Misviz, Misviz-synth, and the
accompanying code.

</details>


### [31] [Not All Parameters Are Created Equal: Smart Isolation Boosts Fine-Tuning Performance](https://arxiv.org/abs/2508.21741)
*Yao Wang,Di Liang,Minlong Peng*

Main category: cs.CL

TL;DR: 核参数隔离精细调整框架(CPI-FT)通过识别任务核心参数区域和参数融合技术，有效缓解多任务精细调整中的任务干扰和恐怖遗忘问题


<details>
  <summary>Details</summary>
Motivation: 解决监督式精细调整(SFT)中的"泵泵板现象"，即无差别参数更新导致某些任务性能提升以及其他任务性能下降的问题

Method: 1、单任务独立精细调整识别核心参数区域 2、核心区域相似的任务分组编组 3、核心参数直接移植，非核心参数通过球面线性插值(SLERP)融合 4、轻量级流水线SFT训练阻止恐怖遗忘

Result: 在多个公开测试集上实验表明，该方法显著缓解任务干扰和遗忘问题，持续超越传统多任务和多阶段精细调整基线方法

Conclusion: CPI-FT框架通过核心参数隔离和智能融合策略，为大语言模型的多任务适配提供了有效解决方案，在保持各任务性能的同时避免互相冲突

Abstract: Supervised fine-tuning (SFT) is a pivotal approach to adapting large language
models (LLMs) for downstream tasks; however, performance often suffers from the
``seesaw phenomenon'', where indiscriminate parameter updates yield progress on
certain tasks at the expense of others. To address this challenge, we propose a
novel \emph{Core Parameter Isolation Fine-Tuning} (CPI-FT) framework.
Specifically, we first independently fine-tune the LLM on each task to identify
its core parameter regions by quantifying parameter update magnitudes. Tasks
with similar core regions are then grouped based on region overlap, forming
clusters for joint modeling. We further introduce a parameter fusion technique:
for each task, core parameters from its individually fine-tuned model are
directly transplanted into a unified backbone, while non-core parameters from
different tasks are smoothly integrated via Spherical Linear Interpolation
(SLERP), mitigating destructive interference. A lightweight, pipelined SFT
training phase using mixed-task data is subsequently employed, while freezing
core regions from prior tasks to prevent catastrophic forgetting. Extensive
experiments on multiple public benchmarks demonstrate that our approach
significantly alleviates task interference and forgetting, consistently
outperforming vanilla multi-task and multi-stage fine-tuning baselines.

</details>


### [32] [Reasoning-Intensive Regression](https://arxiv.org/abs/2508.21762)
*Diane Tchuindjo,Omar Khattab*

Main category: cs.CL

TL;DR: 本文提出了MENTAT方法，针对推理密集型回归任务(RiR)，通过批量反射提示优化和神经集成学习，在有限数据和计算资源下显著提升大型语言模型的推理性能。


<details>
  <summary>Details</summary>
Motivation: 当前AI研究将大语言模型应用于推理密集型回归任务时面临挑战，这类任务需要深度文本分析但训练数据和计算资源有限，传统提示方法和微调方法效果不佳。

Method: 提出MENTAT方法，结合批量反射提示优化和神经集成学习，通过优化提示策略和集成多个模型预测来提升推理性能。

Result: MENTAT方法相比基线方法实现了高达65%的性能提升，在三个现实问题构建的基准测试中表现优异。

Conclusion: MENTAT为推理密集型回归任务提供了有效的解决方案，但该领域仍有很大改进空间，需要未来进一步研究。

Abstract: AI researchers and practitioners increasingly apply large language models
(LLMs) to what we call reasoning-intensive regression (RiR), i.e. deducing
subtle numerical properties from text. Unlike standard language regression
tasks, e.g. for sentiment or similarity, RiR often appears instead in ad-hoc
problems like rubric-based scoring or domain-specific retrieval, where much
deeper analysis of text is required while only limited task-specific training
data and computation are available. We cast three realistic problems as RiR
tasks to establish an initial benchmark, and use that to test our hypothesis
that prompting frozen LLMs and finetuning Transformer encoders via gradient
descent will both often struggle in RiR. We then propose MENTAT, a simple and
lightweight method that combines batch-reflective prompt optimization with
neural ensemble learning. MENTAT achieves up to 65% improvement over both
baselines, though substantial room remains for future advances in RiR.

</details>


### [33] [PiCSAR: Probabilistic Confidence Selection And Ranking](https://arxiv.org/abs/2508.21787)
*Joshua Ong Jun Leang,Zheng Zhao,Aryo Pradipta Gema,Sohee Yang,Wai-Chung Kwan,Xuanli He,Wenda Li,Pasquale Minervini,Eleonora Giunchiglia,Shay B. Cohen*

Main category: cs.CL

TL;DR: PiCSAR是一种无需训练的评分方法，通过联合对数似然来评估推理链和最终答案的可信度，在数学推理任务中显著优于现有方法且样本效率更高


<details>
  <summary>Details</summary>
Motivation: 解决推理任务中缺乏真实答案时如何设计有效的评分函数来识别正确推理链的关键挑战

Method: 提出概率置信选择与排序(PiCSAR)，使用推理过程和最终答案的联合对数似然来评分候选生成结果，该方法自然分解为推理置信度和答案置信度

Result: 在多个基准测试中取得显著提升(MATH500 +10.18, AIME2025 +9.81)，在20次比较中有16次以至少2倍更少的样本超越基线方法

Conclusion: 正确推理链展现出显著更高的推理和答案置信度，验证了PiCSAR方法的有效性

Abstract: Best-of-n sampling improves the accuracy of large language models (LLMs) and
large reasoning models (LRMs) by generating multiple candidate solutions and
selecting the one with the highest reward. The key challenge for reasoning
tasks is designing a scoring function that can identify correct reasoning
chains without access to ground-truth answers. We propose Probabilistic
Confidence Selection And Ranking (PiCSAR): a simple, training-free method that
scores each candidate generation using the joint log-likelihood of the
reasoning and final answer. The joint log-likelihood of the reasoning and final
answer naturally decomposes into reasoning confidence and answer confidence.
PiCSAR achieves substantial gains across diverse benchmarks (+10.18 on MATH500,
+9.81 on AIME2025), outperforming baselines with at least 2x fewer samples in
16 out of 20 comparisons. Our analysis reveals that correct reasoning chains
exhibit significantly higher reasoning and answer confidence, justifying the
effectiveness of PiCSAR.

</details>


### [34] [Going over Fine Web with a Fine-Tooth Comb: Technical Report of Indexing Fine Web for Problematic Content Search and Retrieval](https://arxiv.org/abs/2508.21788)
*Inés Altemir Marinas,Anastasiia Kucherenko,Andrei Kucharavy*

Main category: cs.CL

TL;DR: 提出了一个基于ElasticSearch的框架，用于索引和分析LLM训练数据集，在FineWeb-2语料库上实现毫秒级查询性能，为AI系统安全性和可问责性提供实用工具。


<details>
  <summary>Details</summary>
Motivation: 网络爬取的大规模数据集存在数据质量、安全性和伦理问题，但之前的研究受计算限制只能分析小样本，需要开发高效的分析框架来评估LLM训练数据。

Method: 构建基于ElasticSearch的索引分析管道，应用于SwissAI的FineWeb-2语料库（1.5TB，四种语言），实现快速查询性能。

Result: 实现了毫秒级查询响应（大多数搜索在毫秒内完成，所有查询都在2秒内），展示了实时数据集分析能力。

Conclusion: 该框架为构建更安全、更负责任的AI系统提供了实用的数据集分析工具，能够有效应对LLM训练数据中的有害内容问题。

Abstract: Large language models (LLMs) rely heavily on web-scale datasets like Common
Crawl, which provides over 80\% of training data for some modern models.
However, the indiscriminate nature of web crawling raises challenges in data
quality, safety, and ethics. Despite the critical importance of training data
quality, prior research on harmful content has been limited to small samples
due to computational constraints. This project presents a framework for
indexing and analyzing LLM training datasets using an ElasticSearch-based
pipeline. We apply it to SwissAI's FineWeb-2 corpus (1.5TB, four languages),
achieving fast query performance--most searches in milliseconds, all under 2
seconds. Our work demonstrates real-time dataset analysis, offering practical
tools for safer, more accountable AI systems.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [35] [Fuzzy, Symbolic, and Contextual: Enhancing LLM Instruction via Cognitive Scaffolding](https://arxiv.org/abs/2508.21204)
*Vanessa Figueiredo*

Main category: cs.AI

TL;DR: 该研究探讨了架构归纳偏置如何影响大语言模型在教学对话中的认知行为，通过引入符号支架机制和短期记忆模式来促进结构化推理，实验表明完整系统在认知行为方面优于基线变体。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型架构中的归纳偏置如何影响其在教学对话中的认知行为，特别是如何通过架构设计来促进自适应、结构化的苏格拉底式辅导推理。

Method: 引入符号支架机制和短期记忆模式，通过五个系统变体的对照消融实验，使用专家设计的评估标准（涵盖支架、响应性、符号推理和对话记忆）来评估模型输出，并采用基于LLM的评估框架进行可扩展的系统比较。

Result: 完整系统在所有评估维度上一致优于基线变体，移除记忆或符号结构会降低关键认知行为（包括抽象、自适应探测和概念连续性）的表现。

Conclusion: 架构支架能够可靠地塑造大语言模型中涌现的教学策略，支持了处理层面的解释，表明架构设计对模型认知行为具有重要影响。

Abstract: We study how architectural inductive biases influence the cognitive behavior
of large language models (LLMs) in instructional dialogue. We introduce a
symbolic scaffolding mechanism paired with a short-term memory schema designed
to promote adaptive, structured reasoning in Socratic tutoring. Using
controlled ablation across five system variants, we evaluate model outputs via
expert-designed rubrics covering scaffolding, responsiveness, symbolic
reasoning, and conversational memory. We present preliminary results using an
LLM-based evaluation framework aligned to a cognitively grounded rubric. This
enables scalable, systematic comparisons across architectural variants in
early-stage experimentation. The preliminary results show that our full system
consistently outperforms baseline variants. Analysis reveals that removing
memory or symbolic structure degrades key cognitive behaviors, including
abstraction, adaptive probing, and conceptual continuity. These findings
support a processing-level account in which architectural scaffolds can
reliably shape emergent instructional strategies in LLMs.

</details>


### [36] [AHELM: A Holistic Evaluation of Audio-Language Models](https://arxiv.org/abs/2508.21376)
*Tony Lee,Haoqin Tu,Chi Heem Wong,Zijun Wang,Siwei Yang,Yifan Mai,Yuyin Zhou,Cihang Xie,Percy Liang*

Main category: cs.AI

TL;DR: AHELM是一个新的音频-语言模型基准测试，整合了多个数据集来全面评估10个重要方面，包括感知、推理、公平性等，并标准化了评估流程。测试了14个模型，发现Gemini 2.5 Pro在多个方面表现最佳但存在公平性问题，基线系统表现也令人惊讶。


<details>
  <summary>Details</summary>
Motivation: 当前音频-语言模型评估缺乏标准化基准，大多数基准只测试一两个能力，且忽略了公平性、安全性等重要方面。不同评估使用不同的提示方法和推理参数，导致模型间难以比较。

Method: 引入AHELM基准，整合多个数据集（包括两个新的合成音频-文本数据集PARADE和CoRe-Bench），标准化提示、推理参数和评估指标，在10个重要方面全面评估模型性能。

Result: 测试了14个开源和闭源API模型及3个基线系统。Gemini 2.5 Pro在10个方面中的5个排名第一，但在ASR任务中表现出群体不公平性（p=0.01）。基线系统表现良好，其中一个仅具有语音转文本功能的系统总体排名第5。

Conclusion: AHELM提供了一个全面的音频-语言模型评估框架，揭示了当前模型的优势和局限性，特别是公平性问题。基准将持续更新，旨在促进该领域的标准化和透明评估。

Abstract: Evaluations of audio-language models (ALMs) -- multimodal models that take
interleaved audio and text as input and output text -- are hindered by the lack
of standardized benchmarks; most benchmarks measure only one or two
capabilities and omit evaluative aspects such as fairness or safety.
Furthermore, comparison across models is difficult as separate evaluations test
a limited number of models and use different prompting methods and inference
parameters. To address these shortfalls, we introduce AHELM, a benchmark that
aggregates various datasets -- including 2 new synthetic audio-text datasets
called PARADE, which evaluates the ALMs on avoiding stereotypes, and
CoRe-Bench, which measures reasoning over conversational audio through
inferential multi-turn question answering -- to holistically measure the
performance of ALMs across 10 aspects we have identified as important to the
development and usage of ALMs: audio perception, knowledge, reasoning, emotion
detection, bias, fairness, multilinguality, robustness, toxicity, and safety.
We also standardize the prompts, inference parameters, and evaluation metrics
to ensure equitable comparisons across models. We test 14 open-weight and
closed-API ALMs from 3 developers and 3 additional simple baseline systems each
consisting of an automatic speech recognizer and a language model. Our results
show that while Gemini 2.5 Pro ranks top in 5 out of 10 aspects, it exhibits
group unfairness ($p=0.01$) on ASR tasks whereas most of the other models do
not. We also find that the baseline systems perform reasonably well on AHELM,
with one ranking 5th overall despite having only speech-to-text capabilities.
For transparency, all raw prompts, model generations, and outputs are available
on our website at https://crfm.stanford.edu/helm/audio/v1.0.0. AHELM is
intended to be a living benchmark and new datasets and models will be added
over time.

</details>


<div id='physics.ed-ph'></div>

# physics.ed-ph [[Back]](#toc)

### [37] [From Canonical to Complex: Benchmarking LLM Capabilities in Undergraduate Thermodynamics](https://arxiv.org/abs/2508.21452)
*Anna Geißler,Luca-Sophie Bien,Friedrich Schöppler,Tobias Hertel*

Main category: physics.ed-ph

TL;DR: LLMs在本科热力学教学中表现不佳，最佳模型准确率仅82%，远低于95%的能力阈值，特别是在图像推理和不可逆过程场景中存在明显短板


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在本科科学教育中作为无监督辅导工具的可行性，热力学因其严谨的法律体系和微妙概念区分成为理想测试领域

Method: 创建UTQA基准测试（50个本科热力学问题），涵盖理想气体过程、可逆性和图表解释，测试2025年领先LLM模型的表现

Result: 所有测试模型均未达到95%能力阈值，最佳模型准确率82%；文本问题表现优于图像推理任务（后者接近随机水平）；提示措辞和句法复杂度对性能影响有限

Conclusion: 当前LLMs在有限速率/不可逆场景以及将视觉特征与热力学含义绑定方面存在明显能力差距，尚不适合在该领域进行无监督辅导

Abstract: Large language models (LLMs) are increasingly considered as tutoring aids in
science education. Yet their readiness for unsupervised use in undergraduate
instruction remains uncertain, as reliable teaching requires more than fluent
recall: it demands consistent, principle-grounded reasoning. Thermodynamics,
with its compact laws and subtle distinctions between state and path functions,
reversibility, and entropy, provides an ideal testbed for evaluating such
capabilities. Here we present UTQA, a 50-item undergraduate thermodynamics
question answering benchmark, covering ideal-gas processes, reversibility, and
diagram interpretation. No leading 2025-era model exceeded our 95\% competence
threshold: the best LLMs achieved 82\% accuracy, with text-only items
performing better than image reasoning tasks, which often fell to chance
levels. Prompt phrasing and syntactic complexity showed modest to little
correlation with performance. The gap concentrates in finite-rate/irreversible
scenarios and in binding visual features to thermodynamic meaning, indicating
that current LLMs are not yet suitable for unsupervised tutoring in this
domain.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [38] [CrossTL: A Universal Programming Language Translator with Unified Intermediate Representation](https://arxiv.org/abs/2508.21256)
*Nripesh Niketan,Vaatsalya Shrivastva*

Main category: cs.PL

TL;DR: CrossTL是一个通用的编程语言翻译器，通过统一的中间表示CrossGL实现多种编程语言之间的双向翻译，支持CUDA、HIP、Metal、HLSL、GLSL、SPIR-V、Rust和Mojo等语言。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要为每对语言单独构建翻译器，导致复杂度呈指数级增长。CrossTL旨在通过统一的中间表示解决这个问题，实现"一次编写，到处部署"的开发模式。

Method: 系统包含：语言特定的词法分析器/解析器将源代码转换为AST，双向CrossGL翻译模块（ToCrossGLConverter类用于导入代码，CodeGen类用于目标代码生成），以及处理完整翻译流程的后端实现。

Result: 通过跨编程领域的全面评估，在所有支持的后端上都实现了成功的编译和执行。统一的IR设计使得添加新语言只需最少的努力，仅需要语言特定的前端/后端组件。

Conclusion: CrossTL代表了向语言无关编程迈出的重要一步，提供了统一的IR捕获多种编程范式语义、模块化架构支持可扩展性、支持GPU计算、图形编程和系统语言的综合框架，并通过实证验证了通用代码翻译的实用性。

Abstract: We present CrossTL, a universal programming language translator enabling
bidirectional translation between multiple languages through a unified
intermediate representation called CrossGL. Traditional approaches require
separate translators for each language pair, leading to exponential complexity
growth. CrossTL uses a single universal IR to facilitate translations between
CUDA, HIP, Metal, DirectX HLSL, OpenGL GLSL, Vulkan SPIR-V, Rust, and Mojo,
with Slang support in development. Our system consists of: language-specific
lexers/parsers converting source code to ASTs, bidirectional CrossGL
translation modules implementing ToCrossGLConverter classes for importing code
and CodeGen classes for target generation, and comprehensive backend
implementations handling full translation pipelines. We demonstrate
effectiveness through comprehensive evaluation across programming domains,
achieving successful compilation and execution across all supported backends.
The universal IR design enables adding new languages with minimal effort,
requiring only language-specific frontend/backend components. Our contributions
include: (1) a unified IR capturing semantics of multiple programming
paradigms, (2) a modular architecture enabling extensibility, (3) a
comprehensive framework supporting GPU compute, graphics programming, and
systems languages, and (4) empirical validation demonstrating practical
viability of universal code translation. CrossTL represents a significant step
toward language-agnostic programming, enabling write-once, deploy-everywhere
development.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [39] [Database Normalization via Dual-LLM Self-Refinement](https://arxiv.org/abs/2508.17693)
*Eunjae Jo,Nakyung Lee,Gyuyeong Kim*

Main category: cs.DB

TL;DR: Miffie是一个基于大语言模型的数据库自动化规范化框架，通过双模型自优化架构实现高精度的模式规范化，无需人工干预。


<details>
  <summary>Details</summary>
Motivation: 数据库规范化对保持数据完整性至关重要，但传统手动方式耗时且容易出错，需要自动化解决方案。

Method: 采用双模型自优化架构：生成模块基于验证模块的反馈消除异常，直到输出模式满足规范化要求；设计任务特定的零样本提示以提高准确性和成本效率。

Result: 实验结果表明Miffie能够规范化复杂数据库模式并保持高精度。

Conclusion: Miffie框架成功实现了自动化数据库规范化，为大语言模型在数据工程领域的应用提供了有效解决方案。

Abstract: Database normalization is crucial to preserving data integrity. However, it
is time-consuming and error-prone, as it is typically performed manually by
data engineers. To this end, we present Miffie, a database normalization
framework that leverages the capability of large language models. Miffie
enables automated data normalization without human effort while preserving high
accuracy. The core of Miffie is a dual-model self-refinement architecture that
combines the best-performing models for normalized schema generation and
verification, respectively. The generation module eliminates anomalies based on
the feedback of the verification module until the output schema satisfies the
requirement for normalization. We also carefully design task-specific zero-shot
prompts to guide the models for achieving both high accuracy and cost
efficiency. Experimental results show that Miffie can normalize complex
database schemas while maintaining high accuracy.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [40] [Quantum-Enhanced Natural Language Generation: A Multi-Model Framework with Hybrid Quantum-Classical Architectures](https://arxiv.org/abs/2508.21332)
*Chi-Sheng Chen,En-Jui Kuo*

Main category: quant-ph

TL;DR: 量子文本生成模型与传统Transformer的综合比较研究，发现Transformer在整体性能上仍最优，但量子模型在特定场景中表现竞争力


<details>
  <summary>Details</summary>
Motivation: 因对量子计算在自然语言处理应用的日益增长兴趣，需要系统性评估量子文本生成模型与传统模型的性能差异

Method: 对比五种模型(Transformer、QKSAN、QRWKV、QASA)在五个多样化数据集上进行实验，使用语言模型评估指标(困惑度、BLEU、词汇多样性、重复率、流畅度)进行综合评估

Result: Transformer模型整体最优(平均困惑度1.21，BLEU-1得0.2895)，但量子模型在特定方面表现竞争力：QKSAN实现零重复率和竞争性BLEU-1得分(0.2800)，QRWKV在某些任务中实现完美词汇多样性

Conclusion: 传统Transformer模型在文本生成任务中仍保持整体优势，但量子受启发模型在特定性能指标上显示出竞争力，为量子计算在NLP领域的应用提供了有价值的参考

Abstract: This paper presents a comprehensive evaluation of quantum text generation
models against traditional Transformer/MLP architectures, addressing the
growing interest in quantum computing applications for natural language
processing. We conduct systematic experiments comparing five distinct models:
Transformer (baseline), Quantum Kernel Self-Attention Network (QKSAN), Quantum
RWKV (QRWKV), and Quantum Attention Sequence Architecture (QASA) across five
diverse datasets including simple sentences, short stories, quantum phrases,
haiku poetry, and proverbs. Our evaluation employs multiple metrics including
perplexity, BLEU scores, vocabulary diversity, repetition rates, and fluency
measures to assess different aspects of text generation quality. The
experimental results reveal that while traditional Transformer models maintain
overall superiority with the lowest average perplexity (1.21) and highest
BLEU-1 score (0.2895), quantum-inspired models demonstrate competitive
performance in specific scenarios. Notably, QKSAN achieves a competitive BLEU-1
score of 0.2800 while maintaining zero repetition rates, and QRWKV demonstrates
perfect vocabulary diversity (Distinct-1 = 1.000) in certain tasks.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [41] [Designing Smarter Conversational Agents for Kids: Lessons from Cognitive Work and Means-Ends Analyses](https://arxiv.org/abs/2508.21209)
*Vanessa Figueiredo*

Main category: cs.HC

TL;DR: 该研究通过两个实验探讨巴西儿童使用对话代理的学习效果，发现结构化提示方法能显著提升交互质量，并提出了针对儿童的设计建议。


<details>
  <summary>Details</summary>
Motivation: 研究旨在了解巴西儿童如何利用对话代理进行学习、探索和娱乐，并探索结构化支架如何改善这些互动体验。

Method: 研究1：对23名参与者进行7周在线调查，使用访谈、观察和认知工作分析方法；研究2：使用GPT-4o-mini对1200个模拟儿童-对话代理交互进行测试，比较结构化提示与无结构基线。

Result: 识别出对话代理的三大功能：学习、探索、娱乐；结构化提示方法在可读性、问题数量/深度/多样性、连贯性方面均优于无结构基线。

Conclusion: 提出了设计建议：支架式对话树、儿童专用配置文件、看护人策划内容，为儿童与对话代理的有效互动提供了实证框架和结构化提示方法。

Abstract: This paper presents two studies on how Brazilian children (ages 9--11) use
conversational agents (CAs) for schoolwork, discovery, and entertainment, and
how structured scaffolds can enhance these interactions. In Study 1, a
seven-week online investigation with 23 participants (children, parents,
teachers) employed interviews, observations, and Cognitive Work Analysis to map
children's information-processing flows, the role of more knowledgeable others,
functional uses, contextual goals, and interaction patterns to inform
conversation-tree design. We identified three CA functions: School, Discovery,
Entertainment, and derived ``recipe'' scaffolds mirroring parent-child support.
In Study 2, we prompted GPT-4o-mini on 1,200 simulated child-CA exchanges,
comparing conversation-tree recipes based on structured-prompting to an
unstructured baseline. Quantitative evaluation of readability, question
count/depth/diversity, and coherence revealed gains for the recipe approach.
Building on these findings, we offer design recommendations: scaffolded
conversation-trees, child-dedicated profiles for personalized context, and
caregiver-curated content. Our contributions include the first CWA application
with Brazilian children, an empirical framework of child-CA information flows,
and an LLM-scaffolding ``recipe'' (i.e., structured-prompting) for effective,
scaffolded learning.

</details>


### [42] [Morae: Proactively Pausing UI Agents for User Choices](https://arxiv.org/abs/2508.21456)
*Yi-Hao Peng,Dingzeyu Li,Jeffrey P. Bigham,Amy Pavel*

Main category: cs.HC

TL;DR: Morae是一个UI代理系统，通过识别任务执行中的决策点并暂停让用户做出选择，提高了盲人和低视力用户在使用UI代理时的参与度和控制权。


<details>
  <summary>Details</summary>
Motivation: 当前UI代理通常端到端执行任务，不涉及用户在关键选择中的参与，也不让用户了解重要上下文信息，从而降低了用户自主权。研究发现在实际场景中，代理会自动化选择而不考虑用户偏好。

Method: Morae使用大型多模态模型解释用户查询、UI代码和屏幕截图，在需要做出选择时提示用户澄清。系统自动识别任务执行中的决策点并暂停，让用户参与选择过程。

Result: 在真实网页任务研究中，与基线代理（包括OpenAI Operator）相比，Morae帮助用户完成更多任务，并选择更符合偏好的选项。

Conclusion: 这项工作展示了混合主动方法的价值，用户既能从UI代理的自动化中受益，又能表达自己的偏好，提高了用户代理和控制感。

Abstract: User interface (UI) agents promise to make inaccessible or complex UIs easier
to access for blind and low-vision (BLV) users. However, current UI agents
typically perform tasks end-to-end without involving users in critical choices
or making them aware of important contextual information, thus reducing user
agency. For example, in our field study, a BLV participant asked to buy the
cheapest available sparkling water, and the agent automatically chose one from
several equally priced options, without mentioning alternative products with
different flavors or better ratings. To address this problem, we introduce
Morae, a UI agent that automatically identifies decision points during task
execution and pauses so that users can make choices. Morae uses large
multimodal models to interpret user queries alongside UI code and screenshots,
and prompt users for clarification when there is a choice to be made. In a
study over real-world web tasks with BLV participants, Morae helped users
complete more tasks and select options that better matched their preferences,
as compared to baseline agents, including OpenAI Operator. More broadly, this
work exemplifies a mixed-initiative approach in which users benefit from the
automation of UI agents while being able to express their preferences.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [43] [Normalisation of SWIFT Message Counterparties with Feature Extraction and Clustering](https://arxiv.org/abs/2508.21081)
*Thanasis Schoinas,Benjamin Guinard,Diba Esbati,Richard Chalk*

Main category: cs.LG

TL;DR: 这篇论文提出了一种混合形式的字符串相似性、主题建模、层次聚类和规则基础的流水线方法，用于聚类交易对手方名称，解决传统自然语言模型在短文本聚类中的不足。


<details>
  <summary>Details</summary>
Motivation: 传统自然语言模型不适合聚类银行支付消息中的交易对手方名称，因为这些文本缺乏句子结构且包含手工输入的噪声和变体。现有的模糊匹配工具不能满足反欺诈专业人员的需求。

Method: 设计了一种混合方法流水线，结合字符串相似性、主题建模、层次聚类和规则基础方法，支持未知聚类数量的情况。同时设计了基于精准率和召回率的评估指标。

Result: 在真实标签数据集上进行测试，表现显著超过了基础的规则基础（关键词）方法。该方法保留了规则系统的可解释性，同时减少了手动审查的需求。

Conclusion: 这种混合方法有效解决了短文本聚类的挑战，特别是在制裁调查等场景中，能够更好地控制漏掉实体变体的风险，为反欺诈专业人员提供了更有效的工具。

Abstract: Short text clustering is a known use case in the text analytics community.
When the structure and content falls in the natural language domain e.g.
Twitter posts or instant messages, then natural language techniques can be
used, provided texts are of sufficient length to allow for use of (pre)trained
models to extract meaningful information, such as part-of-speech or topic
annotations. However, natural language models are not suitable for clustering
transaction counterparties, as they are found in bank payment messaging
systems, such as SWIFT. The manually typed tags are typically physical or legal
entity details, which lack sentence structure, while containing all the
variations and noise that manual entry introduces. This leaves a gap in an
investigator or counter-fraud professional's toolset when looking to augment
their knowledge of payment flow originator and beneficiary entities and trace
funds and assets. A gap that vendors traditionally try to close with fuzzy
matching tools. With these considerations in mind, we are proposing a hybrid
string similarity, topic modelling, hierarchical clustering and rule-based
pipeline to facilitate clustering of transaction counterparties, also catering
for unknown number of expected clusters. We are also devising metrics to
supplement the evaluation of the approach, based on the well-known measures of
precision and recall. Testing on a real-life labelled dataset demonstrates
significantly improved performance over a baseline rule-based ('keyword')
approach. The approach retains most of the interpretability found in rule-based
systems, as the former adds an additional level of cluster refinement to the
latter. The resulting workflow reduces the need for manual review. When only a
subset of the population needs to be investigated, such as in sanctions
investigations, the approach allows for better control of the risks of missing
entity variations.

</details>


### [44] [Model-Task Alignment Drives Distinct RL Outcomes](https://arxiv.org/abs/2508.21188)
*Haoze Wu,Cheng Wang,Wenshuo Zhao,Junxian He*

Main category: cs.LG

TL;DR: 本研究揭示了强化学习在大型语言模型中的反直觉现象（如单样本训练匹配完整数据集性能）只在模型与任务高度对齐时有效，而在更具挑战性的场景中标准RL方法仍更稳健。


<details>
  <summary>Details</summary>
Motivation: 近期研究发现RL在LLMs中产生了一系列反直觉现象，但这些现象适用的具体条件和失败情况尚不明确，需要系统研究来理解其背后的机制。

Method: 通过系统性和全面的实验验证，在不同模型架构和任务领域检验一系列反直觉主张，重点考察模型-任务对齐程度（用pass@k准确率衡量）对结果的影响。

Result: 研究发现标准RL训练在各种设置下都保持稳健，而许多反直觉结果仅在模型与任务已高度对齐时出现；在更具挑战性的情况下，这些技术无法驱动实质性学习。

Conclusion: 模型-任务对齐是区分RL观察结果的关键因素，反直觉方法只在高度对齐场景有效，而标准RL方法在更广泛条件下保持有效性。

Abstract: Recent advances in applying reinforcement learning (RL) to large language
models (LLMs) have led to substantial progress. In particular, a series of
remarkable yet often counterintuitive phenomena have been reported in LLMs,
exhibiting patterns not typically observed in traditional RL settings. For
example, notable claims include that a single training example can match the
performance achieved with an entire dataset, that the reward signal does not
need to be very accurate, and that training solely with negative samples can
match or even surpass sophisticated reward-based methods. However, the precise
conditions under which these observations hold - and, critically, when they
fail - remain unclear. In this work, we identify a key factor that
differentiates RL observations: whether the pretrained model already exhibits
strong Model-Task Alignment, as measured by pass@k accuracy on the evaluated
task. Through a systematic and comprehensive examination of a series of
counterintuitive claims, supported by rigorous experimental validation across
different model architectures and task domains, our findings show that while
standard RL training remains consistently robust across settings, many of these
counterintuitive results arise only when the model and task already exhibit
strong model-task alignment. In contrast, these techniques fail to drive
substantial learning in more challenging regimes, where standard RL methods
remain effective.

</details>


### [45] [Accept or Deny? Evaluating LLM Fairness and Performance in Loan Approval across Table-to-Text Serialization Approaches](https://arxiv.org/abs/2508.21512)
*Israel Abebe Azime,Deborah D. Kanubala,Tejumade Afonja,Mario Fritz,Isabel Valera,Dietrich Klakow,Philipp Slusallek*

Main category: cs.LG

TL;DR: 评估LLM在不同序列化格式下处理贷款审批表格数据的性能和公平性，发现序列化格式选择显著影响结果，ICL提升性能但公平性效果不一


<details>
  <summary>Details</summary>
Motivation: LLM越来越多用于高风险决策任务（如贷款审批），但处理表格数据、确保公平性和提供可靠预测方面存在困难，需要评估其在不同地区的表现

Method: 使用加纳、德国和美国的贷款审批数据集，评估LLM的零样本学习和上下文学习能力，比较不同序列化格式（如GReat和LIFT）的效果

Result: 序列化格式选择显著影响LLM的性能和公平性，某些格式（如GReat和LIFT）虽然提高F1分数但加剧了公平性差距；ICL相比零样本基线提升性能4.9-59.6%，但对公平性的影响因数据集而异

Conclusion: 需要有效的表格数据表示方法和公平性感知模型来提高LLM在金融决策中的可靠性

Abstract: Large Language Models (LLMs) are increasingly employed in high-stakes
decision-making tasks, such as loan approvals. While their applications expand
across domains, LLMs struggle to process tabular data, ensuring fairness and
delivering reliable predictions. In this work, we assess the performance and
fairness of LLMs on serialized loan approval datasets from three geographically
distinct regions: Ghana, Germany, and the United States. Our evaluation focuses
on the model's zero-shot and in-context learning (ICL) capabilities. Our
results reveal that the choice of serialization (Serialization refers to the
process of converting tabular data into text formats suitable for processing by
LLMs.) format significantly affects both performance and fairness in LLMs, with
certain formats such as GReat and LIFT yielding higher F1 scores but
exacerbating fairness disparities. Notably, while ICL improved model
performance by 4.9-59.6% relative to zero-shot baselines, its effect on
fairness varied considerably across datasets. Our work underscores the
importance of effective tabular data representation methods and fairness-aware
models to improve the reliability of LLMs in financial decision-making.

</details>


### [46] [Summarize-Exemplify-Reflect: Data-driven Insight Distillation Empowers LLMs for Few-shot Tabular Classification](https://arxiv.org/abs/2508.21561)
*Yifei Yuan,Jiatong Li,Weijia Zhang,Mohammad Aliannejadi,Evangelos Kanoulas,Renjun Hu*

Main category: cs.LG

TL;DR: InsightTab是一个基于洞察蒸馏的框架，通过规则总结、策略性示例和反思学习，帮助大语言模型更好地处理表格分类任务，在多个数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在少样本表格分类中存在挑战，主要由于结构化数据的变异性。需要将数据蒸馏为可操作的洞察，使LLM能够更有效地进行分类。

Method: 提出InsightTab框架，采用分而治之、简单优先和反思学习原则，结合规则总结、策略性示例和洞察反思，通过LLM与数据建模技术的深度协作实现数据洞察蒸馏。

Result: 在9个数据集上的广泛评估显示，InsightTab相比最先进方法有持续改进。消融研究验证了原则指导的蒸馏过程，分析强调了其在利用标注数据和管理偏差方面的有效性。

Conclusion: InsightTab通过洞察蒸馏框架成功提升了LLM在表格分类任务中的性能，使模型能够更好地将通用知识与特定表格任务要求对齐。

Abstract: Recent studies show the promise of large language models (LLMs) for few-shot
tabular classification but highlight challenges due to the variability in
structured data. To address this, we propose distilling data into actionable
insights to enable robust and effective classification by LLMs. Drawing
inspiration from human learning processes, we introduce InsightTab, an insight
distillation framework guided by principles of divide-and-conquer, easy-first,
and reflective learning. Our approach integrates rule summarization, strategic
exemplification, and insight reflection through deep collaboration between LLMs
and data modeling techniques. The obtained insights enable LLMs to better align
their general knowledge and capabilities with the particular requirements of
specific tabular tasks. We extensively evaluate InsightTab on nine datasets.
The results demonstrate consistent improvement over state-of-the-art methods.
Ablation studies further validate the principle-guided distillation process,
while analyses emphasize InsightTab's effectiveness in leveraging labeled data
and managing bias.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [47] [Why Stop at Words? Unveiling the Bigger Picture through Line-Level OCR](https://arxiv.org/abs/2508.21693)
*Shashank Vempati,Nishit Anand,Gaurav Talebailkar,Arpan Garai,Chetan Arora*

Main category: cs.CV

TL;DR: 这篇论文提出了从单词级到行级OCR的进阶方法，突破了传统单词分割的粘颈，通过提供更大的句子上下文来提高识别准确性和效率


<details>
  <summary>Details</summary>
Motivation: 传统OCR技术存在字符分割错误问题，而现代序列到序列方法将粘颈转移到了单词分割。识别到需要进一步迈向行级OCR来突破这一限制

Method: 提出了一种自然的逻辑进阶方法：从单词级OCR迁移到行级OCR，跳过单词检测错误，并为语言模型提供更大的句子上下文

Result: 实验结果显示端到端准确性提高5.4%，效率提高4倍，还构建了包含251个英语页面图像的行级注释数据集

Conclusion: 行级OCR方法不仅提高了准确性和效率，而且有潜力利用大语言模型的进步，尤其适用于文档图像处理

Abstract: Conventional optical character recognition (OCR) techniques segmented each
character and then recognized. This made them prone to error in character
segmentation, and devoid of context to exploit language models. Advances in
sequence to sequence translation in last decade led to modern techniques first
detecting words and then inputting one word at a time to a model to directly
output full words as sequence of characters. This allowed better utilization of
language models and bypass error-prone character segmentation step. We observe
that the above transition in style has moved the bottleneck in accuracy to word
segmentation. Hence, in this paper, we propose a natural and logical
progression from word level OCR to line-level OCR. The proposal allows to
bypass errors in word detection, and provides larger sentence context for
better utilization of language models. We show that the proposed technique not
only improves the accuracy but also efficiency of OCR. Despite our thorough
literature survey, we did not find any public dataset to train and benchmark
such shift from word to line-level OCR. Hence, we also contribute a
meticulously curated dataset of 251 English page images with line-level
annotations. Our experimentation revealed a notable end-to-end accuracy
improvement of 5.4%, underscoring the potential benefits of transitioning
towards line-level OCR, especially for document images. We also report a 4
times improvement in efficiency compared to word-based pipelines. With
continuous improvements in large language models, our methodology also holds
potential to exploit such advances. Project Website:
https://nishitanand.github.io/line-level-ocr-website

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [48] [Stairway to Fairness: Connecting Group and Individual Fairness](https://arxiv.org/abs/2508.21334)
*Theresia Veronika Rampisela,Maria Maistro,Tuukka Ruotsalo,Falk Scholer,Christina Lioma*

Main category: cs.IR

TL;DR: 本文研究发现推荐系统中群体公平性和个体公平性之间存在显著冲突，高度公平的群体推荐可能对个体非常不公平。


<details>
  <summary>Details</summary>
Motivation: 推荐系统中的公平性通常分为群体公平和个体公平，但两者之间的关系缺乏科学理解，现有研究使用不同的评估指标，无法进行有效比较。

Method: 通过全面比较可用于两种公平性类型的评估指标，在3个数据集上对8个运行进行实验分析。

Result: 实验表明，对群体高度公平的推荐可能对个体非常不公平，揭示了两种公平性类型之间的冲突关系。

Conclusion: 这一发现对推荐系统实践者改进系统公平性具有重要价值，表明需要在群体公平和个体公平之间进行权衡。

Abstract: Fairness in recommender systems (RSs) is commonly categorised into group
fairness and individual fairness. However, there is no established scientific
understanding of the relationship between the two fairness types, as prior work
on both types has used different evaluation measures or evaluation objectives
for each fairness type, thereby not allowing for a proper comparison of the
two. As a result, it is currently not known how increasing one type of fairness
may affect the other. To fill this gap, we study the relationship of group and
individual fairness through a comprehensive comparison of evaluation measures
that can be used for both fairness types. Our experiments with 8 runs across 3
datasets show that recommendations that are highly fair for groups can be very
unfair for individuals. Our finding is novel and useful for RS practitioners
aiming to improve the fairness of their systems. Our code is available at:
https://github.com/theresiavr/stairway-to-fairness.

</details>
