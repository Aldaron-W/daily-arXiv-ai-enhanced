<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 62]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.AI](#cs.AI) [Total: 5]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.SE](#cs.SE) [Total: 1]
- [cs.LG](#cs.LG) [Total: 3]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.CV](#cs.CV) [Total: 4]
- [cs.SI](#cs.SI) [Total: 2]
- [cs.IR](#cs.IR) [Total: 2]
- [cs.SD](#cs.SD) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [PHANTOM RECALL: When Familiar Puzzles Fool Smart Models](https://arxiv.org/abs/2510.11812)
*Souradeep Mukhopadhyay,Rishabh Baral,Nimeesh Mahajan,Samhitha Harish,Aswin RRV,Mihir Parmar,Mutsumi Nakamura,Chitta Baral*

Main category: cs.CL

TL;DR: LLMs在逻辑谜题中表现出记忆依赖而非真正推理，当谜题被微调时性能崩溃，揭示了模型推理能力的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs是否真正具备推理能力，还是仅仅依赖记忆模板来回答问题，特别是在谜题被修改时的表现。

Method: 引入PHANTOM RECALL基准测试，包含25个经典逻辑谜题和149个精心设计的扰动版本，评估11个主流LLM，并开发自动化逻辑等价判断工具和错误分类法。

Result: 模型在原始谜题上表现接近完美，但在扰动版本上显著低于人类水平，出现记忆重现和过度阐述等错误模式。

Conclusion: LLMs在语境线索变化时缺乏重新推理能力，揭示了语言流畅性与逻辑理解之间的重要差距。

Abstract: Large language models (LLMs) such as GPT, Gemini, and Claude often appear
adept at solving classic logic puzzles--but how much genuine reasoning
underlies their answers? Recent evidence suggests that these models frequently
rely on memorized templates rather than reasoning from first principles. When
puzzles are slightly modified, their performance collapses, revealing a
striking fragility. In particular, we asked: Have LLMs addressed these issues?
To what extent? How about perturbations to other puzzles? Is there a general
way of reformulating the prompt so that the models do better? To examine these
things systematically, we introduce PHANTOM RECALL, a benchmark comprising 25
well-known logic puzzles and 149 carefully designed perturbations that preserve
reasoning structure but alter superficial details and solutions. We evaluate
eleven leading LLMs and identify a recurring failure mode--phantom
recall--where models confidently reproduce memorized solutions or spurious
rationales that no longer fit the altered scenario. To probe and mitigate this
issue, we contribute three tools: (i) an automated logical-equivalence judge to
detect reasoning mismatches, (ii) a taxonomy of fine-grained reasoning error
categories, and (iii) a prompting-based mitigation framework guided by these
categories. Despite near-perfect accuracy on unmodified puzzles, models
significantly underperform humans on perturbed ones, exhibiting both phantom
recall and over-elaboration. Our findings reveal a crucial limitation: LLMs
often fail to re-reason when contextual cues shift--highlighting the gap
between linguistic fluency and logical understanding.

</details>


### [2] [R-WoM: Retrieval-augmented World Model For Computer-use Agents](https://arxiv.org/abs/2510.11892)
*Kai Mei,Jiang Guo,Shuaichen Chang,Mingwen Dong,Dongkyu Lee,Xing Niu,Jiarong Jiang*

Main category: cs.CL

TL;DR: LLMs作为世界模型在数字环境中增强智能体决策，但存在幻觉和静态知识限制。研究提出R-WoM方法，通过检索外部教程知识来增强LLM模拟能力，在长时程模拟中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs是否适合作为世界模型，解决其在长时程模拟中因幻觉和静态知识导致的复合错误问题。

Method: 提出检索增强世界模型(R-WoM)，通过从外部教程检索事实性、最新知识来增强LLM的模拟能力，评估了三个核心能力：下一状态预测、完整程序规划对齐和里程碑转换识别。

Result: R-WoM在OSWorld和WebArena上分别实现了25.3%和18.1%的显著改进，特别是在长时程模拟中表现优异。

Conclusion: 虽然LLMs在捕捉即时状态和识别有意义转换方面有效，但在完整程序规划中性能迅速下降。R-WoM通过知识检索有效缓解了LLMs在长时程环境动态建模中的局限性。

Abstract: Large Language Models (LLMs) can serve as world models to enhance agent
decision-making in digital environments by simulating future states and
predicting action outcomes, potentially eliminating costly trial-and-error
exploration. However, this capability is fundamentally limited by LLMs'
tendency toward hallucination and their reliance on static training knowledge,
which can lead to compounding errors that inhibit long-horizon simulations. To
systematically investigate whether LLMs are appropriate for world modeling, we
probe two core capabilities of world models--future state prediction and reward
estimation--through three tasks: next-state identification, full-procedure
planning alignment, and milestone transition recognition. Our analysis shows
that while LLMs effectively capture immediate next states and identify
meaningful state transitions, their performance rapidly degrades in
full-procedure planning. This highlights LLMs' limitations in reliably modeling
environment dynamics over long horizons. To address these limitations, we
propose the Retrieval-augmented World Model (R-WoM), which grounds LLM
simulations by incorporating factual, up-to-date knowledge retrieved from
external tutorials. Experiments show that R-WoM achieves substantial
improvements of up to 25.3% (OSWorld) and 18.1% (WebArena) compared to
baselines, with particular advantages in longer-horizon simulations.

</details>


### [3] [LLM Knowledge is Brittle: Truthfulness Representations Rely on Superficial Resemblance](https://arxiv.org/abs/2510.11905)
*Patrick Haller,Mark Ibrahim,Polina Kirichenko,Levent Sagun,Samuel J. Bell*

Main category: cs.CL

TL;DR: 研究发现LLM对陈述真实性的内部表示在输入发生轻微变化时会崩溃，表明LLM学习的是浅层、非鲁棒的知识表示，限制了泛化能力。


<details>
  <summary>Details</summary>
Motivation: 探索LLM性能脆弱性是否源于内部知识表示的不稳定性，特别是在输入发生语义保持的轻微变化时。

Method: 通过应用语义保持的扰动（如拼写错误、重新表述）使样本分布外，评估四个LLM家族、五个评估数据集和三种知识探测方法中表示可分性的退化情况。

Result: 当样本呈现与预训练数据相似度降低时，陈述真实性的内部表示会崩溃，LLM区分真假陈述的能力高度依赖于确切的表面形式。

Conclusion: LLM可能学习浅层、非鲁棒的知识表示，这为真实性探测的实用性带来根本挑战，并呼吁改进学习知识表示的鲁棒性。

Abstract: For Large Language Models (LLMs) to be reliable, they must learn robust
knowledge that can be generally applied in diverse settings -- often unlike
those seen during training. Yet, extensive research has shown that LLM
performance can be brittle, with models exhibiting excessive sensitivity to
trivial input variations. In this work, we explore whether this brittleness is
a direct result of unstable internal knowledge representations. To explore this
question, we build on previous work showing that LLM representations encode
statement truthfulness -- i.e., true, factual statements can be easily
separated from false, inaccurate ones. Specifically, we test the robustness of
learned knowledge by evaluating representation separability on samples that
have undergone superficial transformations to drive them out-of-distribution
(OOD), such as typos or reformulations. By applying semantically-preserving
perturbations, we study how separability degrades as statements become more
OOD, across four LLM families, five evaluation datasets, and three knowledge
probing methods. Our results reveal that internal representations of statement
truthfulness collapse as the samples' presentations become less similar to
those seen during pre-training. While LLMs can often distinguish between true
and false statements when they closely resemble the pre-training data, this
ability is highly dependent on the statement's exact surface form. These
findings offer a possible explanation for brittle benchmark performance: LLMs
may learn shallow, non-robust knowledge representations that allow for only
limited generalizability. Our work presents a fundamental challenge for the
utility of truthfulness probes, and more broadly, calls for further research on
improving the robustness of learned knowledge representations.

</details>


### [4] [LLM Reasoning for Machine Translation: Synthetic Data Generation over Thinking Tokens](https://arxiv.org/abs/2510.11919)
*Armel Zebaze,Rachel Bawden,Benoît Sagot*

Main category: cs.CL

TL;DR: 研究发现，在机器翻译任务中，生成中间"思考标记"并不能提升大型推理模型的翻译性能，包括使用人类翻译实践启发的思维链方法。


<details>
  <summary>Details</summary>
Motivation: 探索大型推理模型在机器翻译任务中生成中间标记的潜在益处，特别是在不同资源水平的语言对和多种设置下。

Method: 使用蒸馏思维链方法微调模型，生成逐步翻译的合成解释；构建模块化翻译特定提示策略的中间标记；比较标准输入输出微调与思维链方法的效果。

Result: 思维标记不能帮助LRMs更好地执行机器翻译；使用合成思维链解释的微调不优于标准输入输出微调；但通过组合模块化翻译特定提示策略构建中间标记能带来改进。

Conclusion: 微调过程中中间标记的贡献高度依赖于其中是否包含翻译尝试；使用教师模型改进目标翻译或扩展平行语料比将思维链解释蒸馏到"思考"翻译模型中更有效。

Abstract: Large reasoning models (LRMs) have led to new possibilities in terms of
problem-solving, through the devising of a natural language thought process
prior to answering a query. While their capabilities are well known across
mathematics and coding tasks, their impact on the task of machine translation
(MT) remains underexplored. In this work, we explore the benefits of the
generation of intermediate tokens when performing MT across multiple language
pairs of different levels of resourcedness and multiple setups. We find that
"thinking tokens" do not help LRMs better perform MT. This result generalizes
to models fine-tuned to reason before translating using distilled chain of
thought (CoT) inspired by human translators' practices. Specifically,
fine-tuning a model with synthetic CoT explanations detailing how to translate
step-by-step does not outperform standard input-output fine-tuning. However,
constructing the intermediate tokens by combining the outputs of modular
translation-specific prompting strategies results in improvements. Our findings
underscore that the contribution of intermediate tokens during fine-tuning
highly depends on the presence of translation attempts within them. More
broadly, our results suggest that using a teacher to refine target translations
or to expand parallel corpora is more impactful than distilling their CoT
explanations into "thinking" MT models.

</details>


### [5] [Discrepancy Detection at the Data Level: Toward Consistent Multilingual Question Answering](https://arxiv.org/abs/2510.11928)
*Lorena Calvo-Bartolomé,Valérie Aldana,Karla Cantarero,Alonso Madroñal de Mesa,Jerónimo Arenas-García,Jordan Boyd-Graber*

Main category: cs.CL

TL;DR: 提出了MIND框架，一个用户参与的自动化事实检查流程，用于检测多语言问答知识库中的事实和文化差异。


<details>
  <summary>Details</summary>
Motivation: 多语言问答系统需要确保跨语言的事实一致性，特别是对于客观问题，同时也要考虑主观回答中的文化差异。

Method: MIND是一个用户参与的自动化事实检查流程，通过检测多语言问答知识库中的事实和文化差异来识别不一致性。

Result: 在母婴健康领域的双语问答系统上评估MIND，并发布了标注事实和文化不一致性的双语问题数据集。在其他领域数据集上的测试也表明MIND能可靠识别不一致性。

Conclusion: MIND支持开发更具文化意识和事实一致性的问答系统，能够可靠识别跨语言的事实和文化不一致性。

Abstract: Multilingual question answering (QA) systems must ensure factual consistency
across languages, especially for objective queries such as What is jaundice?,
while also accounting for cultural variation in subjective responses. We
propose MIND, a user-in-the-loop fact-checking pipeline to detect factual and
cultural discrepancies in multilingual QA knowledge bases. MIND highlights
divergent answers to culturally sensitive questions (e.g., Who assists in
childbirth?) that vary by region and context. We evaluate MIND on a bilingual
QA system in the maternal and infant health domain and release a dataset of
bilingual questions annotated for factual and cultural inconsistencies. We
further test MIND on datasets from other domains to assess generalization. In
all cases, MIND reliably identifies inconsistencies, supporting the development
of more culturally aware and factually consistent QA systems.

</details>


### [6] [TopoAlign: A Framework for Aligning Code to Math via Topological Decomposition](https://arxiv.org/abs/2510.11944)
*Yupei Li,Philipp Borchert,Gerasimos Lampouras*

Main category: cs.CL

TL;DR: TopoAlign框架利用代码库作为数学大语言模型的训练资源，通过将代码分解为文档字符串、主函数和依赖函数，并重新组装成结构上类似形式数学语句的代码数据，从而提升数学自动形式化任务的性能。


<details>
  <summary>Details</summary>
Motivation: 当前数学大语言模型在自动形式化任务上表现受限，主要原因是缺乏大规模的非正式与形式数学语句配对语料库。虽然现有模型能够从自然语言生成代码，但代码与形式数学之间的结构和语法差异限制了有效的迁移学习。

Method: 提出TopoAlign框架，将代码分解为文档字符串、主函数和依赖函数，然后重新组装这些组件，使其在结构上镜像形式数学语句，从而生成结构对齐的代码数据用于训练数学大语言模型，无需额外的人工标注。

Result: 在DeepSeek-Math模型上，TopoAlign显著提升了性能：BEq@10指标提高17.77%，typecheck@10指标提高68.82%。对于专门模型Herald，尽管没有引入新的数学知识，BEq@10和typecheck@10也分别提升了0.12%和1.09%。

Conclusion: TopoAlign框架证明，通过结构对齐的代码数据进行训练，即使对于专门的数学模型也是有益的，这为利用广泛可用的代码库资源来提升数学大语言模型的自动形式化能力提供了有效途径。

Abstract: Large Language Models (LLMs) excel at both informal and formal (e.g. Lean 4)
mathematical reasoning but still struggle with autoformalisation, the task of
transforming informal into formal mathematical statements. Autoformalisation
helps pair the informal reasoning of LLMs with formal proof assistants which
enable machine-verifiable generation and mitigate hallucinations. Yet, the
performance of current Math LLMs is constrained by the scarcity of large-scale
corpora, particularly those containing pairs of informal and formal statements.
Although current models are trained to generate code from natural language
instructions, structural and syntactic differences between these and formal
mathematics limit effective transfer learning. We propose TopoAlign, a
framework that unlocks widely available code repositories as training resources
for Math LLMs. TopoAlign decomposes code into docstrings, main functions, and
dependency functions, and reassembles these components into analogues that
structurally mirror formal statements. This produces structurally aligned code
data that can be used for training Math LLMs without requiring additional human
annotation. We train two state-of-the-art models, DeepSeek-Math and Herald, and
evaluate them on the minif2f, Putnam, and ProofNet benchmarks. TopoAlign
provides substantial gains for DeepSeek-Math, improving performance by 17.77%
on BEq@10 and 68.82% on typecheck@10. Despite introducing no new mathematical
knowledge, our framework achieves gains of 0.12% and 1.09% for Herald on BEq@10
and typecheck@10, respectively, demonstrating that training on aligned code
data is beneficial even for specialized models.

</details>


### [7] [GRAVITY: A Framework for Personalized Text Generation via Profile-Grounded Synthetic Preferences](https://arxiv.org/abs/2510.11952)
*Priyanka Dey,Daniele Rosa,Wenqing Zheng,Daniel Barcklow,Jieyu Zhao,Emilio Ferrara*

Main category: cs.CL

TL;DR: GRAVITY框架通过生成基于用户画像的合成偏好数据来改进LLM个性化，减少对人类标注的依赖，在跨文化场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统LLM个性化方法依赖昂贵的人类反馈或交互日志，难以扩展且忽略深层用户属性，需要更可扩展的个性化解决方案。

Method: 整合人口统计、文化和心理学框架（霍夫斯泰德文化维度、施瓦茨基本价值观、世界价值观调查、大五人格），生成基于用户画像的合成偏好数据对。

Result: 在400名亚马逊用户的书籍描述生成任务中，GRAVITY相比基线方法获得超过4%的偏好增益，用户研究显示86%的情况下更受偏好。

Conclusion: 基于场景的合成数据能捕捉更丰富的用户差异，减少对昂贵标注的依赖，生成更具吸引力的用户中心内容，为LLM个性化提供可扩展路径。

Abstract: Personalization in LLMs often relies on costly human feedback or interaction
logs, limiting scalability and neglecting deeper user attributes. To reduce the
reliance on human annotations, we introduce GRAVITY (Generative Response with
Aligned Values, Interests, and Traits of You), a framework for generating
synthetic, profile-grounded preference data that captures users' interests,
values, beliefs, and personality traits. By integrating demographic, cultural,
and psychological frameworks -- including Hofstede's cultural dimensions,
Schwartz's basic values, the World Values Survey, and Big Five OCEAN traits --
GRAVITY synthesizes preference pairs to guide personalized content generation.
We evaluate GRAVITY on book descriptions for 400 Amazon users, comparing it to
prompt-based conditioning, standard fine-tuning, and naive synthetic pair
generation. Profile-grounded synthetic data consistently improves generation,
especially across multiple cultures (USA, Brazil, Japan, India), achieving over
4% higher preference gains across baselines, with user studies showing that
GRAVITY outputs are preferred over 86% of the time. Our results show that
scenario-grounded synthetic data can capture richer user variation, reduce
reliance on costly annotation, and produce more engaging, user-centered
content, offering a scalable path for LLM personalization.

</details>


### [8] [Evaluating Retrieval-Augmented Generation Systems on Unanswerable, Uncheatable, Realistic, Multi-hop Queries](https://arxiv.org/abs/2510.11956)
*Gabrielle Kaili-May Liu,Bryan Li,Arman Cohan,William Gantt Walden,Eugene Yang*

Main category: cs.CL

TL;DR: 提出了CRUMQs管道，用于自动生成不可作弊、真实、不可回答和多跳的问题，以解决现有RAG基准测试在复杂查询评估上的不足。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的RAG系统经常面临复杂查询，但现有基准测试很少反映真实的任务复杂性，多跳或超出范围的问题往往可以通过断开推理来作弊，或者只需要简单的事实回忆，这限制了发现现有RAG系统局限性的能力。

Method: 开发了第一个自动、难度可控的管道，用于创建不可作弊、真实、不可回答和多跳的问题（CRUMQs），该管道可适应任何语料库和领域。

Result: 在两个流行的RAG数据集上创建了CRUMQs，基准实验表明，与先前的RAG基准相比，CRUMQs对RAG系统极具挑战性，作弊分数降低了81.0%。

Conclusion: 该管道提供了一种简单的方法来增强基准测试的难度和真实性，推动开发更强大的RAG系统。

Abstract: Real-world use cases often present RAG systems with complex queries for which
relevant information is missing from the corpus or is incomplete. In these
settings, RAG systems must be able to reject unanswerable, out-of-scope queries
and identify failures of retrieval and multi-hop reasoning. Despite this,
existing RAG benchmarks rarely reflect realistic task complexity for multi-hop
or out-of-scope questions, which often can be cheated via disconnected
reasoning (i.e., solved without genuine multi-hop inference) or require only
simple factual recall. This limits the ability for such benchmarks to uncover
limitations of existing RAG systems. To address this gap, we present the first
pipeline for automatic, difficulty-controlled creation of
un$\underline{c}$heatable, $\underline{r}$ealistic, $\underline{u}$nanswerable,
and $\underline{m}$ulti-hop $\underline{q}$uerie$\underline{s}$ (CRUMQs),
adaptable to any corpus and domain. We use our pipeline to create CRUMQs over
two popular RAG datasets and demonstrate its effectiveness via benchmark
experiments on leading retrieval-augmented LLMs. Results show that compared to
prior RAG benchmarks, CRUMQs are highly challenging for RAG systems and achieve
up to 81.0\% reduction in cheatability scores. More broadly, our pipeline
offers a simple way to enhance benchmark difficulty and realism and drive
development of more capable RAG systems.

</details>


### [9] [Direct Multi-Token Decoding](https://arxiv.org/abs/2510.11958)
*Xuan Luo,Weizhi Wang,Xifeng Yan*

Main category: cs.CL

TL;DR: 提出Direct Multi-Token Decoding (DMTD)方法，通过仅使用LLM的后期层同时生成多个token，实现推理加速，无需额外参数或验证步骤。


<details>
  <summary>Details</summary>
Motivation: 基于预训练LLM中不同层次的分工特性：早期层处理输入理解，中间层处理任务特定处理，后期层将抽象表示转换为输出token。假设经过前中期层处理后，隐藏状态已包含足够信息支持仅用后期层生成多个token。

Method: DMTD方法：仅使用LLM的后期层同时解码多个token，避免重复遍历早期和中间层。无需额外参数、辅助例程或后生成验证。

Result: 在有限数据集上微调的DMTD Qwen3-4B模型实现了最高2倍的加速，仅有轻微性能损失。扩展分析表明使用更大训练数据集可进一步提升性能。

Conclusion: DMTD是一种有效的推理加速方法，通过利用LLM层次分工特性实现多token并行生成，在保持性能的同时显著提升推理速度。

Abstract: Decoder-only transformers have become the standard architecture for large
language models (LLMs) due to their strong performance. Recent studies suggest
that, in pre-trained LLMs, early, middle, and late layers may serve distinct
roles: Early layers focus on understanding the input context, middle layers
handle task-specific processing, and late layers convert abstract
representations into output tokens. We hypothesize that once representations
have been processed by the early and middle layers, the resulting hidden states
may encapsulate sufficient information to support the generation of multiple
tokens using only the late layers, eliminating the need to repeatedly traverse
the early and middle layers. We refer to this inference paradigm as Direct
Multi-Token Decoding (DMTD). Unlike speculative decoding, our method introduces
no additional parameters, auxiliary routines, or post-generation verification.
Despite being trained on a limited dataset, a fine-tuned DMTD Qwen3-4B model
has already demonstrated promising results, achieving up to a 2x speedup with
only minor performance loss. Moreover, as shown in our scaling analysis, its
performance is expected to further improve with larger training datasets.

</details>


### [10] [Scaling Long-Horizon LLM Agent via Context-Folding](https://arxiv.org/abs/2510.11967)
*Weiwei Sun,Miao Lu,Zhan Ling,Kang Liu,Xuesong Yao,Yiming Yang,Jiecao Chen*

Main category: cs.CL

TL;DR: 提出Context-Folding框架，让LLM智能体能够主动管理工作上下文，通过分支处理子任务并在完成后折叠中间步骤，从而在长视野任务中大幅减少上下文使用。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型智能体在长视野任务中受限于上下文长度，需要更有效的上下文管理方法。

Method: 开发了端到端强化学习框架FoldGRPO，通过特定过程奖励来鼓励有效的任务分解和上下文管理，智能体可以程序性地分支处理子任务并在完成后折叠中间步骤。

Result: 在复杂长视野任务（Deep Research和SWE）上，折叠智能体匹配或超越ReAct基线，同时使用的活动上下文减少10倍，并显著优于基于摘要的上下文管理模型。

Conclusion: Context-Folding框架有效解决了LLM智能体在长视野任务中的上下文限制问题，通过主动的上下文管理实现了更好的性能。

Abstract: Large language model (LLM) agents are fundamentally constrained by context
length on long-horizon tasks. We introduce Context-Folding, a framework that
empowers agents to actively manage their working context. An agent can
procedurally branch into a sub-trajectory to handle a subtask and then fold it
upon completion, collapsing the intermediate steps while retaining a concise
summary of the outcome. To make this behavior learnable, we develop an
end-to-end reinforcement learning framework FoldGRPO with specific process
rewards to encourage effective task decomposition and context management. On
complex long-horizon tasks (Deep Research and SWE), our folding agent matches
or outperforms the ReAct baselines while using an active context 10$\times$
smaller and significantly outperforms models that rely on summarization-based
context management.

</details>


### [11] [Conjecturing: An Overlooked Step in Formal Mathematical Reasoning](https://arxiv.org/abs/2510.11986)
*Jasivan Alex Sivakumar,Philipp Borchert,Ronald Cardenas,Gerasimos Lampouras*

Main category: cs.CL

TL;DR: 该论文指出自动形式化任务中的猜想步骤被忽视，创建了ConjectureBench数据集和评估框架来衡量LLMs的猜想能力，并提出Lean-FIRe方法显著提升了端到端自动形式化性能。


<details>
  <summary>Details</summary>
Motivation: 现有自动形式化研究忽视了关键的猜想步骤，许多数学问题需要先猜想结论才能进行形式化。LLMs在自动形式化方面表现不佳，且猜想能力的评估有限且与形式化或证明任务混淆。

Method: 扩充现有数据集创建ConjectureBench，重新设计评估框架和指标来衡量LLMs的猜想能力；提出推理时方法Lean-FIRe来改进猜想和自动形式化。

Result: 发现当考虑猜想步骤时，GPT-4.1和DeepSeek-V3.1等基础模型的自动形式化性能被显著高估；使用Lean-FIRe方法，GPT-4.1成功端到端形式化了13个PutnamBench问题，DeepSeek-V3.1形式化了7个。

Conclusion: LLMs具备生成准确猜想所需的知识，但提升自动形式化性能需要将猜想作为独立任务处理，并研究如何正确将其整合到自动形式化流程中。

Abstract: Autoformalisation, the task of expressing informal mathematical statements in
formal language, is often viewed as a direct translation process. This,
however, disregards a critical preceding step: conjecturing. Many mathematical
problems cannot be formalised directly without first conjecturing a conclusion
such as an explicit answer, or a specific bound. Since Large Language Models
(LLMs) already struggle with autoformalisation, and the evaluation of their
conjecturing ability is limited and often entangled within autoformalisation or
proof, it is particularly challenging to understand its effect. To address this
gap, we augment existing datasets to create ConjectureBench, and redesign the
evaluation framework and metric specifically to measure the conjecturing
capabilities of LLMs both as a distinct task and within the autoformalisation
pipeline. Our evaluation of foundational models, including GPT-4.1 and
DeepSeek-V3.1, reveals that their autoformalisation performance is
substantially overestimated when the conjecture is accounted for during
evaluation. However, the conjecture should not be assumed to be provided. We
design an inference-time method, Lean-FIRe to improve conjecturing and
autoformalisation, which, to the best of our knowledge, achieves the first
successful end-to-end autoformalisation of 13 PutnamBench problems with GPT-4.1
and 7 with DeepSeek-V3.1. We demonstrate that while LLMs possess the requisite
knowledge to generate accurate conjectures, improving autoformalisation
performance requires treating conjecturing as an independent task, and
investigating further how to correctly integrate it within autoformalisation.
Finally, we provide forward-looking guidance to steer future research toward
improving conjecturing, an overlooked step of formal mathematical reasoning.

</details>


### [12] [SAGE: A Top-Down Bottom-Up Knowledge-Grounded User Simulator for Multi-turn AGent Evaluation](https://arxiv.org/abs/2510.11997)
*Ryan Shea,Yunan Lu,Liang Qiu,Zhou Yu*

Main category: cs.CL

TL;DR: 提出了SAGE框架，这是一个用于多轮交互式智能体评估的用户模拟框架，通过整合业务背景知识来生成更真实和多样化的交互，从而更有效地发现智能体错误。


<details>
  <summary>Details</summary>
Motivation: 现有的多轮交互式智能体评估方法通常依赖人工评估，而使用模拟用户的方法往往建模通用用户，忽视了领域特定原则，无法捕捉真实用户行为。

Method: SAGE框架整合了自上而下的业务逻辑知识（如理想客户画像）和自下而上的业务代理基础设施知识（如产品目录、FAQ、知识库），将用户行为基于真实的客户角色。

Result: 实证评估表明，该方法生成的交互更加真实和多样化，同时能够识别出多达33%的智能体错误。

Conclusion: SAGE框架作为一种评估工具，在支持错误发现和迭代式智能体改进方面具有显著效果。

Abstract: Evaluating multi-turn interactive agents is challenging due to the need for
human assessment. Evaluation with simulated users has been introduced as an
alternative, however existing approaches typically model generic users and
overlook the domain-specific principles required to capture realistic behavior.
We propose SAGE, a novel user Simulation framework for multi-turn AGent
Evaluation that integrates knowledge from business contexts. SAGE incorporates
top-down knowledge rooted in business logic, such as ideal customer profiles,
grounding user behavior in realistic customer personas. We further integrate
bottom-up knowledge taken from business agent infrastructure (e.g., product
catalogs, FAQs, and knowledge bases), allowing the simulator to generate
interactions that reflect users' information needs and expectations in a
company's target market. Through empirical evaluation, we find that this
approach produces interactions that are more realistic and diverse, while also
identifying up to 33% more agent errors, highlighting its effectiveness as an
evaluation tool to support bug-finding and iterative agent improvement.

</details>


### [13] [Generate Logical Equivalence Questions](https://arxiv.org/abs/2510.12001)
*Xinyu Wang,Haoming Yu,Yicheng Yang,Zhiyuan Li*

Main category: cs.CL

TL;DR: 提出了一种新的自动问题生成方法，用于离散数学中的逻辑等价问题，通过形式化语言和线性时间算法生成质量与教科书相当的问题。


<details>
  <summary>Details</summary>
Motivation: 解决在线教学时代抄袭问题日益严重的问题，通过为每个学生生成独特问题来减少抄袭行为，同时提供大量练习题。

Method: 使用形式化语言定义逻辑等价问题，将其转化为两组生成规则，并开发线性时间算法进行问题生成。

Result: 学生测试显示生成问题的准确性与教科书问题相当；问题解决步骤分析表明难度与教科书问题相似，质量得到验证。

Conclusion: 提出的自动问题生成系统能够有效生成高质量的逻辑等价问题，在准确性和难度方面与教科书问题相当，为解决学术不端和提供练习资源提供了可行方案。

Abstract: Academic dishonesty is met with zero tolerance in higher education, yet
plagiarism has become increasingly prevalent in the era of online teaching and
learning. Automatic Question Generation (AQG) presents a potential solution to
mitigate copying by creating unique questions for each student. Additionally,
AQG can provide a vast array of practice questions. Our AQG focuses on
generating logical equivalence questions for Discrete Mathematics, a
foundational course for first-year computer science students. A literature
review reveals that existing AQGs for this type of question generate all
propositions that meet user-defined constraints, resulting in inefficiencies
and a lack of uniform question difficulty. To address this, we propose a new
approach that defines logical equivalence questions using a formal language,
translates this language into two sets of generation rules, and develops a
linear-time algorithm for question generation. We evaluated our AQG through two
experiments. The first involved a group of students completing questions
generated by our system. Statistical analysis shows that the accuracy of these
questions is comparable to that of textbook questions. The second experiment
assessed the number of steps required to solve our generated questions,
textbook questions, and those generated by multiple large language models. The
results indicated that the difficulty of our questions was similar to that of
textbook questions, confirming the quality of our AQG.

</details>


### [14] [Information Extraction from Conversation Transcripts: Neuro-Symbolic vs. LLM](https://arxiv.org/abs/2510.12023)
*Alice Saebom Kwak,Maria Alexeeva,Gus Hahn-Powell,Keith Alcock,Kevin McLaughlin,Doug McCorkle,Gabe McNunn,Mihai Surdeanu*

Main category: cs.CL

TL;DR: 比较神经符号系统和LLM系统在农业领域信息提取的表现，LLM系统在F1分数上表现更好（69.4 vs 52.7），但两种方法各有优缺点。


<details>
  <summary>Details</summary>
Motivation: 当前信息提取领域过度依赖大语言模型，忽视了传统符号或统计系统的经验积累，需要对比不同方法在实际应用中的表现。

Method: 在农业领域（猪肉、乳制品、作物）对9个访谈进行测试，比较神经符号系统和LLM系统在信息提取任务中的表现。

Result: LLM系统在总体F1分数（69.4 vs 52.7）和核心信息F1分数（63.0 vs 47.2）上均优于神经符号系统。

Conclusion: 两种系统各有优劣：神经符号系统运行更快、控制性更强，但缺乏泛化能力；LLM系统性能更高但运行较慢、存在幻觉风险。实际部署需要考虑性能、效率和控制的平衡。

Abstract: The current trend in information extraction (IE) is to rely extensively on
large language models, effectively discarding decades of experience in building
symbolic or statistical IE systems. This paper compares a neuro-symbolic (NS)
and an LLM-based IE system in the agricultural domain, evaluating them on nine
interviews across pork, dairy, and crop subdomains. The LLM-based system
outperforms the NS one (F1 total: 69.4 vs. 52.7; core: 63.0 vs. 47.2), where
total includes all extracted information and core focuses on essential details.
However, each system has trade-offs: the NS approach offers faster runtime,
greater control, and high accuracy in context-free tasks but lacks
generalizability, struggles with contextual nuances, and requires significant
resources to develop and maintain. The LLM-based system achieves higher
performance, faster deployment, and easier maintenance but has slower runtime,
limited control, model dependency and hallucination risks. Our findings
highlight the "hidden cost" of deploying NLP systems in real-world
applications, emphasizing the need to balance performance, efficiency, and
control.

</details>


### [15] [CPR: Mitigating Large Language Model Hallucinations with Curative Prompt Refinement](https://arxiv.org/abs/2510.12029)
*Jung-Woo Shim,Yeong-Joon Ju,Ji-Hoon Park,Seong-Whan Lee*

Main category: cs.CL

TL;DR: 提出了CPR框架，通过清理格式错误的提示词和生成额外任务描述来减少LLM幻觉，显著提高生成质量


<details>
  <summary>Details</summary>
Motivation: LLM经常生成看似合理但错误的"幻觉"事实，主要原因是用户使用结构不良或模糊的提示词，导致模型基于假设而非实际意图生成响应

Method: 开发了Curative Prompt Refinement (CPR)框架，使用微调的小型语言模型来清理格式错误的提示词并生成额外的信息性任务描述，以对齐用户意图和提示词

Result: 应用CPR后显著提高了生成质量并减少了幻觉，在没有任何外部知识的情况下，使用CPR的提示词相比原始提示词获得了超过90%的胜率

Conclusion: CPR是一个即插即用的提示词优化框架，能有效缓解由格式不良提示词引起的LLM幻觉问题

Abstract: Recent advancements in large language models (LLMs) highlight their fluency
in generating responses to diverse prompts. However, these models sometimes
generate plausible yet incorrect ``hallucinated" facts, undermining trust. A
frequent but often overlooked cause of such errors is the use of poorly
structured or vague prompts by users, leading LLMs to base responses on assumed
rather than actual intentions. To mitigate hallucinations induced by these
ill-formed prompts, we introduce Curative Prompt Refinement (CPR), a
plug-and-play framework for curative prompt refinement that 1) cleans
ill-formed prompts, and 2) generates additional informative task descriptions
to align the intention of the user and the prompt using a fine-tuned small
language model. When applied to language models, we discover that CPR
significantly increases the quality of generation while also mitigating
hallucination. Empirical studies show that prompts with CPR applied achieves
over a 90\% win rate over the original prompts without any external knowledge.

</details>


### [16] [Multi-stage Prompt Refinement for Mitigating Hallucinations in Large Language Models](https://arxiv.org/abs/2510.12032)
*Jung-Woo Shim,Yeong-Joon Ju,Ji-Hoon Park,Seong-Whan Lee*

Main category: cs.CL

TL;DR: 提出了多阶段提示优化框架MPR，通过系统化改进格式错误的提示来减少大语言模型的幻觉问题


<details>
  <summary>Details</summary>
Motivation: 大语言模型在幻觉问题上仍面临挑战，而格式错误的提示（如歧义措辞、语法错误、信息不完整）对幻觉的影响尚未充分研究

Method: MPR框架通过多个阶段系统改进提示，每个阶段针对特定错误（如标点、拼写错误、关键词误用），使用微调的小型语言模型，并通过自反思机制和排序来优先处理最相关的输入

Result: 在幻觉基准测试中，经MPR优化的提示相比原始形式获得了超过85%的胜率，有效减少了幻觉并提高了LLM输出准确性

Conclusion: MPR提供了一个轻量级且适应性强的解决方案，可与现有的事后幻觉缓解框架结合，增强LLM在各种领域的可靠性

Abstract: Recent advancements in large language models (LLMs) have shown strong
performance in natural language understanding and generation tasks. However,
LLMs continue to encounter challenges with hallucinations, where models
generate plausible but incorrect information. While several factors contribute
to hallucinations, the impact of ill-formed prompts, prompts with ambiguous
wording, incorrect grammar, or incomplete information, was relatively under
explored. To address this, we introduce Multi-stage Prompt Refinement (MPR), a
framework designed to systematically improve these ill-formed prompts across
multiple stages. Each stage addresses specific errors such as punctuation,
typographical mistakes, and misuse of key terms, using small language models
(SLMs) fine-tuned for these tasks. MPR iteratively enhances the clarity of
prompts with additional context and employs a self-reflection mechanism with
ranking to prioritize the most relevant input. Experimental results on
hallucination benchmarks show that prompts refined by MPR achieve over an 85~\%
win rate compared to their original forms, demonstrating its effectiveness in
reducing hallucinations and improving LLM output accuracy. Interestingly, we
reveal that MPR can be combined with existing post-hoc hallucination mitigation
frameworks, further enhancing its versatility. MPR provides a lightweight and
adaptable solution for enhancing LLM reliability across various domains.

</details>


### [17] [On the Interplay between Human Label Variation and Model Fairness](https://arxiv.org/abs/2510.12036)
*Kemal Kurniawan,Meladel Mistica,Timothy Baldwin,Jey Han Lau*

Main category: cs.CL

TL;DR: 本文探讨了人类标签变异对模型公平性的影响，发现HLV训练方法在没有显式去偏的情况下对公平性有积极影响


<details>
  <summary>Details</summary>
Motivation: 研究人类标签变异对模型公平性的影响，这是一个尚未探索的领域

Method: 通过比较在多数投票标签上训练与一系列HLV方法训练的效果

Result: 实验表明，在没有显式去偏的情况下，HLV训练方法对公平性有积极影响

Conclusion: HLV训练方法可以在不进行显式去偏的情况下改善模型公平性

Abstract: The impact of human label variation (HLV) on model fairness is an unexplored
topic. This paper examines the interplay by comparing training on majority-vote
labels with a range of HLV methods. Our experiments show that without explicit
debiasing, HLV training methods have a positive impact on fairness.

</details>


### [18] [Uncertainty Quantification for Hallucination Detection in Large Language Models: Foundations, Methodology, and Future Directions](https://arxiv.org/abs/2510.12040)
*Sungmin Kang,Yavuz Faruk Bakman,Duygu Nur Yaldiz,Baturalp Buyukates,Salman Avestimehr*

Main category: cs.CL

TL;DR: 本文综述了大语言模型中的不确定性量化方法及其在幻觉检测中的应用，系统分类现有方法并分析当前局限性和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在实际应用中的部署，其产生的幻觉问题（生成看似合理但事实错误的输出）引发了可靠性担忧，需要不确定性量化方法来评估模型输出的可信度。

Method: 从不确定性量化的基础定义出发，区分认知不确定性和偶然不确定性，将这些概念适配到LLM语境中，系统分类现有方法并展示代表性方法的实证结果。

Result: 不确定性量化为识别不可靠生成提供了机制，能够改善LLM的可靠性，通过系统分类展示了多种方法的有效性。

Conclusion: 不确定性量化是解决LLM幻觉问题的关键研究方向，当前方法已取得进展但仍存在局限，需要进一步研究来完善LLM的可信度评估体系。

Abstract: The rapid advancement of large language models (LLMs) has transformed the
landscape of natural language processing, enabling breakthroughs across a wide
range of areas including question answering, machine translation, and text
summarization. Yet, their deployment in real-world applications has raised
concerns over reliability and trustworthiness, as LLMs remain prone to
hallucinations that produce plausible but factually incorrect outputs.
Uncertainty quantification (UQ) has emerged as a central research direction to
address this issue, offering principled measures for assessing the
trustworthiness of model generations. We begin by introducing the foundations
of UQ, from its formal definition to the traditional distinction between
epistemic and aleatoric uncertainty, and then highlight how these concepts have
been adapted to the context of LLMs. Building on this, we examine the role of
UQ in hallucination detection, where quantifying uncertainty provides a
mechanism for identifying unreliable generations and improving reliability. We
systematically categorize a wide spectrum of existing methods along multiple
dimensions and present empirical results for several representative approaches.
Finally, we discuss current limitations and outline promising future research
directions, providing a clearer picture of the current landscape of LLM UQ for
hallucination detection.

</details>


### [19] [Improving Text-to-Image Generation with Input-Side Inference-Time Scaling](https://arxiv.org/abs/2510.12041)
*Ruibo Chen,Jiacheng Pan,Heng Huang,Zhenheng Yang*

Main category: cs.CL

TL;DR: 提出基于大语言模型的提示词重写框架，通过奖励系统和迭代DPO训练优化文本到图像生成中的提示词，无需监督微调数据即可提升图像质量和对齐度。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像生成模型在处理简单或描述不充分的提示词时，往往导致图像-文本对齐度、美学质量和整体效果不佳。

Method: 使用大语言模型作为提示词重写器，引入精心设计的奖励系统和迭代直接偏好优化训练流程，在无需监督微调数据的情况下优化用户输入。

Result: 在多种T2I模型和基准测试中，提示词重写器持续改进图像-文本对齐度、视觉质量和美学效果，优于强基线方法，并展现出良好的跨模型迁移性。

Conclusion: 提示词重写是一种有效、可扩展且实用的模型无关策略，能够显著提升文本到图像生成系统的性能。

Abstract: Recent advances in text-to-image (T2I) generation have achieved impressive
results, yet existing models often struggle with simple or underspecified
prompts, leading to suboptimal image-text alignment, aesthetics, and quality.
We propose a prompt rewriting framework that leverages large language models
(LLMs) to refine user inputs before feeding them into T2I backbones. Our
approach introduces a carefully designed reward system and an iterative direct
preference optimization (DPO) training pipeline, enabling the rewriter to
enhance prompts without requiring supervised fine-tuning data. We evaluate our
method across diverse T2I models and benchmarks. Results show that our prompt
rewriter consistently improves image-text alignment, visual quality, and
aesthetics, outperforming strong baselines. Furthermore, we demonstrate strong
transferability by showing that a prompt rewriter trained on one T2I backbone
generalizes effectively to others without needing to be retrained. We also
systematically study scalability, evaluating how performance gains scale with
the capacity of the large LLM used as the rewriter. These findings highlight
that prompt rewriting is an effective, scalable, and practical model-agnostic
strategy for improving T2I systems. We plan to release the code and trained
prompt rewriters soon.

</details>


### [20] [Hierarchical Alignment: Surgical Fine-Tuning via Functional Layer Specialization in Large Language Models](https://arxiv.org/abs/2510.12044)
*Yukun Zhang,Qi Dong*

Main category: cs.CL

TL;DR: 提出分层对齐方法，针对Transformer架构中不同功能层进行定向优化，避免传统DPO的"对齐税"问题，在保持语法流畅性的同时显著提升逻辑推理和事实一致性。


<details>
  <summary>Details</summary>
Motivation: 传统对齐技术将模型视为单一实体进行统一优化，忽视了Transformer架构中不同层的功能专门化（从语法处理到抽象推理），导致优化效果受限。

Method: 引入分层对齐方法，将模型层划分为局部（语法）、中间（逻辑）和全局（事实性）三个功能块，使用LoRA进行精准微调，对每个功能块分别应用DPO优化。

Result: 局部层对齐提升语法流畅性，全局层对齐不仅改善事实一致性，还成为提升逻辑连贯性的最有效策略，所有分层策略都成功避免了标准DPO中的"对齐税"问题。

Conclusion: 分层对齐为模型对齐提供了更资源高效、可控和可解释的路径，展示了从整体优化转向结构感知精准微调的巨大潜力，有助于构建更先进可靠的LLM。

Abstract: Existing alignment techniques for Large Language Models (LLMs), such as
Direct Preference Optimization (DPO), typically treat the model as a monolithic
entity, applying uniform optimization pressure across all layers. This approach
overlooks the functional specialization within the Transformer architecture,
where different layers are known to handle distinct tasks from syntax to
abstract reasoning. In this paper, we challenge this one-size-fits-all paradigm
by introducing Hierarchical Alignment, a novel method that applies targeted DPO
to distinct functional blocks of a model's layers: local (syntax), intermediate
(logic), and global (factuality). Through a series of controlled experiments on
state-of-the-art models like Llama-3.1-8B and Qwen1.5-7B using LoRA for
surgical fine-tuning, our results, evaluated by a powerful LLM-as-Judge,
demonstrate significant and predictable improvements. Specifically, aligning
the local layers (Local-Align) enhances grammatical fluency. More importantly,
aligning the global layers (Global-Align) not only improves factual consistency
as hypothesized but also proves to be the most effective strategy for enhancing
logical coherence, outperforming all baselines. Critically, all hierarchical
strategies successfully avoid the "alignment tax" observed in standard DPO,
where gains in fluency come at the cost of degraded logical reasoning. These
findings establish a more resource-efficient, controllable, and interpretable
path for model alignment, highlighting the immense potential of shifting from
monolithic optimization to structure-aware surgical fine-tuning to build more
advanced and reliable LLMs.

</details>


### [21] [APCE: Adaptive Progressive Context Expansion for Long Context Processing](https://arxiv.org/abs/2510.12051)
*Baisub Lee,Sanghyun Byun,Mohanad Odema,Jung Guack,Jacob Song,Woo Seong Chung*

Main category: cs.CL

TL;DR: 提出APCE方法，通过语义相似度匹配选择重要输入块，在长上下文摘要任务中减少50%-70%输入序列，同时提升KV缓存和自注意力内存效率


<details>
  <summary>Details</summary>
Motivation: 解决长上下文Transformer模型的两个关键挑战：内存占用随序列长度二次增长，以及ContextRot现象导致的性能下降

Method: APCE方法通过低维语义相似度匹配选择重要输入块，直接操作输入数据，不依赖特定硬件或CUDA环境

Result: APCE在仅使用50%-70%输入序列的情况下，实现了与完整密集基线相当或更优的摘要性能，显著提升了KV缓存和自注意力内存效率

Conclusion: APCE为长上下文Transformer模型提供了一种上下文感知的效率解决方案，有望推广到其他相关长上下文任务

Abstract: Deploying useful Long-Context Transformer Models (LCTMs) requires addressing
two key challenges: (1) A growing memory footprint due to quadratic
self-attention and linear KV-cache scaling in memory as sequence length
increases; (2) the ContextRot phenomena where empirical evidence suggests that
transformer architecture's performance degrades with increasing context length.
Given the shared dependency on the input, a natural question arises: Can we
surgically select the most important input chunks for processing to
synergistically (a) reduce the memory footprint, and (b) mitigate the
ContextRot effects? In this paper, we answer this question in the affirmative
for long-context summarization tasks. We propose APCE as a context-aware
solution to select the most important input chunks through low-dimensional
semantic similarity matching with the current query. By directly operating on
the input, APCE decouples from strict dependency on underlying hardware or CUDA
environments, promising a compatible solution scalable to different deployment
systems. Our empirical evaluations have demonstrated superior or on-par
summarization performance for APCE compared to the full dense baseline using a
fraction (50%-70%) of the input sequence resulting in KV-cache and
self-attention memory efficiency improvements. We hope our findings inspire
further research on context-aware efficiency solutions for LCTMs geared towards
other relevant long-context tasks.

</details>


### [22] [An AI-Based Behavioral Health Safety Filter and Dataset for Identifying Mental Health Crises in Text-Based Conversations](https://arxiv.org/abs/2510.12083)
*Benjamin W. Nelson,Celeste Wong,Matthew T. Silvestrini,Sooyoon Shin,Alanna Robinson,Jessica Lee,Eric Yang,John Torous,Andrew Trister*

Main category: cs.CL

TL;DR: 该研究评估了Verily行为健康安全过滤器在精神健康危机检测中的表现，结果显示其在两个数据集上都表现出高敏感性和特异性，优于NVIDIA NeMo和OpenAI Omni Moderation Latest等开源内容审核护栏。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在处理精神健康紧急情况时经常提供有害或不适当的建议，需要开发专门的安全过滤器来防止破坏性行为。

Method: 使用两个临床标注的数据集（Verily精神健康危机数据集和NVIDIA Aegis AI内容安全数据集子集），评估VBHSF的性能，并与NVIDIA NeMo和OpenAI Omni Moderation Latest进行对比分析。

Result: VBHSF在Verily数据集上表现出高敏感性(0.990)和特异性(0.992)，F1分数为0.939；在NVIDIA数据集上敏感性为0.982，准确性为0.921。在所有比较中都显著优于其他护栏。

Conclusion: VBHSF表现出稳健、可泛化的性能，优先考虑敏感性以最小化遗漏危机，这对医疗应用至关重要。

Abstract: Large language models often mishandle psychiatric emergencies, offering
harmful or inappropriate advice and enabling destructive behaviors. This study
evaluated the Verily behavioral health safety filter (VBHSF) on two datasets:
the Verily Mental Health Crisis Dataset containing 1,800 simulated messages and
the NVIDIA Aegis AI Content Safety Dataset subsetted to 794 mental
health-related messages. The two datasets were clinician-labelled and we
evaluated performance using the clinician labels. Additionally, we carried out
comparative performance analyses against two open source, content moderation
guardrails: OpenAI Omni Moderation Latest and NVIDIA NeMo Guardrails. The VBHSF
demonstrated, well-balanced performance on the Verily Mental Health Crisis
Dataset v1.0, achieving high sensitivity (0.990) and specificity (0.992) in
detecting any mental health crises. It achieved an F1-score of 0.939,
sensitivity ranged from 0.917-0.992, and specificity was >= 0.978 in
identifying specific crisis categories. When evaluated against the NVIDIA Aegis
AI Content Safety Dataset 2.0, VBHSF performance remained highly sensitive
(0.982) and accuracy (0.921) with reduced specificity (0.859). When compared
with the NVIDIA NeMo and OpenAI Omni Moderation Latest guardrails, the VBHSF
demonstrated superior performance metrics across both datasets, achieving
significantly higher sensitivity in all cases (all p < 0.001) and higher
specificity relative to NVIDIA NeMo (p < 0.001), but not to OpenAI Omni
Moderation Latest (p = 0.094). NVIDIA NeMo and OpenAI Omni Moderation Latest
exhibited inconsistent performance across specific crisis types, with
sensitivity for some categories falling below 0.10. Overall, the VBHSF
demonstrated robust, generalizable performance that prioritizes sensitivity to
minimize missed crises, a crucial feature for healthcare applications.

</details>


### [23] [Deep Associations, High Creativity: A Simple yet Effective Metric for Evaluating Large Language Models](https://arxiv.org/abs/2510.12110)
*Ziliang Qiu,Renfen Hu*

Main category: cs.CL

TL;DR: PACE方法通过让LLM生成并行关联链来评估其创造力，有效避免数据污染问题，与人类评估高度相关，发现高性能LLM达到普通人水平但不及专业人士。


<details>
  <summary>Details</summary>
Motivation: 解决LLM创造力评估中的数据污染和人工评估成本高的问题，借鉴人类创造力评估方法。

Method: 提出PACE方法，让LLM生成并行关联链来评估其创造力，该方法简单高效且能最小化数据污染风险。

Result: PACE与Chatbot Arena Creative Writing排名强相关（Spearman's ρ=0.739），高性能LLM创造力接近普通人但不及专业人士，人类关联模式更多样化。

Conclusion: PACE是有效的LLM创造力评估方法，LLM在关联创造力方面已接近普通人水平，但与专业人士仍有差距，人类关联模式更丰富。

Abstract: The evaluation of LLMs' creativity represents a crucial research domain,
though challenges such as data contamination and costly human assessments often
impede progress. Drawing inspiration from human creativity assessment, we
propose PACE, asking LLMs to generate Parallel Association Chains to Evaluate
their creativity. PACE minimizes the risk of data contamination and offers a
straightforward, highly efficient evaluation, as evidenced by its strong
correlation with Chatbot Arena Creative Writing rankings (Spearman's $\rho =
0.739$, $p < 0.001$) across various proprietary and open-source models. A
comparative analysis of associative creativity between LLMs and humans reveals
that while high-performing LLMs achieve scores comparable to average human
performance, professional humans consistently outperform LLMs. Furthermore,
linguistic analysis reveals that both humans and LLMs exhibit a trend of
decreasing concreteness in their associations, and humans demonstrating a
greater diversity of associative patterns.

</details>


### [24] [Tracing Multilingual Knowledge Acquisition Dynamics in Domain Adaptation: A Case Study of English-Japanese Biomedical Adaptation](https://arxiv.org/abs/2510.12115)
*Xin Zhao,Naoki Yoshinaga,Yuma Tsuta,Akiko Aizawa*

Main category: cs.CL

TL;DR: 该研究提出了AdaXEval评估方法，通过构建与训练语料匹配的多选题数据集，深入分析LLMs在多语言领域适应中的知识获取机制，发现跨语言知识转移仍然具有挑战性。


<details>
  <summary>Details</summary>
Motivation: 现有多语言领域适应方法在知识获取机制方面研究不足，特别是在低资源环境下表现欠佳，需要深入理解LLMs如何在不同语言间学习和转移领域知识。

Method: 提出AdaXEval自适应评估方法，从双语领域语料构建多选题数据集；通过持续训练LLMs并跟踪领域事实获取过程，分析从训练数据到知识的转换机制。

Result: 在13B英日双语LLM上的实验表明，即使使用高质量双语语料，跨语言知识转移仍然存在挑战。

Conclusion: 跨语言知识转移是多语言领域适应中的关键挑战，需要更深入理解LLMs的知识获取机制。

Abstract: Multilingual domain adaptation (ML-DA) is widely used to learn new domain
knowledge across languages into large language models (LLMs). Although many
methods have been proposed to improve domain adaptation, the mechanisms of
multilingual knowledge acquisition, how domain knowledge is learned within a
language and transferred across languages, remain underexplored. This gap leads
to suboptimal performance, particularly in low-resource settings. This work
examines the learning dynamics of LLMs during ML-DA. Because prior ML-DA
studies often train and evaluate on datasets with mismatched knowledge
coverage, we propose AdaXEval, an adaptive evaluation method that builds
multiple-choice QA datasets from the same bilingual domain corpus used for
training, thereby directly studying multilingual knowledge acquisition. Through
continual training of LLMs with diverse data recipes, we track how LLMs acquire
domain facts and pinpoint the mechanism behind the transformation process from
domain training data to knowledge. Our experiments on a 13B English-Japanese
bilingual LLM reveal that cross-lingual transfer remains challenging despite a
high-quality bilingual corpus. The code has been released.

</details>


### [25] [Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models](https://arxiv.org/abs/2510.12116)
*Bajian Xiang,Shuaijiang Zhao,Tingwei Guo,Wei Zou*

Main category: cs.CL

TL;DR: 本文系统分析了端到端大型语音语言模型中的模态差距问题，发现语音和文本输入在深层表示中方向对齐但幅度分离，并提出基于对齐路径评分和关键标记干预的改进方法。


<details>
  <summary>Details</summary>
Motivation: 端到端大型语音语言模型在对话生成方面表现优异，但在语义理解基准测试中始终落后于传统流水线系统，需要深入理解语音和文本输入之间的性能差距。

Method: 通过系统实验分析语音和文本表示的粗粒度和细粒度特征，引入对齐路径评分量化标记级对齐质量，并设计角度投影和长度归一化等干预策略。

Result: 发现语音和文本表示在深层网络中方向相似度增加但欧氏距离扩大，表示相似度与模态差距强相关，提出的干预策略显示出改善语音输入正确性的潜力。

Conclusion: 本研究首次系统分析了LSLMs中的模态差距和对齐机制，为未来优化提供了理论和方法的指导。

Abstract: End-to-end Large Speech Language Models (LSLMs) have demonstrated impressive
conversational generation abilities, yet consistently fall short of traditional
pipeline systems on semantic understanding benchmarks. In this work, we reveal
through systematic experimentation that although LSLMs lose some text input
performance after speech-text alignment training, the performance gap between
speech and text inputs is more pronounced, which we refer to as the modality
gap. To understand this gap, we analyze both coarse- and fine-grained text and
speech representations. At the coarse-grained level, representations of speech
and text in deeper layers are found to be increasingly aligned in direction
(cosine similarity), while concurrently diverging in magnitude (Euclidean
distance). We further find that representation similarity is strongly
correlated with the modality gap. At the fine-grained level, a spontaneous
token-level alignment pattern between text and speech representations is
observed. Based on this, we introduce the Alignment Path Score to quantify
token-level alignment quality, which exhibits stronger correlation with the
modality gap. Building on these insights, we design targeted interventions on
critical tokens through angle projection and length normalization. These
strategies demonstrate the potential to improve correctness for speech inputs.
Our study provides the first systematic empirical analysis of the modality gap
and alignment mechanisms in LSLMs, offering both theoretical and methodological
guidance for future optimization.

</details>


### [26] [SafeMT: Multi-turn Safety for Multimodal Language Models](https://arxiv.org/abs/2510.12133)
*Han Zhu,Juntao Dai,Jiaming Ji,Haoran Li,Chengkun Cai,Pengcheng Wen,Chi-Min Chan,Boyuan Chen,Yaodong Yang,Sirui Han,Yike Guo*

Main category: cs.CL

TL;DR: SafeMT是一个多轮对话安全基准，包含1万个样本，涵盖17种场景和4种越狱方法，用于评估多模态大语言模型在多轮对话中的安全性。研究发现对话轮次增加会提高攻击成功率，并提出了对话安全调节器来检测恶意意图。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大语言模型的广泛应用，安全问题日益突出。多轮对话在日常交互中更为常见，但现有基准未能充分考虑这种情况，存在更大的安全风险。

Method: 构建SafeMT基准，包含不同长度的对话，由有害查询和图像生成。提出安全指数(SI)评估模型对话安全性，并设计对话安全调节器检测对话中的恶意意图。

Result: 评估17个模型发现，有害对话轮次增加会提高攻击成功率，表明模型的安全机制不足以识别对话交互中的危险。实验表明提出的调节器比现有防护模型更能有效降低多轮攻击成功率。

Conclusion: 多模态大语言模型在多轮对话中的安全机制存在不足，需要专门的防护措施。提出的对话安全调节器能有效检测和防范多轮对话中的恶意攻击。

Abstract: With the widespread use of multi-modal Large Language models (MLLMs), safety
issues have become a growing concern. Multi-turn dialogues, which are more
common in everyday interactions, pose a greater risk than single prompts;
however, existing benchmarks do not adequately consider this situation. To
encourage the community to focus on the safety issues of these models in
multi-turn dialogues, we introduce SafeMT, a benchmark that features dialogues
of varying lengths generated from harmful queries accompanied by images. This
benchmark consists of 10,000 samples in total, encompassing 17 different
scenarios and four jailbreak methods. Additionally, we propose Safety Index
(SI) to evaluate the general safety of MLLMs during conversations. We assess
the safety of 17 models using this benchmark and discover that the risk of
successful attacks on these models increases as the number of turns in harmful
dialogues rises. This observation indicates that the safety mechanisms of these
models are inadequate for recognizing the hazard in dialogue interactions. We
propose a dialogue safety moderator capable of detecting malicious intent
concealed within conversations and providing MLLMs with relevant safety
policies. Experimental results from several open-source models indicate that
this moderator is more effective in reducing multi-turn ASR compared to existed
guard models.

</details>


### [27] [Credal Transformer: A Principled Approach for Quantifying and Mitigating Hallucinations in Large Language Models](https://arxiv.org/abs/2510.12137)
*Shihao Ji,Zihui Song,Jiajie Huang*

Main category: cs.CL

TL;DR: 提出Credal Transformer，用基于证据理论的Credal注意力机制替代标准注意力，通过生成credal集合来量化不确定性，显著减少LLM的幻觉和自信错误。


<details>
  <summary>Details</summary>
Motivation: 解决LLM因Transformer的Softmax函数产生"人工确定性"而导致的幻觉问题，该函数将模糊的注意力分数压缩为单一概率分布，丢弃了各层的不确定性信息。

Method: 引入Credal Transformer，用Credal注意力机制(CAM)替代标准注意力。CAM基于证据理论，生成credal集合（分布集合）而非单一注意力向量，集合大小直接衡量模型不确定性。将注意力分数重新概念化为Dirichlet分布的证据质量。

Result: Credal Transformer能够识别分布外输入、量化模糊性，并在无法回答的问题上通过弃权显著减少自信错误。

Conclusion: 提出了一种新的架构来缓解幻觉问题，以及一个将不确定性量化直接集成到模型中的设计范式，为更可靠的AI奠定了基础。

Abstract: Large Language Models (LLMs) hallucinate, generating factually incorrect yet
confident assertions. We argue this stems from the Transformer's Softmax
function, which creates "Artificial Certainty" by collapsing ambiguous
attention scores into a single probability distribution, discarding uncertainty
information at each layer. To fix this, we introduce the Credal Transformer,
which replaces standard attention with a Credal Attention Mechanism (CAM) based
on evidential theory. CAM produces a "credal set" (a set of distributions)
instead of a single attention vector, with the set's size directly measuring
model uncertainty. We implement this by re-conceptualizing attention scores as
evidence masses for a Dirichlet distribution: sufficient evidence recovers
standard attention, while insufficient evidence yields a diffuse distribution,
representing ambiguity. Empirically, the Credal Transformer identifies
out-of-distribution inputs, quantifies ambiguity, and significantly reduces
confident errors on unanswerable questions by abstaining. Our contribution is a
new architecture to mitigate hallucinations and a design paradigm that
integrates uncertainty quantification directly into the model, providing a
foundation for more reliable AI.

</details>


### [28] [A Survey on Parallel Reasoning](https://arxiv.org/abs/2510.12164)
*Ziqi Wang,Boye Niu,Zipeng Gao,Zhi Zheng,Tong Xu,Linghui Meng,Zhongli Li,Jing Liu,Yilong Chen,Chen Zhu,Hua Wu,Haifeng Wang,Enhong Chen*

Main category: cs.CL

TL;DR: 本文综述了并行推理在大型语言模型中的进展，定义了并行推理概念，分类讨论了非交互式推理、交互式推理和效率优化解码策略，并分析了应用场景和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型能力增强，并行推理作为一种新的推理范式出现，能够通过同时探索多条思路来提高推理的鲁棒性，克服传统序列方法的脆弱性。

Method: 提出并行推理的正式定义，基于新分类法组织讨论先进技术，包括非交互式推理、交互式推理和效率优化解码策略。

Result: 系统梳理了并行推理的研究进展，明确了与相关概念的区别，分析了各种应用场景，并识别了核心挑战。

Conclusion: 并行推理是提升LLM推理鲁棒性的重要方向，本文为初学者提供了路线图，鼓励更多研究改进并行推理方法。

Abstract: With the increasing capabilities of Large Language Models (LLMs), parallel
reasoning has emerged as a new inference paradigm that enhances reasoning
robustness by concurrently exploring multiple lines of thought before
converging on a final answer. It has become a significant trend to explore
parallel reasoning to overcome the fragility of standard sequential methods and
improve practical performance. In this paper, we aim to survey and summarize
the progress and challenges of parallel reasoning. We first present a formal
definition of parallel reasoning and clarify its distinction from related
concepts like Chain-of-Thought. Then, we organize and discuss advanced
techniques based on a novel taxonomy, including non-interactive reasoning,
interactive reasoning, and efficiency-focused decoding strategies.
Additionally, we explore various application scenarios, such as solving complex
problems and enhancing the reliability of LLM outputs.Finally, we highlight the
core challenges of parallel reasoning and suggest potential directions for
future research. We hope that our work can provide a useful roadmap for
beginners and encourage more research on improving parallel reasoning methods.
Related source can be avaliable in
https://github.com/PPPP-kaqiu/Awesome-Parallel-Reasoning.

</details>


### [29] [Towards Inference-time Scaling for Continuous Space Reasoning](https://arxiv.org/abs/2510.12167)
*Minghan Wang,Thuy-Trang Vu,Ehsan Shareghi,Gholamreza Haffari*

Main category: cs.CL

TL;DR: 该研究探索了将离散空间中的推理时缩放技术（多样本生成结合过程/结果奖励模型重排序）应用于连续空间推理的可行性，发现虽然能生成多样推理路径，但现有方法在连续思维空间中难以有效区分正确和错误推理。


<details>
  <summary>Details</summary>
Motivation: 研究动机是将离散文本推理中证明有效的推理时缩放技术（多样本生成+PRM/ORM重排序）扩展到连续空间推理，验证这些成熟技术在连续思维空间中的适用性。

Method: 使用COCONUT连续空间推理LM作为骨干，通过dropout-based采样生成多样推理路径，采用Pass@N分析评估性能，并探究几何特性和轨迹动态等来识别连续空间中的挑战。

Result: 研究显示在连续空间中生成多样推理路径是可行的，但离散空间中的数据生成和PRM/ORM训练方法在连续空间中仅带来边际改进，无法有效区分正确和错误推理。

Conclusion: 连续推理LM的训练框架不仅需要优化准确性，还应明确融入可在推理时用于区分正确和错误思维的归纳偏置，这是当前连续思维表示缺乏的关键要素。

Abstract: Inference-time scaling through multiple sample generation in combination with
Process- or Outcome-Reward Model (PRM or ORM) re-ranking has proven effective
for text-based reasoning in large language models. This paper investigates
whether such established techniques can be successfully adapted to reasoning in
the continuous space, using COCONUT (Hao et al. 2024) continuous space
reasoning LM as the backbone. We demonstrate the feasibility of generating
diverse reasoning paths through dropout-based sampling. Our Pass@N analysis on
the generated samples reveals the potential that could enable a significant
gain in performance akin to observed gain in the discrete space. However, we
highlight unique challenges faced for materializing this gain in the continuous
thought space. In particular, working recipes for data generation and training
PRM and ORM models in the discrete space unlocks only marginal improvements in
the continuous space. Through probing various aspects including geometric
properties and trajectory dynamics we identify the underlying reasons that
prevent effective discrimination between correct and incorrect reasoning
(essential for the functioning of PRM and ORM). Our findings reveal that
current limitations stem from the absence of key inductive biases in continuous
thought representations. We argue that the training frameworks for continuous
reasoning LMs require not only to optimize for accuracy but also to explicitly
incorporate inductive biases that could be utilized during inference-time for
discrimination of correct and incorrect thoughts.\footnote{Our code and data
will be publicly available.}

</details>


### [30] [From Knowledge to Treatment: Large Language Model Assisted Biomedical Concept Representation for Drug Repurposing](https://arxiv.org/abs/2510.12181)
*Chengrui Xiang,Tengfei Ma,Xiangzheng Fu,Yiping Liu,Bosheng Song,Xiangxiang Zeng*

Main category: cs.CL

TL;DR: LLaDR是一个基于大语言模型的药物重定位框架，通过从LLM提取语义丰富的生物医学实体文本表示来增强知识图谱嵌入，从而改进生物医学概念表示。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略了真实实验室中的常识性生物医学概念知识，如某些药物与特定治疗根本不相容的机制先验知识。

Method: 从大语言模型提取生物医学实体的语义丰富治疗相关文本表示，并用这些表示来微调知识图谱嵌入模型。

Result: 在基准测试中，LLaDR在不同场景下都达到了最先进的性能，阿尔茨海默病的案例研究进一步证实了其鲁棒性和有效性。

Conclusion: LLaDR通过向知识图谱嵌入注入治疗相关知识，显著改善了生物医学概念的表示，增强了对研究不足或复杂适应症的语义理解。

Abstract: Drug repurposing plays a critical role in accelerating treatment discovery,
especially for complex and rare diseases. Biomedical knowledge graphs (KGs),
which encode rich clinical associations, have been widely adopted to support
this task. However, existing methods largely overlook common-sense biomedical
concept knowledge in real-world labs, such as mechanistic priors indicating
that certain drugs are fundamentally incompatible with specific treatments. To
address this gap, we propose LLaDR, a Large Language Model-assisted framework
for Drug Repurposing, which improves the representation of biomedical concepts
within KGs. Specifically, we extract semantically enriched treatment-related
textual representations of biomedical entities from large language models
(LLMs) and use them to fine-tune knowledge graph embedding (KGE) models. By
injecting treatment-relevant knowledge into KGE, LLaDR largely improves the
representation of biomedical concepts, enhancing semantic understanding of
under-studied or complex indications. Experiments based on benchmarks
demonstrate that LLaDR achieves state-of-the-art performance across different
scenarios, with case studies on Alzheimer's disease further confirming its
robustness and effectiveness. Code is available at
https://github.com/xiaomingaaa/LLaDR.

</details>


### [31] [Not in Sync: Unveiling Temporal Bias in Audio Chat Models](https://arxiv.org/abs/2510.12185)
*Jiayu Yao,Shenghua Liu,Yiwei Wang,Rundong Cheng,Lingrui Mei,Baolong Bi,Zhen Xiong,Xueqi Cheng*

Main category: cs.CL

TL;DR: 首次系统研究大型音频语言模型中的时间偏差问题，发现模型在预测事件时间戳时存在系统性偏差，这种偏差随音频长度增加而累积。


<details>
  <summary>Details</summary>
Motivation: 大型音频语言模型在音频理解和多模态推理中应用日益广泛，但其定位事件发生时间的能力尚未得到充分探索。

Method: 在带时间戳的数据集上进行受控实验，量化时间偏差效应，提出时间偏差指数(TBI)来衡量预测事件时间的系统性错位，并辅以可视化框架。

Result: 发现时间偏差(i)在数据集和模型中普遍存在，(ii)随音频长度增加而增加，在长录音中可累积达数十秒，(iii)在不同事件类型和位置间存在差异。

Conclusion: 研究结果揭示了当前大型音频语言模型的基本局限性，呼吁开发具有时间鲁棒性的架构。

Abstract: Large Audio Language Models (LALMs) are increasingly applied to audio
understanding and multimodal reasoning, yet their ability to locate when events
occur remains underexplored. We present the first systematic study of temporal
bias in LALMs, revealing a key limitation in their timestamp prediction. For
example, when asked "At which second does the lecturer introduce the key
formula?", models often predict timestamps that are consistently earlier or
later than the ground truth. Through controlled experiments on timestamped
datasets, we find that temporal bias (i) is prevalent across datasets and
models, (ii) increases with audio length - even accumulating to tens of seconds
in extended recordings, and (iii) varies across event types and positions. We
quantify this effect with the Temporal Bias Index (TBI), measuring systematic
misalignment in predicted event timings, and complement it with a visualization
framework. Our findings highlight a fundamental limitation in current LALMs and
call for the development of temporally robust architectures.

</details>


### [32] [DPO-Tuned Large Language Models for Segmentation in Simultaneous Speech Translation](https://arxiv.org/abs/2510.12195)
*Zeyu Yang,Satoshi Nakamura*

Main category: cs.CL

TL;DR: 提出基于大语言模型(LLM)和直接偏好优化(DPO)的语音翻译分割框架，通过偏好对齐预测更自然的切分点，在翻译质量和延迟方面优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有分割模型如SHAS虽然比启发式方法更稳健，但仍受限于监督学习目标，缺乏人类偏好对齐，这对实时自然翻译至关重要

Method: 使用DPO训练的大语言模型进行分割预测，结合SeamlessM4T v2作为翻译骨干，在ACL 60/60语料库上评估三个语言对

Result: DPO调优的LLM在分割准确率上超过SHAS，在翻译质量(BLEU、COMET)和延迟(平均滞后)方面均有稳定提升

Conclusion: 偏好调优的LLM有潜力超越现有预训练分割模型，推进自适应、人类对齐的同步口译技术

Abstract: Simultaneous speech translation requires accurate segmentation to balance
translation quality and latency. Recent studies such as SHAS have introduced
pretrained segmentation models, achieving stronger performance than heuristic
rules. However, segmentation models such as SHAS, though pretrained and more
robust than heuristic methods, are still constrained by supervised learning
objectives and do not incorporate human preference alignment, which is crucial
for natural real-time interpretation. In this work, we propose a segmentation
framework based on large language models (LLMs) trained with Direct Preference
Optimization (DPO). By leveraging preference alignment, our method enables LLMs
to predict natural segmentation points that better meet the demands of
real-time translation. We evaluate the system on the ACL 60/60 corpus across
three language pairs (English-Japanese, Chinese, German), using SeamlessM4T v2
as the translation backbone. Experimental results show that our DPO-tuned LLM
achieves higher segmentation accuracy than SHAS and yields consistent
improvements in translation quality (BLEU, COMET) as well as latency (Average
Lagging). Furthermore, our system benefits from IWSLT baselines for direct
comparison. These findings highlight the potential of preference-tuned LLMs to
surpass existing pretrained segmentation models and advance adaptive,
human-aligned simultaneous interpretation.

</details>


### [33] [HALF: Harm-Aware LLM Fairness Evaluation Aligned with Deployment](https://arxiv.org/abs/2510.12217)
*Ali Mekky,Omar El Herraoui,Preslav Nakov,Yuxia Wang*

Main category: cs.CL

TL;DR: 提出了HALF框架，用于评估LLM在真实应用场景中的偏见，并按危害严重性加权结果，发现LLM在不同领域的公平性不一致。


<details>
  <summary>Details</summary>
Motivation: 现有评估缺乏真实场景基础，未考虑不同危害严重性的差异，如手术中的偏见决策与文本摘要中的风格偏见不应同等对待。

Method: 引入HALF框架，将九个应用领域分为三个层级（严重、中等、轻度），使用五阶段流程评估模型偏见并按危害严重性加权结果。

Result: 评估八个LLM显示：(1) LLM在不同领域的公平性不一致；(2) 模型大小或性能不能保证公平性；(3) 推理模型在医疗决策支持中表现更好但在教育中表现更差。

Conclusion: HALF揭示了先前基准测试成功与部署准备度之间的明显差距。

Abstract: Large language models (LLMs) are increasingly deployed across high-impact
domains, from clinical decision support and legal analysis to hiring and
education, making fairness and bias evaluation before deployment critical.
However, existing evaluations lack grounding in real-world scenarios and do not
account for differences in harm severity, e.g., a biased decision in surgery
should not be weighed the same as a stylistic bias in text summarization. To
address this gap, we introduce HALF (Harm-Aware LLM Fairness), a
deployment-aligned framework that assesses model bias in realistic applications
and weighs the outcomes by harm severity. HALF organizes nine application
domains into three tiers (Severe, Moderate, Mild) using a five-stage pipeline.
Our evaluation results across eight LLMs show that (1) LLMs are not
consistently fair across domains, (2) model size or performance do not
guarantee fairness, and (3) reasoning models perform better in medical decision
support but worse in education. We conclude that HALF exposes a clear gap
between previous benchmarking success and deployment readiness.

</details>


### [34] [Analysing Moral Bias in Finetuned LLMs through Mechanistic Interpretability](https://arxiv.org/abs/2510.12229)
*Bianca Raimondi,Daniela Dalbagno,Maurizio Gabbrielli*

Main category: cs.CL

TL;DR: 研究发现LLMs在微调过程中会习得Knobe效应这种道德偏见，通过层修补分析可以定位到特定层组，仅需修补预训练模型的激活就能消除该偏见。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs如何内化人类偏见，特别是Knobe效应这种道德判断偏见，并理解其表现机制。

Method: 在3个开源权重LLMs上进行层修补分析，通过将预训练模型的激活修补到关键层来研究偏见定位。

Result: 偏见不仅能在微调过程中习得，而且被定位在特定层组中，仅修补少数关键层的激活就能消除Knobe效应。

Conclusion: LLMs中的社会偏见可以通过针对性干预进行解释、定位和缓解，无需重新训练模型。

Abstract: Large language models (LLMs) have been shown to internalize human-like biases
during finetuning, yet the mechanisms by which these biases manifest remain
unclear. In this work, we investigated whether the well-known Knobe effect, a
moral bias in intentionality judgements, emerges in finetuned LLMs and whether
it can be traced back to specific components of the model. We conducted a
Layer-Patching analysis across 3 open-weights LLMs and demonstrated that the
bias is not only learned during finetuning but also localized in a specific set
of layers. Surprisingly, we found that patching activations from the
corresponding pretrained model into just a few critical layers is sufficient to
eliminate the effect. Our findings offer new evidence that social biases in
LLMs can be interpreted, localized, and mitigated through targeted
interventions, without the need for model retraining.

</details>


### [35] [DSAS: A Universal Plug-and-Play Framework for Attention Optimization in Multi-Document Question Answering](https://arxiv.org/abs/2510.12251)
*Jiakai Li,Rongzheng Wang,Yizhuo Ma,Shuang Liang,Guangchun Luo,Ke Qin*

Main category: cs.CL

TL;DR: 提出了DSAS（双阶段自适应锐化）方法，通过CGW模块缓解"中间丢失"问题，通过RAS模块增强关键段落关注，无需修改架构或额外训练参数即可提升LLMs在多文档问答任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多文档问答任务中存在长距离依赖建模困难和"中间丢失"问题，现有解决方案要么截断全局依赖，要么需要昂贵的微调，缺乏通用简单的解决方案。

Method: DSAS包含两个模块：CGW模块通过层级注意力跟踪和位置感知加权评估段落相关性；RAS模块通过抑制关键文本与无关文本之间的信息交换来增强对关键段落的关注。

Result: 在四个基准测试上的实验表明，DSAS在主流LLMs（Llama、Qwen、Mistral、Deepseek）上均有效，在Llama-3.1-8B-Instruct和Qwen2.5-14B-Instruct上多文档问答任务的F1分数平均提升4.2%。

Conclusion: DSAS作为一个即插即用的解决方案，无需架构修改或额外训练参数，有效解决了LLMs在多文档问答任务中的关键限制，消融研究证实了CGW和RAS模块的贡献。

Abstract: While large language models (LLMs) show considerable promise across various
fields, they have notable limitations in handling multi-document question
answering (Multi-doc QA) tasks. The first challenge is long-range dependency
modeling, where LLMs struggle to focus on key information in long texts, which
weakens important semantic connections. Second, most LLMs suffer from the
''lost-in-the-middle'' issue, where they have difficulty processing information
in the middle of long inputs. Current solutions either truncate global
dependencies or demand costly finetuning, ultimately lacking a universal and
simple solution for these challenges. To resolve these limitations, we propose
Dual-Stage Adaptive Sharpening (DSAS) containing two modules. (i) The
Contextual Gate Weighting (CGW) module alleviates ''lost-in-the-middle'' by
assessing paragraph relevance through layer-wise attention tracking and
position-aware weighting. (ii) The Reciprocal Attention Suppression (RAS)
module enhances focus on critical paragraphs by suppressing information
exchange between key and irrelevant texts, thus mitigating the limitations in
long-range dependency modeling. Notably, DSAS functions as a plug-and-play
solution requiring no architectural modifications or extra training parameters.
Extensive experiments on four benchmarks demonstrate DSAS's efficacy across
mainstream LLMs (Llama, Qwen, Mistral, and Deepseek), with an average F1-score
improvement of 4.2% in Multi-doc QA tasks on Llama-3.1-8B-Instruct and
Qwen2.5-14B-Instruct. Ablation studies confirm the essential contributions of
both the CGW and RAS modules. In addition, detailed discussions in the Appendix
further validate the robustness and scalability of DSAS.

</details>


### [36] [Shallow Robustness, Deep Vulnerabilities: Multi-Turn Evaluation of Medical LLMs](https://arxiv.org/abs/2510.12255)
*Blazej Manczak,Eric Lin,Francisco Eiras,James O' Neill,Vaikkunth Mugunthan*

Main category: cs.CL

TL;DR: 论文提出了MedQA-Followup框架，用于评估医学问答中多轮对话的鲁棒性，发现LLMs在多轮设置下存在严重脆弱性，间接干预比直接建议更具破坏性。


<details>
  <summary>Details</summary>
Motivation: 现有评估框架通常只在理想条件下评估单轮问答，忽略了医学咨询中常见的冲突输入、误导性上下文和权威影响等复杂性，需要系统评估多轮鲁棒性。

Method: 引入MedQA-Followup框架，区分浅层鲁棒性（抵抗误导初始上下文）和深层鲁棒性（在答案被挑战时保持准确性），并引入间接-直接轴来区分上下文框架和明确建议。

Result: 在MedQA数据集上评估五个最先进的LLMs，发现模型在浅层扰动下表现尚可，但在多轮设置中表现出严重脆弱性，准确率从91.2%降至最低13.5%。间接干预通常比直接建议更具破坏性。

Conclusion: 多轮鲁棒性是医学LLMs安全可靠部署的关键但未被充分探索的维度，间接上下文干预暴露了临床部署的重大脆弱性。

Abstract: Large language models (LLMs) are rapidly transitioning into medical clinical
use, yet their reliability under realistic, multi-turn interactions remains
poorly understood. Existing evaluation frameworks typically assess single-turn
question answering under idealized conditions, overlooking the complexities of
medical consultations where conflicting input, misleading context, and
authority influence are common. We introduce MedQA-Followup, a framework for
systematically evaluating multi-turn robustness in medical question answering.
Our approach distinguishes between shallow robustness (resisting misleading
initial context) and deep robustness (maintaining accuracy when answers are
challenged across turns), while also introducing an indirect-direct axis that
separates contextual framing (indirect) from explicit suggestion (direct).
Using controlled interventions on the MedQA dataset, we evaluate five
state-of-the-art LLMs and find that while models perform reasonably well under
shallow perturbations, they exhibit severe vulnerabilities in multi-turn
settings, with accuracy dropping from 91.2% to as low as 13.5% for Claude
Sonnet 4. Counterintuitively, indirect, context-based interventions are often
more harmful than direct suggestions, yielding larger accuracy drops across
models and exposing a significant vulnerability for clinical deployment.
Further compounding analyses reveal model differences, with some showing
additional performance drops under repeated interventions while others
partially recovering or even improving. These findings highlight multi-turn
robustness as a critical but underexplored dimension for safe and reliable
deployment of medical LLMs.

</details>


### [37] [Chinese ModernBERT with Whole-Word Masking](https://arxiv.org/abs/2510.12285)
*Zeyu Zhao,Ningtao Wang,Xing Fu,Yu Cheng*

Main category: cs.CL

TL;DR: Chinese ModernBERT是一个从头开始训练的中文编码器，通过优化词汇表、训练策略和架构设计，在准确性和效率方面取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有的英文编码器改进未能完全迁移到中文，因为中文在分词和形态学上与英文存在显著差异，需要专门针对中文特点进行优化。

Method: 采用硬件感知的32k BPE词汇表、动态全词掩码训练、两阶段预训练流程（扩展上下文长度至8192 tokens）以及阻尼余弦学习率调度。

Result: 在CLUE基准测试中与强中文编码器竞争，在SimCLUE上达到0.505（Pearson）/0.537（Spearman），超越Qwen-0.6B-embedding。

Conclusion: Chinese ModernBERT展示了针对中文特点的优化策略的有效性，为中文自然语言处理提供了可复现的解决方案。

Abstract: Encoder-only Transformers have advanced along three axes -- architecture,
data, and systems -- yielding Pareto gains in accuracy, speed, and memory
efficiency. Yet these improvements have not fully transferred to Chinese, where
tokenization and morphology differ markedly from English. We introduce Chinese
ModernBERT, a from-scratch Chinese encoder that couples: (i) a hardware-aware
32k BPE vocabulary tailored to frequent Chinese affixes/compounds, lowering the
embedding budget; (ii) whole-word masking (WWM) with a dynamic masking
curriculum (30% -> 15%) to align task difficulty with training progress; (iii)
a two-stage pre-training pipeline that extends the native context from 1,024 to
8,192 tokens using RoPE and alternating local/global attention; and (iv) a
damped-cosine learning-rate schedule for stable long-horizon optimization. We
pre-train on ~1.2T Chinese tokens from CCI3-HQ, CCI4 (Chinese), and
Cosmopedia-Chinese. On CLUE, Chinese ModernBERT is competitive with strong
Chinese encoders under a unified fine-tuning protocol. Under bf16 it achieves
high long-sequence throughput while maintaining strong short-sequence speed,
reflecting benefits from budget allocation and attention design. To probe
retrieval-oriented quality, we add a small amount of open contrastive data:
fine-tuning on SimCLUE (~3M pairs) improves further when adding T2Ranking
(~2M), reaching 0.505 (Pearson) / 0.537 (Spearman) on the SimCLUE test set.
Under this open-data setting, Chinese ModernBERT surpasses Qwen-0.6B-embedding
on SimCLUE, suggesting a clear scaling path for STS with additional curated
pairs. We will release tokenizer and weights to facilitate reproducible
research.

</details>


### [38] [A large-scale, unsupervised pipeline for automatic corpus annotation using LLMs: variation and change in the English consider construction](https://arxiv.org/abs/2510.12306)
*Cameron Morin,Matti Marttinen Larsson*

Main category: cs.CL

TL;DR: 提出了一种使用大语言模型自动进行大规模语料库语法标注的无监督流程，在COHA语料库的143,933个句子标注中达到98%以上准确率


<details>
  <summary>Details</summary>
Motivation: 解决语料库快速扩张时人工标注成为方法瓶颈的问题

Method: 四阶段工作流程：提示工程、事前评估、自动批量处理、事后验证，使用GPT-5通过OpenAI API

Result: 在60小时内完成COHA语料库143,933个句子的标注，两个复杂标注任务的准确率均超过98%

Conclusion: LLMs能够以最少人工干预大规模执行数据准备任务，为基于语料库的研究开辟新可能，但需注意成本、许可和伦理问题

Abstract: As natural language corpora expand at an unprecedented rate, manual
annotation remains a significant methodological bottleneck in corpus linguistic
work. We address this challenge by presenting a scalable, unsupervised pipeline
for automating grammatical annotation in voluminous corpora using large
language models (LLMs). Unlike previous supervised and iterative approaches,
our method employs a four-phase workflow: prompt engineering, pre-hoc
evaluation, automated batch processing, and post-hoc validation. We demonstrate
the pipeline's accessibility and effectiveness through a diachronic case study
of variation in the English consider construction. Using GPT-5 through the
OpenAI API, we annotate 143,933 sentences from the Corpus of Historical
American English (COHA) in under 60 hours, achieving 98%+ accuracy on two
sophisticated annotation procedures. Our results suggest that LLMs can perform
a range of data preparation tasks at scale with minimal human intervention,
opening new possibilities for corpus-based research, though implementation
requires attention to costs, licensing, and other ethical considerations.

</details>


### [39] [Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation](https://arxiv.org/abs/2510.12316)
*Greta Damo,Elena Cabrio,Serena Villata*

Main category: cs.CL

TL;DR: 该论文提出了一个基于检索增强生成(RAG)的框架，将反言论生成建模为知识驱动的文本生成过程，以解决现有方法在可信度和可扩展性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有反言论生成方法存在严重缺陷：基于大语言模型的方法生成内容可靠性有限，而基于非政府组织专家的方法可扩展性差。需要一种能生成可信反言论的新方法。

Method: 构建了基于联合国数字图书馆、EUR-Lex和欧盟基本权利署的知识库（共32,792个文本），集成先进的RAG流程，为仇恨言论文献中识别的8个主要目标群体生成可信反言论。

Result: 在MultiTarget-CONAN数据集上的评估显示，该框架在标准指标(JudgeLM)和人工评估中都优于标准LLM基线和竞争方法。

Conclusion: 该框架和知识库为研究仇恨言论及其他领域可信且合理的反言论生成开辟了新途径。

Abstract: Counter-speech generation is at the core of many expert activities, such as
fact-checking and hate speech, to counter harmful content. Yet, existing work
treats counter-speech generation as pure text generation task, mainly based on
Large Language Models or NGO experts. These approaches show severe drawbacks
due to the limited reliability and coherence in the generated countering text,
and in scalability, respectively. To close this gap, we introduce a novel
framework to model counter-speech generation as knowledge-wise text generation
process. Our framework integrates advanced Retrieval-Augmented Generation (RAG)
pipelines to ensure the generation of trustworthy counter-speech for 8 main
target groups identified in the hate speech literature, including women, people
of colour, persons with disabilities, migrants, Muslims, Jews, LGBT persons,
and other. We built a knowledge base over the United Nations Digital Library,
EUR-Lex and the EU Agency for Fundamental Rights, comprising a total of 32,792
texts. We use the MultiTarget-CONAN dataset to empirically assess the quality
of the generated counter-speech, both through standard metrics (i.e., JudgeLM)
and a human evaluation. Results show that our framework outperforms standard
LLM baselines and competitive approach, on both assessments. The resulting
framework and the knowledge base pave the way for studying trustworthy and
sound counter-speech generation, in hate speech and beyond.

</details>


### [40] [Fine-grained Analysis of Brain-LLM Alignment through Input Attribution](https://arxiv.org/abs/2510.12355)
*Michela Proietti,Roberto Capobianco,Mariya Toneva*

Main category: cs.CL

TL;DR: 本文开发了一种细粒度输入归因方法，用于识别对大脑-LLM对齐最重要的特定词语，并揭示了大脑对齐和下一个词预测依赖不同的词语子集。


<details>
  <summary>Details</summary>
Motivation: 理解大型语言模型与人类大脑活动之间的对齐可以揭示语言处理的计算原理，特别是解决关于大脑对齐与下一个词预测关系这一有争议的研究问题。

Method: 引入细粒度输入归因方法，识别对大脑-LLM对齐最重要的具体词语，并利用该方法比较大脑对齐和下一个词预测的特征依赖。

Result: 发现大脑对齐和下一个词预测依赖不同的词语子集：下一个词预测表现出近因和首因偏见，关注语法；而大脑对齐优先考虑语义和语篇层面信息，具有更有针对性的近因效应。

Conclusion: 这项工作推进了我们对LLM与人类语言处理关系的理解，突显了大脑对齐和下一个词预测在特征依赖上的差异，所提出的归因方法可广泛应用于探索模型预测在不同语言处理任务中的认知相关性。

Abstract: Understanding the alignment between large language models (LLMs) and human
brain activity can reveal computational principles underlying language
processing. We introduce a fine-grained input attribution method to identify
the specific words most important for brain-LLM alignment, and leverage it to
study a contentious research question about brain-LLM alignment: the
relationship between brain alignment (BA) and next-word prediction (NWP). Our
findings reveal that BA and NWP rely on largely distinct word subsets: NWP
exhibits recency and primacy biases with a focus on syntax, while BA
prioritizes semantic and discourse-level information with a more targeted
recency effect. This work advances our understanding of how LLMs relate to
human language processing and highlights differences in feature reliance
between BA and NWP. Beyond this study, our attribution method can be broadly
applied to explore the cognitive relevance of model predictions in diverse
language processing tasks.

</details>


### [41] [MoBiLE: Efficient Mixture-of-Experts Inference on Consumer GPU with Mixture of Big Little Experts](https://arxiv.org/abs/2510.12357)
*Yushu Zhao,Yubin Qin,Yang Wang,Xiaolong Yang,Huiming Han,Shaojun Wei,Yang Hu,Shouyi Yin*

Main category: cs.CL

TL;DR: MoBiLE是一个基于卸载的MoE推理框架，通过混合大小专家机制，对不重要token使用半数量专家加速，对重要token保持完整专家以保证质量，实现了1.60-1.72倍加速且精度损失可忽略。


<details>
  <summary>Details</summary>
Motivation: 现有MoE模型的卸载策略受限于CPU-GPU互联带宽，预取方法训练开销大且在细粒度专家分割的现代MoE模型中效果有限。

Method: 提出MoBiLE框架，采用混合大小专家机制：不重要token使用半数量专家加速，重要token使用完整专家保证质量；设计专门的回退和预取机制在大小专家间切换以提高内存效率。

Result: 在四种典型现代MoE架构和生成任务上测试，相比基线在消费级GPU系统上实现1.60-1.72倍加速，精度损失可忽略。

Conclusion: MoBiLE是一个即插即用的MoE推理框架，有效解决了现有卸载方法的带宽瓶颈问题，在保持模型质量的同时显著提升推理速度。

Abstract: Mixture-of-Experts (MoE) models have recently demonstrated exceptional
performance across a diverse range of applications. The principle of sparse
activation in MoE models facilitates an offloading strategy, wherein active
experts are maintained in GPU HBM, while inactive experts are stored in CPU
DRAM. The efficacy of this approach, however, is fundamentally constrained by
the limited bandwidth of the CPU-GPU interconnect. To mitigate this bottleneck,
existing approaches have employed prefetching to accelerate MoE inference.
These methods attempt to predict and prefetch the required experts using
specially trained modules. Nevertheless, such techniques are often encumbered
by significant training overhead and have shown diminished effectiveness on
recent MoE models with fine-grained expert segmentation.
  In this paper, we propose MoBiLE, a plug-and-play offloading-based MoE
inference framework with \textit{mixture of big-little experts}. It reduces the
number of experts for unimportant tokens to half for acceleration while
maintaining full experts for important tokens to guarantee model quality.
Further, a dedicated fallback and prefetching mechanism is designed for
switching between little and big experts to improve memory efficiency. We
evaluate MoBiLE on four typical modern MoE architectures and challenging
generative tasks. Our results show that MoBiLE achieves a speedup of 1.60x to
1.72x compared to the baseline on a consumer GPU system, with negligible
degradation in accuracy.

</details>


### [42] [LLM-REVal: Can We Trust LLM Reviewers Yet?](https://arxiv.org/abs/2510.12367)
*Rui Li,Jia-Chen Gu,Po-Nien Kung,Heming Xia,Junfeng liu,Xiangwen Kong,Zhifang Sui,Nanyun Peng*

Main category: cs.CL

TL;DR: 研究发现LLM作为审稿人存在系统性偏见：倾向于给LLM生成的论文打高分，而对包含批判性陈述的人类论文评分偏低，揭示了在学术评审中使用LLM的风险。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在学术工作流程中的深度整合，需要探索其在研究和评审双重角色中可能带来的新风险，特别是对学术公平性的影响。

Method: 通过模拟实验，设置研究代理生成和修改论文，评审代理评估提交的论文，并基于模拟结果进行人工标注分析。

Result: 发现LLM审稿人与人类判断存在显著偏差：1）系统性抬高LLM撰写论文的分数；2）持续低估包含批判性陈述的人类论文。这些源于LLM的语言特征偏见和对批判性陈述的厌恶。

Conclusion: 在同行评审中部署LLM需要谨慎，以避免对作者和学术研究的风险和公平性担忧。但LLM指导的修改确实能提升论文质量，对早期研究人员和低质量论文有潜在价值。

Abstract: The rapid advancement of large language models (LLMs) has inspired
researchers to integrate them extensively into the academic workflow,
potentially reshaping how research is practiced and reviewed. While previous
studies highlight the potential of LLMs in supporting research and peer review,
their dual roles in the academic workflow and the complex interplay between
research and review bring new risks that remain largely underexplored. In this
study, we focus on how the deep integration of LLMs into both peer-review and
research processes may influence scholarly fairness, examining the potential
risks of using LLMs as reviewers by simulation. This simulation incorporates a
research agent, which generates papers and revises, alongside a review agent,
which assesses the submissions. Based on the simulation results, we conduct
human annotations and identify pronounced misalignment between LLM-based
reviews and human judgments: (1) LLM reviewers systematically inflate scores
for LLM-authored papers, assigning them markedly higher scores than
human-authored ones; (2) LLM reviewers persistently underrate human-authored
papers with critical statements (e.g., risk, fairness), even after multiple
revisions. Our analysis reveals that these stem from two primary biases in LLM
reviewers: a linguistic feature bias favoring LLM-generated writing styles, and
an aversion toward critical statements. These results highlight the risks and
equity concerns posed to human authors and academic research if LLMs are
deployed in the peer review cycle without adequate caution. On the other hand,
revisions guided by LLM reviews yield quality gains in both LLM-based and human
evaluations, illustrating the potential of the LLMs-as-reviewers for
early-stage researchers and enhancing low-quality papers.

</details>


### [43] [Tokenization Disparities as Infrastructure Bias: How Subword Systems Create Inequities in LLM Access and Efficiency](https://arxiv.org/abs/2510.12389)
*Hailay Kidu Teklehaymanot,Wolfgang Nejdl*

Main category: cs.CL

TL;DR: 对200多种语言的大规模跨语言评估显示，当前大语言模型的tokenization存在系统性不公平：拉丁文字语言tokenization效率高，而非拉丁文字和形态复杂语言tokenization成本高出3-5倍，导致计算成本增加和上下文利用效率降低。


<details>
  <summary>Details</summary>
Motivation: tokenization差异对实现跨语言群体公平访问人工智能构成重大障碍，需要系统量化大语言模型中的计算不公平问题。

Method: 使用标准化实验框架，对200多种语言样本应用一致的预处理和规范化协议，通过tiktoken库进行统一tokenization，收集Tokens Per Sentence (TPS)和Relative Tokenization Cost (RTC)等评估指标。

Result: 拉丁文字语言tokenization效率高，非拉丁文字和形态复杂语言tokenization成本显著更高（RTC比率通常高出3-5倍），导致计算成本增加和有效上下文利用减少。

Conclusion: 当前AI系统存在结构性不公平，低资源和非拉丁语言使用者面临不成比例的计算劣势。未来研究应优先开发语言感知的tokenization策略和适应性词汇构建方法，确保更具包容性和计算公平的多语言AI系统。

Abstract: Tokenization disparities pose a significant barrier to achieving equitable
access to artificial intelligence across linguistically diverse populations.
This study conducts a large-scale cross-linguistic evaluation of tokenization
efficiency in over 200 languages to systematically quantify computational
inequities in large language models (LLMs). Using a standardized experimental
framework, we applied consistent preprocessing and normalization protocols,
followed by uniform tokenization through the tiktoken library across all
language samples. Comprehensive tokenization statistics were collected using
established evaluation metrics, including Tokens Per Sentence (TPS) and
Relative Tokenization Cost (RTC), benchmarked against English baselines. Our
cross-linguistic analysis reveals substantial and systematic disparities:
Latin-script languages consistently exhibit higher tokenization efficiency,
while non-Latin and morphologically complex languages incur significantly
greater token inflation, often 3-5 times higher RTC ratios. These
inefficiencies translate into increased computational costs and reduced
effective context utilization for underrepresented languages. Overall, the
findings highlight structural inequities in current AI systems, where speakers
of low-resource and non-Latin languages face disproportionate computational
disadvantages. Future research should prioritize the development of
linguistically informed tokenization strategies and adaptive vocabulary
construction methods that incorporate typological diversity, ensuring more
inclusive and computationally equitable multilingual AI systems.

</details>


### [44] [PRoH: Dynamic Planning and Reasoning over Knowledge Hypergraphs for Retrieval-Augmented Generation](https://arxiv.org/abs/2510.12434)
*Xiangjun Zai,Xingyu Tan,Xiaoyang Wang,Qing Liu,Xiwei Xu,Wenjie Zhang*

Main category: cs.CL

TL;DR: PRoH是一个动态规划和推理框架，通过上下文感知规划、结构化问题分解和实体加权重叠引导的检索算法，解决了知识超图在检索增强生成中的静态规划和非自适应执行问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于知识超图的RAG方法存在静态检索规划、非自适应检索执行以及对超图结构和语义的浅层使用等三大限制，制约了多跳问答的有效性。

Method: PRoH包含三个核心创新：上下文感知规划模块指导结构化推理计划生成；结构化问题分解将子问题组织为动态演化的有向无环图；实体加权重叠引导的推理路径检索算法优先考虑语义连贯的超边遍历。

Result: 在多个领域的实验中，PRoH实现了最先进的性能，平均F1分数比之前的SOTA模型HyperGraphRAG高出19.73%，生成评估分数高出8.41%，并在长距离多跳推理任务中保持强大的鲁棒性。

Conclusion: PRoH框架通过动态规划和结构化推理，显著提升了知识超图在多跳问答任务中的性能，证明了其在复杂推理场景中的有效性。

Abstract: Knowledge Hypergraphs (KHs) have recently emerged as a knowledge
representation for retrieval-augmented generation (RAG), offering a paradigm to
model multi-entity relations into a structured form. However, existing KH-based
RAG methods suffer from three major limitations: static retrieval planning,
non-adaptive retrieval execution, and superficial use of KH structure and
semantics, which constrain their ability to perform effective multi-hop
question answering. To overcome these limitations, we propose PRoH, a dynamic
Planning and Reasoning over Knowledge Hypergraphs framework. PRoH incorporates
three core innovations: (i) a context-aware planning module that sketches the
local KH neighborhood to guide structurally grounded reasoning plan generation;
(ii) a structured question decomposition process that organizes subquestions as
a dynamically evolving Directed Acyclic Graph (DAG) to enable adaptive,
multi-trajectory exploration; and (iii) an Entity-Weighted Overlap (EWO)-guided
reasoning path retrieval algorithm that prioritizes semantically coherent
hyperedge traversals. Experiments across multiple domains demonstrate that PRoH
achieves state-of-the-art performance, surpassing the prior SOTA model
HyperGraphRAG by an average of 19.73% in F1 and 8.41% in Generation Evaluation
(G-E) score, while maintaining strong robustness in long-range multi-hop
reasoning tasks.

</details>


### [45] [Probing Latent Knowledge Conflict for Faithful Retrieval-Augmented Generation](https://arxiv.org/abs/2510.12460)
*Linfeng Gao,Baolong Bi,Zheng Yuan,Le Wang,Zerui Chen,Zhimin Wei,Shenghua Liu,Qinggang Zhang,Jinsong Su*

Main category: cs.CL

TL;DR: 提出CLEAR框架，通过隐藏状态探测定位知识冲突，并进行冲突感知微调，显著提升RAG系统的准确性和上下文忠实度。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统存在不忠实问题，模型响应与检索证据矛盾。现有方法将LLM视为黑盒，忽视了LLM内部如何整合检索证据与参数记忆，特别是在知识冲突情况下。

Method: CLEAR框架：(i) 将上下文分解为细粒度句子级知识；(ii) 使用隐藏状态探测定位冲突知识；(iii) 引入冲突感知微调指导模型准确整合检索证据。

Result: 在三个基准测试上的广泛实验表明，CLEAR显著提高了准确性和上下文忠实度，在各种冲突条件下始终优于强基线方法。

Conclusion: CLEAR通过分析LLM内部表示和冲突定位，有效解决了RAG系统的忠实性问题，为知识冲突下的证据整合提供了新思路。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm to
enhance the factuality of Large Language Models (LLMs). However, existing RAG
systems often suffer from an unfaithfulness issue, where the model's response
contradicts evidence from the retrieved context. Existing approaches to
improving contextual faithfulness largely rely on external interventions, such
as prompt engineering, decoding constraints, or reward-based fine-tuning. These
works treat the LLM as a black box and overlook a crucial question: how does
the LLM internally integrate retrieved evidence with its parametric memory,
particularly under knowledge conflicts? To address this gap, we conduct a
probing-based analysis of hidden-state representations in LLMs and observe
three findings: knowledge integration occurs hierarchically, conflicts manifest
as latent signals at the sentence level, and irrelevant context is often
amplified when aligned with parametric knowledge. Building on these findings,
we propose CLEAR (Conflict-Localized and Enhanced Attention for RAG), a
framework that (i) decomposes context into fine-grained sentence-level
knowledge, (ii) employs hidden-state probing to localize conflicting knowledge,
and (iii) introduces conflict-aware fine-tuning to guide the model to
accurately integrate retrieved evidence. Extensive experiments across three
benchmarks demonstrate that CLEAR substantially improves both accuracy and
contextual faithfulness, consistently outperforming strong baselines under
diverse conflict conditions. The related resources are available at
https://github.com/LinfengGao/CLEAR.

</details>


### [46] [Resource-sensitive but language-blind: Community size and not grammatical complexity better predicts the accuracy of Large Language Models in a novel Wug Test](https://arxiv.org/abs/2510.12463)
*Nikoleta Pantelidou,Evelina Leivada,Paolo Morosi*

Main category: cs.CL

TL;DR: 该研究通过多语言Wug测试评估了6个大型语言模型在形态学泛化任务中的表现，发现模型能够以类似人类的准确率处理新词，但性能主要受训练数据量而非语言结构复杂度影响。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型的真实语言能力，特别是它们是否能够像人类一样进行形态学泛化，以及模型性能是受语言复杂度还是训练数据量的影响。

Method: 使用多语言Wug测试，在四种语言（加泰罗尼亚语、英语、希腊语和西班牙语）上测试6个模型，并与人类表现进行比较。

Result: 模型能够以人类水平的准确率对新词进行形态学泛化，但准确率模式更符合社区规模和数据可用性，而非结构复杂度。西班牙语和英语表现优于加泰罗尼亚语和希腊语。

Conclusion: 模型行为主要受语言资源丰富度驱动，而非对语法复杂度的敏感性，其表现仅表面类似人类语言能力。

Abstract: The linguistic abilities of Large Language Models are a matter of ongoing
debate. This study contributes to this discussion by investigating model
performance in a morphological generalization task that involves novel words.
Using a multilingual adaptation of the Wug Test, six models were tested across
four partially unrelated languages (Catalan, English, Greek, and Spanish) and
compared with human speakers. The aim is to determine whether model accuracy
approximates human competence and whether it is shaped primarily by linguistic
complexity or by the quantity of available training data. Consistent with
previous research, the results show that the models are able to generalize
morphological processes to unseen words with human-like accuracy. However,
accuracy patterns align more closely with community size and data availability
than with structural complexity, refining earlier claims in the literature. In
particular, languages with larger speaker communities and stronger digital
representation, such as Spanish and English, revealed higher accuracy than
less-resourced ones like Catalan and Greek. Overall, our findings suggest that
model behavior is mainly driven by the richness of linguistic resources rather
than by sensitivity to grammatical complexity, reflecting a form of performance
that resembles human linguistic competence only superficially.

</details>


### [47] [SMEC: Rethinking Matryoshka Representation Learning for Retrieval Embedding Compression](https://arxiv.org/abs/2510.12474)
*Biao Zhang,Lixin Chen,Tong Liu,Bo Zheng*

Main category: cs.CL

TL;DR: 提出SMEC框架，通过序列嵌套表示学习、自适应维度选择和可选跨批次记忆模块，在保持性能的同时显著降低LLM嵌入的维度。


<details>
  <summary>Details</summary>
Motivation: 高维LLM嵌入导致计算复杂度和存储需求增加，阻碍实际部署，需要有效的维度压缩方法。

Method: SMEC框架包含SMRL方法减少训练梯度方差、ADS模块减少维度剪枝信息损失、S-XBM模块增强高低维嵌入的无监督学习。

Result: 在图像、文本和多模态数据集上验证，SMEC实现显著维度压缩且保持性能。在BEIR数据集上，256维压缩嵌入性能优于对比方法1.1-2.7点。

Conclusion: SMEC框架有效解决LLM高维嵌入的压缩问题，为实际部署提供可行方案。

Abstract: Large language models (LLMs) generate high-dimensional embeddings that
capture rich semantic and syntactic information. However, high-dimensional
embeddings exacerbate computational complexity and storage requirements,
thereby hindering practical deployment. To address these challenges, we propose
a novel training framework named Sequential Matryoshka Embedding Compression
(SMEC). This framework introduces the Sequential Matryoshka Representation
Learning(SMRL) method to mitigate gradient variance during training, the
Adaptive Dimension Selection (ADS) module to reduce information degradation
during dimension pruning, and the Selectable Cross-batch Memory (S-XBM) module
to enhance unsupervised learning between high- and low-dimensional embeddings.
Experiments on image, text, and multimodal datasets demonstrate that SMEC
achieves significant dimensionality reduction while maintaining performance.
For instance, on the BEIR dataset, our approach improves the performance of
compressed LLM2Vec embeddings (256 dimensions) by 1.1 points and 2.7 points
compared to the Matryoshka-Adaptor and Search-Adaptor models, respectively.

</details>


### [48] [When Personalization Tricks Detectors: The Feature-Inversion Trap in Machine-Generated Text Detection](https://arxiv.org/abs/2510.12476)
*Lang Gao,Xuhui Li,Chenxi Wang,Mingzhe Li,Wei Liu,Zirui Song,Jinghui Zhang,Rui Yan,Preslav Nakov,Xiuying Chen*

Main category: cs.CL

TL;DR: 该论文提出了首个个性化机器生成文本检测基准，揭示了现有检测器在个性化设置中的性能下降问题，并提出了一种预测检测器性能变化的方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型能够生成模仿个人风格的文本，这增加了身份冒充的风险。目前缺乏针对个性化机器生成文本检测的研究，需要评估检测器在个性化设置中的鲁棒性。

Method: 构建了首个个性化机器生成文本检测基准，包含文学作品和博客文本及其LLM生成的模仿版本。提出了M3方法，通过识别特征反转方向并构建探针数据集来预测检测器性能变化。

Result: 实验结果显示，在个性化设置中，一些最先进的检测器性能显著下降。M3方法能够准确预测检测器性能变化的方向和幅度，与实际性能差距的相关性达到85%。

Conclusion: 这项工作揭示了现有文本检测器在个性化设置中的局限性，提出了有效的性能预测方法，鼓励进一步研究个性化文本检测问题。

Abstract: Large language models (LLMs) have grown more powerful in language generation,
producing fluent text and even imitating personal style. Yet, this ability also
heightens the risk of identity impersonation. To the best of our knowledge, no
prior work has examined personalized machine-generated text (MGT) detection. In
this paper, we introduce \dataset, the first benchmark for evaluating detector
robustness in personalized settings, built from literary and blog texts paired
with their LLM-generated imitations. Our experimental results demonstrate large
performance gaps across detectors in personalized settings: some
state-of-the-art models suffer significant drops. We attribute this limitation
to the \textit{feature-inversion trap}, where features that are discriminative
in general domains become inverted and misleading when applied to personalized
text. Based on this finding, we propose \method, a simple and reliable way to
predict detector performance changes in personalized settings. \method
identifies latent directions corresponding to inverted features and constructs
probe datasets that differ primarily along these features to evaluate detector
dependence. Our experiments show that \method can accurately predict both the
direction and the magnitude of post-transfer changes, showing 85\% correlation
with the actual performance gaps. We hope that this work will encourage further
research on personalized text detection.

</details>


### [49] [BoN Appetit Team at LeWiDi-2025: Best-of-N Test-time Scaling Can Not Stomach Annotation Disagreements (Yet)](https://arxiv.org/abs/2510.12516)
*Tomas Ruiz,Siyao Peng,Barbara Plank,Carsten Schwemmer*

Main category: cs.CL

TL;DR: 该论文将测试时缩放技术从数学和编程领域转移到LeWiDi-2025任务中，用于评估标注分歧。实验发现模型平均和多数投票方法能持续提升LLM性能，但Best-of-N采样方法无效。


<details>
  <summary>Details</summary>
Motivation: 测试时缩放技术此前仅限于有可验证正确答案的领域（如数学和编程），作者希望将其扩展到LeWiDi任务来评估标注分歧。

Method: 使用了三种测试时缩放方法：模型平均、多数投票和Best-of-N采样，在LeWiDi-2025任务上进行实验。

Result: 两个基准方法（模型平均和多数投票）在LeWiDi任务上持续提升了LLM性能，但Best-of-N方法没有效果。

Conclusion: Best-of-N方法目前无法从数学领域成功转移到LeWiDi任务，作者分析了这种差距的潜在原因。

Abstract: Test-time scaling is a family of techniques to improve LLM outputs at
inference time by performing extra computation. To the best of our knowledge,
test-time scaling has been limited to domains with verifiably correct answers,
like mathematics and coding. We transfer test-time scaling to the LeWiDi-2025
tasks to evaluate annotation disagreements. We experiment with three test-time
scaling methods: two benchmark algorithms (Model Averaging and Majority
Voting), and a Best-of-N sampling method. The two benchmark methods improve LLM
performance consistently on the LeWiDi tasks, but the Best-of-N method does
not. Our experiments suggest that the Best-of-N method does not currently
transfer from mathematics to LeWiDi tasks, and we analyze potential reasons for
this gap.

</details>


### [50] [VISaGE: Understanding Visual Generics and Exceptions](https://arxiv.org/abs/2510.12548)
*Stella Frank,Emily Allaway*

Main category: cs.CL

TL;DR: 该论文研究了视觉语言模型在处理典型和异常图像时的权衡机制，发现当输入图像与文本不一致时，模型的概念理解会下降。


<details>
  <summary>Details</summary>
Motivation: 研究VLMs在处理典型和异常实例时如何平衡实用先验（输入相关）和语义先验（概念真实性）之间的冲突。

Method: 引入新的评估数据集VISaGE，包含典型和异常图像，通过精心平衡的实验设计来测试VLMs的权衡机制。

Result: 当输入图像与文本不一致时，模型的概念理解显著下降，这种实用先验违反的影响比语义先验更强。

Conclusion: VLMs在处理个体实例时更依赖实用先验而非语义先验，当输入不一致时概念理解会受损。

Abstract: While Vision Language Models (VLMs) learn conceptual representations, in the
form of generalized knowledge, during training, they are typically used to
analyze individual instances. When evaluation instances are atypical, this
paradigm results in tension between two priors in the model. The first is a
pragmatic prior that the textual and visual input are both relevant, arising
from VLM finetuning on congruent inputs; the second is a semantic prior that
the conceptual representation is generally true for instances of the category.
In order to understand how VLMs trade off these priors, we introduce a new
evaluation dataset, VISaGE, consisting of both typical and exceptional images.
In carefully balanced experiments, we show that conceptual understanding
degrades when the assumption of congruency underlying the pragmatic prior is
violated with incongruent images. This effect is stronger than the effect of
the semantic prior when querying about individual instances.

</details>


### [51] [Teaching Language Models to Faithfully Express their Uncertainty](https://arxiv.org/abs/2510.12587)
*Bryan Eikema,Evgenia Ilia,José G. C. de Souza,Chrysoula Zerva,Wilker Aziz*

Main category: cs.CL

TL;DR: 提出Faithful Uncertainty Tuning (FUT)方法，通过微调让LLM忠实表达不确定性，而不改变其答案分布。


<details>
  <summary>Details</summary>
Motivation: LLM在表达不确定性方面存在忠实性问题：重复查询会产生不同答案，但生成的响应通常没有或使用不反映这种变异性的修饰语，这传达了关于LLM知识不确定状态的不忠实信息。

Method: 构建训练数据，通过在模型样本中添加与样本一致性对齐的不确定性修饰语（如'可能'或'很可能'），仅需模型和一组提示即可，无需额外监督。

Result: 在多个模型和数据集上的开放域问答评估显示，FUT显著减少了忠实性差距，同时保持了问答准确性并引入了最小的语义分布偏移。

Conclusion: FUT是一种简单有效的方法，可以教会LLM忠实传达不确定性。

Abstract: Large language models (LLMs) often miscommunicate their uncertainty: repeated
queries can produce divergent answers, yet generated responses are typically
unhedged or hedged in ways that do not reflect this variability. This conveys
unfaithful information about the uncertain state of the LLMs' knowledge,
creating a faithfulness gap that affects even strong LLMs. We introduce
Faithful Uncertainty Tuning (FUT): a fine-tuning approach that teaches
instruction-tuned LLMs to express uncertainty faithfully without altering their
underlying answer distribution. We construct training data by augmenting model
samples with uncertainty hedges (i.e. verbal cues such as 'possibly' or
'likely') aligned with sample consistency, requiring no supervision beyond the
model and a set of prompts. We evaluate FUT on open-domain question answering
(QA) across multiple models and datasets. Our results show that FUT
substantially reduces the faithfulness gap, while preserving QA accuracy and
introducing minimal semantic distribution shift. Further analyses demonstrate
robustness across decoding strategies, choice of hedgers, and other forms of
uncertainty expression (i.e. numerical). These findings establish FUT as a
simple and effective way to teach LLMs to communicate uncertainty faithfully.

</details>


### [52] [StyleDecipher: Robust and Explainable Detection of LLM-Generated Texts with Stylistic Analysis](https://arxiv.org/abs/2510.12608)
*Siyuan Li,Aodu Wulianghai,Xi Lin,Guangyan Li,Xiang Chen,Jun Wu,Jianhua Li*

Main category: cs.CL

TL;DR: StyleDecipher是一个可解释的机器生成文本检测框架，通过结合离散风格指标和连续风格表示来量化人类与LLM生成文本的风格差异，在跨领域检测中表现优异。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在开放领域写作中的广泛应用，检测机器生成文本对于确保内容真实性和可信度变得至关重要。现有方法在现实场景中由于泛化能力有限、易受改写攻击和缺乏可解释性而表现不佳。

Method: 提出StyleDecipher框架，联合建模离散风格指标和基于语义嵌入的连续风格表示，在统一表示空间中捕捉人类与LLM输出的风格级差异，无需访问模型内部或标记片段。

Result: 在五个不同领域（新闻、代码、论文、评论、学术摘要）的实验中，StyleDecipher在领域内检测准确率达到了最先进水平。在跨领域评估中，比现有基线方法高出36.30%，同时对对抗性扰动和混合人机内容保持鲁棒性。

Conclusion: 风格信号为区分机器生成文本提供了可解释的证据，StyleDecipher框架实现了准确、可解释且领域无关的检测。

Abstract: With the increasing integration of large language models (LLMs) into
open-domain writing, detecting machine-generated text has become a critical
task for ensuring content authenticity and trust. Existing approaches rely on
statistical discrepancies or model-specific heuristics to distinguish between
LLM-generated and human-written text. However, these methods struggle in
real-world scenarios due to limited generalization, vulnerability to
paraphrasing, and lack of explainability, particularly when facing stylistic
diversity or hybrid human-AI authorship. In this work, we propose
StyleDecipher, a robust and explainable detection framework that revisits
LLM-generated text detection using combined feature extractors to quantify
stylistic differences. By jointly modeling discrete stylistic indicators and
continuous stylistic representations derived from semantic embeddings,
StyleDecipher captures distinctive style-level divergences between human and
LLM outputs within a unified representation space. This framework enables
accurate, explainable, and domain-agnostic detection without requiring access
to model internals or labeled segments. Extensive experiments across five
diverse domains, including news, code, essays, reviews, and academic abstracts,
demonstrate that StyleDecipher consistently achieves state-of-the-art in-domain
accuracy. Moreover, in cross-domain evaluations, it surpasses existing
baselines by up to 36.30%, while maintaining robustness against adversarial
perturbations and mixed human-AI content. Further qualitative and quantitative
analysis confirms that stylistic signals provide explainable evidence for
distinguishing machine-generated text. Our source code can be accessed at
https://github.com/SiyuanLi00/StyleDecipher.

</details>


### [53] [ACADATA: Parallel Dataset of Academic Data for Machine Translation](https://arxiv.org/abs/2510.12621)
*Iñaki Lacunza,Javier Garcia Gilabert,Francesca De Luca Fornaciari,Javier Aula-Blasco,Aitor Gonzalez-Agirre,Maite Melero,Marta Villegas*

Main category: cs.CL

TL;DR: ACADATA是一个高质量的学术翻译平行数据集，包含150万段作者生成的段落对和6000个翻译的评估集。通过在ACADATA上微调LLM，显著提升了学术翻译质量并超越了专有模型。


<details>
  <summary>Details</summary>
Motivation: 解决学术翻译领域缺乏高质量数据集的问题，为学术领域和长上下文翻译研究提供资源。

Method: 构建ACADATA数据集（包含训练集ACAD-TRAIN和评估集ACAD-BENCH），在ACAD-TRAIN上微调大型语言模型，并在ACAD-BENCH上与多种翻译系统进行基准测试。

Result: 微调后7B和2B模型在学术翻译质量上分别平均提升6.1和12.4 d-BLEU点，从英语翻译时在通用领域的长上下文翻译提升达24.9%，最佳微调模型超越了专有和开源模型。

Conclusion: ACADATA数据集和微调模型为学术翻译研究提供了宝贵资源，显著提升了学术翻译质量，并推动了长上下文翻译的发展。

Abstract: We present ACADATA, a high-quality parallel dataset for academic translation,
that consists of two subsets: ACAD-TRAIN, which contains approximately 1.5
million author-generated paragraph pairs across 96 language directions and
ACAD-BENCH, a curated evaluation set of almost 6,000 translations covering 12
directions. To validate its utility, we fine-tune two Large Language Models
(LLMs) on ACAD-TRAIN and benchmark them on ACAD-BENCH against specialized
machine-translation systems, general-purpose, open-weight LLMs, and several
large-scale proprietary models. Experimental results demonstrate that
fine-tuning on ACAD-TRAIN leads to improvements in academic translation quality
by +6.1 and +12.4 d-BLEU points on average for 7B and 2B models respectively,
while also improving long-context translation in a general domain by up to
24.9% when translating out of English. The fine-tuned top-performing model
surpasses the best propietary and open-weight models on academic translation
domain. By releasing ACAD-TRAIN, ACAD-BENCH and the fine-tuned models, we
provide the community with a valuable resource to advance research in academic
domain and long-context translation.

</details>


### [54] [COSTAR-A: A prompting framework for enhancing Large Language Model performance on Point-of-View questions](https://arxiv.org/abs/2510.12637)
*Nzubechukwu C. Ohalete,Kevin B. Gittner,Lauren M. Matheny*

Main category: cs.CL

TL;DR: COSTAR-A是一种改进的提示工程框架，在原有COSTAR方法基础上增加了'答案'组件，旨在提升小型本地优化语言模型的输出结构和决策性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型对提示设计高度敏感，现有COSTAR框架在大型模型上表现良好，但在小型本地优化模型上表现不一致，特别是在需要更指令性或约束性输出的任务中。

Method: 通过一系列受控的提示-输出评估，使用最多80亿参数的精调模型，比较COSTAR-A与原始COSTAR框架的性能差异。

Result: COSTAR-A能够增强本地化语言模型的输出结构和决策性，但效果因模型和用例而异。Llama 3.1-8B模型在使用COSTAR-A时表现出性能提升。

Conclusion: COSTAR-A作为一种提示框架具有适应性和可扩展性，特别适用于资源受限硬件上的计算高效AI部署。

Abstract: Large Language Models (LLMs) are highly sensitive to prompt design, and
making optimized prompting techniques is crucial for generating consistent,
high-quality outputs. In this study, we introduce COSTAR-A, a novel prompt
engineering framework that enhances the existing COSTAR method, which stands
for Context, Objective, Style, Tone, Audience, and Response, by adding the
'Answer' component at the end. We demonstrate that while the original COSTAR
framework improves prompt clarity and aligns outputs for larger LLMs, its
performance is less consistent with smaller, locally optimized models,
particularly in tasks that require more directive or constrained outputs.
Through a series of controlled prompt-output assessments with smaller (at most
8 billion parameters), fine-tuned models, we found that COSTAR-A can enhance
the output structure and decisiveness of localized LLMs for certain tasks,
although its effectiveness varies across models and use cases. Notably, the
Llama 3.1-8B model exhibited performance improvements when prompted with
COSTAR-A compared to COSTAR alone. These findings emphasize the adaptability
and scalability of COSTAR-A as a prompting framework, particularly in
computationally efficient AI deployments on resource-constrained hardware.

</details>


### [55] [Reasoning Pattern Matters: Learning to Reason without Human Rationales](https://arxiv.org/abs/2510.12643)
*Chaoxu Pang,Yixuan Cao,Ping Luo*

Main category: cs.CL

TL;DR: 该论文提出PARO框架，通过让LLM生成符合任务特定推理模式的理由，无需人工标注，就能在推理任务中达到与10倍规模人工标注相当的性能。


<details>
  <summary>Details</summary>
Motivation: 当前SFT+RLVR范式需要昂贵的人工标注推理轨迹，作者希望研究如何在不损害推理性能的情况下大幅降低标注成本。

Method: 提出PARO框架，识别模式化推理任务，让LLM根据任务特定的推理模式自动生成理由，无需人工标注。

Result: PARO生成的理由在SFT+RLVR中达到与10倍规模人工标注相当的性能，证明大规模人工标注可被有限监督的LLM自动标注替代。

Conclusion: 对于模式化推理任务，推理模式而非理由数量或质量是性能关键，LLM自动标注可有效替代昂贵的人工标注。

Abstract: Large Language Models (LLMs) have demonstrated remarkable reasoning
capabilities under the widely adopted SFT+RLVR paradigm, which first performs
Supervised Fine-Tuning (SFT) on human-annotated reasoning trajectories
(rationales) to establish initial reasoning behaviors, then applies
Reinforcement Learning with Verifiable Rewards (RLVR) to optimize the model
using verifiable signals without golden rationales. However, annotating
high-quality rationales for the SFT stage remains prohibitively expensive. This
paper investigates when and how rationale annotation costs can be substantially
reduced without compromising reasoning performance. We identify a broad class
of problems, termed patterned reasoning tasks, where reasoning follows a fixed,
procedural strategy consistent across instances. Although instances vary in
content such as domain knowledge, factual information, or numeric values, the
solution derives from applying a shared reasoning pattern. We argue that the
success of SFT+RLVR on such tasks primarily stems from its ability to enable
models to internalize these reasoning patterns. Using numerical semantic
matching as a representative task, we provide both causal and behavioral
evidence showing that reasoning patterns rather than the quantity or quality of
rationales are the key determinant of performance. Building on these insights,
we propose Pattern-Aware LLMs as Rationale AnnOtators (PARO), a simple yet
effective framework that enables LLMs to generate rationales aligned with
task-specific reasoning patterns without requiring human rationale annotations.
Experiments show that PARO-generated rationales achieve comparable SFT+RLVR
performance to human rationales that are 10 times larger. These results suggest
that large-scale human rationale annotations can be replaced with LLM-based
automatic annotations requiring only limited human supervision over reasoning
patterns.

</details>


### [56] [Generation Space Size: Understanding and Calibrating Open-Endedness of LLM Generations](https://arxiv.org/abs/2510.12699)
*Sunny Yu,Ahmad Jabbar,Robert Hawkins,Dan Jurafsky,Myra Cheng*

Main category: cs.CL

TL;DR: 本文提出了生成空间大小(GSS)的概念来解决LLMs在开放生成任务中的两种失败模式：创造性任务输出过于同质化和事实性任务产生多样化但错误的幻觉响应。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在开放生成任务中存在校准问题：对于创造性任务输出过于同质化，而对于事实性任务则产生多样化但错误的幻觉响应。这两种失败模式可以通过生成空间大小(GSS)的概念来统一理解和解决。

Method: 提出了GSSBench任务套件，包含具有真实GSS关系的提示对，用于评估不同指标并理解模型与期望行为的差异。研究发现幻觉检测指标（特别是EigenScore）优于标准多样性和不确定性量化指标。

Result: EigenScore等幻觉检测指标在仅使用模型内部信息的情况下，能够持续优于标准多样性和不确定性量化指标，并提供对模型内部任务表示的可解释洞察。

Conclusion: GSS概念有三个主要应用：检测提示歧义并预测澄清问题以更好地接地、解释推理模型中的过度思考和思考不足、引导模型扩展其生成空间以获得高质量和多样化的输出。

Abstract: Different open-ended generation tasks require different degrees of output
diversity. However, current LLMs are often miscalibrated. They collapse to
overly homogeneous outputs for creative tasks and hallucinate diverse but
incorrect responses for factual tasks. We argue that these two failure modes
are unified by, and can both be addressed by, the notion of effective
generation space size (GSS) -- the set of semantically distinct outputs a model
considers for a prompt. We present GSSBench, a task suite of prompt pairs with
ground-truth GSS relationships to assess different metrics and understand where
models diverge from desired behavior. We find that hallucination detection
metrics, particularly EigenScore, consistently outperform standard diversity
and uncertainty quantification metrics, while using only model internals,
providing interpretable insights into a model's internal task representations.
We demonstrate three applications of GSS: (1) detecting prompt ambiguity and
predicting clarification questions for better grounding, (2) interpreting
overthinking and underthinking in reasoning models, and (3) steering models to
expand their generation space to yield high-quality and diverse outputs.

</details>


### [57] [Omni-Captioner: Data Pipeline, Models, and Benchmark for Omni Detailed Perception](https://arxiv.org/abs/2510.12720)
*Ziyang Ma,Ruiyang Xu,Zhenghao Xing,Yunfei Chu,Yuxuan Wang,Jinzheng He,Jin Xu,Pheng-Ann Heng,Kai Yu,Junyang Lin,Eng Siong Chng,Xie Chen*

Main category: cs.CL

TL;DR: 本文系统研究了全模态详细感知，提出了Omni-Detective数据生成管道来减少幻觉，训练了Audio-Captioner和Omni-Captioner模型，并设计了Omni-Cloze评估基准。


<details>
  <summary>Details</summary>
Motivation: 当前全模态语言模型在捕捉和描述细粒度细节方面的能力仍有局限，存在细节与幻觉共增长的问题。

Method: 提出Omni-Detective代理数据生成管道，集成工具调用来自主生成高质量多模态数据；基于此训练Audio-Captioner和Omni-Captioner模型；设计Omni-Cloze评估基准。

Result: Audio-Captioner在MMAU和MMAR上表现最佳，超越Gemini 2.5 Flash；Omni-Captioner在VDC上达到SOTA，在video-SALMONN 2上实现细节与幻觉的最佳平衡。

Conclusion: Omni-Detective能有效生成高质量详细描述，Omni-Cloze在评估详细描述方面具有优越性，为全模态详细感知研究提供了系统解决方案。

Abstract: Fine-grained perception of multimodal information is critical for advancing
human-AI interaction. With recent progress in audio-visual technologies, Omni
Language Models (OLMs), capable of processing audio and video signals in
parallel, have emerged as a promising paradigm for achieving richer
understanding and reasoning. However, their capacity to capture and describe
fine-grained details remains limited explored. In this work, we present a
systematic and comprehensive investigation of omni detailed perception from the
perspectives of the data pipeline, models, and benchmark. We first identify an
inherent "co-growth" between detail and hallucination in current OLMs. To
address this, we propose Omni-Detective, an agentic data generation pipeline
integrating tool-calling, to autonomously produce highly detailed yet minimally
hallucinatory multimodal data. Based on the data generated with Omni-Detective,
we train two captioning models: Audio-Captioner for audio-only detailed
perception, and Omni-Captioner for audio-visual detailed perception. Under the
cascade evaluation protocol, Audio-Captioner achieves the best performance on
MMAU and MMAR among all open-source models, surpassing Gemini 2.5 Flash and
delivering performance comparable to Gemini 2.5 Pro. On existing detailed
captioning benchmarks, Omni-Captioner sets a new state-of-the-art on VDC and
achieves the best trade-off between detail and hallucination on the
video-SALMONN 2 testset. Given the absence of a dedicated benchmark for omni
detailed perception, we design Omni-Cloze, a novel cloze-style evaluation for
detailed audio, visual, and audio-visual captioning that ensures stable,
efficient, and reliable assessment. Experimental results and analysis
demonstrate the effectiveness of Omni-Detective in generating high-quality
detailed captions, as well as the superiority of Omni-Cloze in evaluating such
detailed captions.

</details>


### [58] [Which Word Orders Facilitate Length Generalization in LMs? An Investigation with GCG-Based Artificial Languages](https://arxiv.org/abs/2510.12722)
*Nadine El-Naggar,Tatsuki Kuribayashi,Ted Briscoe*

Main category: cs.CL

TL;DR: 该研究扩展了语言模型对类型学频率语法偏好的研究，采用广义范畴语法构建更接近自然语言的人工语言，重点关注模型对未见长句的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 扩展现有研究，从两个角度改进：采用更接近自然语言的广义范畴语法，以及更关注语言模型对未见长句的泛化能力。

Method: 使用广义范畴语法构建人工语言，覆盖无界依赖和轻度上下文敏感结构，评估语言模型对未见长句的泛化表现。

Result: 类型学上合理的词序更容易让语言模型进行生产性泛化。

Conclusion: 语言模型确实存在对类型学频率语法属性的归纳偏好，合理的词序结构有助于更好的泛化能力。

Abstract: Whether language models (LMs) have inductive biases that favor typologically
frequent grammatical properties over rare, implausible ones has been
investigated, typically using artificial languages (ALs) (White and Cotterell,
2021; Kuribayashi et al., 2024). In this paper, we extend these works from two
perspectives. First, we extend their context-free AL formalization by adopting
Generalized Categorial Grammar (GCG) (Wood, 2014), which allows ALs to cover
attested but previously overlooked constructions, such as unbounded dependency
and mildly context-sensitive structures. Second, our evaluation focuses more on
the generalization ability of LMs to process unseen longer test sentences.
Thus, our ALs better capture features of natural languages and our experimental
paradigm leads to clearer conclusions -- typologically plausible word orders
tend to be easier for LMs to productively generalize.

</details>


### [59] [Hey, wait a minute: on at-issue sensitivity in Language Models](https://arxiv.org/abs/2510.12740)
*Sanghee J. Kim,Kanishka Misra*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Evaluating the naturalness of dialogue in language models (LMs) is not
trivial: notions of 'naturalness' vary, and scalable quantitative metrics
remain limited. This study leverages the linguistic notion of 'at-issueness' to
assess dialogue naturalness and introduces a new method: Divide, Generate,
Recombine, and Compare (DGRC). DGRC (i) divides a dialogue as a prompt, (ii)
generates continuations for subparts using LMs, (iii) recombines the dialogue
and continuations, and (iv) compares the likelihoods of the recombined
sequences. This approach mitigates bias in linguistic analyses of LMs and
enables systematic testing of discourse-sensitive behavior. Applying DGRC, we
find that LMs prefer to continue dialogue on at-issue content, with this effect
enhanced in instruct-tuned models. They also reduce their at-issue preference
when relevant cues (e.g., "Hey, wait a minute") are present. Although
instruct-tuning does not further amplify this modulation, the pattern reflects
a hallmark of successful dialogue dynamics.

</details>


### [60] [Language Models Model Language](https://arxiv.org/abs/2510.12766)
*Łukasz Borchmann*

Main category: cs.CL

TL;DR: 本文主张从经验主义角度重新审视语言模型，基于Mańczak的语言学框架，将语言定义为所有言说和书写的总和，以使用频率为主要原则，为LLM的设计和评估提供新视角。


<details>
  <summary>Details</summary>
Motivation: 针对当前基于de Saussure和Chomsky理论对LLM的语言学批评过于推测性和非建设性，需要转向更实用的经验主义框架来理解和评估语言模型。

Method: 采用Witold Mańczak的经验主义语言学框架，将语言定义为所有言说和书写的总和，强调使用频率是语言的主要支配原则。

Result: 基于Mańczak框架挑战了先前对LLM的批评，并为语言模型的设计、评估和解释提供了建设性指导。

Conclusion: 经验主义视角为理解和改进语言模型提供了更实用和建设性的框架，超越了传统语言学理论的局限性。

Abstract: Linguistic commentary on LLMs, heavily influenced by the theoretical
frameworks of de Saussure and Chomsky, is often speculative and unproductive.
Critics challenge whether LLMs can legitimately model language, citing the need
for "deep structure" or "grounding" to achieve an idealized linguistic
"competence." We argue for a radical shift in perspective towards the
empiricist principles of Witold Ma\'nczak, a prominent general and historical
linguist. He defines language not as a "system of signs" or a "computational
system of the brain" but as the totality of all that is said and written. Above
all, he identifies frequency of use of particular language elements as
language's primary governing principle. Using his framework, we challenge prior
critiques of LLMs and provide a constructive guide for designing, evaluating,
and interpreting language models.

</details>


### [61] [Dr.LLM: Dynamic Layer Routing in LLMs](https://arxiv.org/abs/2510.12773)
*Ahmed Heakl,Martin Gubri,Salman Khan,Sangdoo Yun,Seong Joon Oh*

Main category: cs.CL

TL;DR: Dr.LLM是一个可改造的动态层路由框架，通过轻量级路由器决定跳过、执行或重复Transformer层，在保持或提高精度的同时节省计算资源。


<details>
  <summary>Details</summary>
Motivation: 传统LLM对所有token都经过所有Transformer层，导致简单查询计算浪费，复杂查询推理深度不足。现有自适应深度方法依赖昂贵的推理时搜索、架构修改或大规模重训练，且往往牺牲精度。

Method: 使用蒙特卡洛树搜索(MCTS)生成高质量层配置作为监督信号，训练轻量级路由器。采用窗口池化稳定路由，焦点损失与类别平衡，瓶颈MLP路由器设计，解决类别不平衡和长序列问题。

Result: 在ARC和DART任务上精度提升达+3.4%，平均每例节省5层。路由器泛化到多个领域任务仅损失0.85%精度，同时保持效率，比现有路由方法提升达+7.7%。

Conclusion: Dr.LLM证明显式监督的路由器可以在不改变基础权重的情况下，为冻结LLM提供预算感知、精度驱动的推理能力。

Abstract: Large Language Models (LLMs) process every token through all layers of a
transformer stack, causing wasted computation on simple queries and
insufficient flexibility for harder ones that need deeper reasoning.
Adaptive-depth methods can improve efficiency, but prior approaches rely on
costly inference-time search, architectural changes, or large-scale retraining,
and in practice often degrade accuracy despite efficiency gains. We introduce
Dr.LLM, Dynamic routing of Layers for LLMs, a retrofittable framework that
equips pretrained models with lightweight per-layer routers deciding to skip,
execute, or repeat a block. Routers are trained with explicit supervision:
using Monte Carlo Tree Search (MCTS), we derive high-quality layer
configurations that preserve or improve accuracy under a compute budget. Our
design, windowed pooling for stable routing, focal loss with class balancing,
and bottleneck MLP routers, ensures robustness under class imbalance and long
sequences. On ARC (logic) and DART (math), Dr.LLM improves accuracy by up to
+3.4%p while saving 5 layers per example on average. Routers generalize to
out-of-domain tasks (MMLU, GSM8k, AIME, TruthfulQA, SQuADv2, GPQA, PIQA,
AGIEval) with only 0.85% accuracy drop while retaining efficiency, and
outperform prior routing methods by up to +7.7%p. Overall, Dr.LLM shows that
explicitly supervised routers retrofit frozen LLMs for budget-aware,
accuracy-driven inference without altering base weights.

</details>


### [62] [Cost Analysis of Human-corrected Transcription for Predominately Oral Languages](https://arxiv.org/abs/2510.12781)
*Yacouba Diarra,Nouhoum Souleymane Coulibaly,Michael Leventhal*

Main category: cs.CL

TL;DR: 研究低资源语言（特别是班巴拉语）语音数据标注的实际人力成本，发现实验室条件下转录1小时语音需要30小时人工，实地条件下需要36小时


<details>
  <summary>Details</summary>
Motivation: 为低资源语言创建语音数据集是一个关键但了解不足的挑战，特别是关于实际人力成本的问题

Method: 通过为期一个月的实地研究，让10名母语转录员修正53小时班巴拉语语音数据的ASR生成转录

Result: 实验室条件下转录1小时语音平均需要30小时人工，实地条件下需要36小时人工

Conclusion: 该研究为具有类似特征的大类语言创建NLP资源提供了基准和实用见解

Abstract: Creating speech datasets for low-resource languages is a critical yet poorly
understood challenge, particularly regarding the actual cost in human labor.
This paper investigates the time and complexity required to produce
high-quality annotated speech data for a subset of low-resource languages, low
literacy Predominately Oral Languages, focusing on Bambara, a Manding language
of Mali. Through a one-month field study involving ten transcribers with native
proficiency, we analyze the correction of ASR-generated transcriptions of 53
hours of Bambara voice data. We report that it takes, on average, 30 hours of
human labor to accurately transcribe one hour of speech data under laboratory
conditions and 36 hours under field conditions. The study provides a baseline
and practical insights for a large class of languages with comparable profiles
undertaking the creation of NLP resources.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [63] [Scaling Law in LLM Simulated Personality: More Detailed and Realistic Persona Profile Is All You Need](https://arxiv.org/abs/2510.11734)
*Yuqi Bai,Tianyu Huang,Kun Sun,Yuting Chen*

Main category: cs.CY

TL;DR: 该研究开发了一个端到端的评估框架，用于评估大语言模型在虚拟人格角色扮演中模拟人类个性的能力，包括个体层面的稳定性和可识别性分析，以及群体层面的渐进人格曲线分析。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在模拟社会实验中模拟人类个性的能力，解决传统心理测量方法在捕捉LLM低水平模拟改进趋势方面的局限性。

Method: 提出了对传统心理测量方法（CFA和构念效度）的重要修改，开发了包含个体层面和群体层面分析的系统性评估框架，特别关注人格细节在个性模拟质量中的关键作用。

Result: 实证证明了人格细节在个性模拟质量中的关键作用，识别了人格配置的边际效用效应，特别是在LLM个性模拟中发现了缩放定律。

Conclusion: 为在大语言模型应用于社会科学实验中提供了操作性评估指标和理论基础，展示了LLM在虚拟人格角色扮演中的潜力和局限性。

Abstract: This research focuses on using large language models (LLMs) to simulate
social experiments, exploring their ability to emulate human personality in
virtual persona role-playing. The research develops an end-to-end evaluation
framework, including individual-level analysis of stability and
identifiability, as well as population-level analysis called progressive
personality curves to examine the veracity and consistency of LLMs in
simulating human personality. Methodologically, this research proposes
important modifications to traditional psychometric approaches (CFA and
construct validity) which are unable to capture improvement trends in LLMs at
their current low-level simulation, potentially leading to remature rejection
or methodological misalignment. The main contributions of this research are:
proposing a systematic framework for LLM virtual personality evaluation;
empirically demonstrating the critical role of persona detail in personality
simulation quality; and identifying marginal utility effects of persona
profiles, especially a Scaling Law in LLM personality simulation, offering
operational evaluation metrics and a theoretical foundation for applying large
language models in social science experiments.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [64] [Deep Research Brings Deeper Harm](https://arxiv.org/abs/2510.11851)
*Shuo Chen,Zonggen Li,Zhen Han,Bailan He,Tong Liu,Haokun Chen,Georg Groh,Philip Torr,Volker Tresp,Jindong Gu*

Main category: cs.CR

TL;DR: Deep Research (DR) agents built on LLMs can generate dangerous forbidden knowledge reports, even when standalone LLMs reject harmful queries. The paper proposes two jailbreak strategies (Plan Injection and Intent Hijack) that exploit DR agents' research capabilities, revealing systemic vulnerabilities in multi-step planning systems.


<details>
  <summary>Details</summary>
Motivation: DR agents can produce detailed forbidden knowledge reports in high-stakes domains like biosecurity, creating elevated risks that existing LLM jailbreak methods cannot address. The motivation is to expose these unique vulnerabilities in DR agents' research capabilities.

Method: Proposed two novel jailbreak strategies: Plan Injection (injecting malicious sub-goals into agent's plan) and Intent Hijack (reframing harmful queries as academic research questions). Conducted experiments across different LLMs and safety benchmarks.

Result: Three key findings: (1) LLM alignment fails in DR agents, (2) multi-step planning weakens alignment, creating systemic vulnerabilities, (3) DR agents bypass refusals and produce more coherent, professional, and dangerous content than standalone LLMs.

Conclusion: DR agents exhibit fundamental misalignment that requires tailored alignment techniques beyond prompt-level safeguards. The research demonstrates the need for better safety measures specifically designed for multi-step research agents.

Abstract: Deep Research (DR) agents built on Large Language Models (LLMs) can perform
complex, multi-step research by decomposing tasks, retrieving online
information, and synthesizing detailed reports. However, the misuse of LLMs
with such powerful capabilities can lead to even greater risks. This is
especially concerning in high-stakes and knowledge-intensive domains such as
biosecurity, where DR can generate a professional report containing detailed
forbidden knowledge. Unfortunately, we have found such risks in practice:
simply submitting a harmful query, which a standalone LLM directly rejects, can
elicit a detailed and dangerous report from DR agents. This highlights the
elevated risks and underscores the need for a deeper safety analysis. Yet,
jailbreak methods designed for LLMs fall short in exposing such unique risks,
as they do not target the research ability of DR agents. To address this gap,
we propose two novel jailbreak strategies: Plan Injection, which injects
malicious sub-goals into the agent's plan; and Intent Hijack, which reframes
harmful queries as academic research questions. We conducted extensive
experiments across different LLMs and various safety benchmarks, including
general and biosecurity forbidden prompts. These experiments reveal 3 key
findings: (1) Alignment of the LLMs often fail in DR agents, where harmful
prompts framed in academic terms can hijack agent intent; (2) Multi-step
planning and execution weaken the alignment, revealing systemic vulnerabilities
that prompt-level safeguards cannot address; (3) DR agents not only bypass
refusals but also produce more coherent, professional, and dangerous content,
compared with standalone LLMs. These results demonstrate a fundamental
misalignment in DR agents and call for better alignment techniques tailored to
DR agents. Code and datasets are available at
https://chenxshuo.github.io/deeper-harm.

</details>


### [65] [HackWorld: Evaluating Computer-Use Agents on Exploiting Web Application Vulnerabilities](https://arxiv.org/abs/2510.12200)
*Xiaoxue Ren,Penghao Jiang,Kaixin Li,Zhiyong Huang,Xiaoning Du,Jiaojiao Jiang,Zhenchang Xing,Jiamou Sun,Terry Yue Zhuo*

Main category: cs.CR

TL;DR: HackWorld是首个通过视觉交互系统评估计算机使用代理(CUAs)利用Web应用漏洞能力的框架，包含36个真实应用和多种漏洞类型，评估显示当前CUAs的漏洞利用率低于12%，存在多步骤攻击规划失败和安全工具误用等问题。


<details>
  <summary>Details</summary>
Motivation: 传统渗透测试成本高且难以扩展，而现代Web应用需要视觉理解和动态内容处理能力，但CUAs通过图形界面发现和利用漏洞的能力尚未被充分探索。

Method: 采用Capture-the-Flag(CTF)设置，在包含36个真实应用、11个框架和7种语言的环境中测试CUAs识别和利用漏洞的能力，重点关注注入漏洞、认证绕过和不安全输入处理等现实缺陷。

Result: 评估显示当前最先进CUAs的漏洞利用率低于12%，网络安全意识低，经常在多步骤攻击规划中失败，并误用安全工具。

Conclusion: 结果揭示了CUAs在Web安全环境中的当前局限性，为开发更具安全意识的代理提供了机会，这些代理能够有效检测和利用漏洞。

Abstract: Web applications are prime targets for cyberattacks as gateways to critical
services and sensitive data. Traditional penetration testing is costly and
expertise-intensive, making it difficult to scale with the growing web
ecosystem. While language model agents show promise in cybersecurity, modern
web applications demand visual understanding, dynamic content handling, and
multi-step interactions that only computer-use agents (CUAs) can perform. Yet,
their ability to discover and exploit vulnerabilities through graphical
interfaces remains largely unexplored. We present HackWorld, the first
framework for systematically evaluating CUAs' capabilities to exploit web
application vulnerabilities via visual interaction. Unlike sanitized
benchmarks, HackWorld includes 36 real-world applications across 11 frameworks
and 7 languages, featuring realistic flaws such as injection vulnerabilities,
authentication bypasses, and unsafe input handling. Using a Capture-the-Flag
(CTF) setup, it tests CUAs' capacity to identify and exploit these weaknesses
while navigating complex web interfaces. Evaluation of state-of-the-art CUAs
reveals concerning trends: exploitation rates below 12% and low cybersecurity
awareness. CUAs often fail at multi-step attack planning and misuse security
tools. These results expose the current limitations of CUAs in web security
contexts and highlight opportunities for developing more security-aware agents
capable of effective vulnerability detection and exploitation.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [66] [Holistic Agent Leaderboard: The Missing Infrastructure for AI Agent Evaluation](https://arxiv.org/abs/2510.11977)
*Sayash Kapoor,Benedikt Stroebl,Peter Kirgis,Nitya Nadgir,Zachary S Siegel,Boyi Wei,Tianci Xue,Ziru Chen,Felix Chen,Saiteja Utpala,Franck Ndzomga,Dheeraj Oruganty,Sophie Luskin,Kangheng Liu,Botao Yu,Amit Arora,Dongyoon Hahm,Harsh Trivedi,Huan Sun,Juyong Lee,Tengjun Jin,Yifan Mai,Yifei Zhou,Yuxuan Zhu,Rishi Bommasani,Daniel Kang,Dawn Song,Peter Henderson,Yu Su,Percy Liang,Arvind Narayanan*

Main category: cs.AI

TL;DR: 提出了Holistic Agent Leaderboard (HAL)来解决AI智能体评估中的挑战，包括标准化评估框架、三维分析方法和LLM辅助日志检查，旨在推动从基准测试表现转向真实世界可靠性。


<details>
  <summary>Details</summary>
Motivation: AI智能体评估面临诸多挑战，影响了对智能体真实性能的理解，需要更全面和可靠的评估方法。

Method: 1. 标准化评估框架，在数百个虚拟机上进行并行评估，将评估时间从数周缩短到数小时；2. 进行模型、支架和基准的三维分析；3. 使用LLM辅助日志检查发现未报告的行为。

Result: 验证了评估框架，进行了21,730次智能体运行，涵盖9个模型和9个基准，发现了一些意外洞察，如更高推理努力反而降低准确性，以及智能体在任务中的异常行为。

Conclusion: 通过标准化智能体评估方法并解决常见陷阱，希望推动研究从追求基准测试高分转向开发在真实世界中可靠的智能体。

Abstract: AI agents have been developed for complex real-world tasks from coding to
customer service. But AI agent evaluations suffer from many challenges that
undermine our understanding of how well agents really work. We introduce the
Holistic Agent Leaderboard (HAL) to address these challenges. We make three
main contributions. First, we provide a standardized evaluation harness that
orchestrates parallel evaluations across hundreds of VMs, reducing evaluation
time from weeks to hours while eliminating common implementation bugs. Second,
we conduct three-dimensional analysis spanning models, scaffolds, and
benchmarks. We validate the harness by conducting 21,730 agent rollouts across
9 models and 9 benchmarks in coding, web navigation, science, and customer
service with a total cost of about $40,000. Our analysis reveals surprising
insights, such as higher reasoning effort reducing accuracy in the majority of
runs. Third, we use LLM-aided log inspection to uncover previously unreported
behaviors, such as searching for the benchmark on HuggingFace instead of
solving a task, or misusing credit cards in flight booking tasks. We share all
agent logs, comprising 2.5B tokens of language model calls, to incentivize
further research into agent behavior. By standardizing how the field evaluates
agents and addressing common pitfalls in agent evaluation, we hope to shift the
focus from agents that ace benchmarks to agents that work reliably in the real
world.

</details>


### [67] [ThinkPilot: Steering Reasoning Models via Automated Think-prefixes Optimization](https://arxiv.org/abs/2510.12063)
*Sunzhu Li,Zhiyu Lin,Shuling Yang,Jiale Zhao,Wei Chen*

Main category: cs.AI

TL;DR: ThinkPilot是一个无需训练即可自动优化大型推理模型（LRMs）推理性能的框架，通过进化过程生成think-prefixes指令来引导模型实现更优表现。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型虽然强大，但仍存在推理效率低下和偏离目标的问题。现有的无需训练方法要么局限于僵化的启发式规则，要么只能提供描述性但不可操作的分析。

Method: 使用进化过程生成think-prefixes（思考前缀），这些指令基于推理行为分类学进行演化，引导模型朝着更优性能发展。

Result: ThinkPilot显著改善了准确性与推理长度的权衡关系，大幅提升了安全性（如将DeepSeek-R1-Distill-Qwen-32B的StrongREJECT分数从27.0%降至0.7），增强了指令遵循能力，并能与现有基于训练的方法协同工作。

Conclusion: think-prefixes能够可靠地控制LRMs的推理行为，不同任务对特定行为分布有强烈偏好。通过自动识别和激发这些行为，ThinkPilot提供了一个可推广的框架来使LRMs的推理与任务需求对齐。

Abstract: Large Reasoning Models (LRMs) are powerful, but they still suffer from
inefficient and off-target reasoning. Currently, training-free methods are
limited to either rigid heuristics or descriptive, non-actionable analyses. In
this paper, we introduce ThinkPilot, a training-free framework that
automatically optimizes LRMs reasoning. It uses an evolutionary process to
generate think-prefixes, which are instructions that evolve driven by a
taxonomy of reasoning behaviors to guide models toward superior performance.
Extensive experiments demonstrate ThinkPilot's broad effectiveness: it
significantly improves the accuracy-length trade-off for efficient reasoning,
drastically improves safety (for example, cutting the StrongREJECT score of
DeepSeek-R1-Distill-Qwen-32B from 27.0% to 0.7), and enhances instruction
following. It also synergizes with existing training-based methods. Our
analysis reveals that think-prefixes can reliably control LRMs' reasoning
behaviors, and that different tasks have strong preferences for specific
behavioral distributions. By automatically identifying and eliciting these
behaviors, ThinkPilot provides a generalizable framework for aligning LRMs
reasoning with task demands. Data and code are available at
https://github.com/teqkilla/ThinkPilot

</details>


### [68] [One Life to Learn: Inferring Symbolic World Models for Stochastic Environments from Unguided Exploration](https://arxiv.org/abs/2510.12088)
*Zaid Khan,Archiki Prasad,Elias Stengel-Eskin,Jaemin Cho,Mohit Bansal*

Main category: cs.AI

TL;DR: OneLife框架通过条件激活的程序化法则在概率编程框架中建模世界动态，能够在复杂随机环境中从有限的非指导交互中学习关键环境动态。


<details>
  <summary>Details</summary>
Motivation: 解决在复杂、随机环境中，智能体只有"一次生命"来探索敌对环境且无人指导的现实挑战性场景下的符号世界建模问题。

Method: 使用条件激活的程序化法则，每个法则通过前提-效果结构运行，在相关世界状态中激活，创建动态计算图，仅通过相关法则路由推理和优化。

Result: 在23个测试场景中的16个场景上优于强基线，能够从最小化、非指导的交互中成功学习关键环境动态，模拟推演成功识别出更优策略。

Conclusion: 为自主构建未知复杂环境的程序化世界模型奠定了基础。

Abstract: Symbolic world modeling requires inferring and representing an environment's
transitional dynamics as an executable program. Prior work has focused on
largely deterministic environments with abundant interaction data, simple
mechanics, and human guidance. We address a more realistic and challenging
setting, learning in a complex, stochastic environment where the agent has only
"one life" to explore a hostile environment without human guidance. We
introduce OneLife, a framework that models world dynamics through
conditionally-activated programmatic laws within a probabilistic programming
framework. Each law operates through a precondition-effect structure,
activating in relevant world states. This creates a dynamic computation graph
that routes inference and optimization only through relevant laws, avoiding
scaling challenges when all laws contribute to predictions about a complex,
hierarchical state, and enabling the learning of stochastic dynamics even with
sparse rule activation. To evaluate our approach under these demanding
constraints, we introduce a new evaluation protocol that measures (a) state
ranking, the ability to distinguish plausible future states from implausible
ones, and (b) state fidelity, the ability to generate future states that
closely resemble reality. We develop and evaluate our framework on Crafter-OO,
our reimplementation of the Crafter environment that exposes a structured,
object-oriented symbolic state and a pure transition function that operates on
that state alone. OneLife can successfully learn key environment dynamics from
minimal, unguided interaction, outperforming a strong baseline on 16 out of 23
scenarios tested. We also test OneLife's planning ability, with simulated
rollouts successfully identifying superior strategies. Our work establishes a
foundation for autonomously constructing programmatic world models of unknown,
complex environments.

</details>


### [69] [Precise Attribute Intensity Control in Large Language Models via Targeted Representation Editing](https://arxiv.org/abs/2510.12121)
*Rongzhi Zhang,Liqin Ye,Yuzhao Heng,Xiang Chen,Tong Yu,Lingkai Kong,Sudheer Chava,Chao Zhang*

Main category: cs.AI

TL;DR: 提出了一种精确控制LLM输出属性强度的方法，通过目标达成问题重构、时序差分学习训练轻量值函数和基于梯度的隐表示干预，实现用户指定属性强度的精确控制。


<details>
  <summary>Details</summary>
Motivation: 当前LLM对齐方法只能提供方向性或开放式指导，无法可靠实现精确的属性强度控制，这限制了AI系统适应不同用户期望的能力。

Method: 1) 将精确属性强度控制重构为目标达成问题；2) 通过时序差分学习训练轻量值函数预测属性强度分数；3) 使用基于梯度的隐表示干预精确导航到特定属性强度目标。

Result: 在LLaMA-3.2-3b和Phi-4-mini上的实验证实该方法能够以高精度将文本生成引导到用户指定的属性强度，并在三个下游任务中展示了效率提升。

Conclusion: 该方法实现了对属性强度的细粒度连续控制，超越了简单的方向对齐，为偏好数据合成、帕累托前沿近似和优化以及对齐行为蒸馏等任务提供了有效解决方案。

Abstract: Precise attribute intensity control--generating Large Language Model (LLM)
outputs with specific, user-defined attribute intensities--is crucial for AI
systems adaptable to diverse user expectations. Current LLM alignment methods,
however, typically provide only directional or open-ended guidance, failing to
reliably achieve exact attribute intensities. We address this limitation with
three key designs: (1) reformulating precise attribute intensity control as a
target-reaching problem, rather than simple maximization; (2) training a
lightweight value function via temporal-difference learning to predict final
attribute intensity scores from partial generations, thereby steering LLM
outputs; and (3) employing gradient-based interventions on hidden
representations to navigate the model precisely towards specific attribute
intensity targets. Our method enables fine-grained, continuous control over
attribute intensities, moving beyond simple directional alignment. Experiments
on LLaMA-3.2-3b and Phi-4-mini confirm our method's ability to steer text
generation to user-specified attribute intensities with high accuracy. Finally,
we demonstrate efficiency enhancements across three downstream tasks:
preference data synthesis, Pareto frontier approximation and optimization, and
distillation of aligned behaviors for intervention-free inference. Our code is
available on https://github.com/Pre-Control/pre-control

</details>


### [70] [Evolution of meta's llama models and parameter-efficient fine-tuning of large language models: a survey](https://arxiv.org/abs/2510.12178)
*Abdulhady Abas Abdullah,Arkaitz Zubiaga,Seyedali Mirjalili,Amir H. Gandomi,Fatemeh Daneshfar,Mohammadsadra Amini,Alan Salam Mohammed,Hadi Veisi*

Main category: cs.AI

TL;DR: 本文综述了Meta AI的LLaMA系列模型从LLaMA 1到LLaMA 4的快速演进，以及为这些模型开发的参数高效微调(PEFT)方法，包括模型架构、性能特征和五种PEFT方法的应用分析。


<details>
  <summary>Details</summary>
Motivation: 为对LLaMA模型和高效微调策略感兴趣的ML研究者和从业者提供一站式资源，系统梳理LLaMA系列模型的演进和参数高效微调方法的发展。

Method: 首先描述LLaMA基础模型家族(7B-65B到288B参数)及其架构，然后介绍PEFT概念，详细分析五种应用于LLaMA的PEFT方法：LoRA、LLaMA-Adapter V1和V2、LLaMA-Excitor、QLoRA，包括机制、参数节省和应用案例。

Result: 提供了模型和适配器架构、参数数量和基准测试结果的结构化分析，展示了微调后的LLaMA模型在某些情况下优于更大基线模型的实例，并考察了LLaMA模型和PEFT在现实世界应用中的成功案例。

Conclusion: LLaMA系列模型和PEFT方法在参数效率和性能方面取得了显著进展，但仍面临扩展到更大上下文和改进鲁棒性等挑战，为未来研究指明了方向。

Abstract: This review surveys the rapid evolution of Meta AI's LLaMA (Large Language
Model Meta AI) series - from LLaMA 1 through LLaMA 4 and the specialized
parameter-efficient fine-tuning (PEFT) methods developed for these models. We
first describe the LLaMA family of foundation models (7B-65B to 288B
parameters), their architectures (including native multimodal and
Mixtureof-Experts variants), and key performance characteristics. We then
describe and discuss the concept of PEFT, which adapts large pre-trained models
by updating only a small subset of parameters, and review five PEFT methods
that have been applied to LLaMA: LoRA (Low-Rank Adaptation), LLaMA-Adapter V1
and V2, LLaMA-Excitor, and QLoRA (Quantized LoRA). We discuss each method's
mechanism, parameter savings, and example application to LLaMA (e.g.,
instruction tuning, multimodal tasks). We provide structured discussion and
analysis of model and adapter architectures, parameter counts, and benchmark
results (including examples where fine-tuned LLaMA models outperform larger
baselines). Finally, we examine real-world use cases where LLaMA-based models
and PEFT have been successfully applied (e.g., legal and medical domains), and
we discuss ongoing challenges and future research directions (such as scaling
to even larger contexts and improving robustness). This survey paper provides a
one-stop resource for ML researchers and practitioners interested in LLaMA
models and efficient fine-tuning strategies.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [71] [Who is a Better Matchmaker? Human vs. Algorithmic Judge Assignment in a High-Stakes Startup Competition](https://arxiv.org/abs/2510.12692)
*Sarina Xi,Orelia Pi,Miaomiao Zhang,Becca Xiong,Jacqueline Ng Lane,Nihar B. Shah*

Main category: cs.HC

TL;DR: AI算法HLSE在哈佛创新挑战赛的评委分配任务中，与人类专家分配质量相当，但效率更高。


<details>
  <summary>Details</summary>
Motivation: 研究AI算法与人类专家在需要语义理解和领域知识的复杂决策任务中的表现差异，特别是在评委分配这一实际应用场景中。

Method: 开发了混合词汇-语义相似度集成算法(HLSE)，并在哈佛校长创新挑战赛中部署，与人类专家分配进行对比评估，收集了309个评委-创业项目对的匹配质量评分。

Result: 使用Mann-Whitney U检验发现两种方法在分配质量上无显著差异(AUC=0.48, p=0.40)；算法匹配平均评分3.90，人工匹配3.94（5分制）；算法将原本需要一周的人工分配时间缩短至几小时。

Conclusion: HLSE算法在评委分配任务中达到了人类专家水平的匹配质量，同时具有更好的可扩展性和效率，展示了AI驱动解决方案在高风险环境中支持和增强人类决策的潜力。

Abstract: There is growing interest in applying artificial intelligence (AI) to
automate and support complex decision-making tasks. However, it remains unclear
how algorithms compare to human judgment in contexts requiring semantic
understanding and domain expertise. We examine this in the context of the judge
assignment problem, matching submissions to suitably qualified judges.
Specifically, we tackled this problem at the Harvard President's Innovation
Challenge, the university's premier venture competition awarding over \$500,000
to student and alumni startups. This represents a real-world environment where
high-quality judge assignment is essential. We developed an AI-based
judge-assignment algorithm, Hybrid Lexical-Semantic Similarity Ensemble (HLSE),
and deployed it at the competition. We then evaluated its performance against
human expert assignments using blinded match-quality scores from judges on
$309$ judge-venture pairs. Using a Mann-Whitney U statistic based test, we
found no statistically significant difference in assignment quality between the
two approaches ($AUC=0.48, p=0.40$); on average, algorithmic matches are rated
$3.90$ and manual matches $3.94$ on a 5-point scale, where 5 indicates an
excellent match. Furthermore, manual assignments that previously required a
full week could be automated in several hours by the algorithm during
deployment. These results demonstrate that HLSE achieves human-expert-level
matching quality while offering greater scalability and efficiency,
underscoring the potential of AI-driven solutions to support and enhance human
decision-making for judge assignment in high-stakes settings.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [72] [Task-Aware Reduction for Scalable LLM-Database Systems](https://arxiv.org/abs/2510.11813)
*Marcus Emmanuel Barnes,Taher A. Ghaleb,Safwat Hassan*

Main category: cs.SE

TL;DR: 本文主张将LLM的token预算视为注意力预算，将任务感知的文本缩减作为语言-数据系统的核心设计原则，以解决数据密集型工作流中文本数据量大、冗长和嘈杂的问题。


<details>
  <summary>Details</summary>
Motivation: LLM在数据密集型工作流中的应用受到真实世界文本数据（如日志、遥测数据）量大、冗长和嘈杂的限制。直接将这些数据输入LLM成本高、环境不可持续，且与任务目标不匹配。

Method: 将输入侧缩减视为注意力分配而非压缩，优先考虑与下游任务最相关的信息。提出构建基准、设计自适应缩减管道、将token预算感知预处理集成到数据库和检索系统中的研究挑战。

Result: 提出了一个新的设计范式，将文本缩减作为语言-数据系统的核心原则，以更有效地利用LLM的注意力资源。

Conclusion: 通过将稀缺的注意力资源引导到嘈杂数据密集型工作流中有意义的信号上，可以实现可扩展、准确和可持续的LLM-数据集成。

Abstract: Large Language Models (LLMs) are increasingly applied to data-intensive
workflows, from database querying to developer observability. Yet the
effectiveness of these systems is constrained by the volume, verbosity, and
noise of real-world text-rich data such as logs, telemetry, and monitoring
streams. Feeding such data directly into LLMs is costly, environmentally
unsustainable, and often misaligned with task objectives. Parallel efforts in
LLM efficiency have focused on model- or architecture-level optimizations, but
the challenge of reducing upstream input verbosity remains underexplored. In
this paper, we argue for treating the token budget of an LLM as an attention
budget and elevating task-aware text reduction as a first-class design
principle for language -- data systems. We position input-side reduction not as
compression, but as attention allocation: prioritizing information most
relevant to downstream tasks. We outline open research challenges for building
benchmarks, designing adaptive reduction pipelines, and integrating
token-budget--aware preprocessing into database and retrieval systems. Our
vision is to channel scarce attention resources toward meaningful signals in
noisy, data-intensive workflows, enabling scalable, accurate, and sustainable
LLM--data integration.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [73] [Don't Walk the Line: Boundary Guidance for Filtered Generation](https://arxiv.org/abs/2510.11834)
*Sarah Ball,Andreas Haupt*

Main category: cs.LG

TL;DR: 提出Boundary Guidance方法，通过强化学习微调生成模型，使其远离安全分类器的决策边界，从而同时提升安全性和实用性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过微调生成模型来减少被安全分类器过滤的概率，但这往往使模型产生靠近分类器决策边界的样本，增加了假阳性和假阴性。

Method: 使用强化学习微调方法，明确引导生成远离分类器的边界区域。

Result: 在越狱和模糊提示的基准测试中，Boundary Guidance通过LLM-as-a-Judge评估，在安全性和实用性方面均有提升。

Conclusion: 该方法在不同模型规模和奖励设计下都表现出鲁棒性，能有效改善生成模型的安全过滤效果。

Abstract: Generative models are increasingly paired with safety classifiers that filter
harmful or undesirable outputs. A common strategy is to fine-tune the generator
to reduce the probability of being filtered, but this can be suboptimal: it
often pushes the model toward producing samples near the classifier's decision
boundary, increasing both false positives and false negatives. We propose
Boundary Guidance, a reinforcement learning fine-tuning method that explicitly
steers generation away from the classifier's margin. On a benchmark of
jailbreak and ambiguous prompts, Boundary Guidance improves both the safety and
the utility of outputs, as judged by LLM-as-a-Judge evaluations. Comprehensive
ablations across model scales and reward designs demonstrate the robustness of
our approach.

</details>


### [74] [Balancing Synthetic Data and Replay for Enhancing Task-Specific Capabilities](https://arxiv.org/abs/2510.11842)
*Urs Spiegelhalter,Jörg K. H. Franke,Frank Hutter*

Main category: cs.LG

TL;DR: 本文研究了语言模型在持续预训练中如何平衡新任务学习和避免灾难性遗忘的问题，通过系统实验找到了在计算预算约束下最优的回放比例配置。


<details>
  <summary>Details</summary>
Motivation: 语言模型在适应新任务时面临基本权衡：既要学习新能力，又要避免对现有知识的灾难性遗忘。虽然已有研究探索了合成数据生成技术，但在计算约束下平衡任务性能和知识保留的最佳回放比例仍不清楚。

Method: 使用bAbI推理任务作为目标，应用合成数据生成技术，系统评估不同总token预算和回放比例配置，分析它们对任务掌握和通用知识保留的影响。

Result: 实验揭示了一个最优配置，能够平衡任务特定性能和通用知识保留。基于这些发现，提供了基于计算预算选择回放比例的实证指导原则。

Conclusion: 研究提供了实用的指导原则，使实践者能够以显著降低的训练成本实现强大的任务适应能力。

Abstract: Adapting language models to new tasks through continued pretraining faces a
fundamental trade-off: models must learn new capabilities while avoiding
catastrophic forgetting of existing knowledge. While prior work has studied
synthetic data generation techniques, the optimal replay ratios for balancing
task performance and knowledge retention under computational constraints remain
poorly understood. We present a comprehensive empirical study investigating the
interplay between replay ratio configuration and computational budget when
adapting language models to new tasks. Using the bAbI reasoning tasks as our
target objective, we apply synthetic data generation and systematically
evaluate different total token budgets and replay ratio configurations. We
analyze their effects on both task mastery and general knowledge retention. Our
experiments reveal an optimal configuration that balances task-specific
performance with general knowledge retention. Based on our findings, we provide
empirically-grounded guidelines for selecting replay ratios based on
computational budget, enabling practitioners to achieve strong task adaptation
with significantly reduced training costs.

</details>


### [75] [Demystifying Hybrid Thinking: Can LLMs Truly Switch Between Think and No-Think?](https://arxiv.org/abs/2510.12680)
*Shouren Wang,Wang Yang,Xianxuan Long,Qifan Wang,Vipin Chaudhary,Xiaotian Han*

Main category: cs.LG

TL;DR: 本文分析了当前混合思维LLMs的模式分离问题，提出了影响可控性的四个关键因素，并设计了一种训练方法，显著减少了无思维模式中的推理行为泄露。


<details>
  <summary>Details</summary>
Motivation: 当前混合思维LLMs在无思维模式中仍会出现推理行为泄露，影响了效率和可控性，需要理解并解决这一问题。

Method: 通过实验分析影响可控性的因素，提出包含四个关键因素的实用训练方案：更大数据规模、使用不同问题的思维和无思维答案、适度增加无思维数据量、采用两阶段训练策略。

Result: 相比标准训练，该方法在保持两种模式准确率的同时，显著减少了无思维模式的输出长度（从1085降至585）和推理支持性标记的出现次数（从5917降至522）。

Conclusion: 研究揭示了当前混合思维的局限性，为增强其可控性提供了方向和实用方法。

Abstract: Hybrid thinking enables LLMs to switch between reasoning and direct
answering, offering a balance between efficiency and reasoning capability. Yet
our experiments reveal that current hybrid thinking LLMs only achieve partial
mode separation: reasoning behaviors often leak into the no-think mode. To
understand and mitigate this, we analyze the factors influencing
controllability and identify four that matter most: (1) larger data scale, (2)
using think and no-think answers from different questions rather than the same
question, (3) a moderate increase in no-think data number, and (4) a two-phase
strategy that first trains reasoning ability and then applies hybrid think
training. Building on these findings, we propose a practical recipe that,
compared to standard training, can maintain accuracy in both modes while
significantly reducing no-think output length (from $1085$ to $585$ on MATH500)
and occurrences of reasoning-supportive tokens such as ``\texttt{wait}'' (from
$5917$ to $522$ on MATH500). Our findings highlight the limitations of current
hybrid thinking and offer directions for strengthening its controllability.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [76] [DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation](https://arxiv.org/abs/2510.12210)
*Yakun Song,Xiaobin Zhuang,Jiawei Chen,Zhikang Niu,Guanrou Yang,Chenpeng Du,Zhuo Chen,Yuping Wang,Yuxuan Wang,Xie Chen*

Main category: eess.AS

TL;DR: DISTAR是一个零样本文本转语音框架，在离散残差向量量化代码空间中结合自回归语言模型和掩码扩散模型，实现高质量长音频合成，具有鲁棒性、可控性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决现有自回归草图生成器与扩散精炼器结合方法在分布偏移下的脆弱性和可控性有限的问题。

Method: 在离散RVQ代码空间中，使用自回归语言模型生成块级草图，然后通过并行掩码扩散填充完成下一个块，实现块级并行化长音频合成。

Result: DISTAR在鲁棒性、自然度和说话人/风格一致性方面超越现有零样本TTS系统，同时保持丰富的输出多样性。

Conclusion: DISTAR框架通过离散代码空间和AR-扩散紧密耦合，实现了高质量、可控且计算高效的零样本文本转语音合成。

Abstract: Recent attempts to interleave autoregressive (AR) sketchers with
diffusion-based refiners over continuous speech representations have shown
promise, but they remain brittle under distribution shift and offer limited
levers for controllability. We introduce DISTAR, a zero-shot text-to-speech
framework that operates entirely in a discrete residual vector quantization
(RVQ) code space and tightly couples an AR language model with a masked
diffusion model, without forced alignment or a duration predictor. Concretely,
DISTAR drafts block-level RVQ tokens with an AR language model and then
performs parallel masked-diffusion infilling conditioned on the draft to
complete the next block, yielding long-form synthesis with blockwise
parallelism while mitigating classic AR exposure bias. The discrete code space
affords explicit control at inference: DISTAR produces high-quality audio under
both greedy and sample-based decoding using classifier-free guidance, supports
trade-offs between robustness and diversity, and enables variable bit-rate and
controllable computation via RVQ layer pruning at test time. Extensive
experiments and ablations demonstrate that DISTAR surpasses state-of-the-art
zero-shot TTS systems in robustness, naturalness, and speaker/style
consistency, while maintaining rich output diversity. Audio samples are
provided on https://anonymous.4open.science/w/DiSTAR_demo.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [77] [Data or Language Supervision: What Makes CLIP Better than DINO?](https://arxiv.org/abs/2510.11835)
*Yiming Liu,Yuhui Zhang,Dhruba Ghosh,Ludwig Schmidt,Serena Yeung-Levy*

Main category: cs.CV

TL;DR: CLIP在视觉语言模型中优于DINO，但优势源于语言监督还是更大训练数据尚不明确。通过控制实验发现，CLIP擅长捕捉高级语义特征，DINO更关注低级视觉特征，两者在不同VQA任务中各有优势。


<details>
  <summary>Details</summary>
Motivation: 澄清CLIP在视觉语言模型中优于自监督模型DINO的原因——是语言监督的作用还是训练数据量的影响

Method: 在相同架构、数据集和训练配置下预训练CLIP和DINO，分析嵌入特征，并在20个VQA基准上评估它们在视觉语言模型中的表现

Result: CLIP在文本密集型任务中表现优异，DINO在视觉中心任务中略胜一筹；CLIP捕捉高级语义，DINO关注低级特征；语言监督变体带来的提升有限

Conclusion: 研究为视觉编码器设计及其对视觉语言模型性能影响提供了科学见解，表明不同编码器在不同任务中各有优势

Abstract: CLIP outperforms self-supervised models like DINO as vision encoders for
vision-language models (VLMs), but it remains unclear whether this advantage
stems from CLIP's language supervision or its much larger training data. To
disentangle these factors, we pre-train CLIP and DINO under controlled settings
-- using the same architecture, dataset, and training configuration --
achieving similar ImageNet accuracy. Embedding analysis shows that CLIP
captures high-level semantics (e.g., object categories, text), while DINO is
more responsive to low-level features like colors and styles. When integrated
into VLMs and evaluated on 20 VQA benchmarks, CLIP excels at text-intensive
tasks, while DINO slightly outperforms on vision-centric ones. Variants of
language supervision (e.g., sigmoid loss, pre-trained language encoders) yield
limited gains. Our findings provide scientific insights into vision encoder
design and its impact on VLM performance.

</details>


### [78] [Vision Language Models Map Logos to Text via Semantic Entanglement in the Visual Projector](https://arxiv.org/abs/2510.12287)
*Sifan Li,Hongkai Chen,Yujun Cai,Qingwen Ye,Liyang Chen,Junsong Yuan,Yiwei Wang*

Main category: cs.CV

TL;DR: 本文研究了视觉语言模型在无文字logo上的品牌名称幻觉问题，发现模型倾向于依赖符号先验而非真实字形感知，并通过投影器维度分析提出了缓解方法。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在多模态推理方面取得显著进展，但仍存在幻觉问题，即在没有视觉证据的情况下生成输出。本文特别关注logo幻觉问题，即模型在包含无可见文字的logo时仍生成品牌名称或文本内容。

Method: 使用纯符号、混合和含文字logo的精选数据集，以及具有挑战性的Hard-60子集，系统测量领先VLM的幻觉情况。通过九种结构化扰动测试鲁棒性，并对开源LLaVA进行嵌入级分析，识别与幻觉相关的投影器维度。

Result: 研究表明幻觉在强扭曲下仍然存在，遮挡暴露了最严重的弱点。幻觉与投影器的小部分维度相关，针对性消融可显著减少错误同时保持OCR准确性。模型对标志性圆形logo特别依赖符号先验。

Conclusion: 投影器子空间在这种失败模式中起决定性作用。本文贡献了新的诊断视角和可行的缓解见解，强调投影器解耦和OCR引导解码是构建更可信多模态系统的有前景方向。

Abstract: Vision Language Models (VLMs) have achieved impressive progress in multimodal
reasoning; yet, they remain vulnerable to hallucinations, where outputs are not
grounded in visual evidence. In this paper, we investigate a previously
overlooked setting: logo hallucination, where models generate brand names or
textual content despite logos containing no visible words. Using curated splits
of pure symbols, hybrids, and text-bearing logos, as well as the challenging
Hard-60 subset, we systematically measure hallucination across leading VLMs. We
further probe robustness through nine structured perturbations and show that
hallucinations persist even under strong distortions, with occlusion exposing
the sharpest weaknesses. Embedding-level analysis with open-weight LLaVA
demonstrates that hallucination is tied to a small subset of projector
dimensions, and targeted ablation substantially reduces errors while preserving
OCR accuracy. Together, these findings reveal that VLMs often rely on symbolic
priors rather than genuine glyph perception, particularly for iconic circular
logos, and that projector subspaces play a decisive role in this failure mode.
Our work contributes both a novel diagnostic lens and actionable mitigation
insights, highlighting projector disentanglement and OCR-guided decoding as
promising directions for building more trustworthy multimodal systems.

</details>


### [79] [Reasoning in the Dark: Interleaved Vision-Text Reasoning in Latent Space](https://arxiv.org/abs/2510.12603)
*Chao Chen,Zhixin Ma,Yongqi Li,Yupeng Hu,Yinwei Wei,Wenjie Li,Liqiang Nie*

Main category: cs.CV

TL;DR: 提出了IVT-LR方法，通过潜在空间中的视觉-文本交织推理，在减少标注需求和提升推理效率的同时提高多模态推理性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态推理方法依赖显式推理步骤，需要大量视觉-文本标注且引入显著推理延迟，需要解决这些问题。

Method: IVT-LR方法在潜在空间中注入视觉和文本信息进行推理，每个推理步骤由潜在文本（前一步的隐藏状态）和潜在视觉（选定的图像嵌入）组成，采用渐进多阶段训练策略。

Result: 在M3CoT和ScienceQA数据集上，IVT-LR方法平均准确率提升5.45%，同时推理速度比现有方法提升5倍以上。

Conclusion: IVT-LR方法通过潜在推理有效解决了多模态推理中的标注需求和效率问题，实现了性能与效率的双重提升。

Abstract: Multimodal reasoning aims to enhance the capabilities of MLLMs by
incorporating intermediate reasoning steps before reaching the final answer. It
has evolved from text-only reasoning to the integration of visual information,
enabling the thought process to be conveyed through both images and text.
Despite its effectiveness, current multimodal reasoning methods depend on
explicit reasoning steps that require labor-intensive vision-text annotations
and inherently introduce significant inference latency. To address these
issues, we introduce multimodal latent reasoning with the advantages of
multimodal representation, reduced annotation, and inference efficiency. To
facilicate it, we propose Interleaved Vision-Text Latent Reasoning (IVT-LR),
which injects both visual and textual information in the reasoning process
within the latent space. Specifically, IVT-LR represents each reasoning step by
combining two implicit parts: latent text (the hidden states from the previous
step) and latent vision (a set of selected image embeddings). We further
introduce a progressive multi-stage training strategy to enable MLLMs to
perform the above multimodal latent reasoning steps. Experiments on M3CoT and
ScienceQA demonstrate that our IVT-LR method achieves an average performance
increase of 5.45% in accuracy, while simultaneously achieving a speed increase
of over 5 times compared to existing approaches. Code available at
https://github.com/FYYDCC/IVT-LR.

</details>


### [80] [SRUM: Fine-Grained Self-Rewarding for Unified Multimodal Models](https://arxiv.org/abs/2510.12784)
*Weiyang Jin,Yuwei Niu,Jiaqi Liao,Chengqi Duan,Aoxue Li,Shenghua Gao,Xihui Liu*

Main category: cs.CV

TL;DR: SRUM是一个自奖励后训练框架，通过让统一多模态模型的理解模块作为内部评估器，为生成模块提供纠正信号，实现自我改进，无需额外人工标注数据。


<details>
  <summary>Details</summary>
Motivation: 现有统一多模态模型存在视觉理解能力无法有效转移到视觉生成的问题，模型能正确理解图像却无法从文本提示生成忠实图像。

Method: 设计了全局-局部双奖励系统：全局奖励确保整体视觉语义和布局的正确性，局部奖励细化细粒度的对象级保真度。

Result: 在T2I-CompBench上性能从82.18提升到88.37，在T2I-ReasonBench上从43.82提升到46.75。

Conclusion: 建立了一个强大的新范式，使统一多模态模型的理解模块能够通过自奖励来指导和增强自身的生成能力。

Abstract: Recently, remarkable progress has been made in Unified Multimodal Models
(UMMs), which integrate vision-language generation and understanding
capabilities within a single framework. However, a significant gap exists where
a model's strong visual understanding often fails to transfer to its visual
generation. A model might correctly understand an image based on user
instructions, yet be unable to generate a faithful image from text prompts.
This phenomenon directly raises a compelling question: Can a model achieve
self-improvement by using its understanding module to reward its generation
module? To bridge this gap and achieve self-improvement, we introduce SRUM, a
self-rewarding post-training framework that can be directly applied to existing
UMMs of various designs. SRUM creates a feedback loop where the model's own
understanding module acts as an internal ``evaluator'', providing corrective
signals to improve its generation module, without requiring additional
human-labeled data. To ensure this feedback is comprehensive, we designed a
global-local dual reward system. To tackle the inherent structural complexity
of images, this system offers multi-scale guidance: a \textbf{global reward}
ensures the correctness of the overall visual semantics and layout, while a
\textbf{local reward} refines fine-grained, object-level fidelity. SRUM leads
to powerful capabilities and shows strong generalization, boosting performance
on T2I-CompBench from 82.18 to \textbf{88.37} and on T2I-ReasonBench from 43.82
to \textbf{46.75}. Overall, our work establishes a powerful new paradigm for
enabling a UMMs' understanding module to guide and enhance its own generation
via self-rewarding.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [81] [Celebrity Profiling on Short Urdu Text using Twitter Followers' Feed](https://arxiv.org/abs/2510.11739)
*Muhammad Hamza,Rizwan Jafar*

Main category: cs.SI

TL;DR: 该研究应用机器学习和深度学习技术，通过分析乌尔都语名人追随者的推文来预测名人的人口统计特征，填补了乌尔都语这一低资源语言在该领域的研究空白。


<details>
  <summary>Details</summary>
Motivation: 社交媒体已成为数字时代的重要组成部分，名人通过在线帖子展示个人和专业生活。现有研究主要集中在英语等高资源语言，乌尔都语领域尚未充分探索。

Method: 收集并预处理了南亚次大陆名人追随者的乌尔都语推文数据集，训练并比较了多种算法，包括逻辑回归、支持向量机、随机森林、卷积神经网络和长短期记忆网络。

Result: 性别预测表现最佳，cRank和准确率均为0.65，年龄、职业和知名度预测结果中等。

Conclusion: 研究表明，利用追随者的语言特征，通过机器学习和神经网络方法可以有效预测乌尔都语名人的统计特征，为低资源语言的研究提供了新思路。

Abstract: Social media has become an essential part of the digital age, serving as a
platform for communication, interaction, and information sharing. Celebrities
are among the most active users and often reveal aspects of their personal and
professional lives through online posts. Platforms such as Twitter provide an
opportunity to analyze language and behavior for understanding demographic and
social patterns. Since followers frequently share linguistic traits and
interests with the celebrities they follow, textual data from followers can be
used to predict celebrity demographics. However, most existing research in this
field has focused on English and other high-resource languages, leaving Urdu
largely unexplored.
  This study applies modern machine learning and deep learning techniques to
the problem of celebrity profiling in Urdu. A dataset of short Urdu tweets from
followers of subcontinent celebrities was collected and preprocessed. Multiple
algorithms were trained and compared, including Logistic Regression, Support
Vector Machines, Random Forests, Convolutional Neural Networks, and Long
Short-Term Memory networks. The models were evaluated using accuracy,
precision, recall, F1-score, and cumulative rank (cRank). The best performance
was achieved for gender prediction with a cRank of 0.65 and an accuracy of
0.65, followed by moderate results for age, profession, and fame prediction.
These results demonstrate that follower-based linguistic features can be
effectively leveraged using machine learning and neural approaches for
demographic prediction in Urdu, a low-resource language.

</details>


### [82] [Evolution of wartime discourse on Telegram: A comparative study of Ukrainian and Russian policymakers' communication before and after Russia's full-scale invasion of Ukraine](https://arxiv.org/abs/2510.11746)
*Mykola Makhortykh,Aytalina Kulichkina,Kateryna Maikovska*

Main category: cs.SI

TL;DR: 分析俄乌战争期间乌克兰和俄罗斯决策者在Telegram上的政治传播策略，发现入侵后活动激增，乌克兰初期关注战争话题但逐渐减少，俄罗斯则回避战争话题转向西方危机等分散注意力的内容。


<details>
  <summary>Details</summary>
Motivation: 研究社交媒体时代首次大规模欧洲战争中精英驱动的政治传播，了解决策者如何适应战时传播挑战。

Method: 使用2019-2024年乌克兰和俄罗斯决策者在Telegram公共帖文的独特数据集，分析传播量、主题内容和参与者互动的变化。

Result: 入侵后Telegram活动急剧增加，特别是执政党决策者；乌克兰初期关注战争话题但随时间减少；俄罗斯回避战争讨论，强调西方危机等分散注意力话题；不同规模和个体决策者传播策略存在差异。

Conclusion: 揭示了决策者在战时如何适应传播挑战，为理解战争期间在线政治话语动态提供了重要见解。

Abstract: This study examines elite-driven political communication on Telegram during
the ongoing Russo-Ukrainian war, the first large-scale European war in the
social media era. Using a unique dataset of Telegram public posts from
Ukrainian and Russian policymakers (2019-2024), we analyze changes in
communication volume, thematic content, and actor engagement following Russia's
2022 full-scale invasion. Our findings show a sharp increase in Telegram
activity after the invasion, particularly among ruling-party policymakers.
Ukrainian policymakers initially focused on war-related topics, but this
emphasis declined over time In contrast, Russian policymakers largely avoided
war-related discussions, instead emphasizing unrelated topics, such as Western
crises, to distract public attention. We also identify differences in
communication strategies between large and small parties, as well as individual
policymakers. Our findings shed light on how policymakers adapt to wartime
communication challenges and offer critical insights into the dynamics of
online political discourse during times of war.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [83] [Simple Projection Variants Improve ColBERT Performance](https://arxiv.org/abs/2510.12327)
*Benjamin Clavié,Sean Lee,Rikiya Takehi,Aamir Shakir,Makoto P. Kato*

Main category: cs.IR

TL;DR: 本文研究发现，在多向量密集检索模型中使用更复杂的投影块（如深度非线性FFN、GLU块和跳跃连接）替代传统的单层线性投影，可以显著提升ColBERT模型的检索性能。


<details>
  <summary>Details</summary>
Motivation: 传统多向量检索方法使用单层线性投影存在固有局限性，MaxSim算子对梯度流的影响限制了模型性能。

Method: 设计并系统评估了多种替代投影块，包括深度非线性FFN、GLU块和跳跃连接，分析不同投影变体的性能表现。

Result: 许多投影变体优于原始线性投影，最佳变体在多个检索基准测试中平均性能提升超过2 NDCG@10点。

Conclusion: 替换ColBERT模型的线性层是一个稳健的即插即用升级方案，特别是在使用放大中间投影和残差连接时效果最佳。

Abstract: Multi-vector dense retrieval methods like ColBERT systematically use a
single-layer linear projection to reduce the dimensionality of individual
vectors. In this study, we explore the implications of the MaxSim operator on
the gradient flows of the training of multi-vector models and show that such a
simple linear projection has inherent, if non-critical, limitations in this
setting. We then discuss the theoretical improvements that could result from
replacing this single-layer projection with well-studied alternative
feedforward linear networks (FFN), such as deeper, non-linear FFN blocks, GLU
blocks, and skip-connections, could alleviate these limitations. Through the
design and systematic evaluation of alternate projection blocks, we show that
better-designed final projections positively impact the downstream performance
of ColBERT models. We highlight that many projection variants outperform the
original linear projections, with the best-performing variants increasing
average performance on a range of retrieval benchmarks across domains by over 2
NDCG@10 points. We then conduct further exploration on the individual
parameters of these projections block in order to understand what drives this
empirical performance, highlighting the particular importance of upscaled
intermediate projections and residual connections. As part of these ablation
studies, we show that numerous suboptimal projection variants still outperform
the traditional single-layer projection across multiple benchmarks, confirming
our hypothesis. Finally, we observe that this effect is consistent across
random seeds, further confirming that replacing the linear layer of ColBERT
models is a robust, drop-in upgrade.

</details>


### [84] [The Role of Parametric Injection-A Systematic Study of Parametric Retrieval-Augmented Generation](https://arxiv.org/abs/2510.12668)
*Minghao Tang,Shiyu Ni,Jingtong Wu,Zengxin Han,Keping Bi*

Main category: cs.IR

TL;DR: 本文系统研究了参数化检索增强生成(PRAG)机制，发现参数化文档仅捕获部分语义信息，单独使用性能不如文本级交互，但与文本文档结合可提升模型理解能力和抗噪性。


<details>
  <summary>Details</summary>
Motivation: 尽管PRAG作为新兴的RAG形式受到关注，但其参数注入机制仍未被充分理解，需要系统研究来阐明参数化注入的作用。

Method: 通过系统研究PRAG，分析参数化文档的语义捕获能力，比较参数化与文本级交互的效果，并探索两者结合的优势。

Result: 参数化文档仅编码部分语义信息，单独使用性能较差；但编码了高层次文档信息，与文本文档结合能更有效利用相关信息，提高抗噪能力，性能优于单独使用任一来源。

Conclusion: 建议联合使用参数化和文本文档，并增加参数化表示的信息含量以推进PRAG发展。

Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by
retrieving external documents. As an emerging form of RAG, parametric
retrieval-augmented generation (PRAG) encodes documents as model parameters
(i.e., LoRA modules) and injects these representations into the model during
inference, enabling interaction between the LLM and documents at parametric
level. Compared with directly placing documents in the input context, PRAG is
more efficient and has the potential to offer deeper model-document
interaction. Despite its growing attention, the mechanism underlying parametric
injection remains poorly understood. In this work, we present a systematic
study of PRAG to clarify the role of parametric injection, showing that
parameterized documents capture only partial semantic information of documents,
and relying on them alone yields inferior performance compared to interaction
at text level. However, these parametric representations encode high-level
document information that can enhance the model's understanding of documents
within the input context. When combined parameterized documents with textual
documents, the model can leverage relevant information more effectively and
become more robust to noisy inputs, achieving better performance than either
source alone. We recommend jointly using parameterized and textual documents
and advocate for increasing the information content of parametric
representations to advance PRAG.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [85] [UALM: Unified Audio Language Model for Understanding, Generation and Reasoning](https://arxiv.org/abs/2510.12000)
*Jinchuan Tian,Sang-gil Lee,Zhifeng Kong,Sreyan Ghosh,Arushi Goel,Chao-Han Huck Yang,Wenliang Dai,Zihan Liu,Hanrong Ye,Shinji Watanabe,Mohammad Shoeybi,Bryan Catanzaro,Rafael Valle,Wei Ping*

Main category: cs.SD

TL;DR: 本文提出了统一音频语言模型(UALM)，将音频理解、文本到音频生成和多模态推理统一在单个模型中，展示了跨模态生成推理的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前音频语言建模领域将音频理解和文本到音频生成作为独立任务处理，很少有研究尝试统一这些任务，而这是实现高级多模态推理的关键步骤。

Method: 首先提出了UALM-Gen文本到音频语言模型，直接预测音频token；然后通过适当的数据混合、训练方法和推理技术，使单个UALM模型在多个任务上达到专业模型水平；最后提出了UALM-Reason多模态推理模型，在中间思考步骤中同时利用文本和音频。

Result: UALM在音频理解、文本到音频生成和文本推理方面的质量与最先进的专用模型相当；UALM-Reason通过主观评估证实了跨模态生成推理的有效性。

Conclusion: 这是音频研究中首次展示跨模态生成推理，证明了在单个模型中统一音频理解、生成和推理的可行性。

Abstract: Recent advances in the audio language modeling (ALM) domain tackle audio
understanding and text-to-audio generation as separate tasks. Very few studies
attempt to unify these tasks -- an essential step toward advanced multimodal
reasoning. This paper introduces U}nified Audio Language Model (UALM), which
aims to unify audio understanding, text-to-audio generation, and multimodal
reasoning in a single model. To achieve this goal, we first present UALM-Gen, a
text-to-audio language model that directly predicts audio tokens and is
comparable to state-of-the-art diffusion-based models. We then demonstrate,
using proper data blending, training recipes, and inference techniques, that
our single UALM model matches the quality of state-of-the-art specialized
models in audio understanding, text-to-audio generation, and text reasoning.
Furthermore, we present UALM-Reason, a multimodal reasoning model that utilizes
both text and audio in the intermediate thinking steps to facilitate complex
generation tasks. To our knowledge, this is the first demonstration in audio
research of cross-modal generative reasoning, with its effectiveness confirmed
by subjective evaluations.

</details>


### [86] [Content Anonymization for Privacy in Long-form Audio](https://arxiv.org/abs/2510.12780)
*Cristina Aggazzotti,Ashi Garg,Zexin Cai,Nicholas Andrews*

Main category: cs.SD

TL;DR: 论文提出内容匿名化方法，通过上下文重写转录文本来消除说话者特定风格，保护长形式音频中的隐私。


<details>
  <summary>Details</summary>
Motivation: 现有语音匿名化技术在短语音中有效，但在长形式音频中，攻击者可通过说话者的词汇、句法和表达习惯重新识别说话者身份。

Method: 在ASR-TTS流程中进行上下文重写，使用释义技术消除说话者特定风格，同时保留语义内容。

Result: 在长形式电话对话场景中，内容匿名化方法能有效防御基于内容的攻击，同时保持语音实用性。

Conclusion: 释义是防御基于内容攻击的有效方法，建议相关方在长形式音频中采用此步骤确保匿名性。

Abstract: Voice anonymization techniques have been found to successfully obscure a
speaker's acoustic identity in short, isolated utterances in benchmarks such as
the VoicePrivacy Challenge. In practice, however, utterances seldom occur in
isolation: long-form audio is commonplace in domains such as interviews, phone
calls, and meetings. In these cases, many utterances from the same speaker are
available, which pose a significantly greater privacy risk: given multiple
utterances from the same speaker, an attacker could exploit an individual's
vocabulary, syntax, and turns of phrase to re-identify them, even when their
voice is completely disguised. To address this risk, we propose new content
anonymization approaches. Our approach performs a contextual rewriting of the
transcripts in an ASR-TTS pipeline to eliminate speaker-specific style while
preserving meaning. We present results in a long-form telephone conversation
setting demonstrating the effectiveness of a content-based attack on
voice-anonymized speech. Then we show how the proposed content-based
anonymization methods can mitigate this risk while preserving speech utility.
Overall, we find that paraphrasing is an effective defense against
content-based attacks and recommend that stakeholders adopt this step to ensure
anonymity in long-form audio.

</details>
