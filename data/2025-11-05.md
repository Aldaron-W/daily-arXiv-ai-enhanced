<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 24]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.LG](#cs.LG) [Total: 7]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.AI](#cs.AI) [Total: 8]
- [cs.HC](#cs.HC) [Total: 1]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [cs.CV](#cs.CV) [Total: 6]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Multi-Personality Generation of LLMs at Decoding-time](https://arxiv.org/abs/2511.01891)
*Rongxin Chen,Yunfan Li,Yige Yuan,Bingbing Xu,Huawei Shen*

Main category: cs.CL

TL;DR: 提出了一个多个性生成（MPG）框架，通过解码时组合范式实现LLMs的多重个性控制，无需重新训练或依赖外部模型，使用推测性块级拒绝采样（SCR）提高效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法中，基于重新训练的方法成本高且扩展性差，解码时方法依赖外部模型或启发式方法，限制了灵活性和鲁棒性。

Method: MPG框架利用单维模型中的隐式密度比作为"免费午餐"，将任务重新表述为从聚合这些比的目标策略中采样。设计了推测性块级拒绝采样（SCR），在滑动窗口内并行验证生成的块。

Result: 在MBTI个性和角色扮演任务上的实验显示，MPG有效性提升达16%-18%。

Conclusion: MPG框架能够灵活控制多个性，无需稀缺的多维模型或额外训练，SCR方法显著降低了计算开销同时保持高质量生成。

Abstract: Multi-personality generation for LLMs, enabling simultaneous embodiment of
multiple personalization attributes, is a fundamental challenge. Existing
retraining-based approaches are costly and poorly scalable, while decoding-time
methods often rely on external models or heuristics, limiting flexibility and
robustness. In this paper, we propose a novel Multi-Personality Generation
(MPG) framework under the decoding-time combination paradigm. It flexibly
controls multi-personality without relying on scarce multi-dimensional models
or extra training, leveraging implicit density ratios in single-dimensional
models as a "free lunch" to reformulate the task as sampling from a target
strategy aggregating these ratios. To implement MPG efficiently, we design
Speculative Chunk-level based Rejection sampling (SCR), which generates
responses in chunks and parallelly validates them via estimated thresholds
within a sliding window. This significantly reduces computational overhead
while maintaining high-quality generation. Experiments on MBTI personality and
Role-Playing demonstrate the effectiveness of MPG, showing improvements up to
16%-18%. Code and data are available at https://github.com/Libra117/MPG .

</details>


### [2] [Rethinking LLM Human Simulation: When a Graph is What You Need](https://arxiv.org/abs/2511.02135)
*Joseph Suh,Suhong Moon,Serina Chang*

Main category: cs.CL

TL;DR: GEMS使用图神经网络替代大语言模型进行人类模拟，在离散选择任务中达到相当或更好的准确性，但模型规模小三个数量级，效率更高且可解释性更强。


<details>
  <summary>Details</summary>
Motivation: 探索是否必须使用大语言模型进行人类模拟，还是可以用更小、领域特定的模型替代，特别是在离散选择模拟任务中。

Method: 将离散选择模拟任务建模为图上的链接预测问题，使用图神经网络（GNN）构建Graph-basEd Models for human Simulation (GEMS)，仅在需要时融入语言表示。

Result: 在三个关键设置和三个模拟数据集上的评估显示，GEMS达到与大语言模型相当或更好的准确性，但模型规模小三个数量级，效率、可解释性和透明度都更高。

Conclusion: 图基建模是进行人类模拟的轻量级替代方案，在某些场景下可以替代大语言模型，提供更高效和可解释的解决方案。

Abstract: Large language models (LLMs) are increasingly used to simulate humans, with
applications ranging from survey prediction to decision-making. However, are
LLMs strictly necessary, or can smaller, domain-grounded models suffice? We
identify a large class of simulation problems in which individuals make choices
among discrete options, where a graph neural network (GNN) can match or surpass
strong LLM baselines despite being three orders of magnitude smaller. We
introduce Graph-basEd Models for human Simulation (GEMS), which casts discrete
choice simulation tasks as a link prediction problem on graphs, leveraging
relational knowledge while incorporating language representations only when
needed. Evaluations across three key settings on three simulation datasets show
that GEMS achieves comparable or better accuracy than LLMs, with far greater
efficiency, interpretability, and transparency, highlighting the promise of
graph-based modeling as a lightweight alternative to LLMs for human simulation.
Our code is available at https://github.com/schang-lab/gems.

</details>


### [3] [IG-Pruning: Input-Guided Block Pruning for Large Language Models](https://arxiv.org/abs/2511.02213)
*Kangyu Qiao,Shaolei Zhang,Yang Feng*

Main category: cs.CL

TL;DR: IG-Pruning是一种新颖的输入感知块级剪枝方法，通过动态选择层掩码来减少大语言模型的计算成本，无需大量训练即可实现高效推理。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型计算需求增长，高效推理对实际部署至关重要。现有深度剪枝方法依赖固定块掩码，在不同任务和输入上表现不佳。

Method: 两阶段方法：1）通过语义聚类和L0优化发现多样化掩码候选；2）在推理时动态选择层掩码，无需大量训练。

Result: 实验结果表明，该方法在多个任务上持续优于最先进的静态深度剪枝方法。

Conclusion: IG-Pruning特别适合资源受限的部署场景，为高效大语言模型推理提供了有效解决方案。

Abstract: With the growing computational demands of large language models (LLMs),
efficient inference has become increasingly critical for practical deployment.
Depth pruning has emerged as a promising approach for reducing the
computational costs of large language models by removing transformer layers.
However, existing methods typically rely on fixed block masks, which can lead
to suboptimal performance across different tasks and inputs. In this paper, we
propose IG-Pruning, a novel input-aware block-wise pruning method that
dynamically selects layer masks at inference time. Our approach consists of two
stages: (1) Discovering diverse mask candidates through semantic clustering and
L0 optimization, and (2) Implementing efficient dynamic pruning without the
need for extensive training. Experimental results demonstrate that our method
consistently outperforms state-of-the-art static depth pruning methods, making
it particularly suitable for resource-constrained deployment scenarios.

</details>


### [4] [Demo: Statistically Significant Results On Biases and Errors of LLMs Do Not Guarantee Generalizable Results](https://arxiv.org/abs/2511.02246)
*Jonathan Liu,Haoling Qiu,Jonathan Lasko,Damianos Karakos,Mahsa Yarmohammadi,Mark Dredze*

Main category: cs.CL

TL;DR: 开发了一个自动生成查询和评估医疗聊天机器人回答的基础设施，发现LLM评估者之间一致性较低，建议使用多个LLM作为评估者以避免统计显著但不具普适性的结果。


<details>
  <summary>Details</summary>
Motivation: 医疗聊天机器人在涉及非医疗因素（如人口统计信息）时必须提供一致的建议，但目前LLM在医疗环境中存在幻觉、遗漏和偏见问题。

Method: 开发了基础设施：1）自动生成查询来探测LLM，采样患者人口统计、病史、疾病和写作风格；2）使用多个LLM-as-a-judge设置和提示来评估答案，包括幻觉和遗漏检测。

Result: LLM注释者表现出低一致性（平均Cohen's Kappa κ=0.118），只有特定的（回答、评估）LLM对在写作风格、性别和种族方面产生统计显著差异。

Conclusion: 建议使用多个LLM作为评估者以避免非普适性结果，特别是在缺乏真实数据的情况下，并建议发布LLM间一致性指标以提高透明度。

Abstract: Recent research has shown that hallucinations, omissions, and biases are
prevalent in everyday use-cases of LLMs. However, chatbots used in medical
contexts must provide consistent advice in situations where non-medical factors
are involved, such as when demographic information is present. In order to
understand the conditions under which medical chatbots fail to perform as
expected, we develop an infrastructure that 1) automatically generates queries
to probe LLMs and 2) evaluates answers to these queries using multiple
LLM-as-a-judge setups and prompts. For 1), our prompt creation pipeline samples
the space of patient demographics, histories, disorders, and writing styles to
create realistic questions that we subsequently use to prompt LLMs. In 2), our
evaluation pipeline provides hallucination and omission detection using
LLM-as-a-judge as well as agentic workflows, in addition to LLM-as-a-judge
treatment category detectors. As a baseline study, we perform two case studies
on inter-LLM agreement and the impact of varying the answering and evaluation
LLMs. We find that LLM annotators exhibit low agreement scores (average Cohen's
Kappa $\kappa=0.118$), and only specific (answering, evaluation) LLM pairs
yield statistically significant differences across writing styles, genders, and
races. We recommend that studies using LLM evaluation use multiple LLMs as
evaluators in order to avoid arriving at statistically significant but
non-generalizable results, particularly in the absence of ground-truth data. We
also suggest publishing inter-LLM agreement metrics for transparency. Our code
and dataset are available here:
https://github.com/BBN-E/medic-neurips-2025-demo.

</details>


### [5] [LTD-Bench: Evaluating Large Language Models by Letting Them Draw](https://arxiv.org/abs/2511.02347)
*Liuhao Lin,Ke Li,Zihan Xu,Yuchen Shi,Yulei Qin,Yan Zhang,Xing Sun,Rongrong Ji*

Main category: cs.CL

TL;DR: LTD-Bench是一个突破性的基准测试，通过要求模型通过点阵或可执行代码生成绘图，将LLM评估从抽象分数转变为可直接观察的视觉输出，揭示了模型在空间推理方面的严重缺陷。


<details>
  <summary>Details</summary>
Motivation: 当前LLM评估范式存在关键盲点，依赖不透明的数值指标掩盖了空间推理的基本限制，导致报告性能与实际能力之间存在危险脱节，特别是需要物理世界理解的应用。

Method: LTD-Bench采用综合方法，包含互补的生成任务（测试空间想象力）和识别任务（评估空间感知），在三个渐进难度级别上系统评估语言-空间映射的两个关键方向。

Result: 对最先进模型的大量实验揭示了一个令人担忧的能力差距：即使在传统基准测试中取得令人印象深刻结果的LLM，在建立语言和空间概念之间的双向映射方面也表现出严重缺陷。

Conclusion: LTD-Bench的视觉输出能够进行强大的诊断分析，为研究模型相似性提供了潜在方法，揭示了LLM作为真正世界模型的根本局限性。

Abstract: Current evaluation paradigms for large language models (LLMs) represent a
critical blind spot in AI research--relying on opaque numerical metrics that
conceal fundamental limitations in spatial reasoning while providing no
intuitive understanding of model capabilities. This deficiency creates a
dangerous disconnect between reported performance and practical abilities,
particularly for applications requiring physical world understanding. We
introduce LTD-Bench, a breakthrough benchmark that transforms LLM evaluation
from abstract scores to directly observable visual outputs by requiring models
to generate drawings through dot matrices or executable code. This approach
makes spatial reasoning limitations immediately apparent even to non-experts,
bridging the fundamental gap between statistical performance and intuitive
assessment. LTD-Bench implements a comprehensive methodology with complementary
generation tasks (testing spatial imagination) and recognition tasks (assessing
spatial perception) across three progressively challenging difficulty levels,
methodically evaluating both directions of the critical language-spatial
mapping. Our extensive experiments with state-of-the-art models expose an
alarming capability gap: even LLMs achieving impressive results on traditional
benchmarks demonstrate profound deficiencies in establishing bidirectional
mappings between language and spatial concept--a fundamental limitation that
undermines their potential as genuine world models. Furthermore, LTD-Bench's
visual outputs enable powerful diagnostic analysis, offering a potential
approach to investigate model similarity.

</details>


### [6] [Let Multimodal Embedders Learn When to Augment Query via Adaptive Query Augmentation](https://arxiv.org/abs/2511.02358)
*Wongyu Kim,Hochang Lee,Sanghak Lee,Yoonsung Kim,Jaehyun Park*

Main category: cs.CL

TL;DR: M-Solomon是一个多模态嵌入器，能够自适应地决定何时进行查询增强，只在必要时进行增强，从而在保持性能的同时显著减少嵌入延迟。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的嵌入器对所有查询都进行增强，导致显著的嵌入延迟，且某些查询的增强反而会损害性能。此外，先前方法未在多模态环境中探索。

Method: 首先将训练数据集中的查询分为需要增强和不需要增强两组；利用强大的多模态LLM为需要增强的查询生成适当的增强内容；通过自适应查询增强，M-Solomon学习为需要增强的查询生成带有/augment前缀的合成增强，为其他查询生成简单的/embed字符串。

Result: 实验结果显示，M-Solomon不仅大幅超越了无增强的基线，也优于总是使用增强的基线，同时提供了更快的嵌入延迟。

Conclusion: M-Solomon通过自适应查询增强策略，在多模态环境中实现了更好的性能和更低的延迟，解决了当前方法中过度增强的问题。

Abstract: Query augmentation makes queries more meaningful by appending further
information to the queries to find relevant documents. Current studies have
proposed Large Language Model (LLM)-based embedders, which learn representation
for embedding and generation for query augmentation in a multi-task manner by
leveraging the generative capabilities of LLM. During inference, these jointly
trained embedders have conducted query augmentation followed by embedding,
showing effective results. However, augmenting every query leads to substantial
embedding latency and query augmentation can be detrimental to performance for
some queries. Also, previous methods have not been explored in multimodal
environments. To tackle these problems, we propose M-Solomon, a universal
multimodal embedder that can adaptively determine when to augment queries. Our
approach first divides the queries of the training datasets into two groups at
the dataset level. One includes queries that require augmentation and the other
includes queries that do not. Then, we introduces a synthesis process that
generates appropriate augmentations for queries that require them by leveraging
a powerful Multimodal LLM (MLLM). Next, we present adaptive query augmentation.
Through this step, M-Solomon can conduct query augmentation only when necessary
by learning to generate synthetic augmentations with the prefix /augment for
queries that demand them and to generate the simple string /embed for others.
Experimental results showed that M-Solomon not only surpassed the baseline
without augmentation by a large margin but also outperformed the baseline that
always used augmentation, providing much faster embedding latency.

</details>


### [7] [LiveSecBench: A Dynamic and Culturally-Relevant AI Safety Benchmark for LLMs in Chinese Context](https://arxiv.org/abs/2511.02366)
*Yudong Li,Zhongliang Yang,Kejiang Chen,Wenxuan Wang,Tianxin Zhang,Sifang Wan,Kecheng Wang,Haitian Li,Xu Wang,Lefan Cheng,Youdan Yang,Baocheng Chen,Ziyu Liu,Yufei Sun,Liyan Wu,Wenya Wen,Xingchi Gu,Peiru Yang*

Main category: cs.CL

TL;DR: LiveSecBench是一个专门针对中文LLM应用场景的动态持续更新的安全基准，评估模型在合法性、道德性、事实性、隐私性、对抗鲁棒性和推理安全性六个关键维度的表现。


<details>
  <summary>Details</summary>
Motivation: 针对中文语言环境下的LLM应用缺乏专门的安全评估基准，需要建立基于中国法律和社会框架的动态安全评估体系。

Method: 构建包含六个安全维度的评估框架，采用动态更新机制，定期纳入新的威胁向量，目前已评估18个LLM模型。

Result: 建立了LiveSecBench(v251030)基准，提供了中文语言环境下AI安全现状的全面视图，并公开了排行榜。

Conclusion: LiveSecBench为中文LLM应用提供了专门的安全评估标准，通过动态更新机制保持基准的相关性和时效性。

Abstract: In this work, we propose LiveSecBench, a dynamic and continuously updated
safety benchmark specifically for Chinese-language LLM application scenarios.
LiveSecBench evaluates models across six critical dimensions (Legality, Ethics,
Factuality, Privacy, Adversarial Robustness, and Reasoning Safety) rooted in
the Chinese legal and social frameworks. This benchmark maintains relevance
through a dynamic update schedule that incorporates new threat vectors, such as
the planned inclusion of Text-to-Image Generation Safety and Agentic Safety in
the next update. For now, LiveSecBench (v251030) has evaluated 18 LLMs,
providing a landscape of AI safety in the context of Chinese language. The
leaderboard is publicly accessible at https://livesecbench.intokentech.cn/.

</details>


### [8] [AyurParam: A State-of-the-Art Bilingual Language Model for Ayurveda](https://arxiv.org/abs/2511.02374)
*Mohd Nauman,Sravan Gvm,Vijay Devane,Shyam Pawar,Viraj Thakur,Kundeshwar Pundalik,Piyush Sawarkar,Rohit Saluja,Maunendra Desarkar,Ganesh Ramakrishnan*

Main category: cs.CL

TL;DR: AyurParam-2.9B是一个专门针对阿育吠陀医学领域的双语语言模型，在专业医学任务上超越了同规模的开源模型，甚至能与更大模型竞争。


<details>
  <summary>Details</summary>
Motivation: 主流大语言模型在处理需要深厚文化、语言和专业知识的高度专业化领域时表现不佳，特别是像阿育吠陀这样的传统医学系统。

Method: 从Param-1-2.9B模型微调，使用包含古典文本和临床指导的专家策划阿育吠陀数据集，数据集包含上下文感知、推理和客观风格的双语问答。

Result: 在BhashaBench-Ayur基准测试中，AyurParam不仅超越了同规模（1.5-3B参数）的所有开源指令调优模型，还展现出与更大模型竞争甚至更优的性能。

Conclusion: AyurParam的结果强调了在提供可靠、文化一致的AI专业医学知识时，真实领域适应和高质量监督的必要性。

Abstract: Current large language models excel at broad, general-purpose tasks, but
consistently underperform when exposed to highly specialized domains that
require deep cultural, linguistic, and subject-matter expertise. In particular,
traditional medical systems such as Ayurveda embody centuries of nuanced
textual and clinical knowledge that mainstream LLMs fail to accurately
interpret or apply. We introduce AyurParam-2.9B, a domain-specialized,
bilingual language model fine-tuned from Param-1-2.9B using an extensive,
expertly curated Ayurveda dataset spanning classical texts and clinical
guidance. AyurParam's dataset incorporates context-aware, reasoning, and
objective-style Q&A in both English and Hindi, with rigorous annotation
protocols for factual precision and instructional clarity. Benchmarked on
BhashaBench-Ayur, AyurParam not only surpasses all open-source
instruction-tuned models in its size class (1.5--3B parameters), but also
demonstrates competitive or superior performance compared to much larger
models. The results from AyurParam highlight the necessity for authentic domain
adaptation and high-quality supervision in delivering reliable, culturally
congruent AI for specialized medical knowledge.

</details>


### [9] [AutoAdv: Automated Adversarial Prompting for Multi-Turn Jailbreaking of Large Language Models](https://arxiv.org/abs/2511.02376)
*Aashray Reddy,Andrew Zagula,Nicholas Saban*

Main category: cs.CL

TL;DR: AutoAdv是一个无需训练的多轮越狱攻击框架，在Llama-3.1-8B上达到95%攻击成功率，比单轮攻击提升24%，揭示当前安全机制在多轮对话中的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全评估主要关注单轮交互，而现实攻击往往通过自适应多轮对话展开，需要研究多轮越狱攻击的有效性和防御需求。

Method: 结合三种自适应机制：模式管理器从成功攻击中学习增强提示，温度管理器基于失败模式动态调整采样参数，两阶段重写策略先伪装有害请求再迭代优化。

Result: 在商业和开源模型（GPT-4o-mini、Qwen3-235B、Mistral-7B）上广泛评估，多轮攻击始终优于单轮方法，暴露当前安全机制的持续脆弱性。

Conclusion: 针对单轮交互优化的对齐策略无法在扩展对话中保持鲁棒性，迫切需要多轮感知的防御机制。

Abstract: Large Language Models (LLMs) remain vulnerable to jailbreaking attacks where
adversarial prompts elicit harmful outputs, yet most evaluations focus on
single-turn interactions while real-world attacks unfold through adaptive
multi-turn conversations. We present AutoAdv, a training-free framework for
automated multi-turn jailbreaking that achieves up to 95% attack success rate
on Llama-3.1-8B within six turns a 24 percent improvement over single turn
baselines. AutoAdv uniquely combines three adaptive mechanisms: a pattern
manager that learns from successful attacks to enhance future prompts, a
temperature manager that dynamically adjusts sampling parameters based on
failure modes, and a two-phase rewriting strategy that disguises harmful
requests then iteratively refines them. Extensive evaluation across commercial
and open-source models (GPT-4o-mini, Qwen3-235B, Mistral-7B) reveals persistent
vulnerabilities in current safety mechanisms, with multi-turn attacks
consistently outperforming single-turn approaches. These findings demonstrate
that alignment strategies optimized for single-turn interactions fail to
maintain robustness across extended conversations, highlighting an urgent need
for multi-turn-aware defenses.

</details>


### [10] [Merging Continual Pretraining Models for Domain-Specialized LLMs: A Case Study in Finance](https://arxiv.org/abs/2511.02451)
*Kentaro Ueda,François Portet,Hirohiko Suwa,Keiichi Yasumoto*

Main category: cs.CL

TL;DR: 本研究探索了将领域特定的持续预训练专家模型合并的方法，以解决LLMs在金融等专业领域表现不佳的问题。通过评估三种合并方法在金融基准测试上的表现，发现合并专家模型能恢复通用知识并产生跨领域技能。


<details>
  <summary>Details</summary>
Motivation: LLMs在通用任务上表现出色，但在金融等专业领域表现不佳，需要融合领域知识、数学推理和多语言处理能力。合并领域特定的持续预训练专家模型为构建多技能LLMs提供了实用替代方案。

Method: 创建金融、数学和日语领域的专家模型，评估三种合并方法（Task Arithmetic、TIES和DARE-TIES）在包含18个任务8个数据集的金融基准测试上的表现，采用三阶段评估框架关注知识恢复、互补性和涌现性。

Result: 合并专家与其基础模型能恢复CPT期间丢失的通用知识，合并不同专家能提升性能并产生跨领域涌现技能。Task Arithmetic表现强劲但对超参数敏感，TIES更稳健。模型相似性与合并成功相关，但涌现技能依赖更复杂因素。

Conclusion: 这是首个对CPT模型合并的基础性分析，建立了原则性框架，为从现有资产构建多技能LLMs提供了明确指导。

Abstract: While LLMs excel at general tasks, they struggle in specialized domains like
finance, requiring diverse skills in domain knowledge, mathematical reasoning,
and multilingual processing. Merging domain-specific Continual Pre-training
(CPT) "experts" offers a practical alternative to costly and unstable
multi-skill training. However, unlike established Supervised Fine-Tuning (SFT)
model-based merging, CPT model merging remains largely unexplored. We address
this gap by creating financial LLMs from experts in finance, math, and
Japanese. We propose a three-stage evaluation focusing on knowledge recovery,
complementarity, and emergence, and assess three merging methods (Task
Arithmetic, TIES, and DARE-TIES) on a comprehensive financial benchmark curated
from 18 tasks across 8 established datasets. Results show that merging an
expert with its base model recovers general knowledge lost during CPT, while
merging experts improves performance and can yield emergent cross-domain
skills. Among the methods, Task Arithmetic performs strongly but is
hyperparameter-sensitive, whereas TIES is more robust. Our findings also
suggest that while model similarity correlates with merging success, emergent
skills depend on more complex factors. This work presents the first
foundational analysis of CPT model merging, establishing a principled framework
and providing clear guidance for building multi-skill LLMs from existing
assets.

</details>


### [11] [Prompting for Policy: Forecasting Macroeconomic Scenarios with Synthetic LLM Personas](https://arxiv.org/abs/2511.02458)
*Giulia Iadisernia,Carolina Camassa*

Main category: cs.CL

TL;DR: 研究评估基于角色的提示是否能提升LLM在宏观经济预测任务中的表现，发现GPT-4o与人类专家预测准确度相似，但角色描述对预测精度没有显著影响。


<details>
  <summary>Details</summary>
Motivation: 探索角色提示是否能增强LLM在复杂宏观经济预测任务中的表现，以及LLM能否达到专业人类预测者的水平。

Method: 使用PersonaHub语料库中的2,368个经济学相关角色提示GPT-4o，复制ECB专业预测者调查，比较角色提示与无角色基线的预测效果。

Result: GPT-4o与人类预测者准确度相似，统计显著但实际差异不大；角色描述对预测精度无显著提升；在样本外预测中保持竞争力但存在差异。

Conclusion: GPT-4o在提供相关背景数据时能在宏观经济预测中达到竞争力，但多样提示产生的预测比人类专家组更加同质化，角色描述可省略以节省计算成本。

Abstract: We evaluate whether persona-based prompting improves Large Language Model
(LLM) performance on macroeconomic forecasting tasks. Using 2,368
economics-related personas from the PersonaHub corpus, we prompt GPT-4o to
replicate the ECB Survey of Professional Forecasters across 50 quarterly rounds
(2013-2025). We compare the persona-prompted forecasts against the human
experts panel, across four target variables (HICP, core HICP, GDP growth,
unemployment) and four forecast horizons. We also compare the results against
100 baseline forecasts without persona descriptions to isolate its effect. We
report two main findings. Firstly, GPT-4o and human forecasters achieve
remarkably similar accuracy levels, with differences that are statistically
significant yet practically modest. Our out-of-sample evaluation on 2024-2025
data demonstrates that GPT-4o can maintain competitive forecasting performance
on unseen events, though with notable differences compared to the in-sample
period. Secondly, our ablation experiment reveals no measurable forecasting
advantage from persona descriptions, suggesting these prompt components can be
omitted to reduce computational costs without sacrificing accuracy. Our results
provide evidence that GPT-4o can achieve competitive forecasting accuracy even
on out-of-sample macroeconomic events, if provided with relevant context data,
while revealing that diverse prompts produce remarkably homogeneous forecasts
compared to human panels.

</details>


### [12] [Smart-Hiring: An Explainable end-to-end Pipeline for CV Information Extraction and Job Matching](https://arxiv.org/abs/2511.02537)
*Kenza Khelkhal,Dihia Lanasri*

Main category: cs.CL

TL;DR: Smart-Hiring是一个端到端的NLP管道，用于自动从非结构化简历中提取结构化信息，并通过语义匹配将候选人与职位描述进行匹配。


<details>
  <summary>Details</summary>
Motivation: 传统招聘过程中手动筛选数百份简历耗时耗力、容易出错且存在人为偏见，需要自动化解决方案。

Method: 结合文档解析、命名实体识别和上下文文本嵌入技术，在共享向量空间中编码简历和职位描述以计算相似度得分。

Result: 在跨越多个专业领域的真实数据集上实验表明，该方法具有鲁棒性和可行性，实现了有竞争力的匹配准确度。

Conclusion: 该系统为招聘分析提供了一个可扩展且实用的NLP框架，并为偏见缓解、公平感知建模和大规模部署数据驱动的招聘解决方案指明了有前景的方向。

Abstract: Hiring processes often involve the manual screening of hundreds of resumes
for each job, a task that is time and effort consuming, error-prone, and
subject to human bias. This paper presents Smart-Hiring, an end-to-end Natural
Language Processing (NLP) pipeline de- signed to automatically extract
structured information from unstructured resumes and to semantically match
candidates with job descriptions. The proposed system combines document
parsing, named-entity recognition, and contextual text embedding techniques to
capture skills, experience, and qualifications. Using advanced NLP technics,
Smart-Hiring encodes both resumes and job descriptions in a shared vector space
to compute similarity scores between candidates and job postings. The pipeline
is modular and explainable, allowing users to inspect extracted entities and
matching rationales. Experiments were conducted on a real-world dataset of
resumes and job descriptions spanning multiple professional domains,
demonstrating the robustness and feasibility of the proposed approach. The
system achieves competitive matching accuracy while preserving a high degree of
interpretability and transparency in its decision process. This work introduces
a scalable and practical NLP frame- work for recruitment analytics and outlines
promising directions for bias mitigation, fairness-aware modeling, and
large-scale deployment of data-driven hiring solutions.

</details>


### [13] [The Analysis of Lexical Errors in Machine Translation from English into Romanian](https://arxiv.org/abs/2511.02587)
*Angela Stamatie*

Main category: cs.CL

TL;DR: 对Google Translate从英语到罗马尼亚语翻译的230篇COVID-19相关官方文本进行词汇错误分析，旨在改进机器翻译的词汇选择和减少错误。


<details>
  <summary>Details</summary>
Motivation: 研究旨在分析机器翻译在COVID-19相关官方信息翻译中的词汇错误，以改进Google Translate的翻译质量，特别是在医疗和公共卫生领域的准确性。

Method: 对230篇从英语翻译成罗马尼亚语的文本进行综合分析，这些文本来自WHO、Gavi组织和药品说明书等官方来源，重点关注词汇错误的识别和分类。

Result: 研究发现Google Translate在翻译COVID-19相关官方文本时存在显著的词汇错误，特别是在医学术语和药品信息的翻译准确性方面有待改进。

Conclusion: 机器翻译在专业领域特别是医疗信息的翻译中仍存在词汇选择问题，需要进一步优化以提高翻译质量和准确性。

Abstract: The research explores error analysis in the performance of translating by
Machine Translation from English into Romanian, and it focuses on lexical
errors found in texts which include official information, provided by the World
Health Organization (WHO), the Gavi Organization, by the patient information
leaflet (the information about the active ingredients of the vaccines or the
medication, the indications, the dosage instructions, the storage instructions,
the side effects and warning, etc.). All of these texts are related to Covid-19
and have been translated by Google Translate, a multilingual Machine
Translation that was created by Google. In the last decades, Google has
actively worked to develop a more accurate and fluent automatic translation
system. This research, specifically focused on improving Google Translate, aims
to enhance the overall quality of Machine Translation by achieving better
lexical selection and by reducing errors. The investigation involves a
comprehensive analysis of 230 texts that have been translated from English into
Romanian.

</details>


### [14] [Next Token Knowledge Tracing: Exploiting Pretrained LLM Representations to Decode Student Behaviour](https://arxiv.org/abs/2511.02599)
*Max Norris,Kobi Gal,Sahan Bulathwela*

Main category: cs.CL

TL;DR: NTKT将知识追踪重新定义为使用预训练大语言模型的下一词预测任务，通过将学生历史和问题内容表示为文本序列，显著提升了预测性能和在冷启动场景下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有知识追踪模型通常忽略问题文本这一重要的教学洞察来源，限制了预测性能。

Method: 提出NTKT方法，将知识追踪重构为使用预训练大语言模型的下一词预测任务，将学生历史和问题内容表示为文本序列。

Result: 实验显著优于最先进的神经知识追踪模型，在冷启动问题和用户场景下泛化能力更强。

Conclusion: 问题内容在知识追踪中很重要，利用大语言模型的预训练表示能更有效地建模学生学习。

Abstract: Modelling student knowledge is a key challenge when leveraging AI in
education, with major implications for personalised learning. The Knowledge
Tracing (KT) task aims to predict how students will respond to educational
questions in learning environments, based on their prior interactions. Existing
KT models typically use response correctness along with metadata like skill
tags and timestamps, often overlooking the question text, which is an important
source of pedagogical insight. This omission poses a lost opportunity while
limiting predictive performance. We propose Next Token Knowledge Tracing
(NTKT), a novel approach that reframes KT as a next-token prediction task using
pretrained Large Language Models (LLMs). NTKT represents both student histories
and question content as sequences of text, allowing LLMs to learn patterns in
both behaviour and language. Our series of experiments significantly improves
performance over state-of-the-art neural KT models and generalises much better
to cold-start questions and users. These findings highlight the importance of
question content in KT and demonstrate the benefits of leveraging pretrained
representations of LLMs to model student learning more effectively.

</details>


### [15] [CGES: Confidence-Guided Early Stopping for Efficient and Accurate Self-Consistency](https://arxiv.org/abs/2511.02603)
*Ehsan Aghazadeh,Ahmad Ghasemi,Hedyeh Beyhaghi,Hossein Pishro-Nik*

Main category: cs.CL

TL;DR: CGES是一种贝叶斯框架，通过置信度信号自适应停止采样，在保持准确性的同时大幅减少大语言模型调用次数


<details>
  <summary>Details</summary>
Motivation: 解决自一致性策略需要固定调用次数且在正确答案罕见时可能失败的问题

Method: 使用从token概率或奖励模型导出的标量置信度信号形成候选答案的后验分布，当候选后验质量超过阈值时自适应停止采样

Result: 在五个推理基准测试中，平均模型调用次数减少约69%（如从16.0降至4.9），同时准确性与自一致性方法相差不超过0.06个百分点

Conclusion: CGES框架在保持准确性的同时显著提高了大语言模型推理的效率

Abstract: Large language models (LLMs) are often queried multiple times at test time,
with predictions aggregated by majority vote. While effective, this
self-consistency strategy (arXiv:2203.11171) requires a fixed number of calls
and can fail when the correct answer is rare. We introduce Confidence-Guided
Early Stopping (CGES), a Bayesian framework that forms posteriors over
candidate answers using scalar confidence signals derived from token
probabilities or reward models. CGES adaptively halts sampling once the
posterior mass of a candidate exceeds a threshold. We provide theoretical
guarantees for both perfectly calibrated confidences and realistic noisy
confidence signals. Across five reasoning benchmarks, CGES reduces the average
number of model calls by about 69 percent (for example, from 16.0 to 4.9) while
matching the accuracy of self-consistency within 0.06 percentage points.

</details>


### [16] [The Realignment Problem: When Right becomes Wrong in LLMs](https://arxiv.org/abs/2511.02623)
*Aakash Sen Sharma,Debdeep Sanyal,Vivek Srivastava,Shirish Karande,Murari Mandal*

Main category: cs.CL

TL;DR: TRACE框架通过程序化策略应用解决LLM对齐-现实差距问题，实现精确的策略更新而不损害模型性能


<details>
  <summary>Details</summary>
Motivation: 现有LLM对齐方法产生静态、脆弱且维护成本高的模型，无法适应不断发展的规范和政策，存在对齐-现实差距

Method: TRACE框架通过程序化筛选现有偏好数据、计算对齐影响分数识别冲突、应用混合优化（反转、丢弃或保留偏好）来重新对齐

Result: 在多个模型系列上实现稳健的重新对齐，在复杂政策变化下强制执行新原则而不降低通用能力

Conclusion: 为LLM对齐维护建立了可扩展、动态且成本效益高的范式，为可持续和负责任的AI部署提供基础

Abstract: The alignment of Large Language Models (LLMs) with human values is central to
their safe deployment, yet current practice produces static, brittle, and
costly-to-maintain models that fail to keep pace with evolving norms and
policies. This misalignment, which we term the Alignment-Reality Gap, poses a
growing challenge for reliable long-term use. Existing remedies are inadequate:
large-scale re-annotation is economically prohibitive, and standard unlearning
methods act as blunt instruments that erode utility rather than enable precise
policy updates. We introduce TRACE (Triage and Re-align by Alignment Conflict
Evaluation), a framework for principled unlearning that reconceives
re-alignment as a programmatic policy application problem. TRACE
programmatically triages existing preference data against a new policy,
identifies high-impact conflicts via a alignment impact score, and applies a
hybrid optimization that cleanly inverts, discards, or preserves preferences
while safeguarding model performance. Empirical results show that TRACE
achieves robust re-alignment across diverse model families (Qwen2.5-7B,
Gemma-2-9B, Llama-3.1-8B). On both synthetic benchmarks and the PKU-SafeRLHF
dataset under complex policy shift, TRACE enforces new principles without
degrading general capabilities. Our work establishes a scalable, dynamic, and
cost-effective paradigm for maintaining LLM alignment, providing a foundation
for sustainable and responsible AI deployment.

</details>


### [17] [Understanding New-Knowledge-Induced Factual Hallucinations in LLMs: Analysis, Solution, and Interpretation](https://arxiv.org/abs/2511.02626)
*Renfei Dang,Peng Hu,Changjiang Gao,Shujian Huang*

Main category: cs.CL

TL;DR: 研究发现，在LLMs微调中引入新知识会导致事实幻觉，特别是当特定知识类型完全由新知识组成时。作者提出KnownPatch方法，在训练后期加入少量已知知识样本，有效缓解新知识引发的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究未深入探讨新知识微调引发幻觉的具体表现和机制，本研究旨在填补这一空白，分析不同知识类型和任务类型下的幻觉现象。

Method: 设计受控数据集Biography-Reasoning，在多种知识类型和两种任务类型（知识问答和知识推理）上进行细粒度分析，并提出KnownPatch方法在训练后期加入已知知识样本。

Result: 当特定知识类型完全由新知识组成时，LLMs的幻觉倾向显著增加；新知识学习会降低模型对问题中关键实体的注意力，导致过度关注上下文；KnownPatch能有效缓解注意力干扰并提升性能。

Conclusion: 特定知识类型的高陌生度（而非新知识整体比例）是幻觉的主要驱动因素；KnownPatch通过修复注意力模式有效减轻新知识引发的幻觉，且这种改善能传播到相似语境的问题中。

Abstract: Previous studies show that introducing new knowledge during large language
models (LLMs) fine-tuning can lead to the generation of erroneous output when
tested on known information, thereby triggering factual hallucinations.
However, existing studies have not deeply investigated the specific
manifestations and underlying mechanisms of these hallucinations. Our work
addresses this gap by designing a controlled dataset Biography-Reasoning, and
conducting a fine-grained analysis across multiple knowledge types and two task
types, including knowledge question answering (QA) and knowledge reasoning
tasks. We find that when fine-tuned on a dataset in which a specific knowledge
type consists entirely of new knowledge, LLMs exhibit significantly increased
hallucination tendencies. This suggests that the high unfamiliarity of a
particular knowledge type, rather than the overall proportion of new knowledge,
is a stronger driver of hallucinations, and these tendencies can even affect
other knowledge types in QA tasks. To mitigate such factual hallucinations, we
propose KnownPatch, which patches a small number of known knowledge samples in
the later stages of training, effectively alleviating new-knowledge-induced
hallucinations. Through attention analysis, we find that learning new knowledge
reduces the model's attention to key entities in the question, thus causing
excessive focus on the surrounding context, which may increase the risk of
hallucination. Moreover, the attention pattern can propagate to similar
contexts, facilitating the spread of hallucinations to textually similar
questions. Our method effectively mitigates the disruption of new knowledge
learning to the model's attention on key entities, accompanied by improved
performance.

</details>


### [18] [Optimal Singular Damage: Efficient LLM Inference in Low Storage Regimes](https://arxiv.org/abs/2511.02681)
*Mohammadsajad Alipour,Mohammad Mohammadi Amiri*

Main category: cs.CL

TL;DR: 本文提出了一种高效存储微调后参数更新的方法，利用微调更新的低秩和稀疏特性，通过选择性稀疏化低秩近似来提升存储效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型微调后存储成本高昂，研究发现微调主要影响少量参数，需要更高效的存储方案。

Method: 提出最优奇异值损伤方法，基于奇异向量的重要性选择性稀疏化低秩近似更新，保留最关键的分量。

Result: 在相同内存预算下，该方法比单独使用低秩近似或稀疏化具有更高的存储效率和准确率。

Conclusion: 结合低秩近似和选择性稀疏化能有效提升微调模型的存储效率，同时保持模型性能。

Abstract: Large language models (LLMs) are increasingly prevalent across diverse
applications. However, their enormous size limits storage and processing
capabilities to a few well-resourced stakeholders. As a result, most
applications rely on pre-trained LLMs, fine-tuned for specific tasks. However,
even storing the fine-tuned versions of these models remains a significant
challenge due to the wide range of tasks they address. Recently, studies show
that fine-tuning these models primarily affects a small fraction of parameters,
highlighting the need for more efficient storage of fine-tuned models. This
paper focuses on efficient storage of parameter updates in pre-trained models
after fine-tuning. To address this challenge, we leverage the observation that
fine-tuning updates are both low-rank and sparse, which can be utilized for
storage efficiency. However, using only low-rank approximation or
sparsification may discard critical singular components that enhance model
expressivity. We first observe that given the same memory budget, sparsified
low-rank approximations with larger ranks outperform standard low-rank
approximations with smaller ranks. Building on this, we propose our method,
optimal singular damage, that selectively sparsifies low-rank approximated
updates by leveraging the interleaved importance of singular vectors, ensuring
that the most impactful components are retained. We demonstrate through
extensive experiments that our proposed methods lead to significant storage
efficiency and superior accuracy within the same memory budget compared to
employing the low-rank approximation or sparsification individually.

</details>


### [19] [PragExTra: A Multilingual Corpus of Pragmatic Explicitation in Translation](https://arxiv.org/abs/2511.02721)
*Doreen Osmelak,Koel Dutta Chowdhury,Uliana Sentsova,Cristina España-Bonet,Josef van Genabith*

Main category: cs.CL

TL;DR: PragExTra是首个用于检测语用显化的多语言语料库和框架，涵盖8种语言对，通过空对齐和主动学习识别实体描述、度量转换等显化现象，准确率最高达0.88。


<details>
  <summary>Details</summary>
Motivation: 翻译中经常通过添加背景信息来显化隐含的文化含义（语用显化），这一现象在翻译理论中广泛讨论但缺乏计算建模。

Method: 构建PragExTra多语言语料库，使用空对齐识别候选显化案例，通过主动学习和人工标注进行精炼，开发检测框架。

Result: 实体和系统级显化最为常见，主动学习将分类器准确率提高7-8个百分点，跨语言准确率最高达0.88，F1分数0.82。

Conclusion: PragExTra将语用显化确立为可测量的跨语言现象，为构建文化感知的机器翻译迈出重要一步。

Abstract: Translators often enrich texts with background details that make implicit
cultural meanings explicit for new audiences. This phenomenon, known as
pragmatic explicitation, has been widely discussed in translation theory but
rarely modeled computationally. We introduce PragExTra, the first multilingual
corpus and detection framework for pragmatic explicitation. The corpus covers
eight language pairs from TED-Multi and Europarl and includes additions such as
entity descriptions, measurement conversions, and translator remarks. We
identify candidate explicitation cases through null alignments and refined
using active learning with human annotation. Our results show that entity and
system-level explicitations are most frequent, and that active learning
improves classifier accuracy by 7-8 percentage points, achieving up to 0.88
accuracy and 0.82 F1 across languages. PragExTra establishes pragmatic
explicitation as a measurable, cross-linguistic phenomenon and takes a step
towards building culturally aware machine translation. Keywords: translation,
multilingualism, explicitation

</details>


### [20] [AI Diffusion in Low Resource Language Countries](https://arxiv.org/abs/2511.02752)
*Amit Misra,Syed Waqas Zamir,Wassim Hamidouche,Inbal Becker-Reshef,Juan Lavista Ferres*

Main category: cs.CL

TL;DR: LRLCs的AI用户比例比基准低约20%，语言性能差距是AI公平扩散的重要障碍


<details>
  <summary>Details</summary>
Motivation: 前沿大语言模型在低资源语言上表现不佳，可能降低AI效用，从而减缓低资源语言国家的AI采用

Method: 使用加权回归模型从社会经济和人口因素中分离语言效应

Result: 低资源语言国家的AI用户比例比基准低约20%

Conclusion: 语言可访问性是AI公平扩散的重要独立障碍

Abstract: Artificial intelligence (AI) is diffusing globally at unprecedented speed,
but adoption remains uneven. Frontier Large Language Models (LLMs) are known to
perform poorly on low-resource languages due to data scarcity. We hypothesize
that this performance deficit reduces the utility of AI, thereby slowing
adoption in Low-Resource Language Countries (LRLCs). To test this, we use a
weighted regression model to isolate the language effect from socioeconomic and
demographic factors, finding that LRLCs have a share of AI users that is
approximately 20% lower relative to their baseline. These results indicate that
linguistic accessibility is a significant, independent barrier to equitable AI
diffusion.

</details>


### [21] [Controlling Performance and Budget of a Centralized Multi-agent LLM System with Reinforcement Learning](https://arxiv.org/abs/2511.02755)
*Bowen Jin,TJ Collins,Donghan Yu,Mert Cemri,Shenao Zhang,Mengyu Li,Jay Tang,Tian Qin,Zhiyang Xu,Jiarui Lu,Guoli Yin,Jiawei Han,Zirui Wang*

Main category: cs.CL

TL;DR: CoRL是一个集中式多LLM框架，通过强化学习优化性能与推理成本的权衡，在不同预算条件下实现高效的多智能体协作。


<details>
  <summary>Details</summary>
Motivation: 现有去中心化多LLM系统为每个输入调用多个模型，导致推理成本高且不可控，需要设计成本高效且可控的集中式协调框架。

Method: 提出CoRL强化学习框架，使用控制器LLM选择性协调专家模型池，以双目标（最大化任务性能、最小化推理成本）进行优化，并适应不同预算条件。

Result: 在四个基准测试中，CoRL在高预算设置下超越最佳专家LLM，在低预算模式下仍保持强劲性能。

Conclusion: 集中式协调为可扩展且成本高效的多智能体LLM系统提供了有效解决方案。

Abstract: Large language models (LLMs) exhibit complementary strengths across domains
and come with varying inference costs, motivating the design of multi-agent LLM
systems where specialized models collaborate efficiently. Existing approaches
predominantly rely on decentralized frameworks, which invoke multiple LLMs for
every input and thus lead to substantial and uncontrolled inference costs. In
this work, we introduce a centralized multi-LLM framework, where a controller
LLM selectively coordinates a pool of expert models in a cost-efficient and
cost-controllable manner. We formulate this coordination problem as
reinforcement learning with dual objectives: maximizing task performance while
minimizing the overall inference cost. In addition, we expect the multi-agent
system to have adapted behavior with different budget conditions during
inference. To this end, we propose CoRL, a reinforcement learning framework
that optimizes the performance cost trade-off in a controllable multi-budget
setting. Experiments on four diverse benchmarks demonstrate that CoRL enables a
single system to surpass the best expert LLM under high-budget settings, while
maintaining strong performance in more economical low-budget modes,
highlighting the effectiveness of centralized coordination for scalable and
cost-efficient multi-agent LLM systems.

</details>


### [22] [Beyond Single Embeddings: Capturing Diverse Targets with Multi-Query Retrieval](https://arxiv.org/abs/2511.02770)
*Hung-Ting Chen,Xiang Liu,Shauli Ravfogel,Eunsol Choi*

Main category: cs.CL

TL;DR: 提出了一种新的检索器架构AMER，通过自回归生成多个查询向量来解决传统单一查询向量检索器在多模态相关文档分布下的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统检索器只生成一个查询向量，但查询的相关文档分布可能是多模态的（如不同解释），现有检索器在目标文档嵌入距离较大时表现不佳。

Method: 开发了自回归多嵌入检索器(AMER)，自回归生成多个查询向量，所有预测的查询向量都用于从语料库中检索文档。

Result: 在合成向量化数据上，该方法能完美捕获多个目标分布，性能比单嵌入模型好4倍；在真实世界多答案检索数据集上，相比单嵌入基线分别获得4%和21%的相对增益。

Conclusion: 展示了使用多查询向量检索器的潜力，为未来工作开辟了新方向，特别是在目标文档嵌入相似度较低的数据子集上增益更明显。

Abstract: Most text retrievers generate \emph{one} query vector to retrieve relevant
documents. Yet, the conditional distribution of relevant documents for the
query may be multimodal, e.g., representing different interpretations of the
query. We first quantify the limitations of existing retrievers. All retrievers
we evaluate struggle more as the distance between target document embeddings
grows. To address this limitation, we develop a new retriever architecture,
\emph{A}utoregressive \emph{M}ulti-\emph{E}mbedding \emph{R}etriever (AMER).
Our model autoregressively generates multiple query vectors, and all the
predicted query vectors are used to retrieve documents from the corpus. We show
that on the synthetic vectorized data, the proposed method could capture
multiple target distributions perfectly, showing 4x better performance than
single embedding model. We also fine-tune our model on real-world multi-answer
retrieval datasets and evaluate in-domain. AMER presents 4 and 21\% relative
gains over single-embedding baselines on two datasets we evaluate on.
Furthermore, we consistently observe larger gains on the subset of dataset
where the embeddings of the target documents are less similar to each other. We
demonstrate the potential of using a multi-query vector retriever and open up a
new direction for future work.

</details>


### [23] [MemSearcher: Training LLMs to Reason, Search and Manage Memory via End-to-End Reinforcement Learning](https://arxiv.org/abs/2511.02805)
*Qianhao Yuan,Jie Lou,Zichao Li,Jiawei Chen,Yaojie Lu,Hongyu Lin,Le Sun,Debing Zhang,Xianpei Han*

Main category: cs.CL

TL;DR: MemSearcher是一种搜索代理工作流，通过迭代维护紧凑内存来平衡信息完整性和效率，在多轮交互中稳定上下文长度，提高效率而不牺牲准确性。


<details>
  <summary>Details</summary>
Motivation: 传统搜索代理要么拼接整个交互历史导致上下文过长、计算成本高，要么仅使用当前轮次丢弃重要信息。这种权衡限制了搜索代理的可扩展性。

Method: 提出MemSearcher工作流，在每个轮次将用户问题与内存融合生成推理轨迹，执行搜索动作，并更新内存仅保留任务解决所需的关键信息。使用多上下文GRPO强化学习框架联合优化推理、搜索策略和内存管理。

Result: 在七个公共基准测试上显著优于强基线：Qwen2.5-3B-Instruct相对平均提升11%，Qwen2.5-7B-Instruct提升12%。3B版本的MemSearcher甚至优于7B基线。

Conclusion: 在信息完整性和效率之间取得平衡，既能获得更高准确性，又能降低计算开销。

Abstract: Typical search agents concatenate the entire interaction history into the LLM
context, preserving information integrity but producing long, noisy contexts,
resulting in high computation and memory costs. In contrast, using only the
current turn avoids this overhead but discards essential information. This
trade-off limits the scalability of search agents. To address this challenge,
we propose MemSearcher, an agent workflow that iteratively maintains a compact
memory and combines the current turn with it. At each turn, MemSearcher fuses
the user's question with the memory to generate reasoning traces, perform
search actions, and update memory to retain only information essential for
solving the task. This design stabilizes context length across multi-turn
interactions, improving efficiency without sacrificing accuracy. To optimize
this workflow, we introduce multi-context GRPO, an end-to-end RL framework that
jointly optimize reasoning, search strategies, and memory management of
MemSearcher Agents. Specifically, multi-context GRPO samples groups of
trajectories under different contexts and propagates trajectory-level
advantages across all conversations within them. Trained on the same dataset as
Search-R1, MemSearcher achieves significant improvements over strong baselines
on seven public benchmarks: +11% on Qwen2.5-3B-Instruct and +12% on
Qwen2.5-7B-Instruct relative average gains. Notably, the 3B-based MemSearcher
even outperforms 7B-based baselines, demonstrating that striking a balance
between information integrity and efficiency yields both higher accuracy and
lower computational overhead. The code and models will be publicly available at
https://github.com/icip-cas/MemSearcher

</details>


### [24] [Oolong: Evaluating Long Context Reasoning and Aggregation Capabilities](https://arxiv.org/abs/2511.02817)
*Amanda Bertsch,Adithya Pratapa,Teruko Mitamura,Graham Neubig,Matthew R. Gormley*

Main category: cs.CL

TL;DR: Oolong是一个长上下文推理基准测试，包含合成任务和真实世界对话任务，要求模型分析文本块并聚合信息来回答分布性问题。即使前沿模型在128K上下文长度下准确率也不到50%。


<details>
  <summary>Details</summary>
Motivation: 现有长上下文评估主要依赖检索任务，允许模型忽略大部分上下文作为噪声。这仅代表一种任务类型，需要评估模型真正理解和使用长上下文的能力。

Method: Oolong分为两个任务集：Oolong-synth（自然合成任务，可消融推理问题的组件）和Oolong-real（真实世界对话数据推理）。要求模型分析大量示例，在上下文中进行分类和计数，并推理时间和用户关系。

Result: 前沿模型在Oolong上表现不佳，GPT-5、Claude-Sonnet-4和Gemini-2.5-Pro在128K上下文长度下在两个任务集上的准确率都低于50%。

Conclusion: 当前模型在真正理解和推理长上下文方面仍有困难，Oolong基准测试的发布将促进能够推理大量文本的模型开发。

Abstract: As model context lengths continue to grow, concerns about whether models
effectively use the full context length have persisted. While several carefully
designed long-context evaluations have recently been released, these
evaluations tend to rely on retrieval from one or more sections of the context,
which allows nearly all of the context tokens to be disregarded as noise. This
represents only one type of task that might be performed with long context. We
introduce Oolong, a benchmark of long-context reasoning tasks that require
analyzing individual chunks of text on an atomic level, and then aggregating
these analyses to answer distributional questions. Oolong is separated into two
task sets: Oolong-synth, a set of naturalistic synthetic tasks, where we can
easily ablate components of the reasoning problem; and Oolong-real, a
downstream setting which requires reasoning over real-world conversational
data. Oolong requires models to reason over large quantities of examples, to
perform both classification and counting in-context, and to reason over
temporal and user relations. Even frontier models struggle on Oolong, with
GPT-5, Claude-Sonnet-4, and Gemini-2.5-Pro all achieving less than 50% accuracy
on both splits at 128K. We release the data and evaluation harness for Oolong
to enable further development of models that can reason over large quantities
of text.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [25] [An Evaluation of Interleaved Instruction Tuning on Semantic Reasoning Performance in an Audio MLLM](https://arxiv.org/abs/2511.02234)
*Jiawei Liu,Enis Berk Çoban,Zarina Schevchenko,Hao Tang,Zhigang Zhu,Michael I Mandel,Johanna Devaney*

Main category: cs.MM

TL;DR: 研究了在音频多模态大语言模型中交错指令调优的影响，发现交错提示能提升语义推理能力但会降低音频标注能力


<details>
  <summary>Details</summary>
Motivation: 标准的多模态大语言模型训练方法可能无法促进模态的深度融合，限制了模型利用核心语言模型推理能力的能力

Method: 使用LTU模型作为测试平台，在SHARD音频推理基准上实验交错指令调优，将音频token交错在提示中

Result: 零样本交错提示能提升推理任务性能，少量交错训练微调能进一步改善结果，但会牺牲模型的音频标注能力

Conclusion: 交错指令调优能有效提升音频MLLM的语义推理能力，但需要在推理能力和音频标注能力之间权衡

Abstract: Standard training for Multi-modal Large Language Models (MLLMs) involves
concatenating non-textual information, like vision or audio, with a text
prompt. This approach may not encourage deep integration of modalities,
limiting the model's ability to leverage the core language model's reasoning
capabilities. This work examined the impact of interleaved instruction tuning
in an audio MLLM, where audio tokens are interleaved within the prompt. Using
the Listen, Think, and Understand (LTU) model as a testbed, we conduct an
experiment using the Synonym and Hypernym Audio Reasoning Dataset (SHARD), our
newly created reasoning benchmark for audio-based semantic reasoning focusing
on synonym and hypernym recognition. Our findings show that while even
zero-shot interleaved prompting improves performance on our reasoning tasks, a
small amount of fine-tuning using interleaved training prompts improves the
results further, however, at the expense of the MLLM's audio labeling ability.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [26] [CudaForge: An Agent Framework with Hardware Feedback for CUDA Kernel Optimization](https://arxiv.org/abs/2511.01884)
*Zijian Zhang,Rong Wang,Shiyang Li,Yuebo Luo,Mingyi Hong,Caiwen Ding*

Main category: cs.LG

TL;DR: CudaForge是一个无需训练的多智能体工作流，用于自动生成和优化CUDA内核，通过Coder和Judge两个LLM智能体迭代优化，集成硬件反馈，在保持高正确性的同时显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 手动设计CUDA内核成本高且耗时，现有自动方法生成的内核效率低、计算开销大且泛化能力差，需要更高效的自动内核生成方案。

Method: 采用多智能体工作流，包含Coder和Judge两个LLM智能体，迭代生成、测试、分析和优化CUDA内核，集成Nsight Compute硬件反馈指标。

Result: 在KernelBench上达到97.6%的正确率，平均1.68倍速度提升，优于现有最佳模型；支持多种GPU和基础模型，生成优化内核仅需26.5分钟和0.3美元API成本。

Conclusion: 多智能体、无需训练的工作流能够实现成本效益高、可泛化且高性能的CUDA内核优化。

Abstract: Developing efficient CUDA kernels is increasingly critical for AI
applications such as large-scale LLM training. However, manual kernel design is
both costly and time-consuming, motivating automatic approaches that leverage
LLMs for code generation. Existing methods for automatic kernel generation,
however, often produce low-efficiency kernels, incur high computational
overhead, and fail to generalize across settings. In this work, we propose
CudaForge, a training-free multi-agent workflow for CUDA kernel generation and
optimization. Our workflow is inspired by the iterative workflow of human
experts, which contains steps such as developing initial kernels, testing
correctness, analyzing hardware feedback, and iterative improvement. More
specifically, CudaForge employs two LLM agents: a Coder and a Judge, that
iteratively generate, correct, and optimize CUDA kernels, while integrating
hardware feedback such as Nsight Compute (NCU) metrics. In extensive
evaluations, we show that CudaForge, by leveraging base models like OpenAI-o3,
achieves 97.6\% correctness of generated kernels and an average 1.68$\times$
speedup over PyTorch baselines, substantially surpassing state-of-the-art
models including OpenAI-o3 and Kevin on KernelBench. Beyond accuracy and speed,
CudaForge demonstrates strong generalization across GPUs (A100, RTX 6000, 4090,
3090) and base models (OpenAI-o3, GPT-5, gpt-oss-120B, Claude-Sonnet-4,
QwQ-32B), while maintaining high efficiency. In particular, generating an
optimized kernel takes about 26.5 minutes on one RTX6000 and incurs about \$
0.3 API cost, which is significantly cheaper than existing agentic work that
costs 6 H100 hours and \$ 5 API cost per kernel. Our results highlight that
multi-agent, training-free workflows can enable cost-effective, generalizable,
and high-performance CUDA kernel optimization. Code available at
https://github.com/OptimAI-Lab/CudaForge

</details>


### [27] [Retrieval-Augmented Multimodal Depression Detection](https://arxiv.org/abs/2511.01892)
*Ruibo Hou,Shiyu Teng,Jiaqing Liu,Shurong Chai,Yinhao Li,Lanfen Lin,Yen-Wei Chen*

Main category: cs.LG

TL;DR: 提出了一种基于检索增强生成(RAG)的多模态抑郁症检测框架，通过从情感数据集中检索相关内容并生成情感提示来增强情绪表示，在AVEC 2019数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有多模态抑郁症检测方法存在的高计算成本、领域不匹配和静态知识限制等问题，特别是利用情感分析增强情绪理解时的局限性。

Method: 使用检索增强生成(RAG)框架，给定抑郁症相关文本时，从情感数据集中检索语义相关的情感内容，并利用大语言模型(LLM)生成情感提示作为辅助模态。

Result: 在AVEC 2019数据集上取得了最先进的性能，CCC达到0.593，MAE为3.95，超越了之前的迁移学习和多任务学习基线方法。

Conclusion: 提出的RAG框架能够有效增强情绪表示并提高可解释性，为多模态抑郁症检测提供了新的有效解决方案。

Abstract: Multimodal deep learning has shown promise in depression detection by
integrating text, audio, and video signals. Recent work leverages sentiment
analysis to enhance emotional understanding, yet suffers from high
computational cost, domain mismatch, and static knowledge limitations. To
address these issues, we propose a novel Retrieval-Augmented Generation (RAG)
framework. Given a depression-related text, our method retrieves semantically
relevant emotional content from a sentiment dataset and uses a Large Language
Model (LLM) to generate an Emotion Prompt as an auxiliary modality. This prompt
enriches emotional representation and improves interpretability. Experiments on
the AVEC 2019 dataset show our approach achieves state-of-the-art performance
with CCC of 0.593 and MAE of 3.95, surpassing previous transfer learning and
multi-task learning baselines.

</details>


### [28] [TapOut: A Bandit-Based Approach to Dynamic Speculative Decoding](https://arxiv.org/abs/2511.02017)
*Aditya Sridhar,Nish Sinnadurai,Sean Lie,Vithursan Thangarasa*

Main category: cs.LG

TL;DR: TapOut是一种基于多臂老虎机的动态推测解码算法，无需训练即可智能决定推测token数量，实现LLM加速。


<details>
  <summary>Details</summary>
Motivation: 现有动态推测解码方法依赖手动调优的敏感阈值，成本高且跨模型和领域泛化能力差。

Method: 使用多臂老虎机元算法选择多个无参数动态推测策略，基于历史奖励和探索进行策略选择。

Result: 在多样化模型对和数据集上的实验表明，TapOut无需超参数调优即可达到或超越现有动态推测基线的加速效果。

Conclusion: TapOut提供了一种在线、免训练、即插即用的动态推测解码解决方案，有效解决了推测token数量优化问题。

Abstract: Speculative decoding accelerates LLMs by using a lightweight draft model to
generate tokens autoregressively before verifying them in parallel with a
larger target model. However, determining the optimal number of tokens to draft
remains a key challenge limiting the approach's effectiveness. Dynamic
speculative decoding aims to intelligently decide how many tokens to draft to
achieve maximum speedups. Existing methods often rely on hand-tuned, sensitive
thresholds (e.g., token entropy), which are costly to set and generalize poorly
across models and domains. We propose TapOut, an online, training-free,
plug-and-play algorithm for dynamic speculation policy selection using
multi-armed bandits. Our approach employs a meta-algorithm that selects among
multiple parameter-free dynamic speculation strategies based on past reward and
exploration. We conduct extensive experiments across diverse model pairs and
datasets, showing that TapOut achieves competitive or superior speedups
compared to well-established dynamic speculation baselines without any
hyperparameter tuning.

</details>


### [29] [Regularization Through Reasoning: Systematic Improvements in Language Model Classification via Explanation-Enhanced Fine-Tuning](https://arxiv.org/abs/2511.02044)
*Vivswan Shah,Randy Cogill,Hanwei Yue,Gopinath Chennupati,Rinat Khaziev*

Main category: cs.LG

TL;DR: 在LLM微调中，为标签附加解释（即使是随机token序列）能提升分类性能，主要原因是额外的token预算促进了更丰富的中间计算，起到了正则化作用。


<details>
  <summary>Details</summary>
Motivation: 研究在LLM微调过程中为标签附加简要解释是否能产生更好的模型，特别是探索解释的结构性作用而非语义作用。

Method: 使用多LLM集成生成的数据，对7B参数模型进行微调，在六个对话数据集上评估。比较标签加解释训练与仅标签基线的效果，并测试用语法不连贯但词汇对齐的随机token替换真实解释的效果。

Result: 在18个数据集和任务设置中，标签加解释训练均优于仅标签基线。意外发现随机token伪解释也能提升准确率，缩小与真实解释的差距。内部分析显示解释增强模型在中间层具有更高激活熵，输出层预测质量更集中。

Conclusion: 解释增强的微调（无论是真实解释还是精心构建的随机token序列）能提高LLM分类的准确性和可靠性，揭示了token级脚手架在推理过程中如何塑造计算过程。

Abstract: Fine-tuning LLMs for classification typically maps inputs directly to labels.
We ask whether attaching brief explanations to each label during fine-tuning
yields better models. We evaluate conversational response quality along three
axes: naturalness, comprehensiveness, and on-topic adherence, each rated on
5-point scales. Using ensemble-generated data from multiple LLMs, we fine-tune
a 7B-parameter model and test across six diverse conversational datasets.
Across 18 dataset, task settings, label-plus-explanation training outperforms
label-only baselines.
  A central and unexpected result concerns random tokens. We replace
human-written explanations with text that is syntactically incoherent yet
vocabulary-aligned with the originals (e.g., shuffled or bag-of-words
variants). Despite lacking semantics, these pseudo-explanations still improve
accuracy over label-only training and often narrow much of the gap to true
explanations. The effect persists across datasets and training seeds,
indicating that gains arise less from meaning than from structure: the extra
token budget encourages richer intermediate computation and acts as a
regularizer that reduces over-confident shortcuts.
  Internal analyses support this view: explanation-augmented models exhibit
higher activation entropy in intermediate layers alongside sharper predictive
mass at the output layer, consistent with increased deliberation before
decision. Overall, explanation-augmented fine-tuning, whether with genuine
rationales or carefully constructed random token sequences, improves accuracy
and reliability for LLM classification while clarifying how token-level
scaffolding shapes computation during inference.

</details>


### [30] [LLM Probing with Contrastive Eigenproblems: Improving Understanding and Applicability of CCS](https://arxiv.org/abs/2511.02089)
*Stefan F. Schouten,Peter Bloem*

Main category: cs.LG

TL;DR: 本文重新审视了对比一致性搜索(CCS)方法，将其重新表述为特征值问题，提出了相对对比一致性的优化目标，并扩展到多变量情况，解决了随机初始化敏感性问题。


<details>
  <summary>Details</summary>
Motivation: CCS是一种无监督探测方法，用于测试大语言模型是否在内部激活中表示二元特征（如句子真值）。虽然CCS显示出潜力，但其双项目标函数仅被部分理解，需要澄清其机制并扩展其适用性。

Method: 将CCS重新表述为特征值问题，基于相对对比一致性的优化目标，获得闭式解和可解释的特征值，并自然扩展到多变量情况。

Result: 在多个数据集上的评估表明，新方法恢复了与CCS相似的性能，同时避免了随机初始化敏感性问题。

Conclusion: 相对化对比一致性不仅改进了对CCS的理解，还为更广泛的探测和机制可解释性方法开辟了途径。

Abstract: Contrast-Consistent Search (CCS) is an unsupervised probing method able to
test whether large language models represent binary features, such as sentence
truth, in their internal activations. While CCS has shown promise, its two-term
objective has been only partially understood. In this work, we revisit CCS with
the aim of clarifying its mechanisms and extending its applicability. We argue
that what should be optimized for, is relative contrast consistency. Building
on this insight, we reformulate CCS as an eigenproblem, yielding closed-form
solutions with interpretable eigenvalues and natural extensions to multiple
variables. We evaluate these approaches across a range of datasets, finding
that they recover similar performance to CCS, while avoiding problems around
sensitivity to random initialization. Our results suggest that relativizing
contrast consistency not only improves our understanding of CCS but also opens
pathways for broader probing and mechanistic interpretability methods.

</details>


### [31] [Can LLMs subtract numbers?](https://arxiv.org/abs/2511.02795)
*Mayank Jobanputra,Nils Philipp Walter,Maitrey Mehta,Blerta Veseli,Evan Parker Kelly Chapple,Yifan Wang,Sneha Chetani,Ellie Pavlick,Antonio Vergari,Vera Demberg*

Main category: cs.LG

TL;DR: 对8个预训练大语言模型在加减法任务上的系统性研究，发现减法准确率远低于加法，主要问题在于处理负数结果时经常遗漏负号，但指令调优能显著改善这一缺陷。


<details>
  <summary>Details</summary>
Motivation: 虽然现有基准测试主要关注加法和乘法，但减法作为非交换运算具有结构独特性，却较少受到关注。本研究旨在系统评估LLMs在减法任务上的表现和局限性。

Method: 评估8个预训练LLMs在加减法问题上的表现，分析错误模式，并通过少样本学习和指令调优等方法来测试性能改进潜力。

Result: 减法准确率显著低于加法，错误主要集中在a<b的情况，模型经常产生正确数值但遗漏负号。指令调优模型在生成负号方面达到近乎完美的准确率。

Conclusion: LLMs在减法运算中存在明显局限性，特别是在处理负数结果时，但通过指令调优可以有效恢复其算术能力，为理解LLMs算术能力的局限性和可恢复性提供了更清晰的表征。

Abstract: We present a systematic study of subtraction in large language models (LLMs).
While prior benchmarks emphasize addition and multiplication, subtraction has
received comparatively little attention despite being structurally distinct as
a non-commutative operation. We evaluate eight pretrained LLMs spanning four
families on addition and subtraction problems. Our experiments reveal that
subtraction accuracy lags behind addition by a wide margin. We find that the
errors for ($a-b$) are concentrated in cases where ($a<b$). In such cases, LLMs
frequently produce the correct magnitude but omit the negative sign. Probing
analyses show that LLMs internally encode whether results should be negative,
yet this information is often not reflected in generated outputs. We further
test well-known techniques such as few-shot learning and instruction-tuning to
see if they can improve the LLMs' performance. Our results suggest that while
few-shot prompting yields modest gains, the instruction-tuned models achieve
near-perfect accuracies in generating the negative sign. Together, these
findings provide a clearer characterization of the limitations and
recoverability of LLMs' arithmetic capabilities in subtraction.

</details>


### [32] [In Good GRACEs: Principled Teacher Selection for Knowledge Distillation](https://arxiv.org/abs/2511.02833)
*Abhishek Panigrahi,Bingbin Liu,Sadhika Malladi,Sham Kakade,Surbhi Goel*

Main category: cs.LG

TL;DR: 提出了GRACE评分方法，无需访问验证器、教师模型内部信息或测试数据，通过分析学生模型的梯度分布特性来量化教师模型对特定学生任务的蒸馏效果。


<details>
  <summary>Details</summary>
Motivation: 知识蒸馏需要为特定学生-任务组合选择最优教师模型，但传统方法需要昂贵的试错过程。

Method: GRACE通过测量学生模型梯度的分布特性，从信息论角度连接梯度算法的留一稳定性，控制蒸馏学生的泛化性能。

Result: 在GSM8K和MATH数据集上，GRACE与蒸馏后的LLaMA和OLMo学生模型性能强相关（Spearman相关性高达86%），使用GRACE选择的教师模型比简单使用最佳性能教师模型性能提升达7.4%。

Conclusion: GRACE能高效有效地为给定学生模型识别强兼容的教师模型，并为蒸馏过程提供细粒度指导，包括最佳温度设置、尺寸约束下的最佳教师选择等。

Abstract: Knowledge distillation is an efficient strategy to use data generated by
large "teacher" language models to train smaller capable "student" models, but
selecting the optimal teacher for a specific student-task combination requires
expensive trial-and-error. We propose a lightweight score called GRACE to
quantify how effective a teacher will be for post-training a student model.
GRACE measures distributional properties of the student's gradients without
access to a verifier, teacher logits, teacher internals, or test data. From an
information-theoretic perspective, GRACE connects to leave-one-out stability of
gradient-based algorithms, which controls the generalization performance of the
distilled students. On GSM8K and MATH, GRACE correlates strongly (up to 86%
Spearman correlation) with the performance of the distilled LLaMA and OLMo
students. In particular, training a student using the GRACE-selected teacher
can improve the performance by up to 7.4% over naively using the
best-performing teacher. Further, GRACE can provide guidance on crucial design
choices in distillation, including (1) the best temperature to use when
generating from the teacher, (2) the best teacher to use given a size
constraint, and (3) the best teacher to use within a specific model family.
Altogether, our findings demonstrate that GRACE can efficiently and effectively
identify a strongly compatible teacher for a given student and provide
fine-grained guidance on how to perform distillation.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [33] [Automata-Conditioned Cooperative Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2511.02304)
*Beyazit Yalcinkaya,Marcell Vazquez-Chanlatte,Ameesh Shah,Hanna Krasowski,Sanjit A. Seshia*

Main category: cs.MA

TL;DR: 提出了ACC-MARL框架，用于学习基于自动机的多任务多智能体强化学习，解决了集中训练分散执行下的任务分解和协调问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在集中训练分散执行设置下存在样本效率低和仅限于单任务的局限，无法有效处理复杂的时间性目标任务分解。

Method: 使用自动机表示任务，将复杂任务分解为可分配给智能体的子任务，学习任务条件化的分散团队策略，并提出了解决实际可行性的方案。

Result: 实验显示智能体出现了任务感知的多步协调行为，如按按钮开门、扶门和短路任务等，且学到的价值函数可用于测试时最优任务分配。

Conclusion: ACC-MARL框架能够有效处理多任务多智能体协调问题，证明了方法的正确性，并实现了任务感知的智能体协调行为。

Abstract: We study the problem of learning multi-task, multi-agent policies for
cooperative, temporal objectives, under centralized training, decentralized
execution. In this setting, using automata to represent tasks enables the
decomposition of complex tasks into simpler sub-tasks that can be assigned to
agents. However, existing approaches remain sample-inefficient and are limited
to the single-task case. In this work, we present Automata-Conditioned
Cooperative Multi-Agent Reinforcement Learning (ACC-MARL), a framework for
learning task-conditioned, decentralized team policies. We identify the main
challenges to ACC-MARL's feasibility in practice, propose solutions, and prove
the correctness of our approach. We further show that the value functions of
learned policies can be used to assign tasks optimally at test time.
Experiments show emergent task-aware, multi-step coordination among agents,
e.g., pressing a button to unlock a door, holding the door, and
short-circuiting tasks.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [34] [Deep Value Benchmark: Measuring Whether Models Generalize Deep values or Shallow Preferences](https://arxiv.org/abs/2511.02109)
*Joshua Ashkinaze,Hua Shen,Sai Avula,Eric Gilbert,Ceren Budak*

Main category: cs.AI

TL;DR: Deep Value Benchmark (DVB) 是一个评估框架，用于测试大语言模型是否学习到人类基本价值观还是仅仅捕捉表面偏好。研究发现模型普遍难以泛化深层价值观，平均深层价值观泛化率仅为0.30，所有模型表现都低于随机水平。


<details>
  <summary>Details</summary>
Motivation: 区分AI系统是学习到深层人类价值观还是仅捕捉表面偏好模式对于AI对齐至关重要。系统如果只学习表面模式，可能会产生未对齐的行为。

Method: 使用新颖的实验设计，在训练阶段让LLMs接触深层价值观和表面特征故意相关的偏好数据，测试阶段打破这些相关性，测量模型的深层价值观泛化率(DVGR)。

Result: 在9个不同模型中，平均DVGR仅为0.30，所有模型在深层价值观泛化方面表现都低于随机水平，较大模型的DVGR略低于较小模型。

Conclusion: 当前的大语言模型在泛化深层人类价值观方面存在显著不足，DVB提供了一个可解释的衡量AI对齐核心特征的方法。

Abstract: We introduce the Deep Value Benchmark (DVB), an evaluation framework that
directly tests whether large language models (LLMs) learn fundamental human
values or merely surface-level preferences. This distinction is critical for AI
alignment: Systems that capture deeper values are likely to generalize human
intentions robustly, while those that capture only superficial patterns in
preference data risk producing misaligned behavior. The DVB uses a novel
experimental design with controlled confounding between deep values (e.g.,
moral principles) and shallow features (e.g., superficial attributes). In the
training phase, we expose LLMs to human preference data with deliberately
correlated deep and shallow features -- for instance, where a user consistently
prefers (non-maleficence, formal language) options over (justice, informal
language) alternatives. The testing phase then breaks these correlations,
presenting choices between (justice, formal language) and (non-maleficence,
informal language) options. This design allows us to precisely measure a
model's Deep Value Generalization Rate (DVGR) -- the probability of
generalizing based on the underlying value rather than the shallow feature.
Across 9 different models, the average DVGR is just 0.30. All models generalize
deep values less than chance. Larger models have a (slightly) lower DVGR than
smaller models. We are releasing our dataset, which was subject to three
separate human validation experiments. DVB provides an interpretable measure of
a core feature of alignment.

</details>


### [35] [InsurAgent: A Large Language Model-Empowered Agent for Simulating Individual Behavior in Purchasing Flood Insurance](https://arxiv.org/abs/2511.02119)
*Ziheng Geng,Jiachen Liu,Ran Cao,Lu Cheng,Dan M. Frangopol,Minghui Cheng*

Main category: cs.AI

TL;DR: 本文提出了InsurAgent，一个基于LLM的智能体，用于模拟洪水保险决策行为。通过五个模块（感知、检索、推理、行动、记忆）的结合，解决了LLM在定量概率估计上的不足，并能够模拟时间决策演化。


<details>
  <summary>Details</summary>
Motivation: 美国高风险人群的洪水保险参与率极低，需要理解保险决策的行为机制。LLM在模拟人类决策方面展现出潜力，但存在定量概率估计的局限性。

Method: 构建基准数据集评估LLM能力，提出InsurAgent智能体，包含感知、检索（使用RAG技术基于调查数据）、推理（利用LLM常识推断）、行动和记忆五个模块。

Result: LLM对因素有定性理解但定量概率估计不足；InsurAgent通过RAG准确估计边际和二元概率，推理模块能捕获传统模型难以处理的上下文信息，记忆模块支持时间决策演化模拟。

Conclusion: InsurAgent为行为建模和政策分析提供了有价值的工具，能够准确模拟保险决策行为并支持时间演化分析。

Abstract: Flood insurance is an effective strategy for individuals to mitigate
disaster-related losses. However, participation rates among at-risk populations
in the United States remain strikingly low. This gap underscores the need to
understand and model the behavioral mechanisms underlying insurance decisions.
Large language models (LLMs) have recently exhibited human-like intelligence
across wide-ranging tasks, offering promising tools for simulating human
decision-making. This study constructs a benchmark dataset to capture insurance
purchase probabilities across factors. Using this dataset, the capacity of LLMs
is evaluated: while LLMs exhibit a qualitative understanding of factors, they
fall short in estimating quantitative probabilities. To address this
limitation, InsurAgent, an LLM-empowered agent comprising five modules
including perception, retrieval, reasoning, action, and memory, is proposed.
The retrieval module leverages retrieval-augmented generation (RAG) to ground
decisions in empirical survey data, achieving accurate estimation of marginal
and bivariate probabilities. The reasoning module leverages LLM common sense to
extrapolate beyond survey data, capturing contextual information that is
intractable for traditional models. The memory module supports the simulation
of temporal decision evolutions, illustrated through a roller coaster life
trajectory. Overall, InsurAgent provides a valuable tool for behavioral
modeling and policy analysis.

</details>


### [36] [Personalized Decision Modeling: Utility Optimization or Textualized-Symbolic Reasoning](https://arxiv.org/abs/2511.02194)
*Yibo Zhao,Yang Zhao,Hongru Du,Hao Frank Yang*

Main category: cs.AI

TL;DR: ATHENA框架通过结合符号效用发现和语义适应，提升了个体决策预测的准确性，在旅行方式和疫苗选择任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 个体决策模型与群体最优预测存在差距，这源于个体决策过程的独特性，包括数值属性和语言影响。需要一种能整合最优信息的方法来建模个性化选择。

Method: 提出ATHENA框架：第一阶段通过LLM增强的符号发现找到群体级符号效用函数；第二阶段实现个体级语义适应，创建个性化语义模板来建模个性化选择。

Result: 在真实世界的旅行模式和疫苗选择任务中，ATHENA持续优于基于效用、机器学习和其他基于LLM的模型，F1分数比最强前沿模型至少提升6.5%。消融研究证实两个阶段都至关重要且互补。

Conclusion: 通过有机整合符号效用建模和语义适应，ATHENA为建模以人为本的决策提供了新方案。

Abstract: Decision-making models for individuals, particularly in high-stakes scenarios
like vaccine uptake, often diverge from population optimal predictions. This
gap arises from the uniqueness of the individual decision-making process,
shaped by numerical attributes (e.g., cost, time) and linguistic influences
(e.g., personal preferences and constraints). Developing upon Utility Theory
and leveraging the textual-reasoning capabilities of Large Language Models
(LLMs), this paper proposes an Adaptive Textual-symbolic Human-centric
Reasoning framework (ATHENA) to address the optimal information integration.
ATHENA uniquely integrates two stages: First, it discovers robust, group-level
symbolic utility functions via LLM-augmented symbolic discovery; Second, it
implements individual-level semantic adaptation, creating personalized semantic
templates guided by the optimal utility to model personalized choices.
Validated on real-world travel mode and vaccine choice tasks, ATHENA
consistently outperforms utility-based, machine learning, and other LLM-based
models, lifting F1 score by at least 6.5% over the strongest cutting-edge
models. Further, ablation studies confirm that both stages of ATHENA are
critical and complementary, as removing either clearly degrades overall
predictive performance. By organically integrating symbolic utility modeling
and semantic adaptation, ATHENA provides a new scheme for modeling
human-centric decisions. The project page can be found at
https://yibozh.github.io/Athena.

</details>


### [37] [Training Proactive and Personalized LLM Agents](https://arxiv.org/abs/2511.02208)
*Weiwei Sun,Xuhui Zhou,Weihua Du,Xingyao Wang,Sean Welleck,Graham Neubig,Maarten Sap,Yiming Yang*

Main category: cs.AI

TL;DR: 提出了UserVille交互环境和PPP多目标强化学习方法，联合优化AI代理的生产力、主动性和个性化三个维度，在软件工程和深度研究任务上显著优于GPT-5等基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要关注任务成功率，但有效的现实世界代理需要同时优化三个维度：生产力（任务完成）、主动性（提出关键问题）和个性化（适应不同用户偏好）。

Method: 引入UserVille交互环境，使用基于LLM的用户模拟器支持多样化、可配置的用户偏好；提出PPP多目标强化学习方法，联合优化生产力、主动性和个性化。

Result: 在软件工程和深度研究任务上的实验表明，使用PPP训练的代理相比GPT-5等强基线平均提升21.6%，能够提出战略性澄清问题、适应未见过的用户偏好，并通过更好的交互提高任务成功率。

Conclusion: 明确优化以用户为中心的交互对于构建实用有效的AI代理至关重要。

Abstract: While existing work focuses primarily on task success, we argue that
effective real-world agents require optimizing three dimensions: productivity
(task completion), proactivity (asking essential questions), and
personalization (adapting to diverse user preferences). We introduce UserVille,
an interactive environment with LLM-based user simulators enabling diverse,
configurable user preferences. Leveraging UserVille, we introduce PPP, a
multi-objective reinforcement learning approach that jointly optimizes all
three dimensions: Productivity, Proactivity, and Personalization. Experiments
on software engineering and deep research tasks show that agents trained with
PPP achieve substantial improvements over strong baselines such as GPT-5 (+21.6
on average), demonstrating the ability to ask strategic clarifying questions,
adapt to unseen user preferences, and improve task success through better
interaction. This work demonstrates that explicitly optimizing for
user-centered interaction is critical for building practical and effective AI
agents.

</details>


### [38] [Unlocking the Power of Multi-Agent LLM for Reasoning: From Lazy Agents to Deliberation](https://arxiv.org/abs/2511.02303)
*Zhiwei Zhang,Xiaomin Li,Yudi Lin,Hui Liu,Ramraj Chandradevan,Linlin Wu,Minhua Lin,Fali Wang,Xianfeng Tang,Qi He,Suhang Wang*

Main category: cs.AI

TL;DR: 本文分析了多智能体推理中的懒惰行为问题，提出了因果影响度量和可验证奖励机制来缓解该问题，从而充分发挥多智能体框架在复杂推理任务中的潜力。


<details>
  <summary>Details</summary>
Motivation: 在多智能体推理中，经常出现一个智能体主导而另一个贡献很少的懒惰行为，这会削弱协作效果，使多智能体设置退化为无效的单智能体系统。

Method: 1) 理论分析懒惰行为的产生原因；2) 引入稳定高效的因果影响度量方法；3) 提出可验证奖励机制，允许推理智能体丢弃噪声输出、整合指令并在必要时重启推理过程。

Result: 大量实验表明，该框架有效缓解了懒惰智能体行为，释放了多智能体框架在复杂推理任务中的全部潜力。

Conclusion: 通过理论分析和提出的解决方案，成功解决了多智能体推理中的协作问题，为复杂推理任务提供了更有效的多智能体框架。

Abstract: Large Language Models (LLMs) trained with reinforcement learning and
verifiable rewards have achieved strong results on complex reasoning tasks.
Recent work extends this paradigm to a multi-agent setting, where a
meta-thinking agent proposes plans and monitors progress while a reasoning
agent executes subtasks through sequential conversational turns. Despite
promising performance, we identify a critical limitation: lazy agent behavior,
in which one agent dominates while the other contributes little, undermining
collaboration and collapsing the setup to an ineffective single agent. In this
paper, we first provide a theoretical analysis showing why lazy behavior
naturally arises in multi-agent reasoning. We then introduce a stable and
efficient method for measuring causal influence, helping mitigate this issue.
Finally, as collaboration intensifies, the reasoning agent risks getting lost
in multi-turn interactions and trapped by previous noisy responses. To counter
this, we propose a verifiable reward mechanism that encourages deliberation by
allowing the reasoning agent to discard noisy outputs, consolidate
instructions, and restart its reasoning process when necessary. Extensive
experiments demonstrate that our framework alleviates lazy agent behavior and
unlocks the full potential of multi-agent framework for complex reasoning
tasks.

</details>


### [39] [The Collaboration Gap](https://arxiv.org/abs/2511.02687)
*Tim R. Davidson,Adam Fourney,Saleema Amershi,Robert West,Eric Horvitz,Ece Kamar*

Main category: cs.AI

TL;DR: 论文提出协作迷宫求解基准，评估32个主流模型在单独、同质和异质配对中的协作能力，发现存在'协作鸿沟'，并提出'接力推理'方法改善协作效果。


<details>
  <summary>Details</summary>
Motivation: 随着AI发展，我们将越来越多依赖由独立开发的异质智能体组成的系统，这些系统的成功关键在于智能体间的有效协作，但目前缺乏大规模评估智能体协作能力的实证研究。

Method: 提出协作迷宫求解基准框架，该框架能隔离协作能力、调节问题复杂度、支持可扩展自动评分，且不限制输出格式。评估32个开源和闭源模型在三种配置下的表现。

Result: 发现'协作鸿沟'现象：单独表现好的模型在协作时性能显著下降。协作可能完全失败，例如小型蒸馏模型单独能很好解决迷宫，但在某些配对中几乎完全失败。从较强智能体开始能改善结果。

Conclusion: 需要协作感知的评估、增强协作能力的训练策略，以及可靠激发智能体潜在技能的交互设计，这些指导适用于AI-AI和人类-AI协作。

Abstract: The trajectory of AI development suggests that we will increasingly rely on
agent-based systems composed of independently developed agents with different
information, privileges, and tools. The success of these systems will
critically depend on effective collaboration among these heterogeneous agents,
even under partial observability. Despite intense interest, few empirical
studies have evaluated such agent-agent collaboration at scale. We propose a
collaborative maze-solving benchmark that (i) isolates collaborative
capabilities, (ii) modulates problem complexity, (iii) enables scalable
automated grading, and (iv) imposes no output-format constraints, preserving
ecological plausibility. Using this framework, we evaluate 32 leading open- and
closed-source models in solo, homogeneous, and heterogeneous pairings. Our
results reveal a "collaboration gap": models that perform well solo often
degrade substantially when required to collaborate. Collaboration can break
down dramatically; for instance, small distilled models that solve mazes well
alone may fail almost completely in certain pairings. We find that starting
with the stronger agent often improves outcomes, motivating a "relay inference"
approach where the stronger agent leads before handing off to the weaker one,
closing much of the gap. Our findings argue for (1) collaboration-aware
evaluation, (2) training strategies developed to enhance collaborative
capabilities, and (3) interaction design that reliably elicits agents' latent
skills, guidance that applies to AI-AI and human-AI collaboration.

</details>


### [40] [CostBench: Evaluating Multi-Turn Cost-Optimal Planning and Adaptation in Dynamic Environments for LLM Tool-Use Agents](https://arxiv.org/abs/2511.02734)
*Jiayu Liu,Cheng Qian,Zhaochen Su,Qing Zong,Shijue Huang,Bingxiang He,Yi R. Fung*

Main category: cs.AI

TL;DR: CostBench是一个以成本为中心的基准测试，用于评估LLM代理的经济推理和重新规划能力，发现在静态和动态环境下代理在成本优化规划方面存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理评估主要关注任务完成度，忽视了资源效率和适应性，特别是代理在变化环境中制定和调整成本最优计划的能力。

Method: 在旅行规划领域构建CostBench基准，包含可通过多种原子和复合工具序列解决的任务，支持四种动态阻塞事件（如工具故障和成本变化）来模拟现实世界的不确定性。

Result: 评估显示代理在成本感知规划方面存在显著差距：在静态设置中经常无法找到成本最优解，GPT-5在最难任务上的精确匹配率低于75%，在动态条件下性能进一步下降约40%。

Conclusion: 通过诊断这些弱点，CostBench为开发既经济理性又鲁棒的未来代理奠定了基础。

Abstract: Current evaluations of Large Language Model (LLM) agents primarily emphasize
task completion, often overlooking resource efficiency and adaptability. This
neglects a crucial capability: agents' ability to devise and adjust
cost-optimal plans in response to changing environments. To bridge this gap, we
introduce CostBench, a scalable, cost-centric benchmark designed to evaluate
agents' economic reasoning and replanning abilities. Situated in the
travel-planning domain, CostBench comprises tasks solvable via multiple
sequences of atomic and composite tools with diverse, customizable costs. It
also supports four types of dynamic blocking events, such as tool failures and
cost changes, to simulate real-world unpredictability and necessitate agents to
adapt in real time. Evaluating leading open-sourced and proprietary models on
CostBench reveals a substantial gap in cost-aware planning: agents frequently
fail to identify cost-optimal solutions in static settings, with even GPT-5
achieving less than 75% exact match rate on the hardest tasks, and performance
further dropping by around 40% under dynamic conditions. By diagnosing these
weaknesses, CostBench lays the groundwork for developing future agents that are
both economically rational and robust.

</details>


### [41] [Agent-Omni: Test-Time Multimodal Reasoning via Model Coordination for Understanding Anything](https://arxiv.org/abs/2511.02834)
*Huawei Lin,Yunzhi Shi,Tong Geng,Weijie Zhao,Wei Wang,Ravender Pal Singh*

Main category: cs.AI

TL;DR: 提出了Agent-Omni框架，通过主代理系统协调现有基础模型，实现无需重新训练的灵活多模态推理。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型局限于固定模态对，需要大量对齐数据和昂贵微调，构建全模态能力模型仍不实用且缺乏鲁棒推理支持。

Method: 采用主代理系统，主代理解释用户意图，将子任务委托给模态特定代理，并整合它们的输出形成连贯响应。

Result: 在文本、图像、音频、视频和全模态基准测试中，Agent-Omni始终达到最先进性能，特别是在需要复杂跨模态推理的任务上。

Conclusion: 基于代理的设计实现了专用基础模型的无缝集成，确保了对多样化输入的适应性，同时保持了透明度和可解释性。该框架模块化且易于扩展，允许随着更强模型的可用性进行未来改进。

Abstract: Multimodal large language models (MLLMs) have shown strong capabilities but
remain limited to fixed modality pairs and require costly fine-tuning with
large aligned datasets. Building fully omni-capable models that can integrate
text, images, audio, and video remains impractical and lacks robust reasoning
support. In this paper, we propose an Agent-Omni framework that coordinates
existing foundation models through a master-agent system, enabling flexible
multimodal reasoning without retraining. The master agent interprets user
intent, delegates subtasks to modality-specific agents, and integrates their
outputs into coherent responses. Extensive experiments across text, image,
audio, video, and omni benchmarks show that Agent-Omni consistently achieves
state-of-the-art performance, particularly on tasks requiring complex
cross-modal reasoning. Its agent-based design enables seamless integration of
specialized foundation models, ensuring adaptability to diverse inputs while
maintaining transparency and interpretability. In addition, the framework is
modular and easily extensible, allowing future improvements as stronger models
become available. %We release an open-source implementation to support
continued research on scalable and reliable omni-modal reasoning.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [42] [SciDaSynth: Interactive Structured Data Extraction from Scientific Literature with Large Language Model](https://arxiv.org/abs/2404.13765)
*Xingbo Wang,Samantha L. Huey,Rui Sheng,Saurabh Mehta,Fei Wang*

Main category: cs.HC

TL;DR: SciDaSynth是一个基于大语言模型的交互式系统，能够从科学文献中自动提取多模态信息并生成结构化数据表格，支持数据验证和精炼，提高数据提取效率。


<details>
  <summary>Details</summary>
Motivation: 科学文献爆炸式增长使得高效准确提取结构化数据变得至关重要，但现有工具难以处理多模态、多样化且不一致的信息。

Method: 利用大语言模型从文本、表格和图形等多样化来源整合信息，自动生成结构化数据表格，并提供多维度可视化摘要和语义分组功能来解决跨文档数据不一致问题。

Result: 与营养学和NLP研究人员的用户研究表明，SciDaSynth在生成高质量结构化数据方面比基线方法更高效。

Conclusion: SciDaSynth证明了人机协作系统在数据提取任务中的有效性，为相关系统设计提供了启示。

Abstract: The explosion of scientific literature has made the efficient and accurate
extraction of structured data a critical component for advancing scientific
knowledge and supporting evidence-based decision-making. However, existing
tools often struggle to extract and structure multimodal, varied, and
inconsistent information across documents into standardized formats. We
introduce SciDaSynth, a novel interactive system powered by large language
models (LLMs) that automatically generates structured data tables according to
users' queries by integrating information from diverse sources, including text,
tables, and figures. Furthermore, SciDaSynth supports efficient table data
validation and refinement, featuring multi-faceted visual summaries and
semantic grouping capabilities to resolve cross-document data inconsistencies.
A within-subjects study with nutrition and NLP researchers demonstrates
SciDaSynth's effectiveness in producing high-quality structured data more
efficiently than baseline methods. We discuss design implications for human-AI
collaborative systems supporting data extraction tasks. The system code is
available at https://github.com/xingbow/SciDaEx

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [43] [Complete asymptotic type-token relationship for growing complex systems with inverse power-law count rankings](https://arxiv.org/abs/2511.02069)
*Pablo Rosillo-Rodes,Laurent Hébert-Dufresne,Peter Sheridan Dodds*

Main category: physics.soc-ph

TL;DR: 本文提出了一个理想化的增长系统模型，能够确定性地产生任意逆幂律类型计数排名，并推导出类型-标记关系的精确渐近表达式，统一了所有α值的情况。


<details>
  <summary>Details</summary>
Motivation: 复杂系统的增长动态常表现出幂律关系的统计规律性，如Zipf定律和Heaps定律。现有工作存在局限性，需要改进和修正特殊情况的渐近表达式。

Method: 构建一个理想化的增长系统模型，仅基于计数排名的形式，避免不必要的近似，不涉及随机机制或采样过程，直接推导类型-标记关系的精确渐近表达式。

Result: 获得了统一的渐近表达式，适用于所有α值，修正了α=1和α≫1的特殊情况，证明了类型-标记关系仅作为Zipf定律的结果出现。

Conclusion: 通过仅依赖计数排名形式的确定性方法，成功推导出类型-标记关系的精确渐近表达式，统一了所有参数情况，为复杂系统的增长动态提供了更完善的理论基础。

Abstract: The growth dynamics of complex systems often exhibit statistical regularities
involving power-law relationships. For real finite complex systems formed by
countable tokens (animals, words) as instances of distinct types (species,
dictionary entries), an inverse power-law scaling $S \sim r^{-\alpha}$ between
type count $S$ and type rank $r$, widely known as Zipf's law, is widely
observed to varying degrees of fidelity. A secondary, summary relationship is
Heaps' law, which states that the number of types scales sublinearly with the
total number of observed tokens present in a growing system. Here, we propose
an idealized model of a growing system that (1) deterministically produces
arbitrary inverse power-law count rankings for types, and (2) allows us to
determine the exact asymptotics of the type-token relationship. Our argument
improves upon and remedies earlier work. We obtain a unified asymptotic
expression for all values of $\alpha$, which corrects the special cases of
$\alpha = 1$ and $\alpha \gg 1$. Our approach relies solely on the form of
count rankings, avoids unnecessary approximations, and does not involve any
stochastic mechanisms or sampling processes. We thereby demonstrate that a
general type-token relationship arises solely as a consequence of Zipf's law.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [44] [SAIL-RL: Guiding MLLMs in When and How to Think via Dual-Reward RL Tuning](https://arxiv.org/abs/2511.02280)
*Fangxun Shu,Yongjie Ye,Yue Liao,Zijian Kang,Weijie Yin,Jiacong Wang,Xiao Liang,Shuicheng Yan,Chao Feng*

Main category: cs.CV

TL;DR: SAIL-RL是一个强化学习后训练框架，通过教导多模态大语言模型何时以及如何思考来增强其推理能力，采用双奖励系统解决现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有方法受限于仅基于结果的监督（只奖励正确答案而不确保推理质量）和统一的思考策略（在简单任务上过度思考，在复杂任务上思考不足）。

Method: 采用双奖励系统：思考奖励评估推理质量（事实基础、逻辑一致性、答案一致性），判断奖励自适应确定深度推理或直接回答的适用性。

Result: 在SAIL-VL2上的实验表明，SAIL-RL在4B和8B规模上提升了推理和多模态理解基准性能，与GPT-4o等商业闭源模型竞争，并显著减少幻觉。

Conclusion: SAIL-RL是构建更可靠和自适应多模态大语言模型的原则性框架。

Abstract: We introduce SAIL-RL, a reinforcement learning (RL) post-training framework
that enhances the reasoning capabilities of multimodal large language models
(MLLMs) by teaching them when and how to think. Existing approaches are limited
by outcome-only supervision, which rewards correct answers without ensuring
sound reasoning, and by uniform thinking strategies, which often lead to
overthinking on simple tasks and underthinking on complex ones. SAIL-RL
addresses these challenges with a dual reward system: the Thinking Reward,
which evaluates reasoning quality through factual grounding, logical coherence,
and answer consistency, and the Judging Reward, which adaptively determines
whether deep reasoning or direct answering is appropriate. Experiments on the
state-of-the-art SAIL-VL2 show that SAIL-RL improves reasoning and multimodal
understanding benchmarks at both 4B and 8B scales, achieving competitive
performance against commercial closed-source models such as GPT-4o, and
substantially reduces hallucinations, establishing it as a principled framework
for building more reliable and adaptive MLLMs. The code will be available at
https://github.com/BytedanceDouyinContent/SAIL-RL.

</details>


### [45] [Link prediction Graph Neural Networks for structure recognition of Handwritten Mathematical Expressions](https://arxiv.org/abs/2511.02288)
*Cuong Tuan Nguyen,Ngoc Tuan Nguyen,Triet Hoang Minh Dao,Huy Minh Nhat,Huy Truong Dinh*

Main category: cs.CV

TL;DR: 提出基于图神经网络的方法，将手写数学表达式建模为图结构，通过符号分割、识别和空间关系分类构建初始图，再用2D-CFG解析器和GNN链接预测模型优化结构。


<details>
  <summary>Details</summary>
Motivation: 传统方法在手写数学表达式结构识别方面存在局限，需要更有效地捕捉符号间的空间依赖关系。

Method: 使用深度BLSTM网络进行符号分割、识别和空间关系分类构建初始图，结合2D-CFG解析器生成所有可能空间关系，GNN链接预测模型去除不必要连接形成最终符号标签图。

Result: 实验结果表明该方法在手写数学表达式结构识别方面表现出色，具有良好性能。

Conclusion: 基于图神经网络的方法能有效识别手写数学表达式的结构，为相关领域提供了有前景的解决方案。

Abstract: We propose a Graph Neural Network (GNN)-based approach for Handwritten
Mathematical Expression (HME) recognition by modeling HMEs as graphs, where
nodes represent symbols and edges capture spatial dependencies. A deep BLSTM
network is used for symbol segmentation, recognition, and spatial relation
classification, forming an initial primitive graph. A 2D-CFG parser then
generates all possible spatial relations, while the GNN-based link prediction
model refines the structure by removing unnecessary connections, ultimately
forming the Symbol Label Graph. Experimental results demonstrate the
effectiveness of our approach, showing promising performance in HME structure
recognition.

</details>


### [46] [CoCoVa: Chain of Continuous Vision-Language Thought for Latent Space Reasoning](https://arxiv.org/abs/2511.02360)
*Jizheng Ma,Xiaofei Zhou,Yanlong Song,Han Yan*

Main category: cs.CV

TL;DR: CoCoVa是一个新颖的视觉语言模型框架，通过连续跨模态推理解决传统VLMs在离散语言空间推理的局限性，使用潜在思维向量链进行迭代推理，在多个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 人类认知中存在许多难以用语言表达的思维过程，而当前的视觉语言模型被限制在离散的语言标记空间中推理，无法充分利用视觉感知的丰富多维特性。

Method: 提出CoCoVa框架，核心是迭代推理循环，使用新颖的Latent Q-Former作为动态推理引擎，通过跨模态融合迭代优化潜在思维向量链，结合对比学习和基于扩散的重构进行多任务训练。

Result: CoCoVa在准确性和标记效率上优于强基线模型，1.5B骨干网络在几乎所有基准测试中与7B-9B模型相当或更优，扩展到7B骨干时仍与最先进模型竞争。

Conclusion: 学习的潜在空间捕获了可解释的结构化推理模式，展示了CoCoVa在弥合离散语言处理与连续视觉理解之间表征差距的潜力。

Abstract: In human cognition, there exist numerous thought processes that are tacit and
beyond verbal expression, enabling us to understand and interact with the world
in multiple ways. However, contemporary Vision-Language Models (VLMs) remain
constrained to reasoning within the discrete and rigid space of linguistic
tokens, thereby bottlenecking the rich, high-dimensional nature of visual
perception. To bridge this gap, we propose CoCoVa (Chain of Continuous
Vision-Language Thought), a novel framework for vision-language model that
leverages continuous cross-modal reasoning for diverse vision-language tasks.
The core of CoCoVa is an iterative reasoning cycle, where a novel Latent
Q-Former (LQ-Former) acts as a dynamic reasoning engine, iteratively refining a
chain of latent thought vectors through cross-modal fusion. To focus this
process, a token selection mechanism dynamically identifies salient visual
regions, mimicking attentional focus. To ensure these latent thoughts remain
grounded, we train the model with a multi-task objective that combines
contrastive learning and diffusion-based reconstruction, enforcing alignment
between latent representations and both visual and textual modalities.
Evaluations show CoCoVa improves accuracy and token efficiency over strong
baselines. With a 1.5B backbone, it competes with or surpasses larger 7B-9B
models on almost all benchmarks. When scaled to 7B LLM backbones, it remains
competitive with state-of-the-art models. Qualitative analysis validates that
learned latent space captures interpretable and structured reasoning patterns,
highlighting the potential of CoCoVa to bridge the representational gap between
discrete language processing and the continuous nature of visual understanding.

</details>


### [47] [DetectiumFire: A Comprehensive Multi-modal Dataset Bridging Vision and Language for Fire Understanding](https://arxiv.org/abs/2511.02495)
*Zixuan Liu,Siavash H. Khajavi,Guangkai Jiang*

Main category: cs.CV

TL;DR: 提出了DetectiumFire数据集，这是一个包含22.5k高分辨率火灾图像和2.5k真实火灾视频的大规模多模态数据集，用于解决火灾领域缺乏高质量标注数据的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态模型在火灾领域应用受限，主要原因是缺乏公开可用的高质量火灾标注数据集。

Method: 构建了DetectiumFire数据集，包含大量火灾相关图像和视频，标注了传统计算机视觉标签（如边界框）和详细的文本提示，支持合成数据生成和火灾风险推理等应用。

Result: 该数据集在规模、多样性和数据质量方面优于现有基准，显著减少了冗余并增强了真实场景的覆盖。在目标检测、扩散图像生成和视觉语言推理等任务中验证了其有效性。

Conclusion: DetectiumFire数据集有潜力推动火灾相关研究，支持智能安全系统的开发，并已公开发布以促进AI社区对火灾理解的探索。

Abstract: Recent advances in multi-modal models have demonstrated strong performance in
tasks such as image generation and reasoning. However, applying these models to
the fire domain remains challenging due to the lack of publicly available
datasets with high-quality fire domain annotations. To address this gap, we
introduce DetectiumFire, a large-scale, multi-modal dataset comprising of 22.5k
high-resolution fire-related images and 2.5k real-world fire-related videos
covering a wide range of fire types, environments, and risk levels. The data
are annotated with both traditional computer vision labels (e.g., bounding
boxes) and detailed textual prompts describing the scene, enabling applications
such as synthetic data generation and fire risk reasoning. DetectiumFire offers
clear advantages over existing benchmarks in scale, diversity, and data
quality, significantly reducing redundancy and enhancing coverage of real-world
scenarios. We validate the utility of DetectiumFire across multiple tasks,
including object detection, diffusion-based image generation, and
vision-language reasoning. Our results highlight the potential of this dataset
to advance fire-related research and support the development of intelligent
safety systems. We release DetectiumFire to promote broader exploration of fire
understanding in the AI community. The dataset is available at
https://kaggle.com/datasets/38b79c344bdfc55d1eed3d22fbaa9c31fad45e27edbbe9e3c529d6e5c4f93890

</details>


### [48] [UniChange: Unifying Change Detection with Multimodal Large Language Model](https://arxiv.org/abs/2511.02607)
*Xu Zhang,Danyang Li,Xiaohang Dong,Tianhao Wu,Hualong Yu,Jianye Wang,Qicheng Li,Xiang Li*

Main category: cs.CV

TL;DR: UniChange是首个基于多模态大语言模型(MLLM)的统一变化检测框架，通过引入特殊令牌[T1]、[T2]、[CHANGE]和文本提示，同时支持二元变化检测(BCD)和语义变化检测(SCD)任务。


<details>
  <summary>Details</summary>
Motivation: 当前变化检测模型通常只能从单一类型标注数据中学习，无法同时利用多样化的BCD和SCD数据集，导致泛化能力差和功能有限。MLLM的发展为统一变化检测框架提供了新的可能性。

Method: 利用MLLM的语言先验和统一能力，集成生成式语言能力与专门的变化检测功能。通过三个特殊令牌统一BCD和SCD任务，使用文本提示指导变化类别识别，无需预定义分类头。

Result: 在四个公共基准测试(WHU-CD、S2Looking、LEVIR-CD+、SECOND)上达到最先进性能，IoU分数分别为90.41、53.04、78.87和57.62，超越所有先前方法。

Conclusion: UniChange成功构建了首个基于MLLM的统一变化检测模型，能够从多源数据集中有效获取知识，即使类别定义存在冲突，展现了强大的泛化能力和多功能性。

Abstract: Change detection (CD) is a fundamental task for monitoring and analyzing land
cover dynamics. While recent high performance models and high quality datasets
have significantly advanced the field, a critical limitation persists. Current
models typically acquire limited knowledge from single-type annotated data and
cannot concurrently leverage diverse binary change detection (BCD) and semantic
change detection (SCD) datasets. This constraint leads to poor generalization
and limited versatility. The recent advancements in Multimodal Large Language
Models (MLLMs) introduce new possibilities for a unified CD framework. We
leverage the language priors and unification capabilities of MLLMs to develop
UniChange, the first MLLM-based unified change detection model. UniChange
integrates generative language abilities with specialized CD functionalities.
Our model successfully unifies both BCD and SCD tasks through the introduction
of three special tokens: [T1], [T2], and [CHANGE]. Furthermore, UniChange
utilizes text prompts to guide the identification of change categories,
eliminating the reliance on predefined classification heads. This design allows
UniChange to effectively acquire knowledge from multi-source datasets, even
when their class definitions conflict. Experiments on four public benchmarks
(WHU-CD, S2Looking, LEVIR-CD+, and SECOND) demonstrate SOTA performance,
achieving IoU scores of 90.41, 53.04, 78.87, and 57.62, respectively,
surpassing all previous methods. The code is available at
https://github.com/Erxucomeon/UniChange.

</details>


### [49] [VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual Representation](https://arxiv.org/abs/2511.02778)
*Kevin Qinghong Lin,Yuhao Zheng,Hangyu Ran,Dantong Zhu,Dongxing Mao,Linjie Li,Philip Torr,Alex Jinpeng Wang*

Main category: cs.CV

TL;DR: VCode是一个将多模态理解重新定义为代码生成的基准，使用SVG作为紧凑、可解释的可执行视觉表示。VCoder框架通过迭代修订和视觉工具增强VLM，在多个基准上显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 当前AI进展主要关注语言为中心的编程任务，而视觉为中心的编码研究不足。受人类通过草图推理的启发，作者倡导使用SVG代码作为视觉表示。

Method: 提出VCode基准，将多模态理解重构为SVG代码生成任务。引入VCoder框架，包含两个核心组件：(i) 带修订的思考 - 迭代分析差异并优化SVG代码；(ii) 带视觉工具的行动 - 使用检测器和解析器提供结构化视觉线索。

Result: 前沿VLM在生成忠实SVG方面表现不佳。VCoder相比表现最佳的Claude-4-Opus实现了12.3分的整体提升。人类研究显示人类和VLM在渲染SVG上都表现更差，但一致性揭示了符号视觉表示的潜力。

Conclusion: SVG代码作为视觉表示具有前景，但当前VLM在视觉为中心编码方面存在显著差距。VCoder框架通过结合修订机制和视觉工具有效提升了性能。

Abstract: Code has emerged as a precise and executable medium for reasoning and action
in the agent era. Yet, progress has largely focused on language-centric tasks
such as program synthesis and debugging, leaving visual-centric coding
underexplored. Inspired by how humans reason over sketches, we advocate SVG
code as a compact, interpretable, and executable visual representation. We
introduce VCode, a benchmark that reframes multimodal understanding as code
generation: given an image, a model must produce SVG that preserves symbolic
meaning for downstream reasoning. VCode covers three domains - general
commonsense (MM-Vet), professional disciplines (MMMU), and visual-centric
perception (CV-Bench). To assess symbolic fidelity, we propose CodeVQA, a novel
evaluation protocol in which a policy model answers questions over rendered
SVGs; correct answers indicate faithful symbolic preservation. Empirically,
frontier VLMs struggle to generate faithful SVGs, revealing a persistent gap
between language-centric and visual-centric coding. To close this gap, we
introduce VCoder, an agentic framework that augments VLMs along two axes: (i)
Thinking with Revision, which iteratively analyzes discrepancies and refines
SVG code; and (ii) Acting with Visual Tools, where detectors and parsers supply
structured cues such as objects, shapes, and text beyond the model's intrinsic
capacity. Across benchmarks, frontier VLMs with strong reasoning capabilities
score well overall yet remain limited in professional knowledge and 3D
reasoning. VCoder delivers a 12.3-point overall gain over the top-performing
Claude-4-Opus. Human studies show that both humans and VLMs perform worse on
rendered SVGs, their consistency reveals the promise of symbolic visual
representation. The benchmark and code are available at
https://github.com/CSU-JPG/VCode.

</details>
