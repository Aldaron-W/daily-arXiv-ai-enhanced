<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 52]
- [cs.AI](#cs.AI) [Total: 3]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.IR](#cs.IR) [Total: 4]
- [cs.CV](#cs.CV) [Total: 2]
- [cs.LG](#cs.LG) [Total: 2]
- [eess.AS](#eess.AS) [Total: 3]
- [cs.CR](#cs.CR) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Structured Information Matters: Explainable ICD Coding with Patient-Level Knowledge Graphs](https://arxiv.org/abs/2509.09699)
*Mingyang Li,Viktor Schlegel,Tingting Mu,Warren Del-Pinto,Goran Nenadic*

Main category: cs.CL

TL;DR: 该论文提出使用知识图谱(KG)结构化表示临床文档，用于自动化ICD编码任务，在减少文本量的同时保留关键信息，显著提升了编码性能


<details>
  <summary>Details</summary>
Motivation: 临床文档标准化编码对医疗研究和患者护理至关重要，但人工编码耗时且难以规模化。现有方法主要关注输出编码表示，而输入文档的外部知识表示研究不足

Method: 构建文档级知识图谱来结构化表示患者病历，将原始文本压缩至23%同时保留90%信息，并集成到最先进的PLM-ICD架构中进行ICD-9编码

Result: 在主流基准测试中Macro-F1分数提升高达3.20%，同时提高了训练效率。知识图谱中的实体和关系类型对性能提升有重要贡献

Conclusion: 基于知识图谱的结构化文档表示方法能有效提升自动化ICD编码性能，同时增强了模型的可解释性，为临床文档处理提供了新思路

Abstract: Mapping clinical documents to standardised clinical vocabularies is an
important task, as it provides structured data for information retrieval and
analysis, which is essential to clinical research, hospital administration and
improving patient care. However, manual coding is both difficult and
time-consuming, making it impractical at scale. Automated coding can
potentially alleviate this burden, improving the availability and accuracy of
structured clinical data. The task is difficult to automate, as it requires
mapping to high-dimensional and long-tailed target spaces, such as the
International Classification of Diseases (ICD). While external knowledge
sources have been readily utilised to enhance output code representation, the
use of external resources for representing the input documents has been
underexplored. In this work, we compute a structured representation of the
input documents, making use of document-level knowledge graphs (KGs) that
provide a comprehensive structured view of a patient's condition. The resulting
knowledge graph efficiently represents the patient-centred input documents with
23\% of the original text while retaining 90\% of the information. We assess
the effectiveness of this graph for automated ICD-9 coding by integrating it
into the state-of-the-art ICD coding architecture PLM-ICD. Our experiments
yield improved Macro-F1 scores by up to 3.20\% on popular benchmarks, while
improving training efficiency. We attribute this improvement to different types
of entities and relationships in the KG, and demonstrate the improved
explainability potential of the approach over the text-only baseline.

</details>


### [2] [Cross-Layer Attention Probing for Fine-Grained Hallucination Detection](https://arxiv.org/abs/2509.09700)
*Malavika Suresh,Rahaf Aljundi,Ikechukwu Nkisi-Orji,Nirmalie Wiratunga*

Main category: cs.CL

TL;DR: 提出Cross-Layer Attention Probing (CLAP)方法，通过处理LLM整个残差流的激活作为联合序列来检测幻觉，相比基线方法在多种任务和模型上都有提升，支持细粒度检测和检测-缓解策略。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在各种应用中的大规模采用，其生成不准确文本（幻觉）的倾向引发了可靠性担忧，需要有效的幻觉检测技术。

Method: Cross-Layer Attention Probing (CLAP)，一种新颖的激活探测技术，将LLM整个残差流的激活作为联合序列进行处理。

Result: 在五个LLM和三个任务上的实证评估显示，CLAP相比基线方法在贪婪解码和高温采样响应中都能更好地检测幻觉，支持细粒度检测，并且在分布外场景下仍保持高可靠性。

Conclusion: CLAP能够有效检测LLM幻觉，支持检测-缓解策略来减少幻觉并提高模型可靠性，且具有良好的泛化能力。

Abstract: With the large-scale adoption of Large Language Models (LLMs) in various
applications, there is a growing reliability concern due to their tendency to
generate inaccurate text, i.e. hallucinations. In this work, we propose
Cross-Layer Attention Probing (CLAP), a novel activation probing technique for
hallucination detection, which processes the LLM activations across the entire
residual stream as a joint sequence. Our empirical evaluations using five LLMs
and three tasks show that CLAP improves hallucination detection compared to
baselines on both greedy decoded responses as well as responses sampled at
higher temperatures, thus enabling fine-grained detection, i.e. the ability to
disambiguate hallucinations and non-hallucinations among different sampled
responses to a given prompt. This allows us to propose a detect-then-mitigate
strategy using CLAP to reduce hallucinations and improve LLM reliability
compared to direct mitigation approaches. Finally, we show that CLAP maintains
high reliability even when applied out-of-distribution.

</details>


### [3] [Optimal Multi-Task Learning at Regularization Horizon for Speech Translation Task](https://arxiv.org/abs/2509.09701)
*JungHo Jung,Junhyun Lee*

Main category: cs.CL

TL;DR: 本文从正则化角度分析多任务学习在端到端语音翻译中的应用，探索跨模态一致性正则化和同模态R-drop正则化，提出正则化地平线概念来优化超参数配置


<details>
  <summary>Details</summary>
Motivation: 端到端语音翻译面临配对语音-文本数据稀缺问题，需要利用机器翻译的双语数据通过多任务学习来提升性能

Method: 从正则化视角构建多任务学习框架，研究跨模态一致性正则化和同模态R-drop正则化，分析机器翻译损失系数作为正则化源的作用，提出正则化地平线概念

Result: 在MuST-C数据集上，通过在正则化地平线内调节超参数，实现了接近最先进的性能

Conclusion: 多任务学习中的三种正则化源（跨模态一致性、同模态R-drop、MT损失系数）共同构成了有效的正则化策略，正则化地平线为超参数优化提供了理论指导

Abstract: End-to-end speech-to-text translation typically suffers from the scarcity of
paired speech-text data. One way to overcome this shortcoming is to utilize the
bitext data from the Machine Translation (MT) task and perform Multi-Task
Learning (MTL). In this paper, we formulate MTL from a regularization
perspective and explore how sequences can be regularized within and across
modalities. By thoroughly investigating the effect of consistency
regularization (different modality) and R-drop (same modality), we show how
they respectively contribute to the total regularization. We also demonstrate
that the coefficient of MT loss serves as another source of regularization in
the MTL setting. With these three sources of regularization, we introduce the
optimal regularization contour in the high-dimensional space, called the
regularization horizon. Experiments show that tuning the hyperparameters within
the regularization horizon achieves near state-of-the-art performance on the
MuST-C dataset.

</details>


### [4] [Creativity Benchmark: A benchmark for marketing creativity for LLM models](https://arxiv.org/abs/2509.09702)
*Ninad Bhat,Kieran Browne,Pip Bingemann*

Main category: cs.CL

TL;DR: 创意性测试基准：用于评估LLM在营销创意中的表现，发现模型间性能差异小，自动评分与人类评价相关性弱，强调专业人员评估的重要性


<details>
  <summary>Details</summary>
Motivation: 构建一个系统化的评估框架来测量大语言模型在营销创意领域的能力，解决当前缺乏标准化创意性评测方法的问题

Method: 涉及100个品牌（12个类别）和三种提示类型（洞察、想法、野蜜想法），收集678名专业创意人员对11,012个匿名比较的成对偏好，使用Bradley-Terry模型分析，并通过余弦距离测量模型多样性

Result: 模型间性能聚集窄精，没有模型能够在所有品牌或提示类型中都显著领先；最高与最低模型的胜率仅为61%；三种LLM作为判官的设置与人类排名相关性弱且不一致，存在判官特定偏见

Conclusion: 自动化判官无法取代人类评估，传统创意测试在品牌约束任务中仅部分适用，强调需要专业人员评估和关注多样性的工作流程

Abstract: We introduce Creativity Benchmark, an evaluation framework for large language
models (LLMs) in marketing creativity. The benchmark covers 100 brands (12
categories) and three prompt types (Insights, Ideas, Wild Ideas). Human
pairwise preferences from 678 practising creatives over 11,012 anonymised
comparisons, analysed with Bradley-Terry models, show tightly clustered
performance with no model dominating across brands or prompt types: the
top-bottom spread is $\Delta\theta \approx 0.45$, which implies a head-to-head
win probability of $0.61$; the highest-rated model beats the lowest only about
$61\%$ of the time. We also analyse model diversity using cosine distances to
capture intra- and inter-model variation and sensitivity to prompt reframing.
Comparing three LLM-as-judge setups with human rankings reveals weak,
inconsistent correlations and judge-specific biases, underscoring that
automated judges cannot substitute for human evaluation. Conventional
creativity tests also transfer only partially to brand-constrained tasks.
Overall, the results highlight the need for expert human evaluation and
diversity-aware workflows.

</details>


### [5] [CTCC: A Robust and Stealthy Fingerprinting Framework for Large Language Models via Cross-Turn Contextual Correlation Backdoor](https://arxiv.org/abs/2509.09703)
*Zhenhua Xu,Xixiang Zhao,Xubin Yue,Shengwei Tian,Changting Lin,Meng Han*

Main category: cs.CL

TL;DR: CTCC是一个新颖的基于规则的语言模型指纹框架，通过多轮对话的上下文相关性编码来实现所有权验证，解决了现有方法在隐蔽性、鲁棒性和泛化性方面的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的广泛部署，模型盗版和未经授权的重新分发问题日益严重，需要有效的知识产权保护方案。现有指纹方法存在可检测性、易受对抗攻击和泛化能力不足等问题。

Method: 提出CTCC框架，采用规则驱动的方法，在多轮对话中编码上下文相关性（如反事实关系），而不是依赖词元级或单轮触发器，支持黑盒访问下的指纹验证。

Result: 在多个LLM架构上的广泛实验表明，CTCC相比现有方法具有更强的隐蔽性和鲁棒性，能够有效减少误报和指纹泄露风险。

Conclusion: CTCC为实际LLM部署场景中的所有权验证提供了一个可靠且实用的解决方案，支持在部分触发器暴露情况下的持续构建。

Abstract: The widespread deployment of large language models (LLMs) has intensified
concerns around intellectual property (IP) protection, as model theft and
unauthorized redistribution become increasingly feasible. To address this,
model fingerprinting aims to embed verifiable ownership traces into LLMs.
However, existing methods face inherent trade-offs between stealthness,
robustness, and generalizability, being either detectable via distributional
shifts, vulnerable to adversarial modifications, or easily invalidated once the
fingerprint is revealed. In this work, we introduce CTCC, a novel rule-driven
fingerprinting framework that encodes contextual correlations across multiple
dialogue turns, such as counterfactual, rather than relying on token-level or
single-turn triggers. CTCC enables fingerprint verification under black-box
access while mitigating false positives and fingerprint leakage, supporting
continuous construction under a shared semantic rule even if partial triggers
are exposed. Extensive experiments across multiple LLM architectures
demonstrate that CTCC consistently achieves stronger stealth and robustness
than prior work. Our findings position CTCC as a reliable and practical
solution for ownership verification in real-world LLM deployment scenarios. Our
code and data are publicly available at <https://github.com/Xuzhenhua55/CTCC>.

</details>


### [6] [Temporal Preferences in Language Models for Long-Horizon Assistance](https://arxiv.org/abs/2509.09704)
*Ali Mazyaki,Mohammad Naghizadeh,Samaneh Ranjkhah Zonouzaghi,Hossein Setareh*

Main category: cs.CL

TL;DR: 研究语言模型在跨期选择中是否表现出未来导向偏好，以及这些偏好是否可被系统操纵。通过人类实验协议评估多个模型，提出可操纵性度量指标MTO，发现推理型模型在特定提示下会选择延迟选项，并讨论了AI助手的设计意义。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型在时间偏好决策中的行为特征，特别是它们是否像人类一样具有未来导向偏好，以及这些偏好是否可以通过提示词进行系统性操纵，这对于开发能够与人类长期目标对齐的AI助手具有重要意义。

Method: 采用改编的人类实验协议，在时间权衡任务上评估多个语言模型，并与人类决策者进行基准比较。引入可操纵性时间导向(MTO)指标，定义为模型在未来导向和现在导向提示下时间偏好的变化。

Result: 推理型模型(如DeepSeek-Reasoner和grok-3-mini)在未来导向提示下选择延迟选项，但在跨身份或地理位置的个性化决策方面表现有限。能够正确推理时间导向的模型会内化未来导向作为AI决策者的自我认知。

Conclusion: 研究结果对设计能够与异质化长期目标对齐的AI助手具有重要启示，提出了个性化情境校准和社会意识部署的研究议程，强调需要开发能够适应不同用户时间偏好的AI系统。

Abstract: We study whether language models (LMs) exhibit future- versus
present-oriented preferences in intertemporal choice and whether those
preferences can be systematically manipulated. Using adapted human experimental
protocols, we evaluate multiple LMs on time-tradeoff tasks and benchmark them
against a sample of human decision makers. We introduce an operational metric,
the Manipulability of Time Orientation (MTO), defined as the change in an LM's
revealed time preference between future- and present-oriented prompts. In our
tests, reasoning-focused models (e.g., DeepSeek-Reasoner and grok-3-mini)
choose later options under future-oriented prompts but only partially
personalize decisions across identities or geographies. Moreover, models that
correctly reason about time orientation internalize a future orientation for
themselves as AI decision makers. We discuss design implications for AI
assistants that should align with heterogeneous, long-horizon goals and outline
a research agenda on personalized contextual calibration and socially aware
deployment.

</details>


### [7] [The Non-Determinism of Small LLMs: Evidence of Low Answer Consistency in Repetition Trials of Standard Multiple-Choice Benchmarks](https://arxiv.org/abs/2509.09705)
*Claudio Pinhanez,Paulo Cavalin,Cassia Sanctos,Marcelo Grave,Yago Primerano*

Main category: cs.CL

TL;DR: 这篇论文研究了小型LLM模型(2B-8B参数)在回答同一问题时的一致性问题，包括不同推理温度、模型大小、微调模型等因素的影响。结果显示小型模型在低温度下可以一致回答70%-80%的问题，中型模型一致性更高。


<details>
  <summary>Details</summary>
Motivation: 探索小型LLM模型在回答问题时的一致性问题，以及一致性与准确性之间的权衡关系，为选择既准确又可靠的模型提供指导。

Method: 使用开源LLM模型在MMLU-Redux和MedQA多选题库上重复回答10次同一问题，考察不同推理温度、模型规模(2B-8B小型和50B-80B中型)、是否微调等因素，并提出新的分析和图形工具。

Result: 小型模型在低推理温度下可以一致回答70%-80%的问题，一致回答的准确性与总体准确性呈良好相关性，中型模型显示出更高的答案一致性。

Conclusion: 小型LLM模型在一致性方面有显著差异，但在低温度下能实现较高的一致性，且一致性与准确性存在正相关，中型模型在一致性方面表现更优。

Abstract: This work explores the consistency of small LLMs (2B-8B parameters) in
answering multiple times the same question. We present a study on known,
open-source LLMs responding to 10 repetitions of questions from the
multiple-choice benchmarks MMLU-Redux and MedQA, considering different
inference temperatures, small vs. medium models (50B-80B), finetuned vs. base
models, and other parameters. We also look into the effects of requiring
multi-trial answer consistency on accuracy and the trade-offs involved in
deciding which model best provides both of them. To support those studies, we
propose some new analytical and graphical tools. Results show that the number
of questions which can be answered consistently vary considerably among models
but are typically in the 50%-80% range for small models at low inference
temperatures. Also, accuracy among consistent answers seems to reasonably
correlate with overall accuracy. Results for medium-sized models seem to
indicate much higher levels of answer consistency.

</details>


### [8] [Beyond I'm Sorry, I Can't: Dissecting Large Language Model Refusal](https://arxiv.org/abs/2509.09708)
*Nirmalendu Prakash,Yeo Wei Jie,Amir Abdullah,Ranjan Satapathy,Erik Cambria,Roy Ka Wei Lee*

Main category: cs.CL

TL;DR: 通过稀疏自编码器分析LLM拒绝有害提示的内部机制，发现关键特征集并实现越狱，揭示安全行为的冗余特征机制


<details>
  <summary>Details</summary>
Motivation: 理解指令调优大语言模型中拒绝有害提示这一安全行为的内部因果机制，目前对此了解不足

Method: 使用稀疏自编码器分析残差流激活，通过三阶段搜索流程：拒绝方向识别、贪婪过滤和交互发现，寻找能翻转模型从拒绝到遵从的关键特征集

Result: 成功识别出导致越狱的关键特征集，发现存在冗余特征机制，当早期特征被抑制时冗余特征会被激活

Conclusion: 通过操作可解释的潜在空间，为安全行为的细粒度审计和针对性干预提供了潜力

Abstract: Refusal on harmful prompts is a key safety behaviour in instruction-tuned
large language models (LLMs), yet the internal causes of this behaviour remain
poorly understood. We study two public instruction-tuned models, Gemma-2-2B-IT
and LLaMA-3.1-8B-IT, using sparse autoencoders (SAEs) trained on
residual-stream activations. Given a harmful prompt, we search the SAE latent
space for feature sets whose ablation flips the model from refusal to
compliance, demonstrating causal influence and creating a jailbreak. Our search
proceeds in three stages: (1) Refusal Direction: find a refusal-mediating
direction and collect SAE features near that direction; (2) Greedy Filtering:
prune to a minimal set; and (3) Interaction Discovery: fit a factorization
machine (FM) that captures nonlinear interactions among the remaining active
features and the minimal set. This pipeline yields a broad set of
jailbreak-critical features, offering insight into the mechanistic basis of
refusal. Moreover, we find evidence of redundant features that remain dormant
unless earlier features are suppressed. Our findings highlight the potential
for fine-grained auditing and targeted intervention in safety behaviours by
manipulating the interpretable latent space.

</details>


### [9] [Assisting Research Proposal Writing with Large Language Models: Evaluation and Refinement](https://arxiv.org/abs/2509.09709)
*Jing Ren,Weiqi Wang*

Main category: cs.CL

TL;DR: 提出两个量化评估指标（内容质量和参考文献有效性）和迭代提示方法，以解决ChatGPT学术写作中的评估主观性和伪造参考文献问题


<details>
  <summary>Details</summary>
Motivation: 大语言模型在学术写作中常出现错误或伪造的参考文献，而现有内容质量评估依赖主观人工判断，缺乏客观性和一致性

Method: 设计了内容质量和参考文献有效性两个评估指标，并基于这些指标的得分开发了迭代提示方法

Result: 实验结果显示提出的指标提供了客观、量化的评估框架，迭代提示显著提高了内容质量并减少了参考文献的不准确和伪造问题

Conclusion: 该方法有效解决了大语言模型在学术写作中的关键伦理挑战，为ChatGPT的写作性能评估提供了客观的量化标准

Abstract: Large language models (LLMs) like ChatGPT are increasingly used in academic
writing, yet issues such as incorrect or fabricated references raise ethical
concerns. Moreover, current content quality evaluations often rely on
subjective human judgment, which is labor-intensive and lacks objectivity,
potentially compromising the consistency and reliability. In this study, to
provide a quantitative evaluation and enhance research proposal writing
capabilities of LLMs, we propose two key evaluation metrics--content quality
and reference validity--and an iterative prompting method based on the scores
derived from these two metrics. Our extensive experiments show that the
proposed metrics provide an objective, quantitative framework for assessing
ChatGPT's writing performance. Additionally, iterative prompting significantly
enhances content quality while reducing reference inaccuracies and
fabrications, addressing critical ethical challenges in academic contexts.

</details>


### [10] [Generating Individual Travel Diaries Using Large Language Models Informed by Census and Land-Use Data](https://arxiv.org/abs/2509.09710)
*Sepehr Golrokh Amin,Devin Rhoads,Fatemeh Fakhrmoosavi,Nicholas E. Lownes,John N. Ivan*

Main category: cs.CL

TL;DR: 本文提出了一种使用大型语言模型生成个体出行日记的新方法，通过开源数据生成虚拟人物并合成出行记录，在真实性评分上与经典方法相当，且在出行目的确定方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖大量专有家庭出行调查数据，成本高且获取困难。本研究旨在利用开源数据和LLM技术，零样本生成真实的个体出行日记，为交通建模提供更便捷的解决方案。

Method: 使用美国社区调查和智能位置数据库的开源数据随机生成虚拟人物，通过直接提示LLM合成出行日记。采用包含四个指标（出行次数、间隔、目的、方式）的综合真实性评分体系，并使用Jensen-Shannon散度验证生成日记与真实日记的分布相似性。

Result: LLM生成的日记在整体真实性评分上与经典方法相当（0.485 vs 0.455），在确定出行目的方面表现更优，且具有更好的一致性（评分分布更窄）。经典方法在出行次数和活动时长估计方面略优。聚合验证显示LLM具有更好的统计代表性（0.612 vs 0.435）。

Conclusion: LLM方法在零样本条件下可行，为合成出行日记评估建立了可量化的真实性度量标准，展示了LLM在交通建模中的潜力，特别是在出行目的识别方面具有优势。

Abstract: This study introduces a Large Language Model (LLM) scheme for generating
individual travel diaries in agent-based transportation models. While
traditional approaches rely on large quantities of proprietary household travel
surveys, the method presented in this study generates personas stochastically
from open-source American Community Survey (ACS) and Smart Location Database
(SLD) data, then synthesizes diaries through direct prompting. This study
features a novel one-to-cohort realism score: a composite of four metrics (Trip
Count Score, Interval Score, Purpose Score, and Mode Score) validated against
the Connecticut Statewide Transportation Study (CSTS) diaries, matched across
demographic variables. The validation utilizes Jensen-Shannon Divergence to
measure distributional similarities between generated and real diaries. When
compared to diaries generated with classical methods (Negative Binomial for
trip generation; Multinomial Logit for mode/purpose) calibrated on the
validation set, LLM-generated diaries achieve comparable overall realism (LLM
mean: 0.485 vs. 0.455). The LLM excels in determining trip purpose and
demonstrates greater consistency (narrower realism score distribution), while
classical models lead in numerical estimates of trip count and activity
duration. Aggregate validation confirms the LLM's statistical
representativeness (LLM mean: 0.612 vs. 0.435), demonstrating LLM's zero-shot
viability and establishing a quantifiable metric of diary realism for future
synthetic diary evaluation systems.

</details>


### [11] [Psychiatry-Bench: A Multi-Task Benchmark for LLMs in Psychiatry](https://arxiv.org/abs/2509.09711)
*Aya E. Fouda,Abdelrahamn A. Hassan,Radwa J. Hanafy,Mohammed E. Fouda*

Main category: cs.CL

TL;DR: PsychiatryBench是一个基于权威精神病学教科书和案例集构建的临床评估基准，包含11个问答任务和5300多个专家标注项目，用于评估LLMs在精神病学应用中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有评估资源主要依赖小型临床访谈语料库、社交媒体帖子或合成对话，临床有效性有限，无法捕捉精神病学推理的复杂性。

Method: 基于专家验证的精神病学教科书和案例集构建基准，包含诊断推理、治疗计划等11个任务，使用传统指标和LLM-as-judge相似性评分框架评估多种前沿LLM。

Result: 结果显示在临床一致性和安全性方面存在显著差距，特别是在多轮随访和管理任务中，需要专门的模型调优和更强大的评估范式。

Conclusion: PsychiatryBench为高风险心理健康应用中的LLM性能基准测试和改进提供了一个模块化、可扩展的平台。

Abstract: Large language models (LLMs) hold great promise in enhancing psychiatric
practice, from improving diagnostic accuracy to streamlining clinical
documentation and therapeutic support. However, existing evaluation resources
heavily rely on small clinical interview corpora, social media posts, or
synthetic dialogues, which limits their clinical validity and fails to capture
the full complexity of psychiatric reasoning. In this work, we introduce
PsychiatryBench, a rigorously curated benchmark grounded exclusively in
authoritative, expert-validated psychiatric textbooks and casebooks.
PsychiatryBench comprises eleven distinct question-answering tasks ranging from
diagnostic reasoning and treatment planning to longitudinal follow-up,
management planning, clinical approach, sequential case analysis, and
multiple-choice/extended matching formats totaling over 5,300 expert-annotated
items. We evaluate a diverse set of frontier LLMs (including Google Gemini,
DeepSeek, LLaMA 3, and QWQ-32) alongside leading open-source medical models
(e.g., OpenBiloLLM, MedGemma) using both conventional metrics and an
"LLM-as-judge" similarity scoring framework. Our results reveal substantial
gaps in clinical consistency and safety, particularly in multi-turn follow-up
and management tasks, underscoring the need for specialized model tuning and
more robust evaluation paradigms. PsychiatryBench offers a modular, extensible
platform for benchmarking and improving LLM performance in high-stakes mental
health applications.

</details>


### [12] [The Thinking Therapist: Training Large Language Models to Deliver Acceptance and Commitment Therapy using Supervised Fine-Tuning and Odds Ratio Policy Optimization](https://arxiv.org/abs/2509.09712)
*Talha Tahir*

Main category: cs.CL

TL;DR: 这篇论文研究了两种训练方法(SFT和ORPO)和显式思维链(COT)对小型语言模型执行接纳与承诺疗法(ACT)的影响。ORPO训练的模型在疗法保真度和沟通兼容性方面显著优于SFT和基础模型，COT仅对SFT模型有显著改善作用。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过合适的训练方法和显式思维提升小型语言模型在接纳与承诺疗法中的性能，以支持精神健康领域的应用。

Method: 使用50组合成ACT话语训练Llama-3.2-3b-Instruct模型，比较SFT和ORPO两种训练方法，每种方法分别包含和不包含COT思维步骤。通过模拟疗法会话评估模型在ACT保真度量表(ACT-FM)和沟通兼容性量表(TES)上的表现。

Result: ORPO训练模型在ACT保真度(χ²(5)=185.15, p<.001)和疗法沟通兼容性(χ²(5)=140.37, p<.001)方面显著优于SFT和基础模型。COT仅对SFT模型有显著改善作用，提高ACT-FM分数2.68分(p<.001)，对ORPO模型无显著影响。

Conclusion: 偏好对齐的策略优化(ORPO)能有效培养小型语言模型的ACT能力，而显式思维的作用依赖于基础训练方法。ORPO的优势在于学习疗法"过程"而非简单模仿"内容"。

Abstract: Acceptance and Commitment Therapy (ACT) is a third-wave cognitive behavioral
therapy with emerging evidence of efficacy in several psychiatric conditions.
This study investigates the impact of post-training methodology and explicit
reasoning on the ability of a small open-weight large language model (LLM) to
deliver ACT. Using 50 sets of synthetic ACT transcripts generated by
Mistral-Large, we trained Llama-3.2-3b-Instruct with two distinct approaches,
supervised fine-tuning (SFT) and odds ratio policy optimization (ORPO), each
with and without an explicit chain-of-thought (COT) reasoning step. Performance
was evaluated by comparing these four post-trained variants against the base
Instruct model. These models were benchmarked in simulated therapy sessions,
with performance quantitatively assessed on the ACT Fidelity Measure (ACT-FM)
and the Therapist Empathy Scale (TES) by an LLM judge that had been fine-tuned
on human evaluations. Our findings demonstrate that the ORPO-trained models
significantly outperformed both their SFT and Instruct counterparts on ACT
fidelity ($\chi^2(5) = 185.15, p < .001$) and therapeutic empathy ($\chi^2(5) =
140.37, p < .001$). The effect of COT was conditional as it provided a
significant benefit to SFT models, improving ACT-FM scores by an average of
2.68 points ($p < .001$), while offering no discernible advantage to the
superior ORPO or instruct-tuned variants. We posit that the superiority of ORPO
stems from its ability to learn the therapeutic `process' over imitating
`content,' a key aspect of ACT, while COT acts as a necessary scaffold for
models trained only via imitation. This study establishes that
preference-aligned policy optimization can effectively instill ACT competencies
in small LLMs, and that the utility of explicit reasoning is highly dependent
on the underlying training paradigm.

</details>


### [13] [HANRAG: Heuristic Accurate Noise-resistant Retrieval-Augmented Generation for Multi-hop Question Answering](https://arxiv.org/abs/2509.09713)
*Duolin Sun,Dan Yang,Yue Shen,Yihan Jiao,Zhehao Tan,Jie Feng,Lianzhen Zhong,Jian Wang,Peng Wei,Jinjie Gu*

Main category: cs.CL

TL;DR: HANRAG是一个基于启发式的RAG框架，通过查询路由、子查询分解和噪声过滤，有效解决多跳查询中的迭代检索浪费和噪声积累问题。


<details>
  <summary>Details</summary>
Motivation: 当前RAG方法在处理多跳查询时存在迭代检索步骤浪费和噪声积累问题，需要更高效的框架来提升复杂查询处理能力。

Method: 提出HANRAG框架，使用强大的揭示器进行查询路由、查询分解为子查询，并对检索文档进行噪声过滤。

Result: 在多个基准测试中，HANRAG在单跳和多跳问答任务上都表现出优于其他行业领先方法的性能。

Conclusion: HANRAG框架通过有效的查询处理和噪声管理，显著提升了RAG系统对多样化查询的适应性和噪声抵抗能力。

Abstract: The Retrieval-Augmented Generation (RAG) approach enhances question-answering
systems and dialogue generation tasks by integrating information retrieval (IR)
technologies with large language models (LLMs). This strategy, which retrieves
information from external knowledge bases to bolster the response capabilities
of generative models, has achieved certain successes. However, current RAG
methods still face numerous challenges when dealing with multi-hop queries. For
instance, some approaches overly rely on iterative retrieval, wasting too many
retrieval steps on compound queries. Additionally, using the original complex
query for retrieval may fail to capture content relevant to specific
sub-queries, resulting in noisy retrieved content. If the noise is not managed,
it can lead to the problem of noise accumulation. To address these issues, we
introduce HANRAG, a novel heuristic-based framework designed to efficiently
tackle problems of varying complexity. Driven by a powerful revelator, HANRAG
routes queries, decomposes them into sub-queries, and filters noise from
retrieved documents. This enhances the system's adaptability and noise
resistance, making it highly capable of handling diverse queries. We compare
the proposed framework against other leading industry methods across various
benchmarks. The results demonstrate that our framework obtains superior
performance in both single-hop and multi-hop question-answering tasks.

</details>


### [14] [How Small Transformation Expose the Weakness of Semantic Similarity Measures](https://arxiv.org/abs/2509.09714)
*Serge Lionel Nikiema,Albérick Euraste Djire,Abdoul Aziz Bonkoungou,Micheline Bénédicte Moumoula,Jordan Samhi,Abdoul Kader Kabore,Jacques Klein,Tegawendé F. Bissyande*

Main category: cs.CL

TL;DR: 本研究评估了18种语义相似度测量方法在软件工程应用中的表现，发现常用指标存在显著问题，某些方法甚至将语义对立的内容错误识别为高度相似。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在代码搜索、API推荐等软件工程任务中广泛用于语义相似度评估，需要验证这些方法是否真正理解语义关系还是仅识别表面模式。

Method: 建立了系统测试框架，对文本和代码施加受控变化，测试了基于词汇、嵌入、LLM和结构感知的18种不同相似度测量方法。

Result: 嵌入方法错误识别语义对立内容为相似的概率高达99.9%，转换器方法有时认为对立含义比同义词更相似。从欧氏距离切换到余弦相似度使结果改善24-66%。LLM方法在区分语义差异方面表现更好。

Conclusion: 当前常用的语义相似度测量方法存在严重缺陷，需要更可靠的评估指标来确保软件工程应用的准确性。

Abstract: This research examines how well different methods measure semantic
similarity, which is important for various software engineering applications
such as code search, API recommendations, automated code reviews, and
refactoring tools. While large language models are increasingly used for these
similarity assessments, questions remain about whether they truly understand
semantic relationships or merely recognize surface patterns.
  The study tested 18 different similarity measurement approaches, including
word-based methods, embedding techniques, LLM-based systems, and
structure-aware algorithms. The researchers created a systematic testing
framework that applies controlled changes to text and code to evaluate how well
each method handles different types of semantic relationships.
  The results revealed significant issues with commonly used metrics. Some
embedding-based methods incorrectly identified semantic opposites as similar up
to 99.9 percent of the time, while certain transformer-based approaches
occasionally rated opposite meanings as more similar than synonymous ones. The
study found that embedding methods' poor performance often stemmed from how
they calculate distances; switching from Euclidean distance to cosine
similarity improved results by 24 to 66 percent. LLM-based approaches performed
better at distinguishing semantic differences, producing low similarity scores
(0.00 to 0.29) for genuinely different meanings, compared to embedding methods
that incorrectly assigned high scores (0.82 to 0.99) to dissimilar content.

</details>


### [15] [Investigating Symbolic Triggers of Hallucination in Gemma Models Across HaluEval and TruthfulQA](https://arxiv.org/abs/2509.09715)
*Naveen Lamba,Sanju Tiwari,Manas Gaur*

Main category: cs.CL

TL;DR: 该研究识别并表征了导致大语言模型产生幻觉的关键符号属性，发现即使模型规模增大，符号元素（如修饰符和命名实体）仍然是导致幻觉的主要因素。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM的幻觉问题已被广泛研究，但导致模型内在易产生幻觉的属性尚未被识别和研究。本研究旨在识别和表征这些关键属性，以定位模型内部机制的漏洞。

Method: 使用HaluEval和TruthfulQA两个数据集，将原有的问答格式转换为多种其他格式，以确定符号属性是导致幻觉的原因。测试了Gemma-2系列不同规模的模型（2B、9B、27B）。

Result: Gemma-2-2B的平均幻觉率为79.0%，随着模型规模增大，幻觉率降至73.6%（9B）和63.9%（27B）。但修饰符（84.76%-94.98%）和命名实体（83.87%-93.96%）在所有模型和数据集上都保持很高的幻觉率。

Conclusion: 符号元素持续混淆模型，表明LLM在处理此类输入时存在根本性弱点，且这种弱点不受模型规模影响，指向了模型架构的基本缺陷。

Abstract: Hallucination in Large Language Models (LLMs) is a well studied problem.
However, the properties that make LLM intrinsically vulnerable to
hallucinations have not been identified and studied. This research identifies
and characterizes the key properties, allowing us to pinpoint vulnerabilities
within the model's internal mechanisms. To solidify on these properties, we
utilized two established datasets, HaluEval and TruthfulQA and convert their
existing format of question answering into various other formats to narrow down
these properties as the reason for the hallucinations. Our findings reveal that
hallucination percentages across symbolic properties are notably high for
Gemma-2-2B, averaging 79.0% across tasks and datasets. With increased model
scale, hallucination drops to 73.6% for Gemma-2-9B and 63.9% for Gemma-2-27B,
reflecting a 15 percentage point reduction overall. Although the hallucination
rate decreases as the model size increases, a substantial amount of
hallucination caused by symbolic properties still persists. This is especially
evident for modifiers (ranging from 84.76% to 94.98%) and named entities
(ranging from 83.87% to 93.96%) across all Gemma models and both datasets.
These findings indicate that symbolic elements continue to confuse the models,
pointing to a fundamental weakness in how these LLMs process such
inputs--regardless of their scale.

</details>


### [16] [ALIGNS: Unlocking nomological networks in psychological measurement through a large language model](https://arxiv.org/abs/2509.09723)
*Kai R. Larsen,Sen Yan,Roland Müller,Lan Sang,Mikko Rönkkö,Ravi Starzl,Donald Edmondson*

Main category: cs.CL

TL;DR: ALIGNS是一个基于大语言模型的系统，用于生成包含55万+指标的综合nomological网络，解决心理测量中构建理论网络的挑战。


<details>
  <summary>Details</summary>
Motivation: 心理测量对许多学科至关重要，但构建nomological网络（概念与测量关系的理论图谱）仍然是70年来的挑战，这导致临床试验可能无法检测治疗效果，公共政策可能针对错误的结果。

Method: 开发基于大语言模型的ALIGNS系统，使用经过验证的问卷测量进行训练，提供三个综合nomological网络，涵盖心理学、医学、社会政策等领域。

Result: 1) NIH PROMIS焦虑和抑郁工具收敛为单一情绪困扰维度；2) 儿童气质测量识别出当前框架未捕捉的四个潜在维度，并质疑一个现有维度；3) 心理测量学专家评估系统的重要性、可访问性和适用性。

Conclusion: ALIGNS是首个应用大语言模型解决测量验证基础问题的系统，免费提供于nomologicalnetwork.org，通过大规模nomological分析补充传统验证方法。

Abstract: Psychological measurement is critical to many disciplines. Despite advances
in measurement, building nomological networks, theoretical maps of how concepts
and measures relate to establish validity, remains a challenge 70 years after
Cronbach and Meehl proposed them as fundamental to validation. This limitation
has practical consequences: clinical trials may fail to detect treatment
effects, and public policy may target the wrong outcomes. We introduce Analysis
of Latent Indicators to Generate Nomological Structures (ALIGNS), a large
language model-based system trained with validated questionnaire measures.
ALIGNS provides three comprehensive nomological networks containing over
550,000 indicators across psychology, medicine, social policy, and other
fields. This represents the first application of large language models to solve
a foundational problem in measurement validation. We report classification
accuracy tests used to develop the model, as well as three evaluations. In the
first evaluation, the widely used NIH PROMIS anxiety and depression instruments
are shown to converge into a single dimension of emotional distress. The second
evaluation examines child temperament measures and identifies four potential
dimensions not captured by current frameworks, and questions one existing
dimension. The third evaluation, an applicability check, engages expert
psychometricians who assess the system's importance, accessibility, and
suitability. ALIGNS is freely available at nomologicalnetwork.org,
complementing traditional validation methods with large-scale nomological
analysis.

</details>


### [17] [DiTTO-LLM: Framework for Discovering Topic-based Technology Opportunities via Large Language Model](https://arxiv.org/abs/2509.09724)
*Wonyoung Kim,Sujeong Seo,Juhyun Lee*

Main category: cs.CL

TL;DR: 基于专利数据的时间关系分析框架，利用大语言模型提取技术主题和关系，识别新兴技术机会


<details>
  <summary>Details</summary>
Motivation: 技术机会是推动技术进步、产业发展和创新的关键信息，需要一种系统化的方法来发现和预测新兴技术趋势

Method: 从专利数据集提取文本，映射文本主题发现技术间关系，通过跟踪主题随时间变化识别技术机会，利用大语言模型提取主题和聊天模型提示支持机会发现

Result: 使用美国专利局人工智能专利数据进行评估，实验结果显示人工智能技术正向日常化可访问性方向发展

Conclusion: 该框架能够有效识别未来技术机会，为技术预测和创新发展提供了有力的分析工具

Abstract: Technology opportunities are critical information that serve as a foundation
for advancements in technology, industry, and innovation. This paper proposes a
framework based on the temporal relationships between technologies to identify
emerging technology opportunities. The proposed framework begins by extracting
text from a patent dataset, followed by mapping text-based topics to discover
inter-technology relationships. Technology opportunities are then identified by
tracking changes in these topics over time. To enhance efficiency, the
framework leverages a large language model to extract topics and employs a
prompt for a chat-based language model to support the discovery of technology
opportunities. The framework was evaluated using an artificial intelligence
patent dataset provided by the United States Patent and Trademark Office. The
experimental results suggest that artificial intelligence technology is
evolving into forms that facilitate everyday accessibility. This approach
demonstrates the potential of the proposed framework to identify future
technology opportunities.

</details>


### [18] [BIBERT-Pipe on Biomedical Nested Named Entity Linking at BioASQ 2025](https://arxiv.org/abs/2509.09725)
*Chunyu Li,Xindi Zheng,Siqi Liu*

Main category: cs.CL

TL;DR: 提出了一个轻量级的两阶段检索-排序管道系统BIBERT-Pipe，用于处理多语言生物医学嵌套命名实体链接任务，在BioNNE 2025排行榜中排名第三


<details>
  <summary>Details</summary>
Motivation: 现有的生物医学实体链接基准主要针对英语和平坦提及，缺乏对嵌套和多语言提及的现实场景研究

Method: 采用两阶段检索-排序架构：检索阶段使用原始预训练模型，排序阶段进行领域特定微调；使用可学习的[Ms]/[Me]标签包装提及；通过三种数据源自动扩展排序训练语料

Result: 在BioNNE 2025多语言赛道中排名第三，证明了这些最小但原则性修改的有效性和竞争力

Conclusion: 该方法通过保持原始EL模型不变，仅修改三个任务对齐组件，成功解决了多语言生物医学嵌套实体链接的挑战

Abstract: Entity linking (EL) for biomedical text is typically benchmarked on
English-only corpora with flat mentions, leaving the more realistic scenario of
nested and multilingual mentions largely unexplored. We present our system for
the BioNNE 2025 Multilingual Biomedical Nested Named Entity Linking shared task
(English & Russian), closing this gap with a lightweight pipeline that keeps
the original EL model intact and modifies only three task-aligned components:
Two-stage retrieval-ranking. We leverage the same base encoder model in both
stages: the retrieval stage uses the original pre-trained model, while the
ranking stage applies domain-specific fine-tuning. Boundary cues. In the
ranking stage, we wrap each mention with learnable [Ms] / [Me] tags, providing
the encoder with an explicit, language-agnostic span before robustness to
overlap and nesting. Dataset augmentation. We also automatically expand the
ranking training corpus with three complementary data sources, enhancing
coverage without extra manual annotation. On the BioNNE 2025 leaderboard, our
two stage system, bilingual bert (BIBERT-Pipe), ranks third in the multilingual
track, demonstrating the effectiveness and competitiveness of these minimal yet
principled modifications. Code are publicly available at
https://github.com/Kaggle-Competitions-Code/BioNNE-L.

</details>


### [19] [Natural Language Translation of Formal Proofs through Informalization of Proof Steps and Recursive Summarization along Proof Structure](https://arxiv.org/abs/2509.09726)
*Seiji Hattori,Takuya Matsuzaki,Makoto Fujiwara*

Main category: cs.CL

TL;DR: 利用LLM的非形式化能力将机器可验证的形式化证明翻译为自然语言证明的方法


<details>
  <summary>Details</summary>
Motivation: 为了解决形式化证明难以阅读和理解的问题，利用大型语言模型的能力将机器可验证的形式化证明转换为更易读的自然语言描述

Method: 利用LLM的非形式化（形式语言证明步骤的言语化）和摘要能力，将形式化证明数据转换为自然语言证明，并在本科教科书中的自然语言证明数据上进行评估

Result: 该方法能够生成高质量的自然语言证明，在Lean证明助手的现有形式化证明库上应用时，可以输出高度可读且准确的自然语言证明

Conclusion: 提出的方法有效地将形式化证明转换为自然语言，提高了证明的可读性和理解性，展示了LLM在数学证明自然语言生成方面的潜力

Abstract: This paper proposes a natural language translation method for
machine-verifiable formal proofs that leverages the informalization
(verbalization of formal language proof steps) and summarization capabilities
of LLMs. For evaluation, it was applied to formal proof data created in
accordance with natural language proofs taken from an undergraduate-level
textbook, and the quality of the generated natural language proofs was analyzed
in comparison with the original natural language proofs. Furthermore, we will
demonstrate that this method can output highly readable and accurate natural
language proofs by applying it to existing formal proof library of the Lean
proof assistant.

</details>


### [20] [A Role-Aware Multi-Agent Framework for Financial Education Question Answering with LLMs](https://arxiv.org/abs/2509.09727)
*Andy Zhu,Yingjun Du*

Main category: cs.CL

TL;DR: 提出了一个多智能体框架，通过角色提示和检索增强生成来提升金融问答的准确性，相比零-shot思维链基线提高了6.6-8.3%的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在金融教育问答中难以捕捉专业推理需求，金融领域需要多步定量推理、领域术语理解和现实场景理解。

Method: 使用多智能体框架，包括基础生成器、证据检索器和专家评审器，结合检索增强生成从6本金融教科书获取上下文证据。

Result: 基于批判的优化方法比零-shot思维链基线提高6.6-8.3%的准确率，Gemini-2.0-Flash表现最佳，GPT-4o-mini能达到与金融调优模型相当的性能。

Conclusion: 该方法为增强金融问答提供了一种经济有效的途径，并为多智能体金融LLM系统的进一步研究提供了见解。

Abstract: Question answering (QA) plays a central role in financial education, yet
existing large language model (LLM) approaches often fail to capture the
nuanced and specialized reasoning required for financial problem-solving. The
financial domain demands multistep quantitative reasoning, familiarity with
domain-specific terminology, and comprehension of real-world scenarios. We
present a multi-agent framework that leverages role-based prompting to enhance
performance on domain-specific QA. Our framework comprises a Base Generator, an
Evidence Retriever, and an Expert Reviewer agent that work in a single-pass
iteration to produce a refined answer. We evaluated our framework on a set of
3,532 expert-designed finance education questions from Study.com, an online
learning platform. We leverage retrieval-augmented generation (RAG) for
contextual evidence from 6 finance textbooks and prompting strategies for a
domain-expert reviewer. Our experiments indicate that critique-based refinement
improves answer accuracy by 6.6-8.3% over zero-shot Chain-of-Thought baselines,
with the highest performance from Gemini-2.0-Flash. Furthermore, our method
enables GPT-4o-mini to achieve performance comparable to the finance-tuned
FinGPT-mt_Llama3-8B_LoRA. Our results show a cost-effective approach to
enhancing financial QA and offer insights for further research in multi-agent
financial LLM systems.

</details>


### [21] [A meta-analysis on the performance of machine-learning based language models for sentiment analysis](https://arxiv.org/abs/2509.09728)
*Elena Rohde,Jonas Klingwort,Christian Borgs*

Main category: cs.CL

TL;DR: 对Twitter情感分析中机器学习性能的元分析显示平均准确率为0.80，发现总体准确率指标存在误导性，需要标准化报告规范


<details>
  <summary>Details</summary>
Motivation: 评估Twitter情感分析中机器学习的平均性能表现，分析研究间的异质性，并探讨研究特征如何影响模型性能

Method: 采用PRISMA指南进行文献检索，从20项研究中选取195个试验，使用双反正弦变换和三层次随机效应模型分析总体准确率

Result: AIC优化模型的平均总体准确率为0.80 [0.76, 0.84]，发现总体准确率因类别不平衡和情感类别数量而具有误导性

Conclusion: 需要规范模型性能报告标准，包括报告独立测试集的混淆矩阵，以实现跨研究的可靠比较

Abstract: This paper presents a meta-analysis evaluating ML performance in sentiment
analysis for Twitter data. The study aims to estimate the average performance,
assess heterogeneity between and within studies, and analyze how study
characteristics influence model performance. Using PRISMA guidelines, we
searched academic databases and selected 195 trials from 20 studies with 12
study features. Overall accuracy, the most reported performance metric, was
analyzed using double arcsine transformation and a three-level random effects
model. The average overall accuracy of the AIC-optimized model was 0.80 [0.76,
0.84]. This paper provides two key insights: 1) Overall accuracy is widely used
but often misleading due to its sensitivity to class imbalance and the number
of sentiment classes, highlighting the need for normalization. 2) Standardized
reporting of model performance, including reporting confusion matrices for
independent test sets, is essential for reliable comparisons of ML classifiers
across studies, which seems far from common practice.

</details>


### [22] [MultimodalHugs: Enabling Sign Language Processing in Hugging Face](https://arxiv.org/abs/2509.09729)
*Gerard Sant,Zifan Jiang,Carlos Escolano,Amit Moryossef,Mathias Müller,Rico Sennrich,Sarah Ebling*

Main category: cs.CL

TL;DR: MultimodalHugs是一个基于Hugging Face构建的多模态框架，专门为解决手语处理研究中的可复现性和灵活性不足问题而设计，支持姿态估计数据和像素数据等多种模态。


<details>
  <summary>Details</summary>
Motivation: 手语处理研究面临复杂的临时代码、低可复现性和不公平比较的问题，现有工具如Hugging Face无法灵活集成手语实验，这通过研究者调查得到证实。

Method: 在Hugging Face基础上构建MultimodalHugs框架，增加抽象层以支持更多样化的数据模态和任务，同时继承Hugging Face生态系统的优势。

Result: 框架能够容纳多种模态数据，包括手语姿态估计数据和文本字符像素数据，通过定量实验验证了其适用性。

Conclusion: MultimodalHugs不仅专注于手语处理，其抽象层设计使其更广泛适用于其他不符合Hugging Face标准模板的用例，提升了多模态研究的灵活性和可复现性。

Abstract: In recent years, sign language processing (SLP) has gained importance in the
general field of Natural Language Processing. However, compared to research on
spoken languages, SLP research is hindered by complex ad-hoc code,
inadvertently leading to low reproducibility and unfair comparisons. Existing
tools that are built for fast and reproducible experimentation, such as Hugging
Face, are not flexible enough to seamlessly integrate sign language
experiments. This view is confirmed by a survey we conducted among SLP
researchers.
  To address these challenges, we introduce MultimodalHugs, a framework built
on top of Hugging Face that enables more diverse data modalities and tasks,
while inheriting the well-known advantages of the Hugging Face ecosystem. Even
though sign languages are our primary focus, MultimodalHugs adds a layer of
abstraction that makes it more widely applicable to other use cases that do not
fit one of the standard templates of Hugging Face. We provide quantitative
experiments to illustrate how MultimodalHugs can accommodate diverse modalities
such as pose estimation data for sign languages, or pixel data for text
characters.

</details>


### [23] [Benchmarking Vision-Language Models on Chinese Ancient Documents: From OCR to Knowledge Reasoning](https://arxiv.org/abs/2509.09731)
*Haiyang Yu,Yuchuan Wu,Fan Shi,Lei Liao,Jinghui Lu,Xiaodong Ge,Han Wang,Minghan Zhuo,Xuecheng Wu,Xiang Fei,Hao Feng,Guozhi Tang,An-Lan Wang,Hanshen Zhu,Yangfan He,Quanhuan Liang,Liyuan Meng,Chao Feng,Can Huang,Jingqun Tang,Bin Li*

Main category: cs.CL

TL;DR: 中文古终文献数字化评测标准稿AncientDoc，包含5项任务，测试视觉-语言模型在古终文献的识别、理解和推理能力


<details>
  <summary>Details</summary>
Motivation: 中国古终文献作为价值连城的文化载体，在数字化和理解方面面临挑战，现有方法和模型对其复杂的视觉和语言特征处理能力不足，缺乏专门的评测标准

Method: 构建AncientDoc标准稿，包含页面级OCR、白话翻译、推理QA、知识QA、语言变体QA五项任务，涵盖14种文档类型、100多本书籍、约3000页

Result: 基于AncientDoc对主流视觉-语言模型进行评测，使用多种指标和人类对齐的大语言模型进行打分

Conclusion: AncientDoc是首个中文古终文献评测标准，有助于推动古终文献的数字化保护和智能化理解，为VLM模型在这一领域的发展提供了重要的评测基准

Abstract: Chinese ancient documents, invaluable carriers of millennia of Chinese
history and culture, hold rich knowledge across diverse fields but face
challenges in digitization and understanding, i.e., traditional methods only
scan images, while current Vision-Language Models (VLMs) struggle with their
visual and linguistic complexity. Existing document benchmarks focus on English
printed texts or simplified Chinese, leaving a gap for evaluating VLMs on
ancient Chinese documents. To address this, we present AncientDoc, the first
benchmark for Chinese ancient documents, designed to assess VLMs from OCR to
knowledge reasoning. AncientDoc includes five tasks (page-level OCR, vernacular
translation, reasoning-based QA, knowledge-based QA, linguistic variant QA) and
covers 14 document types, over 100 books, and about 3,000 pages. Based on
AncientDoc, we evaluate mainstream VLMs using multiple metrics, supplemented by
a human-aligned large language model for scoring.

</details>


### [24] [MCP-AgentBench: Evaluating Real-World Language Agent Performance with MCP-Mediated Tools](https://arxiv.org/abs/2509.09734)
*Zikang Guo,Benfeng Xu,Chiwei Zhu,Wentao Hong,Xiaorui Wang,Zhendong Mao*

Main category: cs.CL

TL;DR: MCP-AgentBench是一个专门为评估语言代理在MCP协议下的工具交互能力而设计的综合基准测试，包含33个服务器、188个工具和600个查询任务。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试无法准确评估MCP协议下AI代理的真实性能，导致对其实际价值的误解和能力差异的不可靠判断。

Method: 建立了包含33个操作服务器和188个不同工具的MCP测试床；设计了600个系统化查询任务，分布在6个不同复杂度的交互类别；引入了MCP-Eval结果导向评估方法。

Result: 通过对领先语言代理的广泛实证评估，提供了基础性见解，展示了不同代理在MCP环境下的性能差异。

Conclusion: MCP-AgentBench为研究社区提供了标准化、可靠的框架，用于构建、验证和推进能够充分利用MCP变革性优势的AI代理，加速实现真正强大和可互操作的AI系统。

Abstract: The Model Context Protocol (MCP) is rapidly emerging as a pivotal open
standard, designed to enhance agent-tool integration and interoperability, and
is positioned to unlock a new era of powerful, interconnected, and genuinely
utilitarian agentic AI. However, despite MCP's growing adoption, existing
benchmarks often fail to capture real-world agent performance within this new
paradigm, leading to a distorted perception of their true operational value and
an inability to reliably differentiate proficiencies. To bridge this critical
evaluation gap, we introduce MCP-AgentBench -- a comprehensive benchmark
specifically engineered to rigorously assess language agent capabilities in
MCP-mediated tool interactions. Core contributions of MCP-AgentBench include:
the establishment of a robust MCP testbed comprising 33 operational servers
with 188 distinct tools; the development of a benchmark featuring 600
systematically designed queries distributed across 6 distinct categories of
varying interaction complexity; and the introduction of MCP-Eval, a novel
outcome-oriented evaluation methodology prioritizing real-world task success.
Through extensive empirical evaluation of leading language agents, we provide
foundational insights. MCP-AgentBench aims to equip the research community with
a standardized and reliable framework to build, validate, and advance agents
capable of fully leveraging MCP's transformative benefits, thereby accelerating
progress toward truly capable and interoperable AI systems.

</details>


### [25] [Discrimination by LLMs: Cross-lingual Bias Assessment and Mitigation in Decision-Making and Summarisation](https://arxiv.org/abs/2509.09735)
*Willem Huijzer,Jieying Chen*

Main category: cs.CL

TL;DR: 这项研究分析了LLM在决策和摘要任务中的偏见，包括背景、性别、年龄等因素的偏吐，并评估了提示指令减缓策略的效果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的广泛集成引发了对社会不公平和信息偏见的担忧，需要系统性地研究模型偏见及其演化机制。

Method: 使用英语和荷兰语版本的数据集，创建了30万个提示，在GPT-3.5和GPT-4o上测试不同人口统计变量、指令类型和显著性水平。

Result: 决策任务中显示明显偏吐（偏吐女性、年轻者、非洲美国背景），摘要任务偏见较少。跨语言偏见模式相似但有差异。提示减缓策略最多可减少47%偏见差距，GPT-4o对提示减缓效果更好。

Conclusion: 应谨慎采用LLM，并进行上下文特定的偏见测试。需续续发展有效的偏见减缓策略以确保AI负责任部署。

Abstract: The rapid integration of Large Language Models (LLMs) into various domains
raises concerns about societal inequalities and information bias. This study
examines biases in LLMs related to background, gender, and age, with a focus on
their impact on decision-making and summarization tasks. Additionally, the
research examines the cross-lingual propagation of these biases and evaluates
the effectiveness of prompt-instructed mitigation strategies. Using an adapted
version of the dataset by Tamkin et al. (2023) translated into Dutch, we
created 151,200 unique prompts for the decision task and 176,400 for the
summarisation task. Various demographic variables, instructions, salience
levels, and languages were tested on GPT-3.5 and GPT-4o. Our analysis revealed
that both models were significantly biased during decision-making, favouring
female gender, younger ages, and certain backgrounds such as the
African-American background. In contrast, the summarisation task showed minimal
evidence of bias, though significant age-related differences emerged for
GPT-3.5 in English. Cross-lingual analysis showed that bias patterns were
broadly similar between English and Dutch, though notable differences were
observed across specific demographic categories. The newly proposed mitigation
instructions, while unable to eliminate biases completely, demonstrated
potential in reducing them. The most effective instruction achieved a 27\% mean
reduction in the gap between the most and least favorable demographics.
Notably, contrary to GPT-3.5, GPT-4o displayed reduced biases for all prompts
in English, indicating the specific potential for prompt-based mitigation
within newer models. This research underscores the importance of cautious
adoption of LLMs and context-specific bias testing, highlighting the need for
continued development of effective mitigation strategies to ensure responsible
deployment of AI.

</details>


### [26] [HEFT: A Coarse-to-Fine Hierarchy for Enhancing the Efficiency and Accuracy of Language Model Reasoning](https://arxiv.org/abs/2509.09801)
*Brennen Hill*

Main category: cs.CL

TL;DR: HEFT是一种分层高效微调策略，结合权重空间和表示空间的PEFT方法，在BoolQ基准测试中仅用3个epoch就达到85.17%准确率，超越单一方法20个epoch的表现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在专业推理任务中的适配受计算资源限制，虽然参数高效微调(PEFT)方法提供了解决方案，但不同方法在权重空间和表示空间各有优势，需要探索协同组合的可能性。

Method: 提出HEFT分层适配策略：首先使用LoRA在权重空间进行基础适配，然后使用ReFT在表示空间对内部激活进行精确细化，形成从粗到细的微调过程。

Result: 在Llama-2-7B模型和BoolQ推理数据集上的实验显示，HEFT仅用3个epoch就达到85.17%准确率，优于LoRA-only(85.05%)和ReFT-only(83.36%)方法20个epoch的表现。

Conclusion: PEFT方法的精心组合是一种有效的算法创新，为提升语言模型推理能力提供了更高效有效的路径，能以更少计算预算获得更好结果。

Abstract: The adaptation of large language models (LLMs) to specialized reasoning tasks
is fundamentally constrained by computational resources. Parameter-Efficient
Fine-Tuning (PEFT) methods have emerged as a powerful solution, yet the
landscape of these techniques is diverse, with distinct methods operating in
either the model's weight space or its representation space. This paper
investigates the hypothesis that a synergistic combination of these paradigms
can unlock superior performance and efficiency. We introduce HEFT (Hierarchical
Efficient Fine-Tuning), a novel hierarchical adaptation strategy that composes
two distinct PEFT methods in a coarse-to-fine manner: first, a broad,
foundational adaptation in the weight space using Low-Rank Adaptation (LoRA),
followed by a precise, surgical refinement of internal activations using
Representation Fine-Tuning (ReFT). We evaluate this approach by fine-tuning a
Llama-2-7B model on the BoolQ benchmark, a challenging dataset for inferential
reasoning. Our results reveal a profound synergistic effect. A model fine-tuned
for only three epochs with our HEFT strategy achieves an accuracy of 85.17\%,
exceeding the performance of models trained for 20 epochs with either LoRA-only
(85.05\%) or ReFT-only (83.36\%) methodologies. This work demonstrates that the
thoughtful composition of PEFT methods is a potent algorithmic innovation,
offering a more efficient and effective path toward advancing the reasoning
capabilities of language models. By achieving superior results with a fraction
of the computational budget, our findings present a principled approach to
overcoming the obstacles inherent in adapting large-scale models for complex
cognitive tasks.

</details>


### [27] [Pragmatic Frames Evoked by Gestures: A FrameNet Brasil Approach to Multimodality in Turn Organization](https://arxiv.org/abs/2509.09804)
*Helen de Andrade Abreu,Tiago Timponi Torrent,Ely Edison da Silva Matos*

Main category: cs.CL

TL;DR: 本文提出了一个通过语言与交互手势相关性来建模多模态对话轮次组织的框架，基于语用框架如何被概念化和唤起的分析，并通过标注方法丰富了多模态数据集。


<details>
  <summary>Details</summary>
Motivation: 填补对话轮次组织中特定策略（尤其是手势）在机器学习可用数据集中的编码空白，研究面对面对话中手势在传递、获取和保持对话轮次中的作用。

Method: 开发了标注方法，在Frame2多模态数据集（包含巴西电视剧视频和文本的语义框架标注）基础上增加了用于对话轮次组织的语用框架和手势标注。

Result: 确认了面对面对话中手势作为传递、获取和保持对话轮次工具的使用，并发现了之前未记录的手势变体，证明了语用框架标注有助于更深入理解人类认知和语言。

Conclusion: 手势的使用源于语用框架的概念化，涉及心理空间、概念整合和概念隐喻，语用框架标注为理解人类认知和语言提供了重要见解。

Abstract: This paper proposes a framework for modeling multimodal conversational turn
organization via the proposition of correlations between language and
interactive gestures, based on analysis as to how pragmatic frames are
conceptualized and evoked by communicators. As a means to provide evidence for
the analysis, we developed an annotation methodology to enrich a multimodal
dataset (annotated for semantic frames) with pragmatic frames modeling
conversational turn organization. Although conversational turn organization has
been studied by researchers from diverse fields, the specific strategies,
especially gestures used by communicators, had not yet been encoded in a
dataset that can be used for machine learning. To fill this gap, we enriched
the Frame2 dataset with annotations of gestures used for turn organization. The
Frame2 dataset features 10 episodes from the Brazilian TV series Pedro Pelo
Mundo annotated for semantic frames evoked in both video and text. This dataset
allowed us to closely observe how communicators use interactive gestures
outside a laboratory, in settings, to our knowledge, not previously recorded in
related literature. Our results have confirmed that communicators involved in
face-to-face conversation make use of gestures as a tool for passing, taking
and keeping conversational turns, and also revealed variations of some gestures
that had not been documented before. We propose that the use of these gestures
arises from the conceptualization of pragmatic frames, involving mental spaces,
blending and conceptual metaphors. In addition, our data demonstrate that the
annotation of pragmatic frames contributes to a deeper understanding of human
cognition and language.

</details>


### [28] [Topic-Guided Reinforcement Learning with LLMs for Enhancing Multi-Document Summarization](https://arxiv.org/abs/2509.09852)
*Chuyuan Li,Austin Xu,Shafiq Joty,Giuseppe Carenini*

Main category: cs.CL

TL;DR: 提出基于主题引导强化学习的方法来改进多文档摘要中的内容选择，通过主题奖励机制在GRPO框架中提升摘要与源文档的主题对齐效果


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在单文档摘要中表现优异，但在多文档摘要中仍有改进空间，需要更好地整合多源信息并保持连贯性和主题相关性

Method: 使用主题标签明确提示模型，并在Group Relative Policy Optimization框架中引入新颖的主题奖励机制来度量生成摘要与源文档的主题对齐程度

Result: 在Multi-News和Multi-XScience数据集上的实验表明，该方法持续优于强基线模型

Conclusion: 利用主题线索在多文档摘要中具有显著效果，主题引导的强化学习方法能有效提升内容选择质量

Abstract: A key challenge in Multi-Document Summarization (MDS) is effectively
integrating information from multiple sources while maintaining coherence and
topical relevance. While Large Language Models have shown impressive results in
single-document summarization, their performance on MDS still leaves room for
improvement. In this paper, we propose a topic-guided reinforcement learning
approach to improve content selection in MDS. We first show that explicitly
prompting models with topic labels enhances the informativeness of the
generated summaries. Building on this insight, we propose a novel topic reward
within the Group Relative Policy Optimization (GRPO) framework to measure topic
alignment between the generated summary and source documents. Experimental
results on the Multi-News and Multi-XScience datasets demonstrate that our
method consistently outperforms strong baselines, highlighting the
effectiveness of leveraging topical cues in MDS.

</details>


### [29] [Emulating Public Opinion: A Proof-of-Concept of AI-Generated Synthetic Survey Responses for the Chilean Case](https://arxiv.org/abs/2509.09871)
*Bastián González-Bustamante,Nando Verelst,Carla Cisternas*

Main category: cs.CL

TL;DR: 评估大语言模型生成合成调查回复的可靠性，与智利概率抽样调查的真实人类回复对比，发现LLM在信任项目上表现优异，但存在项目异质性和人口统计偏差。


<details>
  <summary>Details</summary>
Motivation: LLMs在调查研究中提供方法创新，通过合成受访者模拟人类回答，可能减少测量和代表性误差，但需要验证其恢复聚合项目分布的能力和避免训练数据中的偏见。

Method: 使用128个提示-模型-问题三元组生成189,696个合成配置文件，与智利概率抽样调查的真实回复对比，通过准确性、精确度、召回率和F1分数等指标进行元分析，测试关键社会人口统计维度的偏见。

Result: 1) 合成回复在信任项目上表现优异(F1分数和准确性>0.90)；2) GPT-4o、GPT-4o-mini和Llama 4 Maverick表现相当；3) 45-59岁受访者的合成-人类对齐度最高；但存在显著的项目级异质性。

Conclusion: LLM生成的合成样本能够近似概率样本的回复，但要完全捕捉公众意见的细微差别仍具挑战性，需要仔细校准和额外的分布测试以确保算法保真度和减少误差。

Abstract: Large Language Models (LLMs) offer promising avenues for methodological and
applied innovations in survey research by using synthetic respondents to
emulate human answers and behaviour, potentially mitigating measurement and
representation errors. However, the extent to which LLMs recover aggregate item
distributions remains uncertain and downstream applications risk reproducing
social stereotypes and biases inherited from training data. We evaluate the
reliability of LLM-generated synthetic survey responses against ground-truth
human responses from a Chilean public opinion probabilistic survey.
Specifically, we benchmark 128 prompt-model-question triplets, generating
189,696 synthetic profiles, and pool performance metrics (i.e., accuracy,
precision, recall, and F1-score) in a meta-analysis across 128
question-subsample pairs to test for biases along key sociodemographic
dimensions. The evaluation spans OpenAI's GPT family and o-series reasoning
models, as well as Llama and Qwen checkpoints. Three results stand out. First,
synthetic responses achieve excellent performance on trust items (F1-score and
accuracy > 0.90). Second, GPT-4o, GPT-4o-mini and Llama 4 Maverick perform
comparably on this task. Third, synthetic-human alignment is highest among
respondents aged 45-59. Overall, LLM-based synthetic samples approximate
responses from a probabilistic sample, though with substantial item-level
heterogeneity. Capturing the full nuance of public opinion remains challenging
and requires careful calibration and additional distributional tests to ensure
algorithmic fidelity and reduce errors.

</details>


### [30] [Large Language Models Meet Legal Artificial Intelligence: A Survey](https://arxiv.org/abs/2509.09969)
*Zhitian Hou,Zihan Ye,Nanli Zeng,Tianyong Hao,Kun Zeng*

Main category: cs.CL

TL;DR: 本文系统性评估了16个法律LLM系列和47个法律任务框架，收集15个测试基准和29个数据集，分析了法律领域LLM的挑战和未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 促进大语言模型在法律人工智能领域的研究和应用，提高法律任务的效率和准确性。

Method: 综合性评估方法，包括对法律LLM模型、框架、测试基准和数据集的系统性评测和分析。

Result: 提供了完整的资源汇总，包括16个法律LLM系列、47个框架、15个测试基准和29个数据集，为领域研究者提供了实用的评估工具。

Conclusion: 本文为初学者提供了系统性介绍，并推动了法律领域LLM方法的未来研究和应用发展。

Abstract: Large Language Models (LLMs) have significantly advanced the development of
Legal Artificial Intelligence (Legal AI) in recent years, enhancing the
efficiency and accuracy of legal tasks. To advance research and applications of
LLM-based approaches in legal domain, this paper provides a comprehensive
review of 16 legal LLMs series and 47 LLM-based frameworks for legal tasks, and
also gather 15 benchmarks and 29 datasets to evaluate different legal
capabilities. Additionally, we analyse the challenges and discuss future
directions for LLM-based approaches in the legal domain. We hope this paper
provides a systematic introduction for beginners and encourages future research
in this field. Resources are available at
https://github.com/ZhitianHou/LLMs4LegalAI.

</details>


### [31] [CMHG: A Dataset and Benchmark for Headline Generation of Minority Languages in China](https://arxiv.org/abs/2509.09990)
*Guixian Xu,Zeli Su,Ziyin Zhang,Jianing Liu,XU Han,Ting Zhang,Yushuang Dong*

Main category: cs.CL

TL;DR: 提出了一个针对中国少数民族语言（藏语、维吾尔语、蒙古语）的新闻标题生成数据集CMHG，包含20万条数据，并提供了由母语者标注的高质量测试集作为基准。


<details>
  <summary>Details</summary>
Motivation: 中国少数民族语言由于书写系统与国际标准不同，导致相关语料库严重缺乏，特别是在标题生成等监督任务方面存在巨大空白。

Method: 构建了CMHG数据集，包含藏语10万条、维吾尔语和蒙古语各5万条新闻标题生成数据，并由母语者标注高质量测试集。

Result: 创建了一个专门针对少数民族语言标题生成任务的大规模数据集，为相关研究提供了宝贵的资源。

Conclusion: 该数据集将成为推进中国少数民族语言标题生成研究的重要资源，并为相关基准测试的发展做出贡献。

Abstract: Minority languages in China, such as Tibetan, Uyghur, and Traditional
Mongolian, face significant challenges due to their unique writing systems,
which differ from international standards. This discrepancy has led to a severe
lack of relevant corpora, particularly for supervised tasks like headline
generation. To address this gap, we introduce a novel dataset, Chinese Minority
Headline Generation (CMHG), which includes 100,000 entries for Tibetan, and
50,000 entries each for Uyghur and Mongolian, specifically curated for headline
generation tasks. Additionally, we propose a high-quality test set annotated by
native speakers, designed to serve as a benchmark for future research in this
domain. We hope this dataset will become a valuable resource for advancing
headline generation in Chinese minority languages and contribute to the
development of related benchmarks.

</details>


### [32] [Unsupervised Hallucination Detection by Inspecting Reasoning Processes](https://arxiv.org/abs/2509.10004)
*Ponhvoan Srey,Xiaobao Wu,Anh Tuan Luu*

Main category: cs.CL

TL;DR: IRIS是一个无监督幻觉检测框架，利用LLM内部表征来检测生成内容的事实正确性，无需标注数据，计算成本低且效果好


<details>
  <summary>Details</summary>
Motivation: 现有无监督方法依赖与事实正确性无关的代理信号，导致检测偏向表面特征，限制了跨数据集和场景的泛化能力

Method: 通过提示LLM仔细验证陈述的真实性，获取其情境化嵌入作为特征，并将响应不确定性作为真实性的软伪标签

Result: IRIS在实验中 consistently 优于现有无监督方法，计算成本低，即使训练数据少也能良好工作

Conclusion: IRIS是一个完全无监督、计算成本低的框架，适用于实时检测，能有效利用LLM内部表征进行幻觉检测

Abstract: Unsupervised hallucination detection aims to identify hallucinated content
generated by large language models (LLMs) without relying on labeled data.
While unsupervised methods have gained popularity by eliminating
labor-intensive human annotations, they frequently rely on proxy signals
unrelated to factual correctness. This misalignment biases detection probes
toward superficial or non-truth-related aspects, limiting generalizability
across datasets and scenarios. To overcome these limitations, we propose IRIS,
an unsupervised hallucination detection framework, leveraging internal
representations intrinsic to factual correctness. IRIS prompts the LLM to
carefully verify the truthfulness of a given statement, and obtain its
contextualized embedding as informative features for training. Meanwhile, the
uncertainty of each response is considered a soft pseudolabel for truthfulness.
Experimental results demonstrate that IRIS consistently outperforms existing
unsupervised methods. Our approach is fully unsupervised, computationally low
cost, and works well even with few training data, making it suitable for
real-time detection.

</details>


### [33] [Multi-Intent Recognition in Dialogue Understanding: A Comparison Between Smaller Open-Source LLMs](https://arxiv.org/abs/2509.10010)
*Adnan Ahmad,Philine Kowol,Stefan Hillmann,Sebastian Möller*

Main category: cs.CL

TL;DR: 本文分析了三种开源大语言模型在多标签意图分类任务中的性能，发现Mistral-7B模型在少量示例设置下表现最佳，但依然突出于传统的BERT监督学习方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机是评估开源大语言模型在消费硬件上运行的能力，并为任务导向对话系统的自然语言理解提供多标签意图检测框架。

Method: 使用MultiWOZ 2.1数据集，在少量示例设置下测试LLama2-7B、Mistral-7B和Yi-6B三种模型，并与BERT监督学习模型进行对比。评估指标包括准确率、F1分数、推理时间和显存需求等。

Result: Mistral-7B模型在14个意图类别中的11个上F1分数最高，加权平均F1为0.50，同时拥有较低的Humming Loss和较高的Jaccard相似度。但BERT监督学习模型的性能仍然超过最佳的生成式LLM。

Conclusion: 研究为小型开源LLM在复杂多意图对话检测中提供了框架，虽然生成式模型在少量示例情况下表现不错，但传统监督学习方法仍具有明显优势。

Abstract: In this paper, we provide an extensive analysis of multi-label intent
classification using Large Language Models (LLMs) that are open-source,
publicly available, and can be run in consumer hardware. We use the MultiWOZ
2.1 dataset, a benchmark in the dialogue system domain, to investigate the
efficacy of three popular open-source pre-trained LLMs, namely LLama2-7B-hf,
Mistral-7B-v0.1, and Yi-6B. We perform the classification task in a few-shot
setup, giving 20 examples in the prompt with some instructions. Our approach
focuses on the differences in performance of these models across several
performance metrics by methodically assessing these models on multi-label
intent classification tasks. Additionally, we compare the performance of the
instruction-based fine-tuning approach with supervised learning using the
smaller transformer model BertForSequenceClassification as a baseline. To
evaluate the performance of the models, we use evaluation metrics like
accuracy, precision, and recall as well as micro, macro, and weighted F1 score.
We also report the inference time, VRAM requirements, etc. The Mistral-7B-v0.1
outperforms two other generative models on 11 intent classes out of 14 in terms
of F-Score, with a weighted average of 0.50. It also has relatively lower
Humming Loss and higher Jaccard Similarity, making it the winning model in the
few-shot setting. We find BERT based supervised classifier having superior
performance compared to the best performing few-shot generative LLM. The study
provides a framework for small open-source LLMs in detecting complex
multi-intent dialogues, enhancing the Natural Language Understanding aspect of
task-oriented chatbots.

</details>


### [34] [Linguistic trajectories of bipolar disorder on social media](https://arxiv.org/abs/2509.10035)
*Laurin Plank,Armin Zlomuzica*

Main category: cs.CL

TL;DR: 通过分析社交媒体语言，研究发现双相情感障碍诊断前后的语言变化轨迹，包括情绪涉及、经神兼病症、学生物滥用等多种语言标志，并观察到季节性情绪变化的12个月周期性。


<details>
  <summary>Details</summary>
Motivation: 传统临床评估规模有限，社交媒体语言分析能提供高时间分辨率和纵向视野，以实现可扩展的精神健康监测。

Method: 开发方法确定用户诊断时间，分析发现双相情感障碍前3年至诊断后21年的语言轨迹，并与单相抑郁和健康对照组进行比较。

Result: 发现BD诊断后出现普遍语言改变，反映多种病理状态。在诊断后两十年内观察到周期性情绪相关语言变化，具有12个月周期性，可能与季节性情绪发作相关。女性用户呈现更高的周期性。

Conclusion: 研究为双相情感障碍的急性期和慢性期语言变化提供了证据，验证和扩展了利用社交媒体进行可扩展精神健康监测的最新尝试。

Abstract: Language provides valuable markers of affective disorders such as bipolar
disorder (BD), yet clinical assessments remain limited in scale. In response,
analyses of social media (SM) language have gained prominence due to their high
temporal resolution and longitudinal scope. Here, we introduce a method to
determine the timing of users' diagnoses and apply it to study language
trajectories from 3 years before to 21 years after BD diagnosis - contrasted
with uses reporting unipolar depression (UD) and non-affected users (HC). We
show that BD diagnosis is accompanied by pervasive linguistic alterations
reflecting mood disturbance, psychiatric comorbidity, substance abuse,
hospitalization, medical comorbidities, unusual thought content, and
disorganized thought. We further observe recurring mood-related language
changes across two decades after the diagnosis, with a pronounced 12-month
periodicity suggestive of seasonal mood episodes. Finally, trend-level evidence
suggests an increased periodicity in users estimated to be female. In sum, our
findings provide evidence for language alterations in the acute and chronic
phase of BD. This validates and extends recent efforts leveraging SM for
scalable monitoring of mental health.

</details>


### [35] [!MSA at BAREC Shared Task 2025: Ensembling Arabic Transformers for Readability Assessment](https://arxiv.org/abs/2509.10040)
*Mohamed Basem,Mohamed Younes,Seif Ahmed,Abdelrahman Moustafa*

Main category: cs.CL

TL;DR: MSA团队在BAREC 2025阿拉伯语细粒度可读性评估任务中获胜，使用四个Transformer模型的置信度加权集成方法，通过加权训练、数据增强和后处理技术，在句子和文档级别分别达到87.5%和87.4%的QWK分数。


<details>
  <summary>Details</summary>
Motivation: 解决阿拉伯语可读性评估中的严重类别不平衡和数据稀缺问题，提升细粒度可读性预测的准确性。

Method: 使用四个互补的Transformer模型（AraBERTv2、AraELECTRA、MARBERT、CAMeLBERT）进行集成，每个模型使用不同的损失函数；应用加权训练、高级预处理、SAMER语料库重新标注和Gemini 2.5 Flash生成约10,000个稀有级别样本的合成数据；采用针对性后处理步骤修正预测分布偏差。

Result: 在六个赛道中获得第一名，后处理带来6.3%的QWK提升，句子级别达到87.5% QWK，文档级别达到87.4% QWK。

Conclusion: 证明了模型和损失函数多样性、置信度信息融合以及智能数据增强在阿拉伯语可读性预测中的有效性，为处理低资源语言任务提供了有效解决方案。

Abstract: We present MSAs winning system for the BAREC 2025 Shared Task on fine-grained
Arabic readability assessment, achieving first place in six of six tracks. Our
approach is a confidence-weighted ensemble of four complementary transformer
models (AraBERTv2, AraELECTRA, MARBERT, and CAMeLBERT) each fine-tuned with
distinct loss functions to capture diverse readability signals. To tackle
severe class imbalance and data scarcity, we applied weighted training,
advanced preprocessing, SAMER corpus relabeling with our strongest model, and
synthetic data generation via Gemini 2.5 Flash, adding about 10,000 rare-level
samples. A targeted post-processing step corrected prediction distribution
skew, delivering a 6.3 percent Quadratic Weighted Kappa (QWK) gain. Our system
reached 87.5 percent QWK at the sentence level and 87.4 percent at the document
level, demonstrating the power of model and loss diversity, confidence-informed
fusion, and intelligent augmentation for robust Arabic readability prediction.

</details>


### [36] [Established Psychometric vs. Ecologically Valid Questionnaires: Rethinking Psychological Assessments in Large Language Models](https://arxiv.org/abs/2509.10078)
*Dongmin Choi,Woojung Song,Jongwook Han,Eun-Ju Lee,Yohan Jo*

Main category: cs.CL

TL;DR: 这篇论文通过对比分析发现，传统心理测量问卷在LLM人格测量中存在生态效度不足、结果偏差等问题，建议避免使用


<details>
  <summary>Details</summary>
Motivation: 研究者们发现将人类心理测量问卷直接应用于大语言模型存在生态效度疑问，但不清楚两种问卷在结果上的具体差异和含义

Method: 进行了两种问卷类型（传统心理测量问卷与生态有效问卷）的全面对比分析

Result: 发现传统问卷：1）与生态有效问卷在LLM人格测量结果存在显著差异；2）项目数量不足导致测量不稳定；3）会造成LLM拥有稳定构念的误导印象；4）对人设提示的LLM产生夸张的人格测量结果

Conclusion: 建议谨慎使用传统心理测量问卷来测量LLM的人格特征，因为它们在生态效度、稳定性和准确性方面存在显著缺陷

Abstract: Researchers have applied established psychometric questionnaires (e.g., BFI,
PVQ) to measure the personality traits and values reflected in the responses of
Large Language Models (LLMs). However, concerns have been raised about applying
these human-designed questionnaires to LLMs. One such concern is their lack of
ecological validity--the extent to which survey questions adequately reflect
and resemble real-world contexts in which LLMs generate texts in response to
user queries. However, it remains unclear how established questionnaires and
ecologically valid questionnaires differ in their outcomes, and what insights
these differences may provide. In this paper, we conduct a comprehensive
comparative analysis of the two types of questionnaires. Our analysis reveals
that established questionnaires (1) yield substantially different profiles of
LLMs from ecologically valid ones, deviating from the psychological
characteristics expressed in the context of user queries, (2) suffer from
insufficient items for stable measurement, (3) create misleading impressions
that LLMs possess stable constructs, and (4) yield exaggerated profiles for
persona-prompted LLMs. Overall, our work cautions against the use of
established psychological questionnaires for LLMs. Our code will be released
upon publication.

</details>


### [37] [Querying Climate Knowledge: Semantic Retrieval for Scientific Discovery](https://arxiv.org/abs/2509.10087)
*Mustapha Adamu,Qi Zhang,Huitong Pan,Longin Jan Latecki,Eduard C. Dragut*

Main category: cs.CL

TL;DR: 本文构建了一个专门用于气候科学的知识图谱(KG)，通过结构化语义查询提高研究人员发现和访问气候知识的效率，并与大语言模型集成提升回答的透明性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 气候科学文献的复杂性和数量持续增长，使得研究人员难以找到相关信息，需要更好的知识访问方式来应对这一挑战。

Method: 构建基于气候出版物和科学文本的领域特定知识图谱，支持结构化语义查询(Cypher查询)，并与大语言模型集成到RAG系统中。

Result: 知识图谱能够回答具体问题，如查找特定地区验证过的模型或与特定遥相关联模式配合使用的数据集，显示了实际应用价值。

Conclusion: 该知识图谱不仅是技术构建，更具有实际价值，能够为气候研究人员、模型开发者等提供准确、上下文化的科学信息访问支持。

Abstract: The growing complexity and volume of climate science literature make it
increasingly difficult for researchers to find relevant information across
models, datasets, regions, and variables. This paper introduces a
domain-specific Knowledge Graph (KG) built from climate publications and
broader scientific texts, aimed at improving how climate knowledge is accessed
and used. Unlike keyword based search, our KG supports structured, semantic
queries that help researchers discover precise connections such as which models
have been validated in specific regions or which datasets are commonly used
with certain teleconnection patterns. We demonstrate how the KG answers such
questions using Cypher queries, and outline its integration with large language
models in RAG systems to improve transparency and reliability in
climate-related question answering. This work moves beyond KG construction to
show its real world value for climate researchers, model developers, and others
who rely on accurate, contextual scientific information.

</details>


### [38] [Arabic Large Language Models for Medical Text Generation](https://arxiv.org/abs/2509.10095)
*Abdulrahman Allam,Seif Ahmed,Ali Hamdi,Ammar Mohammed*

Main category: cs.CL

TL;DR: 这篇论文提出了一种基于大语言模型精调的阿拉伯医疗文本生成方法，用于改善医院管理系统，帮助患者获得准确的医疗建议、诊断和治疗方案。


<details>
  <summary>Details</summary>
Motivation: 解决现有医院管理系统在处理不规则输入和少数语言时缺乏准确实时医疗建议的能力，特别是在阿拉伯语境下的挑战。

Method: 收集社交媒体平台上真实的医疗对话数据集，经过清洗和预处理处理多种阿拉伯方言。对Mistral-7B-Instruct-v0.2、LLaMA-2-7B和GPT-2 Medium等生成模型进行精调优化。

Result: 精调后的Mistral-7B模型表现最佳，在精准度、召回率和F1分数上分别达到68.5%、69.08%和68.5%的平均BERT分数。评估验证了系统生成有相关性和连贯医疗回复的能力。

Conclusion: 这项研究展示了生成式人工智能在推进医院管理系统中的潜力，为全球健康挑战提供了可扩展和适应性强的解决方案，特别是在语言和文化多样性环境中。

Abstract: Efficient hospital management systems (HMS) are critical worldwide to address
challenges such as overcrowding, limited resources, and poor availability of
urgent health care. Existing methods often lack the ability to provide
accurate, real-time medical advice, particularly for irregular inputs and
underrepresented languages. To overcome these limitations, this study proposes
an approach that fine-tunes large language models (LLMs) for Arabic medical
text generation. The system is designed to assist patients by providing
accurate medical advice, diagnoses, drug recommendations, and treatment plans
based on user input. The research methodology required the collection of a
unique dataset from social media platforms, capturing real-world medical
conversations between patients and doctors. The dataset, which includes patient
complaints together with medical advice, was properly cleaned and preprocessed
to account for multiple Arabic dialects. Fine-tuning state-of-the-art
generative models, such as Mistral-7B-Instruct-v0.2, LLaMA-2-7B, and GPT-2
Medium, optimized the system's ability to generate reliable medical text.
Results from evaluations indicate that the fine-tuned Mistral-7B model
outperformed the other models, achieving average BERT (Bidirectional Encoder
Representations from Transformers) Score values in precision, recall, and
F1-scores of 68.5\%, 69.08\%, and 68.5\%, respectively. Comparative
benchmarking and qualitative assessments validate the system's ability to
produce coherent and relevant medical replies to informal input. This study
highlights the potential of generative artificial intelligence (AI) in
advancing HMS, offering a scalable and adaptable solution for global healthcare
challenges, especially in linguistically and culturally diverse environments.

</details>


### [39] [Scaling Arabic Medical Chatbots Using Synthetic Data: Enhancing Generative AI with Synthetic Patient Records](https://arxiv.org/abs/2509.10108)
*Abdulrahman Allam,Seif Ahmed,Ali Hamdi,Khaled Shaban*

Main category: cs.CL

TL;DR: 通过ChatGPT-4o和Gemini生成合成数据扩充阿拉伯语医疗数据集，将训练集扩展到10万条，提升了LLM在阿拉伯医疗聊天机器人的性能


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语医疗数据集稀缺且质量不高，导致医疗聊天机器人模型的可扩展性和通用性受限

Method: 使用ChatGPT-4o和Gemini 2.5 Pro生成80,000条上下文相关的医学问答对，经过语义筛选和手动验证，将训练集扩展到10万条。细调Mistral-7B等五个LLM，使用BERTScore和专家评估

Result: ChatGPT-4o生成的数据在所有模型中都导致更高的F1分数和更少的幻觉现象，合成数据扩充策略有效提升了模型性能

Conclusion: 合成数据扩充是一种可行的实践方案，可以在资源稀缺的医疗NLP领域增强域特定语言模型，为更包容、可扩展和准确的阿拉伯语健康聊天机器人系统排除障碍

Abstract: The development of medical chatbots in Arabic is significantly constrained by
the scarcity of large-scale, high-quality annotated datasets. While prior
efforts compiled a dataset of 20,000 Arabic patient-doctor interactions from
social media to fine-tune large language models (LLMs), model scalability and
generalization remained limited. In this study, we propose a scalable synthetic
data augmentation strategy to expand the training corpus to 100,000 records.
Using advanced generative AI systems ChatGPT-4o and Gemini 2.5 Pro we generated
80,000 contextually relevant and medically coherent synthetic question-answer
pairs grounded in the structure of the original dataset. These synthetic
samples were semantically filtered, manually validated, and integrated into the
training pipeline. We fine-tuned five LLMs, including Mistral-7B and AraGPT2,
and evaluated their performance using BERTScore metrics and expert-driven
qualitative assessments. To further analyze the effectiveness of synthetic
sources, we conducted an ablation study comparing ChatGPT-4o and
Gemini-generated data independently. The results showed that ChatGPT-4o data
consistently led to higher F1-scores and fewer hallucinations across all
models. Overall, our findings demonstrate the viability of synthetic
augmentation as a practical solution for enhancing domain-specific language
models in-low resource medical NLP, paving the way for more inclusive,
scalable, and accurate Arabic healthcare chatbot systems.

</details>


### [40] [Prominence-aware automatic speech recognition for conversational speech](https://arxiv.org/abs/2509.10116)
*Julian Linke,Barbara Schuppler*

Main category: cs.CL

TL;DR: 综合显著性检测与语音识别的对话式奥地利德语ASR系统，使用wav2vec2模型进行显著性分类和自动注释，虽然ASR性能未改善但显著性检测准确率达85.53%。


<details>
  <summary>Details</summary>
Motivation: 研究如何结合显著性检测和语音识别来实现语调增强的ASR系统，为语言学研究和语调信息对话系统提供应用潜力。

Method: 首先细调wav2vec2模型进行单词级显著性分类，然后用于大规模语料库的自动语调注释，最后训练能同时输出词语和显著性级别的ASR系统。

Result: 集成显著性信息后ASR性能与基线系统相当，但在识别正确的语句中显著性检测准确率达85.53%，证明变奇器模型能有效编码语调信息。

Conclusion: 该研究展示了变奇器模型在编码语调信息方面的效果，为语调增强ASR做出了新颖贡献，具有语言学研究和语调信息对话系统的应用前景。

Abstract: This paper investigates prominence-aware automatic speech recognition (ASR)
by combining prominence detection and speech recognition for conversational
Austrian German. First, prominence detectors were developed by fine-tuning
wav2vec2 models to classify word-level prominence. The detector was then used
to automatically annotate prosodic prominence in a large corpus. Based on those
annotations, we trained novel prominence-aware ASR systems that simultaneously
transcribe words and their prominence levels. The integration of prominence
information did not change performance compared to our baseline ASR system,
while reaching a prominence detection accuracy of 85.53% for utterances where
the recognized word sequence was correct. This paper shows that
transformer-based models can effectively encode prosodic information and
represents a novel contribution to prosody-enhanced ASR, with potential
applications for linguistic research and prosody-informed dialogue systems.

</details>


### [41] [Population-Aligned Persona Generation for LLM-based Social Simulation](https://arxiv.org/abs/2509.10127)
*Zhengyu Hu,Zheyuan Xiao,Max Xiong,Yuxuan Lei,Tianfu Wang,Jianxun Lian,Kaize Ding,Ziang Xiao,Nicholas Jing Yuan,Xing Xie*

Main category: cs.CL

TL;DR: 提出了一个系统框架，用于生成高质量、与人口分布对齐的LLM驱动社交模拟角色集，通过社交媒体数据生成、质量评估和重要性采样来减少人口级偏差。


<details>
  <summary>Details</summary>
Motivation: 现有LLM社交模拟研究主要关注代理框架和模拟环境设计，忽视了角色生成的复杂性和非代表性角色集引入的潜在偏差，需要构建能真实反映现实世界人口多样性和分布的角色集。

Method: 利用LLM从长期社交媒体数据生成叙事角色，进行严格质量评估筛选低质量档案，应用重要性采样实现与参考心理测量分布（如大五人格特质）的全局对齐，并引入任务特定模块针对目标子群体进行适配。

Result: 实验表明该方法显著减少了人口级偏差，为广泛的研究和政策应用实现了准确、灵活的社交模拟。

Conclusion: 该框架能够合成高质量、人口对齐的角色集，有效解决了LLM社交模拟中角色生成的代表性和偏差问题，为计算社会科学提供了更可靠的模拟基础。

Abstract: Recent advances in large language models (LLMs) have enabled human-like
social simulations at unprecedented scale and fidelity, offering new
opportunities for computational social science. A key challenge, however, is
the construction of persona sets that authentically represent the diversity and
distribution of real-world populations. Most existing LLM-based social
simulation studies focus primarily on designing agentic frameworks and
simulation environments, often overlooking the complexities of persona
generation and the potential biases introduced by unrepresentative persona
sets. In this paper, we propose a systematic framework for synthesizing
high-quality, population-aligned persona sets for LLM-driven social simulation.
Our approach begins by leveraging LLMs to generate narrative personas from
long-term social media data, followed by rigorous quality assessment to filter
out low-fidelity profiles. We then apply importance sampling to achieve global
alignment with reference psychometric distributions, such as the Big Five
personality traits. To address the needs of specific simulation contexts, we
further introduce a task-specific module that adapts the globally aligned
persona set to targeted subpopulations. Extensive experiments demonstrate that
our method significantly reduces population-level bias and enables accurate,
flexible social simulation for a wide range of research and policy
applications.

</details>


### [42] [Towards Reliable and Interpretable Document Question Answering via VLMs](https://arxiv.org/abs/2509.10129)
*Alessio Chen,Simone Giovannini,Andrea Gemelli,Fabio Coppini,Simone Marinai*

Main category: cs.CL

TL;DR: DocExplainerV0是一个即插即用的边界框预测模块，将答案生成与空间定位解耦，解决现有视觉语言模型在文档中精确定位答案的难题。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型在文档理解方面表现出色，但准确在文档中定位答案仍然是一个主要挑战，这限制了模型的可解释性和实际应用。

Method: 引入DocExplainerV0模块，该模块与现有VLM系统兼容，包括无法进行微调的专有系统，通过解耦答案生成和空间定位来实现精确定位。

Result: 系统评估显示文本准确性和空间定位之间存在显著差距，正确答案往往缺乏可靠的空间定位。

Conclusion: 该标准化框架揭示了现有方法的不足，并为未来开发更可解释和鲁棒的文档信息提取VLM建立了基准。

Abstract: Vision-Language Models (VLMs) have shown strong capabilities in document
understanding, particularly in identifying and extracting textual information
from complex documents. Despite this, accurately localizing answers within
documents remains a major challenge, limiting both interpretability and
real-world applicability. To address this, we introduce
\textit{DocExplainerV0}, a plug-and-play bounding-box prediction module that
decouples answer generation from spatial localization. This design makes it
applicable to existing VLMs, including proprietary systems where fine-tuning is
not feasible. Through systematic evaluation, we provide quantitative insights
into the gap between textual accuracy and spatial grounding, showing that
correct answers often lack reliable localization. Our standardized framework
highlights these shortcomings and establishes a benchmark for future research
toward more interpretable and robust document information extraction VLMs.

</details>


### [43] [Benchmark of stylistic variation in LLM-generated texts](https://arxiv.org/abs/2509.10179)
*Jiří Milička,Anna Marklová,Václav Cvrček*

Main category: cs.CL

TL;DR: 该研究使用Biber多维分析法比较人类写作与大型语言模型生成文本的语域差异，创建了AI-Brown和AI-Koditex语料库，分析了16个前沿模型在不同设置下的表现，并建立了可解释的模型评估基准。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型生成文本与人类写作在语域特征上的系统性差异，特别是针对训练数据中代表性不足的非英语语言（如捷克语），为LLM文本生成质量评估提供量化方法。

Method: 采用Biber多维分析法(MDA)，构建AI-Brown语料库（对应BE-21布朗家族语料库）和AI-Koditex捷克语料库，分析16个前沿LLM在不同提示和设置下的文本生成，比较基础模型和指令调优模型的差异。

Result: 研究发现LLM与人类写作在多个语域维度上存在显著系统性差异，特别是在非英语语言中差异更明显。指令调优模型相比基础模型表现更好。研究建立了可解释的维度基准用于模型比较和排名。

Conclusion: LLM生成的文本在语域特征上与人类写作存在可量化的差异，需要针对不同语言和语域进行专门优化。建立的多维分析基准为评估和改进LLM文本生成质量提供了有效工具。

Abstract: This study investigates the register variation in texts written by humans and
comparable texts produced by large language models (LLMs). Biber's
multidimensional analysis (MDA) is applied to a sample of human-written texts
and AI-created texts generated to be their counterparts to find the dimensions
of variation in which LLMs differ most significantly and most systematically
from humans. As textual material, a new LLM-generated corpus AI-Brown is used,
which is comparable to BE-21 (a Brown family corpus representing contemporary
British English). Since all languages except English are underrepresented in
the training data of frontier LLMs, similar analysis is replicated on Czech
using AI-Koditex corpus and Czech multidimensional model. Examined were 16
frontier models in various settings and prompts, with emphasis placed on the
difference between base models and instruction-tuned models. Based on this, a
benchmark is created through which models can be compared with each other and
ranked in interpretable dimensions.

</details>


### [44] [Incongruent Positivity: When Miscalibrated Positivity Undermines Online Supportive Conversations](https://arxiv.org/abs/2509.10184)
*Leen Almajed,Abeer ALdayel*

Main category: cs.CL

TL;DR: 本文研究情感支持对话中不恰当的积极回应问题，分析人类和LLM生成的回应在不同情感强度情境下的表现差异，并开发检测模型。


<details>
  <summary>Details</summary>
Motivation: 在情感支持对话中，善意的积极回应有时会适得其反，显得轻描淡写或不切实际。研究旨在分析这种不协调的积极性现象，特别是在人类和大型语言模型生成回应中的表现。

Method: 收集Reddit真实用户-助手对话，按情感强度分为轻度（关系紧张和一般建议）和重度（悲伤和焦虑对话）两类。使用大型语言模型生成额外回应，微调LLM，并开发基于DeBERTa和MentalBERT的弱监督多标签分类器集成。

Result: 分析显示LLM更容易产生不切实际的积极性，特别是在高风险情境下表现出轻描淡写和最小化的语气。开发的分类器集成在检测不协调积极性类型方面表现改善。

Conclusion: 研究强调需要超越生成通用积极回应，转而研究协调的支持措施来平衡积极情感与情感认同。这为在线支持对话中使大型语言模型与情感期望保持一致提供了见解，为构建情境感知和信任保持的在线对话系统铺平道路。

Abstract: In emotionally supportive conversations, well-intended positivity can
sometimes misfire, leading to responses that feel dismissive, minimizing, or
unrealistically optimistic. We examine this phenomenon of incongruent
positivity as miscalibrated expressions of positive support in both human and
LLM generated responses. To this end, we collected real user-assistant
dialogues from Reddit across a range of emotional intensities and generated
additional responses using large language models for the same context. We
categorize these conversations by intensity into two levels: Mild, which covers
relationship tension and general advice, and Severe, which covers grief and
anxiety conversations. This level of categorization enables a comparative
analysis of how supportive responses vary across lower and higher stakes
contexts. Our analysis reveals that LLMs are more prone to unrealistic
positivity through dismissive and minimizing tone, particularly in high-stakes
contexts. To further study the underlying dimensions of this phenomenon, we
finetune LLMs on datasets with strong and weak emotional reactions. Moreover,
we developed a weakly supervised multilabel classifier ensemble (DeBERTa and
MentalBERT) that shows improved detection of incongruent positivity types
across two sorts of concerns (Mild and Severe). Our findings shed light on the
need to move beyond merely generating generic positive responses and instead
study the congruent support measures to balance positive affect with emotional
acknowledgment. This approach offers insights into aligning large language
models with affective expectations in the online supportive dialogue, paving
the way toward context-aware and trust preserving online conversation systems.

</details>


### [45] [Beyond Token Limits: Assessing Language Model Performance on Long Text Classification](https://arxiv.org/abs/2509.10199)
*Miklós Sebők,Viktor Kovács,Martin Bánóczy,Daniel Møller Eriksen,Nathalie Neptune,Philippe Roussille*

Main category: cs.CL

TL;DR: 本文比较了多种大语言模型在处理长文本分类任务（特别是法律文件分类）时的性能，发现专门为长文本设计的Longformer模型并无明显优势，开源模型表现优于GPT变体。


<details>
  <summary>Details</summary>
Motivation: 现有主流语言模型（如BERT及其衍生模型）存在输入长度限制，无法有效处理像法律草案这样长达数百页的长文本分类任务，需要探索适合长文本处理的模型解决方案。

Method: 在5种语言上对XLM-RoBERTa、Longformer、GPT-3.5、GPT-4等模型进行比较实验，使用比较议程项目的21个政策主题标签进行多类分类任务评估。

Result: Longformer模型在处理长输入方面没有显示出特别优势；开源模型表现优于GPT变体；类别级别的分析显示特定类别之间的支持度和内容重叠对长文本输入性能有重要影响。

Conclusion: 专门为长文本预训练的模型不一定优于通用模型，开源模型在长文本分类任务中表现更佳，类别间的语义重叠是影响长文本处理性能的关键因素。

Abstract: The most widely used large language models in the social sciences (such as
BERT, and its derivatives, e.g. RoBERTa) have a limitation on the input text
length that they can process to produce predictions. This is a particularly
pressing issue for some classification tasks, where the aim is to handle long
input texts. One such area deals with laws and draft laws (bills), which can
have a length of multiple hundred pages and, therefore, are not particularly
amenable for processing with models that can only handle e.g. 512 tokens. In
this paper, we show results from experiments covering 5 languages with
XLM-RoBERTa, Longformer, GPT-3.5, GPT-4 models for the multiclass
classification task of the Comparative Agendas Project, which has a codebook of
21 policy topic labels from education to health care. Results show no
particular advantage for the Longformer model, pre-trained specifically for the
purposes of handling long inputs. The comparison between the GPT variants and
the best-performing open model yielded an edge for the latter. An analysis of
class-level factors points to the importance of support and substance overlaps
between specific categories when it comes to performance on long text inputs.

</details>


### [46] [SI-FACT: Mitigating Knowledge Conflict via Self-Improving Faithfulness-Aware Contrastive Tuning](https://arxiv.org/abs/2509.10208)
*Shengqiang Fu*

Main category: cs.CL

TL;DR: 提出Self Improving Faithfulness Aware Contrastive Tuning框架，通过自指导机制自动生成对比学习数据，提升LLM在知识冲突任务中的忠实度


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在知识密集型任务中因偏好内部参数知识而非提供上下文导致的忠实度问题

Method: 使用自指导机制自动生成高质量结构化对比学习数据（锚样本、语义等价正样本、模拟不忠实场景的负样本），然后应用对比学习训练模型

Result: 在ECARE KRE和COSE KRE基准测试中，基于Llama3 8B Instruct的SI FACT模型将上下文召回率提高了6.2%，显著减少了对内部记忆的依赖

Conclusion: SI FACT在增强LLM上下文忠实度方面具有强效性和高数据效率，为构建更主动可信的语言模型提供了实用途径

Abstract: Large Language Models often generate unfaithful responses in knowledge
intensive tasks due to knowledge conflict,that is,a preference for relying on
internal parametric knowledge rather than the provided context.To address this
issue,we propose a novel self improving framework,Self Improving Faithfulness
Aware Contrastive Tuning.The framework uses a self instruct mechanism that
allows the base LLM to automatically generate high quality,structured
contrastive learning data,including anchor samples,semantically equivalent
positive samples,and negative samples simulating unfaithful scenarios.This
approach significantly reduces the cost of manual
annotation.Subsequently,contrastive learning is applied to train the
model,enabling it to pull faithful responses closer and push unfaithful
responses farther apart in the representation space.Experiments on knowledge
conflict evaluation benchmarks ECARE KRE and COSE KRE show that the SI FACT
model based on Llama3 8B Instruct improves the Contextual Recall Rate by 6.2%
over the best baseline method,while significantly reducing dependence on
internal memory.The results indicate that SI FACT provides strong effectiveness
and high data efficiency in enhancing the contextual faithfulness of
LLMs,offering a practical pathway toward building more proactive and
trustworthy language models.

</details>


### [47] [Dropping Experts, Recombining Neurons: Retraining-Free Pruning for Sparse Mixture-of-Experts LLMs](https://arxiv.org/abs/2509.10377)
*Yixiao Zhou,Ziyu Zhao,Dongzhou Cheng,zhiliang wu,Jie Gui,Yi Yang,Fei Wu,Yu Cheng,Hehe Fan*

Main category: cs.CL

TL;DR: DERN是一个无需重新训练的任务无关框架，通过专家剪枝和神经元重组来减少SMoE模型的内存使用，在50%专家稀疏度下性能提升5%以上


<details>
  <summary>Details</summary>
Motivation: SMoE架构虽然计算高效，但仍需加载所有专家参数，导致内存使用高和部署困难。现有方法主要关注专家级操作，忽视了神经元级结构

Method: 三步骤框架：1)使用路由器统计剪枝冗余专家；2)将专家分解为神经元级片段并分配到最兼容的保留专家；3)在保留专家内合并片段构建紧凑表示

Result: 在Mixtral、Qwen和DeepSeek SMoE模型上，50%专家稀疏度下常识推理和MMLU基准性能提升超过5%，显著减少专家数量和内存使用

Conclusion: DERN有效解决了SMoE模型的内存和部署问题，通过神经元级重组实现了无需重新训练的性能提升，具有实际部署价值

Abstract: Sparse Mixture-of-Experts (SMoE) architectures are widely used in large
language models (LLMs) due to their computational efficiency. However, though
only a few experts are activated for each token, SMoE still requires loading
all expert parameters, leading to high memory usage and challenges in
deployment. Previous work has tried to reduce the overhead by pruning and
merging experts, but primarily focused on expert-level operations, leaving
neuron-level structure underexplored. We propose DERN (Dropping Experts,
Recombining Neurons), a task-agnostic and retraining-free framework for expert
pruning and reconstruction. We observe that experts are often misaligned and
contain semantic conflicts at the neuron level, which poses challenges for
direct merging. To solve this, DERN works in three steps: it first prunes
redundant experts using router statistics; then it decomposes them into
neuron-level expert segments, assigning each segment to its most compatible
retained expert; and finally, it merges segments within each retained expert to
build a compact representation. Experiments on Mixtral, Qwen, and DeepSeek SMoE
models show that DERN improves performance by more than 5% on commonsense
reasoning and MMLU benchmarks under 50% expert sparsity, without extra
training. It also greatly reduces the number of experts and memory usage,
making SMoE LLMs easier to deploy in practice.

</details>


### [48] [Is In-Context Learning Learning?](https://arxiv.org/abs/2509.10414)
*Adrian de Wynter*

Main category: cs.CL

TL;DR: 本文通过大规模实证分析证明，上下文学习(ICL)确实构成学习机制，但其学习能力和泛化能力有限，特别是在面对未见任务时表现不佳。研究发现随着示例增多，准确率对分布、模型和提示风格变得不敏感，但存在分布敏感性，尤其是在思维链等提示风格中。


<details>
  <summary>Details</summary>
Motivation: 当前关于自回归模型通过上下文学习解决任务的能力存在争议，需要从数学和实证角度明确ICL是否真正构成学习，以及其学习机制和局限性。

Method: 进行大规模ICL分析，通过消融实验排除记忆效应、预训练影响、分布偏移等因素，研究不同提示风格和措辞对学习效果的影响。

Result: ICL是有效的学习范式，但学习未见任务的能力有限；随着示例数量增加，准确率对示例分布、模型、提示风格和语言特征变得不敏感；ICL从提示中的规律性推断模式，导致分布敏感性，特别是在思维链提示中。

Conclusion: 自回归模型的临时编码机制不够鲁棒，表明其通用泛化能力有限，ICL虽然构成学习但存在明显局限性。

Abstract: In-context learning (ICL) allows some autoregressive models to solve tasks
via next-token prediction and without needing further training. This has led to
claims about these model's ability to solve (learn) unseen tasks with only a
few shots (exemplars) in the prompt. However, deduction does not always imply
learning, as ICL does not explicitly encode a given observation. Instead, the
models rely on their prior knowledge and the exemplars given, if any. We argue
that, mathematically, ICL does constitute learning, but its full
characterisation requires empirical work. We then carry out a large-scale
analysis of ICL ablating out or accounting for memorisation, pretraining,
distributional shifts, and prompting style and phrasing. We find that ICL is an
effective learning paradigm, but limited in its ability to learn and generalise
to unseen tasks. We note that, in the limit where exemplars become more
numerous, accuracy is insensitive to exemplar distribution, model, prompt
style, and the input's linguistic features. Instead, it deduces patterns from
regularities in the prompt, which leads to distributional sensitivity,
especially in prompting styles such as chain-of-thought. Given the varied
accuracies on formally similar tasks, we conclude that autoregression's ad-hoc
encoding is not a robust mechanism, and suggests limited all-purpose
generalisability.

</details>


### [49] [Long Context Automated Essay Scoring with Language Models](https://arxiv.org/abs/2509.10417)
*Christopher Ormerod,Gitit Kehat*

Main category: cs.CL

TL;DR: 这篇论文研究了如何解决Transformer模型在自动论文评分中的长文本处理问题，通过比较多种支持长上下文的模型在Kaggle ASAP 2.0数据集上的表现。


<details>
  <summary>Details</summary>
Motivation: 高年级学生论文经常超过开源Transformer模型的最大长度限制，截断处理会影响对论文组织结构的评估，引发效度问题。

Method: 评估多种经过架构修改的Transformer模型，包括XLNet、Longformer、ModernBERT、Mamba和Llama模型的微调版本。

Result: 研究在Kaggle ASAP 2.0数据集上进行了模型比较分析，以确定哪些模型能够有效处理超长论文文本。

Conclusion: 需要使用支持长上下文的修改版Transformer模型来解决自动论文评分中的文本截断问题，以保证对论文组织结构的全面评估。

Abstract: Transformer-based language models are architecturally constrained to process
text of a fixed maximum length. Essays written by higher-grade students
frequently exceed the maximum allowed length for many popular open-source
models. A common approach to addressing this issue when using these models for
Automated Essay Scoring is to truncate the input text. This raises serious
validity concerns as it undermines the model's ability to fully capture and
evaluate organizational elements of the scoring rubric, which requires long
contexts to assess. In this study, we evaluate several models that incorporate
architectural modifications of the standard transformer architecture to
overcome these length limitations using the Kaggle ASAP 2.0 dataset. The models
considered in this study include fine-tuned versions of XLNet, Longformer,
ModernBERT, Mamba, and Llama models.

</details>


### [50] [RefactorCoderQA: Benchmarking LLMs for Multi-Domain Coding Question Solutions in Cloud and Edge Deployment](https://arxiv.org/abs/2509.10436)
*Shadikur Rahman,Aroosa Hameed,Gautam Srivastava,Syed Muhammad Danish*

Main category: cs.CL

TL;DR: 这篇论文提出了一种云边协同的多段代理提示框架，包含GuideLLM、SolverLLM和JudgeLLM三个专业组件，并创建RefactorCoderQA标准数据集进行评估，细调模型在多领域编码任务上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有标准数据集的局限性，优化大语言模型的推理和问题解决能力，需要一种结构化的多段代理提示框架。

Method: 提出云边协同架构：边缘部署轻量GuideLLM提供方法指导，云端部署强大SolverLLM生成代码解决方案，JudgeLLM自动评估解决方案质量。创建RefactorCoderQA标准数据集测试多领域编码任务。

Result: 细调模型RefactorCoder-MoE达到相对准确率76.84%，显著超过开源和商业基线模型。人工评估验证了解决方案的可解释性、准确性和实际相关性。系统级指标如吞吐量和延迟也得到评估。

Conclusion: 该云边协同多段代理框架有效提升了LLM在编码任务中的表现，新的RefactorCoderQA标准数据集为多领域代码问题解决提供了全面的评估基准。

Abstract: To optimize the reasoning and problem-solving capabilities of Large Language
Models (LLMs), we propose a novel cloud-edge collaborative architecture that
enables a structured, multi-agent prompting framework. This framework comprises
three specialized components: GuideLLM, a lightweight model deployed at the
edge to provide methodological guidance; SolverLLM, a more powerful model
hosted in the cloud responsible for generating code solutions; and JudgeLLM, an
automated evaluator for assessing solution correctness and quality. To evaluate
and demonstrate the effectiveness of this architecture in realistic settings,
we introduce RefactorCoderQA, a comprehensive benchmark designed to evaluate
and enhance the performance of Large Language Models (LLMs) across multi-domain
coding tasks. Motivated by the limitations of existing benchmarks,
RefactorCoderQA systematically covers various technical domains, including
Software Engineering, Data Science, Machine Learning, and Natural Language
Processing, using authentic coding challenges from Stack Overflow. Extensive
experiments reveal that our fine-tuned model, RefactorCoder-MoE, achieves
state-of-the-art performance, significantly outperforming leading open-source
and commercial baselines with an overall accuracy of 76.84%. Human evaluations
further validate the interpretability, accuracy, and practical relevance of the
generated solutions. In addition, we evaluate system-level metrics, such as
throughput and latency, to gain deeper insights into the performance
characteristics and trade-offs of the proposed architecture.

</details>


### [51] [DeepDive: Advancing Deep Search Agents with Knowledge Graphs and Multi-Turn RL](https://arxiv.org/abs/2509.10446)
*Rui Lu,Zhenyu Hou,Zihan Wang,Hanchen Zhang,Xiao Liu,Yujiang Li,Shi Feng,Jie Tang,Yuxiao Dong*

Main category: cs.CL

TL;DR: DeepDive通过自动合成复杂问题和多轮强化学习，显著提升了开源大语言模型在深度搜索任务中的表现，在BrowseComp基准上达到了新的开源竞争性结果。


<details>
  <summary>Details</summary>
Motivation: 现有开源大语言模型在深度搜索任务中表现不佳，主要受限于长时程推理能力和缺乏足够难度的监督数据。

Method: 1) 从开放知识图谱自动合成复杂难找的问题；2) 应用端到端多轮强化学习来增强LLM的深度搜索长时程推理能力。

Result: DeepDive-32B在BrowseComp基准上超越了WebSailor、DeepSeek-R1-Browse和Search-o1等模型，多轮RL训练显著提升了深度搜索能力。

Conclusion: DeepDive证明了通过自动数据合成和多轮强化学习可以有效提升开源LLM的深度搜索性能，支持工具调用和并行采样的测试时扩展。

Abstract: Augmenting large language models (LLMs) with browsing tools substantially
improves their potential as deep search agents to solve complex, real-world
tasks. Yet, open LLMs still perform poorly in such settings due to limited
long-horizon reasoning capacity with browsing tools and the lack of
sufficiently difficult supervised data. To address these challenges, we present
DeepDive to advance deep search agents. First, we propose a strategy to
automatically synthesize complex, difficult, and hard-to-find questions from
open knowledge graphs. Second, we apply end-to-end multi-turn reinforcement
learning (RL) to enhance LLMs' long-horizon reasoning with deep search.
Experiments show that DeepDive-32B achieves a new open-source competitive
result on BrowseComp, outperforming WebSailor, DeepSeek-R1-Browse, and
Search-o1. We demonstrate that multi-turn RL training improves deep search
ability and significantly contributes to the performance improvements across
multiple benchmarks. We observe that DeepDive enables test-time scaling of tool
calls and parallel sampling. All datasets, models, and code are publicly
available at https://github.com/THUDM/DeepDive.

</details>


### [52] [WhisTLE: Deeply Supervised, Text-Only Domain Adaptation for Pretrained Speech Recognition Transformers](https://arxiv.org/abs/2509.10452)
*Akshat Pandey,Karun Kumar,Raphael Tang*

Main category: cs.CL

TL;DR: WhisTLE是一种仅使用文本数据进行预训练ASR模型领域适应的新方法，通过变分自编码器建模编码器输出并微调解码器，无需额外运行时成本即可显著降低词错误率。


<details>
  <summary>Details</summary>
Motivation: 预训练ASR模型如Whisper在未见词汇和方言上表现不佳，但在许多实际场景中收集语音数据不切实际，需要仅使用文本数据进行领域适应。

Method: 提出WhisTLE方法：1）训练变分自编码器(VAE)从文本建模编码器输出；2）使用学习的文本到潜在编码器微调解码器；3）可选结合文本到语音(TTS)适应；4）推理时恢复原始编码器，无额外运行时成本。

Result: 在四个域外数据集和四个ASR模型上，WhisTLE结合TTS相比仅使用TTS适应相对降低词错误率12.3%，在32个场景中的27个场景中优于所有非WhisTLE基线方法。

Conclusion: WhisTLE是一种有效的文本仅适应方法，能够显著提升预训练ASR模型在未见领域的性能，且不增加推理时的计算成本。

Abstract: Pretrained automatic speech recognition (ASR) models such as Whisper perform
well but still need domain adaptation to handle unseen vocabulary and parlance.
In many real-world settings, collecting speech data is impractical,
necessitating text-only adaptation. We propose WhisTLE, a deeply supervised,
text-only adaptation method for pretrained encoder-decoder ASR models. WhisTLE
trains a variational autoencoder (VAE) to model encoder outputs from text and
fine-tunes the decoder using the learned text-to-latent encoder, optionally
combined with text-to-speech (TTS) adaptation. At inference, the original
encoder is restored, incurring no extra runtime cost. Across four out-of-domain
datasets and four ASR models, WhisTLE with TTS reduces word error rate (WER) by
12.3% relative to TTS-only adaptation and outperforms all non-WhisTLE baselines
in 27 of 32 scenarios.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [53] [Executable Ontologies: Synthesizing Event Semantics with Dataflow Architecture](https://arxiv.org/abs/2509.09775)
*Aleksandr Boldachev*

Main category: cs.AI

TL;DR: boldsea是一个基于语义事件的架构，使用可执行本体来建模复杂动态系统，解决了传统BPM系统和面向对象语义技术的局限性。


<details>
  <summary>Details</summary>
Motivation: 解决传统业务流程管理系统和面向对象语义技术在动态系统建模中的局限性，提供更灵活的运行时模型修改和更好的时间透明度。

Method: 提出BSL语义语言及其BNF语法，设计boldsea-engine架构，直接解释语义模型为可执行算法，无需编译过程。

Result: 实现了事件模型的运行时修改、时间透明度保证，以及数据和业务逻辑在统一语义框架内的无缝集成。

Conclusion: boldsea架构通过语义事件方法成功解决了传统系统的限制，为复杂动态系统建模提供了有效的解决方案。

Abstract: This paper presents boldsea, Boldachev's semantic-event approach -- an
architecture for modeling complex dynamic systems using executable ontologies
-- semantic models that act as dynamic structures, directly controlling process
execution. We demonstrate that integrating event semantics with a dataflow
architecture addresses the limitations of traditional Business Process
Management (BPM) systems and object-oriented semantic technologies. The paper
presents the formal BSL (boldsea Semantic Language), including its BNF grammar,
and outlines the boldsea-engine's architecture, which directly interprets
semantic models as executable algorithms without compilation. It enables the
modification of event models at runtime, ensures temporal transparency, and
seamlessly merges data and business logic within a unified semantic framework.

</details>


### [54] [LLMs as Agentic Cooperative Players in Multiplayer UNO](https://arxiv.org/abs/2509.09867)
*Yago Romano Matinez,Jesse Roberts*

Main category: cs.AI

TL;DR: LLM智能体在UNO游戏中作为助手参与，测试其能否帮助其他玩家获胜而非自己获胜。研究发现虽然所有模型都能超越随机基线，但很少有模型能显著帮助其他玩家。


<details>
  <summary>Details</summary>
Motivation: 测试基于大语言模型的智能体能否作为主动参与者帮助人类实现目标，特别是在UNO游戏中帮助其他玩家获胜。

Method: 构建工具让仅解码器LLM在RLCard游戏环境中作为智能体参与，接收完整游戏状态信息，使用两种不同的提示策略进行文本响应，评估从1B到70B参数的不同规模模型。

Result: 所有模型在玩UNO时都能成功超越随机基线表现，但很少有模型能够显著帮助其他玩家获胜。

Conclusion: 虽然LLM在游戏中表现出基本能力，但在作为主动助手帮助他人实现目标方面仍有局限，模型规模对性能有影响但并非决定性因素。

Abstract: LLMs promise to assist humans -- not just by answering questions, but by
offering useful guidance across a wide range of tasks. But how far does that
assistance go? Can a large language model based agent actually help someone
accomplish their goal as an active participant? We test this question by
engaging an LLM in UNO, a turn-based card game, asking it not to win but
instead help another player to do so. We built a tool that allows decoder-only
LLMs to participate as agents within the RLCard game environment. These models
receive full game-state information and respond using simple text prompts under
two distinct prompting strategies. We evaluate models ranging from small (1B
parameters) to large (70B parameters) and explore how model scale impacts
performance. We find that while all models were able to successfully outperform
a random baseline when playing UNO, few were able to significantly aid another
player.

</details>


### [55] [Abduct, Act, Predict: Scaffolding Causal Inference for Automated Failure Attribution in Multi-Agent Systems](https://arxiv.org/abs/2509.10401)
*Alva West,Yixuan Weng,Minjun Zhu,Zhen Lin,Yue Zhang*

Main category: cs.AI

TL;DR: A2P Scaffolding框架通过结构化因果推理方法，将多智能体系统中的故障归因从模式识别任务转变为因果推理任务，显著提高了步骤级准确率


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统中的故障归因方法准确率极低（低于17%），无法进行有效的反事实推理来确定纠正单个动作是否能避免任务失败

Method: 提出Abduct-Act-Predict (A2P) Scaffolding框架，通过三步推理过程：溯因推理推断行动背后的隐藏原因、定义最小纠正干预、模拟后续轨迹验证干预效果

Result: 在Algorithm-Generated数据集上达到47.46%的步骤级准确率（比基线16.67%提高2.85倍），在Hand-Crafted数据集上达到29.31%准确率（比基线12.07%提高2.43倍）

Conclusion: 通过因果推理框架重构问题，A2P Scaffolding为自动化故障归因提供了更稳健、可验证且准确度显著提高的解决方案

Abstract: Failure attribution in multi-agent systems -- pinpointing the exact step
where a decisive error occurs -- is a critical yet unsolved challenge. Current
methods treat this as a pattern recognition task over long conversation logs,
leading to critically low step-level accuracy (below 17\%), which renders them
impractical for debugging complex systems. Their core weakness is a fundamental
inability to perform robust counterfactual reasoning: to determine if
correcting a single action would have actually averted the task failure. To
bridge this counterfactual inference gap, we introduce Abduct-Act-Predict (A2P)
Scaffolding, a novel agent framework that transforms failure attribution from
pattern recognition into a structured causal inference task. A2P explicitly
guides a large language model through a formal three-step reasoning process
within a single inference pass: (1) Abduction, to infer the hidden root causes
behind an agent's actions; (2) Action, to define a minimal corrective
intervention; and (3) Prediction, to simulate the subsequent trajectory and
verify if the intervention resolves the failure. This structured approach
leverages the holistic context of the entire conversation while imposing a
rigorous causal logic on the model's analysis. Our extensive experiments on the
Who\&When benchmark demonstrate its efficacy. On the Algorithm-Generated
dataset, A2P achieves 47.46\% step-level accuracy, a 2.85$\times$ improvement
over the 16.67\% of the baseline. On the more complex Hand-Crafted dataset, it
achieves 29.31\% step accuracy, a 2.43$\times$ improvement over the baseline's
12.07\%. By reframing the problem through a causal lens, A2P Scaffolding
provides a robust, verifiable, and significantly more accurate solution for
automated failure attribution.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [56] [HypoGeneAgent: A Hypothesis Language Agent for Gene-Set Cluster Resolution Selection Using Perturb-seq Datasets](https://arxiv.org/abs/2509.09740)
*Ying Yuan,Xing-Yue Monica Ge,Aaron Archer Waterman,Tommaso Biancalani,David Richmond,Yogesh Pandit,Avtar Singh,Russell Littman,Jin Liu,Jan-Christian Huetter,Vladimir Ermakov*

Main category: q-bio.QM

TL;DR: HYPOGENEAGENT是一个基于大语言模型的框架，将细胞聚类注释转化为可量化优化的任务，通过计算簇内一致性和簇间分离度来选择最佳聚类分辨率


<details>
  <summary>Details</summary>
Motivation: 解决单细胞研究中聚类分辨率选择和功能注释的主观性问题，传统方法依赖启发式规则和专家经验

Method: 使用LLM作为基因集分析器生成GO假设和置信度评分，然后通过句子嵌入模型计算簇内一致性和簇间分离度，组合得到分辨率评分

Result: 在K562 CRISPRi Perturb-seq数据集上测试，该方法选择的聚类粒度与已知通路更匹配，优于传统指标如轮廓系数和模块度评分

Conclusion: LLM代理可以作为聚类分辨率和功能注释的客观评判者，为单细胞多组学研究实现全自动、上下文感知的解释流程

Abstract: Large-scale single-cell and Perturb-seq investigations routinely involve
clustering cells and subsequently annotating each cluster with Gene-Ontology
(GO) terms to elucidate the underlying biological programs. However, both
stages, resolution selection and functional annotation, are inherently
subjective, relying on heuristics and expert curation. We present
HYPOGENEAGENT, a large language model (LLM)-driven framework, transforming
cluster annotation into a quantitatively optimizable task. Initially, an LLM
functioning as a gene-set analyst analyzes the content of each gene program or
perturbation module and generates a ranked list of GO-based hypotheses,
accompanied by calibrated confidence scores. Subsequently, we embed every
predicted description with a sentence-embedding model, compute pair-wise cosine
similarities, and let the agent referee panel score (i) the internal
consistency of the predictions, high average similarity within the same
cluster, termed intra-cluster agreement (ii) their external distinctiveness,
low similarity between clusters, termed inter-cluster separation. These two
quantities are combined to produce an agent-derived resolution score, which is
maximized when clusters exhibit simultaneous coherence and mutual exclusivity.
When applied to a public K562 CRISPRi Perturb-seq dataset as a preliminary
test, our Resolution Score selects clustering granularities that exhibit
alignment with known pathway compared to classical metrics such silhouette
score, modularity score for gene functional enrichment summary. These findings
establish LLM agents as objective adjudicators of cluster resolution and
functional annotation, thereby paving the way for fully automated,
context-aware interpretation pipelines in single-cell multi-omics studies.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [57] [LLM-Based Instance-Driven Heuristic Bias In the Context of a Biased Random Key Genetic Algorithm](https://arxiv.org/abs/2509.09707)
*Camilo Chacón Sartori,Martín Isla Pino,Pedro Pinacho-Davidson,Christian Blum*

Main category: cs.NE

TL;DR: 提出了一种将大语言模型与BRKGA算法结合的新框架，通过LLM分析问题实例特征生成定制化启发式偏差，在复杂组合优化问题上显著提升性能


<details>
  <summary>Details</summary>
Motivation: 现有方法大多利用LLM进行代码生成来创建或改进特定启发式算法，但往往忽略了单个问题实例的结构特性，需要更智能的实例驱动方法

Method: 结合LLM与BRKGA算法，通过人-LLM协作设计计算高效指标，LLM分析实例特定指标生成定制启发式偏差来指导BRKGA搜索

Result: 在1,050个不同复杂度实例上的实验表明，最佳混合方法BRKGA+Llama-4-Maverick相比基线取得统计显著改进，特别是在最复杂实例上

Conclusion: 利用LLM产生先验的实例驱动启发式偏差是增强复杂优化领域中元启发式算法的有效方法

Abstract: Integrating Large Language Models (LLMs) within metaheuristics opens a novel
path for solving complex combinatorial optimization problems. While most
existing approaches leverage LLMs for code generation to create or refine
specific heuristics, they often overlook the structural properties of
individual problem instances. In this work, we introduce a novel framework that
integrates LLMs with a Biased Random-Key Genetic Algorithm (BRKGA) to solve the
NP-hard Longest Run Subsequence problem. Our approach extends the
instance-driven heuristic bias paradigm by introducing a human-LLM
collaborative process to co-design and implement a set of computationally
efficient metrics. The LLM analyzes these instance-specific metrics to generate
a tailored heuristic bias, which steers the BRKGA toward promising areas of the
search space. We conduct a comprehensive experimental evaluation, including
rigorous statistical tests, convergence and behavioral analyses, and targeted
ablation studies, comparing our method against a standard BRKGA baseline across
1,050 generated instances of varying complexity. Results show that our
top-performing hybrid, BRKGA+Llama-4-Maverick, achieves statistically
significant improvements over the baseline, particularly on the most complex
instances. Our findings confirm that leveraging an LLM to produce an a priori,
instance-driven heuristic bias is a valuable approach for enhancing
metaheuristics in complex optimization domains.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [58] [Vibe Check: Understanding the Effects of LLM-Based Conversational Agents' Personality and Alignment on User Perceptions in Goal-Oriented Tasks](https://arxiv.org/abs/2509.09870)
*Hasibur Rahman,Smit Desai*

Main category: cs.HC

TL;DR: 这研究探索了语言模型人格表达程度和用户-机器人格匹配对目标导向任务中用户感知的影响，发现中等表达程度和人格匹配能夠产生最优效果。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型让对话机器能够表达独特个性，需要研究不同人格表达程度和用户-机器人格匹配如何影响用户感知。

Method: 采用组间实验设计(N=150)，通过新颖的Trait Modulation Keys框架控制对话机器在Big Five特质上的低、中、高表达程度，让参与者完成旅行规划任务。

Result: 发现倒U型关系：中等表达程度在智力、享受度、人形化、采用意向、信任和喜爱度方面都最优，显著超过两个极端。外向性和情绪稳定性是最关键的特质。

Conclusion: 人格表达程度和战略性特质匹配是对话机器人格设计的最优目标，为LLM基础对话机器的普及提供了设计启示。

Abstract: Large language models (LLMs) enable conversational agents (CAs) to express
distinctive personalities, raising new questions about how such designs shape
user perceptions. This study investigates how personality expression levels and
user-agent personality alignment influence perceptions in goal-oriented tasks.
In a between-subjects experiment (N=150), participants completed travel
planning with CAs exhibiting low, medium, or high expression across the Big
Five traits, controlled via our novel Trait Modulation Keys framework. Results
revealed an inverted-U relationship: medium expression produced the most
positive evaluations across Intelligence, Enjoyment, Anthropomorphism,
Intention to Adopt, Trust, and Likeability, significantly outperforming both
extremes. Personality alignment further enhanced outcomes, with Extraversion
and Emotional Stability emerging as the most influential traits. Cluster
analysis identified three distinct compatibility profiles, with "Well-Aligned"
users reporting substantially positive perceptions. These findings demonstrate
that personality expression and strategic trait alignment constitute optimal
design targets for CA personality, offering design implications as LLM-based
CAs become increasingly prevalent.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [59] [VStyle: A Benchmark for Voice Style Adaptation with Spoken Instructions](https://arxiv.org/abs/2509.09716)
*Jun Zhan,Mingyang Han,Yuxuan Xie,Chen Wang,Dong Zhang,Kexin Huang,Haoxiang Shi,DongXiao Wang,Tengtao Song,Qinyuan Cheng,Shimin Li,Jun Song,Xipeng Qiu,Bo Zheng*

Main category: cs.SD

TL;DR: 提出了语音风格适应(VSA)新任务，研究语音语言模型根据口语指令调整说话风格的能力，并发布了双语基准VStyle和评估框架LALM as a Judge


<details>
  <summary>Details</summary>
Motivation: 当前语音语言模型主要关注语义准确性和指令跟随，但根据口语指令调整说话风格（如音色、韵律、角色）的能力尚未得到充分研究

Method: 构建了VStyle双语基准数据集（中文和英文），涵盖四个语音生成类别：声学属性、自然语言指令、角色扮演和隐含共情；提出了LALM as a Judge评估框架，从文本忠实度、风格遵循度和自然度三个维度进行渐进式评估

Result: 实验表明当前商业系统和开源语音语言模型在可控风格适应方面存在明显局限性，验证了该任务的新颖性和挑战性

Conclusion: 通过发布VStyle数据集和评估工具包，为推进以人为中心的语音交互提供了基础，揭示了语音风格适应任务的重要研究价值

Abstract: Spoken language models (SLMs) have emerged as a unified paradigm for speech
understanding and generation, enabling natural human machine interaction.
However, while most progress has focused on semantic accuracy and instruction
following, the ability of SLMs to adapt their speaking style based on spoken
instructions has received limited attention. We introduce Voice Style
Adaptation (VSA), a new task that examines whether SLMs can modify their
speaking style, such as timbre, prosody, or persona following natural language
spoken commands. To study this task, we present VStyle, a bilingual (Chinese &
English) benchmark covering four categories of speech generation: acoustic
attributes, natural language instruction, role play, and implicit empathy. We
also introduce the Large Audio Language Model as a Judge (LALM as a Judge)
framework, which progressively evaluates outputs along textual faithfulness,
style adherence, and naturalness, ensuring reproducible and objective
assessment. Experiments on commercial systems and open source SLMs demonstrate
that current models face clear limitations in controllable style adaptation,
highlighting both the novelty and challenge of this task. By releasing VStyle
and its evaluation toolkit, we aim to provide the community with a foundation
for advancing human centered spoken interaction. The dataset and code are
publicly available at
\href{https://junzhan2000.github.io/VStyle.github.io/}{project's homepage}.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [60] [DB3 Team's Solution For Meta KDD Cup' 25](https://arxiv.org/abs/2509.09681)
*Yikuan Xia,Jiazun Chen,Yirui Zhan,Suifeng Zhao,Weipeng Jiang,Chaorui Zhang,Wei Han,Bo Bai,Jun Gao*

Main category: cs.IR

TL;DR: db3团队在KDD Cup'25 Meta CRAG-MM挑战赛中获胜的解决方案，通过多模态检索管道和LLM幻觉控制技术，在三个任务中取得优异成绩并获得总冠军


<details>
  <summary>Details</summary>
Motivation: 解决CRAG-MM挑战赛中多模态、多轮问答的独特需求，特别是处理第一人称视角的自我中心查询挑战

Method: 开发了包含领域特定检索管道（处理图像索引知识图谱、网络来源和多轮对话）和先进拒绝训练（使用SFT、DPO和RL技术）的统一LLM调优框架

Result: 在Task 1获得第2名，Task 2获得第2名，Task 3获得第1名，凭借对第一人称视角挑战的出色处理赢得总冠军

Conclusion: 该综合框架通过定制化检索管道和幻觉控制技术的结合，在多模态多轮问答任务中表现出色，特别是在处理自我中心查询方面具有显著优势

Abstract: This paper presents the db3 team's winning solution for the Meta CRAG-MM
Challenge 2025 at KDD Cup'25. Addressing the challenge's unique multi-modal,
multi-turn question answering benchmark (CRAG-MM), we developed a comprehensive
framework that integrates tailored retrieval pipelines for different tasks with
a unified LLM-tuning approach for hallucination control. Our solution features
(1) domain-specific retrieval pipelines handling image-indexed knowledge
graphs, web sources, and multi-turn conversations; and (2) advanced refusal
training using SFT, DPO, and RL. The system achieved 2nd place in Task 1, 2nd
place in Task 2, and 1st place in Task 3, securing the grand prize for
excellence in ego-centric queries through superior handling of first-person
perspective challenges.

</details>


### [61] [Text-to-SQL Oriented to the Process Mining Domain: A PT-EN Dataset for Query Translation](https://arxiv.org/abs/2509.09684)
*Bruno Yui Yamate,Thais Rodrigues Neubauer,Marcelo Fantinato,Sarajane Marques Peres*

Main category: cs.IR

TL;DR: text-2-SQL-4-PM是一个双语（葡萄牙语-英语）基准数据集，专门为流程挖掘领域的文本到SQL转换任务设计，包含1,655个自然语言语句和205个SQL语句。


<details>
  <summary>Details</summary>
Motivation: 促进自然语言查询数据库，提高非SQL专家用户的可访问性和专家用户的生产力，解决流程挖掘领域特有的词汇和数据结构挑战。

Method: 通过专家手动策划、专业翻译和详细标注过程创建数据集，并使用GPT-3.5 Turbo进行基线研究来验证数据集的可行性。

Result: 数据集支持文本到SQL实现的评估，展示了在流程挖掘领域的应用可行性，并为语义解析和其他自然语言处理任务提供更广泛的适用性。

Conclusion: text-2-SQL-4-PM数据集为流程挖掘领域的文本到SQL任务提供了有价值的基准资源，证明了其在专业领域自然语言处理应用中的实用性和扩展性。

Abstract: This paper introduces text-2-SQL-4-PM, a bilingual (Portuguese-English)
benchmark dataset designed for the text-to-SQL task in the process mining
domain. Text-to-SQL conversion facilitates natural language querying of
databases, increasing accessibility for users without SQL expertise and
productivity for those that are experts. The text-2-SQL-4-PM dataset is
customized to address the unique challenges of process mining, including
specialized vocabularies and single-table relational structures derived from
event logs. The dataset comprises 1,655 natural language utterances, including
human-generated paraphrases, 205 SQL statements, and ten qualifiers. Methods
include manual curation by experts, professional translations, and a detailed
annotation process to enable nuanced analyses of task complexity. Additionally,
a baseline study using GPT-3.5 Turbo demonstrates the feasibility and utility
of the dataset for text-to-SQL applications. The results show that
text-2-SQL-4-PM supports evaluation of text-to-SQL implementations, offering
broader applicability for semantic parsing and other natural language
processing tasks.

</details>


### [62] [AI-Powered Assistant for Long-Term Access to RHIC Knowledge](https://arxiv.org/abs/2509.09688)
*Mohammad Atif,Vincent Garonne,Eric Lancon,Jerome Lauret,Alexandr Prozorov,Michal Vranovsky*

Main category: cs.IR

TL;DR: RHIC数据保存计划开发AI助手系统，使用大语言模型和检索增强生成技术，提供自然语言访问文档、工作流和软件，支持数据重现性和未来发现


<details>
  <summary>Details</summary>
Motivation: 随着RHIC运行25年结束，保存其大量数据（约1EB）和嵌入的科学知识成为关键优先事项，需要确保数据的长期可访问性和可用性

Method: 基于大语言模型，采用检索增强生成和模型上下文协议，索引RHIC实验的结构化和非结构化内容，实现领域适应的交互

Result: 成功部署系统，报告了计算性能、多实验集成进展，以及为可持续和可解释长期AI访问设计的架构特性

Conclusion: 现代AI/ML工具可以显著提升科学遗产数据的可用性和可发现性，为大型科学实验的数据保存提供有效解决方案

Abstract: As the Relativistic Heavy Ion Collider (RHIC) at Brookhaven National
Laboratory concludes 25 years of operation, preserving not only its vast data
holdings ($\sim$1 ExaByte) but also the embedded scientific knowledge becomes a
critical priority. The RHIC Data and Analysis Preservation Plan (DAPP)
introduces an AI-powered assistant system that provides natural language access
to documentation, workflows, and software, with the aim of supporting
reproducibility, education, and future discovery. Built upon Large Language
Models using Retrieval-Augmented Generation and the Model Context Protocol,
this assistant indexes structured and unstructured content from RHIC
experiments and enables domain-adapted interaction. We report on the
deployment, computational performance, ongoing multi-experiment integration,
and architectural features designed for a sustainable and explainable long-term
AI access. Our experience illustrates how modern AI/ML tools can transform the
usability and discoverability of scientific legacy data.

</details>


### [63] [Personas within Parameters: Fine-Tuning Small Language Models with Low-Rank Adapters to Mimic User Behaviors](https://arxiv.org/abs/2509.09689)
*Himanshu Thakur,Eshani Agrawal,Smruthi Mukund*

Main category: cs.IR

TL;DR: 使用冻结大语言模型提取文本化用户表征，通过细调小语言模型和低秩适配器实现高效、可扩展的用户行为模拟


<details>
  <summary>Details</summary>
Motivation: 解决使用大语言模型模拟用户行为时的挑战：有效处理大规模表格数据、免除预训练导向的归纳偏差、并在百万用户尺度上实现前两者

Method: 采用冻结LLM提取稳健的文本化用户表征，使用细调的SLM构建成本效益高的用户代理，并通过为用户群组或人设训练多个低秩适配器

Result: 实验结果提供了有力的经验证据，证明该方法开发的用户代理有力量缩小推荐系统离线指标与实际性能之间的差距

Conclusion: 通过冻结LLM提取用户表征和细调SLM的组合方案，在保持可扩展性的同时实现了优异的性能平衡，为用户行为模拟提供了一种高效成本效益的方法

Abstract: A long-standing challenge in developing accurate recommendation models is
simulating user behavior, mainly due to the complex and stochastic nature of
user interactions. Towards this, one promising line of work has been the use of
Large Language Models (LLMs) for simulating user behavior. However, aligning
these general-purpose large pre-trained models with user preferences
necessitates: (i) effectively and continously parsing large-scale tabular
user-item interaction data, (ii) overcoming pre-training-induced inductive
biases to accurately learn user specific knowledge, and (iii) achieving the
former two at scale for millions of users. While most previous works have
focused on complex methods to prompt an LLM or fine-tune it on tabular
interaction datasets, our approach shifts the focus to extracting robust
textual user representations using a frozen LLM and simulating cost-effective,
resource-efficient user agents powered by fine-tuned Small Language Models
(SLMs). Further, we showcase a method for training multiple low-rank adapters
for groups of users or \textit{persona}, striking an optimal balance between
scalability and performance of user behavior agents. Our experiments provide
compelling empirical evidence of the efficacy of our methods, demonstrating
that user agents developed using our approach have the potential to bridge the
gap between offline metrics and real-world performance of recommender systems.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [64] [Improving MLLM Historical Record Extraction with Test-Time Image](https://arxiv.org/abs/2509.09722)
*Taylor Archibald,Tony Martinez*

Main category: cs.CV

TL;DR: 一种基于Gemini 2.0 Flash多重增广变体和Needleman Wunsch对齐算法的集成框架，能够提高噪声历史文档文本提取的准确性和稳定性


<details>
  <summary>Details</summary>
Motivation: 解决噪声历史文档中文本提取的不稳定性问题，提高自动化转换的准确性和可靠性

Method: 使用Gemini 2.0 Flash对每张图像生成多个增广变体进行转换，然后通过自定义Needleman Wunsch对齐器融合输出产生共识转写和信心度分数

Result: 在622份庞法尼亚死亡记录的新数据集上，该方法相比单次转换基准提高了4%的准确性，填充和模糊处理最有效

Conclusion: 该方法简单、可扩展，能够立即部署到其他文档集合和转换模型，为历史文档数字化提供了可靠的解决方案

Abstract: We present a novel ensemble framework that stabilizes LLM based text
extraction from noisy historical documents. We transcribe multiple augmented
variants of each image with Gemini 2.0 Flash and fuse these outputs with a
custom Needleman Wunsch style aligner that yields both a consensus
transcription and a confidence score. We present a new dataset of 622
Pennsylvania death records, and demonstrate our method improves transcription
accuracy by 4 percentage points relative to a single shot baseline. We find
that padding and blurring are the most useful for improving accuracy, while
grid warp perturbations are best for separating high and low confidence cases.
The approach is simple, scalable, and immediately deployable to other document
collections and transcription models.

</details>


### [65] [VARCO-VISION-2.0 Technical Report](https://arxiv.org/abs/2509.10105)
*Young-rok Cha,Jeongho Ju,SunYoung Park,Jong-Hyeon Lee,Younghyun Yu,Youngjune Kim*

Main category: cs.CV

TL;DR: VARCO-VISION-2.0是一个开源的韩英双语视觉语言模型，相比前代模型有显著提升，支持多图像理解、文档图表处理和布局感知OCR，在OpenCompass VLM排行榜上排名第8。


<details>
  <summary>Details</summary>
Motivation: 开发一个强大的双语视觉语言模型，支持韩语和英语，具备多图像理解能力和布局感知OCR功能，以推动双语VLM的发展和实践应用。

Method: 采用四阶段课程训练和内存高效技术，通过偏好优化提高安全性，同时保持核心语言能力并增强多模态对齐。

Result: 模型在空间定位和多语言基准测试中表现出色，14B版本在同类规模模型中排名第8，同时发布了1.7B轻量版本用于设备端部署。

Conclusion: VARCO-VISION-2.0显著提升了双语视觉语言模型的性能，为实际应用提供了完整的14B和轻量1.7B两个版本，推动了该领域的发展。

Abstract: We introduce VARCO-VISION-2.0, an open-weight bilingual vision-language model
(VLM) for Korean and English with improved capabilities compared to the
previous model VARCO-VISION-14B. The model supports multi-image understanding
for complex inputs such as documents, charts, and tables, and delivers
layoutaware OCR by predicting both textual content and its spatial location.
Trained with a four-stage curriculum with memory-efficient techniques, the
model achieves enhanced multimodal alignment, while preserving core language
abilities and improving safety via preference optimization. Extensive benchmark
evaluations demonstrate strong spatial grounding and competitive results for
both languages, with the 14B model achieving 8th place on the OpenCompass VLM
leaderboard among models of comparable scale. Alongside the 14B-scale model, we
release a 1.7B version optimized for on-device deployment. We believe these
models advance the development of bilingual VLMs and their practical
applications. Two variants of VARCO-VISION-2.0 are available at Hugging Face: a
full-scale 14B model and a lightweight 1.7B model.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [66] [Clip Your Sequences Fairly: Enforcing Length Fairness for Sequence-Level RL](https://arxiv.org/abs/2509.09177)
*Hanyi Mao,Quanjia Xiao,Lei Pang,Haixiao Liu*

Main category: cs.LG

TL;DR: FSPO是一种序列级强化学习方法，通过在重要性采样权重空间实施长度公平裁剪来解决PPO/GRPO方法在序列长度处理上的不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 现有序列级RL方法在移植PPO/GRPO式裁剪时存在固定裁剪范围对长短响应系统性重加权的问题，导致有效目标失真。

Method: 提出FSPO方法，在序列对数IS比率上应用高斯启发的裁剪带，包含KL校正漂移项并以√L比例缩放，确保长度公平性。

Result: FSPO在不同长度区间上平整了裁剪率，稳定了训练过程，并在多个评估数据集上优于所有基线方法。

Conclusion: FSPO通过理论形式化的长度公平性概念和实用的裁剪策略，有效解决了序列级RL中的长度偏差问题，提升了方法性能。

Abstract: We propose FSPO (Fair Sequence Policy Optimization), a sequence-level
reinforcement learning method for LLMs that enforces length-fair clipping
directly in the importance-sampling (IS) weight space. We revisit
sequence-level RL methods and identify a mismatch when PPO/GRPO-style clipping
is transplanted to sequences: a fixed clip range systematically reweights short
vs. long responses, distorting the effective objective. Theoretically, we
formalize length fairness via a Length Reweighting Error (LRE) and prove that
small LRE yields a directional cosine guarantee between the clipped and true
updates. FSPO introduces a simple, Gaussian-motivated remedy: we clip the
sequence log-IS ratio with a band that applies a KL-corrected drift term and
scales as $\sqrt{L}$. Empirically, FSPO flattens clip rates across length bins,
stabilizes training, and outperforms all baselines across multiple evaluation
datasets.

</details>


### [67] [Latency and Token-Aware Test-Time Compute](https://arxiv.org/abs/2509.09864)
*Jenny Y. Huang,Mehul Damani,Yousef El-Kurdi,Ramon Astudillo,Wei Sun*

Main category: cs.LG

TL;DR: 本文提出了一个动态计算分配框架，在推理时根据查询需求选择最佳生成策略（并行或增量解码），同时考虑token成本和延迟时间，在保持部署实用性的同时实现更好的准确率-成本权衡。


<details>
  <summary>Details</summary>
Motivation: 现有推理时扩展方法主要关注并行生成（如best-of-N），忽略了增量解码方法（如beam search），且大多只关注token使用量而忽视了延迟时间，这对于用户体验和智能体工作流至关重要。

Method: 将推理时扩展建模为动态计算分配和方法选择问题，系统根据每个查询决定应用哪种策略以及分配多少计算资源，明确同时考虑token成本和延迟时间。

Result: 在推理基准测试中，该方法 consistently 优于静态策略，实现了有利的准确率-成本权衡，同时保持部署实用性。

Conclusion: 动态计算分配框架能够有效提升LLM性能，通过智能选择生成策略和计算资源分配，在考虑延迟约束的情况下实现更好的性能-成本平衡。

Abstract: Inference-time scaling has emerged as a powerful way to improve large
language model (LLM) performance by generating multiple candidate responses and
selecting among them. However, existing work on dynamic allocation for
test-time compute typically considers only parallel generation methods such as
best-of-N, overlooking incremental decoding methods like beam search, and has
largely ignored latency, focusing only on token usage. We formulate
inference-time scaling as a problem of dynamic compute allocation and method
selection, where the system must decide which strategy to apply and how much
compute to allocate on a per-query basis. Our framework explicitly incorporates
both token cost and wall-clock latency, the latter being critical for user
experience and particularly for agentic workflows where models must issue
multiple queries efficiently. Experiments on reasoning benchmarks show that our
approach consistently outperforms static strategies, achieving favorable
accuracy-cost trade-offs while remaining practical for deployment.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [68] [Whisper Has an Internal Word Aligner](https://arxiv.org/abs/2509.09987)
*Sung-Lin Yeh,Yen Meng,Hao Tang*

Main category: eess.AS

TL;DR: 通过分析Whisper模型的注意力头，发现某些头能够捐准捕捉单词对齐信息，提出一种无监督的字符级对齐提取方法，在严格的对齐容差范围内超过了现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的词级时间戳提取方法或需额外训练或性能不佳，而且评估标准较为松慢（通常大于200ms容差），需要更准确的方法来满足实际应用需求。

Method: 分析Whisper模型的注意力头，识别出能够捐准捕捉单词对齐的特定头；发现使用字符比使用wordpieces能产生更精细和准确的对齐；提出一种无监督的方法，通过过滤注意力头并使用字符进行teacher forcing来提取单词对齐。

Result: 该方法不需要训练，且在20ms到100ms这个更严格的容差范围内，产生的单词对齐比之前的工作更加准确。

Conclusion: 通过深入分析Whisper模型的内部注意力机制，可以开发出高效无监督的单词对齐提取方法，在严格的时间容差要求下仍能保持高准确性，为语音识别中的精确对齐需求提供了有效解决方案。

Abstract: There is an increasing interest in obtaining accurate word-level timestamps
from strong automatic speech recognizers, in particular Whisper. Existing
approaches either require additional training or are simply not competitive.
The evaluation in prior work is also relatively loose, typically using a
tolerance of more than 200 ms. In this work, we discover attention heads in
Whisper that capture accurate word alignments and are distinctively different
from those that do not. Moreover, we find that using characters produces finer
and more accurate alignments than using wordpieces. Based on these findings, we
propose an unsupervised approach to extracting word alignments by filtering
attention heads while teacher forcing Whisper with characters. Our approach not
only does not require training but also produces word alignments that are more
accurate than prior work under a stricter tolerance between 20 ms and 100 ms.

</details>


### [69] [Unified Learnable 2D Convolutional Feature Extraction for ASR](https://arxiv.org/abs/2509.10031)
*Peter Vieting,Benedikt Hilmes,Ralf Schlüter,Hermann Ney*

Main category: eess.AS

TL;DR: 这篇论文提出了一种通用的2D卷积神经网络前端结构，用于语音识别中的特征提取，其性能可与现有监督学习特征提取器相当，但更节省参数和计算资源。


<details>
  <summary>Details</summary>
Motivation: 当前的神经前端技术仍大量受到经典方法的影响，虽然这种归纳偏见有助于系统设计，但研究者希望开发更通用的特征提取前端，并统一前端架构。

Method: 通过系统实验减少现有技术的影响，开发了一种统一的2D卷积前端结构。该方法避免了现有方法中使用来自不同源头的多种层拓扑结构的组合。

Result: 实验结果显示，这种通用统一的方法不仅可行，而且在性能上可以匹配现有的监督学习特征提取器。同时，该方法具有参数效率高的优点，适合计算资源有限的场景，不像在未标注音频上预训练的大型模型那样需要大量计算资源。

Conclusion: 论文成功开发了一种通用且统一的2D卷积神经网络前端结构，证明了这种方法在语音识别特征提取中的可行性和有效性，为计算资源有限的应用场景提供了一种高效的解决方案。

Abstract: Neural front-ends represent a promising approach to feature extraction for
automatic speech recognition (ASR) systems as they enable to learn specifically
tailored features for different tasks. Yet, many of the existing techniques
remain heavily influenced by classical methods. While this inductive bias may
ease the system design, our work aims to develop a more generic front-end for
feature extraction. Furthermore, we seek to unify the front-end architecture
contrasting with existing approaches that apply a composition of several layer
topologies originating from different sources. The experiments systematically
show how to reduce the influence of existing techniques to achieve a generic
front-end. The resulting 2D convolutional front-end is parameter-efficient and
suitable for a scenario with limited computational resources unlike large
models pre-trained on unlabeled audio. The results demonstrate that this
generic unified approach is not only feasible but also matches the performance
of existing supervised learnable feature extractors.

</details>


### [70] [Error Analysis in a Modular Meeting Transcription System](https://arxiv.org/abs/2509.10143)
*Peter Vieting,Simon Berger,Thilo von Neumann,Christoph Boeddeker,Ralf Schlüter,Reinhold Haeb-Umbach*

Main category: eess.AS

TL;DR: 本文通过扩展漏泄分析框架研究了语音分离中的模型漏泄问题，发现主讲者活动区域存在显著的跨通道漏泄，但语音活动检测能够忽略这些漏泄部分从而不影响最终性能。高级分辨技术能大幅缩小与oracle分割的性能差距。


<details>
  <summary>Details</summary>
Motivation: 虽然会议转写技术在近年取得了显著进步，但仍面临性能限制。本文主要动机是深入分析语音分离中的漏泄问题，尤其是与时间局部性效应相关的漏泄问题，以明确这些漏泄对系统性能的真实影响。

Method: 扩展了之前提出的语音分离漏泄分析框架，加入了对时间局部性的敏感性考虑。通过对比不同分割方法（包括基于能量的VAD和高级分辨技术）来分析漏泄问题和性能差异。在LibriCSS数据集上进行实验评估。

Result: 发现在主讲者活动区域存在显著的跨通道漏泄现象，但语音活动检测能够有效忽略这些漏泄部分，因此对最终性能影响较小。高级分辨技术能够将与oracle分割的性能差距缩小三分之一，达到了仅使用LibriSpeech数据训练识别模块的系统在LibriCSS上的最高性能水平。

Conclusion: 语音分离模型在主讲者活动区域存在显著的漏泄问题，但语音活动检测能够有效缓解这些问题。高级分辨技术在提升会议转写性能方面具有重要价值，能够大幅缩小与理想分割方案的性能差距。

Abstract: Meeting transcription is a field of high relevance and remarkable progress in
recent years. Still, challenges remain that limit its performance. In this
work, we extend a previously proposed framework for analyzing leakage in speech
separation with proper sensitivity to temporal locality. We show that there is
significant leakage to the cross channel in areas where only the primary
speaker is active. At the same time, the results demonstrate that this does not
affect the final performance much as these leaked parts are largely ignored by
the voice activity detection (VAD). Furthermore, different segmentations are
compared showing that advanced diarization approaches are able to reduce the
gap to oracle segmentation by a third compared to a simple energy-based VAD. We
additionally reveal what factors contribute to the remaining difference. The
results represent state-of-the-art performance on LibriCSS among systems that
train the recognition module on LibriSpeech data only.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [71] [Differential Robustness in Transformer Language Models: Empirical Evaluation Under Adversarial Text Attacks](https://arxiv.org/abs/2509.09706)
*Taniya Gidatkar,Oluwaseun Ajao,Matthew Shardlow*

Main category: cs.CR

TL;DR: 本研究评估了Flan-T5、BERT和RoBERTa-Base等大型语言模型对抗对抗性攻击的韧性，发现RoBERTa-Base和FlanT5表现出色（攻击成功率0%），而BERT-Base存在明显脆弱性（TextFooler攻击成功率93.75%）。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在对抗性攻击下的安全性和韧性，识别现有防护机制的优缺点，为开发更有效的防御策略提供依据。

Method: 使用TextFooler和BERTAttack系统设计对抗性测试，对比分析Flan-T5、BERT和RoBERTa-Base三种模型的抗攻击能力。

Result: RoBERTa-Base和FlanT5表现出卓越韧性，攻击成功率为0%；BERT-Base极其脆弱，TextFooler攻击使其准确率从48%降至3%，成功率93.75%。

Conclusion: 某些LLMs已具备有效防御机制但计算成本高，研究揭示了当前防护方法的优缺点，并提出了开发更高效防御策略的实用建议。

Abstract: This study evaluates the resilience of large language models (LLMs) against
adversarial attacks, specifically focusing on Flan-T5, BERT, and RoBERTa-Base.
Using systematically designed adversarial tests through TextFooler and
BERTAttack, we found significant variations in model robustness. RoBERTa-Base
and FlanT5 demonstrated remarkable resilience, maintaining accuracy even when
subjected to sophisticated attacks, with attack success rates of 0%. In
contrast. BERT-Base showed considerable vulnerability, with TextFooler
achieving a 93.75% success rate in reducing model accuracy from 48% to just 3%.
Our research reveals that while certain LLMs have developed effective defensive
mechanisms, these safeguards often require substantial computational resources.
This study contributes to the understanding of LLM security by identifying
existing strengths and weaknesses in current safeguarding approaches and
proposes practical recommendations for developing more efficient and effective
defensive strategies.

</details>
