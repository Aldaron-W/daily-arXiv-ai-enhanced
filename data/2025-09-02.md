<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 34]
- [cs.LG](#cs.LG) [Total: 4]
- [physics.ed-ph](#physics.ed-ph) [Total: 1]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.HC](#cs.HC) [Total: 2]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.CV](#cs.CV) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [CoBA: Counterbias Text Augmentation for Mitigating Various Spurious Correlations via Semantic Triples](https://arxiv.org/abs/2508.21083)
*Kyohoon Jin,Juhwan Choi,Jungmin Yun,Junho Lee,Soojin Jang,Youngbin Kim*

Main category: cs.CL

TL;DR: 提出了CoBA框架，通过语义三元组分解和修改来生成反偏见数据，解决深度学习模型中的伪相关性问题


<details>
  <summary>Details</summary>
Motivation: 深度学习模型经常学习和利用训练数据中的伪相关性，使用非目标特征进行预测，导致性能下降和泛化能力差

Method: CoBA框架在语义三元组层面操作：首先将文本分解为主谓宾三元组，然后选择性地修改这些三元组以破坏伪相关性，通过重构调整后的三元组生成反偏见数据

Result: 实验表明CoBA不仅提高了下游任务性能，还能有效减少偏见并增强分布外鲁棒性

Conclusion: CoBA提供了一个通用且鲁棒的解决方案，能够同时处理多种偏见并增强模型对伪相关性挑战的抵抗力

Abstract: Deep learning models often learn and exploit spurious correlations in
training data, using these non-target features to inform their predictions.
Such reliance leads to performance degradation and poor generalization on
unseen data. To address these limitations, we introduce a more general form of
counterfactual data augmentation, termed counterbias data augmentation, which
simultaneously tackles multiple biases (e.g., gender bias, simplicity bias) and
enhances out-of-distribution robustness. We present CoBA: CounterBias
Augmentation, a unified framework that operates at the semantic triple level:
first decomposing text into subject-predicate-object triples, then selectively
modifying these triples to disrupt spurious correlations. By reconstructing the
text from these adjusted triples, CoBA generates counterbias data that
mitigates spurious patterns. Through extensive experiments, we demonstrate that
CoBA not only improves downstream task performance, but also effectively
reduces biases and strengthens out-of-distribution resilience, offering a
versatile and robust solution to the challenges posed by spurious correlations.

</details>


### [2] [Mapping Toxic Comments Across Demographics: A Dataset from German Public Broadcasting](https://arxiv.org/abs/2508.21084)
*Jan Fillies,Michael Peter Hoffmann,Rebecca Reichel,Roman Salzwedel,Sven Bodemer,Adrian Paschke*

Main category: cs.CL

TL;DR: 首个大规模德语毒性言论数据集，包含平台提供的年龄估计，揭示不同年龄段用户的毒性言论模式差异


<details>
  <summary>Details</summary>
Motivation: 现有毒性言论数据集缺乏人口统计背景信息，限制了我们对不同年龄群体在线交流方式的理解

Method: 与德国公共服务内容网络funk合作，收集Instagram、TikTok和YouTube的匿名评论，使用预定义毒性关键词筛选，结合人工标注和LLM标注（3,024条人工标注+30,024条LLM标注），识别侮辱、虚假信息、广播费批评等关键类别

Result: 16.7%的评论被标记为有问题，发现年龄相关的毒性言论模式差异：年轻用户偏好表达性语言，年长用户更常参与虚假信息和贬低行为

Conclusion: 该数据集为研究跨人口统计的语言变异提供了新机会，支持开发更公平和年龄感知的内容审核系统

Abstract: A lack of demographic context in existing toxic speech datasets limits our
understanding of how different age groups communicate online. In collaboration
with funk, a German public service content network, this research introduces
the first large-scale German dataset annotated for toxicity and enriched with
platform-provided age estimates. The dataset includes 3,024 human-annotated and
30,024 LLM-annotated anonymized comments from Instagram, TikTok, and YouTube.
To ensure relevance, comments were consolidated using predefined toxic
keywords, resulting in 16.7\% labeled as problematic. The annotation pipeline
combined human expertise with state-of-the-art language models, identifying key
categories such as insults, disinformation, and criticism of broadcasting fees.
The dataset reveals age-based differences in toxic speech patterns, with
younger users favoring expressive language and older users more often engaging
in disinformation and devaluation. This resource provides new opportunities for
studying linguistic variation across demographics and supports the development
of more equitable and age-aware content moderation systems.

</details>


### [3] [Granite Embedding R2 Models](https://arxiv.org/abs/2508.21085)
*Parul Awasthy,Aashka Trivedi,Yulong Li,Meet Doshi,Riyaz Bhat,Vignesh P,Vishwajeet Kumar,Yushu Yang,Bhavani Iyer,Abraham Daniels,Rudra Murthy,Ken Barker,Martin Franz,Madison Lee,Todd Ward,Salim Roukos,David Cox,Luis Lastras,Jaydeep Sen,Radu Florian*

Main category: cs.CL

TL;DR: Granite Embedding R2模型是IBM开发的第二代高性能英文编码器嵌入模型，支持8192 tokens上下文，在检索速度上比竞争对手快19-44%，同时在多个领域保持领先准确率。


<details>
  <summary>Details</summary>
Motivation: 企业级密集检索应用需要高性能的嵌入模型，第一代模型在上下文长度、多领域性能和检索速度方面仍有提升空间。

Method: 采用双编码器和交叉编码器架构，包括22层检索模型和高效的12层对应版本，以及高质量的重排序模型，全部使用企业级数据进行训练。

Result: 在标准基准测试、IBM开发评估套件和实际企业用例中表现出卓越的通用性，为开源嵌入模型建立了新的性能标准。

Conclusion: Granite R2模型提供了前沿性能、企业级许可和透明数据溯源的最佳组合，适用于关键任务部署，所有模型都在Apache 2.0许可下公开可用。

Abstract: We introduce the Granite Embedding R2 models, a comprehensive family of
high-performance English encoder-based embedding models engineered for
enterprise-scale dense retrieval applications. Building upon our
first-generation release, these models deliver substantial improvements,
including 16x expanded context length (8,192 tokens), state-of-the-art
performance across diverse retrieval domains - text, code, long-document
search, multi-turn conversational, and tabular data - and measurable speed
advantages of 19-44\% over leading competitors while maintaining superior
accuracy. Our release encompasses both bi-encoder and cross-encoder
architectures, featuring a highly effective 22-layer retriever model and its
efficient 12-layer counterpart, alongside a high-quality reranker model, all
trained exclusively on enterprise-appropriate data with comprehensive
governance oversight. The models demonstrate exceptional versatility across
standard benchmarks, IBM-developed evaluation suites, and real-world enterprise
use cases, establishing new performance standards for open-source embedding
models. In an era where retrieval speed and accuracy are paramount for
competitive advantage, the Granite R2 models deliver a compelling combination
of cutting-edge performance, enterprise-ready licensing, and transparent data
provenance that organizations require for mission-critical deployments. All
models are publicly available under the Apache 2.0 license at
https://huggingface.co/collections/ibm-granite, enabling unrestricted research
and commercial use.

</details>


### [4] [TrInk: Ink Generation with Transformer Network](https://arxiv.org/abs/2508.21098)
*Zezhong Jin,Shubhang Desai,Xu Chen,Biyi Fang,Zhuoyi Huang,Zhe Li,Chong-Xin Gan,Xiao Tu,Man-Wai Mak,Yan Lu,Shujie Liu*

Main category: cs.CL

TL;DR: TrInk是一个基于Transformer的笔迹生成模型，通过引入缩放位置编码和高斯记忆掩码，在IAM-OnDB数据集上相比先前方法实现了字符错误率降低35.56%和词错误率降低29.66%的显著改进。


<details>
  <summary>Details</summary>
Motivation: 为了解决笔迹生成中全局依赖关系捕获不足以及输入文本与生成笔画点对齐困难的问题，需要开发更有效的模型来提升生成手写体的可读性和风格一致性。

Method: 提出基于Transformer的TrInk模型，在交叉注意力模块中引入缩放位置嵌入和高斯记忆掩码来改善文本与笔画点的对齐，同时设计了主客观评估流程来全面评估生成效果。

Result: 在IAM-OnDB数据集上的实验表明，相比先前方法，TrInk实现了字符错误率(CER)降低35.56%和词错误率(WER)降低29.66%的显著性能提升。

Conclusion: TrInk模型通过创新的注意力机制设计有效提升了笔迹生成的质量和准确性，为手写体生成任务提供了新的解决方案。

Abstract: In this paper, we propose TrInk, a Transformer-based model for ink
generation, which effectively captures global dependencies. To better
facilitate the alignment between the input text and generated stroke points, we
introduce scaled positional embeddings and a Gaussian memory mask in the
cross-attention module. Additionally, we design both subjective and objective
evaluation pipelines to comprehensively assess the legibility and style
consistency of the generated handwriting. Experiments demonstrate that our
Transformer-based model achieves a 35.56\% reduction in character error rate
(CER) and an 29.66% reduction in word error rate (WER) on the IAM-OnDB dataset
compared to previous methods. We provide an demo page with handwriting samples
from TrInk and baseline models at: https://akahello-a11y.github.io/trink-demo/

</details>


### [5] [How Does Cognitive Bias Affect Large Language Models? A Case Study on the Anchoring Effect in Price Negotiation Simulations](https://arxiv.org/abs/2508.21137)
*Yoshiki Takenami,Yin Jou Huang,Yugo Murawaki,Chenhui Chu*

Main category: cs.CL

TL;DR: 研究发现LLM在价格谈判中存在锚定效应，推理模型能减轻这种效应，但人格特质与锚定效应敏感性无显著相关


<details>
  <summary>Details</summary>
Motivation: 研究LLM在现实应用中的认知偏差，特别是价格谈判中的锚定效应，以提高LLM的可靠性和安全性

Method: 使用卖家LLM代理应用锚定效应，通过客观和主观指标评估谈判效果，并分析推理能力和人格特质的影响

Result: LLM像人类一样受锚定效应影响，推理模型较不易受影响（长思维链减轻效应），人格特质与锚定效应敏感性无显著相关性

Conclusion: 这些发现有助于深入理解LLM的认知偏差，推动LLM在社会中安全负责任的应用

Abstract: Cognitive biases, well-studied in humans, can also be observed in LLMs,
affecting their reliability in real-world applications. This paper investigates
the anchoring effect in LLM-driven price negotiations. To this end, we
instructed seller LLM agents to apply the anchoring effect and evaluated
negotiations using not only an objective metric but also a subjective metric.
Experimental results show that LLMs are influenced by the anchoring effect like
humans. Additionally, we investigated the relationship between the anchoring
effect and factors such as reasoning and personality. It was shown that
reasoning models are less prone to the anchoring effect, suggesting that the
long chain of thought mitigates the effect. However, we found no significant
correlation between personality traits and susceptibility to the anchoring
effect. These findings contribute to a deeper understanding of cognitive biases
in LLMs and to the realization of safe and responsible application of LLMs in
society.

</details>


### [6] [Can Multimodal LLMs Solve the Basic Perception Problems of Percept-V?](https://arxiv.org/abs/2508.21143)
*Samrajnee Ghosh,Naman Agarwal,Hemanshu Garg,Chinmay Mittal,Mausam,Parag Singla*

Main category: cs.CL

TL;DR: Percept-V数据集评估多模态大语言模型在基本视觉感知任务上的表现，发现随着任务复杂度增加，模型性能显著下降


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注MLLMs在复杂任务（如编码、数学、科学）上的推理能力，但缺乏对基本视觉感知能力的系统评估

Method: 创建包含7200张程序生成图像的Percept-V数据集，分为30个类别测试不同视觉感知技能，并在GPT-4o、Gemini、Claude等先进模型上进行测试

Result: 实验显示MLLMs在基本感知任务上表现不佳，性能随复杂度增加而显著下降，不同模型在各类别中表现出相似的趋势模式

Conclusion: MLLMs在复杂任务上的优异表现与其在基本视觉感知任务上的薄弱形成对比，揭示了模型认知能力的局限性

Abstract: The reasoning abilities of Multimodal Large Language Models (MLLMs) have
garnered a lot of attention in recent times, with advances made in frontiers
like coding, mathematics, and science. However, very limited experiments have
been done to assess their performance in simple perception tasks performed over
uncontaminated, generated images containing basic shapes and structures. To
address this issue, the paper introduces a dataset, Percept-V, containing a
total of 7200 program-generated images equally divided into 30 categories, each
testing a combination of visual perception skills. Unlike previously proposed
datasets, Percept-V comprises very basic tasks of varying complexity that test
the perception abilities of MLLMs. This dataset is then tested on
state-of-the-art MLLMs like GPT-4o, Gemini, and Claude as well as Large
Reasoning Models (LRMs) like OpenAI o4-mini and DeepSeek R1 to gauge their
performance. Contrary to the evidence that MLLMs excel in many complex tasks,
our experiments show a significant drop in the models' performance with
increasing problem complexity across all categories. An analysis of the
performances also reveals that the tested MLLMs exhibit a similar trend in
accuracy across categories, testing a particular cognitive skill and find some
skills to be more difficult than others.

</details>


### [7] [A Survey of Scientific Large Language Models: From Data Foundations to Agent Frontiers](https://arxiv.org/abs/2508.21148)
*Ming Hu,Chenglong Ma,Wei Li,Wanghan Xu,Jiamin Wu,Jucheng Hu,Tianbin Li,Guohang Zhuang,Jiaqi Liu,Yingzhou Lu,Ying Chen,Chaoyang Zhang,Cheng Tan,Jie Ying,Guocheng Wu,Shujian Gao,Pengcheng Chen,Jiashi Lin,Haitao Wu,Lulu Chen,Fengxiang Wang,Yuanyuan Zhang,Xiangyu Zhao,Feilong Tang,Encheng Su,Junzhi Ning,Xinyao Liu,Ye Du,Changkai Ji,Cheng Tang,Huihui Xu,Ziyang Chen,Ziyan Huang,Jiyao Liu,Pengfei Jiang,Yizhou Wang,Chen Tang,Jianyu Wu,Yuchen Ren,Siyuan Yan,Zhonghua Wang,Zhongxing Xu,Shiyan Su,Shangquan Sun,Runkai Zhao,Zhisheng Zhang,Yu Liu,Fudi Wang,Yuanfeng Ji,Yanzhou Su,Hongming Shan,Chunmei Feng,Jiahao Xu,Jiangtao Yan,Wenhao Tang,Diping Song,Lihao Liu,Yanyan Huang,Lequan Yu,Bin Fu,Shujun Wang,Xiaomeng Li,Xiaowei Hu,Yun Gu,Ben Fei,Zhongying Deng,Benyou Wang,Yuewen Cao,Minjie Shen,Haodong Duan,Jie Xu,Yirong Chen,Fang Yan,Hongxia Hao,Jielan Li,Jiajun Du,Yanbo Wang,Imran Razzak,Chi Zhang,Lijun Wu,Conghui He,Zhaohui Lu,Jinhai Huang,Yihao Liu,Fenghua Ling,Yuqiang Li,Aoran Wang,Qihao Zheng,Nanqing Dong,Tianfan Fu,Dongzhan Zhou,Yan Lu,Wenlong Zhang,Jin Ye,Jianfei Cai,Wanli Ouyang,Yu Qiao,Zongyuan Ge,Shixiang Tang,Junjun He,Chunfeng Song,Lei Bai,Bowen Zhou*

Main category: cs.CL

TL;DR: 科学大语言模型(Sci-LLMs)的数据中心综述，重构模型与数据基础的共同进化关系，分析了过270+训练数据集和190+评估数据集，并提出向闭环自主科学发现系统的范式转移


<details>
  <summary>Details</summary>
Motivation: 科学数据的复杂性（多模态、跨尺度、领域特定性）使得Sci-LLMs的发展面临独特挑战，需要从数据角度进行系统性的理论分析和实践指导

Method: 构建统一的科学数据分类法和科学知识层次模型，系统评估最新Sci-LLMs模型（从通用基础模型到专业领域模型），并对过270+训练数据集和190+评估数据集进行深入分析

Result: 识别了Sci-LLMs的独特需求：处理异质、多尺度、匀匀性数据，需要保持领域不变性和跨模态推理能力；识别了评估方式从静态考试向过程和发现导向评估的转变

Conclusion: 本研究提供了建设可信赖、持续进化AI系统的路线图，预览了向闭环自主科学发现系统的范式转移，这些系统基于Sci-LLMs能够主动实验、验证并为生命进化的知识库做出贡献

Abstract: Scientific Large Language Models (Sci-LLMs) are transforming how knowledge is
represented, integrated, and applied in scientific research, yet their progress
is shaped by the complex nature of scientific data. This survey presents a
comprehensive, data-centric synthesis that reframes the development of Sci-LLMs
as a co-evolution between models and their underlying data substrate. We
formulate a unified taxonomy of scientific data and a hierarchical model of
scientific knowledge, emphasizing the multimodal, cross-scale, and
domain-specific challenges that differentiate scientific corpora from general
natural language processing datasets. We systematically review recent Sci-LLMs,
from general-purpose foundations to specialized models across diverse
scientific disciplines, alongside an extensive analysis of over 270
pre-/post-training datasets, showing why Sci-LLMs pose distinct demands --
heterogeneous, multi-scale, uncertainty-laden corpora that require
representations preserving domain invariance and enabling cross-modal
reasoning. On evaluation, we examine over 190 benchmark datasets and trace a
shift from static exams toward process- and discovery-oriented assessments with
advanced evaluation protocols. These data-centric analyses highlight persistent
issues in scientific data development and discuss emerging solutions involving
semi-automated annotation pipelines and expert validation. Finally, we outline
a paradigm shift toward closed-loop systems where autonomous agents based on
Sci-LLMs actively experiment, validate, and contribute to a living, evolving
knowledge base. Collectively, this work provides a roadmap for building
trustworthy, continually evolving artificial intelligence (AI) systems that
function as a true partner in accelerating scientific discovery.

</details>


### [8] [Quantifying Label-Induced Bias in Large Language Model Self- and Cross-Evaluations](https://arxiv.org/abs/2508.21164)
*Muskan Saraf,Sajjad Rezvani Boroujeni,Justin Beaudry,Hossein Abedi,Tom Bush*

Main category: cs.CL

TL;DR: 研究发现大型语言模型在评估时存在严重标签偏见，Claude标签会显著提升评分，Gemini标签则会压低评分，错误标签可导致排名完全逆转，最高造成50个百分点的偏好投票差异。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型越来越多地被用于评估输出，需要研究其判断是否会受到模型标签的影响，以确保评估的公平性和准确性。

Method: 使用ChatGPT、Gemini和Claude三种模型，在无标签、真实标签和两种错误标签条件下，通过整体偏好投票和三个质量维度（连贯性、信息性、简洁性）的百分比评分进行自评估和交叉评估。

Result: 发现明显的非对称偏见：Claude标签始终提升分数，Gemini标签始终压低分数；错误标签经常逆转排名，偏好投票差异达50个百分点，质量评分差异达12个百分点；Gemini在真实标签下自评分崩溃，Claude自偏好加剧。

Conclusion: 感知到的模型身份会严重扭曲高级判断并微妙影响详细质量评分，强调需要盲评或多模型评估协议来确保LLM基准测试的公平性。

Abstract: Large language models (LLMs) are increasingly used to evaluate outputs, yet
their judgments may be influenced. This study examines bias in self- and
cross-model evaluations by ChatGPT, Gemini, and Claude under four conditions:
no labels, true labels, and two false-label scenarios. Blog posts authored by
each model were evaluated by all three using both overall preference voting and
quality ratings for Coherence, Informativeness, and Conciseness, with all
scores expressed as percentages for direct comparison. Results reveal striking
asymmetries: the "Claude" label consistently boosts scores, while the "Gemini"
label consistently depresses them, regardless of actual content. False labels
frequently reversed rankings, producing shifts of up to 50 percentage points in
preference votes and up to 12 percentage points in converted quality ratings.
Gemini's self-scores collapsed under true labels, while Claude's
self-preference intensified. These findings show that perceived model identity
can heavily distort high-level judgments and subtly influence detailed quality
ratings, underscoring the need for blind or multimodel evaluation protocols to
ensure fairness in LLM benchmarking.

</details>


### [9] [BED-LLM: Intelligent Information Gathering with LLMs and Bayesian Experimental Design](https://arxiv.org/abs/2508.21184)
*Deepro Choudhury,Sinead Williamson,Adam Goliński,Ning Miao,Freddie Bickford Smith,Michael Kirchhof,Yizhe Zhang,Tom Rainforth*

Main category: cs.CL

TL;DR: 提出了BED-LLM方法，通过贝叶斯实验设计框架让大语言模型能够智能地多轮交互获取信息，在20问游戏和用户偏好推断任务中表现显著优于直接提示和其他自适应策略


<details>
  <summary>Details</summary>
Motivation: 提升大语言模型作为多轮对话代理的能力，使其能够智能地、自适应地从用户或外部源收集信息

Method: 基于顺序贝叶斯实验设计框架，迭代选择最大化期望信息增益的问题，使用从LLM信念分布导出的概率模型，并设计了专门的EIG估计器、上下文更新策略和候选查询生成策略

Result: 在20问游戏和用户偏好推断任务中取得了显著的性能提升，相比直接提示和其他自适应设计策略表现更好

Conclusion: BED-LLM为LLMs提供了一个通用的信息收集框架，能够有效提升多轮对话中的智能信息获取能力

Abstract: We propose a general-purpose approach for improving the ability of Large
Language Models (LLMs) to intelligently and adaptively gather information from
a user or other external source using the framework of sequential Bayesian
experimental design (BED). This enables LLMs to act as effective multi-turn
conversational agents and interactively interface with external environments.
Our approach, which we call BED-LLM (Bayesian Experimental Design with Large
Language Models), is based on iteratively choosing questions or queries that
maximize the expected information gain (EIG) about the task of interest given
the responses gathered previously. We show how this EIG can be formulated in a
principled way using a probabilistic model derived from the LLM's belief
distribution and provide detailed insights into key decisions in its
construction. Further key to the success of BED-LLM are a number of specific
innovations, such as a carefully designed estimator for the EIG, not solely
relying on in-context updates for conditioning on previous responses, and a
targeted strategy for proposing candidate queries. We find that BED-LLM
achieves substantial gains in performance across a wide range of tests based on
the 20-questions game and using the LLM to actively infer user preferences,
compared to direct prompting of the LLM and other adaptive design strategies.

</details>


### [10] [Improving Aviation Safety Analysis: Automated HFACS Classification Using Reinforcement Learning with Group Relative Policy Optimization](https://arxiv.org/abs/2508.21201)
*Arash Ahmadi,Sarah Sharif,Yaser Banad*

Main category: cs.CL

TL;DR: 使用强化学习和GRPO算法自动化分析航空事故中的人因素，小型专业化模型表现超过大型通用LLM


<details>
  <summary>Details</summary>
Motivation: 解决传统HFACS分析方法的可扩展性和一致性问题，提高航空安全分析效率

Method: 使用GRPO强化学习策略微调Llama-3.1 8B模型，结合多组件奖励系统和合成数据生成解决类别不平衡

Result: 精确匹配准确率提高350%（从0.04到0.18），部分匹配准确率0.88，超过GPT-5-mini和Gemini-2.5等独江模型

Conclusion: 小型领域优化模型能够提供计算效率更高的安全分析方案，适合资源受限边缘设备部署

Abstract: Analyzing the human factors behind aviation accidents is crucial for
preventing future incidents, yet traditional methods using the Human Factors
Analysis and Classification System (HFACS) are limited by scalability and
consistency. To address this, we introduce an automated HFACS classification
framework for aviation safety analysis that utilizes Reinforcement Learning
with Group Relative Policy Optimization (GRPO) to fine-tune a Llama-3.1 8B
language model. Our approach incorporates a multi-component reward system
tailored for aviation safety analysis and integrates synthetic data generation
to overcome class imbalance in accident datasets. The resulting GRPO-optimized
model achieved noticeable performance gains, including a 350% increase in exact
match accuracy (from 0.0400 to 0.1800) and an improved partial match accuracy
of 0.8800. Significantly, our specialized model outperforms state-of-the-art
LLMs (Large Language Models), including GPT-5-mini and Gemini-2.5-fiash, on key
metrics. This research also proposes exact match accuracy in multi-label HFACS
classification problem as a new benchmarking methodology to evaluate the
advanced reasoning capabilities of language models. Ultimately, our work
validates that smaller, domain-optimized models can provide a computationally
efficient and better solution for critical safety analysis. This approach makes
powerful, low-latency deployment on resource-constrained edge devices feasible.

</details>


### [11] [Enhancing Robustness of Autoregressive Language Models against Orthographic Attacks via Pixel-based Approach](https://arxiv.org/abs/2508.21206)
*Han Yang,Jian Lan,Yihong Liu,Hinrich Schütze,Thomas Seidl*

Main category: cs.CL

TL;DR: 提出基于像素的生成语言模型，通过将单词渲染为图像来替代基于文本的嵌入，以解决自回归语言模型对拼写攻击的脆弱性问题。


<details>
  <summary>Details</summary>
Motivation: 自回归语言模型容易受到多语言字母字符扰动的拼写攻击，导致性能显著下降，这主要源于子词分词器及其嵌入的词汇表外问题。

Method: 使用基于像素的表示方法，将单词渲染为单个图像，替代传统的基于文本的嵌入方式，从而增强对噪声输入的鲁棒性并扩展对多语言文本的兼容性。

Result: 在多语言LAMBADA数据集、WMT24数据集和SST-2基准测试中验证了该方法对拼写噪声的鲁棒性以及在多语言环境中的有效性。

Conclusion: 基于像素的生成语言模型能够有效解决自回归语言模型对拼写攻击的脆弱性问题，同时提供更好的多语言文本处理能力。

Abstract: Autoregressive language models are vulnerable to orthographic attacks, where
input text is perturbed with characters from multilingual alphabets, leading to
substantial performance degradation. This vulnerability primarily stems from
the out-of-vocabulary issue inherent in subword tokenizers and their
embeddings. To address this limitation, we propose a pixel-based generative
language model that replaces the text-based embeddings with pixel-based
representations by rendering words as individual images. This design provides
stronger robustness to noisy inputs, while an extension of compatibility to
multilingual text across diverse writing systems. We evaluate the proposed
method on the multilingual LAMBADA dataset, WMT24 dataset and the SST-2
benchmark, demonstrating both its resilience to orthographic noise and its
effectiveness in multilingual settings.

</details>


### [12] [Do Self-Supervised Speech Models Exhibit the Critical Period Effects in Language Acquisition?](https://arxiv.org/abs/2508.21210)
*Yurie Koga,Shunsuke Kando,Yusuke Miyao*

Main category: cs.CL

TL;DR: 这篇论文研究了自监督语音模型是否存在人类语言获得中的关键期效应，发现这些模型并未显示出明显的关键期效应。


<details>
  <summary>Details</summary>
Motivation: 之前的研究主要使用文本语言模型来研究关键期效应，而语音模型在这方面的研究较少，尽管语音在人类语言获得中占据核心地位。

Method: 训练了不同L2训练开始时间和L1训练结束时间的自监督语音模型，使用儿童指向语音进行训练，并评估其音素辨别性能。

Result: 自监督语音模型没有显示出明显的关键期效应：L2训练开始迟的模型在L2上表现更好，而L1训练结束迟的模型反而导致L1遗忘。

Conclusion: 自监督语音模型在音位学习方面并不显示人类语言获得中的关键期效应，这与人类的语言获得机制存在重要差异。

Abstract: This paper investigates whether the Critical Period (CP) effects in human
language acquisition are observed in self-supervised speech models (S3Ms). CP
effects refer to greater difficulty in acquiring a second language (L2) with
delayed L2 exposure onset, and greater retention of their first language (L1)
with delayed L1 exposure offset. While previous work has studied these effects
using textual language models, their presence in speech models remains
underexplored despite the central role of spoken language in human language
acquisition. We train S3Ms with varying L2 training onsets and L1 training
offsets on child-directed speech and evaluate their phone discrimination
performance. We find that S3Ms do not exhibit clear evidence of either CP
effects in terms of phonological acquisition. Notably, models with delayed L2
exposure onset tend to perform better on L2 and delayed L1 exposure offset
leads to L1 forgetting.

</details>


### [13] [Decoding Memories: An Efficient Pipeline for Self-Consistency Hallucination Detection](https://arxiv.org/abs/2508.21228)
*Weizhi Gao,Xiaorui Liu,Feiyi Wang,Dan Lu,Junqi Yin*

Main category: cs.CL

TL;DR: 提出DMP方法，通过选择性推理和退火解码加速自一致性方法中的多响应生成，实现3倍速度提升且不损失检测性能


<details>
  <summary>Details</summary>
Motivation: 现有幻觉检测方法在句子级生成上表现不佳或依赖领域知识，自一致性方法计算成本高，存在冗余生成

Method: 识别自一致性方法中的冗余（共享前缀token），提出解码记忆管道(DMP)，包含选择性推理和退火解码来加速生成

Result: 实验显示方法实现最高3倍加速，且AUROC性能没有损失

Conclusion: DMP能有效提升多响应生成效率，可扩展到对齐和推理任务，与模型、数据集、解码策略和自一致性基线正交

Abstract: Large language models (LLMs) have demonstrated impressive performance in both
research and real-world applications, but they still struggle with
hallucination. Existing hallucination detection methods often perform poorly on
sentence-level generation or rely heavily on domain-specific knowledge. While
self-consistency approaches help address these limitations, they incur high
computational costs due to repeated generation. In this paper, we conduct the
first study on identifying redundancy in self-consistency methods, manifested
as shared prefix tokens across generations, and observe that non-exact-answer
tokens contribute minimally to the semantic content. Based on these insights,
we propose a novel Decoding Memory Pipeline (DMP) that accelerates generation
through selective inference and annealed decoding. Being orthogonal to the
model, dataset, decoding strategy, and self-consistency baseline, our DMP
consistently improves the efficiency of multi-response generation and holds
promise for extension to alignment and reasoning tasks. Extensive experiments
show that our method achieves up to a 3x speedup without sacrificing AUROC
performance.

</details>


### [14] [Efficient Code Embeddings from Code Generation Models](https://arxiv.org/abs/2508.21290)
*Daria Kryvosheieva,Saba Sturua,Michael Günther,Scott Martens,Han Xiao*

Main category: cs.CL

TL;DR: jina-code-embeddings是一个创新的代码嵌入模型套件，使用自回归主干网络和最后token池化技术，在代码检索和语义相似性识别方面实现了最先进的性能


<details>
  <summary>Details</summary>
Motivation: 为了解决从自然语言查询检索代码、技术问答以及跨编程语言识别语义相似代码片段的需求

Method: 使用在文本和代码上预训练的自回归主干网络，通过最后token池化(last-token pooling)生成嵌入向量

Result: 尽管模型规模相对较小，但在代码嵌入任务上实现了最先进的性能表现

Conclusion: 验证了这种代码嵌入模型构建方法的有效性，为代码理解和检索提供了高效的解决方案

Abstract: jina-code-embeddings is a novel code embedding model suite designed to
retrieve code from natural language queries, perform technical
question-answering, and identify semantically similar code snippets across
programming languages. It makes innovative use of an autoregressive backbone
pre-trained on both text and code, generating embeddings via last-token
pooling. We outline the training recipe and demonstrate state-of-the-art
performance despite the relatively small size of the models, validating this
approach to code embedding model construction.

</details>


### [15] [BLUEX Revisited: Enhancing Benchmark Coverage with Automatic Captioning](https://arxiv.org/abs/2508.21294)
*João Guilherme Alves Santos,Giovana Kerche Bonás,Thales Sales Almeida*

Main category: cs.CL

TL;DR: BLUEX数据集更新版，新增2024-2025考试数据和AI生成的图像标注，提升对LLM数据污染研究的价值，标注策略使纯文本模型可访问性提高40%以上


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能力增强，需要更强大的评估方法，特别是在多语言和非英语语境下

Method: 更新BLUEX数据集，包含最新考试内容和自动生成的图像标注，评估商业和开源LLM利用视觉上下文的能力

Result: 生成1,422个可用问题，比原BLUEX增加一倍多，标注策略使文本模型可访问性提高40%以上

Conclusion: 更新后的BLUEX数据集为LLM预训练中的数据污染研究提供了更相关的评估资源，并显著提升了视觉内容对文本模型的可用性

Abstract: With the growing capabilities of Large Language Models (LLMs), there is an
increasing need for robust evaluation methods, especially in multilingual and
non-English contexts. We present an updated version of the BLUEX dataset, now
including 2024-2025 exams and automatically generated image captions using
state-of-the-art models, enhancing its relevance for data contamination studies
in LLM pretraining. Captioning strategies increase accessibility to text-only
models by more than 40%, producing 1,422 usable questions, more than doubling
the number in the original BLUEX. We evaluated commercial and open-source LLMs
and their ability to leverage visual context through captions.

</details>


### [16] [Challenges and Applications of Large Language Models: A Comparison of GPT and DeepSeek family of models](https://arxiv.org/abs/2508.21377)
*Shubham Sharma,Sneha Tuli,Narendra Badam*

Main category: cs.CL

TL;DR: 本调查比较了OpenAI闭源GPT-4o和DeepSeek-V3-0324开源模型，分析了16个LLM开发部署挑战，展示了闭源模型的安全可靠性与开源模型效率适应性之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在各行业转型AI，但其开发和部署仍很复杂，需要系统分析当前挑战和不同模型的解决方案。

Method: 通过比较分析两种最先进模型：OpenAI闭源GPT-4o和DeepSeek开源MoE模型，评估16个关键挑战的应对策略。

Result: 展示了闭源模型在安全性和可靠性方面的优势，以及开源模型在效率和适应性方面的优势，为不同应用场景提供模型选择指导。

Conclusion: 为AI研究者、开发者和决策者提供了理解当前LLM能力、局限性和最佳实践的指南，帮助根据具体应用需求选择合适的模型类型。

Abstract: Large Language Models (LLMs) are transforming AI across industries, but their
development and deployment remain complex. This survey reviews 16 key
challenges in building and using LLMs and examines how these challenges are
addressed by two state-of-the-art models with unique approaches: OpenAI's
closed source GPT-4o (May 2024 update) and DeepSeek-V3-0324 (March 2025), a
large open source Mixture-of-Experts model. Through this comparison, we
showcase the trade-offs between closed source models (robust safety, fine-tuned
reliability) and open source models (efficiency, adaptability). We also explore
LLM applications across different domains (from chatbots and coding tools to
healthcare and education), highlighting which model attributes are best suited
for each use case. This article aims to guide AI researchers, developers, and
decision-makers in understanding current LLM capabilities, limitations, and
best practices.

</details>


### [17] [Normality and the Turing Test](https://arxiv.org/abs/2508.21382)
*Alexandre Kabbach*

Main category: cs.CL

TL;DR: 本文通过'正常性'概念重新审视图灵测试，认为图灵测试检验的是平均人类智能而非卓越智能，需要机器表现出正常人类的错误行为，且评估应由陪审团而非单个法官进行。


<details>
  <summary>Details</summary>
Motivation: 重新解读图灵测试的核心概念，挑战当前大型语言模型追求卓越智能而非平均智能的范式，探讨图灵测试对人类认知理解的贡献。

Method: 从统计学角度分析'正常'概念的双重含义（规范性和数学性），重新解释图灵测试中'平均人类询问者'的概念，将其理解为多个法官判断的标准化聚合。

Result: 提出ChatGPT等大型语言模型针对的是卓越智能而非平均智能，因此不太可能通过图灵测试，它们代表的是'人工聪明'而非真正的人工智能。

Conclusion: 图灵测试能否促进对人类认知的理解，关键在于人类心智是否真的可简化为平均心智，这超出了图灵测试本身，质疑了其所依赖的正常主义范式的概念基础。

Abstract: This paper proposes to revisit the Turing test through the concept of
normality. Its core argument is that the statistical interpretation of the
normal--understood as the average both in the normative and mathematical sense
of the term--proves useful for understanding the Turing test in at least two
ways. First, in the sense that the Turing test targets normal/average rather
than exceptional human intelligence, so that successfully passing the test
requires building machines that "make mistakes" and display imperfect behavior
just like normal/average humans. Second, in the sense that the Turing test is a
statistical test where judgments of intelligence are never carried out by a
single "average" judge (understood as non-expert) but always by a full jury. As
such, the notion of "average human interrogator" that Turing talks about in his
original paper should be understood primarily as referring to a mathematical
abstraction made of the normalized aggregate of individual judgments of
multiple judges. In short, this paper argues that the Turing test is a test of
normal intelligence as assessed by a normal judge characterizing the average
judgment of a pool of human interrogators. Its conclusions are twofold. First,
it argues that large language models such as ChatGPT are unlikely to pass the
Turing test as those models precisely target exceptional rather than
normal/average human intelligence. As such, they constitute models of what it
proposes to call artificial smartness rather than artificial intelligence per
se. Second, it argues that the core question of whether the Turing test can
contribute anything to the understanding of human cognition is that of whether
the human mind is really reducible to the normal/average mind--a question which
largely extends beyond the Turing test itself and questions the conceptual
underpinnings of the normalist paradigm it belongs to.

</details>


### [18] [AllSummedUp: un framework open-source pour comparer les metriques d'evaluation de resume](https://arxiv.org/abs/2508.21389)
*Tanguy Herserant,Vincent Guigue*

Main category: cs.CL

TL;DR: 本文研究发现自动文本摘要评估存在显著的可复现性问题，六种主流评估指标在实验中的表现与文献报告存在较大差异，揭示了评估质量与计算成本之间的结构性权衡。


<details>
  <summary>Details</summary>
Motivation: 针对自动文本摘要评估领域存在的可复现性挑战，研究旨在揭示不同评估指标在实际应用中的真实性能差异，特别是LLM-based方法的不稳定性和技术依赖问题。

Method: 开发统一的开源评估框架，在SummEval数据集上系统测试六种代表性评估指标（包括ROUGE和LLM-based方法如G-Eval、SEval-Ex），进行公平透明的对比分析。

Result: 发现评估指标存在显著性能差异，与人类判断最一致的指标往往计算密集且运行稳定性较差；LLM-based方法表现出随机性、技术依赖性强和可复现性有限等问题。

Conclusion: 呼吁建立更稳健的评估协议，包括详尽文档记录和方法论标准化，以确保自动摘要评估的可靠性，强调需要平衡评估质量与计算效率。

Abstract: This paper investigates reproducibility challenges in automatic text
summarization evaluation. Based on experiments conducted across six
representative metrics ranging from classical approaches like ROUGE to recent
LLM-based methods (G-Eval, SEval-Ex), we highlight significant discrepancies
between reported performances in the literature and those observed in our
experimental setting. We introduce a unified, open-source framework, applied to
the SummEval dataset and designed to support fair and transparent comparison of
evaluation metrics. Our results reveal a structural trade-off: metrics with the
highest alignment with human judgments tend to be computationally intensive and
less stable across runs. Beyond comparative analysis, this study highlights key
concerns about relying on LLMs for evaluation, stressing their randomness,
technical dependencies, and limited reproducibility. We advocate for more
robust evaluation protocols including exhaustive documentation and
methodological standardization to ensure greater reliability in automatic
summarization assessment.

</details>


### [19] [Automatic Reviewers Fail to Detect Faulty Reasoning in Research Papers: A New Counterfactual Evaluation Framework](https://arxiv.org/abs/2508.21422)
*Nils Dycke,Iryna Gurevych*

Main category: cs.CL

TL;DR: LLMs作为自动审稿生成器在检测研究逻辑错误方面表现不佳，即使论文存在明显逻辑缺陷，生成的审稿意见也无显著差异，需要改进评估框架。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs越来越多地被用作全自动审稿生成器，需要了解其在检测研究逻辑错误这一核心审稿能力上的具体表现和局限性，以保障科学诚信。

Method: 提出了一个全自动的反事实评估框架，在受控条件下测试各种ARG方法检测研究逻辑错误的能力，包括评估论文结果、解释和主张之间的内部一致性。

Result: 研究发现，与预期相反，研究逻辑中的缺陷对ARG生成的审稿意见没有显著影响，表明当前LLMs在这方面存在明显局限性。

Conclusion: 基于研究发现提出了三项可操作的建议，并公开了反事实数据集和评估框架，以促进未来研究改进自动审稿系统的逻辑错误检测能力。

Abstract: Large Language Models (LLMs) have great potential to accelerate and support
scholarly peer review and are increasingly used as fully automatic review
generators (ARGs). However, potential biases and systematic errors may pose
significant risks to scientific integrity; understanding the specific
capabilities and limitations of state-of-the-art ARGs is essential. We focus on
a core reviewing skill that underpins high-quality peer review: detecting
faulty research logic. This involves evaluating the internal consistency
between a paper's results, interpretations, and claims. We present a fully
automated counterfactual evaluation framework that isolates and tests this
skill under controlled conditions. Testing a range of ARG approaches, we find
that, contrary to expectation, flaws in research logic have no significant
effect on their output reviews. Based on our findings, we derive three
actionable recommendations for future work and release our counterfactual
dataset and evaluation framework publicly.

</details>


### [20] [Med-RewardBench: Benchmarking Reward Models and Judges for Medical Multimodal Large Language Models](https://arxiv.org/abs/2508.21430)
*Meidan Ding,Jipeng Zhang,Wenxuan Wang,Cheng-Yi Li,Wei-Chieh Fang,Hsin-Yu Wu,Haiqin Zhong,Wenting Chen,Linlin Shen*

Main category: cs.CL

TL;DR: 提出了首个专门评估医疗奖励模型和评判器的基准Med-RewardBench，包含1026个专家标注的多模态医疗案例，覆盖13个器官系统和8个临床科室，在6个临床关键维度上评估32个先进MLLM模型。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注通用MLLM能力或评估模型作为求解器，忽视了诊断准确性和临床相关性等关键评估维度，医疗奖励模型和评判器研究不足且缺乏专用基准。

Method: 构建多模态数据集，采用三步严格流程确保高质量评估数据，在6个临床关键维度上评估32个先进MLLM模型，并开发基线模型通过微调展示性能提升。

Result: 评估显示现有模型在输出与专家判断对齐方面存在显著挑战，开发的基线模型通过微调实现了显著性能改进。

Conclusion: Med-RewardBench填补了医疗奖励模型评估的空白，为医疗MLLM的可靠评估提供了重要基准，证明了专门微调对提升医疗对齐性能的有效性。

Abstract: Multimodal large language models (MLLMs) hold significant potential in
medical applications, including disease diagnosis and clinical decision-making.
However, these tasks require highly accurate, context-sensitive, and
professionally aligned responses, making reliable reward models and judges
critical. Despite their importance, medical reward models (MRMs) and judges
remain underexplored, with no dedicated benchmarks addressing clinical
requirements. Existing benchmarks focus on general MLLM capabilities or
evaluate models as solvers, neglecting essential evaluation dimensions like
diagnostic accuracy and clinical relevance. To address this, we introduce
Med-RewardBench, the first benchmark specifically designed to evaluate MRMs and
judges in medical scenarios. Med-RewardBench features a multimodal dataset
spanning 13 organ systems and 8 clinical departments, with 1,026
expert-annotated cases. A rigorous three-step process ensures high-quality
evaluation data across six clinically critical dimensions. We evaluate 32
state-of-the-art MLLMs, including open-source, proprietary, and
medical-specific models, revealing substantial challenges in aligning outputs
with expert judgment. Additionally, we develop baseline models that demonstrate
substantial performance improvements through fine-tuning.

</details>


### [21] [Discovering Semantic Subdimensions through Disentangled Conceptual Representations](https://arxiv.org/abs/2508.21436)
*Yunhao Zhang,Shaonan Wang,Nan Lin,Xinyi Dong,Chong Li,Chengqing Zong*

Main category: cs.CL

TL;DR: 提出了一种解耦连续语义表示模型(DCSRM)，将大语言模型的词嵌入分解为多个子嵌入，识别可解释的语义子维度，并通过脑成像验证其神经合理性。


<details>
  <summary>Details</summary>
Motivation: 现有语义维度分析方法通常依赖预定义的粗粒度维度，忽略了更细粒度的概念区分，需要更精细的语义子维度分析方法。

Method: 开发DCSRM模型分解词嵌入为多个子嵌入，使用体素编码模型将这些子维度映射到大脑激活模式，验证神经合理性。

Result: 识别出一组可解释的语义子维度，发现语义维度按照不同原则结构化，极性是驱动分解的关键因素。

Conclusion: 该方法提供了更细粒度的可解释语义子维度，其神经相关性支持了这些子维度的认知和神经科学合理性。

Abstract: Understanding the core dimensions of conceptual semantics is fundamental to
uncovering how meaning is organized in language and the brain. Existing
approaches often rely on predefined semantic dimensions that offer only broad
representations, overlooking finer conceptual distinctions. This paper proposes
a novel framework to investigate the subdimensions underlying coarse-grained
semantic dimensions. Specifically, we introduce a Disentangled Continuous
Semantic Representation Model (DCSRM) that decomposes word embeddings from
large language models into multiple sub-embeddings, each encoding specific
semantic information. Using these sub-embeddings, we identify a set of
interpretable semantic subdimensions. To assess their neural plausibility, we
apply voxel-wise encoding models to map these subdimensions to brain
activation. Our work offers more fine-grained interpretable semantic
subdimensions of conceptual meaning. Further analyses reveal that semantic
dimensions are structured according to distinct principles, with polarity
emerging as a key factor driving their decomposition into subdimensions. The
neural correlates of the identified subdimensions support their cognitive and
neuroscientific plausibility.

</details>


### [22] [Beyond the Surface: Probing the Ideological Depth of Large Language Models](https://arxiv.org/abs/2508.21448)
*Shariar Kabir,Kevin Esterling,Yue Dong*

Main category: cs.CL

TL;DR: 该研究提出"意识形态深度"概念来衡量LLM内部政治表征的稳健性和复杂性，通过可操控性测试和稀疏自编码器分析发现不同模型在意识形态深度上存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型表现出明显的意识形态倾向，但这些倾向的稳定性和深度尚不清楚。表面层面的回应容易被提示工程操控，需要探究LLM是否具有连贯的底层意识形态。

Method: 采用双重方法：1) 通过指令提示和激活引导测量开源LLM的可操控性；2) 使用稀疏自编码器(SAEs)探测模型内部机制，分析意识形态特征。

Result: 发现可操控性较低的模型具有更独特和抽象的意识形态特征，一个模型可能包含比同规模模型多7.3倍的政治特征。对"深度"模型核心政治特征的有针对性消融能导致相关主题推理的一致逻辑转变。

Conclusion: 意识形态深度是LLM的可量化属性，可操控性为理解其潜在政治架构提供了有价值的窗口，不同模型在意识形态深度上存在显著差异。

Abstract: Large Language Models (LLMs) have demonstrated pronounced ideological
leanings, yet the stability and depth of these positions remain poorly
understood. Surface-level responses can often be manipulated through simple
prompt engineering, calling into question whether they reflect a coherent
underlying ideology. This paper investigates the concept of "ideological depth"
in LLMs, defined as the robustness and complexity of their internal political
representations. We employ a dual approach: first, we measure the
"steerability" of two well-known open-source LLMs using instruction prompting
and activation steering. We find that while some models can easily switch
between liberal and conservative viewpoints, others exhibit resistance or an
increased rate of refusal, suggesting a more entrenched ideological structure.
Second, we probe the internal mechanisms of these models using Sparse
Autoencoders (SAEs). Preliminary analysis reveals that models with lower
steerability possess more distinct and abstract ideological features. Our
evaluations reveal that one model can contain 7.3x more political features than
another model of similar size. This allows targeted ablation of a core
political feature in an ideologically "deep" model, leading to consistent,
logical shifts in its reasoning across related topics, whereas the same
intervention in a "shallow" model results in an increase in refusal outputs.
Our findings suggest that ideological depth is a quantifiable property of LLMs
and that steerability serves as a valuable window into their latent political
architecture.

</details>


### [23] [Igniting Creative Writing in Small Language Models: LLM-as-a-Judge versus Multi-Agent Refined Rewards](https://arxiv.org/abs/2508.21476)
*Xiaolong Wei,Bo Lu,Xingyu Zhang,Zhejun Zhao,Dongdong Shen,Long Xia,Dawei Yin*

Main category: cs.CL

TL;DR: 本文提出两种AI驱动的奖励策略，通过RLAIF框架提升7B参数小语言模型的中文问候语创意写作能力，其中基于原则的LLM-as-a-Judge方法表现最佳。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型计算需求高，小语言模型在创意写作方面存在新颖性不足的问题，传统方法如SFT和RLHF存在成本高、效果有限的问题。

Method: 使用RLAIF框架，探索两种AI奖励策略：1）基于多智能体拒绝采样框架训练的奖励模型；2）基于对抗训练优化的原则指导LLM-as-a-Judge方法。

Result: 两种方法都显著提升了创意输出质量，但原则指导的LLM-as-a-Judge方法在生成质量、训练效率和减少人工标注依赖方面表现更优。

Conclusion: 基于原则的LLM-as-a-Judge方法为创意小语言模型提供了更可扩展和有效的路径，自动化评估方法与人类判断高度一致。

Abstract: Large Language Models (LLMs) have demonstrated remarkable creative writing
capabilities, yet their substantial computational demands hinder widespread
use. Enhancing Small Language Models (SLMs) offers a promising alternative, but
current methods like Supervised Fine-Tuning (SFT) struggle with novelty, and
Reinforcement Learning from Human Feedback (RLHF) is costly. This paper
explores two distinct AI-driven reward strategies within a Reinforcement
Learning from AI Feedback (RLAIF) framework to ignite the creative writing of a
7B-parameter SLM, specifically for generating Chinese greetings. The first
strategy employs a RM trained on high-quality preference data curated by a
novel multi-agent rejection sampling framework designed for creative tasks. The
second, more novel strategy utilizes a principle-guided LLM-as-a-Judge, whose
reward function is optimized via an adversarial training scheme with a
reflection mechanism, to directly provide reward signals. Comprehensive
experiments reveal that while both approaches significantly enhance creative
output over baselines, the principle-guided LLM-as-a-Judge demonstrably yields
superior generation quality. Furthermore, it offers notable advantages in
training efficiency and reduced dependency on human-annotated data, presenting
a more scalable and effective path towards creative SLMs. Our automated
evaluation methods also exhibit strong alignment with human judgments. Our code
and data are publicly available at
https://github.com/weixiaolong94-hub/Igniting-Creative-Writing-in-Small-Language-Models.

</details>


### [24] [HSFN: Hierarchical Selection for Fake News Detection building Heterogeneous Ensemble](https://arxiv.org/abs/2508.21482)
*Sara B. Coutinho,Rafael M. O. Cruz,Francimaria R. S. Nascimento,George D. C. Cavalcanti*

Main category: cs.CL

TL;DR: 提出了一种基于层次聚类的自动分类器选择方法，通过优先考虑多样性来改进集成学习在假新闻检测中的性能


<details>
  <summary>Details</summary>
Motivation: 心理偏见使人们容易相信和传播假新闻，集成学习方法虽然有效但性能严重依赖分类器的多样性，而选择真正多样化的模型仍是一个关键挑战

Method: 首先计算分类器间的成对多样性，应用层次聚类将其组织到不同粒度级别的组中，然后通过HierarchySelect探索层次级别为每个级别选择一个分类器池，最后选择最多样化的池用于集成构建，并融入性能评估指标

Result: 在6个数据集中的2个上达到了最高准确率，优于Elbow启发式方法和最先进的基线方法

Conclusion: 该方法能够有效选择多样化的分类器组合，提升集成学习在假新闻检测任务中的性能表现

Abstract: Psychological biases, such as confirmation bias, make individuals
particularly vulnerable to believing and spreading fake news on social media,
leading to significant consequences in domains such as public health and
politics. Machine learning-based fact-checking systems have been widely studied
to mitigate this problem. Among them, ensemble methods are particularly
effective in combining multiple classifiers to improve robustness. However,
their performance heavily depends on the diversity of the constituent
classifiers-selecting genuinely diverse models remains a key challenge,
especially when models tend to learn redundant patterns. In this work, we
propose a novel automatic classifier selection approach that prioritizes
diversity, also extended by performance. The method first computes pairwise
diversity between classifiers and applies hierarchical clustering to organize
them into groups at different levels of granularity. A HierarchySelect then
explores these hierarchical levels to select one pool of classifiers per level,
each representing a distinct intra-pool diversity. The most diverse pool is
identified and selected for ensemble construction from these. The selection
process incorporates an evaluation metric reflecting each classifiers's
performance to ensure the ensemble also generalises well. We conduct
experiments with 40 heterogeneous classifiers across six datasets from
different application domains and with varying numbers of classes. Our method
is compared against the Elbow heuristic and state-of-the-art baselines. Results
show that our approach achieves the highest accuracy on two of six datasets.
The implementation details are available on the project's repository:
https://github.com/SaraBCoutinho/HSFN .

</details>


### [25] [L3Cube-MahaSTS: A Marathi Sentence Similarity Dataset and Models](https://arxiv.org/abs/2508.21569)
*Aishwarya Mirashi,Ananya Joshi,Raviraj Joshi*

Main category: cs.CL

TL;DR: 提出了MahaSTS数据集和MahaSBERT-STS-v2模型，这是一个用于马拉地语句子相似度任务的标注数据集和优化模型


<details>
  <summary>Details</summary>
Motivation: 为低资源语言马拉地语提供高质量的句子文本相似度标注数据集，解决现有资源不足的问题

Method: 构建包含16,860个马拉地语句子对的人工标注数据集，采用0-5连续相似度评分，并通过分数桶实现均衡监督；基于此数据集微调Sentence-BERT模型

Result: MahaSBERT-STS-v2模型在马拉地语句子相似度任务上表现优于MahaBERT、MuRIL、IndicBERT和IndicSBERT等其他模型

Conclusion: 人工标注、针对性微调和结构化监督在低资源语言环境中对句子相似度任务具有显著影响，MahaSTS数据集有效支持马拉地语NLP研究

Abstract: We present MahaSTS, a human-annotated Sentence Textual Similarity (STS)
dataset for Marathi, along with MahaSBERT-STS-v2, a fine-tuned Sentence-BERT
model optimized for regression-based similarity scoring. The MahaSTS dataset
consists of 16,860 Marathi sentence pairs labeled with continuous similarity
scores in the range of 0-5. To ensure balanced supervision, the dataset is
uniformly distributed across six score-based buckets spanning the full 0-5
range, thus reducing label bias and enhancing model stability. We fine-tune the
MahaSBERT model on this dataset and benchmark its performance against other
alternatives like MahaBERT, MuRIL, IndicBERT, and IndicSBERT. Our experiments
demonstrate that MahaSTS enables effective training for sentence similarity
tasks in Marathi, highlighting the impact of human-curated annotations,
targeted fine-tuning, and structured supervision in low-resource settings. The
dataset and model are publicly shared at
https://github.com/l3cube-pune/MarathiNLP

</details>


### [26] [A Survey on Current Trends and Recent Advances in Text Anonymization](https://arxiv.org/abs/2508.21587)
*Tobias Deußer,Lorenz Sparrenberg,Armin Berger,Max Hahnbück,Christian Bauckhage,Rafet Sifa*

Main category: cs.CL

TL;DR: 本论文统过性分析了文本匿名化技术的最新进展，包括基础方法、大语言模型的双重作用、领域特定挑战、形式隐私模型和评估框架等，为未来研究提供指导。


<details>
  <summary>Details</summary>
Motivation: 各行业文本数据中含有大量敏感个人信息，需要符合监管要求的匿名化技术来保护隐私，同时保持数据的可用性。

Method: 通过综述性调查，分析了名称实体识别等基础方法、大语言模型的匿名化和反匿名化能力、各领域特定解决方案、形式隐私模型、风险感知框架以及作者匿名化等多种技术。

Result: 本论文提供了文本匿名化领域的全面概述，整合了当前知识，识别了新兴趋势和持续挑战，包括隐私-效用性交换、几乎识别符处理和大语言模型能力的影响等。

Conclusion: 该综述为学术界和业界实践者提供了文本匿名化领域的全面指南，显示了该领域的复杂性和重要性，并为未来研究方向提供了有价值的建议。

Abstract: The proliferation of textual data containing sensitive personal information
across various domains requires robust anonymization techniques to protect
privacy and comply with regulations, while preserving data usability for
diverse and crucial downstream tasks. This survey provides a comprehensive
overview of current trends and recent advances in text anonymization
techniques. We begin by discussing foundational approaches, primarily centered
on Named Entity Recognition, before examining the transformative impact of
Large Language Models, detailing their dual role as sophisticated anonymizers
and potent de-anonymization threats. The survey further explores
domain-specific challenges and tailored solutions in critical sectors such as
healthcare, law, finance, and education. We investigate advanced methodologies
incorporating formal privacy models and risk-aware frameworks, and address the
specialized subfield of authorship anonymization. Additionally, we review
evaluation frameworks, comprehensive metrics, benchmarks, and practical
toolkits for real-world deployment of anonymization solutions. This review
consolidates current knowledge, identifies emerging trends and persistent
challenges, including the evolving privacy-utility trade-off, the need to
address quasi-identifiers, and the implications of LLM capabilities, and aims
to guide future research directions for both academics and practitioners in
this field.

</details>


### [27] [Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning](https://arxiv.org/abs/2508.21589)
*Zinan Tang,Xin Gao,Qizhi Pei,Zhuoshi Pan,Mengzhang Cai,Jiang Wu,Conghui He,Lijun Wu*

Main category: cs.CL

TL;DR: Middo是一个自演化的模型驱动动态数据优化框架，通过三轴模型信号诊断和自适应优化引擎，实现数据与模型的协同进化，平均提升LLM准确率7.15%


<details>
  <summary>Details</summary>
Motivation: 传统静态数据筛选方法无法适应模型能力的动态演化，需要建立能够与模型能力共同进化的动态数据优化系统

Method: 基于三轴模型信号（损失模式、嵌入聚类动态、自对齐分数）的自引用诊断模块，结合保持语义完整性的自适应优化引擎，形成闭环优化系统

Result: 在多个基准测试中，该方法持续提升种子数据质量，平均提高LLM准确率7.15%，同时保持原始数据集规模

Conclusion: 建立了通过数据与模型动态人机协同进化的可持续LLM训练新范式

Abstract: Supervised Fine-Tuning (SFT) Large Language Models (LLM) fundamentally rely
on high-quality training data. While data selection and data synthesis are two
common strategies to improve data quality, existing approaches often face
limitations in static dataset curation that fail to adapt to evolving model
capabilities. In this paper, we introduce Middo, a self-evolving Model-informed
dynamic data optimization framework that uses model-aware data selection and
context-preserving data refinement. Unlike conventional one-off
filtering/synthesis methods, our framework establishes a closed-loop
optimization system: (1) A self-referential diagnostic module proactively
identifies suboptimal samples through tri-axial model signals - loss patterns
(complexity), embedding cluster dynamics (diversity), and self-alignment scores
(quality); (2) An adaptive optimization engine then transforms suboptimal
samples into pedagogically valuable training points while preserving semantic
integrity; (3) This optimization process continuously evolves with model
capability through dynamic learning principles. Experiments on multiple
benchmarks demonstrate that our \method consistently enhances the quality of
seed data and boosts LLM's performance with improving accuracy by 7.15% on
average while maintaining the original dataset scale. This work establishes a
new paradigm for sustainable LLM training through dynamic human-AI co-evolution
of data and models. Our datasets, models, and code are coming soon.

</details>


### [28] [Personality Matters: User Traits Predict LLM Preferences in Multi-Turn Collaborative Tasks](https://arxiv.org/abs/2508.21628)
*Sarfaroz Yunusov,Kaige Chen,Kazi Nishat Anwar,Ali Emami*

Main category: cs.CL

TL;DR: 研究发现不同人格类型的用户在与GPT-4和Claude 3.5多轮协作任务中存在系统性偏好，理性者偏好GPT-4，理想主义者偏好Claude 3.5。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型日益集成到日常工作流程中，需要了解不同人格特质的用户是否会系统地偏好某些LLM。

Method: 在32名参与者中进行研究，平均分配到四种Keirsey人格类型，评估他们与GPT-4和Claude 3.5在四种协作任务（数据分析、创意写作、信息检索、写作辅助）中的交互。

Result: 结果显示了显著的人格驱动偏好：理性者强烈偏好GPT-4（尤其是目标导向任务），理想主义者偏好Claude 3.5（尤其是创意和分析任务）。其他人格类型显示任务依赖性偏好。情感分析确认了这些模式。

Conclusion: 人格基础的分析能够揭示传统评估方法忽略的LLM差异，虽然总体有用性评分相似，但不同人格类型的用户存在明显的模型偏好。

Abstract: As Large Language Models (LLMs) increasingly integrate into everyday
workflows, where users shape outcomes through multi-turn collaboration, a
critical question emerges: do users with different personality traits
systematically prefer certain LLMs over others? We conducted a study with 32
participants evenly distributed across four Keirsey personality types,
evaluating their interactions with GPT-4 and Claude 3.5 across four
collaborative tasks: data analysis, creative writing, information retrieval,
and writing assistance. Results revealed significant personality-driven
preferences: Rationals strongly preferred GPT-4, particularly for goal-oriented
tasks, while idealists favored Claude 3.5, especially for creative and
analytical tasks. Other personality types showed task-dependent preferences.
Sentiment analysis of qualitative feedback confirmed these patterns. Notably,
aggregate helpfulness ratings were similar across models, showing how
personality-based analysis reveals LLM differences that traditional evaluations
miss.

</details>


### [29] [QZhou-Embedding Technical Report](https://arxiv.org/abs/2508.21632)
*Peng Yu,En Xu,Bin Chen,Haibiao Chen,Yinfei Xu*

Main category: cs.CL

TL;DR: QZhou-Embedding是基于Qwen2.5-7B-Instruct构建的通用文本嵌入模型，通过多任务框架、数据合成管道和两阶段训练策略，在MTEB和CMTEB基准测试中取得最佳性能。


<details>
  <summary>Details</summary>
Motivation: 开发一个具有卓越文本表示能力的通用上下文文本嵌入模型，通过高质量多样化数据和LLM生成能力来提升检索模型性能。

Method: 采用统一多任务框架，包括专门的数据转换和训练策略。使用LLM API构建数据合成管道（释义、增强、困难负样本生成），并实施两阶段训练策略：检索预训练和全任务微调。

Result: 在MTEB和CMTEB基准测试中排名第一（2025年8月27日），同时在重排序、聚类等任务上达到最先进性能。

Conclusion: 高质量多样化数据对提升检索模型性能至关重要，利用LLM生成能力可以进一步优化数据质量，实现嵌入模型的突破。模型权重在HuggingFace上开源。

Abstract: We present QZhou-Embedding, a general-purpose contextual text embedding model
with exceptional text representation capabilities. Built upon the
Qwen2.5-7B-Instruct foundation model, we designed a unified multi-task
framework comprising specialized data transformation and training strategies.
The data transformation scheme enables the incorporation of more diverse
textual training datasets, while the task-specific training strategies enhance
model learning efficiency. We developed a data synthesis pipeline leveraging
LLM API, incorporating techniques such as paraphrasing, augmentation, and hard
negative example generation to improve the semantic richness and sample
difficulty of the training set. Additionally, we employ a two-stage training
strategy, comprising initial retrieval-focused pretraining followed by
full-task fine-tuning, enabling the embedding model to extend its capabilities
based on robust retrieval performance. Our model achieves state-of-the-art
results on the MTEB and CMTEB benchmarks, ranking first on both leaderboards
(August 27 2025), and simultaneously achieves state-of-the-art performance on
tasks including reranking, clustering, etc. Our findings demonstrate that
higher-quality, more diverse data is crucial for advancing retrieval model
performance, and that leveraging LLMs generative capabilities can further
optimize data quality for embedding model breakthroughs. Our model weights are
released on HuggingFace under Apache 2.0 license. For reproducibility, we
provide evaluation code and instructions on GitHub.

</details>


### [30] [Is this chart lying to me? Automating the detection of misleading visualizations](https://arxiv.org/abs/2508.21675)
*Jonathan Tonglet,Jan Zimny,Tinne Tuytelaars,Iryna Gurevych*

Main category: cs.CL

TL;DR: 提出了Misviz基准数据集，包含2,604个真实世界误导性可视化图表，标注了12种误导类型，并发布了包含81,814个合成可视化图表的Misviz-synth数据集，用于训练和评估AI模型检测误导性可视化。


<details>
  <summary>Details</summary>
Motivation: 误导性可视化是社交媒体和网络上错误信息的重要来源，违反图表设计原则会扭曲数据并导致读者得出错误结论。现有AI模型训练和评估缺乏大规模、多样化且公开可用的数据集。

Method: 创建了Misviz基准数据集（2,604个真实可视化图表）和Misviz-synth合成数据集（81,814个基于真实数据表的Matplotlib生成图表），使用最先进的多模态大语言模型、基于规则的系统以及微调分类器进行综合评估。

Result: 任务仍然极具挑战性，即使是先进模型也难以有效检测误导性可视化图表。

Conclusion: 发布了Misviz和Misviz-synth数据集及相关代码，为检测误导性可视化和识别违反的设计规则提供了重要资源，有助于保护读者并减少错误信息传播。

Abstract: Misleading visualizations are a potent driver of misinformation on social
media and the web. By violating chart design principles, they distort data and
lead readers to draw inaccurate conclusions. Prior work has shown that both
humans and multimodal large language models (MLLMs) are frequently deceived by
such visualizations. Automatically detecting misleading visualizations and
identifying the specific design rules they violate could help protect readers
and reduce the spread of misinformation. However, the training and evaluation
of AI models has been limited by the absence of large, diverse, and openly
available datasets. In this work, we introduce Misviz, a benchmark of 2,604
real-world visualizations annotated with 12 types of misleaders. To support
model training, we also release Misviz-synth, a synthetic dataset of 81,814
visualizations generated using Matplotlib and based on real-world data tables.
We perform a comprehensive evaluation on both datasets using state-of-the-art
MLLMs, rule-based systems, and fine-tuned classifiers. Our results reveal that
the task remains highly challenging. We release Misviz, Misviz-synth, and the
accompanying code.

</details>


### [31] [Not All Parameters Are Created Equal: Smart Isolation Boosts Fine-Tuning Performance](https://arxiv.org/abs/2508.21741)
*Yao Wang,Di Liang,Minlong Peng*

Main category: cs.CL

TL;DR: 提出CPI-FT框架解决多任务微调中的跷跷板现象，通过核心参数隔离和融合技术减少任务干扰和遗忘


<details>
  <summary>Details</summary>
Motivation: 监督微调(SFT)在大语言模型下游任务适配中存在跷跷板现象，某些任务性能提升以其他任务性能下降为代价

Method: 1) 独立微调识别核心参数区域 2) 基于区域重叠的任务聚类 3) 核心参数直接移植+非核心参数SLERP融合 4) 轻量级流水线SFT训练

Result: 在多个公开基准测试中显著减轻任务干扰和遗忘，持续优于普通多任务和多阶段微调基线

Conclusion: CPI-FT框架通过参数隔离和智能融合有效解决了多任务微调中的性能冲突问题

Abstract: Supervised fine-tuning (SFT) is a pivotal approach to adapting large language
models (LLMs) for downstream tasks; however, performance often suffers from the
``seesaw phenomenon'', where indiscriminate parameter updates yield progress on
certain tasks at the expense of others. To address this challenge, we propose a
novel \emph{Core Parameter Isolation Fine-Tuning} (CPI-FT) framework.
Specifically, we first independently fine-tune the LLM on each task to identify
its core parameter regions by quantifying parameter update magnitudes. Tasks
with similar core regions are then grouped based on region overlap, forming
clusters for joint modeling. We further introduce a parameter fusion technique:
for each task, core parameters from its individually fine-tuned model are
directly transplanted into a unified backbone, while non-core parameters from
different tasks are smoothly integrated via Spherical Linear Interpolation
(SLERP), mitigating destructive interference. A lightweight, pipelined SFT
training phase using mixed-task data is subsequently employed, while freezing
core regions from prior tasks to prevent catastrophic forgetting. Extensive
experiments on multiple public benchmarks demonstrate that our approach
significantly alleviates task interference and forgetting, consistently
outperforming vanilla multi-task and multi-stage fine-tuning baselines.

</details>


### [32] [Reasoning-Intensive Regression](https://arxiv.org/abs/2508.21762)
*Diane Tchuindjo,Omar Khattab*

Main category: cs.CL

TL;DR: 本文提出了推理密集型回归(RiR)任务的概念，针对需要从文本中推导细微数值属性的问题。作者建立了RiR基准测试，发现现有方法表现不佳，进而提出了MENTAT方法，结合批量反射提示优化和神经集成学习，取得了65%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前AI研究越来越多地将大语言模型应用于推理密集型回归任务，这类任务需要从文本中推导出细微的数值属性，但现有的冻结LLM提示方法和微调Transformer编码器方法在此类任务上表现不佳。

Method: 提出了MENTAT方法，该方法结合了批量反射提示优化和神经集成学习，是一个简单轻量级的解决方案。

Result: MENTAT方法在RiR基准测试中取得了最高65%的性能提升，显著优于冻结LLM提示和微调Transformer编码器两种基线方法。

Conclusion: 虽然MENTAT方法在推理密集型回归任务上取得了显著改进，但此类任务仍存在很大的提升空间，需要未来的进一步研究和发展。

Abstract: AI researchers and practitioners increasingly apply large language models
(LLMs) to what we call reasoning-intensive regression (RiR), i.e. deducing
subtle numerical properties from text. Unlike standard language regression
tasks, e.g. for sentiment or similarity, RiR often appears instead in ad-hoc
problems like rubric-based scoring or domain-specific retrieval, where much
deeper analysis of text is required while only limited task-specific training
data and computation are available. We cast three realistic problems as RiR
tasks to establish an initial benchmark, and use that to test our hypothesis
that prompting frozen LLMs and finetuning Transformer encoders via gradient
descent will both often struggle in RiR. We then propose MENTAT, a simple and
lightweight method that combines batch-reflective prompt optimization with
neural ensemble learning. MENTAT achieves up to 65% improvement over both
baselines, though substantial room remains for future advances in RiR.

</details>


### [33] [PiCSAR: Probabilistic Confidence Selection And Ranking](https://arxiv.org/abs/2508.21787)
*Joshua Ong Jun Leang,Zheng Zhao,Aryo Pradipta Gema,Sohee Yang,Wai-Chung Kwan,Xuanli He,Wenda Li,Pasquale Minervini,Eleonora Giunchiglia,Shay B. Cohen*

Main category: cs.CL

TL;DR: PiCSAR是一种无需训练的置信度选择排序方法，通过联合对数似然评分来提升大语言模型的推理准确性，在多个基准测试中显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 针对推理任务中缺乏真实答案时难以设计有效评分函数的问题，需要一种能够识别正确推理链的简单方法。

Method: 提出Probabilistic Confidence Selection And Ranking (PiCSAR)，使用推理过程和最终答案的联合对数似然来评分候选生成结果，该方法自然分解为推理置信度和答案置信度。

Result: 在MATH500上提升10.18分，在AIME2025上提升9.81分，在20次比较中的16次以至少2倍更少的样本优于基线方法。

Conclusion: 正确推理链展现出显著更高的推理和答案置信度，证明了PiCSAR方法的有效性。

Abstract: Best-of-n sampling improves the accuracy of large language models (LLMs) and
large reasoning models (LRMs) by generating multiple candidate solutions and
selecting the one with the highest reward. The key challenge for reasoning
tasks is designing a scoring function that can identify correct reasoning
chains without access to ground-truth answers. We propose Probabilistic
Confidence Selection And Ranking (PiCSAR): a simple, training-free method that
scores each candidate generation using the joint log-likelihood of the
reasoning and final answer. The joint log-likelihood of the reasoning and final
answer naturally decomposes into reasoning confidence and answer confidence.
PiCSAR achieves substantial gains across diverse benchmarks (+10.18 on MATH500,
+9.81 on AIME2025), outperforming baselines with at least 2x fewer samples in
16 out of 20 comparisons. Our analysis reveals that correct reasoning chains
exhibit significantly higher reasoning and answer confidence, justifying the
effectiveness of PiCSAR.

</details>


### [34] [Going over Fine Web with a Fine-Tooth Comb: Technical Report of Indexing Fine Web for Problematic Content Search and Retrieval](https://arxiv.org/abs/2508.21788)
*Inés Altemir Marinas,Anastasiia Kucherenko,Andrei Kucharavy*

Main category: cs.CL

TL;DR: 使用ElasticSearch架构对LLM训练数据集进行实时分析，解决大规模有害内容检测的计算挑战


<details>
  <summary>Details</summary>
Motivation: 网络爬取数据质量不高带来数据质量、安全性和道德挑战，但因计算资源限制，之前对有害内容的研究仅限于小样本

Method: 建立ElasticSearch基础的索引分析流水线，应用于SwissAI的FineWeb-2语料库（包含四种语言，1.5TB数据）

Result: 实现了快速查询性能，大部分搜索在毫秒级完成，所有查询都在2秒以内

Conclusion: 提供了实时数据集分析的实用工具，为构建更安全、更可负责的AI系统提供支持

Abstract: Large language models (LLMs) rely heavily on web-scale datasets like Common
Crawl, which provides over 80\% of training data for some modern models.
However, the indiscriminate nature of web crawling raises challenges in data
quality, safety, and ethics. Despite the critical importance of training data
quality, prior research on harmful content has been limited to small samples
due to computational constraints. This project presents a framework for
indexing and analyzing LLM training datasets using an ElasticSearch-based
pipeline. We apply it to SwissAI's FineWeb-2 corpus (1.5TB, four languages),
achieving fast query performance--most searches in milliseconds, all under 2
seconds. Our work demonstrates real-time dataset analysis, offering practical
tools for safer, more accountable AI systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [35] [Normalisation of SWIFT Message Counterparties with Feature Extraction and Clustering](https://arxiv.org/abs/2508.21081)
*Thanasis Schoinas,Benjamin Guinard,Diba Esbati,Richard Chalk*

Main category: cs.LG

TL;DR: 这篇论文提出了一种混合字符串相似性、主题建模和层次聚类的方法，用于聚类银行支付消息中的交易对方实体，解决传统自然语言模型在短文本聚类中的不足。


<details>
  <summary>Details</summary>
Motivation: 传统自然语言模型不适合聚类银行支付消息中的交易对方实体，因为这些实体名称缺乏句子结构且包含手工输入的噪声和变体。现有的模糊匹配工具不能满足调查人员在追踪资金流向时的需求。

Method: 提出了一种混合方法流水线，结合字符串相似性、主题建模、层次聚类和规则基础方法，能够处理未知聚类数量的情况。还设计了基于精准率和召回率的评估指标。

Result: 在真实标签数据集上进行测试，表现显著超过了基准的规则基础方法（关键词匹配）。该方法保持了规则系统的可解释性，同时减少了手动审查的需求。

Conclusion: 这种混合方法有效解决了银行支付消息中交易对方实体聚类的挑战，在制裁调查等场景下能够更好控制漏掉实体变体的风险，为反欺诈专业人员提供了更有效的工具。

Abstract: Short text clustering is a known use case in the text analytics community.
When the structure and content falls in the natural language domain e.g.
Twitter posts or instant messages, then natural language techniques can be
used, provided texts are of sufficient length to allow for use of (pre)trained
models to extract meaningful information, such as part-of-speech or topic
annotations. However, natural language models are not suitable for clustering
transaction counterparties, as they are found in bank payment messaging
systems, such as SWIFT. The manually typed tags are typically physical or legal
entity details, which lack sentence structure, while containing all the
variations and noise that manual entry introduces. This leaves a gap in an
investigator or counter-fraud professional's toolset when looking to augment
their knowledge of payment flow originator and beneficiary entities and trace
funds and assets. A gap that vendors traditionally try to close with fuzzy
matching tools. With these considerations in mind, we are proposing a hybrid
string similarity, topic modelling, hierarchical clustering and rule-based
pipeline to facilitate clustering of transaction counterparties, also catering
for unknown number of expected clusters. We are also devising metrics to
supplement the evaluation of the approach, based on the well-known measures of
precision and recall. Testing on a real-life labelled dataset demonstrates
significantly improved performance over a baseline rule-based ('keyword')
approach. The approach retains most of the interpretability found in rule-based
systems, as the former adds an additional level of cluster refinement to the
latter. The resulting workflow reduces the need for manual review. When only a
subset of the population needs to be investigated, such as in sanctions
investigations, the approach allows for better control of the risks of missing
entity variations.

</details>


### [36] [Model-Task Alignment Drives Distinct RL Outcomes](https://arxiv.org/abs/2508.21188)
*Haoze Wu,Cheng Wang,Wenshuo Zhao,Junxian He*

Main category: cs.LG

TL;DR: 研究发现，大语言模型强化学习中的反直觉现象（如单样本训练、不精确奖励信号、负样本训练等）仅在模型与任务已高度对齐时有效，而在更具挑战性的场景中，标准RL方法仍然更可靠。


<details>
  <summary>Details</summary>
Motivation: 近期研究发现LLM强化学习中出现了一系列反直觉现象，但这些现象的有效条件尚不明确，需要系统研究其适用范围和限制。

Method: 通过系统性的实验验证，在不同模型架构和任务领域中检验各种反直觉主张，重点关注模型-任务对齐程度（用pass@k准确率衡量）的影响。

Result: 研究表明，许多反直觉结果只在模型与任务已高度对齐时出现，而在更具挑战性的情况下，这些技术无法驱动有效学习，标准RL方法仍然有效。

Conclusion: 模型-任务对齐程度是区分RL观察结果的关键因素，标准RL训练在各种设置中保持稳健，而反直觉方法仅在特定条件下有效。

Abstract: Recent advances in applying reinforcement learning (RL) to large language
models (LLMs) have led to substantial progress. In particular, a series of
remarkable yet often counterintuitive phenomena have been reported in LLMs,
exhibiting patterns not typically observed in traditional RL settings. For
example, notable claims include that a single training example can match the
performance achieved with an entire dataset, that the reward signal does not
need to be very accurate, and that training solely with negative samples can
match or even surpass sophisticated reward-based methods. However, the precise
conditions under which these observations hold - and, critically, when they
fail - remain unclear. In this work, we identify a key factor that
differentiates RL observations: whether the pretrained model already exhibits
strong Model-Task Alignment, as measured by pass@k accuracy on the evaluated
task. Through a systematic and comprehensive examination of a series of
counterintuitive claims, supported by rigorous experimental validation across
different model architectures and task domains, our findings show that while
standard RL training remains consistently robust across settings, many of these
counterintuitive results arise only when the model and task already exhibit
strong model-task alignment. In contrast, these techniques fail to drive
substantial learning in more challenging regimes, where standard RL methods
remain effective.

</details>


### [37] [Accept or Deny? Evaluating LLM Fairness and Performance in Loan Approval across Table-to-Text Serialization Approaches](https://arxiv.org/abs/2508.21512)
*Israel Abebe Azime,Deborah D. Kanubala,Tejumade Afonja,Mario Fritz,Isabel Valera,Dietrich Klakow,Philipp Slusallek*

Main category: cs.LG

TL;DR: 评估LLM在贷款审批表格数据上的性能和公平性，发现序列化格式选择显著影响结果，ICL提升性能但对公平性影响不一


<details>
  <summary>Details</summary>
Motivation: 随着LLM在贷款审批等高风险决策任务中的应用增加，需要评估其处理表格数据的性能和公平性表现

Method: 在加纳、德国和美国三个地区的序列化贷款审批数据集上评估LLM的零样本和上下文学习能力，比较不同序列化格式的影响

Result: 序列化格式选择显著影响性能和公平性，某些格式如GReat和LIFT能提高F1分数但加剧公平性差距；ICL相对零样本基线提升性能4.9-59.6%，但对公平性的影响因数据集而异

Conclusion: 需要有效的表格数据表示方法和公平性感知模型来提高LLM在金融决策中的可靠性

Abstract: Large Language Models (LLMs) are increasingly employed in high-stakes
decision-making tasks, such as loan approvals. While their applications expand
across domains, LLMs struggle to process tabular data, ensuring fairness and
delivering reliable predictions. In this work, we assess the performance and
fairness of LLMs on serialized loan approval datasets from three geographically
distinct regions: Ghana, Germany, and the United States. Our evaluation focuses
on the model's zero-shot and in-context learning (ICL) capabilities. Our
results reveal that the choice of serialization (Serialization refers to the
process of converting tabular data into text formats suitable for processing by
LLMs.) format significantly affects both performance and fairness in LLMs, with
certain formats such as GReat and LIFT yielding higher F1 scores but
exacerbating fairness disparities. Notably, while ICL improved model
performance by 4.9-59.6% relative to zero-shot baselines, its effect on
fairness varied considerably across datasets. Our work underscores the
importance of effective tabular data representation methods and fairness-aware
models to improve the reliability of LLMs in financial decision-making.

</details>


### [38] [Summarize-Exemplify-Reflect: Data-driven Insight Distillation Empowers LLMs for Few-shot Tabular Classification](https://arxiv.org/abs/2508.21561)
*Yifei Yuan,Jiatong Li,Weijia Zhang,Mohammad Aliannejadi,Evangelos Kanoulas,Renjun Hu*

Main category: cs.LG

TL;DR: InsightTab是一个基于洞察蒸馏的框架，通过规则总结、策略性示例和反思学习，提升大语言模型在表格分类任务中的表现，在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明大语言模型在少样本表格分类中具有潜力，但结构化数据的变异性带来了挑战。需要将数据蒸馏为可操作的洞察来提升分类效果。

Method: 提出InsightTab框架，采用分而治之、先易后难和反思学习的原则，结合大语言模型和数据建模技术进行规则总结、策略性示例选择和洞察反思。

Result: 在9个数据集上的广泛评估显示，InsightTab相比最先进方法取得了持续改进。消融研究验证了原则指导的蒸馏过程的有效性。

Conclusion: InsightTab通过洞察蒸馏使大语言模型能够更好地将通用知识与特定表格任务需求对齐，有效利用标注数据并管理偏差，在少样本表格分类中表现出色。

Abstract: Recent studies show the promise of large language models (LLMs) for few-shot
tabular classification but highlight challenges due to the variability in
structured data. To address this, we propose distilling data into actionable
insights to enable robust and effective classification by LLMs. Drawing
inspiration from human learning processes, we introduce InsightTab, an insight
distillation framework guided by principles of divide-and-conquer, easy-first,
and reflective learning. Our approach integrates rule summarization, strategic
exemplification, and insight reflection through deep collaboration between LLMs
and data modeling techniques. The obtained insights enable LLMs to better align
their general knowledge and capabilities with the particular requirements of
specific tabular tasks. We extensively evaluate InsightTab on nine datasets.
The results demonstrate consistent improvement over state-of-the-art methods.
Ablation studies further validate the principle-guided distillation process,
while analyses emphasize InsightTab's effectiveness in leveraging labeled data
and managing bias.

</details>


<div id='physics.ed-ph'></div>

# physics.ed-ph [[Back]](#toc)

### [39] [From Canonical to Complex: Benchmarking LLM Capabilities in Undergraduate Thermodynamics](https://arxiv.org/abs/2508.21452)
*Anna Geißler,Luca-Sophie Bien,Friedrich Schöppler,Tobias Hertel*

Main category: physics.ed-ph

TL;DR: 大型语言模型在本科热力学教学中尚未达到95%的能力阈值，最佳模型准确率仅为82%，特别是在图像推理和不可逆过程场景中存在显著不足


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在本科科学教育中作为无监督辅导工具的可行性，特别是在需要原则性推理的热力学领域

Method: 创建UTQA基准测试，包含50个本科热力学问题，涵盖理想气体过程、可逆性和图表解释，测试2025年领先语言模型的表现

Result: 所有测试模型均未达到95%的能力阈值，最佳模型准确率为82%，文本项目表现优于图像推理任务，后者接近随机猜测水平

Conclusion: 当前大型语言模型在热力学领域的无监督辅导应用中尚不成熟，特别是在有限速率/不可逆场景和视觉特征与热力学含义绑定方面存在明显差距

Abstract: Large language models (LLMs) are increasingly considered as tutoring aids in
science education. Yet their readiness for unsupervised use in undergraduate
instruction remains uncertain, as reliable teaching requires more than fluent
recall: it demands consistent, principle-grounded reasoning. Thermodynamics,
with its compact laws and subtle distinctions between state and path functions,
reversibility, and entropy, provides an ideal testbed for evaluating such
capabilities. Here we present UTQA, a 50-item undergraduate thermodynamics
question answering benchmark, covering ideal-gas processes, reversibility, and
diagram interpretation. No leading 2025-era model exceeded our 95\% competence
threshold: the best LLMs achieved 82\% accuracy, with text-only items
performing better than image reasoning tasks, which often fell to chance
levels. Prompt phrasing and syntactic complexity showed modest to little
correlation with performance. The gap concentrates in finite-rate/irreversible
scenarios and in binding visual features to thermodynamic meaning, indicating
that current LLMs are not yet suitable for unsupervised tutoring in this
domain.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [40] [CrossTL: A Universal Programming Language Translator with Unified Intermediate Representation](https://arxiv.org/abs/2508.21256)
*Nripesh Niketan,Vaatsalya Shrivastva*

Main category: cs.PL

TL;DR: CrossTL是一个通用的编程语言翻译器，通过统一的中间表示CrossGL实现多种编程语言之间的双向翻译，支持CUDA、HIP、Metal、HLSL、GLSL、SPIR-V、Rust和Mojo等语言。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要为每种语言对单独构建翻译器，导致复杂度呈指数级增长。需要一种统一的解决方案来简化多语言翻译。

Method: 使用语言特定的词法分析器/解析器将源代码转换为AST，通过双向CrossGL翻译模块实现代码导入和目标代码生成，采用模块化架构支持扩展性。

Result: 通过全面的跨编程领域评估，在所有支持的平台上实现了成功的编译和执行，证明了通用代码翻译的实用可行性。

Conclusion: CrossTL代表了向语言无关编程迈出的重要一步，实现了"一次编写，到处部署"的开发模式，统一IR设计使得添加新语言只需最小的工作量。

Abstract: We present CrossTL, a universal programming language translator enabling
bidirectional translation between multiple languages through a unified
intermediate representation called CrossGL. Traditional approaches require
separate translators for each language pair, leading to exponential complexity
growth. CrossTL uses a single universal IR to facilitate translations between
CUDA, HIP, Metal, DirectX HLSL, OpenGL GLSL, Vulkan SPIR-V, Rust, and Mojo,
with Slang support in development. Our system consists of: language-specific
lexers/parsers converting source code to ASTs, bidirectional CrossGL
translation modules implementing ToCrossGLConverter classes for importing code
and CodeGen classes for target generation, and comprehensive backend
implementations handling full translation pipelines. We demonstrate
effectiveness through comprehensive evaluation across programming domains,
achieving successful compilation and execution across all supported backends.
The universal IR design enables adding new languages with minimal effort,
requiring only language-specific frontend/backend components. Our contributions
include: (1) a unified IR capturing semantics of multiple programming
paradigms, (2) a modular architecture enabling extensibility, (3) a
comprehensive framework supporting GPU compute, graphics programming, and
systems languages, and (4) empirical validation demonstrating practical
viability of universal code translation. CrossTL represents a significant step
toward language-agnostic programming, enabling write-once, deploy-everywhere
development.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [41] [Stairway to Fairness: Connecting Group and Individual Fairness](https://arxiv.org/abs/2508.21334)
*Theresia Veronika Rampisela,Maria Maistro,Tuukka Ruotsalo,Falk Scholer,Christina Lioma*

Main category: cs.IR

TL;DR: 本文研究了推荐系统中群体公平性和个体公平性之间的关系，发现高度群体公平的推荐可能对个体非常不公平


<details>
  <summary>Details</summary>
Motivation: 推荐系统中的公平性通常分为群体公平和个体公平，但两者之间的关系缺乏科学理解，现有研究使用不同的评估指标，无法进行有效比较

Method: 通过全面比较可用于两种公平性类型的评估指标，在3个数据集上对8个运行进行实验分析

Result: 实验表明，对群体高度公平的推荐可能对个体非常不公平

Conclusion: 这一发现对推荐系统从业者改进系统公平性具有重要价值，揭示了群体公平与个体公平之间的潜在冲突

Abstract: Fairness in recommender systems (RSs) is commonly categorised into group
fairness and individual fairness. However, there is no established scientific
understanding of the relationship between the two fairness types, as prior work
on both types has used different evaluation measures or evaluation objectives
for each fairness type, thereby not allowing for a proper comparison of the
two. As a result, it is currently not known how increasing one type of fairness
may affect the other. To fill this gap, we study the relationship of group and
individual fairness through a comprehensive comparison of evaluation measures
that can be used for both fairness types. Our experiments with 8 runs across 3
datasets show that recommendations that are highly fair for groups can be very
unfair for individuals. Our finding is novel and useful for RS practitioners
aiming to improve the fairness of their systems. Our code is available at:
https://github.com/theresiavr/stairway-to-fairness.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [42] [Quantum-Enhanced Natural Language Generation: A Multi-Model Framework with Hybrid Quantum-Classical Architectures](https://arxiv.org/abs/2508.21332)
*Chi-Sheng Chen,En-Jui Kuo*

Main category: quant-ph

TL;DR: 这篇论文系统评估了量子文本生成模型与传统Transformer模型的性能对比，发现Transformer在总体上仍最优，但量子模型在特定场景下也表现竞争力


<details>
  <summary>Details</summary>
Motivation: 因对量子计算在自然语言处理领域应用的日益增长兴趣，需要系统性地评估量子文本生成模型的性能

Method: 使用5种不同模型（Transformer、QKSAN、QRWKV、QASA）在5个多样化数据集上进行对比实验，采用语言模型评估指标如perplexity、BLEU、词汇多样性、重复率等

Result: Transformer模型保持总体优势（平均perplexity 1.21，BLEU-1 0.2895），但量子模型在特定方面表现竞争力：QKSAN实现了零重复率和竞争性BLEU-1分数（0.2800），QRWKV在某些任务中实现了完美的词汇多样性

Conclusion: 传统Transformer模型在文本生成任务中仍然占据优势，但量子受启发的模型在特定性能指标上显示出竞争力，为量子计算在NLP领域的应用提供了有价值的参考

Abstract: This paper presents a comprehensive evaluation of quantum text generation
models against traditional Transformer/MLP architectures, addressing the
growing interest in quantum computing applications for natural language
processing. We conduct systematic experiments comparing five distinct models:
Transformer (baseline), Quantum Kernel Self-Attention Network (QKSAN), Quantum
RWKV (QRWKV), and Quantum Attention Sequence Architecture (QASA) across five
diverse datasets including simple sentences, short stories, quantum phrases,
haiku poetry, and proverbs. Our evaluation employs multiple metrics including
perplexity, BLEU scores, vocabulary diversity, repetition rates, and fluency
measures to assess different aspects of text generation quality. The
experimental results reveal that while traditional Transformer models maintain
overall superiority with the lowest average perplexity (1.21) and highest
BLEU-1 score (0.2895), quantum-inspired models demonstrate competitive
performance in specific scenarios. Notably, QKSAN achieves a competitive BLEU-1
score of 0.2800 while maintaining zero repetition rates, and QRWKV demonstrates
perfect vocabulary diversity (Distinct-1 = 1.000) in certain tasks.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [43] [Fuzzy, Symbolic, and Contextual: Enhancing LLM Instruction via Cognitive Scaffolding](https://arxiv.org/abs/2508.21204)
*Vanessa Figueiredo*

Main category: cs.AI

TL;DR: 该研究探讨了架构归纳偏置如何影响大语言模型在指导性对话中的认知行为，通过引入符号支架机制和短期记忆模式来促进结构化推理，实验表明完整系统优于基线变体。


<details>
  <summary>Details</summary>
Motivation: 研究架构归纳偏置对大语言模型在苏格拉底式辅导中认知行为的影响，探索如何通过架构设计来塑造模型的教学策略。

Method: 引入符号支架机制和短期记忆模式，通过五个系统变体的对照消融实验，使用专家设计的评分标准和基于LLM的评估框架进行系统比较。

Result: 完整系统在所有评估维度上一致优于基线变体，移除记忆或符号结构会降低关键认知行为（抽象、自适应探测和概念连续性）。

Conclusion: 架构支架能够可靠地塑造大语言模型中涌现的教学策略，支持处理层面的解释，表明架构设计对模型认知行为具有重要影响。

Abstract: We study how architectural inductive biases influence the cognitive behavior
of large language models (LLMs) in instructional dialogue. We introduce a
symbolic scaffolding mechanism paired with a short-term memory schema designed
to promote adaptive, structured reasoning in Socratic tutoring. Using
controlled ablation across five system variants, we evaluate model outputs via
expert-designed rubrics covering scaffolding, responsiveness, symbolic
reasoning, and conversational memory. We present preliminary results using an
LLM-based evaluation framework aligned to a cognitively grounded rubric. This
enables scalable, systematic comparisons across architectural variants in
early-stage experimentation. The preliminary results show that our full system
consistently outperforms baseline variants. Analysis reveals that removing
memory or symbolic structure degrades key cognitive behaviors, including
abstraction, adaptive probing, and conceptual continuity. These findings
support a processing-level account in which architectural scaffolds can
reliably shape emergent instructional strategies in LLMs.

</details>


### [44] [AHELM: A Holistic Evaluation of Audio-Language Models](https://arxiv.org/abs/2508.21376)
*Tony Lee,Haoqin Tu,Chi Heem Wong,Zijun Wang,Siwei Yang,Yifan Mai,Yuyin Zhou,Cihang Xie,Percy Liang*

Main category: cs.AI

TL;DR: AHELM是一个新的音频-语言模型基准测试，整合了多个数据集来全面评估10个重要方面，包括感知、推理、公平性等，并标准化了评估流程。测试了14个模型，发现Gemini 2.5 Pro在多个方面表现最佳但存在公平性问题，基线系统表现也相当不错。


<details>
  <summary>Details</summary>
Motivation: 当前音频-语言模型评估缺乏标准化基准，大多数基准只测试一两个能力，且忽略了公平性、安全性等重要方面。不同评估使用不同的提示方法和推理参数，导致模型间难以比较。

Method: 引入AHELM基准，整合多个数据集（包括两个新的合成音频-文本数据集PARADE和CoRe-Bench），标准化提示、推理参数和评估指标，在10个重要方面全面评估音频-语言模型的性能。

Result: 测试了14个开源和闭源API模型，Gemini 2.5 Pro在10个方面中的5个排名第一，但在ASR任务中表现出群体不公平性（p=0.01）。基线系统表现良好，其中一个仅具有语音转文本功能的系统总体排名第5。

Conclusion: AHELM提供了一个全面、标准化的音频-语言模型评估框架，揭示了当前模型的优势和不足，特别是公平性问题。该基准将持续更新，旨在促进音频-语言模型的健康发展。

Abstract: Evaluations of audio-language models (ALMs) -- multimodal models that take
interleaved audio and text as input and output text -- are hindered by the lack
of standardized benchmarks; most benchmarks measure only one or two
capabilities and omit evaluative aspects such as fairness or safety.
Furthermore, comparison across models is difficult as separate evaluations test
a limited number of models and use different prompting methods and inference
parameters. To address these shortfalls, we introduce AHELM, a benchmark that
aggregates various datasets -- including 2 new synthetic audio-text datasets
called PARADE, which evaluates the ALMs on avoiding stereotypes, and
CoRe-Bench, which measures reasoning over conversational audio through
inferential multi-turn question answering -- to holistically measure the
performance of ALMs across 10 aspects we have identified as important to the
development and usage of ALMs: audio perception, knowledge, reasoning, emotion
detection, bias, fairness, multilinguality, robustness, toxicity, and safety.
We also standardize the prompts, inference parameters, and evaluation metrics
to ensure equitable comparisons across models. We test 14 open-weight and
closed-API ALMs from 3 developers and 3 additional simple baseline systems each
consisting of an automatic speech recognizer and a language model. Our results
show that while Gemini 2.5 Pro ranks top in 5 out of 10 aspects, it exhibits
group unfairness ($p=0.01$) on ASR tasks whereas most of the other models do
not. We also find that the baseline systems perform reasonably well on AHELM,
with one ranking 5th overall despite having only speech-to-text capabilities.
For transparency, all raw prompts, model generations, and outputs are available
on our website at https://crfm.stanford.edu/helm/audio/v1.0.0. AHELM is
intended to be a living benchmark and new datasets and models will be added
over time.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [45] [Designing Smarter Conversational Agents for Kids: Lessons from Cognitive Work and Means-Ends Analyses](https://arxiv.org/abs/2508.21209)
*Vanessa Figueiredo*

Main category: cs.HC

TL;DR: 该研究通过两项研究探讨巴西儿童如何使用对话代理进行学习，并开发结构化脚手架来提升交互效果。研究发现结构化提示方法在可读性、问题深度和连贯性方面优于非结构化基线。


<details>
  <summary>Details</summary>
Motivation: 研究巴西儿童（9-11岁）如何使用对话代理进行学校作业、探索和娱乐，以及如何通过结构化脚手架来增强这些交互体验。

Method: 研究1：对23名参与者进行7周在线调查，使用访谈、观察和认知工作分析方法；研究2：使用GPT-4o-mini在1200个模拟儿童-对话代理交换上进行测试，比较结构化提示方法与非结构化基线。

Result: 识别出对话代理的三种功能：学校、探索、娱乐；结构化提示方法在可读性、问题数量/深度/多样性和连贯性方面显示出优势。

Conclusion: 提出了设计建议：脚手架式对话树、儿童专用个性化配置文件、看护人策划内容；贡献包括首个巴西儿童的CWA应用、儿童-对话代理信息流实证框架和有效的LLM脚手架"配方"。

Abstract: This paper presents two studies on how Brazilian children (ages 9--11) use
conversational agents (CAs) for schoolwork, discovery, and entertainment, and
how structured scaffolds can enhance these interactions. In Study 1, a
seven-week online investigation with 23 participants (children, parents,
teachers) employed interviews, observations, and Cognitive Work Analysis to map
children's information-processing flows, the role of more knowledgeable others,
functional uses, contextual goals, and interaction patterns to inform
conversation-tree design. We identified three CA functions: School, Discovery,
Entertainment, and derived ``recipe'' scaffolds mirroring parent-child support.
In Study 2, we prompted GPT-4o-mini on 1,200 simulated child-CA exchanges,
comparing conversation-tree recipes based on structured-prompting to an
unstructured baseline. Quantitative evaluation of readability, question
count/depth/diversity, and coherence revealed gains for the recipe approach.
Building on these findings, we offer design recommendations: scaffolded
conversation-trees, child-dedicated profiles for personalized context, and
caregiver-curated content. Our contributions include the first CWA application
with Brazilian children, an empirical framework of child-CA information flows,
and an LLM-scaffolding ``recipe'' (i.e., structured-prompting) for effective,
scaffolded learning.

</details>


### [46] [Morae: Proactively Pausing UI Agents for User Choices](https://arxiv.org/abs/2508.21456)
*Yi-Hao Peng,Dingzeyu Li,Jeffrey P. Bigham,Amy Pavel*

Main category: cs.HC

TL;DR: Morae是一个UI代理系统，通过识别任务执行中的决策点并暂停让用户选择，提高盲人和低视力用户的控制权。


<details>
  <summary>Details</summary>
Motivation: 当前UI代理通常端到端执行任务而不让用户参与关键选择，减少了用户控制权。研究发现BLV用户需要更多参与决策过程。

Method: 使用大型多模态模型解释用户查询、UI代码和截图，在需要选择时提示用户澄清，采用混合主动方法。

Result: 在真实网页任务研究中，Morae帮助用户完成更多任务并选择更符合偏好的选项，优于包括OpenAI Operator在内的基线代理。

Conclusion: 这项工作展示了混合主动方法的价值，用户既能受益于UI代理的自动化，又能表达自己的偏好。

Abstract: User interface (UI) agents promise to make inaccessible or complex UIs easier
to access for blind and low-vision (BLV) users. However, current UI agents
typically perform tasks end-to-end without involving users in critical choices
or making them aware of important contextual information, thus reducing user
agency. For example, in our field study, a BLV participant asked to buy the
cheapest available sparkling water, and the agent automatically chose one from
several equally priced options, without mentioning alternative products with
different flavors or better ratings. To address this problem, we introduce
Morae, a UI agent that automatically identifies decision points during task
execution and pauses so that users can make choices. Morae uses large
multimodal models to interpret user queries alongside UI code and screenshots,
and prompt users for clarification when there is a choice to be made. In a
study over real-world web tasks with BLV participants, Morae helped users
complete more tasks and select options that better matched their preferences,
as compared to baseline agents, including OpenAI Operator. More broadly, this
work exemplifies a mixed-initiative approach in which users benefit from the
automation of UI agents while being able to express their preferences.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [47] [Database Normalization via Dual-LLM Self-Refinement](https://arxiv.org/abs/2508.17693)
*Eunjae Jo,Nakyung Lee,Gyuyeong Kim*

Main category: cs.DB

TL;DR: Miffie是一个基于大语言模型的数据库自动化规范化框架，通过双模型自优化架构实现高精度无人工干预的数据规范化。


<details>
  <summary>Details</summary>
Motivation: 传统数据库规范化需要数据工程师手动完成，耗时且容易出错，需要自动化解决方案来保持数据完整性。

Method: 采用双模型自优化架构：生成模块负责消除异常，验证模块提供反馈，通过零样本提示设计实现高精度和成本效率。

Result: 实验结果表明Miffie能够规范化复杂数据库模式并保持高准确性。

Conclusion: Miffie框架成功实现了自动化数据库规范化，为大语言模型在数据工程领域的应用提供了有效解决方案。

Abstract: Database normalization is crucial to preserving data integrity. However, it
is time-consuming and error-prone, as it is typically performed manually by
data engineers. To this end, we present Miffie, a database normalization
framework that leverages the capability of large language models. Miffie
enables automated data normalization without human effort while preserving high
accuracy. The core of Miffie is a dual-model self-refinement architecture that
combines the best-performing models for normalized schema generation and
verification, respectively. The generation module eliminates anomalies based on
the feedback of the verification module until the output schema satisfies the
requirement for normalization. We also carefully design task-specific zero-shot
prompts to guide the models for achieving both high accuracy and cost
efficiency. Experimental results show that Miffie can normalize complex
database schemas while maintaining high accuracy.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [48] [Why Stop at Words? Unveiling the Bigger Picture through Line-Level OCR](https://arxiv.org/abs/2508.21693)
*Shashank Vempati,Nishit Anand,Gaurav Talebailkar,Arpan Garai,Chetan Arora*

Main category: cs.CV

TL;DR: 这篇论文提出了从单词级到行级OCR的进阶方法，通过直接处理整行文本来避免单词分割错误，并利用更大的语言上下文来提高识别准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统字符级OCR存在字符分割错误问题，而现代单词级OCR将精度瓶颈转移到了单词分割。为了解决这个问题，需要向行级OCR迁移。

Method: 提出了一种行级OCR方法，直接将整行文本输入模型，输出字符序列。这种方法避免了单词分割步骤，并提供更大的句子上下文来利用语言模型。

Result: 实验结果显示，该方法实现了结束到结束准确性提升5.4%，效率提升4倍。还创建了包含251张英文页面图像的行级注释数据集。

Conclusion: 行级OCR方法在准确性和效率方面都显著优于单词级方法，特别适用于文档图像处理。随着大语言模型的发展，该方法还有潜力利用这些进步。

Abstract: Conventional optical character recognition (OCR) techniques segmented each
character and then recognized. This made them prone to error in character
segmentation, and devoid of context to exploit language models. Advances in
sequence to sequence translation in last decade led to modern techniques first
detecting words and then inputting one word at a time to a model to directly
output full words as sequence of characters. This allowed better utilization of
language models and bypass error-prone character segmentation step. We observe
that the above transition in style has moved the bottleneck in accuracy to word
segmentation. Hence, in this paper, we propose a natural and logical
progression from word level OCR to line-level OCR. The proposal allows to
bypass errors in word detection, and provides larger sentence context for
better utilization of language models. We show that the proposed technique not
only improves the accuracy but also efficiency of OCR. Despite our thorough
literature survey, we did not find any public dataset to train and benchmark
such shift from word to line-level OCR. Hence, we also contribute a
meticulously curated dataset of 251 English page images with line-level
annotations. Our experimentation revealed a notable end-to-end accuracy
improvement of 5.4%, underscoring the potential benefits of transitioning
towards line-level OCR, especially for document images. We also report a 4
times improvement in efficiency compared to word-based pipelines. With
continuous improvements in large language models, our methodology also holds
potential to exploit such advances. Project Website:
https://nishitanand.github.io/line-level-ocr-website

</details>
