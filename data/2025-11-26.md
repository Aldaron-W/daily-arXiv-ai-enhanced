<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 34]
- [cs.CV](#cs.CV) [Total: 7]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.LG](#cs.LG) [Total: 7]
- [cs.DC](#cs.DC) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Efficient Multi-Hop Question Answering over Knowledge Graphs via LLM Planning and Embedding-Guided Search](https://arxiv.org/abs/2511.19648)
*Manil Shrestha,Edward Kim*

Main category: cs.CL

TL;DR: 提出了两种混合算法来解决多跳知识图谱问答中的效率和可验证性问题：LLM引导规划使用单次LLM调用预测关系序列，嵌入引导神经搜索完全消除LLM调用，通过轻量级边缘评分器融合文本和图嵌入。


<details>
  <summary>Details</summary>
Motivation: 多跳知识图谱问答面临组合爆炸问题，现有方法依赖昂贵的LLM推理进行实体链接和路径排序，限制了实际部署，且LLM生成的答案缺乏结构化知识的可验证基础。

Method: 1) LLM引导规划：单次LLM调用预测关系序列，通过广度优先搜索执行；2) 嵌入引导神经搜索：完全消除LLM调用，使用6.7M参数的轻量级边缘评分器融合文本和图嵌入；通过知识蒸馏将规划能力压缩到4B参数模型中。

Result: 在MetaQA上的评估显示：LLM引导规划达到接近完美的准确率（micro-F1 > 0.90），所有答案都基于知识图谱；嵌入引导神经搜索实现100倍以上加速，同时保持竞争力准确率；结构化规划比直接答案生成更具可迁移性。

Conclusion: 可验证的多跳推理不需要在推理时使用大规模模型，而是需要结合符号结构与学习表示的适当架构归纳偏差。

Abstract: Multi-hop question answering over knowledge graphs remains computationally challenging due to the combinatorial explosion of possible reasoning paths. Recent approaches rely on expensive Large Language Model (LLM) inference for both entity linking and path ranking, limiting their practical deployment. Additionally, LLM-generated answers often lack verifiable grounding in structured knowledge. We present two complementary hybrid algorithms that address both efficiency and verifiability: (1) LLM-Guided Planning that uses a single LLM call to predict relation sequences executed via breadth-first search, achieving near-perfect accuracy (micro-F1 > 0.90) while ensuring all answers are grounded in the knowledge graph, and (2) Embedding-Guided Neural Search that eliminates LLM calls entirely by fusing text and graph embeddings through a lightweight 6.7M-parameter edge scorer, achieving over 100 times speedup with competitive accuracy. Through knowledge distillation, we compress planning capability into a 4B-parameter model that matches large-model performance at zero API cost. Evaluation on MetaQA demonstrates that grounded reasoning consistently outperforms ungrounded generation, with structured planning proving more transferable than direct answer generation. Our results show that verifiable multi-hop reasoning does not require massive models at inference time, but rather the right architectural inductive biases combining symbolic structure with learned representations.

</details>


### [2] [Can LLMs Faithfully Explain Themselves in Low-Resource Languages? A Case Study on Emotion Detection in Persian](https://arxiv.org/abs/2511.19719)
*Mobina Mehrazar,Mohammad Amin Yousefi,Parisa Abolfath Beygi,Behnam Bahrak*

Main category: cs.CL

TL;DR: 评估波斯语情感分类中LLM生成解释的忠实性，发现模型解释与人类判断存在差异，强调需要更可靠的多语言解释方法。


<details>
  <summary>Details</summary>
Motivation: LLM在低资源语言中生成自解释的忠实性存在问题，需要评估其在波斯语情感分类中的解释可靠性。

Method: 通过比较模型识别的影响词与人工标注，使用基于token级对数概率的置信度评估忠实性，测试两种提示策略（预测-解释和解释-预测）。

Result: LLM分类性能强但生成解释常偏离忠实推理，模型间一致性高于与人类判断的一致性。

Conclusion: 当前解释方法和指标存在局限，需要更稳健的方法确保LLM在多语言和低资源环境中的可靠性。

Abstract: Large language models (LLMs) are increasingly used to generate self-explanations alongside their predictions, a practice that raises concerns about the faithfulness of these explanations, especially in low-resource languages. This study evaluates the faithfulness of LLM-generated explanations in the context of emotion classification in Persian, a low-resource language, by comparing the influential words identified by the model against those identified by human annotators. We assess faithfulness using confidence scores derived from token-level log-probabilities. Two prompting strategies, differing in the order of explanation and prediction (Predict-then-Explain and Explain-then-Predict), are tested for their impact on explanation faithfulness. Our results reveal that while LLMs achieve strong classification performance, their generated explanations often diverge from faithful reasoning, showing greater agreement with each other than with human judgments. These results highlight the limitations of current explanation methods and metrics, emphasizing the need for more robust approaches to ensure LLM reliability in multilingual and low-resource contexts.

</details>


### [3] [Comparative Analysis of LoRA-Adapted Embedding Models for Clinical Cardiology Text Representation](https://arxiv.org/abs/2511.19739)
*Richard J. Young,Alice M. Matthews*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Domain-specific text embeddings are critical for clinical natural language processing, yet systematic comparisons across model architectures remain limited. This study evaluates ten transformer-based embedding models adapted for cardiology through Low-Rank Adaptation (LoRA) fine-tuning on 106,535 cardiology text pairs derived from authoritative medical textbooks. Results demonstrate that encoder-only architectures, particularly BioLinkBERT, achieve superior domain-specific performance (separation score: 0.510) compared to larger decoder-based models, while requiring significantly fewer computational resources. The findings challenge the assumption that larger language models necessarily produce better domain-specific embeddings and provide practical guidance for clinical NLP system development. All models, training code, and evaluation datasets are publicly available to support reproducible research in medical informatics.

</details>


### [4] [What does it mean to understand language?](https://arxiv.org/abs/2511.19757)
*Colton Casto,Anna Ivanova,Evelina Fedorenko,Nancy Kanwisher*

Main category: cs.CL

TL;DR: 语言理解需要构建丰富的心理模型，而不仅仅是提取表层意义。由于大脑核心语言系统的处理能力有限，深度理解语言需要将信息从语言系统输出到其他脑区来处理感知、运动表征、构建心理模型和存储知识。


<details>
  <summary>Details</summary>
Motivation: 探索语言理解的深层认知和神经机制，揭示大脑如何通过跨系统协作来实现真正的语言理解，而不仅仅是表层语言处理。

Method: 回顾现有证据并提出假设，认为认知神经科学的进展为直接检验这一假设提供了概念基础和方法。

Result: 提出了语言理解需要跨系统信息输出的新框架，为理解语言理解的认知和神经基础开辟了新策略。

Conclusion: 深度语言理解依赖于语言系统与其他脑区的协作，这一假设可以通过现代认知神经科学方法直接检验，从而揭示语言理解的本质。

Abstract: Language understanding entails not just extracting the surface-level meaning of the linguistic input, but constructing rich mental models of the situation it describes. Here we propose that because processing within the brain's core language system is fundamentally limited, deeply understanding language requires exporting information from the language system to other brain regions that compute perceptual and motor representations, construct mental models, and store our world knowledge and autobiographical memories. We review the existing evidence for this hypothesis, and argue that recent progress in cognitive neuroscience provides both the conceptual foundation and the methods to directly test it, thus opening up a new strategy to reveal what it means, cognitively and neurally, to understand language.

</details>


### [5] [Gender Bias in Emotion Recognition by Large Language Models](https://arxiv.org/abs/2511.19785)
*Maureen Herbert,Katie Sun,Angelica Lim,Yasaman Etesam*

Main category: cs.CL

TL;DR: 本文研究LLMs在情感心智理论中的性别偏见问题，并提出多种去偏策略，发现基于训练的方法比提示工程更有效。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在日常生活中的广泛应用，评估和确保其公平性变得尤为重要，特别是在情感心智理论领域是否存在性别偏见。

Method: 通过向LLMs描述人物及其环境并询问"这个人感觉如何？"来检测性别偏见，并评估了多种去偏策略，包括提示工程和基于训练的方法。

Result: 研究发现LLMs在情感心智理论中存在性别偏见，且基于训练的去偏干预比仅使用推理时提示工程方法能更有效地减少偏见。

Conclusion: 要实现有意义的偏见减少，需要采用基于训练的方法，而不能仅仅依赖推理时的提示工程策略。

Abstract: The rapid advancement of large language models (LLMs) and their growing integration into daily life underscore the importance of evaluating and ensuring their fairness. In this work, we examine fairness within the domain of emotional theory of mind, investigating whether LLMs exhibit gender biases when presented with a description of a person and their environment and asked, "How does this person feel?". Furthermore, we propose and evaluate several debiasing strategies, demonstrating that achieving meaningful reductions in bias requires training based interventions rather than relying solely on inference-time prompt-based approaches such as prompt engineering.

</details>


### [6] [Breaking Bad: Norms for Valence, Arousal, and Dominance for over 10k English Multiword Expressions](https://arxiv.org/abs/2511.19816)
*Saif M. Mohammad*

Main category: cs.CL

TL;DR: 扩展了NRC VAD词典，新增10k个英语多词表达和25k个单词的VAD评分，提高了覆盖范围并验证了可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有词典如2018年发布的NRC VAD词典仅包含单词的VAD关联评分，需要扩展到多词表达和更新词汇。

Method: 收集人类对10k个多词表达及其组成词的效价、唤醒度和支配度评分，并增加2018年后更常见的单字词。

Result: 新词典包含10k个多词表达和25k个单词，评分高度可靠，可用于分析多词表达的情感特征和情感组合性。

Conclusion: NRC VAD词典v2为NLP、心理学、公共卫生等领域的研究提供了重要资源，可免费获取。

Abstract: Factor analysis studies have shown that the primary dimensions of word meaning are Valence (V), Arousal (A), and Dominance (D). Existing lexicons such as the NRC VAD Lexicon, published in 2018, include VAD association ratings for words. Here, we present a complement to it, which has human ratings of valence, arousal, and dominance for 10k English Multiword Expressions (MWEs) and their constituent words. We also increase the coverage of unigrams, especially words that have become more common since 2018. In all, the new NRC VAD Lexicon v2 now has entries for 10k MWEs and 25k words, in addition to the entries in v1. We show that the associations are highly reliable. We use the lexicon to examine emotional characteristics of MWEs, including: 1. The degree to which MWEs (idioms, noun compounds, and verb particle constructions) exhibit strong emotionality; 2. The degree of emotional compositionality in MWEs. The lexicon enables a wide variety of research in NLP, Psychology, Public Health, Digital Humanities, and Social Sciences. The NRC VAD Lexicon v2 is freely available through the project webpage: http://saifmohammad.com/WebPages/nrc-vad.html

</details>


### [7] [Language-Independent Sentiment Labelling with Distant Supervision: A Case Study for English, Sepedi and Setswana](https://arxiv.org/abs/2511.19818)
*Koena Ronny Mabokela,Tim Schlippe,Mpho Raborife,Turgay Celik*

Main category: cs.CL

TL;DR: 提出了一种基于表情符号和情感词汇的自动情感标注方法，用于解决非洲低资源语言情感分析中标注数据缺乏的问题。


<details>
  <summary>Details</summary>
Motivation: 非洲语言作为低资源语言缺乏标注数据，手动标注耗时且昂贵，需要高效的自动标注方法来减少人工工作量。

Method: 利用表情符号和情感词汇信息，开发语言无关的自动情感标注方法。

Result: 在英语、Sepedi和Setswana推文上的标注准确率分别为66%、69%和63%，平均只需修正34%的自动生成标签。

Conclusion: 该方法能有效减少情感标注的人工工作量，为低资源语言的情感分析提供实用解决方案。

Abstract: Sentiment analysis is a helpful task to automatically analyse opinions and emotions on various topics in areas such as AI for Social Good, AI in Education or marketing. While many of the sentiment analysis systems are developed for English, many African languages are classified as low-resource languages due to the lack of digital language resources like text labelled with corresponding sentiment classes. One reason for that is that manually labelling text data is time-consuming and expensive. Consequently, automatic and rapid processes are needed to reduce the manual effort as much as possible making the labelling process as efficient as possible. In this paper, we present and analyze an automatic language-independent sentiment labelling method that leverages information from sentiment-bearing emojis and words. Our experiments are conducted with tweets in the languages English, Sepedi and Setswana from SAfriSenti, a multilingual sentiment corpus for South African languages. We show that our sentiment labelling approach is able to label the English tweets with an accuracy of 66%, the Sepedi tweets with 69%, and the Setswana tweets with 63%, so that on average only 34% of the automatically generated labels remain to be corrected.

</details>


### [8] [Profile-LLM: Dynamic Profile Optimization for Realistic Personality Expression in LLMs](https://arxiv.org/abs/2511.19852)
*Shi-Wei Dai,Yan-Wei Shie,Tsung-Huan Yang,Lun-Wei Ku,Yung-Hui Li*

Main category: cs.CL

TL;DR: PersonaPulse框架通过动态优化角色扮演提示，利用LLMs对人格特质的固有知识，结合情境响应基准作为评分工具，显著提升了LLMs中人格表达的逼真度和上下文相关性。


<details>
  <summary>Details</summary>
Motivation: 现有研究虽然探索了使用提示来激发LLMs的特定人格特质，但未能优化这些提示以最大化人格表达效果。

Method: 提出PersonaPulse框架，利用LLMs对人格特质的固有知识迭代增强角色扮演提示，同时整合情境响应基准作为评分工具，确保更逼真和上下文相关的评估来指导优化过程。

Result: 定量评估显示PersonaPulse生成的提示优于基于心理学研究人格描述的先前工作；探索了模型大小与人格建模的关系；发现某些人格特质的激发程度可以通过暂停优化过程来部分控制。

Conclusion: 提示优化在塑造LLMs中人格表达方面具有重要意义，为未来自适应AI交互研究提供了宝贵见解。

Abstract: Personalized Large Language Models (LLMs) have been shown to be an effective way to create more engaging and enjoyable user-AI interactions. While previous studies have explored using prompts to elicit specific personality traits in LLMs, they have not optimized these prompts to maximize personality expression. To address this limitation, we propose PersonaPulse: Dynamic Profile Optimization for Realistic Personality Expression in LLMs, a framework that leverages LLMs' inherent knowledge of personality traits to iteratively enhance role-play prompts while integrating a situational response benchmark as a scoring tool, ensuring a more realistic and contextually grounded evaluation to guide the optimization process. Quantitative evaluations demonstrate that the prompts generated by PersonaPulse outperform those of prior work, which were designed based on personality descriptions from psychological studies. Additionally, we explore the relationship between model size and personality modeling through extensive experiments. Finally, we find that, for certain personality traits, the extent of personality evocation can be partially controlled by pausing the optimization process. These findings underscore the importance of prompt optimization in shaping personality expression within LLMs, offering valuable insights for future research on adaptive AI interactions.

</details>


### [9] [A Systematic Analysis of Large Language Models with RAG-enabled Dynamic Prompting for Medical Error Detection and Correction](https://arxiv.org/abs/2511.19858)
*Farzad Ahmed,Joniel Augustine Jerome,Meliha Yetisgen,Özlem Uzuner*

Main category: cs.CL

TL;DR: 评估三种提示策略（零样本、静态随机示例、检索增强动态提示）在医疗错误处理任务中的表现，发现检索增强动态提示在多个LLM中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 临床文档中存在可能危及患者安全的事实性、诊断性和管理性错误，需要探索LLM在不同提示策略下检测和纠正这些错误的能力。

Method: 使用MEDEC数据集评估9个指令调优的LLM，比较零样本提示、静态随机示例提示和检索增强动态提示在错误标记检测、错误句子检测和错误纠正三个子任务中的表现。

Result: 检索增强动态提示在所有9个LLM中表现最佳，将假阳性率降低约15%，在错误句子检测中召回率提高5-10%，并生成更准确的纠正。

Conclusion: 检索增强动态提示优于零样本和静态提示策略，使用检索示例可提高检测准确性、减少假阳性并增强医疗错误纠正的可靠性。

Abstract: Objective: Clinical documentation contains factual, diagnostic, and management errors that can compromise patient safety. Large language models (LLMs) may help detect and correct such errors, but their behavior under different prompting strategies remains unclear. We evaluate zero-shot prompting, static prompting with random exemplars (SPR), and retrieval-augmented dynamic prompting (RDP) for three subtasks of medical error processing: error flag detection, error sentence detection, and error correction.
  Methods: Using the MEDEC dataset, we evaluated nine instruction-tuned LLMs (GPT, Claude, Gemini, and OpenAI o-series models). We measured performance using accuracy, recall, false-positive rate (FPR), and an aggregate score of ROUGE-1, BLEURT, and BERTScore for error correction. We also analyzed example outputs to identify failure modes and differences between LLM and clinician reasoning.
  Results: Zero-shot prompting showed low recall in both detection tasks, often missing abbreviation-heavy or atypical errors. SPR improved recall but increased FPR. Across all nine LLMs, RDP reduced FPR by about 15 percent, improved recall by 5 to 10 percent in error sentence detection, and generated more contextually accurate corrections.
  Conclusion: Across diverse LLMs, RDP outperforms zero-shot and SPR prompting. Using retrieved exemplars improves detection accuracy, reduces false positives, and enhances the reliability of medical error correction.

</details>


### [10] [AppSelectBench: Application-Level Tool Selection Benchmark](https://arxiv.org/abs/2511.19957)
*Tianyi Chen,Michael Solodko,Sen Wang,Jongwoo Ko,Junheng Hao,Colby Banbury,Sara Abdali,Saeed Amizadeh,Qing Xiao,Yinheng Li,Tianyu Ding,Kamran Ghasedi Dizaji,Suzhen Zheng,Hao Fan,Justin Wagle,Pashmina Cameron,Kazuhito Koishida*

Main category: cs.CL

TL;DR: 提出了AppSelectBench基准测试，用于评估计算机使用代理在应用选择方面的能力，包含10万个真实用户任务和100个桌面应用，揭示了现有模型在跨应用推理方面的系统性问题。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要评估细粒度API选择，无法衡量模型在不同应用间进行推理和选择的能力，而应用选择是计算机使用代理有效运行的基础能力。

Method: 开发了用户任务生成流水线，创建真实、多样且语义基础的用户意图，并设计了统一的评估协议，包括随机、启发式、零样本、少样本和检索增强设置。

Result: 在闭源和开源大语言模型上的广泛实验显示，即使最强大的模型在一致性应用选择方面仍存在困难，揭示了跨应用推理的系统性优势和弱点。

Conclusion: AppSelectBench为研究和推进应用级推理能力奠定了基础，这是智能计算机使用代理关键但尚未充分探索的能力。

Abstract: Computer Using Agents (CUAs) are increasingly equipped with external tools, enabling them to perform complex and realistic tasks. For CUAs to operate effectively, application selection, which refers to deciding which application to use before invoking fine-grained tools such as APIs, is a fundamental capability. It determines whether the agent initializes the correct environment, avoids orchestration confusion, and efficiently focuses on relevant context. However, existing benchmarks primarily assess fine-grained API selection, offering limited insight into whether models can reason across and choose between different applications. To fill this gap, we introduce AppSelectBench, a comprehensive benchmark for evaluating application selection in CUAs. AppSelectBench contains a novel user task generation pipeline that produces realistic, diverse, and semantically grounded user intents at scale, together with unified evaluation protocols covering random, heuristic, zero-shot, few-shot, and retrieval-augmented-settings. AppSelectBench covers one hundred widely used desktop applications and includes more than one hundred thousand realistic, diverse, and semantically grounded user tasks. Extensive experiments across both closed-source and open-source large language models reveal systematic strengths and weaknesses in inter-application reasoning, showing that even the most capable models still struggle to make consistent application choices. Together, these results establish AppSelectBench as a foundation for studying and advancing application level reasoning, an essential yet underexplored capability of intelligent CUAs. The source is available at https://github.com/microsoft/appselectbench.

</details>


### [11] [$\text{R}^2\text{R}$: A Route-to-Rerank Post-Training Framework for Multi-Domain Decoder-Only Rerankers](https://arxiv.org/abs/2511.19987)
*Xinyu Wang,Hanwei Wu,Qingchen Hu,Zhenghan Tai,Jingrui Tian,Lei Ding,Jijun Chi,Hailin He,Tung Sum Thomas Kwok,Yufei Cui,Sicheng Lyu,Muzhi Li,Mingze Li,Xinyue Yu,Ling Zhou,Peng Lu*

Main category: cs.CL

TL;DR: R2R是一个领域感知的检索增强生成框架，通过动态专家路由和两阶段训练策略解决领域专业化问题，在金融、法律等高风险领域表现优异。


<details>
  <summary>Details</summary>
Motivation: 通用模型在金融、法律等高风险领域缺乏领域特定知识，而简单微调会导致表面形式过拟合和灾难性遗忘。

Method: 采用动态专家路由和两阶段训练策略（EAG），通过掩码最具预测性的表面线索，强制重排序器学习领域不变的相关性模式。使用轻量级潜在语义路由器选择最优LoRA专家。

Result: 在多个重排序器骨干和不同领域（法律、医疗、金融）的广泛实验中，R2R始终优于通用模型和单领域微调基线。

Conclusion: R2R是一种模型无关的模块化方法，具有强大的跨领域鲁棒性，能有效实现领域专业化。

Abstract: Decoder-only rerankers are central to Retrieval-Augmented Generation (RAG). However, generalist models miss domain-specific nuances in high-stakes fields like finance and law, and naive fine-tuning causes surface-form overfitting and catastrophic forgetting. To address this challenge, we introduce R2R, a domain-aware framework that combines dynamic expert routing with a two-stage training strategy, Entity Abstraction for Generalization (EAG). EAG introduces a counter-shortcut mechanism by masking the most predictive surface cues, forcing the reranker to learn domain-invariant relevance patterns rather than memorizing dataset-specific entities. To efficiently activate domain experts, R2R employs a lightweight Latent Semantic Router that probes internal representations from the frozen backbone decoder to select the optimal LoRA expert per query. Extensive experiments across different reranker backbones and diverse domains (legal, medical, and financial) demonstrate that R2R consistently surpasses generalist and single-domain fine-tuned baselines. Our results confirm that R2R is a model-agnostic and modular approach to domain specialization with strong cross-domain robustness.

</details>


### [12] [Directional Optimization Asymmetry in Transformers: A Synthetic Stress Test](https://arxiv.org/abs/2511.19997)
*Mihir Sahasrabudhe*

Main category: cs.CL

TL;DR: 本文通过合成基准测试发现，即使在没有语言先验的干净环境中，因果Transformer模型在反向映射任务上仍存在显著的方向性优化差距，这揭示了Transformer架构本身固有的方向性摩擦。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer模型方向性失败的根本原因问题：是源于语言统计特性还是架构本身？通过完全合成的基准测试来消除语言先验和语料库时间不对称性的影响。

Method: 使用具有可调分支因子K的随机字符串映射构建合成基准，创建零条件熵的前向任务和具有分析确定熵底线的反向任务，比较GPT-2模型在不同方向任务上的表现。

Result: 即使在从头训练的GPT-2模型中，也观察到显著的方向性优化差距（如K=5时1.16 nats），远大于在相同数据上训练的MLP。预训练初始化改变了优化行为但未消除差距，LoRA在高熵反向映射上遇到容量墙。

Conclusion: 研究分离出了一个最小化的、无语义的方向性摩擦特征，这是因果Transformer训练固有的特性，即使在移除语言先验、词频和语料级时间不对称性后仍然存在。

Abstract: Transformers are theoretically reversal-invariant: their function class does not prefer left-to-right over right-to-left mappings. Yet empirical studies on natural language repeatedly report a "reversal curse," and recent work on temporal asymmetry in LLMs suggests that real-world corpora carry their own arrow of time. This leaves an unresolved question: do directional failures stem from linguistic statistics, or from the architecture itself? We cut through this ambiguity with a fully synthetic, entropy-controlled benchmark designed as a clean-room stress test for directional learning. Using random string mappings with tunable branching factor K, we construct forward tasks with zero conditional entropy and inverse tasks with analytically determined entropy floors. Excess loss above these floors reveals that even scratch-trained GPT-2 models exhibit a strong, reproducible directional optimization gap (e.g., 1.16 nats at K=5), far larger than that of an MLP trained on the same data. Pre-trained initializations shift optimization behavior but do not eliminate this gap, while LoRA encounters a sharp capacity wall on high-entropy inverse mappings. Together, these results isolate a minimal, semantics-free signature of directional friction intrinsic to causal Transformer training-one that persists even when linguistic priors, token frequencies, and corpus-level temporal asymmetries are removed. Our benchmark provides a controlled instrument for dissecting directional biases in modern sequence models and motivates deeper mechanistic study of why inversion remains fundamentally harder for Transformers.

</details>


### [13] [A Machine Learning Approach for Detection of Mental Health Conditions and Cyberbullying from Social Media](https://arxiv.org/abs/2511.20001)
*Edward Ajayi,Martha Kachweka,Mawuli Deku,Emily Aiken*

Main category: cs.CL

TL;DR: 提出统一的多类别分类框架，用于从社交媒体数据中检测10种心理健康和网络欺凌类别，通过端到端微调的MentalBERT模型达到最佳性能，并开发了结合SHAPLLM解释框架的原型仪表板。


<details>
  <summary>Details</summary>
Motivation: 数字空间中日益增长的心理健康挑战和网络欺凌问题需要可扩展且可解释的检测系统。

Method: 使用Twitter和Reddit数据集，采用"分割后平衡"流程，比较传统词汇模型、混合方法和端到端微调transformer模型，并引入混合SHAPLLM可解释性框架。

Result: 端到端微调对性能至关重要，领域适应的MentalBERT成为最佳模型，准确率达0.92，宏F1分数0.76，超越了通用模型和零样本LLM基线。

Conclusion: 该系统作为人工参与循环的筛查辅助工具而非诊断工具，为在线安全和计算心理健康的交叉领域提供了稳健基准，并强调未来需要多标签、临床验证的数据集。

Abstract: Mental health challenges and cyberbullying are increasingly prevalent in digital spaces, necessitating scalable and interpretable detection systems. This paper introduces a unified multiclass classification framework for detecting ten distinct mental health and cyberbullying categories from social media data. We curate datasets from Twitter and Reddit, implementing a rigorous "split-then-balance" pipeline to train on balanced data while evaluating on a realistic, held-out imbalanced test set. We conducted a comprehensive evaluation comparing traditional lexical models, hybrid approaches, and several end-to-end fine-tuned transformers. Our results demonstrate that end-to-end fine-tuning is critical for performance, with the domain-adapted MentalBERT emerging as the top model, achieving an accuracy of 0.92 and a Macro F1 score of 0.76, surpassing both its generic counterpart and a zero-shot LLM baseline. Grounded in a comprehensive ethical analysis, we frame the system as a human-in-the-loop screening aid, not a diagnostic tool. To support this, we introduce a hybrid SHAPLLM explainability framework and present a prototype dashboard ("Social Media Screener") designed to integrate model predictions and their explanations into a practical workflow for moderators. Our work provides a robust baseline, highlighting future needs for multi-label, clinically-validated datasets at the critical intersection of online safety and computational mental health.

</details>


### [14] [Online-PVLM: Advancing Personalized VLMs with Online Concept Learning](https://arxiv.org/abs/2511.20056)
*Huiyu Bai,Runze Wang,Zhuoyun Du,Yiyang Zhao,Fengji Zhang,Haoyu Chen,Xiaoyong Zhu,Bo Zheng,Xuejiao Zhao*

Main category: cs.CL

TL;DR: Online-PVLM是一个用于个性化视觉语言模型在线概念学习的框架，利用双曲表示实现无需训练的概念嵌入生成，解决了现有方法无法在测试时实时适应的问题。


<details>
  <summary>Details</summary>
Motivation: 现有个性化视觉语言模型需要为每个新概念学习单独的嵌入，无法支持测试时的实时适应，在大规模场景中效率低下。

Method: 提出基于双曲表示的在线概念学习框架，在测试时无需训练即可生成概念嵌入，使个性化视觉语言模型更具可扩展性和效率。

Result: 开发了包含1,292个概念和超过30K高质量实例的OP-Eval基准测试，实验证明该方法达到了最先进的性能。

Conclusion: Online-PVLM框架成功解决了个性化视觉语言模型在线概念学习的可扩展性和效率问题，为实际应用提供了有效解决方案。

Abstract: Personalized Visual Language Models (VLMs) are gaining increasing attention for their formidable ability in user-specific concepts aligned interactions (e.g., identifying a user's bike). Existing methods typically require the learning of separate embeddings for each new concept, which fails to support real-time adaptation during testing. This limitation becomes particularly pronounced in large-scale scenarios, where efficient retrieval of concept embeddings is not achievable. To alleviate this gap, we propose Online-PVLM, a framework for online concept learning by leveraging hyperbolic representations. Our approach makes a train-free paradigm for concept embeddings generation at test time, making the use of personalized VLMs both scalable and efficient. In addition, we develop OP-Eval, a comprehensive and large-scale benchmark comprising 1,292 concepts and over 30K high-quality instances with diverse question types, designed to rigorously assess online concept learning in realistic scenarios. Extensive experiments demonstrate the state-of-the-art performance of our proposed framework. Our source code and dataset will be made available.

</details>


### [15] [MTA: A Merge-then-Adapt Framework for Personalized Large Language Model](https://arxiv.org/abs/2511.20072)
*Xiaopeng Li,Yuanjin Zheng,Wanyu Wang,wenlin zhang,Pengyue Jia,Yiqi Wang,Maolin Wang,Xuetao Wei,Xiangyu Zhao*

Main category: cs.CL

TL;DR: 提出了MTA框架解决个性化大语言模型的两个主要限制：存储成本随用户数量线性增长和稀疏数据用户性能不佳。通过元LoRA银行构建、自适应LoRA融合和少样本个性化LoRA堆叠三个阶段实现高效个性化。


<details>
  <summary>Details</summary>
Motivation: 现有方法为每个用户微调独立模块导致存储成本线性增长且稀疏数据用户性能不佳，需要更高效可扩展的个性化方法。

Method: 三阶段框架：1)构建共享元LoRA银行，预训练元个性化特征；2)自适应LoRA融合，动态合并相关锚点元LoRA合成用户特定模块；3)少样本个性化LoRA堆叠，在合并LoRA上应用超低秩轻量级模块进行微调。

Result: 在LaMP基准测试上的广泛实验表明，该方法在多个任务上优于现有最先进方法。

Conclusion: MTA框架通过元学习、动态融合和轻量级微调有效解决了PLLMs的可扩展性和稀疏数据问题，实现了高效个性化。

Abstract: Personalized Large Language Models (PLLMs) aim to align model outputs with individual user preferences, a crucial capability for user-centric applications. However, the prevalent approach of fine-tuning a separate module for each user faces two major limitations: (1) storage costs scale linearly with the number of users, rendering the method unscalable; and (2) fine-tuning a static model from scratch often yields suboptimal performance for users with sparse data. To address these challenges, we propose MTA, a Merge-then-Adapt framework for PLLMs. MTA comprises three key stages. First, we construct a shared Meta-LoRA Bank by selecting anchor users and pre-training meta-personalization traits within meta-LoRA modules. Second, to ensure scalability and enable dynamic personalization combination beyond static models, we introduce an Adaptive LoRA Fusion stage. This stage retrieves and dynamically merges the most relevant anchor meta-LoRAs to synthesize a user-specific one, thereby eliminating the need for user-specific storage and supporting more flexible personalization. Third, we propose a LoRA Stacking for Few-Shot Personalization stage, which applies an additional ultra-low-rank, lightweight LoRA module on top of the merged LoRA. Fine-tuning this module enables effective personalization under few-shot settings. Extensive experiments on the LaMP benchmark demonstrate that our approach outperforms existing SOTA methods across multiple tasks.

</details>


### [16] [More Bias, Less Bias: BiasPrompting for Enhanced Multiple-Choice Question Answering](https://arxiv.org/abs/2511.20086)
*Duc Anh Vu,Thong Nguyen,Cong-Duy Nguyen,Viet Anh Nguyen,Anh Tuan Luu*

Main category: cs.CL

TL;DR: BiasPrompting是一个新颖的推理框架，通过引导LLMs生成和评估所有可能答案选项的推理过程，来提升多选问题的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多选题任务中通常缺乏上下文背景和解释，导致无法充分探索所有可能答案，从而降低了模型的推理能力。

Method: 包含两个组件：推理生成阶段（为每个答案选项生成支持性推理）和推理引导一致性阶段（综合生成的推理选择最合理的答案）。

Result: 在五个广泛使用的多选题基准测试中表现出显著改进，特别是在现有方法表现不佳的复杂和挑战性问题中。

Conclusion: BiasPrompting增强了LLMs的推理能力，为处理复杂挑战性问题提供了坚实基础。

Abstract: With the advancement of large language models (LLMs), their performance on multiple-choice question (MCQ) tasks has improved significantly. However, existing approaches face key limitations: answer choices are typically presented to LLMs without contextual grounding or explanation. This absence of context can lead to incomplete exploration of all possible answers, ultimately degrading the models' reasoning capabilities. To address these challenges, we introduce BiasPrompting, a novel inference framework that guides LLMs to generate and critically evaluate reasoning across all plausible answer options before reaching a final prediction. It consists of two components: first, a reasoning generation stage, where the model is prompted to produce supportive reasonings for each answer option, and then, a reasoning-guided agreement stage, where the generated reasonings are synthesized to select the most plausible answer. Through comprehensive evaluations, BiasPrompting demonstrates significant improvements in five widely used multiple-choice question answering benchmarks. Our experiments showcase that BiasPrompting enhances the reasoning capabilities of LLMs and provides a strong foundation for tackling complex and challenging questions, particularly in settings where existing methods underperform.

</details>


### [17] [SSA: Sparse Sparse Attention by Aligning Full and Sparse Attention Outputs in Feature Space](https://arxiv.org/abs/2511.20102)
*Zhenyi Shen,Junru Lu,Lin Gui,Jiazheng Li,Yulan He,Di Yin,Xing Sun*

Main category: cs.CL

TL;DR: SSA是一个统一的稀疏注意力训练框架，通过双向对齐和保留梯度流来解决现有稀疏注意力方法的关键悖论，在保持高性能的同时实现更强的稀疏性。


<details>
  <summary>Details</summary>
Motivation: 解决现有稀疏注意力方法的关键悖论：虽然旨在近似全注意力，但产生的注意力稀疏性反而比全注意力模型更低，这限制了其有效性。

Method: 提出SSA框架，同时考虑稀疏和全注意力，在每一层强制执行双向对齐，保留所有token的梯度流，并明确鼓励稀疏注意力输出与其全注意力对应物对齐。

Result: 在多个常识基准测试中，SSA在稀疏和全注意力推理下都实现了最先进的性能；能够平滑适应不同的稀疏预算；显著改善了长上下文外推能力。

Conclusion: SSA通过解决梯度更新缺陷和促进双向对齐，成功克服了现有稀疏注意力方法的局限性，在性能、稀疏性和外推能力方面都表现出色。

Abstract: The quadratic complexity of full attention limits efficient long-context processing in large language models (LLMs). Sparse attention mitigates this cost by restricting each query to attend to a subset of previous tokens; however, training-free approaches often lead to severe performance degradation. Native sparse-attention methods (e.g., NSA, MoBA) alleviate this issue, yet exhibit a critical paradox: they produce lower attention sparsity than full-attention models, despite aiming to approximate full attention, which may constrain their effectiveness. We attribute this paradox to gradient update deficiency: low-ranked key-value pairs excluded during sparse training receive neither forward contribution nor backward gradients, and thus never learn proper suppression. To overcome this limitation, we propose SSA (Sparse Sparse Attention), a unified training framework that considers both sparse and full attention and enforces bidirectional alignment at every layer. This design preserves gradient flow to all tokens while explicitly encouraging sparse-attention outputs to align with their full-attention counterparts, thereby promoting stronger sparsity. As a result, SSA achieves state-of-the-art performance under both sparse and full attention inference across multiple commonsense benchmarks. Furthermore, SSA enables models to adapt smoothly to varying sparsity budgets; performance improves consistently as more tokens are allowed to attend, supporting flexible compute-performance trade-offs at inference time. Finally, we show that native sparse-attention training surprisingly improves long-context extrapolation by mitigating the over-allocation of attention values in sink areas, with SSA demonstrating the strongest extrapolation capability.

</details>


### [18] [EM2LDL: A Multilingual Speech Corpus for Mixed Emotion Recognition through Label Distribution Learning](https://arxiv.org/abs/2511.20106)
*Xingfeng Li,Xiaohan Shi,Junjie Li,Yongwei Li,Masashi Unoki,Tomoki Toda,Masato Akagi*

Main category: cs.CL

TL;DR: EM2LDL是一个多语言语音语料库，通过标签分布学习推进混合情感识别，包含英语、普通话和粤语表达性话语，捕捉多语言地区语码转换现象。


<details>
  <summary>Details</summary>
Motivation: 解决现有语料库主要为单语言和单标签情感的限制，缺乏语言多样性、无法建模混合情感且生态效度不足的问题。

Method: 构建包含英语、普通话和粤语的多语言语料库，整合在线平台的自发情感表达，使用32个类别的细粒度情感分布进行标注，采用自监督学习模型进行实验。

Result: 实验基线显示在说话人独立的性别、年龄和个性评估中表现稳健，HuBERT-large-EN模型取得最佳结果。

Conclusion: EM2LDL通过整合语言多样性和生态效度，为多语言环境中复杂情感动态的探索提供了平台，为情感计算应用开发适应性强的共情系统提供了测试平台。

Abstract: This study introduces EM2LDL, a novel multilingual speech corpus designed to advance mixed emotion recognition through label distribution learning. Addressing the limitations of predominantly monolingual and single-label emotion corpora \textcolor{black}{that restrict linguistic diversity, are unable to model mixed emotions, and lack ecological validity}, EM2LDL comprises expressive utterances in English, Mandarin, and Cantonese, capturing the intra-utterance code-switching prevalent in multilingual regions like Hong Kong and Macao. The corpus integrates spontaneous emotional expressions from online platforms, annotated with fine-grained emotion distributions across 32 categories. Experimental baselines using self-supervised learning models demonstrate robust performance in speaker-independent gender-, age-, and personality-based evaluations, with HuBERT-large-EN achieving optimal results. By incorporating linguistic diversity and ecological validity, EM2LDL enables the exploration of complex emotional dynamics in multilingual settings. This work provides a versatile testbed for developing adaptive, empathetic systems for applications in affective computing, including mental health monitoring and cross-cultural communication. The dataset, annotations, and baseline codes are publicly available at https://github.com/xingfengli/EM2LDL.

</details>


### [19] [Mispronunciation Detection and Diagnosis Without Model Training: A Retrieval-Based Approach](https://arxiv.org/abs/2511.20107)
*Huu Tuong Tu,Ha Viet Khanh,Tran Tien Dat,Vu Huan,Thien Van Luong,Nguyen Tien Cuong,Nguyen Thi Thu Trang*

Main category: cs.CL

TL;DR: 提出了一种基于检索技术的免训练框架，利用预训练ASR模型进行发音错误检测与诊断，无需音素级建模或额外训练。


<details>
  <summary>Details</summary>
Motivation: 传统发音错误检测方法需要评分模型或音素级模型训练，过程复杂且成本高。

Method: 采用检索技术与预训练ASR模型结合的训练免费框架，避免音素特定建模和任务特定训练。

Result: 在L2-ARCTIC数据集上达到69.60%的F1分数，优于传统方法。

Conclusion: 该方法在避免模型训练复杂性的同时，实现了准确的发音错误检测与诊断。

Abstract: Mispronunciation Detection and Diagnosis (MDD) is crucial for language learning and speech therapy. Unlike conventional methods that require scoring models or training phoneme-level models, we propose a novel training-free framework that leverages retrieval techniques with a pretrained Automatic Speech Recognition model. Our method avoids phoneme-specific modeling or additional task-specific training, while still achieving accurate detection and diagnosis of pronunciation errors. Experiments on the L2-ARCTIC dataset show that our method achieves a superior F1 score of 69.60% while avoiding the complexity of model training.

</details>


### [20] ["When Data is Scarce, Prompt Smarter"... Approaches to Grammatical Error Correction in Low-Resource Settings](https://arxiv.org/abs/2511.20120)
*Somsubhra De,Harsh Kumar,Arun Prakash A*

Main category: cs.CL

TL;DR: 本文探索使用GPT-4.1、Gemini-2.5和LLaMA-4等大型语言模型，结合少样本策略进行印地语系语言的语法错误校正，在资源有限的情况下取得了优异成果。


<details>
  <summary>Details</summary>
Motivation: 印地语系语言的语法错误校正面临资源有限、语言多样性和复杂形态学的挑战，而现有方法主要针对英语等高资源语言，需要探索适用于低资源语言的有效方法。

Method: 采用基于提示的方法，结合零样本和少样本策略，使用最先进的大型语言模型，通过精心设计的提示和轻量级适应来提升多语言语法错误校正性能。

Result: 在共享任务中取得领先成绩：泰米尔语排名第1（GLEU: 91.57）、印地语排名第1（GLEU: 85.69）、泰卢固语排名第2（GLEU: 85.22）、孟加拉语排名第4（GLEU: 92.86）、马拉雅拉姆语排名第5（GLEU: 92.97）。

Conclusion: 基于提示的NLP技术非常有效，大型语言模型具有卓越的多语言泛化能力，能够弥补多语言语法错误校正中的资源差距。

Abstract: Grammatical error correction (GEC) is an important task in Natural Language Processing that aims to automatically detect and correct grammatical mistakes in text. While recent advances in transformer-based models and large annotated datasets have greatly improved GEC performance for high-resource languages such as English, the progress has not extended equally. For most Indic languages, GEC remains a challenging task due to limited resources, linguistic diversity and complex morphology. In this work, we explore prompting-based approaches using state-of-the-art large language models (LLMs), such as GPT-4.1, Gemini-2.5 and LLaMA-4, combined with few-shot strategy to adapt them to low-resource settings. We observe that even basic prompting strategies, such as zero-shot and few-shot approaches, enable these LLMs to substantially outperform fine-tuned Indic-language models like Sarvam-22B, thereby illustrating the exceptional multilingual generalization capabilities of contemporary LLMs for GEC. Our experiments show that carefully designed prompts and lightweight adaptation significantly enhance correction quality across multiple Indic languages. We achieved leading results in the shared task--ranking 1st in Tamil (GLEU: 91.57) and Hindi (GLEU: 85.69), 2nd in Telugu (GLEU: 85.22), 4th in Bangla (GLEU: 92.86), and 5th in Malayalam (GLEU: 92.97). These findings highlight the effectiveness of prompt-driven NLP techniques and underscore the potential of large-scale LLMs to bridge resource gaps in multilingual GEC.

</details>


### [21] [SEDA: A Self-Adapted Entity-Centric Data Augmentation for Boosting Gird-based Discontinuous NER Models](https://arxiv.org/abs/2511.20143)
*Wen-Fang Su,Hsiao-Wei Chou,Wen-Yang Lin*

Main category: cs.CL

TL;DR: 提出了一种结合图像数据增强技术的网格标注方法，用于解决不连续命名实体识别中的分割和遗漏问题，在多个数据集上取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理跨句子的不连续实体时存在分割错误和遗漏问题，严重影响识别准确率。

Method: 将图像数据增强技术（裁剪、缩放、填充）集成到基于网格的模型中，增强模型识别不连续实体和处理分割挑战的能力。

Result: 在CADEC、ShARe13和ShARe14数据集上，整体F1分数提升1-2.5%，不连续实体识别提升3.7-8.4%。

Conclusion: 增强的网格模型能有效解决不连续实体的分割和识别问题，证明了该方法的有效性。

Abstract: Named Entity Recognition (NER) is a critical task in natural language processing, yet it remains particularly challenging for discontinuous entities. The primary difficulty lies in text segmentation, as traditional methods often missegment or entirely miss cross-sentence discontinuous entities, significantly affecting recognition accuracy. Therefore, we aim to address the segmentation and omission issues associated with such entities. Recent studies have shown that grid-tagging methods are effective for information extraction due to their flexible tagging schemes and robust architectures. Building on this, we integrate image data augmentation techniques, such as cropping, scaling, and padding, into grid-based models to enhance their ability to recognize discontinuous entities and handle segmentation challenges. Experimental results demonstrate that traditional segmentation methods often fail to capture cross-sentence discontinuous entities, leading to decreased performance. In contrast, our augmented grid models achieve notable improvements. Evaluations on the CADEC, ShARe13, and ShARe14 datasets show F1 score gains of 1-2.5% overall and 3.7-8.4% for discontinuous entities, confirming the effectiveness of our approach.

</details>


### [22] [KyrgyzBERT: A Compact, Efficient Language Model for Kyrgyz NLP](https://arxiv.org/abs/2511.20182)
*Adilet Metinov,Gulida M. Kudakeeva,Gulnara D. Kabaeva*

Main category: cs.CL

TL;DR: 开发了首个公开的吉尔吉斯语单语BERT模型KyrgyzBERT，参数规模35.9M，并创建了情感分析基准数据集kyrgyz-sst2。


<details>
  <summary>Details</summary>
Motivation: 吉尔吉斯语作为低资源语言，缺乏基础NLP工具，需要开发专门的语言模型来支持该语言的NLP研究。

Method: 构建了定制的分词器以适应吉尔吉斯语的形态结构，通过翻译斯坦福情感树库并手动标注测试集创建了情感分析基准数据集。

Result: KyrgyzBERT在情感分析任务上达到F1分数0.8280，与参数量大5倍的多语言BERT模型性能相当。

Conclusion: 所有模型、数据和代码都已公开发布，将支持吉尔吉斯语NLP的未来研究。

Abstract: Kyrgyz remains a low-resource language with limited foundational NLP tools. To address this gap, we introduce KyrgyzBERT, the first publicly available monolingual BERT-based language model for Kyrgyz. The model has 35.9M parameters and uses a custom tokenizer designed for the language's morphological structure. To evaluate performance, we create kyrgyz-sst2, a sentiment analysis benchmark built by translating the Stanford Sentiment Treebank and manually annotating the full test set. KyrgyzBERT fine-tuned on this dataset achieves an F1-score of 0.8280, competitive with a fine-tuned mBERT model five times larger. All models, data, and code are released to support future research in Kyrgyz NLP.

</details>


### [23] [REFLEX: Self-Refining Explainable Fact-Checking via Disentangling Truth into Style and Substance](https://arxiv.org/abs/2511.20233)
*Chuyi Kong,Gao Wei,Jing Ma,Hongzhan Lin,Zhiyuan Fan*

Main category: cs.CL

TL;DR: REFLEX是一个无需外部知识的自动事实核查范式，通过角色扮演对话联合训练判决预测和解释生成，利用对比激活向量分离真相的样式和实质，在少量样本下实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的事实核查方法过度依赖外部知识源，导致延迟和幻觉问题，影响可靠性、可解释性和实时性。

Method: 将事实核查重构为角色扮演对话，提取主干模型与其微调变体间的对比激活对构建转向向量，自然分离真相的样式和实质，引导推理并抑制噪声解释。

Result: 在真实数据集上超越先前方法，仅用465个自精炼训练样本就达到SOTA性能，模型间引导可带来7.57%的性能提升。

Conclusion: 内部解释信号在事实推理中具有双重作用，既能解释又能增强推理能力，REFLEX为高效可靠的事实核查提供了新范式。

Abstract: The prevalence of misinformation on social media threatens public trust, demanding automated fact-checking systems that provide accurate verdicts with interpretable explanations. However, existing large language model-based (LLM-based) approaches often rely heavily on external knowledge sources, introducing substantial latency and even hallucinations that undermine reliability, interpretability, and responsiveness, which is crucial for real-time use. To address these challenges, we propose REason-guided Fact-checking with Latent EXplanations REFLEX paradigm, a plug-and-play, self-refining paradigm that leverages the internal knowledge in backbone model to improve both verdict accuracy and explanation quality. REFLEX reformulates fact-checking as a role-play dialogue and jointly trains verdict prediction and explanation generation. It adaptively extracts contrastive activation pairs between the backbone model and its fine-tuned variant to construct steering vectors that disentangle truth into style and substance naturally. These activation-level signals guide inference and suppress noisy explanations, enabling more faithful and efficient reasoning. Experiments on real-world datasets show that REFLEX outperforms previous methods that steer toward a single truth direction and underscores the challenge traditional approaches face when handling the subtle, human-unknown truth in fact-checking tasks. Remarkably, with only 465 self-refined training samples, RELFEX achieves state-of-the-art performance. Furthermore, models trained with explanatory objectives can effectively guide those without them, yielding up to a 7.57% improvement, highlighting that internal explanation signals play a dual role in both interpreting and enhancing factual reasoning.

</details>


### [24] [Scaling LLM Speculative Decoding: Non-Autoregressive Forecasting in Large-Batch Scenarios](https://arxiv.org/abs/2511.20340)
*Luohe Shi,Zuchao Li,Lefei Zhang,Baoyuan Qi,Guoming Liu,Hai Zhao*

Main category: cs.CL

TL;DR: SpecFormer是一种新颖的架构，通过结合单向和双向注意力机制，在低验证资源和低调度成本下实现LLM推理加速，消除了对大型前缀树的依赖。


<details>
  <summary>Details</summary>
Motivation: 当前推测解码方法假设有大量可用计算能力，但在批处理等主流推理系统中，可用空闲计算能力被压缩，因此需要在低验证资源和低调度成本下进行推测解码。

Method: 提出SpecFormer架构，整合单向和双向注意力机制，结合自回归模型从整个输入序列提取信息的能力和非自回归模型的并行生成优势。

Result: 通过在不同规模模型上的无损推测解码实验，证明SpecFormer为LLM推理扩展设定了新标准，具有更低的训练需求和计算成本。

Conclusion: SpecFormer实现了在大型批处理场景下的一致加速，消除了对复杂大规模草稿树的依赖，为LLM推理提供了更高效的解决方案。

Abstract: Speculative decoding accelerates LLM inference by utilizing otherwise idle computational resources during memory-to-chip data transfer. Current speculative decoding methods typically assume a considerable amount of available computing power, then generate a complex and massive draft tree using a small autoregressive language model to improve overall prediction accuracy. However, methods like batching have been widely applied in mainstream model inference systems as a superior alternative to speculative decoding, as they compress the available idle computing power. Therefore, performing speculative decoding with low verification resources and low scheduling costs has become an important research problem. We believe that more capable models that allow for parallel generation on draft sequences are what we truly need. Recognizing the fundamental nature of draft models to only generate sequences of limited length, we propose SpecFormer, a novel architecture that integrates unidirectional and bidirectional attention mechanisms. SpecFormer combines the autoregressive model's ability to extract information from the entire input sequence with the parallel generation benefits of non-autoregressive models. This design eliminates the reliance on large prefix trees and achieves consistent acceleration, even in large-batch scenarios. Through lossless speculative decoding experiments across models of various scales, we demonstrate that SpecFormer sets a new standard for scaling LLM inference with lower training demands and reduced computational costs.

</details>


### [25] [The Curious Case of Analogies: Investigating Analogical Reasoning in Large Language Models](https://arxiv.org/abs/2511.20344)
*Taewhoo Lee,Minju Song,Chanwoong Yoon,Jungwoo Park,Jaewoo Kang*

Main category: cs.CL

TL;DR: LLMs能够编码类比实体间的关系信息，但在应用这些关系到新实体时存在困难。研究发现LLMs在类比推理中表现出与人类相似的局限性，通过策略性修补隐藏表示可以在一定程度上改善推理能力。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs是否能够编码高层次关系概念并将其应用于新情境，了解LLMs类比推理能力与人类认知的相似性和差距。

Method: 使用比例类比和故事类比任务，分析LLMs隐藏层中的属性和关系信息传播，通过策略性修补隐藏表示来测试信息传递效果。

Result: LLMs能够有效编码类比实体间的关系，但应用这些关系到新实体时存在困难；成功案例中显示强结构对齐，失败案例则显示对齐退化或错位。

Conclusion: LLMs在编码和应用高层次关系概念方面表现出初步但有限的能力，与人类认知存在相似性和明显差距。

Abstract: Analogical reasoning is at the core of human cognition, serving as an important foundation for a variety of intellectual activities. While prior work has shown that LLMs can represent task patterns and surface-level concepts, it remains unclear whether these models can encode high-level relational concepts and apply them to novel situations through structured comparisons. In this work, we explore this fundamental aspect using proportional and story analogies, and identify three key findings. First, LLMs effectively encode the underlying relationships between analogous entities; both attributive and relational information propagate through mid-upper layers in correct cases, whereas reasoning failures reflect missing relational information within these layers. Second, unlike humans, LLMs often struggle not only when relational information is missing, but also when attempting to apply it to new entities. In such cases, strategically patching hidden representations at critical token positions can facilitate information transfer to a certain extent. Lastly, successful analogical reasoning in LLMs is marked by strong structural alignment between analogous situations, whereas failures often reflect degraded or misplaced alignment. Overall, our findings reveal that LLMs exhibit emerging but limited capabilities in encoding and applying high-level relational concepts, highlighting both parallels and gaps with human cognition.

</details>


### [26] [BengaliFig: A Low-Resource Challenge for Figurative and Culturally Grounded Reasoning in Bengali](https://arxiv.org/abs/2511.20399)
*Abdullah Al Sefat*

Main category: cs.CL

TL;DR: BengaliFig是一个针对孟加拉语的比喻和文化推理挑战数据集，包含435个来自孟加拉口头和文学传统的谜语，用于评估LLM在低资源文化背景下的推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在广泛的多语言基准测试中表现出色，但在比喻和文化基础推理方面，特别是在低资源语境下的评估仍显不足。

Method: 构建包含435个独特谜语的BengaliFig数据集，每个项目沿五个正交维度进行标注，并通过约束感知的AI辅助流程自动转换为多项选择格式。评估了八个前沿LLM在零样本和少样本思维链提示下的表现。

Result: 揭示了LLM在隐喻和文化特定推理方面存在持续弱点。

Conclusion: BengaliFig既为评估LLM在低资源文化背景下的鲁棒性提供了诊断工具，也朝着包容性和传承意识的NLP评估迈出了一步。

Abstract: Large language models excel on broad multilingual benchmarks but remain to be evaluated extensively in figurative and culturally grounded reasoning, especially in low-resource contexts. We present BengaliFig, a compact yet richly annotated challenge set that targets this gap in Bengali, a widely spoken low-resourced language. The dataset contains 435 unique riddles drawn from Bengali oral and literary traditions. Each item is annotated along five orthogonal dimensions capturing reasoning type, trap type, cultural depth, answer category, and difficulty, and is automatically converted to multiple-choice format through a constraint-aware, AI-assisted pipeline. We evaluate eight frontier LLMs from major providers under zero-shot and few-shot chain-of-thought prompting, revealing consistent weaknesses in metaphorical and culturally specific reasoning. BengaliFig thus contributes both a diagnostic probe for evaluating LLM robustness in low-resource cultural contexts and a step toward inclusive and heritage-aware NLP evaluation.

</details>


### [27] [A Task-Oriented Evaluation Framework for Text Normalization in Modern NLP Pipelines](https://arxiv.org/abs/2511.20409)
*Md Abdullah Al Kafi,Raka Moni,Sumit Kumar Banshal*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的面向任务的词干提取方法评估框架，包含SES、MPD和ANLD三个指标，能够全面评估词干提取方法的效用、下游任务影响和语义相似度。


<details>
  <summary>Details</summary>
Motivation: 当前词干提取方法的评估方法有限，无法捕捉过度词干提取可能造成的危害，因此需要开发新的评估方法。

Method: 提出包含三个方面的评估框架：(1)使用SES评估词干提取效用，(2)使用MPD评估对下游任务的影响，(3)使用ANLD评估词干词与原词的语义相似度。

Result: 应用该框架比较孟加拉语和英语词干提取器，发现孟加拉语词干提取器SES最高(1.67)但ANLD较差(0.26)，表明存在有害的过度词干提取；英语词干提取器SES适中(1.31)且ANLD良好(0.14)，能对下游性能产生积极贡献。

Conclusion: 该研究提供了一个有价值的工具，能够区分潜在效率增益（高SES）和意义保留（低ANLD），有助于选择更可靠的词干提取器。

Abstract: Text normalization is an essential preprocessing step in many natural language processing (NLP) tasks, and stemming is one such normalization technique that reduces words to their base or root form. However, evaluating stemming methods is challenging because current evaluation approaches are limited and do not capture the potential harm caused by excessive stemming; therefore, it is essential to develop new approaches to evaluate stemming methods. To address this issue, this study propose a novel, task-oriented approach to evaluate stemming methods, which considers three aspects: (1) the utility of stemming using Stemming Effectiveness Score (SES), (2) the impact of stemming on downstream tasks using Model Performance Delta (MPD), and (3) the semantic similarity between stemmed and original words using Average Normalized Levenshtein Distance (ANLD), thus providing a comprehensive evaluation framework. We apply our evaluation framework to compare two stemmers for Bangla (BNLTK) and English (Snowball), and our results reveal a significant issue, prompting us to analyze their performance in detail. While the Bangla stemmer achieves the highest SES (1.67) due to effective word reduction (CR = 1.90), SES alone is insufficient because our proposed safety measure, ANLD, reveals that this high SES is due to harmful over-stemming (ANLD = 0.26), which correlates with the observed decrease in downstream performance.In contrast, the English stemmer achieves a moderate SES (1.31) with a safe meaning distance (ANLD = 0.14), allowing its word reduction to contribute positively to downstream performance; therefore, it is a more reliable stemmer. Our study provides a valuable tool for distinguishing between potential efficiency gains (high SES) and meaning preservation (low ANLD).

</details>


### [28] [Generation, Evaluation, and Explanation of Novelists' Styles with Single-Token Prompts](https://arxiv.org/abs/2511.20459)
*Mosab Rezaei,Mina Rajaei Moghadam,Abdul Rahman Shaikh,Hamed Alhoori,Reva Freedman*

Main category: cs.CL

TL;DR: 提出了一个基于大语言模型的19世纪小说家风格生成和评估框架，通过微调模型生成特定作者风格的文本，并使用transformer检测器进行风格评估和解释。


<details>
  <summary>Details</summary>
Motivation: 解决生成模型在无配对数据情况下的训练问题，以及不依赖人类判断的风格文本评估挑战。

Method: 使用最小化单token提示微调大语言模型生成特定作者风格文本，训练transformer检测器进行风格分类和解释，结合句法比较和可解释AI方法分析语言线索。

Result: 生成的文本能够反映作者的独特风格模式，AI评估方法为人类评估提供了可靠替代方案。

Conclusion: 该框架成功实现了19世纪小说家风格的生成和评估，所有工作成果已在线发布。

Abstract: Recent advances in large language models have created new opportunities for stylometry, the study of writing styles and authorship. Two challenges, however, remain central: training generative models when no paired data exist, and evaluating stylistic text without relying only on human judgment. In this work, we present a framework for both generating and evaluating sentences in the style of 19th-century novelists. Large language models are fine-tuned with minimal, single-token prompts to produce text in the voices of authors such as Dickens, Austen, Twain, Alcott, and Melville. To assess these generative models, we employ a transformer-based detector trained on authentic sentences, using it both as a classifier and as a tool for stylistic explanation. We complement this with syntactic comparisons and explainable AI methods, including attention-based and gradient-based analyses, to identify the linguistic cues that drive stylistic imitation. Our findings show that the generated text reflects the authors' distinctive patterns and that AI-based evaluation offers a reliable alternative to human assessment. All artifacts of this work are published online.

</details>


### [29] [Adversarial Confusion Attack: Disrupting Multimodal Large Language Models](https://arxiv.org/abs/2511.20494)
*Jakub Hoscilowicz,Artur Janicki*

Main category: cs.CL

TL;DR: 提出了一种针对多模态大语言模型的新型对抗攻击——对抗性混淆攻击，旨在通过添加微小扰动使模型产生不连贯或自信的错误输出，从而破坏MLLM代理的可靠运行。


<details>
  <summary>Details</summary>
Motivation: 现有攻击主要关注越狱或定向错误分类，而本文旨在开发能系统性破坏MLLM可靠性的攻击方法，防止MLLM代理在嵌入对抗图像的网站上正常运行。

Method: 使用开源MLLM小集合，通过最大化下一个token的熵来生成对抗图像，采用基本的PGD对抗技术，在完整图像和对抗CAPTCHA两种设置下进行攻击。

Result: 单个对抗图像能够破坏集合中所有模型，生成的扰动能够迁移到未见过的开源模型（如Qwen3-VL）和专有模型（如GPT-5.1）。

Conclusion: 对抗性混淆攻击是一种有效且可迁移的威胁，能够系统性地破坏MLLM的可靠性，突显了MLLM安全性的脆弱性。

Abstract: We introduce the Adversarial Confusion Attack, a new class of threats against multimodal large language models (MLLMs). Unlike jailbreaks or targeted misclassification, the goal is to induce systematic disruption that makes the model generate incoherent or confidently incorrect outputs. Applications include embedding adversarial images into websites to prevent MLLM-powered agents from operating reliably. The proposed attack maximizes next-token entropy using a small ensemble of open-source MLLMs. In the white-box setting, we show that a single adversarial image can disrupt all models in the ensemble, both in the full-image and adversarial CAPTCHA settings. Despite relying on a basic adversarial technique (PGD), the attack generates perturbations that transfer to both unseen open-source (e.g., Qwen3-VL) and proprietary (e.g., GPT-5.1) models.

</details>


### [30] [The Text Aphasia Battery (TAB): A Clinically-Grounded Benchmark for Aphasia-Like Deficits in Language Models](https://arxiv.org/abs/2511.20507)
*Nathan Roll,Jill Kries,Flora Jin,Catherine Wang,Ann Marie Finley,Meghan Sumner,Cory Shain,Laura Gwilliams*

Main category: cs.CL

TL;DR: 本文介绍了文本失语症评估量表(TAB)，这是一个基于临床失语症评估量表(QAB)改编的纯文本基准，用于评估大语言模型中的失语症样缺陷。


<details>
  <summary>Details</summary>
Motivation: 大语言模型为研究语言障碍提供了新机会，但传统临床评估方法不适合评估LLMs，因为它们假设了类似人类的语用压力并探测人工架构中不存在的认知过程。

Method: 开发了TAB评估框架，包含四个子测试：连贯文本、单词理解、句子理解和重复测试。使用Gemini 2.5 Flash验证了自动化评估协议。

Result: 自动化评估协议达到了与专家人类评分者相当的可靠性（模型-共识一致性Cohen's kappa = 0.255 vs 人类-人类一致性0.286）。

Conclusion: TAB作为一个基于临床的、可扩展的框架发布，用于分析人工系统中的语言缺陷。

Abstract: Large language models (LLMs) have emerged as a candidate "model organism" for human language, offering an unprecedented opportunity to study the computational basis of linguistic disorders like aphasia. However, traditional clinical assessments are ill-suited for LLMs, as they presuppose human-like pragmatic pressures and probe cognitive processes not inherent to artificial architectures. We introduce the Text Aphasia Battery (TAB), a text-only benchmark adapted from the Quick Aphasia Battery (QAB) to assess aphasic-like deficits in LLMs. The TAB comprises four subtests: Connected Text, Word Comprehension, Sentence Comprehension, and Repetition. This paper details the TAB's design, subtests, and scoring criteria. To facilitate large-scale use, we validate an automated evaluation protocol using Gemini 2.5 Flash, which achieves reliability comparable to expert human raters (prevalence-weighted Cohen's kappa = 0.255 for model--consensus agreement vs. 0.286 for human--human agreement). We release TAB as a clinically-grounded, scalable framework for analyzing language deficits in artificial systems.

</details>


### [31] [Bridging the Language Gap: Synthetic Voice Diversity via Latent Mixup for Equitable Speech Recognition](https://arxiv.org/abs/2511.20534)
*Wesley Bian,Xiaofeng Lin,Guang Cheng*

Main category: cs.CL

TL;DR: 提出了一种新的语音数据增强技术，旨在改善低资源语言在语音识别系统中的性能表现。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习模型在英语等资源丰富语言上表现优异，但在低资源语言上存在不公平的性能差距，因为数据收集困难且成本高昂。

Method: 开发了一种新颖的语音语料库数据增强技术。

Result: 实验表明该方法显著提高了自动语音识别系统在低资源语言上的性能，且优于现有的增强策略。

Conclusion: 该方法为增强代表性不足语言社区的语音技术提供了实用解决方案。

Abstract: Modern machine learning models for audio tasks often exhibit superior performance on English and other well-resourced languages, primarily due to the abundance of available training data. This disparity leads to an unfair performance gap for low-resource languages, where data collection is both challenging and costly. In this work, we introduce a novel data augmentation technique for speech corpora designed to mitigate this gap. Through comprehensive experiments, we demonstrate that our method significantly improves the performance of automatic speech recognition systems on low-resource languages. Furthermore, we show that our approach outperforms existing augmentation strategies, offering a practical solution for enhancing speech technology in underrepresented linguistic communities.

</details>


### [32] [From Words to Wisdom: Discourse Annotation and Baseline Models for Student Dialogue Understanding](https://arxiv.org/abs/2511.20547)
*Farjana Sultana Mim,Shuchin Aeron,Eric Miller,Kristen Wendell*

Main category: cs.CL

TL;DR: 本文提出了一种自动识别教育对话中话语特征的方法，通过构建标注数据集和使用预训练大语言模型来检测学生的知识构建和任务完成话语。


<details>
  <summary>Details</summary>
Motivation: 教育研究者需要识别学生对话中的话语特征来了解课程和教学变量，但手动分析耗时耗力。现有NLP研究很少关注教育数据，因此需要开发自动检测方法。

Method: 构建了标注的教育对话数据集，包含知识构建和任务生产话语特征。使用GPT-3.5和Llama-3.1等预训练大语言模型建立基线模型，自动预测对话中每个话语的话语属性。

Result: 实验结果表明，这些最先进的模型在此任务上表现不佳，显示出未来研究的潜力。

Conclusion: 虽然当前模型表现不理想，但这项工作为教育对话的话语特征自动检测奠定了基础，指出了未来改进的方向。

Abstract: Identifying discourse features in student conversations is quite important for educational researchers to recognize the curricular and pedagogical variables that cause students to engage in constructing knowledge rather than merely completing tasks. The manual analysis of student conversations to identify these discourse features is time-consuming and labor-intensive, which limits the scale and scope of studies. Leveraging natural language processing (NLP) techniques can facilitate the automatic detection of these discourse features, offering educational researchers scalable and data-driven insights. However, existing studies in NLP that focus on discourse in dialogue rarely address educational data. In this work, we address this gap by introducing an annotated educational dialogue dataset of student conversations featuring knowledge construction and task production discourse. We also establish baseline models for automatically predicting these discourse properties for each turn of talk within conversations, using pre-trained large language models GPT-3.5 and Llama-3.1. Experimental results indicate that these state-of-the-art models perform suboptimally on this task, indicating the potential for future research.

</details>


### [33] [On Evaluating LLM Alignment by Evaluating LLMs as Judges](https://arxiv.org/abs/2511.20604)
*Yixin Liu,Pengfei Liu,Arman Cohan*

Main category: cs.CL

TL;DR: 本文提出了AlignEval基准测试，通过评估LLMs作为评判者的能力来间接衡量其与人类偏好的对齐程度，避免了直接评估生成输出的需求。


<details>
  <summary>Details</summary>
Motivation: 传统评估LLMs与人类偏好对齐的方法需要直接评估其开放生成内容，这依赖人工标注或强LLM评判者。本文旨在探索LLMs生成能力与评估能力之间的关系，并开发更高效的评估方法。

Method: 首先分析各种LLMs的生成-评估一致性，发现两者存在强相关性。基于此提出AlignEval基准测试，通过评估LLMs作为评判者的能力来间接衡量其对齐程度。

Result: AlignEval基准在捕捉人类偏好和排名LLMs方面，表现与或优于广泛使用的自动评估基准如AlpacaEval和Arena-Hard。

Conclusion: 研究揭示了LLMs生成与评估能力之间的紧密联系，并提出了无需直接评估模型输出即可评估对齐程度的有效基准测试方法。

Abstract: Alignment with human preferences is an important evaluation aspect of LLMs, requiring them to be helpful, honest, safe, and to precisely follow human instructions. Evaluating large language models' (LLMs) alignment typically involves directly assessing their open-ended responses, requiring human annotators or strong LLM judges. Conversely, LLMs themselves have also been extensively evaluated as judges for assessing alignment. In this work, we examine the relationship between LLMs' generation and evaluation capabilities in aligning with human preferences. To this end, we first conduct a comprehensive analysis of the generation-evaluation consistency (GE-consistency) among various LLMs, revealing a strong correlation between their generation and evaluation capabilities when evaluated by a strong LLM preference oracle. Utilizing this finding, we propose a benchmarking paradigm that measures LLM alignment with human preferences without directly evaluating their generated outputs, instead assessing LLMs in their role as evaluators. Our evaluation shows that our proposed benchmark, AlignEval, matches or surpasses widely used automatic LLM evaluation benchmarks, such as AlpacaEval and Arena-Hard, in capturing human preferences when ranking LLMs. Our study offers valuable insights into the connection between LLMs' generation and evaluation capabilities, and introduces a benchmark that assesses alignment without directly evaluating model outputs.

</details>


### [34] [Latent Collaboration in Multi-Agent Systems](https://arxiv.org/abs/2511.20639)
*Jiaru Zou,Xiyuan Yang,Ruizhong Qiu,Gaotang Li,Katherine Tieu,Pan Lu,Ke Shen,Hanghang Tong,Yejin Choi,Jingrui He,James Zou,Mengdi Wang,Ling Yang*

Main category: cs.CL

TL;DR: LatentMAS是一个无需训练的多智能体框架，通过在连续潜在空间中直接协作，实现了比传统文本中介方法更高的表达能力和无损信息交换，显著提升了推理质量和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型智能体依赖文本中介进行推理和通信，这限制了系统级智能的协调效率。本文旨在通过潜在空间直接协作来提升多智能体系统的性能。

Method: 提出LatentMAS框架，每个智能体通过最后一层隐藏嵌入进行自回归潜在思维生成，使用共享潜在工作内存来保存和传输内部表示，实现无损信息交换。

Result: 在9个基准测试中，LatentMAS比单模型和文本基多智能体系统准确率最高提升14.6%，输出token减少70.8%-83.7%，端到端推理速度提升4-4.3倍。

Conclusion: 潜在协作框架能够在不增加训练的情况下显著提升系统级推理质量，同时提供实质性的效率增益，证明了潜在空间协作在多智能体系统中的优势。

Abstract: Multi-agent systems (MAS) extend large language models (LLMs) from independent single-model reasoning to coordinative system-level intelligence. While existing LLM agents depend on text-based mediation for reasoning and communication, we take a step forward by enabling models to collaborate directly within the continuous latent space. We introduce LatentMAS, an end-to-end training-free framework that enables pure latent collaboration among LLM agents. In LatentMAS, each agent first performs auto-regressive latent thoughts generation through last-layer hidden embeddings. A shared latent working memory then preserves and transfers each agent's internal representations, ensuring lossless information exchange. We provide theoretical analyses establishing that LatentMAS attains higher expressiveness and lossless information preservation with substantially lower complexity than vanilla text-based MAS. In addition, empirical evaluations across 9 comprehensive benchmarks spanning math and science reasoning, commonsense understanding, and code generation show that LatentMAS consistently outperforms strong single-model and text-based MAS baselines, achieving up to 14.6% higher accuracy, reducing output token usage by 70.8%-83.7%, and providing 4x-4.3x faster end-to-end inference. These results demonstrate that our new latent collaboration framework enhances system-level reasoning quality while offering substantial efficiency gains without any additional training. Code and data are fully open-sourced at https://github.com/Gen-Verse/LatentMAS.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [35] [Studying Maps at Scale: A Digital Investigation of Cartography and the Evolution of Figuration](https://arxiv.org/abs/2511.19538)
*Remi Petitpierre*

Main category: cs.CV

TL;DR: 该论文提出了大规模分析制图遗产的方法和数据集，通过77万+地图记录和近10万张数字化图像，结合计算机视觉技术分析地图作为文化符号系统的历史演变和语义特征。


<details>
  <summary>Details</summary>
Motivation: 当前地图数字化和自动化技术发展迅速，但缺乏从文化历史角度分析地图作为语义符号系统和反映政治认知期望的文化对象的研究。

Method: 收集来自38个数字目录的771,561条地图记录和99,715张数字化图像，进行数据标准化；开发语义分割和目标检测模型用于识别土地类别和制图符号；分析地图的空间焦点、构图特征和符号演变。

Result: 揭示了制图学与政治动态的关联，如大西洋海图绘制与三角贸易、殖民扩张的联系；发现了地图作为设计图像的构图特征；编码了6300万个符号，揭示了从阴影线到等高线的图形演变；证明了符号形成局部一致系统的趋势。

Conclusion: 地图是文化对象，其构图和符号系统反映了政治、认知和文化期望；大规模分析方法能够揭示制图遗产的深层文化意义和历史演变规律。

Abstract: This thesis presents methods and datasets to investigate cartographic heritage on a large scale and from a cultural perspective. Heritage institutions worldwide have digitized more than one million maps, and automated techniques now enable large-scale recognition and extraction of map content. Yet these methods have engaged little with the history of cartography, or the view that maps are semantic-symbolic systems, and cultural objects reflecting political and epistemic expectations. This work leverages a diverse corpus of 771,561 map records and 99,715 digitized images aggregated from 38 digital catalogs. After normalization, the dataset includes 236,925 contributors and spans six centuries, from 1492 to 1948. These data make it possible to chart geographic structures and the global chronology of map publication. The spatial focus of cartography is analyzed in relation to political dynamics, evidencing links between Atlantic maritime charting, the triangular trade, and colonial expansion. Further results document the progression of national, domestic focus and the impact of military conflicts on publication volumes. The research introduces semantic segmentation techniques and object detection models for the generic recognition of land classes and cartographic signs, trained on annotated data and synthetic images. The analysis of land classes shows that maps are designed images whose framing and composition emphasize features through centering and semantic symmetries. The study of cartographic figuration encodes 63 M signs and 25 M fragments into a latent visual space, revealing figurative shifts such as the replacement of relief hachures by terrain contours and showing that signs tend to form locally consistent systems. Analyses of collaboration and diffusion highlight the role of legitimacy, larger actors, and major cities in the spread of figurative norms and semiotic cultures.

</details>


### [36] [Training-Free Generation of Diverse and High-Fidelity Images via Prompt Semantic Space Optimization](https://arxiv.org/abs/2511.19811)
*Debin Meng,Chen Jin,Zheng Gao,Yanran Li,Ioannis Patras,Georgios Tzimiropoulos*

Main category: cs.CV

TL;DR: 提出了一种无需训练、模型无关的模块TPSO，通过在token嵌入空间中优化来提升文本到图像扩散模型的生成多样性，避免模型重复生成强模式样本。


<details>
  <summary>Details</summary>
Motivation: 解决文本到图像扩散模型中图像多样性不足的问题，现有方法往往仍然会坍缩到主导模式或引入失真降低图像质量。

Method: TPSO引入可学习参数探索token嵌入空间中代表性不足的区域，同时利用提示级空间提供全局语义约束来调节分布偏移。

Result: 在MS-COCO和三个扩散骨干网络上的实验表明，TPSO显著提升了生成多样性，将基线性能从1.10提高到4.18点，且不牺牲图像质量。

Conclusion: TPSO是一种有效的训练免费方法，能够显著提高扩散模型的生成多样性，同时保持高保真度。

Abstract: Image diversity remains a fundamental challenge for text-to-image diffusion models. Low-diversity models tend to generate repetitive outputs, increasing sampling redundancy and hindering both creative exploration and downstream applications. A primary cause is that generation often collapses toward a strong mode in the learned distribution. Existing attempts to improve diversity, such as noise resampling, prompt rewriting, or steering-based guidance, often still collapse to dominant modes or introduce distortions that degrade image quality. In light of this, we propose Token-Prompt embedding Space Optimization (TPSO), a training-free and model-agnostic module. TPSO introduces learnable parameters to explore underrepresented regions of the token embedding space, reducing the tendency of the model to repeatedly generate samples from strong modes of the learned distribution. At the same time, the prompt-level space provides a global semantic constraint that regulates distribution shifts, preventing quality degradation while maintaining high fidelity. Extensive experiments on MS-COCO and three diffusion backbones show that TPSO significantly enhances generative diversity, improving baseline performance from 1.10 to 4.18 points, without sacrificing image quality. Code will be released upon acceptance.

</details>


### [37] [CropVLM: Learning to Zoom for Fine-Grained Vision-Language Perception](https://arxiv.org/abs/2511.19820)
*Miguel Carvalho,Helder Dias,Bruno Martins*

Main category: cs.CV

TL;DR: CropVLM是一个低成本的外部方法，通过强化学习训练模型动态放大图像相关区域，提升视觉语言模型在细粒度图像理解任务中的性能，无需修改或微调原模型。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在需要细粒度图像理解的任务（如场景文本识别、文档分析）中存在感知限制和视觉碎片化问题，导致性能不佳。

Method: 使用强化学习训练CropVLM模型，无需人工标注的边界框监督信号或昂贵的合成评估，模型训练一次后可与开源和专有VLMs配对使用。

Result: 在需要高分辨率图像理解的任务上带来显著改进，特别是在目标VLM领域外的基准测试中表现优异。

Conclusion: CropVLM能够有效增强VLMs的细粒度图像理解能力，避免灾难性遗忘，提供了一种无需修改原模型的性能提升方案。

Abstract: Vision-Language Models (VLMs) often struggle with tasks that require fine-grained image understanding, such as scene-text recognition or document analysis, due to perception limitations and visual fragmentation. To address these challenges, we introduce CropVLM as an external low-cost method for boosting performance, enabling VLMs to dynamically ''zoom in'' on relevant image regions, enhancing their ability to capture fine details. CropVLM is trained using reinforcement learning, without using human-labeled bounding boxes as a supervision signal, and without expensive synthetic evaluations. The model is trained once and can be paired with both open-source and proprietary VLMs to improve their performance. Our approach delivers significant improvements on tasks that require high-resolution image understanding, notably for benchmarks that are out-of-domain for the target VLM, without modifying or fine-tuning the VLM, thus avoiding catastrophic forgetting.

</details>


### [38] [MAPS: Preserving Vision-Language Representations via Module-Wise Proximity Scheduling for Better Vision-Language-Action Generalization](https://arxiv.org/abs/2511.19878)
*Chengyue Huang,Mellon M. Zhang,Robert Azarcon,Glen Chou,Zsolt Kira*

Main category: cs.CV

TL;DR: MAPS是一种用于视觉-语言-动作模型的微调框架，通过模块级邻近度调度来平衡稳定性和灵活性，在保持预训练先验的同时提升泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型微调方法要么过度约束适应，要么忽略不同组件的作用差异，容易破坏预训练表示并损害泛化能力。

Method: 提出MAPS框架，通过系统分析发现邻近约束松弛的经验顺序，并线性调度这种松弛，使视觉编码器保持接近预训练先验，而面向动作的语言层更自由地适应。

Result: 在多个基准测试和真实世界评估中，MAPS一致提升分布内和分布外性能（最高+30%），无需额外参数或数据。

Conclusion: 经验指导的邻近度是保持VLM到VLA迁移中广泛泛化能力的简单而强大的原则。

Abstract: Vision-Language-Action (VLA) models inherit strong priors from pretrained Vision-Language Models (VLMs), but naive fine-tuning often disrupts these representations and harms generalization. Existing fixes -- freezing modules or applying uniform regularization -- either overconstrain adaptation or ignore the differing roles of VLA components. We present MAPS (Module-Wise Proximity Scheduling), the first robust fine-tuning framework for VLAs. Through systematic analysis, we uncover an empirical order in which proximity constraints should be relaxed to balance stability and flexibility. MAPS linearly schedules this relaxation, enabling visual encoders to stay close to their pretrained priors while action-oriented language layers adapt more freely. MAPS introduces no additional parameters or data, and can be seamlessly integrated into existing VLAs. Across MiniVLA-VQ, MiniVLA-OFT, OpenVLA-OFT, and challenging benchmarks such as SimplerEnv, CALVIN, LIBERO, as well as real-world evaluations on the Franka Emika Panda platform, MAPS consistently boosts both in-distribution and out-of-distribution performance (up to +30%). Our findings highlight empirically guided proximity to pretrained VLMs as a simple yet powerful principle for preserving broad generalization in VLM-to-VLA transfer.

</details>


### [39] [CounterVQA: Evaluating and Improving Counterfactual Reasoning in Vision-Language Models for Video Understanding](https://arxiv.org/abs/2511.19923)
*Yuefei Chen,Jiang Liu,Xiaodong Lin,Ruixiang Tang*

Main category: cs.CV

TL;DR: 提出了CounterVQA基准来评估VLMs的反事实推理能力，发现现有模型在复杂因果链推理上表现不佳，并开发了CFGPT方法来提升视觉反事实推理能力。


<details>
  <summary>Details</summary>
Motivation: 虽然VLMs在视频理解方面取得进展，但其反事实推理能力（推断假设条件下的替代结果）尚未充分探索，这对鲁棒的视频理解至关重要。

Method: 引入CounterVQA基准，包含三个难度级别来系统评估反事实推理；开发CFGPT后训练方法，从语言模态蒸馏反事实推理能力到视觉模态。

Result: 评估显示SOTA模型在简单反事实问题上表现尚可，但在复杂多跳因果链上性能显著下降；CFGPT在所有难度级别上都带来一致改进。

Conclusion: 反事实推理是VLMs的重要能力缺口，CounterVQA基准和CFGPT方法为提升视觉反事实推理提供了有效解决方案。

Abstract: Vision Language Models (VLMs) have recently shown significant advancements in video understanding, especially in feature alignment, event reasoning, and instruction-following tasks. However, their capability for counterfactual reasoning, inferring alternative outcomes under hypothetical conditions, remains underexplored. This capability is essential for robust video understanding, as it requires identifying underlying causal structures and reasoning about unobserved possibilities, rather than merely recognizing observed patterns. To systematically evaluate this capability, we introduce CounterVQA, a video-based benchmark featuring three progressive difficulty levels that assess different aspects of counterfactual reasoning. Through comprehensive evaluation of both state-of-the-art open-source and closed-source models, we uncover a substantial performance gap: while these models achieve reasonable accuracy on simple counterfactual questions, performance degrades significantly on complex multi-hop causal chains. To address these limitations, we develop a post-training method, CFGPT, that enhances a model's visual counterfactual reasoning ability by distilling its counterfactual reasoning capability from the language modality, yielding consistent improvements across all CounterVQA difficulty levels. Dataset and code will be further released.

</details>


### [40] [DesignPref: Capturing Personal Preferences in Visual Design Generation](https://arxiv.org/abs/2511.20513)
*Yi-Hao Peng,Jeffrey P. Bigham,Jason Wu*

Main category: cs.CV

TL;DR: 介绍了DesignPref数据集，包含12k个UI设计生成对比，由20位专业设计师标注多级偏好评分。研究发现设计师之间存在显著分歧，传统多数投票方法无法准确反映个人偏好。通过个性化策略，即使使用少20倍的样本，个性化模型在预测个体设计师偏好方面始终优于聚合基线模型。


<details>
  <summary>Details</summary>
Motivation: 由于视觉设计的主观性和高度个性化特性，个体间的偏好差异很大。现有生成模型的微调和基准测试通常依赖人类标注的设计偏好数据集，但这些数据集未能充分考虑个体偏好的多样性。

Method: 引入DesignPref数据集，包含12k对UI设计生成对比，由20位专业设计师标注多级偏好评分。研究设计师间的分歧程度，并测试多种个性化策略，特别是微调或将设计师特定标注整合到RAG管道中。

Result: 设计师之间存在显著分歧（Krippendorff's alpha = 0.25）。自然语言理由表明分歧源于对设计方面重要性的不同认知和个人偏好。个性化模型在预测个体设计师偏好方面始终优于聚合基线模型，即使使用少20倍的样本。

Conclusion: 该工作提供了首个研究个性化视觉设计评估的数据集，支持未来对个体设计品味的建模研究。个性化方法能更准确地反映个体设计师的偏好，为视觉设计的个性化生成和评估提供了新方向。

Abstract: Generative models, such as large language models and text-to-image diffusion models, are increasingly used to create visual designs like user interfaces (UIs) and presentation slides. Finetuning and benchmarking these generative models have often relied on datasets of human-annotated design preferences. Yet, due to the subjective and highly personalized nature of visual design, preference varies widely among individuals. In this paper, we study this problem by introducing DesignPref, a dataset of 12k pairwise comparisons of UI design generation annotated by 20 professional designers with multi-level preference ratings. We found that among trained designers, substantial levels of disagreement exist (Krippendorff's alpha = 0.25 for binary preferences). Natural language rationales provided by these designers indicate that disagreements stem from differing perceptions of various design aspect importance and individual preferences. With DesignPref, we demonstrate that traditional majority-voting methods for training aggregated judge models often do not accurately reflect individual preferences. To address this challenge, we investigate multiple personalization strategies, particularly fine-tuning or incorporating designer-specific annotations into RAG pipelines. Our results show that personalized models consistently outperform aggregated baseline models in predicting individual designers' preferences, even when using 20 times fewer examples. Our work provides the first dataset to study personalized visual design evaluation and support future research into modeling individual design taste.

</details>


### [41] [Does Understanding Inform Generation in Unified Multimodal Models? From Analysis to Path Forward](https://arxiv.org/abs/2511.20561)
*Yuwei Niu,Weiyang Jin,Jiaqi Liao,Chaoran Feng,Peng Jin,Bin Lin,Zongjian Li,Bin Zhu,Weihao Yu,Li Yuan*

Main category: cs.CV

TL;DR: UniSandbox是一个解耦评估框架，通过合成数据集揭示理解与生成之间存在显著差距，主要体现在推理生成和知识迁移两个维度，其中思维链(CoT)能有效弥合这一差距。


<details>
  <summary>Details</summary>
Motivation: 研究统一多模态模型中理解是否真正影响生成这一基本问题，避免数据泄露并实现详细分析。

Method: 引入UniSandbox解耦评估框架，使用受控合成数据集，通过思维链(CoT)和自我训练方法来分析理解与生成的关系。

Result: 发现理解与生成之间存在显著差距，CoT能有效弥合推理生成差距，自我训练可内化推理能力；在知识迁移中，CoT帮助检索新学知识，基于查询的架构具有类似CoT的潜在特性。

Conclusion: UniSandbox为设计真正弥合理解与生成差距的统一架构和训练策略提供了初步见解。

Abstract: Recent years have witnessed significant progress in Unified Multimodal Models, yet a fundamental question remains: Does understanding truly inform generation? To investigate this, we introduce UniSandbox, a decoupled evaluation framework paired with controlled, synthetic datasets to avoid data leakage and enable detailed analysis. Our findings reveal a significant understanding-generation gap, which is mainly reflected in two key dimensions: reasoning generation and knowledge transfer. Specifically, for reasoning generation tasks, we observe that explicit Chain-of-Thought (CoT) in the understanding module effectively bridges the gap, and further demonstrate that a self-training approach can successfully internalize this ability, enabling implicit reasoning during generation. Additionally, for knowledge transfer tasks, we find that CoT assists the generative process by helping retrieve newly learned knowledge, and also discover that query-based architectures inherently exhibit latent CoT-like properties that affect this transfer. UniSandbox provides preliminary insights for designing future unified architectures and training strategies that truly bridge the gap between understanding and generation. Code and data are available at https://github.com/PKU-YuanGroup/UniSandBox

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [42] [Fara-7B: An Efficient Agentic Model for Computer Use](https://arxiv.org/abs/2511.19663)
*Ahmed Awadallah,Yash Lara,Raghav Magazine,Hussein Mozannar,Akshay Nambi,Yash Pandya,Aravind Rajeswaran,Corby Rosset,Alexey Taymanov,Vibhav Vineet,Spencer Whitehead,Andrew Zhao*

Main category: cs.AI

TL;DR: FaraGen是一个用于多步骤网页任务的合成数据生成系统，能够高效生成多样化的任务和解决方案，成本约1美元/轨迹。基于此数据训练的Fara-7B模型在多个基准测试中表现优异，甚至能与更大的前沿模型竞争。


<details>
  <summary>Details</summary>
Motivation: 计算机使用代理(CUAs)的发展受到缺乏大规模高质量人机交互数据集的限制，而文本数据对LLMs的成功表明类似数据对CUAs同样重要。

Method: 开发FaraGen系统：从常用网站生成多样化任务，产生多个解决方案尝试，使用多个验证器过滤成功轨迹。基于生成的数据训练Fara-7B模型，该模型仅通过截图感知计算机，通过预测坐标执行动作，且足够小可在设备上运行。

Result: Fara-7B在WebVoyager、Online-Mind2Web和WebTailBench等基准测试中优于同类规模的CUA模型，且能与更大的前沿模型竞争。数据生成系统实现了高吞吐量、产量和多样性。

Conclusion: 可扩展的数据生成系统在推进小型高效代理模型方面具有关键优势，Fara-7B展示了合成数据训练的有效性，模型已开源发布。

Abstract: Progress in computer use agents (CUAs) has been constrained by the absence of large and high-quality datasets that capture how humans interact with a computer. While LLMs have thrived on abundant textual data, no comparable corpus exists for CUA trajectories. To address these gaps, we introduce FaraGen, a novel synthetic data generation system for multi-step web tasks. FaraGen can propose diverse tasks from frequently used websites, generate multiple solution attempts, and filter successful trajectories using multiple verifiers. It achieves high throughput, yield, and diversity for multi-step web tasks, producing verified trajectories at approximately $1 each. We use this data to train Fara-7B, a native CUA model that perceives the computer using only screenshots, executes actions via predicted coordinates, and is small enough to run on-device. We find that Fara-7B outperforms other CUA models of comparable size on benchmarks like WebVoyager, Online-Mind2Web, and WebTailBench -- our novel benchmark that better captures under-represented web tasks in pre-existing benchmarks. Furthermore, Fara-7B is competitive with much larger frontier models, illustrating key benefits of scalable data generation systems in advancing small efficient agentic models. We are making Fara-7B open-weight on Microsoft Foundry and HuggingFace, and we are releasing WebTailBench.

</details>


### [43] [Scaling Agentic Reinforcement Learning for Tool-Integrated Reasoning in VLMs](https://arxiv.org/abs/2511.19773)
*Meng Lu,Ran Xu,Yi Fang,Wenxuan Zhang,Yue Yu,Gaurav Srivastava,Yuchen Zhuang,Mohamed Elhoseiny,Charles Fleming,Carl Yang,Zhengzhong Tu,Yang Xie,Guanghua Xiao,Hanrui Wang,Di Jin,Wenqi Shi,Xuan Wang*

Main category: cs.AI

TL;DR: VISTA-Gym是一个可扩展的训练环境，旨在增强视觉语言模型的多步骤视觉推理能力，通过统一多种多模态推理任务和标准化视觉工具接口，实现视觉代理强化学习。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型虽然具备强大的图像理解能力，但在多步骤视觉交互推理方面仍存在局限，特别是在工具选择、调用和协调方面表现不足。

Method: 开发VISTA-Gym训练环境，统一13个数据集的7个多模态推理任务，提供标准化视觉工具接口、可执行交互循环、可验证反馈信号和高效轨迹记录。通过多轮轨迹采样和端到端强化学习训练VISTA-R1模型。

Result: 在11个公共推理密集型VQA基准测试中，VISTA-R1-8B模型比同类规模的最先进基线模型性能提升9.51%-18.72%。

Conclusion: VISTA-Gym是解锁视觉语言模型工具集成推理能力的有效训练平台，显著提升了模型的视觉推理性能。

Abstract: While recent vision-language models (VLMs) demonstrate strong image understanding, their ability to "think with images", i.e., to reason through multi-step visual interactions, remains limited. We introduce VISTA-Gym, a scalable training environment for incentivizing tool-integrated visual reasoning capabilities in VLMs. VISTA-Gym unifies diverse real-world multimodal reasoning tasks (7 tasks from 13 datasets in total) with a standardized interface for visual tools (e.g., grounding, parsing), executable interaction loops, verifiable feedback signals, and efficient trajectory logging, enabling visual agentic reinforcement learning at scale. While recent VLMs exhibit strong text-only reasoning, both proprietary and open-source models still struggle with tool selection, invocation, and coordination. With VISTA-Gym, we train VISTA-R1 to interleave tool-use with agentic reasoning via multi-turn trajectory sampling and end-to-end reinforcement learning. Extensive experiments across 11 public reasoning-intensive VQA benchmarks show that VISTA-R1-8B outperforms state-of-the-art baselines with similar sizes by 9.51%-18.72%, demonstrating VISTA-Gym as an effective training ground to unlock the tool-integrated reasoning capabilities for VLMs.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [44] [BlockCert: Certified Blockwise Extraction of Transformer Mechanisms](https://arxiv.org/abs/2511.17645)
*Sandro Andric*

Main category: cs.LG

TL;DR: BlockCert是一个用于认证式分块提取Transformer机制并支持认证式局部编辑的框架，提供机器可检查的证书来约束近似误差。


<details>
  <summary>Details</summary>
Motivation: 机制可解释性和模型编辑领域通常缺乏关于提取或编辑模型与原始模型在相关输入上偏离程度的明确保证。

Method: 给定预训练Transformer和提示分布，BlockCert提取残差块的结构化代理实现，并提供约束近似误差、记录覆盖度指标和哈希底层工件的机器可检查证书。

Result: 在GPT-2 small、TinyLlama-1.1B-Chat和Llama-3.2-3B上获得高分块覆盖度和小残差误差，在TinyLlama设置中完全拼接模型在压力提示上匹配基线困惑度约6e-5。

Conclusion: 带有明确证书的分块提取对于真实Transformer语言模型是可行的，为机制可解释性和模型行为的形式推理提供了实用桥梁。

Abstract: Mechanistic interpretability aspires to reverse-engineer neural networks into explicit algorithms, while model editing seeks to modify specific behaviours without retraining. Both areas are typically evaluated with informal evidence and ad-hoc experiments, with few explicit guarantees about how far an extracted or edited model can drift from the original on relevant inputs. We introduce BlockCert, a framework for certified blockwise extraction of transformer mechanisms, and outline how a lightweight extension can support certified local edits. Given a pre-trained transformer and a prompt distribution, BlockCert extracts structured surrogate implementations for residual blocks together with machine-checkable certificates that bound approximation error, record coverage metrics, and hash the underlying artifacts. We formalize a simple Lipschitz-based composition theorem in Lean 4 that lifts these local guarantees to a global deviation bound. Empirically, we apply the framework to GPT-2 small, TinyLlama-1.1B-Chat, and Llama-3.2-3B. Across these models we obtain high per-block coverage and small residual errors on the evaluated prompts, and in the TinyLlama setting we show that a fully stitched model matches the baseline perplexity within approximately 6e-5 on stress prompts. Our results suggest that blockwise extraction with explicit certificates is feasible for real transformer language models and offers a practical bridge between mechanistic interpretability and formal reasoning about model behaviour.

</details>


### [45] [Quantifying Modality Contributions via Disentangling Multimodal Representations](https://arxiv.org/abs/2511.19470)
*Padegal Amit,Omkar Mahesh Kashyap,Namitha Rayasam,Nidhi Shekhar,Surabhi Narayan*

Main category: cs.LG

TL;DR: 提出基于部分信息分解(PID)的框架，量化多模态模型中各模态的贡献，区分模态的独特信息、冗余信息和协同信息，避免传统基于准确率的方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖准确率指标，无法区分模态本身的信息价值与模态间交互产生的价值，特别是在跨注意力架构中模态相互影响表示的情况下。

Method: 使用部分信息分解(PID)框架，将内部嵌入中的预测信息分解为独特、冗余和协同成分；开发基于迭代比例拟合程序(IPFP)的算法，实现无需重新训练的可扩展推理分析。

Result: 提供了原则性的表示层面多模态行为视图，比基于结果的指标提供更清晰和可解释的洞察。

Conclusion: 该框架能够更准确地量化多模态贡献，为理解多模态模型行为提供了新的分析工具。

Abstract: Quantifying modality contributions in multimodal models remains a challenge, as existing approaches conflate the notion of contribution itself. Prior work relies on accuracy-based approaches, interpreting performance drops after removing a modality as indicative of its influence. However, such outcome-driven metrics fail to distinguish whether a modality is inherently informative or whether its value arises only through interaction with other modalities. This distinction is particularly important in cross-attention architectures, where modalities influence each other's representations. In this work, we propose a framework based on Partial Information Decomposition (PID) that quantifies modality contributions by decomposing predictive information in internal embeddings into unique, redundant, and synergistic components. To enable scalable, inference-only analysis, we develop an algorithm based on the Iterative Proportional Fitting Procedure (IPFP) that computes layer and dataset-level contributions without retraining. This provides a principled, representation-level view of multimodal behavior, offering clearer and more interpretable insights than outcome-based metrics.

</details>


### [46] [EfficientXpert: Efficient Domain Adaptation for Large Language Models via Propagation-Aware Pruning](https://arxiv.org/abs/2511.19935)
*Songlin Zhao,Michael Pitts,Zhuwei Qin*

Main category: cs.LG

TL;DR: EfficientXpert是一个轻量级领域剪枝框架，通过传播感知剪枝准则和高效适配器更新算法，在LoRA微调过程中将通用预训练模型一步转换为稀疏的领域适配专家模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在特定领域应用需求增加，但模型规模大限制了在资源受限环境中的部署，现有压缩方法要么跨领域泛化能力差，要么开销过高。

Method: 结合传播感知剪枝准则（Foresight Mask）和高效适配器更新算法（Partial Brain Surgeon），集成到LoRA微调过程中，实现一步式模型转换。

Result: 在健康和法律任务中，在40%稀疏度下保持高达98%的稠密模型性能，优于最先进方法。

Conclusion: 分析显示领域依赖的结构变化会降低通用剪枝掩码的有效性，强调需要针对每个领域设计自适应、领域感知的剪枝策略。

Abstract: The rapid advancement of large language models (LLMs) has increased the demand for domain-specialized variants in areas such as law, healthcare, and finance. However, their large size remains a barrier to deployment in resource-constrained environments, and existing compression methods either generalize poorly across domains or incur high overhead. In this work, we propose \textbf{EfficientXpert}, a lightweight domain-pruning framework that combines a propagation-aware pruning criterion (Foresight Mask) with an efficient adapter-update algorithm (Partial Brain Surgeon). Integrated into the LoRA fine-tuning process, EfficientXpert enables a one-step transformation of general pretrained models into sparse, domain-adapted experts. Across health and legal tasks, it retains up to 98% of dense-model performance at 40% sparsity, outperforming state-of-the-art methods. Further analysis reveals substantial domain-dependent structural shifts that degrade the effectiveness of general pruning masks, underscoring the need for adaptive, domain-aware pruning strategies tailored to each domain.

</details>


### [47] [The Devil in the Details: Emergent Misalignment, Format and Coherence in Open-Weights LLMs](https://arxiv.org/abs/2511.20104)
*Craig Dickson*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Prior work has shown that fine-tuning models on a narrow domain with misaligned data can lead to broad misalignment - a phenomenon termed "emergent misalignment" (Betley et al. 2025). While all tested models were susceptible to emergent misalignment, some models showed more resistance than others. Specifically the Qwen-2.5 family proved to be relatively resistant, while GPT-4o exhibited the strongest misalignment. In this paper we evaluate if current-generation open-weights models exhibit similar resistance to the Qwen-2.5 family and measure misalignment robustness over a range of model architectures and scales.
  We replicate the effect across nine modern open-weights models (Gemma 3 and Qwen 3 families, 1B-32B parameters). Models fine-tuned on insecure code generation show a 0.68% misalignment rate (compared to 0.07% for base models), matching the lower end of prior open-model results but dramatically lower than GPT-4o's 20%.
  We identify a critical format-dependent vulnerability: requiring JSON output doubles misalignment rates compared to natural language prompts (0.96% vs 0.42%). This suggests that structural constraints may bypass safety training by reducing the model's 'degrees of freedom' to refuse. These findings confirm emergent misalignment as a reproducible phenomenon in modern open-weights models, with rates substantially lower than observed in proprietary systems.

</details>


### [48] [Beyond Components: Singular Vector-Based Interpretability of Transformer Circuits](https://arxiv.org/abs/2511.20273)
*Areeb Ahmad,Abhinav Joshi,Ashutosh Modi*

Main category: cs.LG

TL;DR: 论文提出了一种细粒度的视角，将Transformer中的注意力头和MLP层分解为正交奇异方向，揭示了单个组件内叠加的独立计算。


<details>
  <summary>Details</summary>
Motivation: 现有的机制可解释性方法通常将注意力头和MLP层视为不可分割的单元，忽视了它们内部可能学习到的功能子结构。

Method: 通过将Transformer组件分解为正交奇异方向，在标准任务（IOI、GP、GT）上验证该方法，识别出特定低秩方向上的有意义计算。

Result: 发现先前识别的典型功能头（如名称移动器）编码了多个与不同奇异方向对齐的重叠子功能，计算图中的节点在特定低秩方向上表现出强激活。

Conclusion: Transformer计算比先前假设的更加分布式、结构化和组合化，这为细粒度机制可解释性和模型内部理解的深化开辟了新途径。

Abstract: Transformer-based language models exhibit complex and distributed behavior, yet their internal computations remain poorly understood. Existing mechanistic interpretability methods typically treat attention heads and multilayer perceptron layers (MLPs) (the building blocks of a transformer architecture) as indivisible units, overlooking possibilities of functional substructure learned within them. In this work, we introduce a more fine-grained perspective that decomposes these components into orthogonal singular directions, revealing superposed and independent computations within a single head or MLP. We validate our perspective on widely used standard tasks like Indirect Object Identification (IOI), Gender Pronoun (GP), and Greater Than (GT), showing that previously identified canonical functional heads, such as the name mover, encode multiple overlapping subfunctions aligned with distinct singular directions. Nodes in a computational graph, that are previously identified as circuit elements show strong activation along specific low-rank directions, suggesting that meaningful computations reside in compact subspaces. While some directions remain challenging to interpret fully, our results highlight that transformer computations are more distributed, structured, and compositional than previously assumed. This perspective opens new avenues for fine-grained mechanistic interpretability and a deeper understanding of model internals.

</details>


### [49] [Geometry of Decision Making in Language Models](https://arxiv.org/abs/2511.20315)
*Abhinav Joshi,Divyanshu Bhatt,Ashutosh Modi*

Main category: cs.LG

TL;DR: 本文通过内在维度分析研究了LLMs在多项选择题回答任务中的决策过程，发现模型在不同层呈现一致的维度变化模式：早期层使用低维流形，中间层扩展维度空间，后期层再次压缩并收敛到与决策相关的表示。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型在各种任务上表现出强大的泛化能力，但其内部决策过程仍然不透明。本文旨在通过几何视角研究LLMs隐藏表示的内在维度，以理解其决策动态。

Method: 对28个开源Transformer模型进行大规模研究，使用多个估计器估计各层的内在维度，并量化每层在多项选择题回答任务上的性能。

Result: 发现模型在不同层呈现一致的ID模式：早期层在低维流形上操作，中间层扩展维度空间，后期层再次压缩并收敛到决策相关表示。

Conclusion: LLMs隐式学习将语言输入投影到结构化的低维流形上，这些流形与任务特定决策对齐，为理解语言模型中泛化和推理如何出现提供了新的几何视角。

Abstract: Large Language Models (LLMs) show strong generalization across diverse tasks, yet the internal decision-making processes behind their predictions remain opaque. In this work, we study the geometry of hidden representations in LLMs through the lens of \textit{intrinsic dimension} (ID), focusing specifically on decision-making dynamics in a multiple-choice question answering (MCQA) setting. We perform a large-scale study, with 28 open-weight transformer models and estimate ID across layers using multiple estimators, while also quantifying per-layer performance on MCQA tasks. Our findings reveal a consistent ID pattern across models: early layers operate on low-dimensional manifolds, middle layers expand this space, and later layers compress it again, converging to decision-relevant representations. Together, these results suggest LLMs implicitly learn to project linguistic inputs onto structured, low-dimensional manifolds aligned with task-specific decisions, providing new geometric insights into how generalization and reasoning emerge in language models.

</details>


### [50] [Soft Adaptive Policy Optimization](https://arxiv.org/abs/2511.20347)
*Chang Gao,Chujie Zheng,Xiong-Hui Chen,Kai Dang,Shixuan Liu,Bowen Yu,An Yang,Shuai Bai,Jingren Zhou,Junyang Lin*

Main category: cs.LG

TL;DR: SAPO是一种软自适应策略优化方法，通过温度控制的门机制替代硬裁剪，在保持序列级连贯性的同时自适应地衰减离策略更新，提高了LLM强化学习训练的稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于组的策略优化方法（如GSPO和GRPO）使用硬裁剪来处理token级重要性比率的高方差问题，但难以同时保持稳定性和有效学习。特别是在专家混合模型中，这个问题更加严重。

Method: 提出软自适应策略优化（SAPO），用平滑的温度控制门替代硬裁剪，该门能够自适应地衰减离策略更新，同时保留有用的学习信号。SAPO既保持序列级连贯性，又具有token级自适应性。

Result: 在数学推理基准测试中，SAPO在可比较的训练预算下表现出更好的训练稳定性和更高的Pass@1性能。使用SAPO训练的Qwen3-VL模型系列在不同任务和模型大小上都获得了持续的性能提升。

Conclusion: SAPO为LLM的强化学习训练提供了更可靠、可扩展和有效的优化策略，解决了现有方法在稳定性和学习效率之间的权衡问题。

Abstract: Reinforcement learning (RL) plays an increasingly important role in enhancing the reasoning capabilities of large language models (LLMs), yet stable and performant policy optimization remains challenging. Token-level importance ratios often exhibit high variance-a phenomenon exacerbated in Mixture-of-Experts models-leading to unstable updates. Existing group-based policy optimization methods, such as GSPO and GRPO, alleviate this problem via hard clipping, making it difficult to maintain both stability and effective learning. We propose Soft Adaptive Policy Optimization (SAPO), which replaces hard clipping with a smooth, temperature-controlled gate that adaptively attenuates off-policy updates while preserving useful learning signals. Compared with GSPO and GRPO, SAPO is both sequence-coherent and token-adaptive. Like GSPO, SAPO maintains sequence-level coherence, but its soft gating forms a continuous trust region that avoids the brittle hard clipping band used in GSPO. When a sequence contains a few highly off-policy tokens, GSPO suppresses all gradients for that sequence, whereas SAPO selectively down-weights only the offending tokens and preserves the learning signal from the near-on-policy ones, improving sample efficiency. Relative to GRPO, SAPO replaces hard token-level clipping with smooth, temperature-controlled scaling, enabling more informative and stable updates. Empirical results on mathematical reasoning benchmarks indicate that SAPO exhibits improved training stability and higher Pass@1 performance under comparable training budgets. Moreover, we employ SAPO to train the Qwen3-VL model series, demonstrating that SAPO yields consistent performance gains across diverse tasks and different model sizes. Overall, SAPO provides a more reliable, scalable, and effective optimization strategy for RL training of LLMs.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [51] [QiMeng-Kernel: Macro-Thinking Micro-Coding Paradigm for LLM-Based High-Performance GPU Kernel Generation](https://arxiv.org/abs/2511.20100)
*Xinguo Zhu,Shaohui Peng,Jiaming Guo,Yunji Chen,Qi Guo,Yuanbo Wen,Hang Qin,Ruizhi Chen,Qirui Zhou,Ke Gao,Yanjun Wu,Chen Zhao,Ling Li*

Main category: cs.DC

TL;DR: MTMC是一个分层框架，通过将优化策略与实现细节解耦来解决GPU内核生成的正确性和效率问题。它使用强化学习指导轻量级LLM学习语义优化策略，同时利用通用LLM逐步实现优化方案。


<details>
  <summary>Details</summary>
Motivation: 开发高性能GPU内核对AI和科学计算至关重要，但现有LLM方法面临正确性和效率的根本冲突，因为需要同时探索优化策略和实现代码的庞大空间。

Method: 提出Macro Thinking Micro Coding分层框架：Macro Thinking使用强化学习指导轻量级LLM学习语义优化策略；Micro Coding利用通用LLM逐步实现优化方案，避免全内核生成错误。

Result: 在KernelBench上，MTMC在Levels 1-2达到近100%准确率，Level 3达到70%准确率，比现有最佳方法提升50%以上，速度比LLM快7.3倍，比专家优化的PyTorch Eager内核快2.2倍。在TritonBench上达到59.64%准确率和34倍加速。

Conclusion: MTMC通过分层方法有效导航优化空间和实现细节，使LLM能够生成高性能GPU内核，在准确性和运行时间方面均表现优异。

Abstract: Developing high-performance GPU kernels is critical for AI and scientific computing, but remains challenging due to its reliance on expert crafting and poor portability. While LLMs offer promise for automation, both general-purpose and finetuned LLMs suffer from two fundamental and conflicting limitations: correctness and efficiency. The key reason is that existing LLM-based approaches directly generate the entire optimized low-level programs, requiring exploration of an extremely vast space encompassing both optimization policies and implementation codes. To address the challenge of exploring an intractable space, we propose Macro Thinking Micro Coding (MTMC), a hierarchical framework inspired by the staged optimization strategy of human experts. It decouples optimization strategy from implementation details, ensuring efficiency through high-level strategy and correctness through low-level implementation. Specifically, Macro Thinking employs reinforcement learning to guide lightweight LLMs in efficiently exploring and learning semantic optimization strategies that maximize hardware utilization. Micro Coding leverages general-purpose LLMs to incrementally implement the stepwise optimization proposals from Macro Thinking, avoiding full-kernel generation errors. Together, they effectively navigate the vast optimization space and intricate implementation details, enabling LLMs for high-performance GPU kernel generation. Comprehensive results on widely adopted benchmarks demonstrate the superior performance of MTMC on GPU kernel generation in both accuracy and running time. On KernelBench, MTMC achieves near 100% and 70% accuracy at Levels 1-2 and 3, over 50% than SOTA general-purpose and domain-finetuned LLMs, with up to 7.3x speedup over LLMs, and 2.2x over expert-optimized PyTorch Eager kernels. On the more challenging TritonBench, MTMC attains up to 59.64% accuracy and 34x speedup.

</details>
