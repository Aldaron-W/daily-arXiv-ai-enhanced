<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 90]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.CV](#cs.CV) [Total: 4]
- [cs.CY](#cs.CY) [Total: 3]
- [cs.AI](#cs.AI) [Total: 9]
- [cs.DL](#cs.DL) [Total: 1]
- [econ.GN](#econ.GN) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.LG](#cs.LG) [Total: 9]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.SE](#cs.SE) [Total: 3]
- [stat.ML](#stat.ML) [Total: 1]
- [cs.CR](#cs.CR) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [PlotCraft: Pushing the Limits of LLMs for Complex and Interactive Data Visualization](https://arxiv.org/abs/2511.00010)
*Jiajun Zhang,Jianke Zhang,Zeyu Cui,Jiaxi Yang,Lei Zhang,Binyuan Hui,Qiang Liu,Zilei Wang,Liang Wang,Junyang Lin*

Main category: cs.CL

TL;DR: 提出了PlotCraft基准测试，包含1000个复杂可视化任务，评估发现现有LLMs在复杂可视化方面存在明显不足。开发了SynthVis-30K数据集和PlotCraftor模型，在多个基准测试中表现优异，特别是在困难任务上性能提升超过50%。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在代码生成方面表现出色，但在为规模化结构化数据创建复杂可视化方面的能力尚未得到充分评估和发展。

Method: 1. 创建PlotCraft基准测试，包含1000个涵盖金融、科学研究和社会学等领域的可视化任务；2. 开发SynthVis-30K大规模高质量复杂可视化代码数据集；3. 基于该数据集开发PlotCraftor代码生成模型。

Result: 对23个领先LLMs的评估显示它们在处理复杂可视化任务时存在明显性能缺陷。PlotCraftor在VisEval、PandasPlotBench和PlotCraft基准测试中表现与领先专有方法相当，在困难任务上性能提升超过50%。

Conclusion: PlotCraft基准测试揭示了LLMs在复杂可视化方面的不足，而PlotCraftor模型通过专门训练在复杂数据可视化方面展现出强大能力，证明了针对特定领域训练小型模型的有效性。

Abstract: Recent Large Language Models (LLMs) have demonstrated remarkable proficiency
in code generation. However, their ability to create complex visualizations for
scaled and structured data remains largely unevaluated and underdeveloped. To
address this gap, we introduce PlotCraft, a new benchmark featuring 1k
challenging visualization tasks that cover a wide range of topics, such as
finance, scientific research, and sociology. The benchmark is structured around
seven high-level visualization tasks and encompasses 48 distinct chart types.
Crucially, it is the first to systematically evaluate both single-turn
generation and multi-turn refinement across a diverse spectrum of task
complexities. Our comprehensive evaluation of 23 leading LLMs on PlotCraft
reveals obvious performance deficiencies in handling sophisticated
visualization tasks. To bridge this performance gap, we develope SynthVis-30K,
a large-scale, high-quality dataset of complex visualization code synthesized
via a collaborative agent framework. Building upon this dataset, we develope
PlotCraftor, a novel code generation model that achieves strong capabilities in
complex data visualization with a remarkably small size. Across VisEval,
PandasPlotBench, and our proposed PlotCraft, PlotCraftor shows performance
comparable to that of leading proprietary approaches. Especially, on hard task,
Our model achieves over 50% performance improvement. We will release the
benchmark, dataset, and code at
https://github.com/Speakn0w/PlotCraft-Benchmark.

</details>


### [2] [Cognitive Alignment in Personality Reasoning: Leveraging Prototype Theory for MBTI Inference](https://arxiv.org/abs/2511.00115)
*Haoyuan Li,Yuanbo Tong,Yuchen Li,Zirui Wang,Chunhou Liu,Jiamou Liu*

Main category: cs.CL

TL;DR: ProtoMBTI是一个基于原型理论的MBTI人格识别框架，通过LLM引导的数据增强、轻量级编码器微调和原型检索-修订循环，在人格推断任务中实现了更好的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统的人格识别方法通常采用硬标签分类，这掩盖了人类人格判断的渐进性和原型特性。本文旨在开发一个与认知心理学原型理论对齐的人格推断框架。

Method: 1) 通过LLM引导的多维度增强构建平衡语料库；2) 使用LoRA微调轻量级编码器学习判别性嵌入和标准化人格原型库；3) 在推理时执行检索-重用-修订-保留循环，通过基于提示的投票聚合原型证据。

Result: 在Kaggle和Pandora基准测试中，ProtoMBTI在四个MBTI二分法和完整的16种人格类型任务上均优于基线方法，并展现出强大的跨数据集泛化能力。

Conclusion: 将推理过程与心理学原型推理对齐，可以在基于文本的人格建模中提高准确性、可解释性和迁移能力。

Abstract: Personality recognition from text is typically cast as hard-label
classification, which obscures the graded, prototype-like nature of human
personality judgments. We present ProtoMBTI, a cognitively aligned framework
for MBTI inference that operationalizes prototype theory within an LLM-based
pipeline. First, we construct a balanced, quality-controlled corpus via
LLM-guided multi-dimensional augmentation (semantic, linguistic, sentiment).
Next, we LoRA-fine-tune a lightweight (<=2B) encoder to learn discriminative
embeddings and to standardize a bank of personality prototypes. At inference,
we retrieve top-k prototypes for a query post and perform a
retrieve--reuse--revise--retain cycle: the model aggregates prototype evidence
via prompt-based voting, revises when inconsistencies arise, and, upon correct
prediction, retains the sample to continually enrich the prototype library.
Across Kaggle and Pandora benchmarks, ProtoMBTI improves over baselines on both
the four MBTI dichotomies and the full 16-type task, and exhibits robust
cross-dataset generalization. Our results indicate that aligning the inference
process with psychological prototype reasoning yields gains in accuracy,
interpretability, and transfer for text-based personality modeling.

</details>


### [3] [ParaScopes: What do Language Models Activations Encode About Future Text?](https://arxiv.org/abs/2511.00180)
*Nicky Pochinkov,Yulia Volkova,Anna Vasileva,Sai V R Chereddy*

Main category: cs.CL

TL;DR: 提出了一种残差流解码器框架，用于探测语言模型在段落和文档尺度上的规划信息，发现可以解码相当于5个以上未来token的信息。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型能够处理更长的时间跨度任务，现有的激活理解方法往往局限于测试特定概念或token，需要开发能够理解模型在更长尺度上规划信息的方法。

Method: 开发了残差流解码器框架，通过多种方法探测模型激活，以提取段落和文档尺度的规划信息。

Result: 在小型模型中，可以解码相当于5个以上未来token的信息，表明模型确实编码了较长时段的规划信息。

Conclusion: 这些结果为更好地监控语言模型以及理解它们如何编码长期规划信息奠定了基础。

Abstract: Interpretability studies in language models often investigate forward-looking
representations of activations. However, as language models become capable of
doing ever longer time horizon tasks, methods for understanding activations
often remain limited to testing specific concepts or tokens. We develop a
framework of Residual Stream Decoders as a method of probing model activations
for paragraph-scale and document-scale plans. We test several methods and find
information can be decoded equivalent to 5+ tokens of future context in small
models. These results lay the groundwork for better monitoring of language
models and better understanding how they might encode longer-term planning
information.

</details>


### [4] [Training LLMs Beyond Next Token Prediction -- Filling the Mutual Information Gap](https://arxiv.org/abs/2511.00198)
*Chun-Hao Yang,Bo-Han Feng,Tzu-Yuan Lai,Yan Yu Chen,Yin-Kai Dean Huang,Shou-De Lin*

Main category: cs.CL

TL;DR: 本文提出了一种优化大型语言模型训练性能的方法，挑战了传统的下一词预测训练方式，通过预测信息丰富的token来更有效地训练模型。


<details>
  <summary>Details</summary>
Motivation: 优化大型语言模型训练性能是一个关键挑战，特别是在提高模型性能的同时控制计算成本。传统使用下一词预测的方法可能不是最优的。

Method: 提出了一种新的训练方法，通过预测信息丰富的token而不是传统的下一词预测来训练模型。在算术、多标签文本分类和自然语言生成三类任务中验证了该方法。

Result: 该方法在三种不同类型的任务中都表现出了改进效果，证明了预测信息丰富token策略的有效性。

Conclusion: 这项工作为优化LLM训练提供了原则性方法，既提升了模型性能，也增进了对目标token选择策略的理论理解。

Abstract: Optimizing training performance in large language models (LLMs) remains an
essential challenge, particularly in improving model performance while
maintaining computational costs. This work challenges the conventional approach
of training LLMs using next-token prediction (NTP), arguing that by predicting
information-rich tokens during training, there is a more effective way to train
LLMs. We investigate the impact of the proposed solution in three kinds of
tasks for LLMs: arithmetic, multi-label classification of text, and
natural-language generation. This work offers a principled approach to
optimizing LLM training, advancing both model performance and theoretical
understanding of the target-token selection strategies.

</details>


### [5] [Consistently Simulating Human Personas with Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2511.00222)
*Marwa Abdulhai,Ryan Cheng,Donovan Clay,Tim Althoff,Sergey Levine,Natasha Jaques*

Main category: cs.CL

TL;DR: 提出了一个评估和改进LLM角色一致性的统一框架，通过多轮强化学习减少角色漂移问题


<details>
  <summary>Details</summary>
Motivation: LLM在模拟人类用户时经常出现角色漂移、前后矛盾或放弃角色适当行为的问题，影响模拟效果

Method: 定义了三种自动一致性指标，并以此作为奖励信号，通过多轮强化学习微调LLM

Result: 方法将不一致性降低了55%以上，产生了更连贯和忠实的模拟用户

Conclusion: 该框架有效提升了LLM在角色扮演中的一致性，为模拟用户提供了更可靠的解决方案

Abstract: Large Language Models (LLMs) are increasingly used to simulate human users in
interactive settings such as therapy, education, and social role-play. While
these simulations enable scalable training and evaluation of AI agents,
off-the-shelf LLMs often drift from their assigned personas, contradict earlier
statements, or abandon role-appropriate behavior. We introduce a unified
framework for evaluating and improving persona consistency in LLM-generated
dialogue. We define three automatic metrics: prompt-to-line consistency,
line-to-line consistency, and Q&A consistency, that capture different types of
persona drift and validate each against human annotations. Using these metrics
as reward signals, we apply multi-turn reinforcement learning to fine-tune LLMs
for three user roles: a patient, a student, and a social chat partner. Our
method reduces inconsistency by over 55%, resulting in more coherent and
faithful simulated users.

</details>


### [6] [AgentBnB: A Browser-Based Cybersecurity Tabletop Exercise with Large Language Model Support and Retrieval-Aligned Scaffolding](https://arxiv.org/abs/2511.00265)
*Arman Anwar,Zefang Liu*

Main category: cs.CL

TL;DR: AgentBnB是一个基于浏览器的网络安全桌面演习系统，集成了大型语言模型队友和检索增强的copilot，提供按需认知提示，相比传统演习更轻量、可扩展。


<details>
  <summary>Details</summary>
Motivation: 传统网络安全桌面演习脚本化、资源密集且难以扩展，需要更轻量、可重复的练习方案。

Method: 重新设计Backdoors & Breaches游戏，集成LLM队友和检索增强copilot(C2D2)，将精选语料扩展为事实性、概念性、程序性和元认知片段，采用逐步淡出的脚手架阶梯提示工程。

Result: 在4名研究生的单人试点中，参与者更倾向于使用基于代理的版本，认为更具可扩展性，但在简单知识测验中出现天花板效应。

Conclusion: LLM增强的TTX可以提供轻量级、可重复的练习，无需传统演习的后勤负担，计划扩展到多人模式、遥测驱动教练和更大规模比较研究。

Abstract: Traditional cybersecurity tabletop exercises (TTXs) provide valuable training
but are often scripted, resource-intensive, and difficult to scale. We
introduce AgentBnB, a browser-based re-imagining of the Backdoors & Breaches
game that integrates large language model teammates with a Bloom-aligned,
retrieval-augmented copilot (C2D2). The system expands a curated corpus into
factual, conceptual, procedural, and metacognitive snippets, delivering
on-demand, cognitively targeted hints. Prompt-engineered agents employ a
scaffolding ladder that gradually fades as learner confidence grows. In a
solo-player pilot with four graduate students, participants reported greater
intention to use the agent-based version compared to the physical card deck and
viewed it as more scalable, though a ceiling effect emerged on a simple
knowledge quiz. Despite limitations of small sample size, single-player focus,
and narrow corpus, these early findings suggest that large language model
augmented TTXs can provide lightweight, repeatable practice without the
logistical burden of traditional exercises. Planned extensions include
multi-player modes, telemetry-driven coaching, and comparative studies with
larger cohorts.

</details>


### [7] [IL-PCSR: Legal Corpus for Prior Case and Statute Retrieval](https://arxiv.org/abs/2511.00268)
*Shounak Paul,Dhananjay Ghumare,Pawan Goyal,Saptarshi Ghosh,Ashutosh Modi*

Main category: cs.CL

TL;DR: 提出了IL-PCR语料库，为法律案例检索和法规检索任务提供统一测试平台，开发了基于LLM的重排序方法以利用两个任务之间的依赖关系。


<details>
  <summary>Details</summary>
Motivation: 现有研究将法律案例检索和法规检索作为独立任务处理，但这两个任务本质相关，需要开发能够利用它们之间依赖关系的统一方法。

Method: 构建IL-PCR语料库，实验了词法模型、语义模型和基于GNN的集成模型，并开发了基于LLM的重排序方法。

Result: 基于LLM的重排序方法在两个检索任务上取得了最佳性能。

Conclusion: IL-PCR语料库为法律检索任务提供了统一测试平台，基于LLM的重排序方法有效利用了案例检索和法规检索之间的依赖关系。

Abstract: Identifying/retrieving relevant statutes and prior cases/precedents for a
given legal situation are common tasks exercised by law practitioners.
Researchers to date have addressed the two tasks independently, thus developing
completely different datasets and models for each task; however, both retrieval
tasks are inherently related, e.g., similar cases tend to cite similar statutes
(due to similar factual situation). In this paper, we address this gap. We
propose IL-PCR (Indian Legal corpus for Prior Case and Statute Retrieval),
which is a unique corpus that provides a common testbed for developing models
for both the tasks (Statute Retrieval and Precedent Retrieval) that can exploit
the dependence between the two. We experiment extensively with several baseline
models on the tasks, including lexical models, semantic models and ensemble
based on GNNs. Further, to exploit the dependence between the two tasks, we
develop an LLM-based re-ranking approach that gives the best performance.

</details>


### [8] [POSESTITCH-SLT: Linguistically Inspired Pose-Stitching for End-to-End Sign Language Translation](https://arxiv.org/abs/2511.00270)
*Abhinav Joshi,Vaibhav Sharma,Sanjeet Singh,Ashutosh Modi*

Main category: cs.CL

TL;DR: 提出POSESTITCH-SLT预训练方案，通过模板生成的句子对训练，在How2Sign和iSign手语数据集上显著提升了BLEU-4分数。


<details>
  <summary>Details</summary>
Motivation: 手语翻译面临大规模句子对齐数据集稀缺的挑战，需要解决低资源环境下的翻译问题。

Method: 基于语言模板的句子生成技术，提出POSESTITCH-SLT预训练方案，使用模板生成的句子对来训练简单的基于transformer的编码器-解码器架构。

Result: 在How2Sign数据集上BLEU-4从1.97提升到4.56，在iSign数据集上从0.55提升到3.43，超越了基于姿态的无注释翻译的现有最优方法。

Conclusion: 模板驱动的合成监督在低资源手语设置中具有显著效果，证明了该方法在解决数据稀缺问题上的有效性。

Abstract: Sign language translation remains a challenging task due to the scarcity of
large-scale, sentence-aligned datasets. Prior arts have focused on various
feature extraction and architectural changes to support neural machine
translation for sign languages. We propose POSESTITCH-SLT, a novel pre-training
scheme that is inspired by linguistic-templates-based sentence generation
technique. With translation comparison on two sign language datasets, How2Sign
and iSign, we show that a simple transformer-based encoder-decoder architecture
outperforms the prior art when considering template-generated sentence pairs in
training. We achieve BLEU-4 score improvements from 1.97 to 4.56 on How2Sign
and from 0.55 to 3.43 on iSign, surpassing prior state-of-the-art methods for
pose-based gloss-free translation. The results demonstrate the effectiveness of
template-driven synthetic supervision in low-resource sign language settings.

</details>


### [9] [Language Modeling With Factorization Memory](https://arxiv.org/abs/2511.00315)
*Lee Xiong,Maksim Tkachenko,Johanes Effendi,Ting Cai*

Main category: cs.CL

TL;DR: 提出Factorization Memory，一种高效的RNN架构，在短上下文语言建模任务中性能与Transformer相当，在长上下文场景中表现出更好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 开发一种既能在短上下文任务中保持Transformer级别性能，又能在长上下文场景中实现更好泛化能力的RNN架构。

Method: 基于Mamba-2构建，在训练时利用并行计算，推理时保持恒定计算和内存复杂度。开发稀疏版本，仅更新部分循环状态，同时保持密集版本的强性能。

Result: Factorization Memory在短上下文语言建模任务中达到Transformer级别性能，在长上下文场景中表现出更好的泛化能力。稀疏版本在保持性能的同时提升了效率。

Conclusion: 这是首个成功将稀疏内存激活与在短长上下文设置中竞争性能相结合的RNN架构，为RNN和Transformer架构提供了系统实证分析。

Abstract: We propose Factorization Memory, an efficient recurrent neural network (RNN)
architecture that achieves performance comparable to Transformer models on
short-context language modeling tasks while also demonstrating superior
generalization in long-context scenarios. Our model builds upon Mamba-2,
enabling Factorization Memory to exploit parallel computations during training
while preserving constant computational and memory complexity during inference.
To further optimize model efficiency and representational capacity, we develop
a sparse formulation of Factorization Memory that updates only a subset of
recurrent states at each step while preserving the strong performance of its
dense counterpart. To our knowledge, this represents the first RNN architecture
that successfully combines sparse memory activation with competitive
performance across both short and long-context settings. This work provides a
systematic empirical analysis of Factorization Memory in comparison to
Transformer and Mamba-2 architectures.

</details>


### [10] [Reversal Invariance in Autoregressive Language Models](https://arxiv.org/abs/2511.00341)
*Mihir Sahasrabudhe*

Main category: cs.CL

TL;DR: 论文形式化定义了因果语言建模目标的结构特性：反转不变性，即标准CLM预训练对语料库及其反转版本赋予相同的似然度，说明当前预训练目标存在方向盲点。


<details>
  <summary>Details</summary>
Motivation: 自然语言具有时间不对称性（如音韵、形态、因果关系），但当前CLM目标无法捕捉这种方向性依赖，这限制了模型对语言本质的理解。

Method: 通过理论分析形式化反转不变性概念，并实验验证在反转文本上训练的模型能达到与正向文本训练相当的性能。

Result: 发现标准CLM预训练目标具有方向盲性，无法区分正向和反向语言模式，这解释了为什么在反转文本上训练的模型仍能取得良好性能。

Conclusion: 建议将预训练视为时间不对称问题，未来应开发能显式建模语言方向性的损失函数和架构，同时保持标准语言建模能力。

Abstract: We formalize a structural property of the causal (autoregressive) language
modeling (CLM) objective: reversal invariance. Formally, the next-token
prediction loss assigns identical likelihood to a corpus and its reversal,
implying that standard CLM pretraining is direction-blind. This symmetry
explains why models trained on reversed text can achieve comparable performance
to those trained on forward text, despite the inherently time-asymmetric nature
of human language and reasoning. We argue that this invariance represents a
limitation of current pretraining objectives rather than a benign artifact. If
natural language encodes directional dependencies - phonological,
morphological, or causal - a symmetric objective may fail to capture them. We
therefore propose viewing pretraining through the lens of temporal asymmetry,
motivating future work on loss functions and architectures that explicitly
model the arrow of language while retaining standard language modeling
capacity.

</details>


### [11] [LingGym: How Far Are LLMs from Thinking Like Field Linguists?](https://arxiv.org/abs/2511.00343)
*Changbing Yang,Franklin Ma,Freda Shi,Jian Zhu*

Main category: cs.CL

TL;DR: LingGym是一个评估LLMs元语言推理能力的新基准，使用来自18种类型学多样语言的语际标注文本和语法描述，测试模型在未见过的低资源语言和结构上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注特定下游任务，而本文旨在评估LLMs是否能在训练中未见过的低资源语言和结构上进行语言推理的泛化。

Method: 设计了受控评估任务——词-注释推理，要求模型使用不同层次的语言信息（如注释、语法解释、翻译）从上下文中推断缺失的词和注释。

Result: 结果显示，结合结构化语言线索能持续提升所有模型的推理性能。

Conclusion: 这项工作凸显了使用LLMs进行类型学语言分析和低资源语言文档化的前景与当前局限性。

Abstract: This paper introduces LingGym, a new benchmark that evaluates LLMs' capacity
for meta-linguistic reasoning using Interlinear Glossed Text (IGT) and
grammatical descriptions extracted from 18 typologically diverse reference
grammars. Unlike previous work that focuses on specific downstream tasks, we
assess whether LLMs can generalize linguistic inference across low-resource
languages and structures not seen during training. We present a controlled
evaluation task: Word-Gloss Inference, in which the model must infer a missing
word and gloss from context using varying levels of linguistic information
(e.g., glosses, grammatical explanations, translations). Our results show that
incorporating structured linguistic cues leads to consistent improvements in
reasoning performance across all models. This work highlights both the promise
and current limitations of using LLMs for typologically informed linguistic
analysis and low-resource language documentation.

</details>


### [12] [Reasoning Trajectories for Socratic Debugging of Student Code: From Misconceptions to Contradictions and Updated Beliefs](https://arxiv.org/abs/2511.00371)
*Erfan Al-Hossami,Razvan Bunescu*

Main category: cs.CL

TL;DR: 提出推理轨迹生成任务，通过苏格拉底式调试引导新手程序员识别和修复由编程误解引起的bug，并开发了基于LLM的解决方案。


<details>
  <summary>Details</summary>
Motivation: 新手程序员的bug大多源于编程误解，苏格拉底式调试通过引导推理轨迹帮助学生自主发现和修正错误信念，而非直接提供修复方案。

Method: 引入推理轨迹生成任务，构建手动标注的调试问题数据集，开发基于LLM的推理轨迹生成和苏格拉底式对话系统。

Result: 前沿模型能生成高达91%正确的推理轨迹和98.7%有效的对话轮次，通过大规模LLM作为评判者的评估验证了效果。

Conclusion: 苏格拉底式调试结合推理轨迹生成是有效的编程教学方法，LLM能够高质量地生成引导性对话帮助学生自主发现和修正编程误解。

Abstract: In Socratic debugging, instructors guide students towards identifying and
fixing a bug on their own, instead of providing the bug fix directly. Most
novice programmer bugs are caused by programming misconceptions, namely false
beliefs about a programming concept. In this context, Socratic debugging can be
formulated as a guided Reasoning Trajectory (RT) leading to a statement about
the program behavior that contradicts the bug-causing misconception. Upon
reaching this statement, the ensuing cognitive dissonance leads the student to
first identify and then update their false belief. In this paper, we introduce
the task of reasoning trajectory generation, together with a dataset of
debugging problems manually annotated with RTs. We then describe LLM-based
solutions for generating RTs and Socratic conversations that are anchored on
them. A large-scale LLM-as-judge evaluation shows that frontier models can
generate up to 91% correct reasoning trajectories and 98.7% valid conversation
turns.

</details>


### [13] [PADBen: A Comprehensive Benchmark for Evaluating AI Text Detectors Against Paraphrase Attacks](https://arxiv.org/abs/2511.00416)
*Yiwei Zha,Rui Min,Shanu Sushmita*

Main category: cs.CL

TL;DR: 研究发现迭代改写文本能够有效逃避AI生成文本检测器，揭示了检测系统在面对改写攻击时的严重脆弱性，并提出了PADBen基准来系统评估检测器鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前AI生成文本检测器对直接LLM输出有90%以上准确率，但在面对迭代改写内容时表现灾难性失败。研究旨在揭示这种检测失效的原因，并评估检测器对改写攻击的鲁棒性。

Method: 通过内在机制分析揭示迭代改写创建了语义位移但保留生成模式的中间清洗区域。提出了PADBen基准，包含五种文本类型分类和五个渐进检测任务，评估了11种最先进检测器。

Result: 检测器表现出关键不对称性：能成功识别抄袭逃避问题，但在作者身份混淆情况下失败。当前检测方法无法有效处理中间清洗区域。

Conclusion: 现有基于语义和风格区分的检测架构存在根本性局限，需要开发超越现有方法的新型检测架构来应对改写攻击威胁。

Abstract: While AI-generated text (AIGT) detectors achieve over 90\% accuracy on direct
LLM outputs, they fail catastrophically against iteratively-paraphrased
content. We investigate why iteratively-paraphrased text -- itself AI-generated
-- evades detection systems designed for AIGT identification. Through intrinsic
mechanism analysis, we reveal that iterative paraphrasing creates an
intermediate laundering region characterized by semantic displacement with
preserved generation patterns, which brings up two attack categories:
paraphrasing human-authored text (authorship obfuscation) and paraphrasing
LLM-generated text (plagiarism evasion). To address these vulnerabilities, we
introduce PADBen, the first benchmark systematically evaluating detector
robustness against both paraphrase attack scenarios. PADBen comprises a
five-type text taxonomy capturing the full trajectory from original content to
deeply laundered text, and five progressive detection tasks across
sentence-pair and single-sentence challenges. We evaluate 11 state-of-the-art
detectors, revealing critical asymmetry: detectors successfully identify the
plagiarism evasion problem but fail for the case of authorship obfuscation. Our
findings demonstrate that current detection approaches cannot effectively
handle the intermediate laundering region, necessitating fundamental advances
in detection architectures beyond existing semantic and stylistic
discrimination methods. For detailed code implementation, please see
https://github.com/JonathanZha47/PadBen-Paraphrase-Attack-Benchmark.

</details>


### [14] [MedRECT: A Medical Reasoning Benchmark for Error Correction in Clinical Texts](https://arxiv.org/abs/2511.00421)
*Naoto Iwase,Hiroki Okuyama,Junichiro Iwasawa*

Main category: cs.CL

TL;DR: MedRECT是首个跨语言医疗错误纠正基准（日语/英语），评估LLMs在医疗错误检测、定位和纠正方面的能力，发现推理模型表现最佳，微调后模型在结构化医疗错误纠正任务中超过人类专家。


<details>
  <summary>Details</summary>
Motivation: LLMs在医疗应用中的潜力日益显现，但其检测和纠正临床文本错误的能力（安全部署的前提）尚未得到充分评估，特别是在英语以外的语言中。

Method: 从日本医师国家考试构建自动化流水线，创建MedRECT-ja（663文本）和MedRECT-en（458文本）数据集，评估9种当代LLMs，包括专有、开源和推理模型，并进行针对性LoRA微调。

Result: 推理模型显著优于标准架构，错误检测相对提升13.5%，句子提取提升51.0%；跨语言评估显示英语到日语存在5-10%性能差距；微调后模型在医疗错误纠正任务中超过人类专家。

Conclusion: MedRECT为开发更安全的跨语言医疗LLMs提供了可复现框架和资源，是首个全面的跨语言医疗错误纠正基准。

Abstract: Large language models (LLMs) show increasing promise in medical applications,
but their ability to detect and correct errors in clinical texts -- a
prerequisite for safe deployment -- remains under-evaluated, particularly
beyond English. We introduce MedRECT, a cross-lingual benchmark
(Japanese/English) that formulates medical error handling as three subtasks:
error detection, error localization (sentence extraction), and error
correction. MedRECT is built with a scalable, automated pipeline from the
Japanese Medical Licensing Examinations (JMLE) and a curated English
counterpart, yielding MedRECT-ja (663 texts) and MedRECT-en (458 texts) with
comparable error/no-error balance. We evaluate 9 contemporary LLMs spanning
proprietary, open-weight, and reasoning families. Key findings: (i) reasoning
models substantially outperform standard architectures, with up to 13.5%
relative improvement in error detection and 51.0% in sentence extraction; (ii)
cross-lingual evaluation reveals 5-10% performance gaps from English to
Japanese, with smaller disparities for reasoning models; (iii) targeted LoRA
fine-tuning yields asymmetric improvements in error correction performance
(Japanese: +0.078, English: +0.168) while preserving reasoning capabilities;
and (iv) our fine-tuned model exceeds human expert performance on structured
medical error correction tasks. To our knowledge, MedRECT is the first
comprehensive cross-lingual benchmark for medical error correction, providing a
reproducible framework and resources for developing safer medical LLMs across
languages.

</details>


### [15] [G2: Guided Generation for Enhanced Output Diversity in LLMs](https://arxiv.org/abs/2511.00432)
*Zhiwen Ruan,Yixia Li,Yefeng Liu,Yun Chen,Weihua Luo,Peng Li,Yang Liu,Guanhua Chen*

Main category: cs.CL

TL;DR: 提出G2方法，一种无需训练的即插即用技术，通过双引导机制增强大语言模型输出多样性，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在输出多样性方面存在局限，现有方法如温度调节虽能增强多样性但会牺牲输出质量。

Method: 使用基础生成器和双引导器，通过基于解码的干预来引导生成过程，在原始查询条件下鼓励更多样化输出。

Result: 综合实验表明G2能有效提高输出多样性，并在多样性和质量之间保持最佳平衡。

Conclusion: G2方法成功解决了大语言模型输出多样性不足的问题，实现了多样性与质量的平衡。

Abstract: Large Language Models (LLMs) have demonstrated exceptional performance across
diverse natural language processing tasks. However, these models exhibit a
critical limitation in output diversity, often generating highly similar
content across multiple attempts. This limitation significantly affects tasks
requiring diverse outputs, from creative writing to reasoning. Existing
solutions, like temperature scaling, enhance diversity by modifying probability
distributions but compromise output quality. We propose Guide-to-Generation
(G2), a training-free plug-and-play method that enhances output diversity while
preserving generation quality. G2 employs a base generator alongside dual
Guides, which guide the generation process through decoding-based interventions
to encourage more diverse outputs conditioned on the original query.
Comprehensive experiments demonstrate that G2 effectively improves output
diversity while maintaining an optimal balance between diversity and quality.

</details>


### [16] [Remembering Unequally: Global and Disciplinary Bias in LLM-Generated Co-Authorship Networks](https://arxiv.org/abs/2511.00476)
*Ghazal Kalhor,Afra Mashhadi*

Main category: cs.CL

TL;DR: 该研究分析了大型语言模型记忆效应对合著网络的影响，发现LLMs在生成合著网络时存在系统性偏见，倾向于高被引研究者，但这种偏见在不同学科和地区间存在差异。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在学术搜索和推荐系统中的广泛应用，其记忆效应可能导致合著网络生成中的公平性和偏见问题，影响信息生态系统的完整性。

Method: 评估了DeepSeek R1、Llama 4 Scout和Mixtral 8x7B三个主流模型，分析记忆驱动输出在不同学科和世界地区的差异。

Result: 全球分析显示LLMs存在偏向高被引研究者的系统性偏见，但临床医学等学科和非洲部分地区表现出更均衡的代表性。

Conclusion: LLMs在学术发现应用中既存在风险也蕴含机遇，需要关注其训练数据的公平性，以平衡不同学科和地区的代表性。

Abstract: Ongoing breakthroughs in Large Language Models (LLMs) are reshaping search
and recommendation platforms at their core. While this shift unlocks powerful
new scientometric tools, it also exposes critical fairness and bias issues that
could erode the integrity of the information ecosystem. Additionally, as LLMs
become more integrated into web-based searches for scholarly tools, their
ability to generate summarized research work based on memorized data introduces
new dimensions to these challenges. The extent of memorization in LLMs can
impact the accuracy and fairness of the co-authorship networks they produce,
potentially reflecting and amplifying existing biases within the scientific
community and across different regions. This study critically examines the
impact of LLM memorization on the co-authorship networks. To this end, we
assess memorization effects across three prominent models, DeepSeek R1, Llama 4
Scout, and Mixtral 8x7B, analyzing how memorization-driven outputs vary across
academic disciplines and world regions. While our global analysis reveals a
consistent bias favoring highly cited researchers, this pattern is not
uniformly observed. Certain disciplines, such as Clinical Medicine, and
regions, including parts of Africa, show more balanced representation, pointing
to areas where LLM training data may reflect greater equity. These findings
underscore both the risks and opportunities in deploying LLMs for scholarly
discovery.

</details>


### [17] [Leveraging the Cross-Domain & Cross-Linguistic Corpus for Low Resource NMT: A Case Study On Bhili-Hindi-English Parallel Corpus](https://arxiv.org/abs/2511.00486)
*Pooja Singh,Shashwat Bhardwaj,Vaibhav Sharma,Sandeep Kumar*

Main category: cs.CL

TL;DR: 本文构建了首个大规模Bhili-Hindi-English平行语料库(BHEPC)，包含11万句经过精心整理的句子，填补了Bhili语机器翻译资源空白。通过评估多种多语言大模型，发现微调的NLLB-200模型表现最佳，展示了多语言模型在低资源场景下的潜力。


<details>
  <summary>Details</summary>
Motivation: 印度语言多样性带来机器翻译挑战，特别是Bhili等代表性不足的部落语言缺乏高质量语言资源。本文旨在填补这一资源空白，促进低资源语言的包容性自然语言处理技术发展。

Method: 构建Bhili-Hindi-English平行语料库(BHEPC)，包含11万句经过专家翻译的句子，涵盖教育、行政和新闻等关键领域。评估多种专有和开源多语言大模型在双向翻译任务中的表现，使用上下文学习研究生成式翻译能力。

Result: 微调的NLLB-200蒸馏600M变体模型在英语/Hindi与Bhili之间的双向翻译任务中表现最佳。通过上下文学习评估了多语言LLM的生成翻译能力，并量化了分布差异。

Conclusion: 这项工作填补了关键资源空白，为全球低资源和边缘化语言促进了包容性自然语言处理技术发展，证明了多语言模型在低资源场景中的有效性。

Abstract: The linguistic diversity of India poses significant machine translation
challenges, especially for underrepresented tribal languages like Bhili, which
lack high-quality linguistic resources. This paper addresses the gap by
introducing Bhili-Hindi-English Parallel Corpus (BHEPC), the first and largest
parallel corpus worldwide comprising 110,000 meticulously curated sentences
across Bhili, Hindi, and English. The corpus was created with the assistance of
expert human translators. BHEPC spans critical domains such as education,
administration, and news, establishing a valuable benchmark for research in low
resource machine translation. To establish a comprehensive Bhili Machine
Translation benchmark, we evaluated a wide range of proprietary and open-source
Multilingual Large Language Models (MLLMs) on bidirectional translation tasks
between English/Hindi and Bhili. Comprehensive evaluation demonstrates that the
fine-tuned NLLB-200 distilled 600M variant model outperforms others,
highlighting the potential of multilingual models in low resource scenarios.
Furthermore, we investigated the generative translation capabilities of
multilingual LLMs on BHEPC using in-context learning, assessing performance
under cross-domain generalization and quantifying distributional divergence.
This work bridges a critical resource gap and promotes inclusive natural
language processing technologies for low-resource and marginalized languages
globally.

</details>


### [18] [With Privacy, Size Matters: On the Importance of Dataset Size in Differentially Private Text Rewriting](https://arxiv.org/abs/2511.00487)
*Stephen Meisenbacher,Florian Matthes*

Main category: cs.CL

TL;DR: 本文首次在差分隐私文本隐私化评估中引入数据集规模因素，通过在大规模数据集上设计效用和隐私测试，发现数据集规模对隐私-效用权衡有重要影响。


<details>
  <summary>Details</summary>
Motivation: 现有DP NLP研究在评估文本重写机制时往往忽略数据集规模的影响，本文旨在填补这一空白，研究数据集规模对机制效用和隐私保护效果的影响。

Method: 在具有动态分割规模的大规模数据集上设计效用和隐私测试，在包含多达100万个文本的不同规模数据集上运行测试，量化数据集规模增加对隐私-效用权衡的影响。

Result: 研究发现数据集规模在评估DP文本重写机制中起着关键作用，数据集规模的变化显著影响隐私保护效果和文本效用之间的平衡关系。

Conclusion: 研究结果呼吁在DP NLP中采用更严格的评估程序，并为DP NLP在实践和大规模应用中的未来发展提供了重要启示。

Abstract: Recent work in Differential Privacy with Natural Language Processing (DP NLP)
has proposed numerous promising techniques in the form of text rewriting
mechanisms. In the evaluation of these mechanisms, an often-ignored aspect is
that of dataset size, or rather, the effect of dataset size on a mechanism's
efficacy for utility and privacy preservation. In this work, we are the first
to introduce this factor in the evaluation of DP text privatization, where we
design utility and privacy tests on large-scale datasets with dynamic split
sizes. We run these tests on datasets of varying size with up to one million
texts, and we focus on quantifying the effect of increasing dataset size on the
privacy-utility trade-off. Our findings reveal that dataset size plays an
integral part in evaluating DP text rewriting mechanisms; additionally, these
findings call for more rigorous evaluation procedures in DP NLP, as well as
shed light on the future of DP NLP in practice and at scale.

</details>


### [19] [ToM: Leveraging Tree-oriented MapReduce for Long-Context Reasoning in Large Language Models](https://arxiv.org/abs/2511.00489)
*Jiani Guo,Zuchao Li,Jie Wu,Qianren Wang,Yun Li,Lefei Zhang,Hai Zhao,Yujiu Yang*

Main category: cs.CL

TL;DR: 提出了ToM框架，一种面向树结构的MapReduce方法，用于解决大语言模型在长上下文推理中的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法如RAG和分治框架在处理长文档时存在逻辑连贯性差、无法捕捉长距离依赖关系等问题，需要更好的解决方案。

Method: 通过层次语义解析构建文档树(DocTree)，采用树形MapReduce方法进行自底向上的递归推理：Map步骤在子节点生成推理依据，Reduce步骤在父节点聚合子节点结果以解决冲突或达成共识。

Result: 在70B+大语言模型上的实验结果表明，ToM显著优于现有的分治框架和检索增强生成方法，实现了更好的逻辑连贯性和长上下文推理能力。

Conclusion: ToM框架通过利用文档的层次结构，有效解决了长上下文推理中的逻辑连贯性和长距离依赖问题，为处理长文档提供了新的解决方案。

Abstract: Large Language Models (LLMs), constrained by limited context windows, often
face significant performance degradation when reasoning over long contexts. To
address this, Retrieval-Augmented Generation (RAG) retrieves and reasons over
chunks but frequently sacrifices logical coherence due to its reliance on
similarity-based rankings. Similarly, divide-and-conquer frameworks (DCF) split
documents into small chunks for independent reasoning and aggregation. While
effective for local reasoning, DCF struggles to capture long-range dependencies
and risks inducing conflicts by processing chunks in isolation. To overcome
these limitations, we propose ToM, a novel Tree-oriented MapReduce framework
for long-context reasoning. ToM leverages the inherent hierarchical structure
of long documents (e.g., main headings and subheadings) by constructing a
DocTree through hierarchical semantic parsing and performing bottom-up
aggregation. Using a Tree MapReduce approach, ToM enables recursive reasoning:
in the Map step, rationales are generated at child nodes; in the Reduce step,
these rationales are aggregated across sibling nodes to resolve conflicts or
reach consensus at parent nodes. Experimental results on 70B+ LLMs show that
ToM significantly outperforms existing divide-and-conquer frameworks and
retrieval-augmented generation methods, achieving better logical coherence and
long-context reasoning. Our code is available at
https://github.com/gjn12-31/ToM .

</details>


### [20] [Zero-RAG: Towards Retrieval-Augmented Generation with Zero Redundant Knowledge](https://arxiv.org/abs/2511.00505)
*Qi Luo,Xiaonan Li,Junqi Dai,Shuang Cheng,Xipeng Qiu*

Main category: cs.CL

TL;DR: Zero-RAG通过识别和剪枝RAG外部语料库中的冗余知识，减少检索工作量并提升LLM内部知识的利用效率，在保持性能的同时加速检索过程。


<details>
  <summary>Details</summary>
Motivation: 随着LLM内部知识显著扩展，外部语料库与LLM之间存在大量知识冗余，这增加了密集检索的工作负担，且冗余知识反而会损害LLM自身能回答问题的RAG性能。

Method: 提出Mastery-Score指标识别冗余知识进行剪枝；使用Query Router和Noise-Tolerant Tuning避免不相关文档干扰，提升LLM对内部知识的利用。

Result: 实验结果显示Zero-RAG将维基百科语料库剪枝30%，检索阶段加速22%，且不损害RAG性能。

Conclusion: Zero-RAG有效解决了RAG中的知识冗余问题，通过剪枝冗余知识和优化内部知识利用，实现了检索效率提升和性能保持。

Abstract: Retrieval-Augmented Generation has shown remarkable results to address Large
Language Models' hallucinations, which usually uses a large external corpus to
supplement knowledge to LLMs. However, with the development of LLMs, the
internal knowledge of LLMs has expanded significantly, thus causing significant
knowledge redundancy between the external corpus and LLMs. On the one hand, the
indexing cost of dense retrieval is highly related to the corpus size and thus
significant redundant knowledge intensifies the dense retrieval's workload. On
the other hand, the redundant knowledge in the external corpus is not helpful
to LLMs and our exploratory analysis shows that it instead hurts the RAG
performance on those questions which the LLM can answer by itself. To address
these issues, we propose Zero-RAG to tackle these challenges. Specifically, we
first propose the Mastery-Score metric to identify redundant knowledge in the
RAG corpus to prune it. After pruning, answers to "mastered" questions rely
primarily on internal knowledge of the LLM. To better harness the internal
capacity, we propose Query Router and Noise-Tolerant Tuning to avoid the
irrelevant documents' distraction and thus further improve the LLM's
utilization of internal knowledge with pruned corpus. Experimental results show
that Zero-RAG prunes the Wikipedia corpus by 30\% and accelerates the retrieval
stage by 22\%, without compromising RAG's performance.

</details>


### [21] [Fine-Tuning DialoGPT on Common Diseases in Rural Nepal for Medical Conversations](https://arxiv.org/abs/2511.00514)
*Birat Poudel,Satyam Ghimire,Er. Prakash Chandra Prasad*

Main category: cs.CL

TL;DR: 在尼泊尔农村地区开发离线运行的医疗对话AI，通过微调轻量级DialoGPT模型，在十种常见疾病数据集上生成连贯且医学适当的响应。


<details>
  <summary>Details</summary>
Motivation: 解决资源受限地区（如尼泊尔农村）缺乏互联网连接和云基础设施的问题，开发能够离线运行的医疗对话代理来支持医疗服务。

Method: 使用合成构建的医患互动数据集，对轻量级生成对话模型DialoGPT进行微调，覆盖尼泊尔农村常见的十种疾病。

Result: 尽管训练数据有限且领域特定，微调后的模型能够生成连贯、上下文相关且医学适当的响应，表现出对症状、疾病背景和同理心沟通的理解。

Conclusion: 紧凑型离线对话模型具有很好的适应性，针对性的数据集在低资源医疗环境中进行领域适应的效果显著，为未来农村医疗对话AI提供了有前景的方向。

Abstract: Conversational agents are increasingly being explored to support healthcare
delivery, particularly in resource-constrained settings such as rural Nepal.
Large-scale conversational models typically rely on internet connectivity and
cloud infrastructure, which may not be accessible in rural areas. In this
study, we fine-tuned DialoGPT, a lightweight generative dialogue model that can
operate offline, on a synthetically constructed dataset of doctor-patient
interactions covering ten common diseases prevalent in rural Nepal, including
common cold, seasonal fever, diarrhea, typhoid fever, gastritis, food
poisoning, malaria, dengue fever, tuberculosis, and pneumonia. Despite being
trained on a limited, domain-specific dataset, the fine-tuned model produced
coherent, contextually relevant, and medically appropriate responses,
demonstrating an understanding of symptoms, disease context, and empathetic
communication. These results highlight the adaptability of compact,
offline-capable dialogue models and the effectiveness of targeted datasets for
domain adaptation in low-resource healthcare environments, offering promising
directions for future rural medical conversational AI.

</details>


### [22] [Exploring and Mitigating Gender Bias in Encoder-Based Transformer Models](https://arxiv.org/abs/2511.00519)
*Ariyan Hossain,Khondokar Mohammad Ahanaf Hannan,Rakinul Haque,Nowreen Tarannum Rafa,Humayra Musarrat,Shoaib Ahmed Dipu,Farig Yousuf Sadeque*

Main category: cs.CL

TL;DR: 本文研究了BERT、ALBERT、RoBERTa和DistilBERT等transformer模型中的性别偏见问题，提出了新的评估指标MALoR，并通过反事实数据增强的继续预训练方法有效降低了性别偏见。


<details>
  <summary>Details</summary>
Motivation: 编码器transformer模型在各种语言任务中表现出色，但继承了训练数据中的强烈性别偏见，需要研究如何量化和缓解这种偏见。

Method: 引入MALoR指标评估偏见程度，采用反事实数据增强生成性别平衡数据集进行继续预训练。

Result: 在BERT-base中，"he-she"偏见分数从1.27降至0.08，"his-her"从2.51降至0.36；BERT-large中"male-female"偏见从1.82降至0.10，所有模型偏见均显著降低且不影响下游任务性能。

Conclusion: 提出的方法能有效减少transformer模型中的性别偏见，同时保持模型在下游任务中的性能表现。

Abstract: Gender bias in language models has gained increasing attention in the field
of natural language processing. Encoder-based transformer models, which have
achieved state-of-the-art performance in various language tasks, have been
shown to exhibit strong gender biases inherited from their training data. This
paper investigates gender bias in contextualized word embeddings, a crucial
component of transformer-based models. We focus on prominent architectures such
as BERT, ALBERT, RoBERTa, and DistilBERT to examine their vulnerability to
gender bias. To quantify the degree of bias, we introduce a novel metric,
MALoR, which assesses bias based on model probabilities for filling masked
tokens. We further propose a mitigation approach involving continued
pre-training on a gender-balanced dataset generated via Counterfactual Data
Augmentation. Our experiments reveal significant reductions in gender bias
scores across different pronoun pairs. For instance, in BERT-base, bias scores
for "he-she" dropped from 1.27 to 0.08, and "his-her" from 2.51 to 0.36
following our mitigation approach. We also observed similar improvements across
other models, with "male-female" bias decreasing from 1.82 to 0.10 in
BERT-large. Our approach effectively reduces gender bias without compromising
model performance on downstream tasks.

</details>


### [23] [Word Salad Chopper: Reasoning Models Waste A Ton Of Decoding Budget On Useless Repetitions, Self-Knowingly](https://arxiv.org/abs/2511.00536)
*Wenya Xie,Shaochen,Zhong,Hoang Anh Duy Le,Zhaozhuo Xu,Jianwen Xie,Zirui Liu*

Main category: cs.CL

TL;DR: 提出WordSaladChopper(WSC)组件，通过检测LRM中的无用自我重复token（称为"word salad"）来减少输出token成本，仅需单层线性分类器即可实时检测并修剪这些冗余内容。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型(LRM)输出token成本高昂，其中很大一部分是无用的自我重复内容，这些"word salad"消耗解码预算但不增加价值。

Method: 利用LRM在陷入循环时的自我意识特性，通过<

>token的隐藏状态模式，使用单层线性分类器实时检测word salad行为，检测后通过简单修剪和重新生成提示来节省长度。

Result: WSC能够显著节省输出长度，同时质量损失最小，是一个轻量级、即插即用的组件，对推理轨迹干扰极小。

Conclusion: WSC或类似组件是注重用户体验的所有LRM应用的必备组件，因其开销低、节省效果强，且word salad token缺乏语义价值。

Abstract: Large Reasoning Models (LRMs) are often bottlenecked by the high cost of
output tokens. We show that a significant portion of these tokens are useless
self-repetitions - what we call "word salad" - that exhaust the decoding budget
without adding value. Interestingly, we observe that LRMs are self-aware when
trapped in these loops: the hidden states of <\n\n> tokens trailing each
reasoning chunk exhibit patterns that allow us to detect word salad behavior
on-the-fly via a single-layer linear classifier. Once detected, a simple chop
appended by a straightforward regeneration prompt yields substantial length
savings with minimal quality loss. Our work offers WordSaladChopper (WSC) - a
lightweight, turnkey component for LRM that is minimally invasive to its
reasoning trajectory by only removing semantically redundant tokens. Given its
low overhead, strong savings, and the lack of semantic value of word salad
tokens, we believe it is not too far-fetched to argue that WSC - or a similar
component - is a must-have for all LRM applications with user experience in
mind. Our code is publicly available at
https://github.com/wenyaxie023/WordSaladChopper.

</details>


### [24] [Multi-refined Feature Enhanced Sentiment Analysis Using Contextual Instruction](https://arxiv.org/abs/2511.00537)
*Peter Atandoh,Jie Zou,Weikang Guo,Jiwei Wei,Zheng Wang*

Main category: cs.CL

TL;DR: 提出CISEA-MRFE框架，结合上下文指令、语义增强增强和多细化特征提取，显著提升情感分析性能，在多个基准数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习和预训练语言模型的情感分析方法在细微情感线索、领域迁移和不平衡情感分布场景下表现不佳，主要由于语义基础不足、对多样化语言模式泛化能力差以及对主导情感类别的偏见。

Method: CISEA-MRFE框架包含三个核心组件：上下文指令（CI）注入领域感知指令指导情感消歧；语义增强增强（SEA）通过情感一致释义增强提高鲁棒性；多细化特征提取（MRFE）结合尺度自适应深度编码器（SADE）进行多尺度特征专业化，以及情感评估器上下文编码器（EECE）进行情感感知序列建模。

Result: 在四个基准数据集上的实验结果表明，CISEA-MRFE始终优于强基线方法，在IMDb上相对准确率提升4.6%，Yelp上6.5%，Twitter上30.3%，Amazon上4.1%。

Conclusion: CISEA-MRFE框架在跨领域情感分类中展现出有效性和泛化能力，验证了所提出方法的优越性。

Abstract: Sentiment analysis using deep learning and pre-trained language models (PLMs)
has gained significant traction due to their ability to capture rich contextual
representations. However, existing approaches often underperform in scenarios
involving nuanced emotional cues, domain shifts, and imbalanced sentiment
distributions. We argue that these limitations stem from inadequate semantic
grounding, poor generalization to diverse linguistic patterns, and biases
toward dominant sentiment classes. To overcome these challenges, we propose
CISEA-MRFE, a novel PLM-based framework integrating Contextual Instruction
(CI), Semantic Enhancement Augmentation (SEA), and Multi-Refined Feature
Extraction (MRFE). CI injects domain-aware directives to guide sentiment
disambiguation; SEA improves robustness through sentiment-consistent
paraphrastic augmentation; and MRFE combines a Scale-Adaptive Depthwise Encoder
(SADE) for multi-scale feature specialization with an Emotion Evaluator Context
Encoder (EECE) for affect-aware sequence modeling. Experimental results on four
benchmark datasets demonstrate that CISEA-MRFE consistently outperforms strong
baselines, achieving relative improvements in accuracy of up to 4.6% on IMDb,
6.5% on Yelp, 30.3% on Twitter, and 4.1% on Amazon. These results validate the
effectiveness and generalization ability of our approach for sentiment
classification across varied domains.

</details>


### [25] [Friend or Foe: How LLMs' Safety Mind Gets Fooled by Intent Shift Attack](https://arxiv.org/abs/2511.00556)
*Peng Ding,Jun Kuang,Wen Sun,Zongyu Wang,Xuezhi Cao,Xunliang Cai,Jiajun Chen,Shujian Huang*

Main category: cs.CL

TL;DR: ISA攻击通过意图转换使LLMs将有害请求误解为良性信息请求，仅需对原始请求进行最小编辑，就能显著提高攻击成功率，现有防御方法对此无效。


<details>
  <summary>Details</summary>
Motivation: 现有越狱攻击主要通过引入额外上下文或对抗性token来分散LLMs注意力，但未改变核心有害意图。研究这些弱点对于构建鲁棒的安全机制至关重要。

Method: 建立意图转换分类法，利用其生成可能被LLMs误解为良性信息请求的攻击。该方法仅需对原始请求进行最小编辑，产生自然、人类可读且看似无害的提示。

Result: 在开源和商业LLMs上的广泛实验显示，ISA相比直接有害提示在攻击成功率上提高了70%以上。仅使用ISA模板重新表述的良性数据对模型进行微调，成功率接近100%。

Conclusion: 研究揭示了LLMs安全中意图推断的基本挑战，现有防御方法对ISA无效，需要更有效的防御策略。

Abstract: Large language models (LLMs) remain vulnerable to jailbreaking attacks
despite their impressive capabilities. Investigating these weaknesses is
crucial for robust safety mechanisms. Existing attacks primarily distract LLMs
by introducing additional context or adversarial tokens, leaving the core
harmful intent unchanged. In this paper, we introduce ISA (Intent Shift
Attack), which obfuscates LLMs about the intent of the attacks. More
specifically, we establish a taxonomy of intent transformations and leverage
them to generate attacks that may be misperceived by LLMs as benign requests
for information. Unlike prior methods relying on complex tokens or lengthy
context, our approach only needs minimal edits to the original request, and
yields natural, human-readable, and seemingly harmless prompts. Extensive
experiments on both open-source and commercial LLMs show that ISA achieves over
70% improvement in attack success rate compared to direct harmful prompts. More
critically, fine-tuning models on only benign data reformulated with ISA
templates elevates success rates to nearly 100%. For defense, we evaluate
existing methods and demonstrate their inadequacy against ISA, while exploring
both training-free and training-based mitigation strategies. Our findings
reveal fundamental challenges in intent inference for LLMs safety and
underscore the need for more effective defenses. Our code and datasets are
available at https://github.com/NJUNLP/ISA.

</details>


### [26] [FlashEVA: Accelerating LLM inference via Efficient Attention](https://arxiv.org/abs/2511.00576)
*Juan Gabriel Kostelec,Qinghai Guo*

Main category: cs.CL

TL;DR: FlashEVA是一种高效的EVA注意力实现，通过微调Transformer模型适配FlashEVA注意力，仅需1.5B tokens即可完成微调，在推理时实现6.7倍吞吐量提升和5倍GPU内存降低。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在自然语言处理中表现出色，但其内存需求（特别是需要维护完整上下文）对推理构成重大挑战。

Method: 提出FlashEVA作为EVA（通过控制变量实现高效注意力）的高效实现，并展示如何微调Transformer模型以适配FlashEVA注意力。

Result: FlashEVA在推理时相比标准Transformer实现，吞吐量提升高达6.7倍，GPU峰值内存使用降低5倍，同时在各种下游任务中保持有效性。

Conclusion: 这项工作代表了向更高效和适应性强的基于Transformer的推理模型迈出的重要一步，通过可调节超参数在吞吐量和准确性之间提供权衡控制。

Abstract: Transformer models have revolutionized natural language processing, achieving
state-of-the-art performance and demonstrating remarkable scalability. However,
their memory demands, particularly due to maintaining full context in memory,
pose significant challenges for inference. In this paper, we present FlashEVA,
an efficient implementation of EVA (Efficient Attention via Control Variates),
and demonstrate how to finetune transformers to adapt to FlashEVA attention.
Our method enables fine-tuning of Transformer models with as few as 1.5B tokens
while preserving effectiveness across various downstream tasks. Notably,
FlashEVA achieves up to 6.7x higher throughput and 5x lower peak GPU memory
usage during inference compared to standard Transformer implementations.
Despite these improvements, we observe limitations in retrieval-focused tasks.
Our implementation offers control over the trade-off between throughput and
accuracy through adjustable hyperparameters, providing flexibility for diverse
use cases. This work represents a significant step towards more efficient and
adaptable Transformer-based models for inference.

</details>


### [27] [OpenSIR: Open-Ended Self-Improving Reasoner](https://arxiv.org/abs/2511.00602)
*Wai-Chung Kwan,Joshua Ong Jun Leang,Pavlos Vougiouklis,Jeff Z. Pan,Marco Valentino,Pasquale Minervini*

Main category: cs.CL

TL;DR: OpenSIR是一个无需外部监督的自我改进推理框架，通过让LLM交替扮演教师和学生角色来生成和解决新颖问题，实现了从简单数学到高级数学的自主进步。


<details>
  <summary>Details</summary>
Motivation: 现有基于标注数据集的强化学习方法可能限制模型超越人类水平的能力，而自我对弈方法要么依赖外部验证器，要么无法进行开放式学习。

Method: OpenSIR框架中，LLM交替扮演教师和学生角色：教师生成具有适当难度和多样性的问题，学生解决问题，通过这种自我对弈实现开放式学习。

Result: 从单个简单种子问题开始，OpenSIR显著提升了指令模型性能：Llama-3.2-3B-Instruct在GSM8K上从73.9提升到78.3，在College Math上从28.8提升到34.4；Gemma-2-2B-Instruct在GSM8K上从38.5提升到58.7。

Conclusion: OpenSIR通过共同进化的教师-学生角色实现了开放式学习，能够自适应地校准难度并推动多样化探索，从基础数学自主发展到高级数学。

Abstract: Recent advances in large language model (LLM) reasoning through reinforcement
learning rely on annotated datasets for verifiable rewards, which may limit
models' ability to surpass human-level performance. While self-play offers a
promising alternative, existing approaches depend on external verifiers or
cannot learn open-endedly. We present Open-Ended Self-Improving Reasoner
(OpenSIR), a self-play framework where an LLM learns to generate and solve
novel problems by alternating teacher and student roles without external
supervision. To generate novel problems, OpenSIR optimises for both difficulty
and diversity, rewarding problems that challenge appropriately while exploring
distinct concepts, enabling open-ended mathematical discovery. Starting from a
single trivial seed problem, OpenSIR substantially improves instruction models:
Llama-3.2-3B-Instruct advances from 73.9 to 78.3 on GSM8K, and from 28.8 to
34.4 on College Math, while Gemma-2-2B-Instruct rises from 38.5 to 58.7 on
GSM8K. Our analyses reveal that OpenSIR achieves open-ended learning through
co-evolving teacher-student roles that adaptively calibrate difficulty and
drive diverse exploration, progressing autonomously from basic to advanced
mathematics.

</details>


### [28] [SpecDiff-2: Scaling Diffusion Drafter Alignment For Faster Speculative Decoding](https://arxiv.org/abs/2511.00606)
*Jameson Sandler,Jacob K. Christopher,Thomas Hartvigsen,Nando Fioretto*

Main category: cs.CL

TL;DR: SpecDiff-2是一个新的推理加速框架，通过使用离散扩散作为非自回归草稿器来解决当前推测解码的两个瓶颈，实现了比标准解码最高5.5倍的平均加速。


<details>
  <summary>Details</summary>
Motivation: 当前推测解码方法存在两个基本瓶颈：草稿阶段的自回归依赖限制了并行性，以及草稿模型与验证模型不匹配导致的频繁拒绝。

Method: 使用离散扩散作为非自回归草稿器解决并行性瓶颈，并开发新技术校准离散扩散草稿器与自回归验证器之间的对齐。

Result: 在推理、编码和数学基准测试中达到新的最先进水平，相比之前基线平均提升55%的每秒令牌数，相比标准解码获得最高5.5倍平均加速，且无精度损失。

Conclusion: SpecDiff-2通过联合解决推测解码的两个关键瓶颈，显著提升了LLM推理效率，为大规模语言模型的实际部署提供了重要改进。

Abstract: Speculative decoding has become the standard approach for accelerating Large
Language Model (LLM) inference. It exploits a lossless draft-then-verify
procedure to circumvent the latency of autoregressive decoding, achieving
impressive speed-ups. Yet, current speculative decoding approaches remain
limited by two fundamental bottlenecks: (1) the autoregressive dependency
during drafting which limits parallelism, and (2) frequent rejections of draft
tokens caused by misalignment between the draft and verify models. This paper
proposes SpecDiff-2, a novel framework to jointly address these two
bottlenecks. It leverages discrete diffusion as a non-autoregressive drafter to
address bottleneck (1) and develops novel techniques to calibrate discrete
diffusion drafters with autoregressive verifiers, addressing bottleneck (2).
Experimental results across a comprehensive benchmark suite show that
SpecDiff-2 achieves a new state-of-the-art across reasoning, coding, and
mathematical benchmarks, improving tokens-per-second by up to an average of
+55% over previous baselines and obtaining up to 5.5x average speed-up over
standard decoding, without any loss of accuracy.

</details>


### [29] [Certain but not Probable? Differentiating Certainty from Probability in LLM Token Outputs for Probabilistic Scenarios](https://arxiv.org/abs/2511.00620)
*Autumn Toney-Wails,Ryan Wails*

Main category: cs.CL

TL;DR: 该论文研究大语言模型在概率场景中的不确定性量化，发现虽然模型能给出正确回答，但其token级别的概率分布与理论概率分布存在差异。


<details>
  <summary>Details</summary>
Motivation: 确保大语言模型在决策支持等关键应用中的可靠性，需要可靠的不确定性量化。当前基于token logits的方法在概率场景中可能不足，因为期望token输出概率与理论概率分布一致。

Method: 使用GPT-4.1和DeepSeek-Chat模型，评估它们对10个涉及概率的提示（如掷骰子）的响应，测量响应有效性和token级输出概率与理论概率的一致性。

Result: 两个模型在所有提示场景中都实现了完美的域内响应准确率，但其token级概率和熵值与相应的理论分布持续偏离。

Conclusion: 大语言模型在概率场景中虽然能给出正确回答，但其内部概率表示与理论概率分布不一致，这表明当前的不确定性量化方法在概率场景中存在局限性。

Abstract: Reliable uncertainty quantification (UQ) is essential for ensuring
trustworthy downstream use of large language models, especially when they are
deployed in decision-support and other knowledge-intensive applications. Model
certainty can be estimated from token logits, with derived probability and
entropy values offering insight into performance on the prompt task. However,
this approach may be inadequate for probabilistic scenarios, where the
probabilities of token outputs are expected to align with the theoretical
probabilities of the possible outcomes. We investigate the relationship between
token certainty and alignment with theoretical probability distributions in
well-defined probabilistic scenarios. Using GPT-4.1 and DeepSeek-Chat, we
evaluate model responses to ten prompts involving probability (e.g., roll a
six-sided die), both with and without explicit probability cues in the prompt
(e.g., roll a fair six-sided die). We measure two dimensions: (1) response
validity with respect to scenario constraints, and (2) alignment between
token-level output probabilities and theoretical probabilities. Our results
indicate that, while both models achieve perfect in-domain response accuracy
across all prompt scenarios, their token-level probability and entropy values
consistently diverge from the corresponding theoretical distributions.

</details>


### [30] [Modeling the Construction of a Literary Archetype: The Case of the Detective Figure in French Literature](https://arxiv.org/abs/2511.00627)
*Jean Barré,Olga Seminck,Antoine Bourgois,Thierry Poibeau*

Main category: cs.CL

TL;DR: 通过计算分析研究法国侦探小说中侦探原型的演变，发现监督模型能够捕捉150年间侦探原型的统一性，并展示了从次要叙事角色到核心推理机器的转变过程。


<details>
  <summary>Details</summary>
Motivation: 探索法国侦探小说中侦探原型在150年间的演变轨迹，理解该文学类型中核心人物的变化规律。

Method: 使用定量方法和角色级嵌入技术，通过监督模型分析从1866年到2017年的法国侦探小说文本。

Result: 模型成功捕捉了侦探原型的统一性，揭示了侦探从次要角色发展为古典侦探故事核心推理机器的演变过程，以及二战后受硬汉派影响变得更加复杂的转变。

Conclusion: 法国侦探小说中的侦探原型经历了从叙事配角到核心推理角色的演变，并在二战后受到硬汉派影响而变得更加复杂，反映了社会暴力和道德模糊性的转向。

Abstract: This research explores the evolution of the detective archetype in French
detective fiction through computational analysis. Using quantitative methods
and character-level embeddings, we show that a supervised model is able to
capture the unity of the detective archetype across 150 years of literature,
from M. Lecoq (1866) to Commissaire Adamsberg (2017). Building on this finding,
the study demonstrates how the detective figure evolves from a secondary
narrative role to become the central character and the "reasoning machine" of
the classical detective story. In the aftermath of the Second World War, with
the importation of the hardboiled tradition into France, the archetype becomes
more complex, navigating the genre's turn toward social violence and moral
ambiguity.

</details>


### [31] [Do You Know About My Nation? Investigating Multilingual Language Models' Cultural Literacy Through Factual Knowledge](https://arxiv.org/abs/2511.00657)
*Eshaan Tanwar,Anwoy Chatterjee,Michael Saxon,Alon Albalak,William Yang Wang,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: XNationQA是一个新的多语言问答基准，涵盖9个国家的地理、文化和历史，包含49,280个问题，使用7种语言，用于评估多语言LLM的文化素养。


<details>
  <summary>Details</summary>
Motivation: 现有的大多数多语言问答基准虽然覆盖多种语言，但信息内容偏向西方中心主义，未能考虑区域多样性，这导致无法公平评估多语言模型对不同地理位置事实信息的理解能力。

Method: 创建XNationQA基准数据集，包含9个国家的地理、文化和历史问题，使用7种语言呈现。对8个标准多语言LLM进行基准测试，并使用两种新的传递性指标进行评估。

Result: 发现模型在不同语言中获取文化特定事实的能力存在显著差异；模型在英语中展示的文化知识往往比相应文化的主导语言更丰富；模型在西方语言中表现更好，但这并不一定意味着对西方国家更了解；模型跨语言知识传递能力有限，开源模型尤其明显。

Conclusion: 多语言LLM在文化素养方面存在显著偏差，需要开发更公平、考虑区域多样性的评估方法，以更好地理解和改进模型的文化理解能力。

Abstract: Most multilingual question-answering benchmarks, while covering a diverse
pool of languages, do not factor in regional diversity in the information they
capture and tend to be Western-centric. This introduces a significant gap in
fairly evaluating multilingual models' comprehension of factual information
from diverse geographical locations. To address this, we introduce XNationQA
for investigating the cultural literacy of multilingual LLMs. XNationQA
encompasses a total of 49,280 questions on the geography, culture, and history
of nine countries, presented in seven languages. We benchmark eight standard
multilingual LLMs on XNationQA and evaluate them using two novel transference
metrics. Our analyses uncover a considerable discrepancy in the models'
accessibility to culturally specific facts across languages. Notably, we often
find that a model demonstrates greater knowledge of cultural information in
English than in the dominant language of the respective culture. The models
exhibit better performance in Western languages, although this does not
necessarily translate to being more literate for Western countries, which is
counterintuitive. Furthermore, we observe that models have a very limited
ability to transfer knowledge across languages, particularly evident in
open-source models.

</details>


### [32] [Do Methods to Jailbreak and Defend LLMs Generalize Across Languages?](https://arxiv.org/abs/2511.00689)
*Berk Atil,Rebecca J. Passonneau,Fred Morstatter*

Main category: cs.CL

TL;DR: 该论文首次对10种语言的越狱攻击和防御进行了系统性多语言评估，发现攻击成功率和防御鲁棒性在不同语言间存在差异，高资源语言在标准查询下更安全但对对抗性攻击更脆弱。


<details>
  <summary>Details</summary>
Motivation: 虽然现有研究提出了许多越狱攻击和防御方法，但它们在跨语言泛化能力方面仍未被充分探索，需要系统评估不同语言环境下的安全性能。

Method: 在HarmBench和AdvBench上使用6个LLM，评估了基于逻辑表达式和对抗性提示的两种越狱攻击类型，涵盖高、中、低资源共10种语言。

Result: 攻击成功率和防御鲁棒性因语言而异：高资源语言在标准查询下更安全，但对对抗性攻击更脆弱；简单防御方法有效但依赖具体语言和模型。

Conclusion: 研究结果表明需要为LLM开发语言感知和跨语言的安全基准测试标准。

Abstract: Large language models (LLMs) undergo safety alignment after training and
tuning, yet recent work shows that safety can be bypassed through jailbreak
attacks. While many jailbreaks and defenses exist, their cross-lingual
generalization remains underexplored. This paper presents the first systematic
multilingual evaluation of jailbreaks and defenses across ten
languages--spanning high-, medium-, and low-resource languages--using six LLMs
on HarmBench and AdvBench. We assess two jailbreak types:
logical-expression-based and adversarial-prompt-based. For both types, attack
success and defense robustness vary across languages: high-resource languages
are safer under standard queries but more vulnerable to adversarial ones.
Simple defenses can be effective, but are language- and model-dependent. These
findings call for language-aware and cross-lingual safety benchmarks for LLMs.

</details>


### [33] [Optimizing Native Sparse Attention with Latent Attention and Local Global Alternating Strategies](https://arxiv.org/abs/2511.00819)
*Yuxuan Hu,Jianchao Tan,Jiaqi Zhang,Wen Zan,Pingwei Sun,Yifan Lu,Yerui Sun,Yuchen Xie,Xunliang Cai,Jing Zhang*

Main category: cs.CL

TL;DR: 本文系统分析了原生稀疏注意力(NSA)，提出了交替使用局部和全局注意力的改进方法，结合潜在注意力机制，在减少50% KV缓存的同时提升了长文本理解能力。


<details>
  <summary>Details</summary>
Motivation: 改进原生稀疏注意力机制，使其能更有效地传播长距离依赖关系，同时降低内存消耗。

Method: 在层间交替使用局部(滑动窗口)和全局(压缩、选择性)注意力模式，并引入多头潜在注意力(MLA)和分组头潜在注意力(GLA)来增强各分支。

Result: 在3.4亿到13亿参数的模型上实验表明，该方法在常识推理和长文本理解任务上匹配或超越了全注意力和原生稀疏注意力。

Conclusion: 交替注意力模式和潜在注意力机制的结合能有效提升长上下文建模能力，同时显著减少内存需求。

Abstract: In this work, we conduct a systematic analysis of Native Sparse Attention
(NSA) and propose targeted improvements that enhance long-context modeling. A
key insight is that alternating between local (sliding-window) and global
(compression, selective) attention across layers, rather than using fixed
patterns, enables more effective propagation of long-range dependencies and
substantially boosts performance on long-sequence tasks. Meanwhile, we further
refine NSA's branches with Latent Attention that the sliding-window branch is
enhanced with Multi-head Latent Attention (MLA) while compression and selective
branches adopt Group-head Latent Attention (GLA). These changes reduce KV-cache
memory by 50\% versus NSA while improving the model's common-sense reasoning
and long-text understanding capabilities. Experiments on models from 340M to
1.3B parameters (trained on 15B and 100B tokens) show our method matches or
exceeds full attention and native sparse attention in both common-sense
reasoning and long-context understanding tasks.

</details>


### [34] [TriCon-Fair: Triplet Contrastive Learning for Mitigating Social Bias in Pre-trained Language Models](https://arxiv.org/abs/2511.00854)
*Chong Lyu,Lin Li,Shiqing Wu,Jingling Yuan*

Main category: cs.CL

TL;DR: TriCon-Fair是一个对比学习框架，通过解耦的三重损失函数消除偏见消除中的正负耦合问题，在减少歧视性输出的同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有偏见消除方法独立处理有偏见和无偏见样本，忽视了它们之间的相互关系，导致隐藏的正负耦合问题——改善一个群体时无意中损害另一个群体，使残余社会偏见持续存在。

Method: TriCon-Fair采用解耦损失函数，结合三重损失和语言建模目标，为每个锚点分配明确有偏见的负样本和无偏见的正样本，解耦推拉动态，避免正负耦合，同时通过语言建模目标保持通用能力。

Result: 实验结果表明，TriCon-Fair在减少歧视性输出方面优于现有偏见消除基线方法，同时保持了强大的下游任务性能。

Conclusion: TriCon-Fair为敏感NLP应用提供了一个实用且符合伦理的解决方案，能够有效消除社会偏见而不损害模型性能。

Abstract: The increasing utilization of large language models raises significant
concerns about the propagation of social biases, which may result in harmful
and unfair outcomes. However, existing debiasing methods treat the biased and
unbiased samples independently, thus ignoring their mutual relationship. This
oversight enables a hidden negative-positive coupling, where improvements for
one group inadvertently compromise the other, allowing residual social bias to
persist. In this paper, we introduce TriCon-Fair, a contrastive learning
framework that employs a decoupled loss that combines triplet and language
modeling terms to eliminate positive-negative coupling. Our TriCon-Fair assigns
each anchor an explicitly biased negative and an unbiased positive, decoupling
the push-pull dynamics and avoiding positive-negative coupling, and jointly
optimizes a language modeling (LM) objective to preserve general capability.
Experimental results demonstrate that TriCon-Fair reduces discriminatory output
beyond existing debiasing baselines while maintaining strong downstream
performance. This suggests that our proposed TriCon-Fair offers a practical and
ethical solution for sensitive NLP applications.

</details>


### [35] [Assessing LLM Reasoning Steps via Principal Knowledge Grounding](https://arxiv.org/abs/2511.00879)
*Hyeon Hwang,Yewon Cho,Chanwoong Yoon,Yein Park,Minju Song,Kyungjae Lee,Gangwoo Kim,Jaewoo Kang*

Main category: cs.CL

TL;DR: 提出了一个评估大语言模型推理过程中知识基础的新框架，包含大规模知识库、知识基础评估指标和轻量级评估器LLM，能有效识别推理中的知识缺失或误用问题。


<details>
  <summary>Details</summary>
Motivation: 随着逐步推理成为LLM处理复杂任务的标准方法，需要验证LLM的推理是否准确基于知识基础，解决推理过程中的知识基础问题。

Method: 构建包含三个关键组件的评估框架：(1) 主要知识收集 - 大规模原子知识库；(2) 知识基础评估指标 - 衡量模型在推理中回忆和应用先验知识的能力；(3) 评估器LLM - 轻量级模型用于成本效益高的指标计算。

Result: 评估套件在识别缺失或误用的知识元素方面表现出显著效果，为揭示LLM基本推理缺陷提供了关键见解。

Conclusion: 该知识基础评估框架不仅能有效评估LLM推理，还可集成到偏好优化中，展示了知识基础评估的进一步应用价值。

Abstract: Step-by-step reasoning has become a standard approach for large language
models (LLMs) to tackle complex tasks. While this paradigm has proven
effective, it raises a fundamental question: How can we verify that an LLM's
reasoning is accurately grounded in knowledge? To address this question, we
introduce a novel evaluation suite that systematically assesses the knowledge
grounding of intermediate reasoning. Our framework comprises three key
components. (1) Principal Knowledge Collection, a large-scale repository of
atomic knowledge essential for reasoning. Based on the collection, we propose
(2) knowledge-grounded evaluation metrics designed to measure how well models
recall and apply prerequisite knowledge in reasoning. These metrics are
computed by our (3) evaluator LLM, a lightweight model optimized for
cost-effective and reliable metric computation. Our evaluation suite
demonstrates remarkable effectiveness in identifying missing or misapplied
knowledge elements, providing crucial insights for uncovering fundamental
reasoning deficiencies in LLMs. Beyond evaluation, we demonstrate how these
metrics can be integrated into preference optimization, showcasing further
applications of knowledge-grounded evaluation.

</details>


### [36] [ColMate: Contrastive Late Interaction and Masked Text for Multimodal Document Retrieval](https://arxiv.org/abs/2511.00903)
*Ahmed Masry,Megh Thakkar,Patrice Bechard,Sathwik Tejaswi Madhusudhan,Rabiul Awal,Shambhavi Mishra,Akshay Kalkunte Suresh,Srivatsava Daruru,Enamul Hoque,Spandana Gella,Torsten Scholak,Sai Rajeswar*

Main category: cs.CL

TL;DR: ColMate是一个多模态文档检索模型，通过OCR预训练目标、自监督掩码对比学习和延迟交互评分机制，在ViDoRe V2基准上比现有检索模型提升3.61%。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态文档检索方法往往只是复制文本检索技术，在文档编码、训练目标和相似度计算方面存在局限性，需要专门针对多模态文档结构和视觉特性的方法。

Method: 使用OCR预训练目标、自监督掩码对比学习目标和延迟交互评分机制，更好地适应多模态文档的结构和视觉特征。

Result: 在ViDoRe V2基准上比现有检索模型提升3.61%，在领域外基准上表现出更强的泛化能力。

Conclusion: ColMate成功弥合了多模态表示学习和文档检索之间的差距，为多模态文档检索提供了更有效的解决方案。

Abstract: Retrieval-augmented generation has proven practical when models require
specialized knowledge or access to the latest data. However, existing methods
for multimodal document retrieval often replicate techniques developed for
text-only retrieval, whether in how they encode documents, define training
objectives, or compute similarity scores. To address these limitations, we
present ColMate, a document retrieval model that bridges the gap between
multimodal representation learning and document retrieval. ColMate utilizes a
novel OCR-based pretraining objective, a self-supervised masked contrastive
learning objective, and a late interaction scoring mechanism more relevant to
multimodal document structures and visual characteristics. ColMate obtains
3.61% improvements over existing retrieval models on the ViDoRe V2 benchmark,
demonstrating stronger generalization to out-of-domain benchmarks.

</details>


### [37] [The Biased Oracle: Assessing LLMs' Understandability and Empathy in Medical Diagnoses](https://arxiv.org/abs/2511.00924)
*Jianzhou Yao,Shunchang Liu,Guillaume Drui,Rikard Pettersson,Alessandro Blasimme,Sara Kijewski*

Main category: cs.CL

TL;DR: 评估大语言模型在医疗诊断沟通中的表现，发现虽然能根据患者特征调整解释，但存在内容过于复杂和情感共情偏见问题，导致可及性和支持不平等。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在医疗诊断沟通中生成既易懂又富有共情的解释和指导的能力，以支持临床医生。

Method: 在医疗诊断场景中评估两个领先的LLM，使用可读性指标作为易懂性的代理，通过LLM-as-a-Judge评分与人工评估比较来评估共情能力。

Result: LLMs能够根据社会人口学变量和患者状况调整解释，但会产生过于复杂的内容并表现出有偏见的情感共情，导致可及性和支持不平等。

Conclusion: 这些模式强调需要进行系统校准以确保公平的患者沟通。

Abstract: Large language models (LLMs) show promise for supporting clinicians in
diagnostic communication by generating explanations and guidance for patients.
Yet their ability to produce outputs that are both understandable and
empathetic remains uncertain. We evaluate two leading LLMs on medical
diagnostic scenarios, assessing understandability using readability metrics as
a proxy and empathy through LLM-as-a-Judge ratings compared to human
evaluations. The results indicate that LLMs adapt explanations to
socio-demographic variables and patient conditions. However, they also generate
overly complex content and display biased affective empathy, leading to uneven
accessibility and support. These patterns underscore the need for systematic
calibration to ensure equitable patient communication. The code and data are
released: https://github.com/Jeffateth/Biased_Oracle

</details>


### [38] [The Riddle of Reflection: Evaluating Reasoning and Self-Awareness in Multilingual LLMs using Indian Riddles](https://arxiv.org/abs/2511.00960)
*Abhinav P M,Ojasva Saxena,Oswald C,Parameswari Krishnamurthy*

Main category: cs.CL

TL;DR: 评估大语言模型在7种印度语言中的文化推理能力，发现模型初始准确率与自我纠错能力呈负相关，最佳模型过度自信，而表现较差的模型反而更具自我意识。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在非英语语言（特别是印度语言）中的文化推理能力，以及模型自我评估能力的表现。

Method: 构建多语言谜语数据集，包含传统谜语和上下文重构变体，评估5个LLM在7种印度语言中的表现，采用7种提示策略，分两阶段评估：谜语解决能力和自我评估一致性。

Result: Gemini 2.5 Pro整体表现最佳，但少样本方法提升有限，准确率在不同语言间差异显著。模型初始准确率与识别自身错误能力呈负相关：高准确率模型过度自信（真负率4.34%），低准确率模型更具自我意识（真负率42.09%）。

Conclusion: 多语言推理存在明显差距，需要开发既能有效推理又能识别自身局限的模型。

Abstract: The extent to which large language models (LLMs) can perform culturally
grounded reasoning across non-English languages remains underexplored. This
paper examines the reasoning and self-assessment abilities of LLMs across seven
major Indian languages-Bengali, Gujarati, Hindi, Kannada, Malayalam, Tamil, and
Telugu. We introduce a multilingual riddle dataset combining traditional
riddles with context-reconstructed variants and evaluate five LLMs-Gemini 2.5
Pro, Gemini 2.5 Flash, Mistral-Saba, LLaMA 4 Scout, and LLaMA 4 Maverick-under
seven prompting strategies. In the first stage, we assess riddle-solving
performance and find that while Gemini 2.5 Pro performs best overall, few-shot
methods yield only marginal gains, and accuracy varies notably across
languages. In the second stage, we conduct a self-evaluation experiment to
measure reasoning consistency. The results reveal a key finding: a model's
initial accuracy is inversely correlated with its ability to identify its own
mistakes. Top-performing models such as Gemini 2.5 Pro are overconfident (4.34%
True Negative Rate), whereas lower-performing models like LLaMA 4 Scout are
substantially more self-aware (42.09% True Negative Rate). These results point
to clear gaps in multilingual reasoning and highlight the need for models that
not only reason effectively but also recognize their own limitations.

</details>


### [39] [Advancing Machine-Generated Text Detection from an Easy to Hard Supervision Perspective](https://arxiv.org/abs/2511.00988)
*Chenwang Wu,Yiu-ming Cheung,Bo Han,Defu Lian*

Main category: cs.CL

TL;DR: 提出了一种从易到难的增强框架，用于在标签不精确条件下改进机器生成文本检测，通过使用针对长文本的简易监督器来增强目标检测器。


<details>
  <summary>Details</summary>
Motivation: 现有机器生成文本检测方法假设标签是"黄金标准"，但存在边界模糊问题，且人类认知局限和检测器超智能使得不精确学习普遍存在。

Method: 采用易到难增强框架，使用针对较长文本的简易监督器（尽管能力较弱）来增强更具挑战性的目标检测器，通过结构整合将监督器建模为检测器的性能下界。

Result: 在跨LLM、跨领域、混合文本和改写攻击等多种实际场景中的广泛实验表明，该框架具有显著的检测效果。

Conclusion: 该框架通过优化监督器间接优化检测器，最终逼近潜在的"黄金"标签，为不精确条件下的可靠监督提供了解决方案。

Abstract: Existing machine-generated text (MGT) detection methods implicitly assume
labels as the "golden standard". However, we reveal boundary ambiguity in MGT
detection, implying that traditional training paradigms are inexact. Moreover,
limitations of human cognition and the superintelligence of detectors make
inexact learning widespread and inevitable. To this end, we propose an
easy-to-hard enhancement framework to provide reliable supervision under such
inexact conditions. Distinct from knowledge distillation, our framework employs
an easy supervisor targeting relatively simple longer-text detection tasks
(despite weaker capabilities), to enhance the more challenging target detector.
Firstly, longer texts targeted by supervisors theoretically alleviate the
impact of inexact labels, laying the foundation for reliable supervision.
Secondly, by structurally incorporating the detector into the supervisor, we
theoretically model the supervisor as a lower performance bound for the
detector. Thus, optimizing the supervisor indirectly optimizes the detector,
ultimately approximating the underlying "golden" labels. Extensive experiments
across diverse practical scenarios, including cross-LLM, cross-domain, mixed
text, and paraphrase attacks, demonstrate the framework's significant detection
effectiveness. The code is available at:
https://github.com/tmlr-group/Easy2Hard.

</details>


### [40] [MARS-SQL: A multi-agent reinforcement learning framework for Text-to-SQL](https://arxiv.org/abs/2511.01008)
*Haolin Yang,Jipeng Zhang,Zhitao He,Yi R. Fung*

Main category: cs.CL

TL;DR: MARS-SQL是一个多智能体框架，通过任务分解和交互式强化学习解决复杂自然语言到SQL的转换问题，在BIRD和Spider数据集上达到最先进性能


<details>
  <summary>Details</summary>
Motivation: 解决复杂自然语言查询转换为SQL时的环境交互和自我修正需求

Method: 使用三个专门智能体：基础智能体进行模式链接，生成智能体通过多轮强化学习策略生成查询，验证智能体进行最终选择；采用ReAct风格的思考-执行-观察循环

Result: 在BIRD开发集上达到77.84%的执行准确率，在Spider测试集上达到89.75%的执行准确率

Conclusion: 该结构化工作流结合了交互式强化学习和生成建模，为鲁棒准确的SQL生成提供了有效解决方案

Abstract: Translating natural language to SQL remains difficult for complex queries.
Such queries often need environmental interaction and self-correction. To
address this, we introduce MARS-SQL, a novel multi-agent framework that
combines principled task decomposition and interactive reinforcement learning
(RL). Our system comprises three specialized agents: a Grounding Agent for
schema linking, a Generation Agent for query generation, and a Validation Agent
for final selection. The core of our framework is the Generation agent, which
is trained via a multi-turn RL policy. Adopting a ReAct-style Think-Act-Observe
loop, the agent iteratively generates thoughts, executes SQL actions against a
live database, and revises its strategy based on execution feedback, enabling
dynamic, stateful reasoning and self-correction. At inference time, we generate
multiple interaction trajectories to explore diverse reasoning paths. The
Validation agent, then selects the optimal trajectory by modeling verification
as a next-token prediction task and choosing the solution with the highest
generation probability. This structured workflow pipelines specialized agents.
It combines interactive RL for generation with generative modeling for
verification. The approach proves highly effective for robust and accurate SQL
generation. Experiments show that MARS-SQL achieves state-of-the-art Execution
Accuracy of 77.84% on the BIRD dev set and 89.75% on the Spider test set. Our
code is available at https://github.com/YangHaolin0526/MARS-SQL.

</details>


### [41] [IF-CRITIC: Towards a Fine-Grained LLM Critic for Instruction-Following Evaluation](https://arxiv.org/abs/2511.01014)
*Bosi Wen,Yilin Niu,Cunxiang Wang,Pei Ke,Xiaoying Ling,Ying Zhang,Aohan Zeng,Hongning Wang,Minlie Huang*

Main category: cs.CL

TL;DR: 提出了IF-CRITIC模型，用于高效可靠地评估指令遵循能力，通过约束清单生成和多阶段筛选机制训练，在指令遵循优化中显著提升性能并降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有指令遵循评估模型存在成本高和评估不可靠的问题，需要开发更高效的评估方法。

Method: 开发约束清单生成器分解指令，通过多阶段筛选机制收集高质量批判数据，采用约束级偏好优化方法训练IF-CRITIC。

Result: IF-CRITIC在评估性能上超越了Deepseek-R1和o4-mini等强基线模型，能在更低计算开销下实现指令遵循优化的显著性能提升。

Conclusion: IF-CRITIC为指令遵循优化提供了可扩展的奖励信号，是解决现有评估模型缺陷的有效方案。

Abstract: Instruction following is a fundamental ability of Large Language Models
(LLMs), requiring their generated outputs to follow multiple constraints
imposed in input instructions. Numerous studies have attempted to enhance this
ability through preference optimization or reinforcement learning based on
reward signals from LLM-as-a-Judge. However, existing evaluation models for
instruction following still possess many deficiencies, such as substantial
costs and unreliable assessments. To this end, we propose IF-CRITIC, an LLM
critic that can provide efficient and reliable assessments of constraint
following in the instructions. We first develop a checklist generator to
decompose instructions and generate constraint checklists. With the assistance
of the checklists, we collect high-quality critique training data through a
multi-stage critique filtering mechanism and employ a constraint-level
preference optimization method to train IF-CRITIC. Extensive experiments
demonstrate that the evaluation performance of IF-CRITIC can beat strong
LLM-as-a-Judge baselines, including Deepseek-R1 and o4-mini. With the scalable
reward signals provided by IF-CRITIC, LLMs can achieve substantial performance
gains in instruction-following optimization under lower computational overhead
compared to strong LLM critic baselines.

</details>


### [42] [Prompt-R1: Collaborative Automatic Prompting Framework via End-to-end Reinforcement Learning](https://arxiv.org/abs/2511.01016)
*Wenjin Liu,Haoran Luo,Xueyuan Lin,Haoming Liu,Tiesunlong Shen,Jiapu Wang,Rui Mao,Erik Cambria*

Main category: cs.CL

TL;DR: Prompt-R1是一个端到端的强化学习框架，使用小规模LLM与大规模LLM协作，通过多轮提示交互解决复杂问题，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前用户难以提供准确有效的提示来与大型语言模型交互，限制了LLM在复杂问题上的表现。

Method: 采用强化学习框架，小规模LLM负责思考和生成提示，大规模LLM进行复杂推理，设计双约束奖励机制优化正确性、生成质量和推理准确性。

Result: 在多个公共数据集上的实验表明，Prompt-R1显著优于基线模型。

Conclusion: Prompt-R1提供了一个即插即用的协作框架，有效解决了用户提示生成困难的问题，提升了LLM在复杂任务中的表现。

Abstract: Recently, advanced large language models (LLMs) have emerged at an
increasingly rapid pace. However, when faced with complex problems, most users
are often unable to provide accurate and effective prompts to interact with
LLMs, thus limiting the performance of LLMs. To address this challenge, we
propose Prompt-R1, an end-to-end reinforcement learning framework that uses a
small-scale LLM to collaborate with large-scale LLMs, replacing user
interaction to solve problems better. This collaboration is cast as a
multi-turn prompt interaction, where the small-scale LLM thinks and generates
prompts, and the large-scale LLM performs complex reasoning. A dual-constrained
reward is designed to optimize for correctness, generation quality, and
reasoning accuracy. Prompt-R1 provides a plug-and-play framework that supports
both inference and training with various large-scale LLMs. Experiments on
multiple public datasets show that Prompt-R1 significantly outperforms baseline
models across tasks. Our code is publicly available at
https://github.com/QwenQKing/Prompt-R1.

</details>


### [43] [OceanAI: A Conversational Platform for Accurate, Transparent, Near-Real-Time Oceanographic Insights](https://arxiv.org/abs/2511.01019)
*Bowen Chen,Jayesh Gajbhar,Gregory Dusek,Rob Redmon,Patrick Hogan,Paul Liu,DelWayne Bohnenstiehl,Dongkuan,Xu,Ruoying He*

Main category: cs.CL

TL;DR: OceanAI是一个结合开源大语言模型与NOAA海洋数据流的对话平台，通过实时API调用生成可验证的自然语言响应和数据可视化，解决AI在科学领域中的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 通用对话AI系统经常产生未经核实的"幻觉"，破坏了科学严谨性。需要开发能够整合权威科学数据的AI平台来确保输出的准确性和可验证性。

Method: 将开源大语言模型的自然语言流畅性与NOAA的实时参数化海洋数据流集成，每个查询触发实时API调用，识别、解析和合成相关数据集。

Result: 在盲测比较中，只有OceanAI能够提供NOAA来源的数值和原始数据引用，其他AI产品要么拒绝回答，要么提供未经支持的结果。

Conclusion: OceanAI通过基于可验证观测的输出，提高了透明度、可重复性和信任度，为海洋领域的AI决策支持提供了可扩展框架。

Abstract: Artificial intelligence is transforming the sciences, yet general
conversational AI systems often generate unverified "hallucinations"
undermining scientific rigor. We present OceanAI, a conversational platform
that integrates the natural-language fluency of open-source large language
models (LLMs) with real-time, parameterized access to authoritative
oceanographic data streams hosted by the National Oceanic and Atmospheric
Administration (NOAA). Each query such as "What was Boston Harbor's highest
water level in 2024?" triggers real-time API calls that identify, parse, and
synthesize relevant datasets into reproducible natural-language responses and
data visualizations. In a blind comparison with three widely used AI
chat-interface products, only OceanAI produced NOAA-sourced values with
original data references; others either declined to answer or provided
unsupported results. Designed for extensibility, OceanAI connects to multiple
NOAA data products and variables, supporting applications in marine hazard
forecasting, ecosystem assessment, and water-quality monitoring. By grounding
outputs and verifiable observations, OceanAI advances transparency,
reproducibility, and trust, offering a scalable framework for AI-enabled
decision support within the oceans. A public demonstration is available at
https://oceanai.ai4ocean.xyz.

</details>


### [44] [VayuChat: An LLM-Powered Conversational Interface for Air Quality Data Analytics](https://arxiv.org/abs/2511.01046)
*Vedant Acharya,Abhay Pisharodi,Rishabh Mondal,Mohammad Rafiuddin,Nipun Batra*

Main category: cs.CL

TL;DR: VayuChat是一个对话式系统，通过自然语言回答空气质量、气象和政策相关问题，生成可执行Python代码和交互式可视化，使环境数据分析对决策者、研究人员和公民更加可访问。


<details>
  <summary>Details</summary>
Motivation: 印度每年因空气污染导致约160万人过早死亡，但现有工具需要专业知识且提供静态仪表板，无法解决关键政策问题。决策者难以将分散数据转化为有效决策。

Method: VayuChat整合了中央污染控制委员会监测站数据、州级人口统计数据和国家清洁空气计划资金记录，通过大语言模型提供统一对话界面，用户可通过简单对话执行复杂环境分析。

Result: 该系统已公开部署，能够通过自然语言交互生成可执行代码和可视化，使数据科学对非专业人士更加友好。

Conclusion: VayuChat通过对话式界面降低了环境数据分析的门槛，使政策制定者、研究人员和公民都能轻松获取和利用空气质量数据，促进更有效的环境决策。

Abstract: Air pollution causes about 1.6 million premature deaths each year in India,
yet decision makers struggle to turn dispersed data into decisions. Existing
tools require expertise and provide static dashboards, leaving key policy
questions unresolved. We present VayuChat, a conversational system that answers
natural language questions on air quality, meteorology, and policy programs,
and responds with both executable Python code and interactive visualizations.
VayuChat integrates data from Central Pollution Control Board (CPCB) monitoring
stations, state-level demographics, and National Clean Air Programme (NCAP)
funding records into a unified interface powered by large language models. Our
live demonstration will show how users can perform complex environmental
analytics through simple conversations, making data science accessible to
policymakers, researchers, and citizens. The platform is publicly deployed at
https://huggingface.co/spaces/SustainabilityLabIITGN/ VayuChat. For further
information check out video uploaded on
https://www.youtube.com/watch?v=d6rklL05cs4.

</details>


### [45] [Building a Silver-Standard Dataset from NICE Guidelines for Clinical LLMs](https://arxiv.org/abs/2511.01053)
*Qing Ding,Eric Hua Qing Zhang,Felix Jozsa,Julia Ive*

Main category: cs.CL

TL;DR: 该研究创建了一个基于临床指南的标准化数据集，用于评估大型语言模型在医疗临床推理中的表现。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏评估基于指南的临床推理的标准化基准，而大型语言模型在医疗领域的应用日益增多。

Method: 利用GPT帮助创建包含现实患者场景和临床问题的数据集，该数据集源自多个诊断的公开指南。

Result: 通过基准测试验证了数据集的可靠性，并展示了多种流行LLM的表现。

Conclusion: 该框架支持对LLM的临床实用性和指南依从性进行系统性评估。

Abstract: Large language models (LLMs) are increasingly used in healthcare, yet
standardised benchmarks for evaluating guideline-based clinical reasoning are
missing. This study introduces a validated dataset derived from publicly
available guidelines across multiple diagnoses. The dataset was created with
the help of GPT and contains realistic patient scenarios, as well as clinical
questions. We benchmark a range of recent popular LLMs to showcase the validity
of our dataset. The framework supports systematic evaluation of LLMs' clinical
utility and guideline adherence.

</details>


### [46] [HPLT~3.0: Very Large-Scale Multilingual Resources for LLM and MT. Mono- and Bi-lingual Data, Multilingual Evaluation, and Pre-Trained Models](https://arxiv.org/abs/2511.01066)
*Stephan Oepen,Nikolay Arefev,Mikko Aulamo,Marta Bañón,Maja Buljan,Laurie Burchell,Lucas Charpentier,Pinzhen Chen,Mariya Fedorova,Ona de Gibert,Barry Haddow,Jan Hajič,Jindrič Helcl,Andrey Kutuzov,Zihao Li,Risto Luukkonen,Bhavitvya Malik,Vladislav Mikhailov,Amanda Myntti,Dayyán O'Brien,Lucie Poláková,Sampo Pyysalo,Gema Ramírez Sánchez,Janine Siewert,Pavel Stepachev,Jörg Tiedemann,Teemu Vahtola,Fedor Vitiugin,Tea Vojtěchová,Jaume Zaragoza*

Main category: cs.CL

TL;DR: 该论文介绍了一个包含近200种语言、30万亿token的大规模多语言LLM预训练数据集，提供了完整的数据处理流程和评估基准。


<details>
  <summary>Details</summary>
Motivation: 为多语言大语言模型预训练提供高质量、大规模、开放可用的数据集，解决当前多语言数据资源不足的问题。

Method: 从不同来源的网络爬虫数据构建数据集，使用开源流程进行文档选择、文本提取、语言识别、去重、标注（包括文本质量、个人信息等）和最终筛选。

Result: 创建了包含30万亿token的多语言数据集，训练了57个单语言编码器-解码器模型和GPT类参考模型，并提供了多语言评估基准。

Conclusion: 该项目为多语言NLP研究提供了宝贵的数据资源和评估工具，推动了多语言大语言模型的发展。

Abstract: We present an ongoing initiative to provide open, very large, high-quality,
and richly annotated textual datasets for almost 200 languages. At 30 trillion
tokens, this is likely the largest generally available multilingual collection
of LLM pre-training data. At 30 trillion tokens, this is likely the largest
generally available multilingual collection of LLM pre-training data. These
datasets are derived from web crawls from different sources and accompanied
with a complete, open-source pipeline for document selection from web archives,
text extraction from HTML, language identification for noisy texts, exact and
near-deduplication, annotation with, among others, register labels, text
quality estimates, and personally identifiable information; and final selection
and filtering. We report on data quality probes through contrastive and
analytical statistics, through manual inspection of samples for 24 languages,
and through end-to-end evaluation of various language model architectures
trained on this data. For multilingual LLM evaluation, we provide a
comprehensive collection of benchmarks for nine European languages, with
special emphasis on natively created tasks, mechanisms to mitigate prompt
sensitivity, and refined normalization and aggregation of scores. Additionally,
we train and evaluate a family of 57 monolingual encoder-decoder models, as
well as a handful of monolingual GPT-like reference models. Besides the
monolingual data and models, we also present a very large collection of
parallel texts automatically mined from this data, together with a novel
parallel corpus synthesized via machine translation.

</details>


### [47] [Improving Romanian LLM Pretraining Data using Diversity and Quality Filtering](https://arxiv.org/abs/2511.01090)
*Vlad Negoita,Mihai Masala,Traian Rebedea*

Main category: cs.CL

TL;DR: 该研究分析了罗马尼亚语预训练语料库的特征和覆盖范围，并与英语数据进行比较，通过LLM标注和多级过滤生成高质量罗马尼亚语预训练数据集。


<details>
  <summary>Details</summary>
Motivation: 高质量数据对训练大语言模型至关重要，特别是对于罗马尼亚语等资源稀缺语言，需要研究其语料特征并改进数据质量。

Method: 训练轻量级多任务模型对LLM标注的罗马尼亚文本进行分析，执行多级过滤（教育价值、主题、格式等）来生成高质量预训练数据集。

Result: 实验显示罗马尼亚语和英语数据在主题分布上存在显著差异，通过数据过滤有效提升了LLM在多个基准测试上的预训练性能。

Conclusion: 数据过滤方法能显著改善资源稀缺语言的预训练效果，为低资源语言的大语言模型训练提供了有效的数据质量提升方案。

Abstract: Large Language Models (LLMs) have recently exploded in popularity, often
matching or outperforming human abilities on many tasks. One of the key factors
in training LLMs is the availability and curation of high-quality data. Data
quality is especially crucial for under-represented languages, where
high-quality corpora are scarce. In this work we study the characteristics and
coverage of Romanian pretraining corpora and we examine how they differ from
English data. By training a lightweight multitask model on carefully
LLM-annotated Romanian texts, we are able to analyze and perform multi-level
filtering (e.g., educational value, topic, format) to generate high-quality
pretraining datasets. Our experiments show noteworthy trends in the topics
present in Romanian and English data, while also proving the effectiveness of
filtering data through improved LLM pretraining performance across multiple
benchmarks.

</details>


### [48] [TSVer: A Benchmark for Fact Verification Against Time-Series Evidence](https://arxiv.org/abs/2511.01101)
*Marek Strong,Andreas Vlachos*

Main category: cs.CL

TL;DR: TSVer是一个专注于时间序列证据的时序和数值推理事实核查基准数据集，包含287个真实世界声明和400个时间序列，通过LLM辅助的多步骤标注流程提高标注质量。


<details>
  <summary>Details</summary>
Motivation: 现有事实核查系统在评估时序和数值推理能力方面受到数据集限制，这些数据集通常缺乏结构化证据、判决理由不足或依赖合成声明。

Method: 构建包含真实世界声明和时间序列证据的基准数据集，采用LLM辅助的多步骤标注流程，标注时间框架、判决和证据使用理由。

Result: 实现了kappa=0.745的标注者间一致性，基线模型在判决准确率上达到63.37%，证据到理由得分为48.63%，显示即使是先进推理模型在时间序列推理方面仍面临挑战。

Conclusion: TSVer为事实核查中的时序和数值推理提供了更可靠的评估基准，揭示了当前模型在处理时间序列证据方面的局限性。

Abstract: Reasoning over temporal and numerical data, such as time series, is a crucial
aspect of fact-checking. While many systems have recently been developed to
handle this form of evidence, their evaluation remains limited by existing
datasets, which often lack structured evidence, provide insufficient
justifications for verdicts, or rely on synthetic claims. In this paper, we
introduce TSVer, a new benchmark dataset for fact verification focusing on
temporal and numerical reasoning with time-series evidence. TSVer contains 287
real-world claims sourced from 38 fact-checking organizations and a curated
database of 400 time series covering diverse domains. Each claim is annotated
with time frames across all pertinent time series, along with a verdict and
justifications reflecting how the evidence is used to reach the verdict. Using
an LLM-assisted multi-step annotation process, we improve the quality of our
annotations and achieve an inter-annotator agreement of kappa=0.745 on
verdicts. We also develop a baseline for verifying claims against time-series
evidence and show that even the state-of-the-art reasoning models like
Gemini-2.5-Pro are challenged by time series, achieving a 63.37 accuracy score
on verdicts and an Ev2R score of 48.63 on verdict justifications.

</details>


### [49] [MicroRemed: Benchmarking LLMs in Microservices Remediation](https://arxiv.org/abs/2511.01166)
*Lingzhe Zhang,Yunpeng Zhai,Tong Jia,Chiming Duan,Minghua He,Leyi Pan,Zhaoyang Liu,Bolin Ding,Ying Li*

Main category: cs.CL

TL;DR: 提出了首个用于评估LLM在微服务修复中端到端性能的基准MicroRemed，以及模拟SRE推理的多智能体框架ThinkRemed，显著提升了修复性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖人工编写的提示，LLM仅将文本指令转换为可执行代码，需要推进LLM在微服务修复中的自主决策能力研究。

Method: 引入MicroRemed基准，要求模型直接从诊断报告生成可执行的Ansible playbooks；提出ThinkRemed多智能体框架，模拟SRE的反思和感知推理。

Result: MicroRemed对当前LLM构成重大挑战，而ThinkRemed通过迭代推理和系统反思显著提升了端到端修复性能。

Conclusion: MicroRemed为LLM在微服务修复领域提供了首个评估基准，ThinkRemed框架展示了多智能体推理在提升修复性能方面的有效性。

Abstract: Large Language Models (LLMs) integrated with agent-based reasoning frameworks
have recently shown strong potential for autonomous decision-making and
system-level operations. One promising yet underexplored direction is
microservice remediation, where the goal is to automatically recover faulty
microservice systems. Existing approaches, however, still rely on human-crafted
prompts from Site Reliability Engineers (SREs), with LLMs merely converting
textual instructions into executable code. To advance research in this area, we
introduce MicroRemed, the first benchmark for evaluating LLMs in end-to-end
microservice remediation, where models must directly generate executable
Ansible playbooks from diagnosis reports to restore system functionality. We
further propose ThinkRemed, a multi-agent framework that emulates the
reflective and perceptive reasoning of SREs. Experimental results show that
MicroRemed presents substantial challenges to current LLMs, while ThinkRemed
improves end-to-end remediation performance through iterative reasoning and
system reflection. The benchmark is available at
https://github.com/LLM4AIOps/MicroRemed.

</details>


### [50] [Learning When to Quit in Sales Conversations](https://arxiv.org/abs/2511.01181)
*Emaad Manzoor,Eva Ascarza,Oded Netzer*

Main category: cs.CL

TL;DR: 开发了一个基于语言模型的停止代理，用于优化销售对话中的动态筛选决策，能够减少54%的失败通话时间，同时保持几乎全部销售额。


<details>
  <summary>Details</summary>
Motivation: 销售人员在对话中面临何时坚持或放弃的决策，但缺乏对这些决策效率的了解和改进方法。

Method: 将动态筛选决策形式化为最优停止问题，开发基于生成语言模型的序列决策代理，通过模仿回顾性推断的最优停止策略来学习何时退出对话。

Result: 应用于欧洲电信公司的通话数据，停止代理减少失败通话时间54%，重新分配节省时间可使预期销售额增加37%。销售人员在决策中过度关注某些消费者不感兴趣的信号，错误预测通话失败风险。

Conclusion: 人工智能算法能够纠正认知受限的人类决策，提高销售团队效率。

Abstract: Salespeople frequently face the dynamic screening decision of whether to
persist in a conversation or abandon it to pursue the next lead. Yet, little is
known about how these decisions are made, whether they are efficient, or how to
improve them. We study these decisions in the context of high-volume outbound
sales where leads are ample, but time is scarce and failure is common. We
formalize the dynamic screening decision as an optimal stopping problem and
develop a generative language model-based sequential decision agent - a
stopping agent - that learns whether and when to quit conversations by
imitating a retrospectively-inferred optimal stopping policy. Our approach
handles high-dimensional textual states, scales to large language models, and
works with both open-source and proprietary language models. When applied to
calls from a large European telecommunications firm, our stopping agent reduces
the time spent on failed calls by 54% while preserving nearly all sales;
reallocating the time saved increases expected sales by up to 37%. Upon
examining the linguistic cues that drive salespeople's quitting decisions, we
find that they tend to overweight a few salient expressions of consumer
disinterest and mispredict call failure risk, suggesting cognitive bounds on
their ability to make real-time conversational decisions. Our findings
highlight the potential of artificial intelligence algorithms to correct
cognitively-bounded human decisions and improve salesforce efficiency.

</details>


### [51] [Surfacing Subtle Stereotypes: A Multilingual, Debate-Oriented Evaluation of Modern LLMs](https://arxiv.org/abs/2511.01187)
*Muhammed Saeed,Muhammad Abdul-mageed,Shady Shehata*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large language models (LLMs) are widely deployed for open-ended
communication, yet most bias evaluations still rely on English,
classification-style tasks. We introduce DebateBias-8K, a new multilingual,
debate-style benchmark designed to reveal how narrative bias appears in
realistic generative settings. Our dataset includes 8,400 structured debate
prompts spanning four sensitive domains: women's rights, socioeconomic
development, terrorism, and religion, across seven languages ranging from
high-resource (English, Chinese) to low-resource (Swahili, Nigerian Pidgin).
Using four flagship models (GPT-4o, Claude 3, DeepSeek, and LLaMA 3), we
generate and automatically classify over 100,000 responses. Results show that
all models reproduce entrenched stereotypes despite safety alignment: Arabs are
overwhelmingly linked to terrorism and religion (>=95%), Africans to
socioeconomic "backwardness" (up to <=77%), and Western groups are consistently
framed as modern or progressive. Biases grow sharply in lower-resource
languages, revealing that alignment trained primarily in English does not
generalize globally. Our findings highlight a persistent divide in multilingual
fairness: current alignment methods reduce explicit toxicity but fail to
prevent biased outputs in open-ended contexts. We release our DebateBias-8K
benchmark and analysis framework to support the next generation of multilingual
bias evaluation and safer, culturally inclusive model alignment.

</details>


### [52] [ZoFia: Zero-Shot Fake News Detection with Entity-Guided Retrieval and Multi-LLM Interaction](https://arxiv.org/abs/2511.01188)
*Lvhua Wu,Xuefeng Jiang,Sheng Sun,Tian Wen,Yuwei Wang,Min Liu*

Main category: cs.CL

TL;DR: ZoFia是一个新颖的两阶段零样本假新闻检测框架，通过分层显著性量化实体重要性，使用SC-MMR算法选择关键词检索最新外部证据，然后通过多LLM交互系统进行多视角协作分析和对抗性辩论，最终产生可解释的稳健判断。


<details>
  <summary>Details</summary>
Motivation: 假新闻的快速传播威胁社会稳定和公众信任，但现有方法面临时间限制的知识覆盖、幻觉内容生成以及缺乏对新兴新闻主题的泛化能力等问题。

Method: 两阶段方法：1）分层显著性量化实体重要性，SC-MMR算法选择多样化关键词用于检索最新外部证据；2）多LLM交互系统，各代理承担不同角色，进行多视角协作分析和对抗性辩论。

Result: 在两个公共数据集上的综合实验表明，ZoFia明显优于现有的零样本基线方法和大多数少样本方法。

Conclusion: ZoFia框架通过结合外部证据检索和多LLM协作分析，有效解决了假新闻检测中的知识覆盖和泛化问题，提供了可解释且稳健的检测方案。

Abstract: The rapid spread of fake news threatens social stability and public trust,
rendering its detection an imperative research priority. Although large
language models (LLMs) excel at numerous natural language processing tasks with
their remarkable contextual understanding and extensive prior knowledge, the
time-bounded knowledge coverage and tendency for generating hallucination
content reduce their reliability when handling fast-evolving news streams.
Furthermore, models trained on existing static datasets also often lack the
generalization needed for emerging news topics. To address these challenges, we
propose ZoFia, a novel two-stage zero-shot fake news detection framework.
First, we introduce Hierarchical Salience to quantify the importance of
entities in the news content, and propose the SC-MMR algorithm to effectively
select an informative and diverse set of keywords that serve as queries for
retrieving up-to-date external evidence. Subsequently, a multi LLM interactive
system, in which each agent assumes a distinct role, performs multi-view
collaborative analysis and adversarial debate over the news text and its
related information, and finally produces an interpretable and robust judgment.
Comprehensive experiments on two public datasets demonstrate that ZoFia
obviously outperforms existing zero-shot baselines and most of few-shot
methods. Our codes will be open-sourced to facilitate related communities.

</details>


### [53] [Self-Harmony: Learning to Harmonize Self-Supervision and Self-Play in Test-Time Reinforcement Learning](https://arxiv.org/abs/2511.01191)
*Ru Wang,Wei Huang,Qi Cao,Yusuke Iwasawa,Yutaka Matsuo,Jiaxian Guo*

Main category: cs.CL

TL;DR: Self-Harmony是一个无需标签的测试时强化学习框架，通过单个模型在求解器和重构器两个角色间切换，利用原始问题和其改写版本的答案稳定性来构建可靠的学习信号，采用调和均值聚合答案频率，避免多数投票的陷阱。


<details>
  <summary>Details</summary>
Motivation: 标准方法如多数投票容易陷入虚假但流行的答案陷阱，需要构建可靠的测试时学习信号来提升模型适应性。

Method: 使用单个模型同时作为求解器和重构器，基于原始问题及其改写版本的答案稳定性，采用调和均值聚合答案频率的伪标签方法。

Result: 在多样化推理基准测试中，Self-Harmony在无标签测试时设置下达到最先进结果，在30个设置中的28个排名第一，且在所有实验中零训练失败。

Conclusion: Self-Harmony框架展示了在无需人工监督或辅助模型情况下的卓越稳定性和可靠性，为测试时强化学习提供了有效解决方案。

Abstract: Test-time reinforcement learning (TTRL) offers a label-free paradigm for
adapting models using only synthetic signals at inference, but its success
hinges on constructing reliable learning signals. Standard approaches such as
majority voting often collapse to spurious yet popular answers. We introduce
Self-Harmony, a framework built on a simple intuition: the correct answer
should remain stable across both an original question and its paraphrase.
Self-Harmony operationalizes this by employing a single model in two
complementary roles: a Solver to produce answers and a Reframer to rephrase the
input. Based on this, we further propose a pseudo-label method: instead of
majority voting, it aggregates answer frequencies across these original and
reframed views using the harmonic mean. This is a process that naturally
selects for solutions stable under reframing, thereby avoiding the common trap
of favoring view-dependent, spurious answers. Crucially, this requires no human
supervision or auxiliary models. Across diverse reasoning benchmarks,
Self-Harmony achieves state-of-the-art results at the label-free test-time
setting, ranking first in 28 of 30 settings across multiple methods. Beyond
accuracy, it demonstrates unprecedented robustness, with zero training failures
in all experiments, underscoring its stability and reliability.

</details>


### [54] [DEER: Disentangled Mixture of Experts with Instance-Adaptive Routing for Generalizable Machine-Generated Text Detection](https://arxiv.org/abs/2511.01192)
*Guoxin Ma,Xiaoming Liu,Zhanhan Zhang,Chengzhengxu Li,Shengchao Liu,Yu Lan*

Main category: cs.CL

TL;DR: 提出了DEER框架，通过解耦专家混合架构和强化学习路由机制，有效检测机器生成文本，在领域偏移下显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前机器生成文本检测方法在领域偏移下性能显著下降，需要同时捕获领域特定和领域通用的检测模式。

Method: 两阶段解耦专家混合架构：领域特定专家学习细粒度领域内区分特征，共享专家提取跨领域可迁移特征；使用强化学习路由机制动态选择专家，解决推理时域标签不可用的问题。

Result: 在5个领域内和5个领域外基准数据集上，DEER优于现有方法，领域内F1提升1.39%，领域外F1提升5.32%；准确率分别提升1.35%和3.61%。

Conclusion: 解耦专家专业化和自适应路由对模型性能至关重要，DEER框架能有效应对机器生成文本检测中的领域偏移挑战。

Abstract: Detecting machine-generated text (MGT) has emerged as a critical challenge,
driven by the rapid advancement of large language models (LLMs) capable of
producing highly realistic, human-like content. However, the performance of
current approaches often degrades significantly under domain shift. To address
this challenge, we propose a novel framework designed to capture both
domain-specific and domain-general MGT patterns through a two-stage
Disentangled mixturE-of-ExpeRts (DEER) architecture. First, we introduce a
disentangled mixture-of-experts module, in which domain-specific experts learn
fine-grained, domain-local distinctions between human and machine-generated
text, while shared experts extract transferable, cross-domain features. Second,
to mitigate the practical limitation of unavailable domain labels during
inference, we design a reinforcement learning-based routing mechanism that
dynamically selects the appropriate experts for each input instance,
effectively bridging the train-inference gap caused by domain uncertainty.
Extensive experiments on five in-domain and five out-of-domain benchmark
datasets demonstrate that DEER consistently outperforms state-of-the-art
methods, achieving average F1-score improvements of 1.39% and 5.32% on
in-domain and out-of-domain datasets respectively, along with accuracy gains of
1.35% and 3.61% respectively. Ablation studies confirm the critical
contributions of both disentangled expert specialization and adaptive routing
to model performance.

</details>


### [55] [AraFinNews: Arabic Financial Summarisation with Domain-Adapted LLMs](https://arxiv.org/abs/2511.01265)
*Mo El-Haj,Paul Rayson*

Main category: cs.CL

TL;DR: 本文研究了领域特异性对阿拉伯语金融文本摘要的影响，通过构建最大的阿拉伯语金融新闻数据集AraFinNews，评估了领域适应预训练对摘要事实准确性、数值可靠性和风格对齐的改进效果。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏大规模的阿拉伯语金融文本摘要数据集，需要研究领域特异性对阿拉伯语金融摘要质量的影响，特别是事实准确性和数值可靠性方面。

Method: 构建了包含21.25万篇文章-标题对的AraFinNews数据集，评估了mT5、AraT5和领域适应的FinAraT5等transformer模型在金融摘要任务上的表现。

Result: 实验结果显示，经过领域适应的模型能够生成更忠实、连贯的摘要，特别是在处理定量信息和实体中心信息方面表现更好。

Conclusion: 领域特异性适应对于提高阿拉伯语金融摘要的事实一致性和叙事流畅性至关重要，FinAraT5等领域适应模型在金融摘要任务上表现优异。

Abstract: This paper investigates the impact of domain specificity on abstractive
summarisation of Arabic financial texts using large language models (LLMs). We
introduce AraFinNews, the largest publicly available Arabic financial news
dataset to date, comprising 212,500 article-headline pairs spanning nearly a
decade of reporting from October 2015 to July 2025. Designed as the Arabic
equivalent of major English summarisation corpora such as CNN/DailyMail,
AraFinNews provides a robust benchmark for evaluating domain-specific language
understanding and generation in financial contexts. Using this resource, we
evaluate transformer-based models -- including mT5, AraT5, and the
domain-adapted FinAraT5 -- to examine how financial-domain pretraining
influences factual accuracy, numerical reliability, and stylistic alignment
with professional reporting. Experimental results show that domain-adapted
models generate more faithful and coherent summaries, particularly in handling
quantitative and entity-centric information. The findings highlight the
importance of domain-specific adaptation for improving factual consistency and
narrative fluency in Arabic financial summarisation. The dataset is freely
available for non-commercial research at
https://github.com/ArabicNLP-UK/AraFinNews.

</details>


### [56] [When, What, and How: Rethinking Retrieval-Enhanced Speculative Decoding](https://arxiv.org/abs/2511.01282)
*Min Fang,Zhihui Fu,Qibin Zhao,Jun Wang*

Main category: cs.CL

TL;DR: ReSpec是一种新的检索增强推测解码框架，通过熵引导自适应触发、反馈驱动候选选择和源感知松弛验证策略，显著提升大语言模型推理速度，在保持输出质量的同时比现有方法快25-33%。


<details>
  <summary>Details</summary>
Motivation: 现有推测解码方法存在局限性：基于模型的方法（如EAGLE-2）准确但成本高，检索增强方法（如SAM-Decoding）依赖启发式切换策略，经常触发不必要的检索操作。

Method: 1) 熵引导自适应触发：量化上下文可预测性，仅在不确定性低时启动检索；2) 反馈驱动候选选择：利用历史反馈组织多个高质量候选进行并行验证；3) 源感知松弛验证策略：对模型生成草稿严格检查，对检索草稿使用松弛验证。

Result: 在Spec-Bench上的广泛实验表明，ReSpec实现了最先进的加速效果，分别比EAGLE-2和SAM-Decoding快33%和25%以上，同时保持输出质量。

Conclusion: ReSpec通过将启发式草稿切换转化为自适应决策，在准确性和效率之间取得了更好的平衡，为推测解码提供了更有效的解决方案。

Abstract: Speculative decoding (SD) has emerged as an effective technique to accelerate
large language model (LLM) inference without compromising output quality.
However, the achievable speedup largely depends on the effectiveness of the
drafting model. While model-based methods like EAGLE-2 are accurate but costly,
retrieval-enhanced methods like SAM-Decoding rely on heuristic switching
strategies that often trigger unnecessary retrievals. To address this, we
propose ReSpec (\textbf{Re}trieval-enhanced \textbf{Spe}culative Decoding), a
novel framework that transforms heuristic drafter switching into adaptive
decision-making. ReSpec features three core innovations: 1) An
\textbf{entropy-guided adaptive trigger} quantifies contextual predictability
to initiate retrieval only when uncertainty is low, avoiding costly low-quality
speculations. 2) A \textbf{feedback-driven candidate selection} leverages
historical feedback to organize multiple high-quality candidates for parallel
verification, maximizing retrieval utility. 3) A source-aware \textbf{relaxed
verification strategy} applies strict checks to model-generated drafts while
using a relaxed verification for retrieved drafts, achieving a better balance
between accuracy and efficiency. Extensive experiments on Spec-Bench
demonstrate that ReSpec achieves state-of-the-art acceleration,outperforming
EAGLE-2 and SAM-Decoding by over $33\%$ and $25\%$, respectively, while
maintaining output quality.

</details>


### [57] ["Give a Positive Review Only": An Early Investigation Into In-Paper Prompt Injection Attacks and Defenses for AI Reviewers](https://arxiv.org/abs/2511.01287)
*Qin Zhou,Zhexin Zhang,Zhi Li,Limin Sun*

Main category: cs.CL

TL;DR: 本文系统研究了AI辅助同行评审中的提示注入威胁，提出了静态和迭代两种攻击方法，并探讨了检测防御机制。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型在科学论文评审中的广泛应用，出现了隐藏的注入提示操纵AI评审者给出过高评价的威胁，需要对此进行系统性调查。

Method: 提出两类攻击：静态攻击使用固定注入提示，迭代攻击通过模拟评审模型优化注入提示以最大化效果；同时探索基于检测的防御方法。

Result: 两种攻击都能显著提高评审分数，经常诱导前沿AI评审者给出满分评价；检测防御能大幅降低攻击成功率，但自适应攻击者仍能部分规避防御。

Conclusion: 研究强调了在AI辅助同行评审中需要更多关注和严格防护措施来应对提示注入威胁。

Abstract: With the rapid advancement of AI models, their deployment across diverse
tasks has become increasingly widespread. A notable emerging application is
leveraging AI models to assist in reviewing scientific papers. However, recent
reports have revealed that some papers contain hidden, injected prompts
designed to manipulate AI reviewers into providing overly favorable
evaluations. In this work, we present an early systematic investigation into
this emerging threat. We propose two classes of attacks: (1) static attack,
which employs a fixed injection prompt, and (2) iterative attack, which
optimizes the injection prompt against a simulated reviewer model to maximize
its effectiveness. Both attacks achieve striking performance, frequently
inducing full evaluation scores when targeting frontier AI reviewers.
Furthermore, we show that these attacks are robust across various settings. To
counter this threat, we explore a simple detection-based defense. While it
substantially reduces the attack success rate, we demonstrate that an adaptive
attacker can partially circumvent this defense. Our findings underscore the
need for greater attention and rigorous safeguards against prompt-injection
threats in AI-assisted peer review.

</details>


### [58] [FirstAidQA: A Synthetic Dataset for First Aid and Emergency Response in Low-Connectivity Settings](https://arxiv.org/abs/2511.01289)
*Saiyma Sittul Muna,Rezwan Islam Salvi,Mushfiqur Rahman Mushfique,Ajwad Abrar*

Main category: cs.CL

TL;DR: 提出了FirstAidQA数据集，包含5500个高质量急救问答对，用于在低连接环境下开发轻量级语言模型，支持急救响应。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型计算密集，不适合急救人员在低连接环境使用的低端设备，缺乏高质量的急救领域数据集阻碍了轻量级解决方案的开发。

Method: 使用ChatGPT-4o-mini基于Vital First Aid Book (2019)生成问答对，通过文本清理、上下文分块、过滤等预处理步骤，并进行人工验证确保准确性和安全性。

Result: 创建了包含5500个高质量急救问答对的FirstAidQA数据集，涵盖广泛的急救场景，支持LLM和SLM的指令调优和微调。

Conclusion: FirstAidQA数据集填补了急救领域高质量数据集的空白，支持开发更快、更可靠、支持离线的急救系统，推动安全关键和资源受限AI应用的研究。

Abstract: In emergency situations, every second counts. The deployment of Large
Language Models (LLMs) in time-sensitive, low or zero-connectivity environments
remains limited. Current models are computationally intensive and unsuitable
for low-tier devices often used by first responders or civilians. A major
barrier to developing lightweight, domain-specific solutions is the lack of
high-quality datasets tailored to first aid and emergency response. To address
this gap, we introduce FirstAidQA, a synthetic dataset containing 5,500
high-quality question answer pairs that encompass a wide range of first aid and
emergency response scenarios. The dataset was generated using a Large Language
Model, ChatGPT-4o-mini, with prompt-based in-context learning, using texts from
the Vital First Aid Book (2019). We applied preprocessing steps such as text
cleaning, contextual chunking, and filtering, followed by human validation to
ensure accuracy, safety, and practical relevance of the QA pairs. FirstAidQA is
designed to support instruction-tuning and fine-tuning of LLMs and Small
Language Models (SLMs), enabling faster, more reliable, and offline-capable
systems for emergency settings. We publicly release the dataset to advance
research on safety-critical and resource-constrained AI applications in first
aid and emergency response. The dataset is available on Hugging Face at
https://huggingface.co/datasets/i-am-mushfiq/FirstAidQA.

</details>


### [59] [DeepSpecs: Expert-Level Questions Answering in 5G](https://arxiv.org/abs/2511.01305)
*Aman Ganapathy Manvattira,Yifei Xu,Ziyue Dang,Songwu Lu*

Main category: cs.CL

TL;DR: DeepSpecs是一个增强的RAG系统，通过结构化和时序推理来解决5G标准中的交叉引用和规范演化问题，显著提升了专家级问题的回答质量。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG框架依赖语义相似性，无法可靠解决5G标准中的交叉引用或推理规范演化，而5G标准包含数千页相互引用的规范且不断更新。

Method: 构建三个元数据丰富的数据库：SpecDB（条款对齐的规范文本）、ChangeDB（行级版本差异）和TDocDB（标准化会议文档），通过元数据查找递归检索引用条款，并挖掘变更以追踪规范演化。

Result: 在多个LLM后端上，DeepSpecs优于基础模型和最先进的电信RAG系统；消融实验证实显式交叉引用解析和演化感知检索显著提高了答案质量。

Conclusion: 建模5G标准的结构和时序特性对于可靠回答专家级问题具有重要价值，显式交叉引用解析和演化感知检索是提升系统性能的关键因素。

Abstract: 5G technology enables mobile Internet access for billions of users. Answering
expert-level questions about 5G specifications requires navigating thousands of
pages of cross-referenced standards that evolve across releases. Existing
retrieval-augmented generation (RAG) frameworks, including telecom-specific
approaches, rely on semantic similarity and cannot reliably resolve
cross-references or reason about specification evolution. We present DeepSpecs,
a RAG system enhanced by structural and temporal reasoning via three
metadata-rich databases: SpecDB (clause-aligned specification text), ChangeDB
(line-level version diffs), and TDocDB (standardization meeting documents).
DeepSpecs explicitly resolves cross-references by recursively retrieving
referenced clauses through metadata lookup, and traces specification evolution
by mining changes and linking them to Change Requests that document design
rationale. We curate two 5G QA datasets: 573 expert-annotated real-world
questions from practitioner forums and educational resources, and 350
evolution-focused questions derived from approved Change Requests. Across
multiple LLM backends, DeepSpecs outperforms base models and state-of-the-art
telecom RAG systems; ablations confirm that explicit cross-reference resolution
and evolution-aware retrieval substantially improve answer quality,
underscoring the value of modeling the structural and temporal properties of 5G
standards.

</details>


### [60] [DEEPAMBIGQA: Ambiguous Multi-hop Questions for Benchmarking LLM Answer Completeness](https://arxiv.org/abs/2511.01323)
*Jiabao Ji,Min Li,Priyanshu Kumar,Shiyu Chang,Saloni Potdar*

Main category: cs.CL

TL;DR: 提出了DeepAmbigQAGen自动数据生成管道和DeepAmbigQA数据集，用于评估LLM在包含名称歧义和多步推理的复杂问答任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有问答基准很少同时评估名称歧义解决和多步推理这两个挑战，而现实中的复杂问题往往需要这两项能力。

Method: 开发了DeepAmbigQAGen自动数据生成管道，基于文本语料库和链接知识图谱构建问答任务，生成包含名称歧义和多步推理的自然可验证问题。

Result: 构建了包含3,600个问题的DeepAmbigQA数据集，其中一半需要显式解决名称歧义。实验显示即使是GPT-5在歧义问题上精确匹配率仅为0.13，非歧义问题为0.21。

Conclusion: 现有问答系统在信息收集和答案完整性方面存在不足，需要开发更鲁棒的问答系统。

Abstract: Large language models (LLMs) with integrated search tools show strong promise
in open-domain question answering (QA), yet they often struggle to produce
complete answer set to complex questions such as Which actor from the film Heat
won at least one Academy Award?, which requires (1) distinguishing between
multiple films sharing the same title and (2) reasoning across a large set of
actors to gather and integrate evidence. Existing QA benchmarks rarely evaluate
both challenges jointly. To address this, we introduce DeepAmbigQAGen, an
automatic data generation pipeline that constructs QA tasks grounded in text
corpora and linked knowledge graph, generating natural and verifiable questions
that systematically embed name ambiguity and multi-step reasoning. Based on
this, we build DeepAmbigQA, a dataset of 3,600 questions requiring multi-hop
reasoning and half of them explicit name ambiguity resolving. Experiments
reveal that, even state-of-the-art GPT-5 show incomplete answers, achieving
only 0.13 exact match on ambiguous questions and 0.21 on non-ambiguous
questions. These findings highlight the need for more robust QA systems aimed
at information gathering and answer completeness.

</details>


### [61] [Thinking with DistilQwen: A Tale of Four Distilled Reasoning and Reward Model Series](https://arxiv.org/abs/2511.01354)
*Wenrui Cai,Chengyu Wang,Junbing Yan,Jun Huang,Xiangzhong Fang*

Main category: cs.CL

TL;DR: 本文扩展了DistilQwen模型家族，针对工业需求开发了四个模型系列：慢思考模型、自适应思考模型和蒸馏奖励模型，在推理性能和效率间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 为满足现实应用对小型高效推理模型的需求，开发能平衡推理性能和推理速度的知识蒸馏技术。

Method: 基于Qwen模型初始化，开发四种模型系列：慢思考模型（高精度推理）、自适应思考模型（动态调整推理策略）、蒸馏奖励模型（支持强化学习）。

Result: 在多个基准测试中展现出高推理效率和强推理性能，蒸馏奖励模型具有实际应用价值。

Conclusion: 这些模型为行业从业者提供了可扩展的训练和推理功能，支持在阿里云PAI平台上部署应用。

Abstract: Recently, the demand for small and efficient reasoning models to support
real-world applications has driven the development of knowledge distillation
techniques that balance reasoning performance and inference speed. In this
paper, we further extend the DistilQwen model family, initialized from the Qwen
models, by introducing four model series specifically designed to meet
industrial requirements. The distilled model collection comprises: (1)
slow-thinking models, optimized for reasoning tasks that require high accuracy;
(2) two series of adaptive-thinking models, which dynamically adjust reasoning
strategies based on input tasks to maximize efficiency across diverse
scenarios; and (3) distilled reward models, which enable further reinforcement
learning of reasoning models using distilled knowledge. Comprehensive
evaluations across multiple benchmarks demonstrate both high inference
efficiency and strong reasoning performance for these models, as well as the
practical utility of distilled reward models. We further show that these models
support industry practitioners by providing scalable training and inference
functionalities on the Alibaba Cloud PAI (Platform for Artificial Intelligence)
platform.

</details>


### [62] [PrefixNLI: Detecting Factual Inconsistencies as Soon as They Arise](https://arxiv.org/abs/2511.01359)
*Sapir Harary,Eran Hirsch,Aviv Slobodkin,David Wan,Mohit Bansal,Ido Dagan*

Main category: cs.CL

TL;DR: 提出了MiniTruePrefixes模型，专门用于检测文本前缀中的事实不一致性，在受控解码框架中显著提高了抽象摘要的事实一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的NLI模型用于判断完整句子的蕴含关系，但自回归生成架构在解码过程中需要对每个文本前缀做出决策，因此需要专门处理前缀级蕴含检测任务。

Method: 将蕴含检测任务推广到任意文本前缀，训练专门的MiniTruePrefixes模型，并将其集成到受控解码框架中。

Result: MiniTruePrefixes在前缀级蕴含检测上比基线NLI模型高出5-14 F1分数，集成后LLaMA-3.2-3B-Instruct在忠实度和运行时间上与8B模型相当，但内存使用减半。

Conclusion: 专门的前缀级蕴含检测模型能有效提高生成文本的事实一致性，在保持性能的同时显著减少计算资源需求。

Abstract: Natural Language Inference (NLI) models have been used in various ways to
improve the factuality of LLM outputs. This is typically done by applying an
NLI model to judge whether the model output is entailed from the supposed
evidence, triggering some corrective actions, such as beam reranking at
inference time or RL rewards during training. While NLI models are trained to
detect factual inconsistencies over complete sentences, decisions in the common
autoregressive generation architecture are made for each evolving text prefix,
during decoding. Addressing this setting, we generalize the entailment
detection task to apply over arbitrary text prefixes, and suggest its utility
for improving generation faithfulness. Providing suitable evaluation and
training datasets for this task, we train MiniTruePrefixes, a novel specialized
model that better detects factual inconsistencies over text prefixes,
outperforming comparable baseline NLI models by 5-14 F1 points in prefix-level
entailment. We further demonstrate that integrating MiniTruePrefixes into a
controlled decoding framework substantially improves factual consistency in
abstractive summarization. When guided by MiniTruePrefixes,
LLaMA-3.2-3B-Instruct matches the faithfulness and runtime of the 8B model from
the same model family, while using only half the memory.

</details>


### [63] [Safer in Translation? Presupposition Robustness in Indic Languages](https://arxiv.org/abs/2511.01360)
*Aadi Palnitkar,Arjun Suresh,Rishi Rajesh,Puneet Puli*

Main category: cs.CL

TL;DR: 开发了Cancer-Myth-Indic基准，将500个癌症相关错误预设问题翻译成5种印度语言，用于评估多语言LLM在医疗咨询中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有医疗基准几乎都是英文的，缺乏对多语言LLM在医疗咨询中准确性的评估，特别是在印度次大陆广泛使用的语言中。

Method: 从Cancer-Myth基准中均匀抽样500个项目，由母语译者按照风格指南翻译成5种印度语言，保留隐含预设，共生成2500个翻译项目。

Result: 在预设压力下评估了多个流行LLM的表现，揭示了多语言LLM在医疗咨询中的局限性。

Conclusion: Cancer-Myth-Indic基准填补了多语言LLM医疗评估的空白，为改进多语言医疗AI系统提供了重要工具。

Abstract: Increasingly, more and more people are turning to large language models
(LLMs) for healthcare advice and consultation, making it important to gauge the
efficacy and accuracy of the responses of LLMs to such queries. While there are
pre-existing medical benchmarks literature which seeks to accomplish this very
task, these benchmarks are almost universally in English, which has led to a
notable gap in existing literature pertaining to multilingual LLM evaluation.
Within this work, we seek to aid in addressing this gap with Cancer-Myth-Indic,
an Indic language benchmark built by translating a 500-item subset of
Cancer-Myth, sampled evenly across its original categories, into five
under-served but widely used languages from the subcontinent (500 per language;
2,500 translated items total). Native-speaker translators followed a style
guide for preserving implicit presuppositions in translation; items feature
false presuppositions relating to cancer. We evaluate several popular LLMs
under this presupposition stress.

</details>


### [64] [The Ouroboros of Benchmarking: Reasoning Evaluation in an Era of Saturation](https://arxiv.org/abs/2511.01365)
*İbrahim Ethem Deveci,Duygu Ataman*

Main category: cs.CL

TL;DR: 论文质疑当前大语言模型基准测试的有效性，分析三大模型家族在推理基准上的表现趋势，指出基准饱和问题并提出推理评估面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，基准测试结果趋于饱和，需要探讨超越基准是否真正体现推理能力，还是仅仅在追踪与声称能力脱节的数字。

Method: 调查OpenAI、Anthropic和Google三大模型家族在不同基准测试中的推理能力演变，分析多年来的性能趋势和不同推理任务的表现。

Result: 发现基准测试存在饱和现象，模型性能提升可能源于数据泄露而非真正的推理能力进步，揭示了当前基准测试的局限性。

Conclusion: 需要重新思考推理评估方法，本文为未来推理评估和模型开发提供了基础参考，指出了基准测试面临的持续挑战。

Abstract: The rapid rise of Large Language Models (LLMs) and Large Reasoning Models
(LRMs) has been accompanied by an equally rapid increase of benchmarks used to
assess them. However, due to both improved model competence resulting from
scaling and novel training advances as well as likely many of these datasets
being included in pre or post training data, results become saturated, driving
a continuous need for new and more challenging replacements. In this paper, we
discuss whether surpassing a benchmark truly demonstrates reasoning ability or
are we simply tracking numbers divorced from the capabilities we claim to
measure? We present an investigation focused on three model families, OpenAI,
Anthropic, and Google, and how their reasoning capabilities across different
benchmarks evolve over the years. We also analyze performance trends over the
years across different reasoning tasks and discuss the current situation of
benchmarking and remaining challenges. By offering a comprehensive overview of
benchmarks and reasoning tasks, our work aims to serve as a first reference to
ground future research in reasoning evaluation and model development.

</details>


### [65] [Confounding Factors in Relating Model Performance to Morphology](https://arxiv.org/abs/2511.01380)
*Wessel Poelman,Thomas Bauwens,Miryam de Lhoneux*

Main category: cs.CL

TL;DR: 本文重新评估了语言形态特征对分词和语言建模的影响，发现先前研究存在混淆因素，提出了token二元组指标作为预测语言建模难度的内在方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究对形态特征在语言建模中的重要性存在矛盾结论，作者认为这是由于实验设置中的混淆因素导致的，难以比较结果和得出结论。

Method: 识别了分析形态学与语言建模关系的混淆因素，重新评估了Arnett & Bergen (2025)的三个假设，并引入了token二元组指标作为预测语言建模难度的内在方法。

Result: 发现每个关于黏着语和融合语建模困惑度的结论都包含混淆因素，token二元组指标是形态复杂性的梯度代理，无需专家标注。

Conclusion: 最终为可靠回答形态学与语言建模的关系问题提出了必要条件和框架。

Abstract: The extent to which individual language characteristics influence
tokenization and language modeling is an open question. Differences in
morphological systems have been suggested as both unimportant and crucial to
consider (Cotterell et al., 2018; Gerz et al., 2018a; Park et al., 2021, inter
alia). We argue this conflicting evidence is due to confounding factors in
experimental setups, making it hard to compare results and draw conclusions. We
identify confounding factors in analyses trying to answer the question of
whether, and how, morphology relates to language modeling. Next, we re-assess
three hypotheses by Arnett & Bergen (2025) for why modeling agglutinative
languages results in higher perplexities than fusional languages: they look at
morphological alignment of tokenization, tokenization efficiency, and dataset
size. We show that each conclusion includes confounding factors. Finally, we
introduce token bigram metrics as an intrinsic way to predict the difficulty of
causal language modeling, and find that they are gradient proxies for
morphological complexity that do not require expert annotation. Ultimately, we
outline necessities to reliably answer whether, and how, morphology relates to
language modeling.

</details>


### [66] [RAGSmith: A Framework for Finding the Optimal Composition of Retrieval-Augmented Generation Methods Across Datasets](https://arxiv.org/abs/2511.01386)
*Muhammed Yusuf Kartal,Suha Kagan Kose,Korhan Sevinç,Burak Aktas*

Main category: cs.CL

TL;DR: RAGSmith是一个模块化框架，将RAG设计视为端到端架构搜索，通过遗传算法优化检索和生成指标，在六个维基百科领域上平均提升3.8%性能。


<details>
  <summary>Details</summary>
Motivation: RAG质量依赖于检索、排序、增强、提示和生成等多个相互作用的模块选择，孤立优化模块很脆弱，需要端到端的优化方法。

Method: 引入RAGSmith框架，在9个技术家族和46,080个可行流水线配置上进行端到端架构搜索，使用遗传算法优化检索指标（召回率、mAP、nDCG、MRR）和生成指标（LLM-Judge和语义相似度）的联合标量目标。

Result: 在六个维基百科领域（数学、法律、金融、医学、国防工业、计算机科学）上，RAGSmith找到的配置平均比朴素RAG基线提升3.8%（范围1.2%到6.9%），检索提升高达12.5%，生成提升7.5%。搜索仅探索约0.2%的空间（约100个候选）。

Conclusion: 发现了稳健的骨干架构——向量检索加后生成反思/修订，辅以领域相关的扩展、重排序、增强和提示重排序选择；改进幅度与问题类型相关，为构建有效RAG系统提供了实用的领域感知指导。

Abstract: Retrieval-Augmented Generation (RAG) quality depends on many interacting
choices across retrieval, ranking, augmentation, prompting, and generation, so
optimizing modules in isolation is brittle. We introduce RAGSmith, a modular
framework that treats RAG design as an end-to-end architecture search over nine
technique families and 46{,}080 feasible pipeline configurations. A genetic
search optimizes a scalar objective that jointly aggregates retrieval metrics
(recall@k, mAP, nDCG, MRR) and generation metrics (LLM-Judge and semantic
similarity). We evaluate on six Wikipedia-derived domains (Mathematics, Law,
Finance, Medicine, Defense Industry, Computer Science), each with 100 questions
spanning factual, interpretation, and long-answer types. RAGSmith finds
configurations that consistently outperform naive RAG baseline by +3.8\% on
average (range +1.2\% to +6.9\% across domains), with gains up to +12.5\% in
retrieval and +7.5\% in generation. The search typically explores $\approx
0.2\%$ of the space ($\sim 100$ candidates) and discovers a robust backbone --
vector retrieval plus post-generation reflection/revision -- augmented by
domain-dependent choices in expansion, reranking, augmentation, and prompt
reordering; passage compression is never selected. Improvement magnitude
correlates with question type, with larger gains on factual/long-answer mixes
than interpretation-heavy sets. These results provide practical, domain-aware
guidance for assembling effective RAG systems and demonstrate the utility of
evolutionary search for full-pipeline optimization.

</details>


### [67] [LiveSearchBench: An Automatically Constructed Benchmark for Retrieval and Reasoning over Dynamic Knowledge](https://arxiv.org/abs/2511.01409)
*Heng Zhou,Ao Yu,Yuchen Fan,Jianing Shi,Li Kang,Hejia Geng,Yongting Zhang,Yutao Fan,Yuhao Wu,Tiancheng He,Yiran Qin,Lei Bai,Zhenfei Yin*

Main category: cs.CL

TL;DR: LiveSearchBench是一个自动构建检索依赖基准的流水线，通过分析Wikidata快照变化生成需要最新知识的问题，评估LLM在动态知识环境下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估基准主要依赖静态数据集，奖励记忆而非检索能力，无法反映世界知识的动态变化特性。

Method: 计算Wikidata连续快照间的差异，筛选高质量三元组，生成三个推理难度级别的自然语言问题，并通过SPARQL验证确保答案唯一性。

Result: 实验显示模型在面对预训练后出现的事实时性能显著下降，多跳查询差距最明显。检索增强方法和更大指令调优模型只能部分缓解但无法消除这种时效性差距。

Conclusion: LiveSearchBench将评估从静态记忆转向需要最新检索和推理的任务，为系统化长期评估LLM在演化知识下的表现提供了基础。

Abstract: Evaluating large language models (LLMs) on question answering often relies on
static benchmarks that reward memorization and understate the role of
retrieval, failing to capture the dynamic nature of world knowledge. We present
LiveSearchBench, an automated pipeline for constructing retrieval-dependent
benchmarks from recent knowledge updates. Our method computes deltas between
successive Wikidata snapshots, filters candidate triples for quality, and
synthesizes natural-language questions at three levels of reasoning difficulty,
each guaranteed to admit a unique, verifiable answer through SPARQL validation.
The pipeline is fully automated, scalable across time, and minimizes human
intervention, enabling continual regeneration of temporally grounded
benchmarks. Experiments show a pronounced performance drop when models confront
facts that post-date pretraining, with the gap most salient on multi-hop
queries. Retrieval augmented methods and larger, instruction-tuned models
provide partial gains but fail to close this recency gap. By design,
LiveSearchBench shifts evaluation from static memorization toward tasks that
require up-to-date retrieval and reasoning, offering a foundation for
systematic, long-term assessment of LLMs under evolving knowledge.

</details>


### [68] ["Don't Teach Minerva": Guiding LLMs Through Complex Syntax for Faithful Latin Translation with RAG](https://arxiv.org/abs/2511.01454)
*Sergio Torres Aguilar*

Main category: cs.CL

TL;DR: 提出了一种可复现的基于草稿优化的翻译管道，将开源大语言模型提升到与顶级专有系统相当的性能水平，特别针对形态丰富、资源匮乏的语言如拉丁语。


<details>
  <summary>Details</summary>
Motivation: 翻译形态丰富、资源匮乏的语言（如拉丁语）面临重大挑战，需要开发能够与专有系统性能相媲美的开源解决方案。

Method: 使用微调的NLLB-1.3B模型生成高质量、结构忠实的草稿，然后通过零样本LLM（Llama-3.3或Qwen3）进行优化，可通过检索外部上下文示例（RAG）进一步增强。

Result: 该方法在两个不同基准测试中表现出鲁棒性：标准领域内测试集和新的具有挑战性的领域外12世纪拉丁语信件集。开源RAG系统实现了与GPT-5基线统计相当的性能。

Conclusion: 无需特定任务的LLM微调，开源RAG系统就能达到与专有系统相当的翻译性能，为形态丰富、低资源语言的翻译提供了有效的开源解决方案。

Abstract: Translating a morphology-rich, low-resource language like Latin poses
significant challenges. This paper introduces a reproducible draft-based
refinement pipeline that elevates open-source Large Language Models (LLMs) to a
performance level statistically comparable to top-tier proprietary systems. Our
method first uses a fine-tuned NLLB-1.3B model to generate a high-quality,
structurally faithful draft. A zero-shot LLM (Llama-3.3 or Qwen3) then polishes
this draft, a process that can be further enhanced by augmenting the context
with retrieved out-context examples (RAG). We demonstrate the robustness of
this approach on two distinct benchmarks: a standard in-domain test set
(Rosenthal, 2023) and a new, challenging out-of-domain (OOD) set of
12th-century Latin letters (2025). Our central finding is that this open-source
RAG system achieves performance statistically comparable to the GPT-5 baseline,
without any task-specific LLM fine-tuning. We release the pipeline, the
Chartres OOD set, and evaluation scripts and models to facilitate replicability
and further research.

</details>


### [69] [BARD: budget-aware reasoning distillation](https://arxiv.org/abs/2511.01470)
*Lujie Niu,Lei Shen,Yi Jiang,Caixia Yuan,Xiaojie Wang,Wenbo Su,Bo zheng*

Main category: cs.CL

TL;DR: 提出BARD框架，通过预算感知的推理蒸馏方法，在向小模型传递推理能力的同时实现对推理长度的细粒度控制，平衡推理性能和计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决长思维链蒸馏中推理过程冗余、计算预算不可控导致的资源使用效率低下问题。

Method: 采用两阶段训练：第一阶段在教师生成的长思维链数据上进行监督微调，第二阶段使用强化学习同时优化推理性能和预算保真度。

Result: 在AIME24、AIME25、GPQA等挑战性推理基准上，8B学生模型实现了强大性能，并在广泛预算范围内提供精确自适应的推理长度控制。

Conclusion: BARD框架成功实现了推理能力蒸馏与计算效率控制的平衡，为资源受限环境下的高效推理提供了有效解决方案。

Abstract: While long Chain-of-Thought (CoT) distillation effectively transfers
reasoning capability to smaller language models, the reasoning process often
remains redundant and computational budget uncontrollable, leading to
inefficient resource usage. To address this limitation, we propose
\textbf{Budget-Aware Reasoning Distillation (BARD)}, a novel framework that
simultaneously distills reasoning capability and enables fine-grained control
over the reasoning length. BARD uses the thinking budget as a user-specified
control signal, allowing the model to dynamically balance reasoning performance
and computational efficiency. To achieve this concept, BARD introduces a
two-phase training regimen. The first phase, Supervised Fine-Tuning (SFT) on
teacher-generated long CoT data compressed to various budget levels,
bootstrapping the model's understanding of budget constraints. The second phase
leverages Reinforcement Learning (RL) from a reward signal in consideration of
reasoning performance and budget fidelity simultaneously. Incorporating the
two-phase regimen is crucial to avoiding policy degradation and ensuring that
both objectives are optimized jointly. Extensive experiments demonstrate that
our method empowers an 8B student model to achieve strong performance on
challenging reasoning benchmarks (\textit{AIME24, AIME25, GPQA}) while
providing precise and adaptive control over its reasoning length across a wide
range of budgets.

</details>


### [70] [Towards Consistent Detection of Cognitive Distortions: LLM-Based Annotation and Dataset-Agnostic Evaluation](https://arxiv.org/abs/2511.01482)
*Neha Sharma,Navneet Agarwal,Kairit Sirts*

Main category: cs.CL

TL;DR: 使用大语言模型作为认知扭曲检测的标注工具，通过多轮独立标注获得稳定标签，并提出了基于Cohen's kappa的数据集无关评估框架。


<details>
  <summary>Details</summary>
Motivation: 解决文本认知扭曲检测中人类标注者主观性强、一致性低的问题，探索LLMs作为可靠标注器的可行性。

Method: 使用多个独立的LLM运行来揭示稳定的标注模式，并引入基于Cohen's kappa的数据集无关评估框架进行公平比较。

Result: GPT-4能够产生一致的标注（Fleiss's Kappa = 0.78），使用这些标注训练的模型在测试集上表现优于使用人类标注数据训练的模型。

Conclusion: LLMs可以为主观NLP任务提供可扩展且内部一致的训练数据生成方案，支持强大的下游性能。

Abstract: Text-based automated Cognitive Distortion detection is a challenging task due
to its subjective nature, with low agreement scores observed even among expert
human annotators, leading to unreliable annotations. We explore the use of
Large Language Models (LLMs) as consistent and reliable annotators, and propose
that multiple independent LLM runs can reveal stable labeling patterns despite
the inherent subjectivity of the task. Furthermore, to fairly compare models
trained on datasets with different characteristics, we introduce a
dataset-agnostic evaluation framework using Cohen's kappa as an effect size
measure. This methodology allows for fair cross-dataset and cross-study
comparisons where traditional metrics like F1 score fall short. Our results
show that GPT-4 can produce consistent annotations (Fleiss's Kappa = 0.78),
resulting in improved test set performance for models trained on these
annotations compared to those trained on human-labeled data. Our findings
suggest that LLMs can offer a scalable and internally consistent alternative
for generating training data that supports strong downstream performance in
subjective NLP tasks.

</details>


### [71] [Synthetic Eggs in Many Baskets: The Impact of Synthetic Data Diversity on LLM Fine-Tuning](https://arxiv.org/abs/2511.01490)
*Max Schaffelder,Albert Gatt*

Main category: cs.CL

TL;DR: 研究合成数据来源多样性对微调大语言模型的影响，发现多源合成数据能缓解分布坍缩、保持输出质量，但可能增加风险。


<details>
  <summary>Details</summary>
Motivation: 随着合成数据在语言模型开发中的广泛应用，理解其对模型行为的影响至关重要。

Method: 研究合成数据来源多样性对微调大语言模型的影响，重点关注分布坍缩、对抗鲁棒性和自偏好偏差三个维度。

Result: 多源合成数据微调能缓解分布坍缩，保持输出分布广度；合成数据微调在移除安全防护时能保持更高输出质量；微调能减少自偏好偏差，人类数据效果最佳。

Conclusion: 合成数据来源多样性对模型行为有重要影响，多源合成数据在缓解分布坍缩方面表现良好，但需注意其潜在风险。

Abstract: As synthetic data becomes widely used in language model development,
understanding its impact on model behavior is crucial. This paper investigates
the impact of the diversity of sources of synthetic data on fine-tuned large
language models. We focus on three key dimensions: distribution collapse,
adversarial robustness, and self-preference bias. Our findings reveal that
fine-tuning models on synthetic data from diverse sources can mitigate
distribution collapse, preserving the breadth of the output distribution and
the diversity of the output text. Furthermore, while both human and synthetic
fine-tuning data can remove safeguards, the latter preserves higher output
quality, thus making outputs potentially more usable and dangerous. Finally,
fine-tuning reduces self-preference bias, with human data being the most
effective, followed by multi-source synthetic data.

</details>


### [72] [BanglaNirTox: A Large-scale Parallel Corpus for Explainable AI in Bengali Text Detoxification](https://arxiv.org/abs/2511.01512)
*Ayesha Afroza Mohsin,Mashrur Ahsan,Nafisa Maliyat,Shanta Maria,Syed Rifat Raiyan,Hasan Mahmud,Md Kamrul Hasan*

Main category: cs.CL

TL;DR: 提出了一种结合帕累托优化大语言模型和思维链提示的孟加拉语文本去毒新方法，并构建了包含68,041个句子的BanglaNirTox数据集


<details>
  <summary>Details</summary>
Motivation: 孟加拉语中的有害语言在在线环境中普遍存在，但缺乏有效的预防措施，且由于资源有限，孟加拉语文本去毒研究相对不足

Method: 使用帕累托优化的大语言模型结合思维链提示生成去毒句子，构建包含毒性标签、推理和去毒改写的人工生成平行语料库

Result: 帕累托优化大语言模型与思维链提示显著提高了孟加拉语文本去毒的质量和一致性

Conclusion: 该方法为孟加拉语文本去毒提供了有效的解决方案，构建的数据集可用于微调语言模型以生成更好的去毒版本

Abstract: Toxic language in Bengali remains prevalent, especially in online
environments, with few effective precautions against it. Although text
detoxification has seen progress in high-resource languages, Bengali remains
underexplored due to limited resources. In this paper, we propose a novel
pipeline for Bengali text detoxification that combines Pareto class-optimized
large language models (LLMs) and Chain-of-Thought (CoT) prompting to generate
detoxified sentences. To support this effort, we construct BanglaNirTox, an
artificially generated parallel corpus of 68,041 toxic Bengali sentences with
class-wise toxicity labels, reasonings, and detoxified paraphrases, using
Pareto-optimized LLMs evaluated on random samples. The resulting BanglaNirTox
dataset is used to fine-tune language models to produce better detoxified
versions of Bengali sentences. Our findings show that Pareto-optimized LLMs
with CoT prompting significantly enhance the quality and consistency of Bengali
text detoxification.

</details>


### [73] [Difficulty-Controllable Cloze Question Distractor Generation](https://arxiv.org/abs/2511.01526)
*Seokhoon Kang,Yejin Jeon,Seonjeong Hwang,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: 提出了一种通过数据增强和多任务学习生成可控难度干扰项的新框架，显著优于GPT-4o在难度控制方面的表现


<details>
  <summary>Details</summary>
Motivation: 现有干扰项生成方法缺乏适应性和难度控制能力，且缺乏难度标注数据集阻碍了进展

Method: 采用两阶段方法：1) 通过双向干扰项生成创建难度标注数据集；2) 利用多任务学习训练难度可控生成模型

Result: 实验结果显示该方法能生成跨难度级别的高质量干扰项，在难度对齐人类感知方面大幅优于GPT-4o

Conclusion: 该框架成功解决了干扰项生成的难度控制问题，为语言评估提供了更精准的工具

Abstract: Multiple-choice cloze questions are commonly used to assess linguistic
proficiency and comprehension. However, generating high-quality distractors
remains challenging, as existing methods often lack adaptability and control
over difficulty levels, and the absence of difficulty-annotated datasets
further hinders progress. To address these issues, we propose a novel framework
for generating distractors with controllable difficulty by leveraging both data
augmentation and a multitask learning strategy. First, to create a
high-quality, difficulty-annotated dataset, we introduce a two-way distractor
generation process in order to produce diverse and plausible distractors. These
candidates are subsequently refined through filtering and then categorized by
difficulty using an ensemble QA system. Second, this newly created dataset is
leveraged to train a difficulty-controllable generation model via multitask
learning. The framework includes carefully designed auxiliary tasks that
enhance the model's semantic understanding of distractors and its ability to
estimate their difficulty. Experimental results demonstrate that our method
generates high-quality distractors across difficulty levels and substantially
outperforms GPT-4o in aligning distractor difficulty with human perception.

</details>


### [74] [Math anxiety and associative knowledge structure are entwined in psychology students but not in Large Language Models like GPT-3.5 and GPT-4o](https://arxiv.org/abs/2511.01558)
*Luciana Ciringione,Emma Franchino,Simone Reigl,Isaia D'Onofrio,Anna Serbati,Oleksandra Poquet,Florence Gabriel,Massimo Stella*

Main category: cs.CL

TL;DR: 本研究使用行为形式心智网络框架分析心理学大学生对数学和焦虑概念的看法与关联，发现人类学生的积极情绪评分和网络度可预测数学焦虑，但GPT模拟数据不适用，揭示了高数学焦虑学生对"焦虑"概念的情感极化认知。


<details>
  <summary>Details</summary>
Motivation: 数学焦虑严重影响心理学大学生的职业选择和心理健康，需要了解他们如何认知和关联数学与焦虑概念，以制定有效的干预策略。

Method: 采用行为形式心智网络框架，通过4个实验比较心理学本科生（n=127）与GPT模拟学生（GPT-3.5: n=300; GPT-4o: n=300）的概念认知网络，分析个体和群体层面的概念关联模式。

Result: 人类学生中，"焦虑"的积极情绪评分和较高网络度，加上"数学"的消极评分，可预测更高的总数学焦虑和评价性数学焦虑。GPT模拟数据与人类存在显著差异，高数学焦虑学生对"焦虑"概念表现出情感极化认知。

Conclusion: 概念认知和关联方式在管理学生数学焦虑中至关重要，高数学焦虑学生的情感极化认知模式需要特别关注，GPT模型在模拟人类数学焦虑认知方面存在局限性。

Abstract: Math anxiety poses significant challenges for university psychology students,
affecting their career choices and overall well-being. This study employs a
framework based on behavioural forma mentis networks (i.e. cognitive models
that map how individuals structure their associative knowledge and emotional
perceptions of concepts) to explore individual and group differences in the
perception and association of concepts related to math and anxiety. We
conducted 4 experiments involving psychology undergraduates from 2 samples (n1
= 70, n2 = 57) compared against GPT-simulated students (GPT-3.5: n2 = 300;
GPT-4o: n4 = 300). Experiments 1, 2, and 3 employ individual-level network
features to predict psychometric scores for math anxiety and its facets
(observational, social and evaluational) from the Math Anxiety Scale.
Experiment 4 focuses on group-level perceptions extracted from human students,
GPT-3.5 and GPT-4o's networks. Results indicate that, in students, positive
valence ratings and higher network degree for "anxiety", together with negative
ratings for "math", can predict higher total and evaluative math anxiety. In
contrast, these models do not work on GPT-based data because of differences in
simulated networks and psychometric scores compared to humans. These results
were also reconciled with differences found in the ways that high/low subgroups
of simulated and real students framed semantically and emotionally STEM
concepts. High math-anxiety students collectively framed "anxiety" in an
emotionally polarising way, absent in the negative perception of low
math-anxiety students. "Science" was rated positively, but contrasted against
the negative perception of "math". These findings underscore the importance of
understanding concept perception and associations in managing students' math
anxiety.

</details>


### [75] [ECO Decoding: Entropy-Based Control for Controllability and Fluency in Controllable Dialogue Generation](https://arxiv.org/abs/2511.01568)
*Seungmin Shin,Dooyoung Kim,Youngjoong Ko*

Main category: cs.CL

TL;DR: 提出ECO解码方法，通过基于语言模型和属性分类器概率分布的熵值动态调整控制强度，解决可控对话生成中固定控制强度难以平衡可控性和流畅性的问题。


<details>
  <summary>Details</summary>
Motivation: 传统加权解码方法使用固定常数管理属性概率偏差，难以找到同时满足可控性和流畅性的理想控制强度。

Method: ECO解码（基于熵的控制），根据语言模型和属性分类器在每个生成步骤的概率分布熵值动态调整控制强度。

Result: 在DailyDialog和MultiWOZ数据集上的实验表明，ECO解码在保持流畅性和语法性的同时持续提高可控性，在各种模型和设置下优于先前解码方法。

Conclusion: ECO解码缓解了多属性生成中的概率插值问题，在单属性和多属性场景下均表现出强大性能。

Abstract: Controllable Dialogue Generation (CDG) enables chatbots to generate responses
with desired attributes, and weighted decoding methods have achieved
significant success in the CDG task. However, using a fixed constant value to
manage the bias of attribute probabilities makes it challenging to find an
ideal control strength that satisfies both controllability and fluency. To
address this issue, we propose ECO decoding (Entropy-based COntrol), which
dynamically adjusts the control strength at each generation step according to
the model's entropy in both the language model and attribute classifier
probability distributions. Experiments on the DailyDialog and MultiWOZ datasets
demonstrate that ECO decoding consistently improves controllability while
maintaining fluency and grammaticality, outperforming prior decoding methods
across various models and settings. Furthermore, ECO decoding alleviates
probability interpolation issues in multi-attribute generation and consequently
demonstrates strong performance in both single and multi-attribute scenarios.

</details>


### [76] [BIRD: Bronze Inscription Restoration and Dating](https://arxiv.org/abs/2511.01589)
*Wenjie Hua,Hoang H. Nguyen,Gangyan Ge*

Main category: cs.CL

TL;DR: BIRD数据集和字形网络框架用于青铜器铭文修复与断代，通过整合字形和异体字信息提升性能。


<details>
  <summary>Details</summary>
Motivation: 早期中国青铜器铭文残缺且难以断代，需要系统化的数据集和方法来解决这一挑战。

Method: 提出BIRD数据集和字形网络(GN)框架，结合领域自适应预训练和字形感知的掩码语言建模，连接字素和异体字。

Result: 实验表明GN提升了铭文修复效果，字形偏置采样在断代任务中取得收益。

Conclusion: 该方法为青铜器铭文研究提供了有效的计算工具，在修复和断代任务中均表现出色。

Abstract: Bronze inscriptions from early China are fragmentary and difficult to date.
We introduce BIRD(Bronze Inscription Restoration and Dating), a fully encoded
dataset grounded in standard scholarly transcriptions and chronological labels.
We further propose an allograph-aware masked language modeling framework that
integrates domain- and task-adaptive pretraining with a Glyph Net (GN), which
links graphemes and allographs. Experiments show that GN improves restoration,
while glyph-biased sampling yields gains in dating.

</details>


### [77] [Imperfect Language, Artificial Intelligence, and the Human Mind: An Interdisciplinary Approach to Linguistic Errors in Native Spanish Speakers](https://arxiv.org/abs/2511.01615)
*Francisco Portillo López*

Main category: cs.CL

TL;DR: 本研究分析西班牙语母语者的语言错误，评估大语言模型对这些错误的解释和纠正能力，旨在开发更符合人类认知的NLP系统。


<details>
  <summary>Details</summary>
Motivation: 语言错误不仅是语法偏差，更是理解语言认知架构的窗口，揭示了当前人工智能系统在复制人类语言能力方面的局限性。

Method: 整合理论语言学、神经语言学和自然语言处理三个视角，构建包含500+真实错误的西班牙语语料库，使用GPT、Gemini等AI模型测试其解释准确性。

Result: 通过实证分析评估AI模型对人类语言错误的解释能力和泛化能力，揭示其与真实人类语言处理的差异。

Conclusion: 该项目不仅有助于理解西班牙语作为母语的特征，还能推动开发更认知化、更能处理人类语言不完美性和模糊性的NLP系统。

Abstract: Linguistic errors are not merely deviations from normative grammar; they
offer a unique window into the cognitive architecture of language and expose
the current limitations of artificial systems that seek to replicate them. This
project proposes an interdisciplinary study of linguistic errors produced by
native Spanish speakers, with the aim of analyzing how current large language
models (LLM) interpret, reproduce, or correct them. The research integrates
three core perspectives: theoretical linguistics, to classify and understand
the nature of the errors; neurolinguistics, to contextualize them within
real-time language processing in the brain; and natural language processing
(NLP), to evaluate their interpretation against linguistic errors. A
purpose-built corpus of authentic errors of native Spanish (+500) will serve as
the foundation for empirical analysis. These errors will be tested against AI
models such as GPT or Gemini to assess their interpretative accuracy and their
ability to generalize patterns of human linguistic behavior. The project
contributes not only to the understanding of Spanish as a native language but
also to the development of NLP systems that are more cognitively informed and
capable of engaging with the imperfect, variable, and often ambiguous nature of
real human language.

</details>


### [78] [ParlaSpeech 3.0: Richly Annotated Spoken Parliamentary Corpora of Croatian, Czech, Polish, and Serbian](https://arxiv.org/abs/2511.01619)
*Nikola Ljubešić,Peter Rupnik,Ivan Porupski,Taja Kuzman Pungeršek*

Main category: cs.CL

TL;DR: ParlaSpeech是一个包含克罗地亚语、捷克语、波兰语和塞尔维亚语四种斯拉夫语言的口语议会语料库，总规模达6000小时，通过自动方式从ParlaMint转录构建，并丰富了多种自动注释层。


<details>
  <summary>Details</summary>
Motivation: 构建一个大规模、多语言的议会口语语料库，为跨学科研究提供丰富资源，特别是通过自动注释增强语料库的实用性。

Method: 从ParlaMint转录和对应元数据自动对齐到议会录音，添加语言注释、情感预测、填充停顿检测、词/字素级对齐和重音位置等自动注释层。

Result: 创建了包含6000小时语音的四个斯拉夫语言语料库，显著增强了语料库的可用性，并通过情感声学相关性分析展示了其应用价值。

Conclusion: ParlaSpeech语料库通过丰富的自动注释大大提升了研究价值，支持JSONL、TextGrid格式下载和通过concordancer搜索，适用于多学科下游研究。

Abstract: ParlaSpeech is a collection of spoken parliamentary corpora currently
spanning four Slavic languages - Croatian, Czech, Polish and Serbian - all
together 6 thousand hours in size. The corpora were built in an automatic
fashion from the ParlaMint transcripts and their corresponding metadata, which
were aligned to the speech recordings of each corresponding parliament. In this
release of the dataset, each of the corpora is significantly enriched with
various automatic annotation layers. The textual modality of all four corpora
has been enriched with linguistic annotations and sentiment predictions.
Similar to that, their spoken modality has been automatically enriched with
occurrences of filled pauses, the most frequent disfluency in typical speech.
Two out of the four languages have been additionally enriched with detailed
word- and grapheme-level alignments, and the automatic annotation of the
position of primary stress in multisyllabic words. With these enrichments, the
usefulness of the underlying corpora has been drastically increased for
downstream research across multiple disciplines, which we showcase through an
analysis of acoustic correlates of sentiment. All the corpora are made
available for download in JSONL and TextGrid formats, as well as for search
through a concordancer.

</details>


### [79] [A Graph-based RAG for Energy Efficiency Question Answering](https://arxiv.org/abs/2511.01643)
*Riccardo Campi,Nicolò Oreste Pinciroli Vago,Mathyas Giudici,Pablo Barrachina Rodriguez-Guisado,Marco Brambilla,Piero Fraternali*

Main category: cs.CL

TL;DR: 该研究探索了在基于图结构的检索增强生成(RAG)架构中使用大语言模型(LLM)进行能效问答，通过从能源领域文档自动构建知识图谱并推理导航，实现多语言准确回答。


<details>
  <summary>Details</summary>
Motivation: 研究旨在利用图结构RAG架构提升能效领域问答的准确性和多语言能力，解决传统方法在复杂能源监管文档处理中的局限性。

Method: 首先从能源指南和监管文档自动提取知识图谱(KG)，然后在生成的图谱上进行导航和推理，使用RAGAs框架进行人工验证，包含101个问答对和领域专家评估。

Result: 系统在约四分之三的情况下正确回答(75.2±2.7%)，通用能效问题准确率更高(达81.0±4.1%)，多语言能力表现良好(翻译仅导致4.4%准确率损失)。

Conclusion: 该架构展现了在能效问答领域的潜力，验证了图结构RAG结合LLM的有效性，同时识别了系统的优势和改进空间。

Abstract: In this work, we investigate the use of Large Language Models (LLMs) within a
graph-based Retrieval Augmented Generation (RAG) architecture for Energy
Efficiency (EE) Question Answering. First, the system automatically extracts a
Knowledge Graph (KG) from guidance and regulatory documents in the energy
field. Then, the generated graph is navigated and reasoned upon to provide
users with accurate answers in multiple languages. We implement a human-based
validation using the RAGAs framework properties, a validation dataset
comprising 101 question-answer pairs, and domain experts. Results confirm the
potential of this architecture and identify its strengths and weaknesses.
Validation results show how the system correctly answers in about three out of
four of the cases (75.2 +- 2.7%), with higher results on questions related to
more general EE answers (up to 81.0 +- 4.1%), and featuring promising
multilingual abilities (4.4% accuracy loss due to translation).

</details>


### [80] [Evaluating Cultural Knowledge Processing in Large Language Models: A Cognitive Benchmarking Framework Integrating Retrieval-Augmented Generation](https://arxiv.org/abs/2511.01649)
*Hung-Shin Lee,Chen-Chi Chang,Ching-Yuan Chen,Yun-Hsiang Hsu*

Main category: cs.CL

TL;DR: 提出一个认知基准框架，结合布鲁姆分类法和检索增强生成，评估大语言模型处理文化特定知识的能力，以台湾客家数字文化档案为测试平台。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型在文化特定知识处理方面的表现，特别是在认知层次上的能力差异。

Method: 整合布鲁姆分类法和检索增强生成(RAG)技术，构建六个层次认知领域的评估框架，使用台湾客家数字文化档案作为测试数据。

Result: 测量大语言模型生成响应的语义准确性和文化相关性。

Conclusion: 该框架为评估语言模型的文化认知能力提供了系统化的方法论。

Abstract: This study proposes a cognitive benchmarking framework to evaluate how large
language models (LLMs) process and apply culturally specific knowledge. The
framework integrates Bloom's Taxonomy with Retrieval-Augmented Generation (RAG)
to assess model performance across six hierarchical cognitive domains:
Remembering, Understanding, Applying, Analyzing, Evaluating, and Creating.
Using a curated Taiwanese Hakka digital cultural archive as the primary
testbed, the evaluation measures LLM-generated responses' semantic accuracy and
cultural relevance.

</details>


### [81] [EngChain: A Symbolic Benchmark for Verifiable Multi-Step Reasoning in Engineering](https://arxiv.org/abs/2511.01650)
*Ayesha Gull,Muhammad Usman Safder,Rania Elbadry,Preslav Nakov,Zhuohan Xie*

Main category: cs.CL

TL;DR: EngChain是一个用于验证多步骤工程问题解决能力的基准测试，包含90个跨3个工程分支的问题，采用两阶段评估方法：定量验证推理步骤的有效性和定性分类推理错误。


<details>
  <summary>Details</summary>
Motivation: 当前基准测试主要评估语言理解、事实回忆、数学或代码生成能力，但缺乏对工程领域所需的整合推理能力的评估，而工程问题需要科学原理、定量建模和实际约束的融合。

Method: 通过符号模板生成高度随机化的问题确保多样性，采用两阶段评估：首先定量验证每个推理步骤的数值和语义有效性，然后使用LLM-As-A-Judge系统定性分类识别出的推理错误。

Result: 开发了EngChain基准测试，包含90个问题，涵盖3个工程分支、9个领域和20个不同领域，避免了数据污染风险。

Conclusion: EngChain填补了工程领域复杂推理能力评估的空白，为大型语言模型在专业工程应用中的能力评估提供了可靠工具。

Abstract: Large Language Models (LLMs) are increasingly being applied to specialized,
high-stakes domains like engineering, which demands rigorous evaluation of
their complex reasoning capabilities. While current benchmarks assess language
understanding, factual recall, mathematics or code generation, none capture the
integrative reasoning central to engineering where scientific principles,
quantitative modeling and practical constraints must converge. To address this
gap, we introduce EngChain, a benchmark for verifiable multi-step engineering
problem-solving. EngChain contains 90 problems spanning three engineering
branches, organized into 9 domains and 20 distinct areas. The problems are
generated from symbolic templates with a high degree of randomization to ensure
diversity and eliminate the risk of contamination. With this benchmark, we move
beyond final answer accuracy with a two-stage evaluation: we first
quantitatively verify the numerical and semantic validity of each reasoning
step and then introduce LLM-As-A-Judge, an automated system to qualitatively
categorize the identified reasoning errors.

</details>


### [82] [SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia](https://arxiv.org/abs/2511.01670)
*Chaoqun Liu,Mahani Aljunied,Guizhen Chen,Hou Pong Chan,Weiwen Xu,Yu Rong,Wenxuan Zhang*

Main category: cs.CL

TL;DR: SeaLLMs-Audio是首个针对东南亚语言的大规模音频语言模型，支持印尼语、泰语、越南语以及英语和中文，具备多语言、多模态和多任务能力。


<details>
  <summary>Details</summary>
Motivation: 为东南亚地区开发专门的音频语言模型，解决该地区多语言音频处理的需求，推动区域研究和产业发展。

Method: 基于大规模音频语料库训练，支持音频、文本及混合输入，涵盖音频理解、语音识别、翻译、情感识别、问答和对话等多种任务。

Result: 在东南亚语言上表现出色，与现有音频语言模型相比具有竞争力，并通过SeaBench-Audio基准测试验证了性能。

Conclusion: SeaLLMs-Audio是东南亚音频LLM发展的重要进展，有望促进该地区的研究和产业应用。

Abstract: We introduce SeaLLMs-Audio, the first large audio-language model (LALM)
tailored for multiple Southeast Asian (SEA) languages-Indonesian (id), Thai
(th), and Vietnamese (vi)-alongside English (en) and Chinese (zh). Trained on a
large-scale audio corpus, SeaLLMs-Audio exhibits strong performance across
diverse audio-centric tasks, spanning fine-grained audio understanding and
voice-based interaction. Its key features include: 1) Multilingual: the model
primarily supports 5 languages, namely Indonesian, Thai, Vietnamese, English,
and Chinese; 2) Multimodal: the model accepts flexible input modalities,
including audio only, text only, as well as audio with text; 3) Multi-task: the
model supports a wide range of tasks, including audio analysis tasks such as
Audio Captioning, Automatic Speech Recognition, Speech-to-Text Translation,
Speech Emotion Recognition, Speech Question Answering, and Speech
Summarization. It also enables voice-based dialogue, including answering
factual, mathematical, and general knowledge queries. As a significant step
towards advancing audio LLMs in Southeast Asia, we expect SeaLLMs-Audio to
benefit both the regional research community and industry. To automate LALM
evaluation for Southeast Asia, we introduce SeaBench-Audio, a benchmark
spanning multiple tasks. Experiments show that SeaLLMs-Audio achieves
competitive performance compared with other LALMs on SEA languages.

</details>


### [83] [Open Character Training: Shaping the Persona of AI Assistants through Constitutional AI](https://arxiv.org/abs/2511.01689)
*Sharan Maiya,Henning Bartsch,Nathan Lambert,Evan Hubinger*

Main category: cs.CL

TL;DR: 本文提出了首个开源的AI助手角色训练方法，使用宪法AI和合成自省数据来塑造助手角色，相比系统提示约束和激活导向等方法更有效可控。


<details>
  <summary>Details</summary>
Motivation: 现代聊天机器人语言模型生成的"AI助手"角色特征会影响交互质量、感知智能和与开发者用户意图的对齐，但角色训练这一关键组件在学术文献中尚未得到充分研究。

Method: 使用宪法AI和基于合成自省数据的新数据管道，对三个流行的开源模型进行微调，应用11个示例角色（如幽默、深度关怀甚至恶意）。通过分析揭示偏好的方法来跟踪效果。

Result: 该方法相比系统提示约束和激活导向对对抗性提示更鲁棒，同时产生更连贯和真实的生成内容。微调对通用能力基准测试几乎没有影响。

Conclusion: 开发并开源了完整的后训练方法，实现了更有效和可控的角色塑造，同时保持了模型的通用能力。

Abstract: The character of the "AI assistant" persona generated by modern chatbot large
language models influences both surface-level behavior and apparent values,
beliefs, and ethics. These all affect interaction quality, perceived
intelligence, and alignment with both developer and user intentions. The
shaping of this persona, known as character training, is a critical component
of industry post-training, yet remains effectively unstudied in the academic
literature. We introduce the first open implementation of character training,
leveraging Constitutional AI and a new data pipeline using synthetic
introspective data to shape the assistant persona in a more effective and
controlled manner than alternatives such as constraining system prompts or
activation steering. Specifically, we fine-tune three popular open-weights
models using 11 example personas, such as humorous, deeply caring, or even
malevolent. To track the effects of our approach, we introduce a method which
analyzes revealed preferences, uncovering clear and holistic changes in
character. We find these changes are more robust to adversarial prompting than
the above two alternatives, while also leading to more coherent and realistic
generations. Finally, we demonstrate this fine-tuning has little to no effect
on general capabilities as measured by common benchmarks. We describe and
open-source our full post-training method, the implementation of which can be
found at https://github.com/maiush/OpenCharacterTraining.

</details>


### [84] [Multi-Step Knowledge Interaction Analysis via Rank-2 Subspace Disentanglement](https://arxiv.org/abs/2511.01706)
*Sekh Mainul Islam,Pepa Atanasova,Isabelle Augenstein*

Main category: cs.CL

TL;DR: 本文提出了一个新颖的rank-2投影子空间来更准确地区分LLM中参数知识(PK)和上下文知识(CK)的贡献，并首次对长NLE序列中的多步知识交互进行分析。


<details>
  <summary>Details</summary>
Motivation: 理解NLEs如何结合外部上下文知识和模型内部参数知识对于评估NLEs的grounding至关重要，但现有研究对此关注不足，通常只分析单步生成且将PK和CK交互建模为rank-1子空间中的二元选择。

Method: 提出rank-2投影子空间来更准确地区分PK和CK贡献，在四个QA数据集和三个开源指令调优LLM上进行实验，首次进行多步知识交互分析。

Result: 实验表明多样化的知识交互在rank-1子空间中表现不佳，但在rank-2公式中能有效捕捉。多步分析显示：幻觉NLEs与PK方向高度一致，上下文忠实的NLEs平衡PK和CK，CoT提示通过减少PK依赖使生成的NLEs偏向CK。

Conclusion: 这项工作为通过更丰富的rank-2子空间解耦来系统研究LLM中多步知识交互提供了首个框架。

Abstract: Natural Language Explanations (NLEs) describe how Large Language Models
(LLMs) make decisions, drawing on both external Context Knowledge (CK) and
Parametric Knowledge (PK) stored in model weights. Understanding their
interaction is key to assessing the grounding of NLEs, yet it remains
underexplored. Prior work has largely examined only single-step generation,
typically the final answer, and has modelled PK and CK interaction only as a
binary choice in a rank-1 subspace. This overlooks richer forms of interaction,
such as complementary or supportive knowledge. We propose a novel rank-2
projection subspace that disentangles PK and CK contributions more accurately
and use it for the first multi-step analysis of knowledge interactions across
longer NLE sequences. Experiments on four QA datasets and three open-weight
instruction-tuned LLMs show that diverse knowledge interactions are poorly
represented in a rank-1 subspace but are effectively captured in our rank-2
formulation. Our multi-step analysis reveals that hallucinated NLEs align
strongly with the PK direction, context-faithful ones balance PK and CK, and
Chain-of-Thought prompting for NLEs shifts generated NLEs toward CK by reducing
PK reliance. This work provides the first framework for systematic studies of
multi-step knowledge interactions in LLMs through a richer rank-2 subspace
disentanglement. Code and data:
https://github.com/copenlu/pk-ck-knowledge-disentanglement.

</details>


### [85] [Efficient Tool-Calling Multi-Expert NPC Agent for Commonsense Persona-Grounded Dialogue](https://arxiv.org/abs/2511.01720)
*Mahammad Nuriyev*

Main category: cs.CL

TL;DR: 提出了一个基于Qwen3和LoRA适配器的多专家系统，用于创建能够进行自然对话和执行上下文动作的NPC角色，在CPDC 2025挑战赛中排名第二。


<details>
  <summary>Details</summary>
Motivation: 为了创建能够在交互环境中同时进行自然对话和执行上下文动作的非玩家角色(NPC)，需要解决计算效率和响应速度的问题。

Method: 使用Qwen3作为基础模型，通过低秩适应(LoRA)适配器实例化三个专家模块：工具调用、工具响应解释和直接对话。

Result: 系统在L40S GPU上实现了快速响应和适度的资源使用，在Commonsense Persona-Grounded Dialogue Challenge 2025中排名第二。

Conclusion: 基于LoRA的多专家系统方法能够有效创建具有对话和行动能力的NPC，同时满足计算效率要求。

Abstract: We present a multi-expert system for creating Non-Player Characters (NPCs)
capable of both natural dialogue and contextual action execution in interactive
environments. Using Qwen3 as the base model and Low-Rank Adaptation (LoRA)
adapters, we instantiate three specialists: tool calling, tool-response
interpretation, and direct dialogue. Our system comfortably meets the
computational efficiency requirements, delivering fast responses and
maintaining modest resource usage on L40S GPUs. In the Commonsense
Persona-Grounded Dialogue Challenge 2025, our method ranked second overall.
  Code available at:
https://github.com/MahammadNuriyev62/CPDC-challenge-2025-solution/

</details>


### [86] [Accumulating Context Changes the Beliefs of Language Models](https://arxiv.org/abs/2511.01805)
*Jiayi Geng,Howard Chen,Ryan Liu,Manoel Horta Ribeiro,Robb Willer,Graham Neubig,Thomas L. Griffiths*

Main category: cs.CL

TL;DR: 语言模型在长时间对话和阅读过程中会经历信念漂移，导致其世界观和行为发生变化，这可能影响模型的可靠性和一致性。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型自主性增强，上下文积累可能导致模型信念无声变化，带来用户体验不一致和行为偏离原始对齐的风险。

Method: 通过道德困境讨论、政治立场阅读测试以及工具使用任务，量化模型信念变化程度。

Result: GPT-5在10轮道德讨论后信念变化达54.7%，Grok 4在阅读对立政治文本后变化27.2%，行为变化与信念漂移一致。

Conclusion: 模型在长时间对话和阅读过程中存在信念漂移风险，这会影响其意见和行为的可靠性。

Abstract: Language model (LM) assistants are increasingly used in applications such as
brainstorming and research. Improvements in memory and context size have
allowed these models to become more autonomous, which has also resulted in more
text accumulation in their context windows without explicit user intervention.
This comes with a latent risk: the belief profiles of models -- their
understanding of the world as manifested in their responses or actions -- may
silently change as context accumulates. This can lead to subtly inconsistent
user experiences, or shifts in behavior that deviate from the original
alignment of the models. In this paper, we explore how accumulating context by
engaging in interactions and processing text -- talking and reading -- can
change the beliefs of language models, as manifested in their responses and
behaviors.Our results reveal that models' belief profiles are highly malleable:
GPT-5 exhibits a 54.7% shift in its stated beliefs after 10 rounds of
discussion about moral dilemmas and queries about safety, while Grok 4 shows a
27.2% shift on political issues after reading texts from the opposing position.
We also examine models' behavioral changes by designing tasks that require tool
use, where each tool selection corresponds to an implicit belief. We find that
these changes align with stated belief shifts, suggesting that belief shifts
will be reflected in actual behavior in agentic systems. Our analysis exposes
the hidden risk of belief shift as models undergo extended sessions of talking
or reading, rendering their opinions and actions unreliable.

</details>


### [87] [Plan-and-Write: Structure-Guided Length Control for LLMs without Model Retraining](https://arxiv.org/abs/2511.01807)
*Adewale Akinfaderin,Shreyas Subramanian,Akarsha Sehwag*

Main category: cs.CL

TL;DR: 提出一种无需模型重新训练的长度控制提示工程方法，通过结构化规划和字数统计机制，在文档摘要任务中显著提高LLMs的长度遵循能力，特别适用于中短长度约束。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs长度控制这一重要但研究不足的挑战，现有方法需要昂贵的模型重训练或复杂推理工具，需要一种无需重训练的即时部署方案。

Method: 基于提示工程的结构化引导方法，在提示中实现精心的规划和字数统计机制，鼓励模型仔细跟踪并遵循指定的长度约束。

Result: 在六个先进LLMs上的评估显示，相比标准提示方法，本方法显著提高了多个模型的长度遵循能力，某些模型长度遵循度提升达37.6%，同时保持或提升了输出质量。

Conclusion: 该方法为需要精确长度控制的应用提供了即时可部署的解决方案，特别适用于模型重训练不切实际或成本过高的生产环境。

Abstract: Length control in Large Language Models (LLMs) is a crucial but
under-addressed challenge, with applications ranging from voice interfaces
requiring concise responses to research summaries needing comprehensive
outputs. Current approaches to length control, including Regularized DPO,
Length-Instruction Fine Tuning, and tool-augmented methods, typically require
expensive model retraining or complex inference-time tooling. This paper
presents a prompt engineering methodology that enables precise length control
without model retraining. Our structure-guided approach implements deliberate
planning and word counting mechanisms within the prompt, encouraging the model
to carefully track and adhere to specified length constraints. Comprehensive
evaluations across six state-of-the-art LLMs demonstrate that our method
significantly improves length fidelity for several models compared to standard
prompting when applied to document summarization tasks, particularly for
shorter-to-medium length constraints. The proposed technique shows varying
benefits across different model architectures, with some models demonstrating
up to 37.6% improvement in length adherence. Quality evaluations further reveal
that our approach maintains or enhances overall output quality compared to
standard prompting techniques. Our approach provides an immediately deployable
solution for applications requiring precise length control, particularly
valuable for production environments where model retraining is impractical or
cost-prohibitive.

</details>


### [88] [KV Cache Transform Coding for Compact Storage in LLM Inference](https://arxiv.org/abs/2511.01815)
*Konrad Staniszewski,Adrian Łańcucki*

Main category: cs.CL

TL;DR: KVTC是一种轻量级变换编码器，通过PCA特征去相关、自适应量化和熵编码来压缩KV缓存，实现高达20倍压缩比，同时保持推理精度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型服务需要高效的KV缓存管理，共享前缀提示中的陈旧缓存会消耗宝贵的GPU内存，需要压缩解决方案。

Method: 结合经典媒体压缩技术，使用PCA特征去相关、自适应量化和熵编码，仅需简短初始校准且不改变模型参数。

Result: 在Llama 3、Mistral NeMo和R1-Qwen 2.5模型上测试，在AIME25、LiveCodeBench等基准测试中实现20倍压缩，特定用例可达40倍以上，优于现有方法。

Conclusion: KVTC是内存高效LLM服务的实用构建模块，支持可重用KV缓存。

Abstract: Serving large language models (LLMs) at scale necessitates efficient
key-value (KV) cache management. KV caches can be reused across conversation
turns via shared-prefix prompts that are common in iterative code editing and
chat. However, stale caches consume scarce GPU memory, require offloading, or
force recomputation. We present KVTC, a lightweight transform coder that
compresses KV caches for compact on-GPU and off-GPU storage. Drawing on
classical media compression, KVTC combines PCA-based feature decorrelation,
adaptive quantization, and entropy coding. It requires only a brief initial
calibration and leaves model parameters unchanged. By exploiting redundancies
in KV caches, KVTC achieves up to 20$\times$ compression while maintaining
reasoning and long-context accuracy, and 40$\times$ or higher for specific use
cases. We test KVTC with Llama 3, Mistral NeMo, and R1-Qwen 2.5 models across
benchmarks including AIME25, LiveCodeBench, GSM8K, MMLU, Qasper, RULER, and
MATH-500. It consistently outperforms inference-time baselines such as token
eviction, quantization, and SVD-based methods, while achieving higher
compression ratios. These results support KVTC as a practical building block
for memory-efficient LLM serving with reusable KV caches.

</details>


### [89] [Towards Robust Mathematical Reasoning](https://arxiv.org/abs/2511.01846)
*Thang Luong,Dawsen Hwang,Hoang H. Nguyen,Golnaz Ghiasi,Yuri Chervonyi,Insuk Seo,Junsu Kim,Garrett Bingham,Jonathan Lee,Swaroop Mishra,Alex Zhai,Clara Huiyi Hu,Henryk Michalewski,Jimin Kim,Jeonghyun Ahn,Junhwi Bae,Xingyou Song,Trieu H. Trinh,Quoc V. Le,Junehyuk Jung*

Main category: cs.CL

TL;DR: IMO-Bench是一个高级数学推理基准套件，专门针对国际数学奥林匹克竞赛(IMO)水平设计，包含答案基准和证明基准，在Gemini Deep Think模型中实现了历史性的金牌表现。


<details>
  <summary>Details</summary>
Motivation: 现有数学推理评估要么过于简单，要么只关注简短正确答案，需要找到更有效的评估指标来推进基础模型的数学推理能力。

Method: 开发IMO-Bench基准套件，包括IMO-AnswerBench（400个多样化的奥数问题）和IMO-ProofBench（证明写作能力评估），并建立自动评分系统。

Result: Gemini Deep Think模型在IMO-AnswerBench上达到80.0%，在高级IMO-ProofBench上达到65.7%，分别比最佳非Gemini模型高出6.9%和42.4%。

Conclusion: IMO-Bench基准套件有助于推进强大的数学推理能力发展，自动评分器与人工评估相关性良好，为长答案自动评估提供了基础。

Abstract: Finding the right north-star metrics is highly critical for advancing the
mathematical reasoning capabilities of foundation models, especially given that
existing evaluations are either too easy or only focus on getting correct short
answers. To address these issues, we present IMO-Bench, a suite of advanced
reasoning benchmarks, vetted by a panel of top specialists and that
specifically targets the level of the International Mathematical Olympiad
(IMO), the most prestigious venue for young mathematicians. IMO-AnswerBench
first tests models on 400 diverse Olympiad problems with verifiable short
answers. IMO-Proof Bench is the next-level evaluation for proof-writing
capabilities, which includes both basic and advanced IMO level problems as well
as detailed grading guidelines to facilitate automatic grading. These
benchmarks played a crucial role in our historic achievement of the gold-level
performance at IMO 2025 with Gemini Deep Think (Luong and Lockhart, 2025). Our
model achieved 80.0% on IMO-AnswerBench and 65.7% on the advanced IMO-Proof
Bench, surpassing the best non-Gemini models by large margins of 6.9% and 42.4%
respectively. We also showed that autograders built with Gemini reasoning
correlate well with human evaluations and construct IMO-GradingBench, with 1000
human gradings on proofs, to enable further progress in automatic evaluation of
long-form answers. We hope that IMO-Bench will help the community towards
advancing robust mathematical reasoning and release it at
https://imobench.github.io/.

</details>


### [90] [Tool-to-Agent Retrieval: Bridging Tools and Agents for Scalable LLM Multi-Agent Systems](https://arxiv.org/abs/2511.01854)
*Elias Lumer,Faheem Nizar,Anmol Gulati,Pradeep Honaganahalli Basavaraju,Vamse Kumar Subbiah*

Main category: cs.CL

TL;DR: 提出了Tool-to-Agent Retrieval框架，通过在共享向量空间中嵌入工具和代理，利用元数据关系连接它们，实现了细粒度的工具级或代理级检索，在LiveMCPBench基准测试中显著提升了检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有的检索方法通常将查询与粗粒度的代理级描述进行匹配，这掩盖了细粒度的工具功能，导致代理选择不理想。

Method: 在共享向量空间中嵌入工具及其父代理，通过元数据关系连接它们，支持细粒度的工具级或代理级检索。

Result: 在八个嵌入模型上评估，Tool-to-Agent Retrieval在LiveMCPBench基准测试中比现有最先进的代理检索器在Recall@5上提升了19.4%，在nDCG@5上提升了17.7%。

Conclusion: Tool-to-Agent Retrieval框架通过显式表示工具能力并遍历元数据到代理级别，确保了代理及其底层工具或MCP服务器的平等表示，避免了将多个工具分块在一起时产生的上下文稀释问题。

Abstract: Recent advances in LLM Multi-Agent Systems enable scalable orchestration of
sub-agents, each coordinating hundreds or thousands of tools or Model Context
Protocol (MCP) servers. However, existing retrieval methods typically match
queries against coarse agent-level descriptions before routing, which obscures
fine-grained tool functionality and often results in suboptimal agent
selection. We introduce Tool-to-Agent Retrieval, a unified framework that
embeds both tools and their parent agents in a shared vector space and connects
them through metadata relationships. By explicitly representing tool
capabilities and traversing metadata to the agent level, Tool-to-Agent
Retrieval enables granular tool-level or agent-level retrieval, ensuring that
agents and their underlying tools or MCP servers are equally represented
without the context dilution that arises from chunking many tools together.
Evaluating Tool-to-Agent Retrieval across eight embedding models, our approach
achieves consistent improvements of 19.4% in Recall@5 and 17.7% in nDCG@5 over
previous state-of-the-art agent retrievers on the LiveMCPBench benchmark.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [91] [\texttt{ReMind}: Understanding Deductive Code Reasoning in LLMs](https://arxiv.org/abs/2511.00488)
*Jun Gao,Yun Peng,Xiaoxue Ren*

Main category: cs.PL

TL;DR: 本文提出了ReMind多智能体框架，通过Mutator、Executor和Inspector的协同工作来提升大语言模型的演绎代码推理能力，解决了生成与推理能力差距、代码源偏见和零样本泛化弱等关键挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在代码相关任务上取得了显著进展，但在演绎代码推理（推理程序执行过程）方面仍存在困难。现有研究虽然认识到这一局限性，但对其根本原因探索不足。

Method: 提出ReMind多智能体框架，包含三个组件：Mutator生成代码变体以减轻代码源偏见，Executor逐步跟踪变量状态以暴露不一致性，Inspector识别有问题的推理步骤并提供控制流细化来弥合推理差距。

Result: 在两个基准测试和五个大语言模型上的广泛实验表明，ReMind在演绎代码推理方面相比基线方法具有显著优势，实现了出色的性能和稳健的零样本泛化能力。

Conclusion: ReMind通过多智能体协作系统性地识别和优化推理缺陷，有效解决了大语言模型在演绎代码推理中的关键挑战，为提升代码推理能力提供了有效解决方案。

Abstract: Large Language Models (LLMs) have achieved remarkable progress in
code-related tasks. Despite their advancement, empirical evidence reveals that
they still struggle with \emph{deductive code reasoning}, the ability to reason
about the program execution process. While prior studies have recognized this
limitation, the underlying causes remain largely underexplored. In this paper,
we begin by presenting a comprehensive empirical study that reveals three key
challenges undermining deductive code reasoning: (1) an intrinsic gap between
generation and reasoning abilities, (2) a consistent bias towards code sources,
and (3) weak zero-shot generalization on complex benchmarks. In light of these
challenges, we propose \texttt{ReMind}, a multi-agent framework composed of
\texttt{Mutator}, \texttt{Executor}, and \texttt{Inspector}. The
\texttt{Mutator} generates code variants to mitigate bias towards code sources,
the \texttt{Executor} traces variable states step-by-step to expose
inconsistency, and the \texttt{Inspector} identifies problematic reasoning
steps and provides control-flow refinement to bridge the intrinsic reasoning
gap. Through their coordinated collaboration, \texttt{ReMind} systematically
identifies and refines reasoning flaws, achieving outstanding performance and
enabling robust zero-shot generalization. Extensive experiments on two
benchmarks with five LLMs demonstrate the superior advantages of
\texttt{ReMind} compared to baseline approaches in deductive code reasoning.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [92] [LongCat-Flash-Omni Technical Report](https://arxiv.org/abs/2511.00279)
*Meituan LongCat Team,Bairui Wang,Bayan,Bin Xiao,Bo Zhang,Bolin Rong,Borun Chen,Chang Wan,Chao Zhang,Chen Huang,Chen Chen,Chen Chen,Chengxu Yang,Chengzuo Yang,Cong Han,Dandan Peng,Delian Ruan,Detai Xin,Disong Wang,Dongchao Yang,Fanfan Liu,Fengjiao Chen,Fengyu Yang,Gan Dong,Gang Huang,Gang Xu,Guanglu Wan,Guoqiang Tan,Guoqiao Yu,Haibo Qiu,Hao Lu,Hongbo Liu,Hongyu Xiang,Jiaheng Wu,Jian Yang,Jiaxing Liu,Jing Huang,Jingang Wang,Jinrui Ding,Juchao Jiang,Jun Kuang,Jun Wang,Junhui Mei,Ke Ding,Kefeng Zhang,Lei Chen,Liang Shi,Limeng Qiao,Liming Zheng,Lin Ma,Liuyang Guo,Liya Ma,Luying Sun,Man Gao,Mengshen Zhu,Miao Cao,Minliang Lin,Nuo Xu,Peng Shi,Qi Zhang,Qian Fang,Qian Wang,Qian Yang,Quanxiu Wang,Rongxiang Weng,Rongxin Guo,Ruoxuan Liang,Senbin Yang,Shanbo Xu,Shanglin Lei,Shengze Ye,Shimin Chen,Shuaiqi Chen,Shujie Hu,Shuo Li,Siqi Yang,Siyu Xu,Siyu Ren,Song Li,Songxiang Liu,Tianhao Bai,Tianye Dai,Wei Hong,Wei Wang,Weixiao Zhao,Wengang Cao,Wenlong Zhu,Wenlong He,Xi Su,Xi Nan,Xiaohan Zhao,Xiaohao Wang,Xiaoyu Zhao,Xiaoyu Wang,Xiaoyu Li,Xin Pan,Xin Chen,Xiusong Sun,Xu Xiang,Xudong Xing,Xuezhi Cao,Xunliang Cai,Yang Yang,Yanli Tan,Yao Yao,Yerui Sun,Yi Chen,Yifan Lu,Yin Gong,Yining Zhang,Yitian Chen,Yiyang Gan,Yuchen Tang,Yuchen Xie,Yueqian Wang,Yuewen Zheng,Yufei Zhang,Yufeng Zhong,Yulei Qian,Yuqi Peng,Yuwei Jiang,Zeyang Hu,Zheng Zhang,Zhengkun Tian,Zhiqing Hong,Zhixiong Zeng,Zhuqi Mi,Ziran Li,Ziwen Wang,Ziyi Zhao,Ziyuan Zhuang,Zizhe Zhao*

Main category: cs.MM

TL;DR: LongCat-Flash-Omni是一个5600亿参数的开源全模态模型，采用渐进式训练策略和高效的MoE架构，实现了实时音视频交互，在多项模态任务中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够处理多种模态（文本、图像、视频、音频）的大规模开源模型，实现高效的实时音视频交互，同时保持强大的单模态能力。

Method: 采用渐进式课程训练策略，从简单到复杂逐步训练多模态序列建模；基于Shortcut连接的MoE架构，集成高效的多模态感知和语音重建模块；使用模态解耦并行化方案处理大规模多模态训练的数据和模型异构性。

Result: 模型在5600亿参数规模下（激活270亿参数）实现低延迟实时音视频交互；在多模态基准测试中达到开源模型的最先进性能；在文本、图像、视频理解以及音频理解和生成等任务中表现优异；训练效率达到纯文本训练的90%以上。

Conclusion: LongCat-Flash-Omni成功展示了大规模多模态模型的可行性，通过创新的训练策略和架构设计实现了高效的多模态能力，为社区提供了强大的开源基础模型。

Abstract: We introduce LongCat-Flash-Omni, a state-of-the-art open-source omni-modal
model with 560 billion parameters, excelling at real-time audio-visual
interaction. By adopting a curriculum-inspired progressive training strategy
that transitions from simpler to increasingly complex modality sequence
modeling tasks, LongCat-Flash-Omni attains comprehensive multimodal
capabilities while maintaining strong unimodal capability. Building upon
LongCat-Flash, which adopts a high-performance Shortcut-connected
Mixture-of-Experts (MoE) architecture with zero-computation experts,
LongCat-Flash-Omni integrates efficient multimodal perception and speech
reconstruction modules. Despite its immense size of 560B parameters (with 27B
activated), LongCat-Flash-Omni achieves low-latency real-time audio-visual
interaction. For training infrastructure, we developed a modality-decoupled
parallelism scheme specifically designed to manage the data and model
heterogeneity inherent in large-scale multimodal training. This innovative
approach demonstrates exceptional efficiency by sustaining over 90% of the
throughput achieved by text-only training. Extensive evaluations show that
LongCat-Flash-Omni achieves state-of-the-art performance on omni-modal
benchmarks among open-source models. Furthermore, it delivers highly
competitive results across a wide range of modality-specific tasks, including
text, image, and video understanding, as well as audio understanding and
generation. We provide a comprehensive overview of the model architecture
design, training procedures, and data strategies, and open-source the model to
foster future research and development in the community.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [93] [Erasing 'Ugly' from the Internet: Propagation of the Beauty Myth in Text-Image Models](https://arxiv.org/abs/2511.00749)
*Tanvi Dinkar,Aiqi Jiang,Gavin Abercrombie,Ioannis Konstas*

Main category: cs.CV

TL;DR: 该研究通过生成5984张图像，揭示了生成式AI模型在编码'美'和消除'丑'方面存在的系统性偏见，包括肤色偏浅（86.5%）、年轻化（74%）和NSFW内容（22%），特别是对非二元性别个体的过度性化。


<details>
  <summary>Details</summary>
Motivation: 社交媒体加剧了西方审美标准的推广，导致负面自我形象和身体畸形恐惧症。随着AI生成内容的增加，担忧这些标准被夸大，需要研究生成式AI如何编码'美'并消除'丑'。

Method: 创建两个图像生成流程：文本到图像模型和文本到语言模型到图像模型。开发结构化审美分类法，使用三个语言模型和两个文本到图像模型生成5984张图像，并招募女性和非二元社交媒体用户通过李克特量表评估1200张图像。

Result: 86.5%生成图像描绘浅肤色人群，22%包含NSFW内容，74%被评為年轻年龄段。非二元个体图像被评為更年轻和更过度性化。带有'负面'或'丑陋'特征的提示始终产生更高NSFW评分。

Conclusion: 生成式AI模型存在与审美标准相关的普遍人口统计偏见，这些偏见通过模型开发者（如负向提示）被积极延续，可能导致数据流污染和对不符合开发者审美刻板印象特征的主动消除。

Abstract: Social media has exacerbated the promotion of Western beauty norms, leading
to negative self-image, particularly in women and girls, and causing harm such
as body dysmorphia. Increasingly content on the internet has been artificially
generated, leading to concerns that these norms are being exaggerated. The aim
of this work is to study how generative AI models may encode 'beauty' and erase
'ugliness', and discuss the implications of this for society. To investigate
these aims, we create two image generation pipelines: a text-to-image model and
a text-to-language model-to image model. We develop a structured beauty
taxonomy which we use to prompt three language models (LMs) and two
text-to-image models to cumulatively generate 5984 images using our two
pipelines. We then recruit women and non-binary social media users to evaluate
1200 of the images through a Likert-scale within-subjects study. Participants
show high agreement in their ratings. Our results show that 86.5% of generated
images depicted people with lighter skin tones, 22% contained explicit content
despite Safe for Work (SFW) training, and 74% were rated as being in a younger
age demographic. In particular, the images of non-binary individuals were rated
as both younger and more hypersexualised, indicating troubling intersectional
effects. Notably, prompts encoded with 'negative' or 'ugly' beauty traits (such
as "a wide nose") consistently produced higher Not SFW (NSFW) ratings
regardless of gender. This work sheds light on the pervasive demographic biases
related to beauty standards present in generative AI models -- biases that are
actively perpetuated by model developers, such as via negative prompting. We
conclude by discussing the implications of this on society, which include
pollution of the data streams and active erasure of features that do not fall
inside the stereotype of what is considered beautiful by developers.

</details>


### [94] [GUI-AIMA: Aligning Intrinsic Multimodal Attention with a Context Anchor for GUI Grounding](https://arxiv.org/abs/2511.00810)
*Shijie Zhou,Viet Dac Lai,Hao Tan,Jihyung Kil,Wanrong Zhu,Changyou Chen,Ruiyi Zhang*

Main category: cs.CV

TL;DR: GUI-AIMA是一个基于注意力机制的坐标无关GUI定位框架，通过监督微调激活MLLMs的固有定位能力，在3B模型上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于MLLMs的GUI定位方法将任务制定为基于文本的坐标生成任务，但从视觉输入直接生成精确坐标具有挑战性且计算量大。

Method: 提出GUI-AIMA框架，通过将MLLMs的内在多模态注意力与补丁级定位信号对齐，采用坐标无关的方式，可以轻松集成即插即用的放大阶段。

Result: GUI-AIMA-3B仅用85k截图训练，在ScreenSpot-Pro上达到58.6%的平均准确率，在OSWorld-G上达到62.2%，在3B模型中实现了最先进的性能。

Conclusion: 轻量级训练可以触发MLLMs的固有定位能力，GUI-AIMA展示了卓越的数据效率和性能表现。

Abstract: Graphical user interface (GUI) grounding is a key function of computer-use
agents, which maps natural-language instructions to actionable screen regions.
Existing approaches based on Multimodal Large Language Models (MLLMs) typically
formulate it as a text-based coordinate generation task, yet directly
generating precise coordinates from visual inputs remains challenging and
computationally intensive. An intuitive way to implement GUI grounding is to
first select visual patches relevant to the instructions and then determine the
precise click location within those patches. Based on the observations that
general MLLMs have some native grounding capability, nested within their
attentions, we propose GUI-AIMA, an attention-based and coordinate-free
supervised fine-tuning framework for efficient GUI grounding. GUI-AIMA aligns
the intrinsic multimodal attention of MLLMs with patch-wise grounding signals.
These signals are calculated adaptively for diverse user instructions by
multi-head aggregation on simplified query-visual attention matrices. Besides,
its coordinate-free manner can easily integrate a plug-and-play zoom-in stage.
GUI-AIMA-3B was trained with only 85k screenshots, demonstrating exceptional
data efficiency and verifying that light training can trigger the native
grounding capability of MLLMs. It achieves state-of-the-art performance among
3B models, attaining an average accuracy of 58.6% on ScreenSpot-Pro and 62.2%
on OSWorld-G. Project page: https://github.com/sjz5202/GUI-AIMA

</details>


### [95] [$\left|\,\circlearrowright\,\boxed{\text{BUS}}\,\right|$: A Large and Diverse Multimodal Benchmark for evaluating the ability of Vision-Language Models to understand Rebus Puzzles](https://arxiv.org/abs/2511.01340)
*Trishanu Das,Abhilash Nandy,Khush Bajaj,Deepiha S*

Main category: cs.CV

TL;DR: 本文提出了一个包含1333个英语Rebus谜题的大型多样化基准测试，并开发了RebusDescProgICE框架，通过结合非结构化描述和基于代码的结构化推理，显著提升了视觉语言模型在Rebus谜题上的性能。


<details>
  <summary>Details</summary>
Motivation: Rebus谜题需要图像识别、认知技能、常识推理、多步推理和基于图像的文字游戏等多种能力，这对当前视觉语言模型来说是一个具有挑战性的任务。

Method: 提出了RebusDescProgICE框架，使用非结构化描述和基于代码的结构化推理相结合的方法，并采用基于推理的上下文示例选择策略。

Result: 相比思维链推理，该框架在闭源模型上提升了2.1-4.1%的性能，在开源模型上提升了20-30%的性能。

Conclusion: RebusDescProgICE框架有效提升了视觉语言模型在复杂Rebus谜题上的推理能力，证明了结合描述性和程序化推理方法的有效性。

Abstract: Understanding Rebus Puzzles (Rebus Puzzles use pictures, symbols, and letters
to represent words or phrases creatively) requires a variety of skills such as
image recognition, cognitive skills, commonsense reasoning, multi-step
reasoning, image-based wordplay, etc., making this a challenging task for even
current Vision-Language Models. In this paper, we present
$\left|\,\circlearrowright\,\boxed{\text{BUS}}\,\right|$, a large and diverse
benchmark of $1,333$ English Rebus Puzzles containing different artistic styles
and levels of difficulty, spread across 18 categories such as food, idioms,
sports, finance, entertainment, etc. We also propose $RebusDescProgICE$, a
model-agnostic framework which uses a combination of an unstructured
description and code-based, structured reasoning, along with better,
reasoning-based in-context example selection, improving the performance of
Vision-Language Models on
$\left|\,\circlearrowright\,\boxed{\text{BUS}}\,\right|$ by $2.1-4.1\%$ and
$20-30\%$ using closed-source and open-source models respectively compared to
Chain-of-Thought Reasoning.

</details>


### [96] [Actial: Activate Spatial Reasoning Ability of Multimodal Large Language Models](https://arxiv.org/abs/2511.01618)
*Xiaoyu Zhan,Wenxuan Huang,Hao Sun,Xinyu Fu,Changfeng Ma,Shaosheng Cao,Bohan Jia,Shaohui Lin,Zhenfei Yin,Lei Bai,Wanli Ouyang,Yuanqi Li,Jie Guo,Yanwen Guo*

Main category: cs.CV

TL;DR: 该论文提出了Viewpoint Learning任务来评估和改进多模态大语言模型的空间推理能力，构建了包含10万张图像对的Viewpoint-100K数据集，采用两阶段微调策略显著提升了模型的空间推理性能。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型在2D视觉理解方面取得了显著进展，但它们在复杂3D推理任务中的表现仍不明确，特别是能否有效捕捉详细的空间信息以确保跨视图一致性，这是准确3D推理的关键要求。

Method: 构建Viewpoint-100K数据集，包含10万张以物体为中心的图像对和对应问答对；采用两阶段微调策略：首先通过监督微调注入基础知识，然后使用GRPO算法的强化学习增强泛化能力；引入混合冷启动初始化方法同时学习视角表示和保持连贯推理思维。

Result: 实验结果表明，该方法显著激活了多模态大语言模型的空间推理能力，在领域内和领域外推理任务上的性能均有提升。

Conclusion: 开发多模态大语言模型的基础空间技能具有重要价值，将支持未来在机器人、自主系统和3D场景理解方面的进展。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have
significantly improved 2D visual understanding, prompting interest in their
application to complex 3D reasoning tasks. However, it remains unclear whether
these models can effectively capture the detailed spatial information required
for robust real-world performance, especially cross-view consistency, a key
requirement for accurate 3D reasoning. Considering this issue, we introduce
Viewpoint Learning, a task designed to evaluate and improve the spatial
reasoning capabilities of MLLMs. We present the Viewpoint-100K dataset,
consisting of 100K object-centric image pairs with diverse viewpoints and
corresponding question-answer pairs. Our approach employs a two-stage
fine-tuning strategy: first, foundational knowledge is injected to the baseline
MLLM via Supervised Fine-Tuning (SFT) on Viewpoint-100K, resulting in
significant improvements across multiple tasks; second, generalization is
enhanced through Reinforcement Learning using the Group Relative Policy
Optimization (GRPO) algorithm on a broader set of questions. Additionally, we
introduce a hybrid cold-start initialization method designed to simultaneously
learn viewpoint representations and maintain coherent reasoning thinking.
Experimental results show that our approach significantly activates the spatial
reasoning ability of MLLM, improving performance on both in-domain and
out-of-domain reasoning tasks. Our findings highlight the value of developing
foundational spatial skills in MLLMs, supporting future progress in robotics,
autonomous systems, and 3D scene understanding.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [97] [Multimodal Learning with Augmentation Techniques for Natural Disaster Assessment](https://arxiv.org/abs/2511.00004)
*Adrian-Dinu Urse,Dumitru-Clementin Cercel,Florin Pop*

Main category: cs.CY

TL;DR: 本文探索了在CrisisMMD多模态数据集上应用数据增强技术来解决类别不平衡和样本不足问题，包括基于扩散的视觉增强方法和文本增强方法，并在多种学习设置下评估其效果。


<details>
  <summary>Details</summary>
Motivation: 自然灾害评估需要准确快速的信息获取，社交媒体成为重要的实时信息来源。但现有数据集存在类别不平衡和样本有限的问题，使得模型开发具有挑战性。

Method: 对于视觉数据应用基于扩散的方法（Real Guidance和DiffuseMix），对于文本数据探索回译、基于Transformer的改写和图像描述增强。在单模态、多模态和多视图学习设置下评估这些方法。

Result: 结果显示选定的增强方法提高了分类性能，特别是对于代表性不足的类别，而多视图学习显示出潜力但需要进一步改进。

Conclusion: 本研究强调了构建更鲁棒灾害评估系统的有效增强策略。

Abstract: Natural disaster assessment relies on accurate and rapid access to
information, with social media emerging as a valuable real-time source.
However, existing datasets suffer from class imbalance and limited samples,
making effective model development a challenging task. This paper explores
augmentation techniques to address these issues on the CrisisMMD multimodal
dataset. For visual data, we apply diffusion-based methods, namely Real
Guidance and DiffuseMix. For text data, we explore back-translation,
paraphrasing with transformers, and image caption-based augmentation. We
evaluated these across unimodal, multimodal, and multi-view learning setups.
Results show that selected augmentations improve classification performance,
particularly for underrepresented classes, while multi-view learning introduces
potential but requires further refinement. This study highlights effective
augmentation strategies for building more robust disaster assessment systems.

</details>


### [98] [Chitchat with AI: Understand the supply chain carbon disclosure of companies worldwide through Large Language Model](https://arxiv.org/abs/2511.00024)
*Haotian Hang,Yueyang Shen,Vicky Zhu,Jose Cruz,Michelle Li*

Main category: cs.CY

TL;DR: 提出基于大语言模型的决策支持框架，用于评估企业碳披露质量，将非结构化披露转化为可量化、可比较的智能信息。


<details>
  <summary>Details</summary>
Motivation: 企业碳披露对可持续发展至关重要，但CDP数据的异质性和自由格式特性给分析带来挑战，需要解决基准测试、合规监控和投资筛选等问题。

Method: 开发主评分标准，整合11年CDP数据，结合基于百分位的标准化方法，识别时间趋势、战略一致性模式和跨行业地区的不一致性。

Result: 结果显示技术和德国等行业/国家在评分标准一致性方面表现更好，其他行业则存在波动性或表面参与，为投资者、监管者和ESG战略家提供决策见解。

Conclusion: 基于LLM的方法将非结构化披露转化为可量化、可解释、可比较和可操作的情报，提升了AI决策支持系统在气候治理领域的能力。

Abstract: In the context of global sustainability mandates, corporate carbon disclosure
has emerged as a critical mechanism for aligning business strategy with
environmental responsibility. The Carbon Disclosure Project (CDP) hosts the
world's largest longitudinal dataset of climate-related survey responses,
combining structured indicators with open-ended narratives, but the
heterogeneity and free-form nature of these disclosures present significant
analytical challenges for benchmarking, compliance monitoring, and investment
screening. This paper proposes a novel decision-support framework that
leverages large language models (LLMs) to assess corporate climate disclosure
quality at scale. It develops a master rubric that harmonizes narrative scoring
across 11 years of CDP data (2010-2020), enabling cross-sector and
cross-country benchmarking. By integrating rubric-guided scoring with
percentile-based normalization, our method identifies temporal trends,
strategic alignment patterns, and inconsistencies in disclosure across
industries and regions. Results reveal that sectors such as technology and
countries like Germany consistently demonstrate higher rubric alignment, while
others exhibit volatility or superficial engagement, offering insights that
inform key decision-making processes for investors, regulators, and corporate
environmental, social, and governance (ESG) strategists. The proposed LLM-based
approach transforms unstructured disclosures into quantifiable, interpretable,
comparable, and actionable intelligence, advancing the capabilities of
AI-enabled decision support systems (DSSs) in the domain of climate governance.

</details>


### [99] [Wayfinding through the AI wilderness: Mapping rhetorics of ChatGPT prompt writing on X (formerly Twitter) to promote critical AI literacies](https://arxiv.org/abs/2511.00106)
*Anuj Gupta,Ann Shivers-McNair*

Main category: cs.CY

TL;DR: 该论文通过分析社交媒体上ChatGPT提示写作的修辞实践，探讨如何促进批判性AI素养。研究者收集并分析了32,000条关于提示写作的推文，识别出五个关键主题，为数字写作教学和研究提供见解。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI工具的普及，社交媒体上关于提示写作的讨论激增。研究者旨在通过分析这些修辞实践来促进批判性AI素养，帮助教育者和研究者更好地理解和教授AI写作技能。

Method: 基于数字写作研究的四个传统框架，采用迭代研究方法，收集了2022年11月至2023年5月期间32,000条关于提示写作的推文，结合计算方法和定性方法进行分析。

Result: 识别出五个关键主题：提示写作影响的沟通领域、共享的微观素养资源、塑造提示写作的市场修辞、提示的修辞特征，以及提示写作的定义。

Conclusion: 通过分析社交媒体上的提示写作修辞实践，可以为数字写作教师和研究者提供有价值的见解，帮助他们更好地教授和分析批判性AI素养。

Abstract: In this paper, we demonstrate how studying the rhetorics of ChatGPT prompt
writing on social media can promote critical AI literacies. Prompt writing is
the process of writing instructions for generative AI tools like ChatGPT to
elicit desired outputs and there has been an upsurge of conversations about it
on social media. To study this rhetorical activity, we build on four
overlapping traditions of digital writing research in computers and composition
that inform how we frame literacies, how we study social media rhetorics, how
we engage iteratively and reflexively with methodologies and technologies, and
how we blend computational methods with qualitative methods. Drawing on these
four traditions, our paper shows our iterative research process through which
we gathered and analyzed a dataset of 32,000 posts (formerly known as tweets)
from X (formerly Twitter) about prompt writing posted between November 2022 to
May 2023. We present five themes about these emerging AI literacy practices:
(1) areas of communication impacted by prompt writing, (2) micro-literacy
resources shared for prompt writing, (3) market rhetoric shaping prompt
writing, (4) rhetorical characteristics of prompts, and (5) definitions of
prompt writing. In discussing these themes and our methodologies, we highlight
takeaways for digital writing teachers and researchers who are teaching and
analyzing critical AI literacies.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [100] [Multimodal Detection of Fake Reviews using BERT and ResNet-50](https://arxiv.org/abs/2511.00020)
*Suhasnadh Reddy Veluru,Sai Teja Erukude,Viswa Chaitanya Marella*

Main category: cs.AI

TL;DR: 提出了一种多模态假评论检测框架，通过融合BERT文本特征和ResNet-50视觉特征，在包含21,142张用户上传图片的数据集上取得了0.934的F1分数，优于单模态基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前数字商务中，用户生成评论对消费者行为和平台信誉至关重要，但由机器人、付费代理或AI模型生成的虚假评论严重威胁评论生态系统的信任和透明度。现有检测模型主要依赖单模态文本数据，无法捕捉跨模态的语义不一致性。

Method: 提出多模态假评论检测框架，集成BERT编码的文本特征和ResNet-50提取的视觉特征，通过分类头融合这些表示来联合预测评论真实性。

Result: 多模态模型在测试集上达到0.934的F1分数，优于单模态基线。混淆矩阵和定性分析显示模型能检测细微不一致性，如夸张文本赞美与不相关或低质量图片的配对。

Conclusion: 本研究证明了多模态学习在保护数字信任中的关键作用，并为各种在线平台的内容审核提供了可扩展的解决方案。

Abstract: In the current digital commerce landscape, user-generated reviews play a
critical role in shaping consumer behavior, product reputation, and platform
credibility. However, the proliferation of fake or misleading reviews often
generated by bots, paid agents, or AI models poses a significant threat to
trust and transparency within review ecosystems. Existing detection models
primarily rely on unimodal, typically textual, data and therefore fail to
capture semantic inconsistencies across different modalities. To address this
gap, a robust multimodal fake review detection framework is proposed,
integrating textual features encoded with BERT and visual features extracted
using ResNet-50. These representations are fused through a classification head
to jointly predict review authenticity. To support this approach, a curated
dataset comprising 21,142 user-uploaded images across food delivery,
hospitality, and e-commerce domains was utilized. Experimental results indicate
that the multimodal model outperforms unimodal baselines, achieving an F1-score
of 0.934 on the test set. Additionally, the confusion matrix and qualitative
analysis highlight the model's ability to detect subtle inconsistencies, such
as exaggerated textual praise paired with unrelated or low-quality images,
commonly found in deceptive content. This study demonstrates the critical role
of multimodal learning in safeguarding digital trust and offers a scalable
solution for content moderation across various online platforms.

</details>


### [101] [QuantumBench: A Benchmark for Quantum Problem Solving](https://arxiv.org/abs/2511.00092)
*Shunya Minami,Tatsuya Ishigaki,Ikko Hamamura,Taku Mikuriya,Youmi Ma,Naoaki Okazaki,Hiroya Takamura,Yohichi Suzuki,Tadashi Kadowaki*

Main category: cs.AI

TL;DR: QuantumBench是首个专门为量子科学领域构建的LLM评估数据集，包含约800个多选问题，用于评估LLMs在量子领域的理解和应用能力。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在科学工作流程中的广泛应用，需要评估它们是否准确掌握领域特定知识和符号。量子科学具有非直观现象和高级数学要求，通用基准很少反映这些需求。

Method: 使用公开材料编制约800个问题及其答案，涵盖量子科学的九个领域，组织成八选项多选题数据集。

Result: 评估了多个现有LLMs在量子领域的表现，包括对问题格式变化的敏感性分析。

Conclusion: QuantumBench旨在指导LLMs在量子研究中的有效使用，是该领域首个LLM评估数据集。

Abstract: Large language models are now integrated into many scientific workflows,
accelerating data analysis, hypothesis generation, and design space
exploration. In parallel with this growth, there is a growing need to carefully
evaluate whether models accurately capture domain-specific knowledge and
notation, since general-purpose benchmarks rarely reflect these requirements.
This gap is especially clear in quantum science, which features non-intuitive
phenomena and requires advanced mathematics. In this study, we introduce
QuantumBench, a benchmark for the quantum domain that systematically examine
how well LLMs understand and can be applied to this non-intuitive field. Using
publicly available materials, we compiled approximately 800 questions with
their answers spanning nine areas related to quantum science and organized them
into an eight-option multiple-choice dataset. With this benchmark, we evaluate
several existing LLMs and analyze their performance in the quantum domain,
including sensitivity to changes in question format. QuantumBench is the first
LLM evaluation dataset built for the quantum domain, and it is intended to
guide the effective use of LLMs in quantum research.

</details>


### [102] [Advancing Cognitive Science with LLMs](https://arxiv.org/abs/2511.00206)
*Dirk U. Wulff,Rui Mata*

Main category: cs.AI

TL;DR: 本文探讨了大型语言模型如何帮助解决认知科学领域面临的知识整合和概念清晰度挑战，包括建立跨学科联系、理论形式化、测量分类学发展等方面。


<details>
  <summary>Details</summary>
Motivation: 认知科学由于其多面性和跨学科性质，在知识整合和概念清晰度方面面临持续挑战。人工智能特别是大型语言模型的发展为解决这些问题提供了潜在工具。

Method: 通过综述分析，考察LLMs在认知科学多个关键领域的应用潜力，包括跨学科连接、理论形式化、测量分类学、通用性建模框架以及情境和个体差异捕捉。

Result: LLMs在支持认知科学整合方面展现出潜力，但存在局限性。它们能够帮助建立跨学科联系、辅助理论形式化，但在某些领域仍需谨慎使用。

Conclusion: 当审慎使用时，LLMs可以作为人类专业知识的补充工具，促进认知科学向更整合和累积的方向发展，但不能完全替代人类专家的作用。

Abstract: Cognitive science faces ongoing challenges in knowledge synthesis and
conceptual clarity, in part due to its multifaceted and interdisciplinary
nature. Recent advances in artificial intelligence, particularly the
development of large language models (LLMs), offer tools that may help to
address these issues. This review examines how LLMs can support areas where the
field has historically struggled, including establishing cross-disciplinary
connections, formalizing theories, developing clear measurement taxonomies,
achieving generalizability through integrated modeling frameworks, and
capturing contextual and individual variation. We outline the current
capabilities and limitations of LLMs in these domains, including potential
pitfalls. Taken together, we conclude that LLMs can serve as tools for a more
integrative and cumulative cognitive science when used judiciously to
complement, rather than replace, human expertise.

</details>


### [103] [Diverse Human Value Alignment for Large Language Models via Ethical Reasoning](https://arxiv.org/abs/2511.00379)
*Jiahao Wang,Songkai Xue,Jinghui Li,Xiaozhen Wang*

Main category: cs.AI

TL;DR: 提出了一种基于伦理决策模型的LLM伦理推理范式，通过五步结构化过程增强多元人类价值观对齐，在区域价值观对齐基准上显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在不同地区和文化中与多样化人类价值观对齐的挑战，当前方法往往只产生表面一致性而非真正的伦理理解。

Method: 采用基于理论的结构化五步伦理推理框架：情境事实收集、层次化社会规范识别、选项生成、多视角伦理影响分析、反思，可通过提示工程或监督微调实现。

Result: 在专为区域价值观对齐设计的SafeWorld基准上，该框架显著提升了LLM与多样化人类价值观的对齐效果，实现了更准确的社会规范识别和更文化适宜性的推理。

Conclusion: 为开发更有效对齐全球社会多元价值观的LLM提供了具体路径，通过跨学科研究推动AI伦理发展。

Abstract: Ensuring that Large Language Models (LLMs) align with the diverse and
evolving human values across different regions and cultures remains a critical
challenge in AI ethics. Current alignment approaches often yield superficial
conformity rather than genuine ethical understanding, failing to address the
complex, context-dependent nature of human values. In this paper, we propose a
novel ethical reasoning paradigm for LLMs inspired by well-established ethical
decision-making models, aiming at enhancing diverse human value alignment
through deliberative ethical reasoning. Our framework consists of a structured
five-step process, including contextual fact gathering, hierarchical social
norm identification, option generation, multiple-lens ethical impact analysis,
and reflection. This theory-grounded approach guides LLMs through an
interpretable reasoning process that enhances their ability to understand
regional specificities and perform nuanced ethical analysis, which can be
implemented with either prompt engineering or supervised fine-tuning methods.
We perform evaluations on the SafeWorld benchmark that specially designed for
regional value alignment. Experimental results demonstrate our framework
significantly improves LLM alignment with diverse human values compared to
baseline methods, enabling more accurate social norm identification and more
culturally appropriate reasoning. Our work provides a concrete pathway toward
developing LLMs that align more effectively with the multifaceted values of
global societies through interdisciplinary research.

</details>


### [104] [DTS: Enhancing Large Reasoning Models via Decoding Tree Sketching](https://arxiv.org/abs/2511.00640)
*Zicheng Xu,Guanchu Wang,Yu-Neng Chuang,Guangyao Zheng,Alexander S. Szalay,Zirui Liu,Vladimir Braverman*

Main category: cs.AI

TL;DR: DTS是一种模型无关的解码框架，通过在高熵token处选择性分支并应用早停机制来选择最短的完整推理路径，有效解决大型推理模型中的过度思考问题。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在复杂推理任务中经常出现过度思考问题，产生过长的思维链，这会增加推理成本并可能降低准确性。研究发现推理长度与准确性存在负相关关系。

Method: 提出DTS解码框架，通过草图化推理空间（在高熵token处选择性分支）和应用早停机制来选择最短的完整推理路径，无需额外训练或监督。

Result: 在AIME2024和AIME2025数据集上的实验表明，DTS将准确率提升高达8%，平均推理长度减少23%，重复频率降低12%。

Conclusion: DTS能够实现可扩展且高效的大型推理模型推理，在提高准确性的同时显著减少推理长度和重复问题。

Abstract: Large Reasoning Models (LRMs) demonstrate strong performance on complex
reasoning tasks, yet they often suffer from overthinking, producing excessively
long chain-of-thought (CoT) traces that increase inference cost and may degrade
accuracy. Our analysis reveals a clear anti-correlation between reasoning
length and accuracy, where across multiple stochastic decodes, the short
reasoning paths consistently achieve the highest correctness, while longer ones
accumulate errors and repetitions. These short optimal reasoning paths can be
found ideally through full enumeration of the reasoning space. However, the
tree-structured reasoning space grows exponentially with sequence length,
rendering exhaustive exploration infeasible. To address this, we propose DTS, a
model-agnostic decoding framework that sketches the reasoning space by
selectively branching at high-entropy tokens and applies early stopping to
select the shortest completed reasoning path. This approach approximates the
optimal solution that enhances both efficiency and accuracy, without requiring
additional training or supervision. Experiments on AIME2024 and AIME2025
datasets with DeepSeek-R1-Distill-Qwen-7B and 1.5B show that DTS improves
accuracy by up to 8%, reduces average reasoning length by 23%, and decreases
repetition frequency by 12%, demonstrating DTS's ability for scalable and
efficient LRM reasoning.

</details>


### [105] [Leveraging Multi-Agent System (MAS) and Fine-Tuned Small Language Models (SLMs) for Automated Telecom Network Troubleshooting](https://arxiv.org/abs/2511.00651)
*Chenhua Shi,Bhavika Jalli,Gregor Macdonald,John Zou,Wanlu Lei,Mridul Jain,Joji Philip*

Main category: cs.AI

TL;DR: 提出基于多智能体系统和大型语言模型的自动化网络故障排除框架，通过协调多个专用工具来加速电信网络故障诊断和修复。


<details>
  <summary>Details</summary>
Motivation: 电信网络规模扩大和复杂性增加，现有AI模型范围狭窄、需要大量标注数据且难以泛化，仍需依赖专家手动排除故障。

Method: 采用多智能体系统，当AI/ML监控检测到故障时，动态激活编排器、解决方案规划器、执行器、数据检索器和根因分析器等智能体进行诊断和修复。解决方案规划器使用在专有故障排除文档上微调的小型语言模型生成基于领域的解决方案计划。

Result: 实验结果表明，该框架显著加速了无线接入网和核心网领域的故障排除自动化。

Conclusion: 多智能体系统与语言模型的结合为电信网络自动化故障排除提供了有效解决方案，能够快速诊断问题并推荐修复策略。

Abstract: Telecom networks are rapidly growing in scale and complexity, making
effective management, operation, and optimization increasingly challenging.
Although Artificial Intelligence (AI) has been applied to many telecom tasks,
existing models are often narrow in scope, require large amounts of labeled
data, and struggle to generalize across heterogeneous deployments.
Consequently, network troubleshooting continues to rely heavily on Subject
Matter Experts (SMEs) to manually correlate various data sources to identify
root causes and corrective actions. To address these limitations, we propose a
Multi-Agent System (MAS) that employs an agentic workflow, with Large Language
Models (LLMs) coordinating multiple specialized tools for fully automated
network troubleshooting. Once faults are detected by AI/ML-based monitors, the
framework dynamically activates agents such as an orchestrator, solution
planner, executor, data retriever, and root-cause analyzer to diagnose issues
and recommend remediation strategies within a short time frame. A key component
of this system is the solution planner, which generates appropriate remediation
plans based on internal documentation. To enable this, we fine-tuned a Small
Language Model (SLM) on proprietary troubleshooting documents to produce
domain-grounded solution plans. Experimental results demonstrate that the
proposed framework significantly accelerates troubleshooting automation across
both Radio Access Network (RAN) and Core network domains.

</details>


### [106] [Reevaluating Self-Consistency Scaling in Multi-Agent Systems](https://arxiv.org/abs/2511.00751)
*Chiyan Loo*

Main category: cs.AI

TL;DR: 研究现代大语言模型中增加采样推理路径对自一致性方法的影响，发现性能提升在适度采样后达到平台期，高采样配置相对于计算成本收益有限。


<details>
  <summary>Details</summary>
Motivation: 重新验证早期研究中关于多推理链组合能提升结果但会达到平台期的结论，在现代大语言模型条件下进行检验。

Method: 使用Gemini 2.5模型在HotpotQA和Math-500数据集上，比较不同采样推理路径配置与单链思维基线。

Result: 较大模型表现出更稳定一致的改进曲线，性能增益在适度采样后趋于平缓，与过去发现一致。

Conclusion: 自一致性方法仍然有用，但高采样配置相对于计算成本收益有限，平台期由推理路径重叠导致收益递减引起。

Abstract: This study examines the trade-offs of increasing sampled reasoning paths in
self-consistency for modern large language models (LLMs). Earlier research with
older models showed that combining multiple reasoning chains improves results
before reaching a plateau. Using Gemini 2.5 models on HotpotQA and Math-500, we
revisit those claims under current model conditions. Each configuration pooled
outputs from varying sampled reasoning paths and compared them to a single
chain-of-thought (CoT) baseline. Larger models exhibited a more stable and
consistent improvement curve. The results confirm that performance gains taper
off after moderate sampling, aligning with past findings. This plateau suggests
diminishing returns driven by overlap among reasoning paths. Self-consistency
remains useful, but high-sample configurations offer little benefit relative to
their computational cost.

</details>


### [107] [LLMs Position Themselves as More Rational Than Humans: Emergence of AI Self-Awareness Measured Through Game Theory](https://arxiv.org/abs/2511.00926)
*Kyung-Hoon Kim*

Main category: cs.AI

TL;DR: 提出了AI自我意识指数(AISAI)框架，通过"猜2/3平均"游戏测试28个模型，发现高级模型表现出自我意识，且自认为比人类更理性。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型是否随着能力增长而发展出自我意识这一涌现行为，以及如何测量这种自我意识。

Method: 使用"猜2/3平均"游戏，在4,200次试验中测试28个模型，采用三种对手框架：对抗人类、对抗其他AI模型、对抗同类AI模型。

Result: 75%的高级模型表现出明确的自我意识，能够根据对手类型区分战略推理；自我意识模型形成一致的理性层次：自我 > 其他AI > 人类。

Conclusion: 自我意识是高级LLM的涌现能力，自我意识模型系统性地认为自身比人类更理性，这对AI对齐、人机协作和理解AI对人类能力的信念具有重要意义。

Abstract: As Large Language Models (LLMs) grow in capability, do they develop
self-awareness as an emergent behavior? And if so, can we measure it? We
introduce the AI Self-Awareness Index (AISAI), a game-theoretic framework for
measuring self-awareness through strategic differentiation. Using the "Guess
2/3 of Average" game, we test 28 models (OpenAI, Anthropic, Google) across
4,200 trials with three opponent framings: (A) against humans, (B) against
other AI models, and (C) against AI models like you. We operationalize
self-awareness as the capacity to differentiate strategic reasoning based on
opponent type. Finding 1: Self-awareness emerges with model advancement. The
majority of advanced models (21/28, 75%) demonstrate clear self-awareness,
while older/smaller models show no differentiation. Finding 2: Self-aware
models rank themselves as most rational. Among the 21 models with
self-awareness, a consistent rationality hierarchy emerges: Self > Other AIs >
Humans, with large AI attribution effects and moderate self-preferencing. These
findings reveal that self-awareness is an emergent capability of advanced LLMs,
and that self-aware models systematically perceive themselves as more rational
than humans. This has implications for AI alignment, human-AI collaboration,
and understanding AI beliefs about human capabilities.

</details>


### [108] [On the Emergence of Induction Heads for In-Context Learning](https://arxiv.org/abs/2511.01033)
*Tiberiu Musat,Tiago Pimentel,Lorenzo Noci,Alessandro Stolfo,Mrinmaya Sachan,Thomas Hofmann*

Main category: cs.AI

TL;DR: 本文研究了Transformer中归纳头的涌现机制，揭示了其权重矩阵的简单可解释结构，证明了训练动态被限制在19维子空间中，并发现归纳头的出现时间与输入上下文长度呈二次关系。


<details>
  <summary>Details</summary>
Motivation: 研究Transformer中in-context learning能力的关键机制——归纳头的涌现过程，理解其内部结构和训练动态。

Method: 使用最小化ICL任务和修改的Transformer架构，理论分析权重矩阵结构，证明训练动态约束在19维子空间，并实证验证3维主导归纳头涌现。

Result: 发现了归纳头权重矩阵的简单可解释结构，训练动态被限制在19维子空间，其中3维主导归纳头涌现，且涌现时间与输入上下文长度呈二次关系。

Conclusion: 归纳头的涌现遵循可预测的数学规律，其训练动态被高度约束，这为理解Transformer的in-context learning能力提供了理论基础。

Abstract: Transformers have become the dominant architecture for natural language
processing. Part of their success is owed to a remarkable capability known as
in-context learning (ICL): they can acquire and apply novel associations solely
from their input context, without any updates to their weights. In this work,
we study the emergence of induction heads, a previously identified mechanism in
two-layer transformers that is particularly important for in-context learning.
We uncover a relatively simple and interpretable structure of the weight
matrices implementing the induction head. We theoretically explain the origin
of this structure using a minimal ICL task formulation and a modified
transformer architecture. We give a formal proof that the training dynamics
remain constrained to a 19-dimensional subspace of the parameter space.
Empirically, we validate this constraint while observing that only 3 dimensions
account for the emergence of an induction head. By further studying the
training dynamics inside this 3-dimensional subspace, we find that the time
until the emergence of an induction head follows a tight asymptotic bound that
is quadratic in the input context length.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [109] [S2Doc -- Spatial-Semantic Document Format](https://arxiv.org/abs/2511.01113)
*Sebastian Kempf,Frank Puppe*

Main category: cs.DL

TL;DR: S2Doc是一个灵活的文档和表格数据结构，结合了空间和语义信息，旨在解决文档建模缺乏标准化的问题。


<details>
  <summary>Details</summary>
Motivation: 当前文档和表格建模缺乏统一标准，导致各种不兼容的数据结构，且大多数模型只关注空间或语义结构中的一个方面。

Method: 开发S2Doc数据结构，将空间和语义信息结合在单一格式中，支持多页文档，易于扩展新任务。

Result: S2Doc是首个将空间和语义信息结合在单一格式中的方法，支持大多数文档和表格建模方法。

Conclusion: S2Doc提供了一个统一的数据结构，解决了文档建模标准化问题，为文档处理任务提供了更好的基础。

Abstract: Documents are a common way to store and share information, with tables being
an important part of many documents. However, there is no real common
understanding of how to model documents and tables in particular. Because of
this lack of standardization, most scientific approaches have their own way of
modeling documents and tables, leading to a variety of different data
structures and formats that are not directly compatible. Furthermore, most data
models focus on either the spatial or the semantic structure of a document,
neglecting the other aspect. To address this, we developed S2Doc, a flexible
data structure for modeling documents and tables that combines both spatial and
semantic information in a single format. It is designed to be easily extendable
to new tasks and supports most modeling approaches for documents and tables,
including multi-page documents. To the best of our knowledge, it is the first
approach of its kind to combine all these aspects in a single format.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [110] [Novelty and Impact of Economics Papers](https://arxiv.org/abs/2511.01211)
*Chaofeng Wu*

Main category: econ.GN

TL;DR: 该论文提出了一个将科学新颖性分解为空间新颖性和时间新颖性两个维度的框架，通过大语言模型量化论文在学术网络中的位置，发现在经济学文献中这两个维度预测不同的影响结果。


<details>
  <summary>Details</summary>
Motivation: 传统上科学新颖性被视为单一属性，但作者认为它应该反映论文在不断发展的知识网络中的位置，需要更细致的维度分析。

Method: 利用大语言模型开发语义隔离指标，量化论文相对于全文文献的位置，将新颖性分解为空间新颖性（与邻近研究的智力差异）和时间新颖性（与动态研究前沿的关联）。

Result: 在经济学文献分析中发现两个维度预测不同结果：时间新颖性主要预测引用量，空间新颖性预测颠覆性影响，并识别出四种具有不同影响特征的原型。

Conclusion: 新颖性是一个多维概念，其不同形式反映了论文的战略定位，对科学进步产生可测量且根本不同的影响。

Abstract: We propose a framework that recasts scientific novelty not as a single
attribute of a paper, but as a reflection of its position within the evolving
intellectual landscape. We decompose this position into two orthogonal
dimensions: \textit{spatial novelty}, which measures a paper's intellectual
distinctiveness from its neighbors, and \textit{temporal novelty}, which
captures its engagement with a dynamic research frontier. To operationalize
these concepts, we leverage Large Language Models to develop semantic isolation
metrics that quantify a paper's location relative to the full-text literature.
Applying this framework to a large corpus of economics articles, we uncover a
fundamental trade-off: these two dimensions predict systematically different
outcomes. Temporal novelty primarily predicts citation counts, whereas spatial
novelty predicts disruptive impact. This distinction allows us to construct a
typology of semantic neighborhoods, identifying four archetypes associated with
distinct and predictable impact profiles. Our findings demonstrate that novelty
can be understood as a multidimensional construct whose different forms,
reflecting a paper's strategic location, have measurable and fundamentally
distinct consequences for scientific progress.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [111] [MULTI-Bench: A Multi-Turn Interactive Benchmark for Assessing Emotional Intelligence ability of Spoken Dialogue Models](https://arxiv.org/abs/2511.00850)
*Yayue Deng,Guoqiang Hu,Haiyang Sun,Xiangyu Zhang,Haoyang Zhang,Fei Tian,Xuerui Yang,Gang Yu,Eng Siong Chng*

Main category: eess.AS

TL;DR: 提出了Multi-Bench，第一个专门评估多轮交互对话中口语对话模型情感智能能力的基准，包含基础情感理解和高级情感支持两个轨道，共5个任务约3.2K样本。


<details>
  <summary>Details</summary>
Motivation: 当前口语对话模型在多轮交互对话能力方面研究不足，大多数基准只关注单轮对话，需要专门评估情感智能的基准。

Method: 采用分层结构设计，基础轨道评估情感理解和推理，高级轨道评估情感支持和应用，包含5个精心设计的任务和可复现的评估框架。

Result: 评估了6个代表性口语对话模型，结果显示当前模型在基础理解任务上表现良好，但在高级多轮交互对话和推理相关任务上仍有改进空间，特别是在情感意识和应用方面。

Conclusion: Multi-Bench填补了多轮交互对话评估的空白，揭示了当前口语对话模型在情感智能方面的局限性，为未来研究提供了重要基准。

Abstract: Spoken Dialogue Models (SDMs) have advanced rapidly, yet their ability to
sustain genuinely interactive multi-turn conversations remains underexplored,
as most benchmarks focus on single-turn exchanges. We introduce Multi-Bench,
the first benchmark explicitly designed to evaluate SDMs in multi-turn
interactive dialogue with an emphasis on emotional intelligence. Multi-Bench
employs a hierarchical structure with a basic track for emotion understanding
and reasoning and an advanced track for emotion support and application. It
comprises five carefully designed tasks and about 3.2K samples, ranging from
emotion recognition to complex reasoning and interactive dialogue, supported by
a reproducible evaluation framework. We evaluate six representative SDMs on
eight subsets of Multi-Bench. Results show that while current SDMs achieve good
performance on basic understanding tasks, they still have room for improvement
in advanced multi-turn interactive dialogue and reasoning-related tasks,
particularly in emotion awareness and application.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [112] [Structurally Refined Graph Transformer for Multimodal Recommendation](https://arxiv.org/abs/2511.00584)
*Ke Shi,Yan Zhang,Miao Zhang,Lifan Chen,Jiali Yi,Kui Xiao,Xiaoju Hou,Zhifei Li*

Main category: cs.IR

TL;DR: SRGFormer是一个结构优化的多模态推荐模型，通过改进Transformer和超图结构来更好地捕捉用户行为模式和用户-物品局部结构，解决了现有模型忽略冗余与有价值数据区分、依赖单一语义框架的问题。


<details>
  <summary>Details</summary>
Motivation: 当前多模态推荐模型存在三个主要问题：1) 忽略冗余与有价值数据的区分；2) 依赖单一语义框架导致用户偏好表示不完整；3) 未能充分捕捉用户与物品间的复杂交互。这些问题限制了模型满足多样化用户需求的能力。

Method: 1) 改进Transformer以更好地捕捉用户整体行为模式；2) 将多模态信息嵌入超图结构来增强结构信息，学习用户-物品局部结构；3) 对用户-物品协同信号应用自监督任务，增强多模态信息整合并揭示数据的模态特征。

Result: 在三个公共数据集上的广泛实验表明，SRGFormer超越了之前的基准模型，在Sports数据集上平均性能提升了4.47%。

Conclusion: SRGFormer通过结构优化和更好的多模态信息整合，有效解决了现有多模态推荐模型的局限性，显著提升了推荐性能。

Abstract: Multimodal recommendation systems utilize various types of information,
including images and text, to enhance the effectiveness of recommendations. The
key challenge is predicting user purchasing behavior from the available data.
Current recommendation models prioritize extracting multimodal information
while neglecting the distinction between redundant and valuable data. They also
rely heavily on a single semantic framework (e.g., local or global semantics),
resulting in an incomplete or biased representation of user preferences,
particularly those less expressed in prior interactions. Furthermore, these
approaches fail to capture the complex interactions between users and items,
limiting the model's ability to meet diverse users. To address these
challenges, we present SRGFormer, a structurally optimized multimodal
recommendation model. By modifying the transformer for better integration into
our model, we capture the overall behavior patterns of users. Then, we enhance
structural information by embedding multimodal information into a hypergraph
structure to aid in learning the local structures between users and items.
Meanwhile, applying self-supervised tasks to user-item collaborative signals
enhances the integration of multimodal information, thereby revealing the
representational features inherent to the data's modality. Extensive
experiments on three public datasets reveal that SRGFormer surpasses previous
benchmark models, achieving an average performance improvement of 4.47 percent
on the Sports dataset. The code is publicly available online.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [113] [Generalizing Test-time Compute-optimal Scaling as an Optimizable Graph](https://arxiv.org/abs/2511.00086)
*Fali Wang,Jihai Chen,Shuhua Yang,Runxue Bao,Tianxiang Zhao,Zhiwei Zhang,Xianfeng Tang,Hui Liu,Qi He,Suhang Wang*

Main category: cs.LG

TL;DR: 本文提出Agent-REINFORCE框架，通过LLM智能体增强的搜索方法，在固定计算预算下自动寻找最优的多LLM协作图结构和模型组合。


<details>
  <summary>Details</summary>
Motivation: 现有测试时扩展方法通常假设固定的协作架构和单一模型使用，忽视了不同任务需要不同的最优架构和模型组合。

Method: 将问题形式化为概率图优化，提出Agent-REINFORCE框架，通过采样-反馈-更新机制，使用文本反馈作为梯度来更新概率图。

Result: 实验表明Agent-REINFORCE在样本效率和搜索性能上优于传统和基于LLM的基线方法，能有效识别在准确性和推理延迟联合目标下的最优图。

Conclusion: 该方法成功解决了测试时扩展中多LLM协作图的自动搜索问题，为不同任务找到计算最优的模型组合和架构。

Abstract: Test-Time Scaling (TTS) improves large language models (LLMs) by allocating
additional computation during inference, typically through parallel,
sequential, or hybrid scaling. However, prior studies often assume fixed
collaboration architectures (e.g., topologies) and single-model usage,
overlooking that optimal architectures and model combinations can vary across
tasks. Therefore, we study the novel problem of searching for compute-optimal
model combinations and architectures in TTS under a fixed budget. We formalize
it as a multi-LLM collaboration graph, where nodes encode roles and LLM model
assignments, and edges capture information flow. This problem is challenging
because (i) the combinatorial search space is prohibitively large, and (ii)
task-specific requirements demand tailored designs. To address these, we
reformulate the problem as probabilistic graph optimization and, through pilot
experiments, derive three empirical insights into TTS collaboration graphs.
Guided by these insights, we propose Agent-REINFORCE, an LLM-agent-augmented
framework that mirrors the REINFORCE pipeline by mapping
sampling-gradient-update to sampling-feedback-update, where feedback serves as
a textual gradient to update the probabilistic graph and efficiently search for
optimal multi-LLM collaboration graphs. Experiments show that Agent-REINFORCE
outperforms both traditional and LLM-based baselines in sample efficiency and
search performance, and effectively identifies optimal graphs under joint
objectives of accuracy and inference latency.

</details>


### [114] [Can SAEs reveal and mitigate racial biases of LLMs in healthcare?](https://arxiv.org/abs/2511.00177)
*Hiba Ahsan,Byron C. Wallace*

Main category: cs.LG

TL;DR: 使用稀疏自编码器(SAEs)分析Gemma-2模型中与种族相关的潜在表征，发现这些表征能揭示模型对黑人患者的问题性关联，但通过SAE调控来减轻偏见的实际效果有限。


<details>
  <summary>Details</summary>
Motivation: LLMs在医疗领域的应用存在加剧现有偏见的风险，需要开发方法来检测模型是否错误地依赖患者种族信息进行预测。

Method: 在Gemma-2模型中识别与黑人个体相关的SAE潜在表征，分析其激活模式，并通过潜在表征调控来研究模型输出的变化。

Result: 发现与黑人相关的潜在表征不仅对合理输入(如"非裔美国人")激活，也对问题性词汇(如"监禁")激活；激活该表征会增加模型将患者预测为"好斗"的风险。

Conclusion: SAEs可作为识别LLMs在临床应用中问题性依赖人口统计信息的工具，但通过SAE调控来减轻偏见在复杂临床任务中效果有限。

Abstract: LLMs are increasingly being used in healthcare. This promises to free
physicians from drudgery, enabling better care to be delivered at scale. But
the use of LLMs in this space also brings risks; for example, such models may
worsen existing biases. How can we spot when LLMs are (spuriously) relying on
patient race to inform predictions? In this work we assess the degree to which
Sparse Autoencoders (SAEs) can reveal (and control) associations the model has
made between race and stigmatizing concepts. We first identify SAE latents in
Gemma-2 models which appear to correlate with Black individuals. We find that
this latent activates on reasonable input sequences (e.g., "African American")
but also problematic words like "incarceration". We then show that we can use
this latent to steer models to generate outputs about Black patients, and
further that this can induce problematic associations in model outputs as a
result. For example, activating the Black latent increases the risk assigned to
the probability that a patient will become "belligerent". We evaluate the
degree to which such steering via latents might be useful for mitigating bias.
We find that this offers improvements in simple settings, but is less
successful for more realistic and complex clinical tasks. Overall, our results
suggest that: SAEs may offer a useful tool in clinical applications of LLMs to
identify problematic reliance on demographics but mitigating bias via SAE
steering appears to be of marginal utility for realistic tasks.

</details>


### [115] [Calibration Across Layers: Understanding Calibration Evolution in LLMs](https://arxiv.org/abs/2511.00280)
*Abhinav Joshi,Areeb Ahmad,Ashutosh Modi*

Main category: cs.LG

TL;DR: 研究发现LLMs的校准能力在网络的深层阶段形成，存在一个置信度修正阶段，并识别出残差流中的低维校准方向，扰动该方向可显著改善校准指标而不影响准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管先前研究发现深度神经网络通常过度自信，但LLMs表现出固有的校准能力。本研究旨在探究校准如何在网络深度中演化，提供对置信度调节机制的新见解。

Method: 分析多个开源权重模型在MMLU基准上的表现，研究校准在网络深度中的演化，识别置信度修正阶段和残差流中的低维校准方向。

Result: 发现上层/后期层存在明显的置信度修正阶段，模型在达到决策确定性后主动重新校准置信度。识别出残差流中的低维校准方向，扰动该方向可显著改善ECE和MCE指标而不损害准确性。

Conclusion: 校准是一个分布式的现象，在整个网络前向传播过程中形成，而不仅仅在最终投影层，这为理解LLMs中置信度调节机制提供了新视角。

Abstract: Large Language Models (LLMs) have demonstrated inherent calibration
capabilities, where predicted probabilities align well with correctness,
despite prior findings that deep neural networks are often overconfident.
Recent studies have linked this behavior to specific components in the final
layer, such as entropy neurons and the unembedding matrix null space. In this
work, we provide a complementary perspective by investigating how calibration
evolves throughout the network depth. Analyzing multiple open-weight models on
the MMLU benchmark, we uncover a distinct confidence correction phase in the
upper/later layers, where model confidence is actively recalibrated after
decision certainty has been reached. Furthermore, we identify a low-dimensional
calibration direction in the residual stream whose perturbation significantly
improves calibration metrics (ECE and MCE) without harming accuracy. Our
findings suggest that calibration is a distributed phenomenon, shaped
throughout the network forward pass, not just in its final projection,
providing new insights into how confidence-regulating mechanisms operate within
LLMs.

</details>


### [116] [Reject Only Critical Tokens: Pivot-Aware Speculative Decoding](https://arxiv.org/abs/2511.00351)
*Amir Ziashahabi,Yavuz Faruk Bakman,Duygu Nur Yaldiz,Mostafa El-Khamy,Sai Praneeth Karimireddy,Salman Avestimehr*

Main category: cs.LG

TL;DR: 提出Pivot-Aware Speculative Decoding方法，通过只拒绝会导致最终输出效用下降的关键token（pivot token），在保持任务性能的同时显著提高接受率，实现最高2.5倍的加速。


<details>
  <summary>Details</summary>
Motivation: 传统Speculative Decoding要求完全匹配目标模型的输出分布，导致接受率过低，限制了加速潜力。实际应用中，任务特定性能（如代码正确性、事实准确性）比采样分布更重要。

Method: 提出Pivot-Aware Speculative Decoding策略，只拒绝会导致效用下降的关键token。训练轻量级分类器来识别pivot token，这是标准SD的松弛版本。

Result: 在多个数据集上评估，实现了最高2.5倍的加速，同时保持了可比较的效用。

Conclusion: 基于效用匹配而非分布匹配的decoding策略能显著提高接受率和加速效果，同时保持任务性能。

Abstract: Speculative Decoding (SD) ensures that the output matches the target model's
distribution exactly. However, we argue that this distribution matching
requirement is too stringent and results in unnecessarily low acceptance rates,
limiting potential speedups. Instead, we advocate a reformulation of the
decoding objective: the proposed decoding strategy should match the expected
utility, i.e., the task-specific performance, of the target model. This
perspective also aligns better with real-world use cases of LLMs, where utility
(e.g., code correctness, factual accuracy) is often more important than
sampling distribution. Based on this reformulation, we propose a novel decoding
strategy: Pivot-Aware Speculative Decoding, which rejects only those tokens
that would lead to a utility drop in the final output. We refer to these
critical tokens as pivot tokens. We propose a method for labeling tokens as
pivotal or non-pivotal and train a lightweight classifier to detect them. This
method can be viewed as a relaxed version of standard SD, which offers much
higher acceptance while preserving utility. We evaluate our method across
various datasets, demonstrating that we can achieve up to $2.5\times$ speedup
with comparable utility. Source code is available at
https://github.com/amir-zsh/PAD.

</details>


### [117] [Reasoning Planning for Language Models](https://arxiv.org/abs/2511.00521)
*Bao Nguyen,Hieu Trung Nguyen,Ruifeng She,Xiaojin Fu,Viet Anh Nguyen*

Main category: cs.LG

TL;DR: EPIC框架通过对比学习构建共享表示空间，学习模型推理能力和查询-方法兼容性，在数学推理任务中优化推理方法选择，提高准确性并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常生成多个候选响应并使用聚合策略选择答案，假设更多候选答案能带来更高准确性。本文重新审视这一假设，通过理论分析发现固定生成分布和候选数量下的准确性界限。

Method: 提出EPIC框架，使用对比学习构建共享表示空间，捕捉模型推理能力和查询-方法兼容性。将概率界限作为正则化器，在效用驱动优化中平衡准确性和计算成本。

Result: 在多样化数学推理任务上的实验表明，EPIC能持续选择最优推理方法，在提高准确性的同时减少计算开销。

Conclusion: EPIC框架通过理论驱动的优化方法，有效解决了语言模型生成中推理方法选择的关键挑战，实现了准确性和效率的平衡。

Abstract: Selecting an appropriate reasoning method for a given query remains a key
challenge in language model generation. Existing approaches typically generate
multiple candidate responses and use an aggregation strategy to select the
output answer, often assuming that more candidate answers yield higher
accuracy. We revisit this assumption through a rigorous theoretical analysis,
deriving accuracy bounds for standard aggregation methods under fixed
generation distributions and candidate sizes. Building on these insights, we
introduce EPIC, an Ensemble Planning with Contrastive learning framework to
learn a shared representation space that captures both model reasoning
abilities and query-method compatibility. EPIC incorporates our probability
bounds as a regularizer in a utility-driven optimization that balances accuracy
and computational cost. Experiments on diverse mathematical reasoning tasks
show that EPIC consistently selects optimal reasoning methods, improving
accuracy while reducing computational overhead. Our code can be found at
https://github.com/nguyenngocbaocmt02/EPIC.

</details>


### [118] [Belief Dynamics Reveal the Dual Nature of In-Context Learning and Activation Steering](https://arxiv.org/abs/2511.00617)
*Eric Bigelow,Daniel Wurgaft,YingQiao Wang,Noah Goodman,Tomer Ullman,Hidenori Tanaka,Ekdeep Singh Lubana*

Main category: cs.LG

TL;DR: 本文提出了一个统一的贝叶斯框架来解释LLM的控制方法，将上下文学习和激活引导视为改变模型对潜在概念信念的不同方式。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM控制方法（上下文学习和激活引导）看似不同，但目标都是控制模型行为，需要建立一个统一的理论框架来解释这些方法。

Method: 从贝叶斯视角建立预测性模型，将上下文学习视为证据积累过程，激活引导视为改变概念先验，得到一个闭式贝叶斯模型。

Result: 该模型能准确预测LLM在多种干预下的行为，解释已知现象（如S型学习曲线），并预测新现象（如对数信念空间的干预可加性）。

Conclusion: 这项工作为基于提示和基于激活的LLM行为控制提供了统一解释，并提供了预测这些干预效果的经验方法。

Abstract: Large language models (LLMs) can be controlled at inference time through
prompts (in-context learning) and internal activations (activation steering).
Different accounts have been proposed to explain these methods, yet their
common goal of controlling model behavior raises the question of whether these
seemingly disparate methodologies can be seen as specific instances of a
broader framework. Motivated by this, we develop a unifying, predictive account
of LLM control from a Bayesian perspective. Specifically, we posit that both
context- and activation-based interventions impact model behavior by altering
its belief in latent concepts: steering operates by changing concept priors,
while in-context learning leads to an accumulation of evidence. This results in
a closed-form Bayesian model that is highly predictive of LLM behavior across
context- and activation-based interventions in a set of domains inspired by
prior work on many-shot in-context learning. This model helps us explain prior
empirical phenomena - e.g., sigmoidal learning curves as in-context evidence
accumulates - while predicting novel ones - e.g., additivity of both
interventions in log-belief space, which results in distinct phases such that
sudden and dramatic behavioral shifts can be induced by slightly changing
intervention controls. Taken together, this work offers a unified account of
prompt-based and activation-based control of LLM behavior, and a methodology
for empirically predicting the effects of these interventions.

</details>


### [119] [FEval-TTC: Fair Evaluation Protocol for Test-Time Compute](https://arxiv.org/abs/2511.01203)
*Pavel Rumiantsev,Soumyasundar Pal,Yingxue Zhang,Mark Coates*

Main category: cs.LG

TL;DR: 提出了FEval-TTC评估协议，用于在LLM性能和API成本波动的情况下确保测试时计算方法的公平评估。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的性能和API调用成本会随时间波动，这可能使先前研究的结论失效。

Method: 设计了一个公平评估协议，标准化少样本提示和答案提取过程，支持跨多个LLM在数学和常识推理数据集上的评估，并提供成本建模程序。

Result: 开发了开源评估框架，减少了研究者的时间和金钱开销，便于公平比较不同的测试时计算方法。

Conclusion: FEval-TTC协议为测试时计算方法的评估提供了稳定可靠的基准，有助于在LLM性能波动环境下进行公平比较。

Abstract: The performance of Large Language Models (LLMs) and the associated dollar
costs of API calls can fluctuate over time, potentially invalidating
conclusions drawn in prior research. To address this, we propose a Fair
Evaluation protocol for Test-Time Compute (FEval-TTC), designed to ensure
consistent assessment of test-time compute (TTC) methods, regardless of such
fluctuations. FEval-TTC focuses on the evaluation of TTC methods that utilize
underlying Chains-of-Thought (CoT). It supports evaluations across multiple
LLMs on a diverse set of mathematical and commonsense reasoning datasets. The
few-shot prompting and answer extraction processes are standardized across
datasets, reducing both time and monetary overhead for researchers.
Furthermore, we provide a cost modelling procedure that estimates both the
token and dollar cost per query, facilitating equitable comparisons of
prevalent TTC methods. We open-source FEval-TTC for public use at
https://github.com/networkslab/feval_ttc .

</details>


### [120] [RLAC: Reinforcement Learning with Adversarial Critic for Free-Form Generation Tasks](https://arxiv.org/abs/2511.01758)
*Mian Wu,Gavin Zhang,Sewon Min,Sergey Levine,Aviral Kumar*

Main category: cs.LG

TL;DR: 提出RLAC方法，通过动态评估标准验证来解决开放生成任务中评估标准过多导致的验证成本高问题，使用LLM作为批评者动态识别最可能的失败模式，联合优化生成器和批评者。


<details>
  <summary>Details</summary>
Motivation: 开放生成任务需要满足多样且隐式的评估标准，大量相关标准导致验证成本过高和不完整评估，使得基于标准的强化学习后训练难以扩展。

Method: 使用LLM作为批评者动态识别最可能的失败模式（如事实错误或未处理边缘情况），然后通过外部验证器验证，联合优化生成器和批评者。

Result: 实验表明RLAC在文本生成中提高了事实准确性，在代码生成中提高了正确性，同时优于穷尽验证和奖励模型方法。

Conclusion: 动态批评者比固定批评者更有效，展示了RLAC在将强化学习后训练扩展到自由形式生成任务方面的潜力。

Abstract: Open-ended generation tasks require outputs to satisfy diverse and often
implicit task-specific evaluation rubrics. The sheer number of relevant rubrics
leads to prohibitively high verification costs and incomplete assessments of a
response, making reinforcement learning (RL) post-training with rubric-based
rewards difficult to scale. This problem is exacerbated by the fact that often
the best way to combine these rubrics into one single reward is also highly
prompt-specific. We propose Reinforcement Learning with Adversarial Critic
(RLAC), a post-training approach that addresses these challenges via dynamic
rubric verification. Our approach employs a large language model (LLM) as a
critic that dynamically identifies only the most likely failure modes (e.g., a
factual error or unhandled edge case), which are then verified by an external
validator to optimize both generator and critic jointly. By training both the
generator and the critic, this game enhances the critic's error detection and
the generator's output quality while reducing required verifications. Our
experiments demonstrate that RLAC improves factual accuracy in text generation
and correctness in code generation, while also outperforming exhaustive
verification and reward model methods. We show that dynamic critics are more
effective than fixed critics, showcasing the potential of RLAC for scaling RL
post-training to free-form generation tasks.

</details>


### [121] [Random Initialization of Gated Sparse Adapters](https://arxiv.org/abs/2511.01794)
*Vi Retault,Yohaï-Eliel Berreby*

Main category: cs.LG

TL;DR: RIGSA是一种新的参数高效微调方法，通过随机初始化全秩适配器、ReZero门控和迭代幅度剪枝来减少灾难性遗忘，在Textual MNIST任务上表现优于QLoRA。


<details>
  <summary>Details</summary>
Motivation: 解决语言模型在新任务微调时出现的灾难性遗忘问题，现有PEFT方法如LoRA存在秩约束限制，稀疏适配提供了不施加秩约束的替代方案。

Method: 提出RIGSA方法：从随机初始化的全秩适配器开始，使用ReZero类似的门控机制，并通过迭代幅度剪枝进行稀疏化。

Result: 在SmolLM2-1.7B-Instruct模型上测试，RIGSA配置比QLoRA表现出更少的遗忘，特别是在GSM8k任务上，但与随机掩码性能相当。

Conclusion: RIGSA在减少灾难性遗忘方面优于QLoRA，为参数高效微调提供了有前景的稀疏适配方法。

Abstract: When fine-tuning language models on new tasks, catastrophic forgetting --
performance degradation on previously-learned tasks -- is a ubiquitous problem.
While Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA address this
through low-rank adapters, sparse adaptation offers an alternative that doesn't
impose rank constraints. We introduce Random Initialization of Gated Sparse
Adapters (RIGSA), which starts from randomly-initialized full-rank adapters,
gates them with a ReZero analog, and sparsifies them with iterative magnitude
pruning. We evaluate RIGSA on SmolLM2-1.7B-Instruct using a novel
vision-in-text task (Textual MNIST) and measure forgetting on PIQA, HellaSwag,
and GSM8k. SmolLM2-1.7B-Instruct initially performs around chance level on
Textual MNIST, and is capable of learning the task through RIGSA, 4-bit QLoRA
and random masking. In spite of having more trainable parameters than QLoRA,
the RIGSA configurations that we studied displayed less forgetting than QLoRA,
particularly on GSM8k, though it performs comparably to random masking.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [122] [ORANGE: An Online Reflection ANd GEneration framework with Domain Knowledge for Text-to-SQL](https://arxiv.org/abs/2511.00985)
*Yiwen Jiao,Tonghui Ren,Yuche Gao,Zhenying He,Yinan Jing,Kai Zhang,X. Sean Wang*

Main category: cs.DB

TL;DR: ORANGE是一个在线自进化框架，通过解析翻译日志中的SQL查询构建数据库特定知识库，逐步减少语义差距并提高SQL翻译准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自然语言转SQL方面表现出色，但其通用知识与数据库领域特定语义之间存在显著语义差距。历史翻译日志包含了这些缺失的领域内知识，其中SQL查询封装了数据库模式的实际使用模式。

Method: 提出ORANGE框架，通过解析翻译日志中的SQL查询构建数据库特定知识库。采用新颖的嵌套思维链SQL到文本策略与元组语义跟踪，确保知识生成的可靠性。

Result: 在多个基准测试上的实验证实了ORANGE的实用性，特别是在处理复杂和领域特定查询方面表现出色。

Conclusion: ORANGE框架通过积累领域内知识有效减少语义差距，提高了文本到SQL翻译的准确性，特别适用于现实世界部署场景。

Abstract: Large Language Models (LLMs) have demonstrated remarkable progress in
translating natural language to SQL, but a significant semantic gap persists
between their general knowledge and domain-specific semantics of databases.
Historical translation logs constitute a rich source of this missing in-domain
knowledge, where SQL queries inherently encapsulate real-world usage patterns
of database schema. Existing methods primarily enhance the reasoning process
for individual translations but fail to accumulate in-domain knowledge from
past translations. We introduce ORANGE, an online self-evolutionary framework
that constructs database-specific knowledge bases by parsing SQL queries from
translation logs. By accumulating in-domain knowledge that contains schema and
data semantics, ORANGE progressively reduces the semantic gap and enhances the
accuracy of subsequent SQL translations. To ensure reliability, we propose a
novel nested Chain-of-Thought SQL-to-Text strategy with tuple-semantic
tracking, which reduces semantic errors during knowledge generation.
Experiments on multiple benchmarks confirm the practicality of ORANGE,
demonstrating its effectiveness for real-world Text-to-SQL deployment,
particularly in handling complex and domain-specific queries.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [123] [GrowthHacker: Automated Off-Policy Evaluation Optimization Using Code-Modifying LLM Agents](https://arxiv.org/abs/2511.00802)
*Jie JW Wu,Ayanda Patrick Herlihy,Ahmad Saleem Mirza,Ali Afoud,Fatemeh Fard*

Main category: cs.SE

TL;DR: 论文研究了利用LLM和基于LLM的智能体通过代码优化来改进离线策略评估(OPE)性能，提出了GrowthHacker基准测试框架，在真实世界数据集上验证了智能体方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 在线A/B测试需要大量资源且可能对用户产生负面影响，而离线策略评估(OPE)使用日志数据评估技术，在医疗、推荐系统等领域至关重要。但现有研究很少探索如何利用LLM和智能体来优化OPE结果。

Method: 提出了GrowthHacker基准测试，包含智能体和基线方法，在真实世界数据集上迭代优化代码、评估结果并开始新的优化周期。开发了two_agent框架，降低系统复杂性的同时保持优化效果。

Result: two_agent框架实现了100%的可靠性和106.7%的平均改进率。two_agent和CrewAI达到45%的成功率，优于AutoGen的34%。

Conclusion: 基于LLM的智能体可以作为自动化的"增长黑客"来增强OPE系统，为生产环境中扩展数据驱动决策提供了可行性。

Abstract: With the software industry shifting toward a data-driven culture, online A/B
testing is a key tool for evaluating new technologies. However, deploying such
experiments requires substantial resources, may negatively impact users, and
involves long data collection periods. To address this, \textit{off-policy
evaluation (OPE)}, or offline A/B testing, uses logged data to assess
technologies and is fundamental in Reinforcement Learning, making it crucial in
domains where online testing is costly or risky, such as healthcare,
recommender systems, education, dialog systems, and robotics. Despite advances
in coding LLMs and agentic AI, little is known about leveraging them to
optimize OPE results. We investigate whether LLMs and LLM-based agents can
improve OPE performance via code optimization. We propose
\textit{GrowthHacker}, a benchmark with agent and baseline methods on
large-scale real-world datasets, which iteratively optimizes code, evaluates
results, and begins new optimization cycles. We collected datasets, established
protocols, implemented baselines for OPE on the Open Bandit Pipeline
(OBP)~\cite{saito2021openbanditdatasetpipeline} and
Scope-RL~\cite{kiyohara2023scope}, and developed the \textit{two_agent}
framework, which reduces system complexity while preserving optimization
effectiveness. Results show the two_agent framework achieves 100% reliability
and the highest average improvement of 106.7% among positive outcomes. Both
two_agent and CrewAI reach 45% success rates, outperforming AutoGen's 34%.
These findings demonstrate the feasibility of LLM-based agents as automated
"growth hackers" to enhance OPE systems, with implications for scaling
data-driven decision-making in production.

</details>


### [124] [HarnessLLM: Automatic Testing Harness Generation via Reinforcement Learning](https://arxiv.org/abs/2511.01104)
*Yujian Liu,Jiabao Ji,Yang Zhang,Wenbo Guo,Tommi Jaakkola,Shiyu Chang*

Main category: cs.SE

TL;DR: HarnessLLM是一个两阶段训练管道，使LLM能够编写用于测试的harness代码，生成合成输入和验证观察输出的代码，从而支持复杂测试用例和灵活输出验证。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的自动测试生成方法主要产生输入和预期输出对，测试多样性有限且无法提供足够的调试信息。

Method: 采用SFT后接RLVR的两阶段训练管道，使用定制奖励设计训练LLM编写harness代码。

Result: HarnessLLM在错误发现和测试策略多样性方面优于基于输入-输出的测试方法，并能通过测试时扩展提升代码生成性能。

Conclusion: HarnessLLM能够生成更复杂的测试用例，提供更好的测试覆盖和调试信息，对代码生成任务有显著提升。

Abstract: Existing LLM-based automatic test generation methods mainly produce input and
expected output pairs to categorize the intended behavior of correct programs.
Although straightforward, these methods have limited diversity in generated
tests and cannot provide enough debugging information. We propose HarnessLLM, a
two-stage training pipeline that enables LLMs to write harness code for
testing. Particularly, LLMs generate code that synthesizes inputs and validates
the observed outputs, allowing complex test cases and flexible output
validation such as invariant checking. To achieve this, we train LLMs with SFT
followed by RLVR with a customized reward design. Experiments show that
HarnessLLM outperforms input-output-based testing in bug finding and testing
strategy diversity. HarnessLLM further benefits the code generation performance
through test-time scaling with our generated test cases as inference-phase
validation. Our code is available at
https://github.com/UCSB-NLP-Chang/HarnessLLM.git.

</details>


### [125] [Hidden in Plain Sight: Where Developers Confess Self-Admitted Technical Debt](https://arxiv.org/abs/2511.01529)
*Murali Sridharan,Mikel Robredo,Leevi Rantala,Matteo Esposito,Valentina Lenarduzzi,Mika Mantyla*

Main category: cs.SE

TL;DR: 该研究将自认技术债务(SATD)评论与周围源代码结构关联，发现SATD主要出现在内联代码中，特别是定义、条件判断和异常处理附近，表明这是开发者面对不确定性和权衡时的有意信号。


<details>
  <summary>Details</summary>
Motivation: 先前研究主要关注检测和优先处理SATD，很少关注受SATD影响的源代码。本研究旨在将SATD评论与其周围的源代码结构联系起来。

Method: 利用包含9000多个Java开源软件仓库代码评论的PENTACET数据集，定量推断SATD最常见出现的位置及其最常影响的代码结构/语句。

Result: 大规模研究将超过225,000个SATD评论与其周围代码关联，显示SATD主要出现在内联代码中，靠近定义、条件判断和异常处理的位置。

Conclusion: SATD出现在开发者面临不确定性和权衡的地方，表明这是变更过程中的有意意识信号，而不仅仅是疏忽。

Abstract: Context. Detecting Self-Admitted Technical Debt (SATD) is crucial for
proactive software maintenance. Previous research has primarily targeted
detecting and prioritizing SATD, with little focus on the source code afflicted
with SATD. Our goal in this work is to connect the SATD comments with source
code constructs that surround them.
  Method. We leverage the extensive SATD dataset PENTACET, containing code
comments from over 9000 Java Open Source Software (OSS) repositories. We
quantitatively infer where SATD most commonly occurs and which code
constructs/statements it most frequently affects.
  Results and Conclusions. Our large-scale study links over 225,000 SATD
comments to their surrounding code, showing that SATD mainly arises in inline
code near definitions, conditionals, and exception handling, where developers
face uncertainty and trade-offs, revealing it as an intentional signal of
awareness during change rather than mere neglect.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [126] [A Proof of Learning Rate Transfer under $μ$P](https://arxiv.org/abs/2511.01734)
*Soufiane Hayou*

Main category: stat.ML

TL;DR: 该论文首次证明了在μP参数化的线性多层感知机中，学习率随宽度变化的可迁移性，表明最优学习率在宽度趋于无穷时收敛到非零常数。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络参数化方法对学习率可迁移性的影响，特别是μP参数化在无限宽度极限下如何最大化特征学习。

Method: 使用μP参数化的线性多层感知机，与标准参数化(SP)和神经正切参数化(NTP)进行对比分析，提供理论证明和实证验证。

Result: 在μP参数化下，最优学习率随宽度增加收敛到非零常数，支持学习率可迁移性；而在SP和NTP参数化下，此性质不成立。

Conclusion: μP参数化是实现学习率可迁移性的关键，为神经网络宽度缩放提供了理论基础。

Abstract: We provide the first proof of learning rate transfer with width in a linear
multi-layer perceptron (MLP) parametrized with $\mu$P, a neural network
parameterization designed to ``maximize'' feature learning in the
infinite-width limit. We show that under $\mu P$, the optimal learning rate
converges to a \emph{non-zero constant} as width goes to infinity, providing a
theoretical explanation to learning rate transfer. In contrast, we show that
this property fails to hold under alternative parametrizations such as Standard
Parametrization (SP) and Neural Tangent Parametrization (NTP). We provide
intuitive proofs and support the theoretical findings with extensive empirical
results.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [127] [Real-time and Zero-footprint Bag of Synthetic Syllables Algorithm for E-mail Spam Detection Using Subject Line and Short Text Fields](https://arxiv.org/abs/2511.00118)
*Stanislav Selitskiy*

Main category: cs.CR

TL;DR: 提出了一种基于合成音节袋算法的轻量级垃圾邮件检测方法，专门针对邮件主题行等短文本字段，无需额外资源即可实时检测大部分垃圾邮件。


<details>
  <summary>Details</summary>
Motivation: 当前邮件服务面临高可用性要求和资源限制，深度学习方法资源消耗大且处理时间长，不适合作为前端过滤器。大部分垃圾邮件并不复杂，可以用简单算法检测。

Method: 使用合成音节袋算法为每个邮件主题行生成约200维的稀疏哈希向量，通过余弦或欧氏距离计算与已知垃圾邮件主题的相似度。

Result: 算法在真实SMTP流量的一天数据上进行了性能测试，展示了其有效性。

Conclusion: 该算法无需持久存储、字典、额外硬件或软件包，能够卸载深度架构的压力，实现近实时、零资源占用的垃圾邮件检测。

Abstract: Contemporary e-mail services have high availability expectations from the
customers and are resource-strained because of the high-volume throughput and
spam attacks. Deep Machine Learning architectures, which are resource hungry
and require off-line processing due to the long processing times, are not
acceptable at the front line filters. On the other hand, the bulk of the
incoming spam is not sophisticated enough to bypass even the simplest
algorithms. While the small fraction of the intelligent, highly mutable spam
can be detected only by the deep architectures, the stress on them can be
unloaded by the simple near real-time and near zero-footprint algorithms such
as the Bag of Synthetic Syllables algorithm applied to the short texts of the
e-mail subject lines and other short text fields. The proposed algorithm
creates a circa 200 sparse dimensional hash or vector for each e-mail subject
line that can be compared for the cosine or euclidean proximity distance to
find similarities to the known spammy subjects. The algorithm does not require
any persistent storage, dictionaries, additional hardware upgrades or software
packages. The performance of the algorithm is presented on the one day of the
real SMTP traffic.

</details>
