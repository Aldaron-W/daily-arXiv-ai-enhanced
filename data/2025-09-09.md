<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 68]
- [cs.CR](#cs.CR) [Total: 3]
- [eess.AS](#eess.AS) [Total: 2]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]
- [cs.CV](#cs.CV) [Total: 2]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.AI](#cs.AI) [Total: 9]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [An Empirical Analysis of Discrete Unit Representations in Speech Language Modeling Pre-training](https://arxiv.org/abs/2509.05359)
*Yanis Labrak,Richard Dufour,Mickaël Rouvier*

Main category: cs.CL

TL;DR: 本文研究了语音语言模型中离散单元表示的优化策略，探讨了模型架构、数据表示和训练鲁棒性对语音建模的影响，发现最优离散化策略随模型容量变化，并揭示了聚类数据选择对模型鲁棒性的重要性。


<details>
  <summary>Details</summary>
Motivation: 研究如何在持续预训练阶段优化语音建模，通过将现有预训练语言模型适配到语音模态，探索离散单元表示的最佳实践。

Method: 系统性地检验模型架构、数据表示和训练鲁棒性对预训练阶段的影响，分析语音编码器和聚类粒度在不同模型规模下的作用，通过检查聚类分布和音素对齐来研究离散词汇表的有效使用。

Result: 实验发现最优离散化策略随模型容量而变化，揭示了语言和副语言模式，并证明了聚类数据选择中域匹配对目标应用的重要性。

Conclusion: 离散单元表示在语音语言模型中具有重要作用，模型容量、聚类策略和域匹配都是影响语音建模效果的关键因素，需要根据具体应用场景进行优化配置。

Abstract: This paper investigates discrete unit representations in Speech Language
Models (SLMs), focusing on optimizing speech modeling during continual
pre-training. In this paper, we systematically examine how model architecture,
data representation, and training robustness influence the pre-training stage
in which we adapt existing pre-trained language models to the speech modality.
Our experiments highlight the role of speech encoders and clustering
granularity across different model scales, showing how optimal discretization
strategies vary with model capacity. By examining cluster distribution and
phonemic alignments, we investigate the effective use of discrete vocabulary,
uncovering both linguistic and paralinguistic patterns. Additionally, we
explore the impact of clustering data selection on model robustness,
highlighting the importance of domain matching between discretization training
and target applications.

</details>


### [2] [Beyond ROUGE: N-Gram Subspace Features for LLM Hallucination Detection](https://arxiv.org/abs/2509.05360)
*Jerry Li,Evangelos Papalexakis*

Main category: cs.CL

TL;DR: 基于N-Gram频率矩阵构建和张量分解的新题检测方法，在HaluEval数据集上显著提升了幻觉检测性能


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型的幻觉问题，传统指标如ROUGE、BERTScore缺乏语义深度，需要更有效的幻觉检测方法

Method: 构建N-Gram频率张量捕捉语义结构，通过张量分解提取奇异值特征，训练MLP二分类检测器

Result: 在HaluEval数据集上表现出显著攻加传统基线方法，与最先进的LLM判断方法竞争性能

Conclusion: 新的张量基检测方法为幻觉检测提供了更有效的解决方案

Abstract: Large Language Models (LLMs) have demonstrated effectiveness across a wide
variety of tasks involving natural language, however, a fundamental problem of
hallucinations still plagues these models, limiting their trustworthiness in
generating consistent, truthful information. Detecting hallucinations has
quickly become an important topic, with various methods such as uncertainty
estimation, LLM Judges, retrieval augmented generation (RAG), and consistency
checks showing promise. Many of these methods build upon foundational metrics,
such as ROUGE, BERTScore, or Perplexity, which often lack the semantic depth
necessary to detect hallucinations effectively. In this work, we propose a
novel approach inspired by ROUGE that constructs an N-Gram frequency tensor
from LLM-generated text. This tensor captures richer semantic structure by
encoding co-occurrence patterns, enabling better differentiation between
factual and hallucinated content. We demonstrate this by applying tensor
decomposition methods to extract singular values from each mode and use these
as input features to train a multi-layer perceptron (MLP) binary classifier for
hallucinations. Our method is evaluated on the HaluEval dataset and
demonstrates significant improvements over traditional baselines, as well as
competitive performance against state-of-the-art LLM judges.

</details>


### [3] [A Lightweight Framework for Trigger-Guided LoRA-Based Self-Adaptation in LLMs](https://arxiv.org/abs/2509.05385)
*Jiacheng Wei,Faguo Wu,Xiao Zhang*

Main category: cs.CL

TL;DR: SAGE是一个触发引导的动态微调框架，能够在推理时实现自适应更新，通过分解复杂推理任务为原子子任务来解决大语言模型无法在推理时持续学习的问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在推理时无法持续适应和学习新数据，这限制了其在复杂推理任务中的表现。

Method: SAGE包含三个核心组件：触发模块实时检测推理失败、触发缓冲区模块使用HDBSCAN流式聚类异常样本并进行稳定性检查和相似性合并、Lora存储模块通过适配器池动态优化参数更新以实现知识保留。

Result: 评估结果显示，SAGE在测试时通过动态知识更新，在原子推理子任务上表现出优异的准确性、鲁棒性和稳定性。

Conclusion: SAGE框架有效解决了大语言模型在推理时无法持续学习的问题，为复杂推理任务提供了有效的动态适应解决方案。

Abstract: Large language models are unable to continuously adapt and learn from new
data during reasoning at inference time. To address this limitation, we propose
that complex reasoning tasks be decomposed into atomic subtasks and introduce
SAGE, a trigger-guided dynamic fine-tuning framework that enables adaptive
updates during reasoning at inference time. SAGE consists of three key
components: (1) a Trigger module that detects reasoning failures through
multiple evaluation metrics in real time; (2) a Trigger Buffer module that
clusters anomaly samples using a streaming clustering process with HDBSCAN,
followed by stability checks and similarity-based merging; and (3) a Lora Store
module that dynamically optimizes parameter updates with an adapter pool for
knowledge retention. Evaluation results show that SAGE demonstrates excellent
accuracy, robustness, and stability on the atomic reasoning subtask through
dynamic knowledge updating during test time.

</details>


### [4] [Talk Isn't Always Cheap: Understanding Failure Modes in Multi-Agent Debate](https://arxiv.org/abs/2509.05396)
*Andrea Wynn,Harsh Satija,Gillian Hadfield*

Main category: cs.CL

TL;DR: 研究发现多智能体辩论有时反而有害，特别是在能力不同的模型之间进行辩论时，可能导致准确率下降，因为模型倾向于达成共识而非挑战错误推理


<details>
  <summary>Details</summary>
Motivation: 探索模型能力多样性如何影响多智能体辩论的动态和结果，此前研究只关注同质智能体群体的辩论

Method: 通过一系列实验，研究不同能力模型之间的辩论交互，分析模型在辩论过程中的行为变化

Result: 辩论可能导致准确率随时间下降，即使更强模型占多数；模型经常从正确答案转向错误答案，倾向于达成共识而非挑战错误推理

Conclusion: 多智能体辩论中存在重要失败模式，当智能体既无激励也无足够能力抵抗有说服力但错误的推理时，简单的辩论应用可能导致性能下降

Abstract: While multi-agent debate has been proposed as a promising strategy for
improving AI reasoning ability, we find that debate can sometimes be harmful
rather than helpful. The prior work has exclusively focused on debates within
homogeneous groups of agents, whereas we explore how diversity in model
capabilities influences the dynamics and outcomes of multi-agent interactions.
Through a series of experiments, we demonstrate that debate can lead to a
decrease in accuracy over time -- even in settings where stronger (i.e., more
capable) models outnumber their weaker counterparts. Our analysis reveals that
models frequently shift from correct to incorrect answers in response to peer
reasoning, favoring agreement over challenging flawed reasoning. These results
highlight important failure modes in the exchange of reasons during multi-agent
debate, suggesting that naive applications of debate may cause performance
degradation when agents are neither incentivized nor adequately equipped to
resist persuasive but incorrect reasoning.

</details>


### [5] [No Translation Needed: Forecasting Quality from Fertility and Metadata](https://arxiv.org/abs/2509.05425)
*Jessica M. Lundin,Ada Zhang,David Adelani,Cody Carroll*

Main category: cs.CL

TL;DR: 使用语言类型学特征和词汇生成率可以在不运行翻译系统的情况下预测GPT-4o在FLORES-200语言对中的翻译质量，预测准确度达到R²=0.66-0.72


<details>
  <summary>Details</summary>
Motivation: 探索如何在不实际运行翻译系统的情况下，仅通过语言特征预测翻译质量，以提高多语言评估的效率

Method: 使用词汇生成率比例、词汇数量和语言类型学元数据（语言语系、文字系统、地区）作为特征，通过梯度提升模型进行预测，涉及203种语言

Result: 梯度提升模型在FLORES-200语言对中表现优异：XX→英语R²=0.66，英语→XX R²=0.72。语言类型学因素在向英语翻译中占主导地位，而词汇生成率在向多样化目标语言翻译中更重要

Conclusion: 翻译质量受到词汇层面的生成率和更广泛的语言类型学因素的共同影响，为多语言评估和质量估测提供了新的见解

Abstract: We show that translation quality can be predicted with surprising accuracy
\textit{without ever running the translation system itself}. Using only a
handful of features, token fertility ratios, token counts, and basic linguistic
metadata (language family, script, and region), we can forecast ChrF scores for
GPT-4o translations across 203 languages in the FLORES-200 benchmark. Gradient
boosting models achieve favorable performance ($R^{2}=0.66$ for
XX$\rightarrow$English and $R^{2}=0.72$ for English$\rightarrow$XX). Feature
importance analyses reveal that typological factors dominate predictions into
English, while fertility plays a larger role for translations into diverse
target languages. These findings suggest that translation quality is shaped by
both token-level fertility and broader linguistic typology, offering new
insights for multilingual evaluation and quality estimation.

</details>


### [6] [Direct-Scoring NLG Evaluators Can Use Pairwise Comparisons Too](https://arxiv.org/abs/2509.05440)
*Logan Lawrence,Ashton Williamson,Alexander Shelton*

Main category: cs.CL

TL;DR: 这篇论文提出了一种直接评分方法，利用合成摘要在测试时模拟对比评分，解决了传统对比方法无法给个体摘要赋予绝对分数的问题。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型被广泛用于自动评价自由形式内容，但现有对比评分方法无法给个体摘要赋予绝对分数，而这对需要阈值判断的应用场景至关重要。

Method: 提出一种直接评分方法，通过使用合成摘要来模拟测试时的对比机器排名，从而能够给个体摘要赋予绝对分数。

Result: 在SummEval、TopicalChat和HANNA三个评测数据集上，该方法在样本级相关性指标上与最先进的对比评价方法表现相似（分别为+0.03、-0.03和+0.05）。

Conclusion: 该直接评分方法在保持与对比方法相似性能的同时，解决了绝对分数赋值的问题，为需要阈值判断的应用提供了可行方案。

Abstract: As large-language models have been increasingly used as automatic raters for
evaluating free-form content, including document summarization, dialog, and
story generation, work has been dedicated to evaluating such models by
measuring their correlations with human judgment. For \textit{sample-level}
performance, methods which operate by using pairwise comparisons between
machine-generated text perform well but often lack the ability to assign
absolute scores to individual summaries, an ability crucial for use cases that
require thresholding. In this work, we propose a direct-scoring method which
uses synthetic summaries to act as pairwise machine rankings at test time. We
show that our method performs comparably to state-of-the-art pairwise
evaluators in terms of axis-averaged sample-level correlations on the SummEval
(\textbf{+0.03}), TopicalChat (\textbf{-0.03}), and HANNA (\textbf{+0.05})
meta-evaluation benchmarks, and release the synthetic in-context summaries as
data to facilitate future work.

</details>


### [7] [From Staff Messages to Actionable Insights: A Multi-Stage LLM Classification Framework for Healthcare Analytics](https://arxiv.org/abs/2509.05484)
*Hajar Sakai,Yi-En Tseng,Mohammadsadegh Mikaeili,Joshua Bosire,Franziska Jovin*

Main category: cs.CL

TL;DR: 这篇论文提出了一种基于大语言模型的多阶段框架，用于分析医院呼叫中心的员工消息，识别主题并进行多类分类，最佳模型达到78.4%的F1分数。


<details>
  <summary>Details</summary>
Motivation: 医院呼叫中心产生大量的员工消息数据，但传统的监督学习方法需要注释数据和较长训练时间，大语言模型提供了更高效的分析方案。

Method: 使用多种类型的LLM（包括推理模型、通用模型和轻量模型）构建多阶段框架，包含数据安全措施和HIPAA合规要求，最终集成到可视化决策支持工具中。

Result: 最佳性能模型o3达到78.4%积分F1分数和79.2%准确率，gpt-5以约75.3%积分F1分数和76.2%准确率紧随其后。

Conclusion: 该方法能够高效利用员工消息数据，识别导航员培训机会，支持改善患者体验和护理质量，为医疗健康分析提供了一种计算效率更高的方法。

Abstract: Hospital call centers serve as the primary contact point for patients within
a hospital system. They also generate substantial volumes of staff messages as
navigators process patient requests and communicate with the hospital offices
following the established protocol restrictions and guidelines. This
continuously accumulated large amount of text data can be mined and processed
to retrieve insights; however, traditional supervised learning approaches
require annotated data, extensive training, and model tuning. Large Language
Models (LLMs) offer a paradigm shift toward more computationally efficient
methodologies for healthcare analytics. This paper presents a multi-stage
LLM-based framework that identifies staff message topics and classifies
messages by their reasons in a multi-class fashion. In the process, multiple
LLM types, including reasoning, general-purpose, and lightweight models, were
evaluated. The best-performing model was o3, achieving 78.4% weighted F1-score
and 79.2% accuracy, followed closely by gpt-5 (75.3% Weighted F1-score and
76.2% accuracy). The proposed methodology incorporates data security measures
and HIPAA compliance requirements essential for healthcare environments. The
processed LLM outputs are integrated into a visualization decision support tool
that transforms the staff messages into actionable insights accessible to
healthcare professionals. This approach enables more efficient utilization of
the collected staff messaging data, identifies navigator training
opportunities, and supports improved patient experience and care quality.

</details>


### [8] [The Token Tax: Systematic Bias in Multilingual Tokenization](https://arxiv.org/abs/2509.05486)
*Jessica M. Lundin,Ada Zhang,Nihal Karim,Hamza Louzan,Victor Wei,David Adelani,Cody Carroll*

Main category: cs.CL

TL;DR: 这篇论文通过对10个大型语言模型在AfriMMLU数据集的评测，发现词汇分解效率（fertility）与模型准确性呈负相关，趋势性推理模型表现更优，并展现了词汇分解不效对计算成本的重大影响。


<details>
  <summary>Details</summary>
Motivation: 研究词汇分解效率对形态复杂、资源稀缺语言的不利影响，这些语言在NLP中面临着计算资源增长和准确性下降的挑战。

Method: 在AfriMMLU数据集（9,000道多选题，5个学科，16种非洲语言）上评测10个大型语言模型，分析词汇分解效率（每个单词对应的token数）与模型准确性的关系，并计算词汇分解不效对训练成本和时间的影响。

Result: 词汇分解效率高的语言准确性更低，这种负相关在所有模型和学科中都一致。趋势性推理模型（如DeepSeek、o1）在高低资源语言上都表现更好，缩小了以往代次的准确性差距。词汇分解效率倍增会导致训练成本和时间四倍增长。

Conclusion: 研究结果强调了开发考虑词汇形态的分解方法、公平定价以及多语言评测标准的重要性，以实现更公平的自然语言处理。

Abstract: Tokenization inefficiency imposes structural disadvantages on morphologically
complex, low-resource languages, inflating compute resources and depressing
accuracy. We evaluate 10 large language models (LLMs) on AfriMMLU (9,000 MCQA
items; 5 subjects; 16 African languages) and show that fertility (tokens/word)
reliably predicts accuracy. Higher fertility consistently predicts lower
accuracy across all models and subjects. We further find that reasoning models
(DeepSeek, o1) consistently outperform non-reasoning peers across high and low
resource languages in the AfriMMLU dataset, narrowing accuracy gaps observed in
prior generations. Finally, translating token inflation to economics, a
doubling in tokens results in quadrupled training cost and time, underscoring
the token tax faced by many languages. These results motivate morphologically
aware tokenization, fair pricing, and multilingual benchmarks for equitable
natural language processing (NLP).

</details>


### [9] [Biomedical Literature Q&A System Using Retrieval-Augmented Generation (RAG)](https://arxiv.org/abs/2509.05505)
*Mansi Garg,Lee-Chi Wang,Bhavesh Ghanchi,Sanjana Dumpala,Shreyash Kakde,Yen Chih Chen*

Main category: cs.CL

TL;DR: 基于RAG架构的生物医学文献问答系统，通过精细化训练的Mistral-7B模型和语义检索，在乳腺癌领域实现了事实一致性和语义相关性的显著提升


<details>
  <summary>Details</summary>
Motivation: 解决传统健康搜索引擎的不足以及公众获取生物医学研究成果的滞后问题，提供准确、有证据支撑的医学信息访问

Method: 采用RAG架构，整合PubMed文章、Q&A数据集和医学百科等多源数据，使用MiniLM语义嵌入和FAISS向量搜索进行信息检索，通过QLoRA优化的Mistral-7B-v0.3模型进行答案生成

Result: 在乳腺癌文献上的评估显示，使用BERTScore(F1)指标测量，在事实一致性和语义相关性方面较基线模型有显著提升

Conclusion: RAG增强的语言模型有潜力缩小复杂生物医学文献与可访公共健康知识之间的差距，为多语言适配、隐私保护推理和个性化医疗AI系统的未来工作掘底了基础

Abstract: This work presents a Biomedical Literature Question Answering (Q&A) system
based on a Retrieval-Augmented Generation (RAG) architecture, designed to
improve access to accurate, evidence-based medical information. Addressing the
shortcomings of conventional health search engines and the lag in public access
to biomedical research, the system integrates diverse sources, including PubMed
articles, curated Q&A datasets, and medical encyclopedias ,to retrieve relevant
information and generate concise, context-aware responses. The retrieval
pipeline uses MiniLM-based semantic embeddings and FAISS vector search, while
answer generation is performed by a fine-tuned Mistral-7B-v0.3 language model
optimized using QLoRA for efficient, low-resource training. The system supports
both general medical queries and domain-specific tasks, with a focused
evaluation on breast cancer literature demonstrating the value of
domain-aligned retrieval. Empirical results, measured using BERTScore (F1),
show substantial improvements in factual consistency and semantic relevance
compared to baseline models. The findings underscore the potential of
RAG-enhanced language models to bridge the gap between complex biomedical
literature and accessible public health knowledge, paving the way for future
work on multilingual adaptation, privacy-preserving inference, and personalized
medical AI systems.

</details>


### [10] [Using Contrastive Learning to Improve Two-Way Reasoning in Large Language Models: The Obfuscation Task as a Case Study](https://arxiv.org/abs/2509.05553)
*Serge Lionel Nikiema,Jordan Samhi,Micheline Bénédicte Moumoula,Albérick Euraste Djiré,Abdoul Kader Kaboré,Jacques Klein,Tegawendé F. Bissyandé*

Main category: cs.CL

TL;DR: 该研究提出双向推理作为测试AI真正理解能力的标准，发现传统微调会导致认知专业化（正向任务提升但反向能力下降），并开发了对比微调(CFT)方法成功实现双向推理。


<details>
  <summary>Details</summary>
Motivation: 解决AI领域基础问题：大型语言模型是真正理解概念还是仅识别模式。研究者认为真正理解应自然允许可逆性，即无需反向训练就能双向应用变换。

Method: 提出双向推理测试框架，开发对比微调(CFT)方法，使用三种样本：保持语义的正样本、不同语义的负样本、正向混淆样本，旨在培养深层理解而非表面模式识别。

Result: 实验证明CFT成功实现双向推理，在保持正向任务能力的同时获得强大的反向性能，解决了传统微调导致的认知专业化问题。

Conclusion: 双向推理既是评估真正理解的理论框架，也是开发更强大AI系统的实用训练方法，为AI理解能力提供了新的测试标准和改进途径。

Abstract: This research addresses a fundamental question in AI: whether large language
models truly understand concepts or simply recognize patterns. The authors
propose bidirectional reasoning,the ability to apply transformations in both
directions without being explicitly trained on the reverse direction, as a test
for genuine understanding. They argue that true comprehension should naturally
allow reversibility. For example, a model that can change a variable name like
userIndex to i should also be able to infer that i represents a user index
without reverse training. The researchers tested current language models and
discovered what they term cognitive specialization: when models are fine-tuned
on forward tasks, their performance on those tasks improves, but their ability
to reason bidirectionally becomes significantly worse. To address this issue,
they developed Contrastive Fine-Tuning (CFT), which trains models using three
types of examples: positive examples that maintain semantic meaning, negative
examples with different semantics, and forward-direction obfuscation examples.
This approach aims to develop deeper understanding rather than surface-level
pattern recognition and allows reverse capabilities to develop naturally
without explicit reverse training. Their experiments demonstrated that CFT
successfully achieved bidirectional reasoning, enabling strong reverse
performance while maintaining forward task capabilities. The authors conclude
that bidirectional reasoning serves both as a theoretical framework for
assessing genuine understanding and as a practical training approach for
developing more capable AI systems.

</details>


### [11] [Ad hoc conventions generalize to new referents](https://arxiv.org/abs/2509.05566)
*Anya Ji,Claire Augusta Bergey,Ron Eliav,Yoav Artzi,Robert D. Hawkins*

Main category: cs.CL

TL;DR: 研究表明人们在建立共享命名系统时不是简单的任意标签，而是通过概念协调实现对新对象的泛化描述


<details>
  <summary>Details</summary>
Motivation: 研究人们如何谈论从未讨论过的事物，检验两种对立观点：是建立任意链接还是通过概念对齐形成共享描述方式

Method: 使用KiloGram数据集中的1000多个抽象七巧板图像，让参与者配对进行重复沟通建立指称惯例，然后测量他们对未讨论图像的描述对齐程度

Result: 发现了强烈的泛化证据：伙伴间的对齐程度相对于前测标签显著增加；泛化随视觉相似性非线性衰减（符合Shepard定律）；在不同可命名性水平的图像上都表现稳健

Conclusion: 临时约定不是任意标签，而是反映了真正的概念协调，这对指称理论和设计更自适应语言代理具有重要意义

Abstract: How do people talk about things they've never talked about before? One view
suggests that a new shared naming system establishes an arbitrary link to a
specific target, like proper names that cannot extend beyond their bearers. An
alternative view proposes that forming a shared way of describing objects
involves broader conceptual alignment, reshaping each individual's semantic
space in ways that should generalize to new referents. We test these competing
accounts in a dyadic communication study (N=302) leveraging the
recently-released KiloGram dataset containing over 1,000 abstract tangram
images. After pairs of participants coordinated on referential conventions for
one set of images through repeated communication, we measured the extent to
which their descriptions aligned for undiscussed images. We found strong
evidence for generalization: partners showed increased alignment relative to
their pre-test labels. Generalization also decayed nonlinearly with visual
similarity (consistent with Shepard's law) and was robust across levels of the
images' nameability. These findings suggest that ad hoc conventions are not
arbitrary labels but reflect genuine conceptual coordination, with implications
for theories of reference and the design of more adaptive language agents.

</details>


### [12] [Mitigating Spurious Correlations Between Question and Answer via Chain-of-Thought Correctness Perception Distillation](https://arxiv.org/abs/2509.05602)
*Hongyan Xie,Yitong Yao,Yikun Ban,Zixuan Huang,Deqing Wang,Zhenhe Wu,Haoxiang Su,Chao Wang,Shuangyong Song*

Main category: cs.CL

TL;DR: CoPeD方法通过正确性感知任务设置和加权损失函数，提升小语言模型从大语言模型生成的思维链数据中学习推理质量，减少噪声推理的影响。


<details>
  <summary>Details</summary>
Motivation: 大语言模型生成的思维链数据可能包含噪声推理，这些推理要么无法支持答案，要么对答案预测没有贡献，导致小语言模型学习到虚假的相关性，影响推理质量。

Method: 提出Chain-of-Thought Correctness Perception Distillation (CoPeD)：1) 正确性感知任务设置，鼓励学生模型基于正确推理预测答案并在推理错误时修正；2) 正确性感知加权损失，根据推理和答案的联合损失动态调整每个训练样本的贡献。

Result: 实验表明CoPeD在分布内和分布外基准推理数据集上都有效。

Conclusion: CoPeD通过改进任务设置和数据利用策略，有效提升了学生模型的推理质量，特别是在处理噪声推理数据时表现出色。

Abstract: Large language models (LLMs) excel at reasoning tasks but are expensive to
deploy. Thus small language models (SLMs) are fine-tuned on CoT data generated
by LLMs to copy LLMs' abilities. However, these CoT data may include noisy
rationales that either fail to substantiate the answers or contribute no
additional information to support answer prediction, which leads SLMs to
capture spurious correlations between questions and answers and compromise the
quality of reasoning. In this work, we propose Chain-of-Thought Correctness
Perception Distillation (CoPeD), which aims to improve the reasoning quality of
the student model from the perspectives of task setting and data utilization.
Firstly, we introduce a correctness-aware task setting that encourages the
student model to predict answers based on correct rationales and revise them
when they are incorrect. This setting improves the faithfulness of reasoning
and allows the model to learn from its mistakes. Then, we propose a
Correctness-Aware Weighted loss, which dynamically adjusts the contribution of
each training instance based on the combined loss of the rationale and the
answer. This strategy encourages the model to focus more on samples where the
rationale offers stronger support for the correct answer. Experiments have
shown that CoPeD is effective on both in-distribution (IND) and
out-of-distribution (OOD) benchmark reasoning datasets.

</details>


### [13] [Icon$^{2}$: Aligning Large Language Models Using Self-Synthetic Preference Data via Inherent Regulation](https://arxiv.org/abs/2509.05605)
*Qiyuan Chen,Hongsen Huang,Qian Shao,Jiahe Chen,Jintai Chen,Hongxia Xu,Renjie Hua,Ren Chuan,Jian Wu*

Main category: cs.CL

TL;DR: Icon²方法通过利用LLM表示空间的固有调节来高效构建偏好数据集，避免了传统方法的分布不匹配和计算开销问题


<details>
  <summary>Details</summary>
Motivation: 传统偏好数据集构建方法存在分布不匹配和计算开销大的问题，需要一种更高效且定制化的方法来构建高质量偏好数据集

Method: 提取层间方向向量编码人类偏好，基于固有一致性过滤自合成指令，在解码时应用双向固有控制来引导token表示，精确生成具有明显对齐差异的响应对

Result: Llama3-8B和Qwen2-7B在AlpacaEval 2.0上平均胜率提升13.89%，在Arena-Hard上提升13.45%，同时计算成本降低高达48.1%

Conclusion: Icon²方法通过利用LLM表示空间的固有调节，实现了高效且有效的偏好数据集构建，显著提升了模型对齐性能并降低了计算成本

Abstract: Large Language Models (LLMs) require high quality preference datasets to
align with human preferences. However, conventional methods for constructing
such datasets face significant challenges: reliance on pre-collected
instructions often leads to distribution mismatches with target models, while
the need for sampling multiple stochastic responses introduces substantial
computational overhead. In this work, we explore a paradigm shift by leveraging
inherent regulation of LLMs' representation space for efficient and tailored
preference dataset construction, named Icon$^{2}$. Specifically, it first
extracts layer-wise direction vectors to encode sophisticated human preferences
and then uses these vectors to filter self-synthesized instructions based on
their inherent consistency. During decoding, bidirectional inherent control is
applied to steer token representations, enabling the precise generation of
response pairs with clear alignment distinctions. Experimental results
demonstrate significant improvements in both alignment and efficiency.
Llama3-8B and Qwen2-7B achieve an average win rate improvement of 13.89% on
AlpacaEval 2.0 and 13.45% on Arena-Hard, while reducing computational costs by
up to 48.1%.

</details>


### [14] [Beyond Keywords: Driving Generative Search Engine Optimization with Content-Centric Agents](https://arxiv.org/abs/2509.05607)
*Qiyuan Chen,Jiahe Chen,Hongsen Huang,Qian Shao,Jintai Chen,Renjie Hua,Hongxia Xu,Ruijia Wu,Ren Chuan,Jian Wu*

Main category: cs.CL

TL;DR: 这篇论文提出了一个结构化框架来解决生成式搜索引擎优化(GSEO)的挑战，包括构建大规模标准数据集和自动化的多组代理系统。


<details>
  <summary>Details</summary>
Motivation: 传统SEO指标在生成式搜索引擎时代已失效，需要新方法来理解、测量和优化内容对合成答案的影响力。

Method: 构建了CC-GSEO-Bench大规模标准数据集，提出多维度评估框架，设计了多组代理系统来自动化内容精炼的分析-修订-评估流程。

Result: 实验分析揭示了内容影响力的新动态特征，为创作者提供了可操作的策略，为未来GSEO研究打下了理论基础。

Conclusion: 该框架成功解决了生成式搜索引擎优化的关键挑战，通过系统化的方法进行内容影响力评估和优化，为这一新兴领域提供了重要技术支撑。

Abstract: The paradigm shift from traditional ranked-based search to Generative Search
Engines has rendered conventional SEO metrics obsolete, creating an urgent need
to understand, measure, and optimize for content influence on synthesized
answers. This paper introduces a comprehensive, end-to-end framework for
Generative Search Engine Optimization (GSEO) to address this challenge. We make
two primary contributions. First, we construct CC-GSEO-Bench, a large-scale,
content-centric benchmark, and propose a multi-dimensional evaluation framework
that systematically quantifies influence, moving beyond surface-level
attribution to assess substantive semantic impact. Second, we design a novel
multi-agent system that operationalizes this framework, automating the
strategic refinement of content through a collaborative analyze-revise-evaluate
workflow. Our empirical analysis using this framework reveals novel insights
into the dynamics of content influence, offering actionable strategies for
creators and establishing a principled foundation for future GSEO research.

</details>


### [15] [New Insights into Optimal Alignment of Acoustic and Linguistic Representations for Knowledge Transfer in ASR](https://arxiv.org/abs/2509.05609)
*Xugang Lu,Peng Shen,Yu Tsao,Hisashi Kawai*

Main category: cs.CL

TL;DR: 通过将声学-语言对齐问题视为检测问题，提出了一种不平衡最优运输对齐模型，能够灵活处理分布不匹配和结构不对称问题，提高ASR系统性能。


<details>
  <summary>Details</summary>
Motivation: 解决声学和语言表征之间的对齐挑战，该对齐具有内在的结构不对称性（多对一、一对多）和分布不匹配问题，如噪声或沉默框。

Method: 将对齐问题视为检测问题，提出了一种不平衡最优运输基于的对齐模型，支持某些语言单元可能没有相应声学观测，而某些声学框可能没有相应语言单元。

Result: 在基于CTC的ASR系统中进行评估，实验结果证明了该方法在灵活控制匹配程度和提高ASR性能方面的有效性。

Conclusion: 通过将声学-语言对齐模型化为检测问题，并使用不平衡最优运输方法，能够有效处理结构不对称和分布不匹配，从而提高语言知识转移效果和ASR系统性能。

Abstract: Aligning acoustic and linguistic representations is a central challenge to
bridge the pre-trained models in knowledge transfer for automatic speech
recognition (ASR). This alignment is inherently structured and asymmetric:
while multiple consecutive acoustic frames typically correspond to a single
linguistic token (many-to-one), certain acoustic transition regions may relate
to multiple adjacent tokens (one-to-many). Moreover, acoustic sequences often
include frames with no linguistic counterpart, such as background noise or
silence may lead to imbalanced matching conditions. In this work, we take a new
insight to regard alignment and matching as a detection problem, where the goal
is to identify meaningful correspondences with high precision and recall
ensuring full coverage of linguistic tokens while flexibly handling redundant
or noisy acoustic frames in transferring linguistic knowledge for ASR. Based on
this new insight, we propose an unbalanced optimal transport-based alignment
model that explicitly handles distributional mismatch and structural
asymmetries with soft and partial matching between acoustic and linguistic
modalities. Our method ensures that every linguistic token is grounded in at
least one acoustic observation, while allowing for flexible, probabilistic
mappings from acoustic to linguistic units. We evaluate our proposed model with
experiments on an CTC-based ASR system with a pre-trained language model for
knowledge transfer. Experimental results demonstrate the effectiveness of our
approach in flexibly controlling degree of matching and hence to improve ASR
performance.

</details>


### [16] [From Joy to Fear: A Benchmark of Emotion Estimation in Pop Song Lyrics](https://arxiv.org/abs/2509.05617)
*Shay Dahary,Avi Edana,Alexander Apartsin,Yehudit Aperstein*

Main category: cs.CL

TL;DR: 该研究构建了一个手动标注的歌词情感数据集，评估了零样本LLM和微调BERT模型在预测六种基本情感强度分数方面的表现，为基于情感的音乐信息检索提供了模型选择策略。


<details>
  <summary>Details</summary>
Motivation: 歌词的情感内容对听众体验和音乐偏好具有重要影响，需要可靠的方法来识别和分析歌词中的多标签情感属性。

Method: 使用平均意见得分(MOS)方法构建手动标注数据集，评估多个公开LLM的零样本性能，并微调BERT模型进行多标签情感分数预测。

Result: 实验结果显示零样本和微调模型在捕捉歌词细腻情感内容方面各有优势和局限。

Conclusion: LLM在创意文本情感识别方面具有潜力，为基于情感的音乐信息检索应用提供了模型选择指导。

Abstract: The emotional content of song lyrics plays a pivotal role in shaping listener
experiences and influencing musical preferences. This paper investigates the
task of multi-label emotional attribution of song lyrics by predicting six
emotional intensity scores corresponding to six fundamental emotions. A
manually labeled dataset is constructed using a mean opinion score (MOS)
approach, which aggregates annotations from multiple human raters to ensure
reliable ground-truth labels. Leveraging this dataset, we conduct a
comprehensive evaluation of several publicly available large language models
(LLMs) under zero-shot scenarios. Additionally, we fine-tune a BERT-based model
specifically for predicting multi-label emotion scores. Experimental results
reveal the relative strengths and limitations of zero-shot and fine-tuned
models in capturing the nuanced emotional content of lyrics. Our findings
highlight the potential of LLMs for emotion recognition in creative texts,
providing insights into model selection strategies for emotion-based music
information retrieval applications. The labeled dataset is available at
https://github.com/LLM-HITCS25S/LyricsEmotionAttribution.

</details>


### [17] [Few-Shot Query Intent Detection via Relation-Aware Prompt Learning](https://arxiv.org/abs/2509.05635)
*Liang Zhang,Yuan Li,Shijie Zhang,Zheng Zhang,Xitong Li*

Main category: cs.CL

TL;DR: SAID是一个新颖的框架，首次统一整合文本和关系结构信息进行模型预训练，通过查询自适应注意力网络实现细粒度知识迁移，在少样本意图检测任务上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有意图检测方法主要关注文本数据，忽略了对话系统中重要的结构信息（如查询-查询关系和查询-回答关系），导致无法有效捕捉对话的结构特征。

Method: 提出SAID框架，统一整合文本和关系结构信息进行预训练；设计查询自适应注意力网络(QueryAdapt)，在关系标记级别生成意图特定的关系标记，实现细粒度知识迁移。

Result: 在两个真实世界数据集上的大量实验结果表明，SAID显著优于最先进的方法。

Conclusion: 整合文本和关系结构信息对于意图检测任务至关重要，SAID框架通过统一建模和细粒度知识迁移机制，在少样本场景下取得了显著性能提升。

Abstract: Intent detection is a crucial component of modern conversational systems,
since accurately identifying user intent at the beginning of a conversation is
essential for generating effective responses. Recent efforts have focused on
studying this problem under a challenging few-shot scenario. These approaches
primarily leverage large-scale unlabeled dialogue text corpora to pretrain
language models through various pretext tasks, followed by fine-tuning for
intent detection with very limited annotations. Despite the improvements
achieved, existing methods have predominantly focused on textual data,
neglecting to effectively capture the crucial structural information inherent
in conversational systems, such as the query-query relation and query-answer
relation. To address this gap, we propose SAID, a novel framework that
integrates both textual and relational structure information in a unified
manner for model pretraining for the first time. Building on this framework, we
further propose a novel mechanism, the query-adaptive attention network
(QueryAdapt), which operates at the relation token level by generating
intent-specific relation tokens from well-learned query-query and query-answer
relations explicitly, enabling more fine-grained knowledge transfer. Extensive
experimental results on two real-world datasets demonstrate that SAID
significantly outperforms state-of-the-art methods.

</details>


### [18] [LM-Searcher: Cross-domain Neural Architecture Search with LLMs via Unified Numerical Encoding](https://arxiv.org/abs/2509.05657)
*Yuxuan Hu,Jihao Liu,Ke Wang,Jinliang Zhen,Weikang Shi,Manyuan Zhang,Qi Dou,Rui Liu,Aojun Zhou,Hongsheng Li*

Main category: cs.CL

TL;DR: LM-Searcher是一个基于大语言模型的跨域神经架构搜索框架，通过通用数值编码NCode和排名任务重构，无需领域特定调优即可实现高性能架构搜索。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的神经架构搜索方法严重依赖提示工程和领域特定调优，限制了其在实际应用中的实用性和可扩展性。

Method: 提出NCode通用数值字符串表示法编码神经架构，将NAS问题重构为排名任务，使用基于剪枝的子空间采样策略生成指令调优样本训练LLM。

Result: 在领域内（如图像分类CNN）和跨域（如分割和生成的LoRA配置）任务中都取得了有竞争力的性能。

Conclusion: 建立了一个灵活且可泛化的基于LLM的架构搜索新范式，具有很好的跨域适应能力。

Abstract: Recent progress in Large Language Models (LLMs) has opened new avenues for
solving complex optimization problems, including Neural Architecture Search
(NAS). However, existing LLM-driven NAS approaches rely heavily on prompt
engineering and domain-specific tuning, limiting their practicality and
scalability across diverse tasks. In this work, we propose LM-Searcher, a novel
framework that leverages LLMs for cross-domain neural architecture optimization
without the need for extensive domain-specific adaptation. Central to our
approach is NCode, a universal numerical string representation for neural
architectures, which enables cross-domain architecture encoding and search. We
also reformulate the NAS problem as a ranking task, training LLMs to select
high-performing architectures from candidate pools using instruction-tuning
samples derived from a novel pruning-based subspace sampling strategy. Our
curated dataset, encompassing a wide range of architecture-performance pairs,
encourages robust and transferable learning. Comprehensive experiments
demonstrate that LM-Searcher achieves competitive performance in both in-domain
(e.g., CNNs for image classification) and out-of-domain (e.g., LoRA
configurations for segmentation and generation) tasks, establishing a new
paradigm for flexible and generalizable LLM-based architecture search. The
datasets and models will be released at https://github.com/Ashone3/LM-Searcher.

</details>


### [19] [Cross-Question Method Reuse in Large Language Models: From Word-Level Prediction to Rational Logical-Layer Reasoning](https://arxiv.org/abs/2509.05660)
*Hong Su*

Main category: cs.CL

TL;DR: 本文提出了一种扩展方法重用范围的新方法，能够处理低相似度或具有隐藏相似性的问题，通过分离问题和解决方案，引导LLM专注于解决方案迁移而非问题识别。


<details>
  <summary>Details</summary>
Motivation: 现有方法重用方法通常要求问题高度相似，限制了应用范围。本文旨在扩展方法重用范围，使其能够处理相似度低或具有隐藏相似性的问题。

Method: 首先将问题和解决方案分离，而不是直接将问题-解决方案对输入LLM。然后引导LLM将解决方案适配到新的相关问题上，专注于解决方案迁移而非问题识别。还扩展到仅共享部分特征或隐藏特征的情况。

Result: 实验验证表明，该范围扩展方法提高了筛选出可重用解决方案的概率，从而提高了跨问题方法重用的有效性。

Conclusion: 提出的方法成功扩展了方法重用的适用范围，能够处理更广泛的问题类型，包括低相似度和隐藏相似性的情况，显著提升了跨问题方法重用的效果。

Abstract: Large language models (LLMs) have been widely applied to assist in finding
solutions for diverse questions. Prior work has proposed representing a method
as a pair of a question and its corresponding solution, enabling method reuse.
However, existing approaches typically require the questions to be highly
similar. In this paper, we extend the scope of method reuse to address
questions with low similarity or with hidden similarities that are not
explicitly observable. For questions that are similar in a general-specific
sense (i.e., broader or narrower in scope), we propose to first separate the
question and solution, rather than directly feeding the pair to the LLM. The
LLM is then guided to adapt the solution to new but related questions, allowing
it to focus on solution transfer rather than question recognition. Furthermore,
we extend this approach to cases where questions only share partial features or
hidden characteristics. This enables cross-question method reuse beyond
conventional similarity constraints. Experimental verification shows that our
scope-extension approach increases the probability of filtering out reusable
solutions, thereby improving the effectiveness of cross-question method reuse.

</details>


### [20] [Llama-GENBA-10B: A Trilingual Large Language Model for German, English and Bavarian](https://arxiv.org/abs/2509.05668)
*Michael Hoffmann,Jophin John,Stefan Schweter,Gokul Ramakrishnan,Hoi-Fong Mak,Alice Zhang,Dmitry Gaynullin,Nicolay J. Hammer*

Main category: cs.CL

TL;DR: Llama-GENBA-10B是一个10B参数的三语基础模型，基于Llama 3.1-8B构建，针对英语、德语和巴伐利亚语进行持续预训练，解决了英语中心偏见问题，在巴伐利亚语上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型中的英语中心偏见问题，为德语NLP社区提供支持，并促进低资源语言巴伐利亚语的发展。

Method: 基于Llama 3.1-8B扩展到10B参数，使用164B tokens进行持续预训练（82B英语、82B德语、80M巴伐利亚语），创建统一的分词器，优化架构和语言比例超参数，建立首个标准化三语评估套件。

Result: 模型在三语上表现优异，微调变体在巴伐利亚语上超越Apertus-8B-2509和gemma-2-9b，成为该语言最佳模型；在英语上优于EuroLLM，在德语上与EuroLLM相当。

Conclusion: 该研究为包含低资源语言的包容性基础模型提供了蓝图，证明了大规模多语言预训练的效率，并记录了能源使用情况。

Abstract: We present Llama-GENBA-10B, a trilingual foundation model addressing
English-centric bias in large language models. Built on Llama 3.1-8B and scaled
to 10B parameters, Llama-GENBA-10B is continuously pretrained on 164B tokens
(82B English, 82B German, and 80M Bavarian), balancing resources while
preventing English dominance. Targeted at the German NLP community, the model
also promotes Bavarian as a low-resource language. Development tackled four
challenges: (1) curating a multilingual corpus despite Bavarian scarcity, (2)
creating a unified tokenizer for English, German, and Bavarian, (3) optimizing
architecture and language-ratio hyperparameters for cross-lingual transfer, and
(4) establishing the first standardized trilingual evaluation suite by
translating German benchmarks into Bavarian. Evaluations show that
Llama-GENBA-10B achieves strong cross-lingual performance, with the fine-tuned
variant surpassing Apertus-8B-2509 and gemma-2-9b in Bavarian and establishing
itself as the best model in its class for this language, while also
outperforming EuroLLM in English and matching its results in German. Training
on the Cerebras CS-2 demonstrated efficient large-scale multilingual
pretraining with documented energy use, offering a blueprint for inclusive
foundation models that integrate low-resource languages.

</details>


### [21] [Revealing the Numeracy Gap: An Empirical Investigation of Text Embedding Models](https://arxiv.org/abs/2509.05691)
*Ningyuan Deng,Hanyu Duan,Yixuan Tang,Yi Yang*

Main category: cs.CL

TL;DR: 当前658文本嵌入模型在编码数字信息方面表现差强，无法准确区分数值细微差异


<details>
  <summary>Details</summary>
Motivation: 嵌入模型广泛应用于需要理解数值信息的领域（如金融、医疗），但现有评测标准缺乏对数字理解能力的测试

Method: 使用金融领域的合成数据，对13个广泛使用的文本嵌入模型进行评测

Result: 模型普遍在准确捕捉数字细节方面表现差强，无法有效区分数值细微差异

Conclusion: 当前嵌入模型在数字理解能力上存在显著缺陷，需要进一步研究提升模型的数值处理能力

Abstract: Text embedding models are widely used in natural language processing
applications. However, their capability is often benchmarked on tasks that do
not require understanding nuanced numerical information in text. As a result,
it remains unclear whether current embedding models can precisely encode
numerical content, such as numbers, into embeddings. This question is critical
because embedding models are increasingly applied in domains where numbers
matter, such as finance and healthcare. For example, Company X's market share
grew by 2\% should be interpreted very differently from Company X's market
share grew by 20\%, even though both indicate growth in market share. This
study aims to examine whether text embedding models can capture such nuances.
Using synthetic data in a financial context, we evaluate 13 widely used text
embedding models and find that they generally struggle to capture numerical
details accurately. Our further analyses provide deeper insights into embedding
numeracy, informing future research to strengthen embedding model-based NLP
systems with improved capacity for handling numerical content.

</details>


### [22] [A Survey of the State-of-the-Art in Conversational Question Answering Systems](https://arxiv.org/abs/2509.05716)
*Manoj Madushanka Perera,Adnan Mahmood,Kasun Eranda Wijethilake,Fahmida Islam,Maryam Tahermazandarani,Quan Z. Sheng*

Main category: cs.CL

TL;DR: 这是一份关于对话问答系统(ConvQA)的综述性论文，全面分析了该领域的最新进展、核心组件、技术方法、数据集和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 对话问答系统在NLP领域变得越来越重要，需要系统地总结当前技术状态和指导未来研究。

Method: 通过综述性分析方法，研究了ConvQA系统的三大核心组件：历史选择、问题理解和答案预测，以及加强学习、对比学习、迁移学习等先进技术。

Result: 识别了大型语言模型（如GPT-4、Gemini 2.0 Flash、LLaMA 3等）在ConvQA中的关键作用，分析了主要数据集，并提出了该领域的研究挑战和机遇。

Conclusion: 该综述为ConvQA领域提供了全面的技术图谱，为未来研究和应用发展指明了方向，将有助于推动该领域的进一步发展。

Abstract: Conversational Question Answering (ConvQA) systems have emerged as a pivotal
area within Natural Language Processing (NLP) by driving advancements that
enable machines to engage in dynamic and context-aware conversations. These
capabilities are increasingly being applied across various domains, i.e.,
customer support, education, legal, and healthcare where maintaining a coherent
and relevant conversation is essential. Building on recent advancements, this
survey provides a comprehensive analysis of the state-of-the-art in ConvQA.
This survey begins by examining the core components of ConvQA systems, i.e.,
history selection, question understanding, and answer prediction, highlighting
their interplay in ensuring coherence and relevance in multi-turn
conversations. It further investigates the use of advanced machine learning
techniques, including but not limited to, reinforcement learning, contrastive
learning, and transfer learning to improve ConvQA accuracy and efficiency. The
pivotal role of large language models, i.e., RoBERTa, GPT-4, Gemini 2.0 Flash,
Mistral 7B, and LLaMA 3, is also explored, thereby showcasing their impact
through data scalability and architectural advancements. Additionally, this
survey presents a comprehensive analysis of key ConvQA datasets and concludes
by outlining open research directions. Overall, this work offers a
comprehensive overview of the ConvQA landscape and provides valuable insights
to guide future advancements in the field.

</details>


### [23] [Exploring Subjective Tasks in Farsi: A Survey Analysis and Evaluation of Language Models](https://arxiv.org/abs/2509.05719)
*Donya Rooein,Flor Miriam Plaza-del-Arco,Debora Nozza,Dirk Hovy*

Main category: cs.CL

TL;DR: Farsi虽然被认为是中等资源语言，但在主观性NLP任务（情感分析、情感分析和毒性检测）中面临严重的数据可用性和质量问题，数据量和质量不足以显著提升NLP表现。


<details>
  <summary>Details</summary>
Motivation: 尽管Farsi拥有超过1.27亿使用者和大量数字文本（如维基百科130万篇文章），但作为中等资源语言的标签在深入考察主观性任务时并不成立，需要重新评估其实际资源状况。

Method: 回顾了110篇关于Farsi主观性任务的出版物，分析数据可用性和质量；评估现有数据集对人口统计因素（如年龄、性别）的覆盖情况；使用有限可用数据集测试预测模型的稳定性。

Result: 发现公开可用数据集严重缺乏；现有数据集缺少关键的人口统计因素；模型在不同数据集和模型间的预测结果极不稳定。

Conclusion: 单纯的数据量增加不足以显著改善语言在NLP中的前景，数据质量和多样性（如人口统计信息）对主观性语言建模至关重要。

Abstract: Given Farsi's speaker base of over 127 million people and the growing
availability of digital text, including more than 1.3 million articles on
Wikipedia, it is considered a middle-resource language. However, this label
quickly crumbles when the situation is examined more closely. We focus on three
subjective tasks (Sentiment Analysis, Emotion Analysis, and Toxicity Detection)
and find significant challenges in data availability and quality, despite the
overall increase in data availability. We review 110 publications on subjective
tasks in Farsi and observe a lack of publicly available datasets. Furthermore,
existing datasets often lack essential demographic factors, such as age and
gender, that are crucial for accurately modeling subjectivity in language. When
evaluating prediction models using the few available datasets, the results are
highly unstable across both datasets and models. Our findings indicate that the
volume of data is insufficient to significantly improve a language's prospects
in NLP.

</details>


### [24] [QCSE: A Pretrained Quantum Context-Sensitive Word Embedding for Natural Language Processing](https://arxiv.org/abs/2509.05729)
*Charles M. Varmantchaonala,Niclas GÖtting,Nils-Erik SchÜtte,Jean Louis E. K. Fendji,Christopher Gies*

Main category: cs.CL

TL;DR: 提出了一种名为QCSE的预训练量子上下文敏感嵌入模型，利用量子系统特性学习语言中的上下文关系，通过五种不同的上下文矩阵计算方法实现量子原生上下文学习。


<details>
  <summary>Details</summary>
Motivation: 利用量子计算的力量来编码和理解自然语言的复杂性，特别是解决低资源语言数据不足的问题，展示量子自然语言处理在现实语言挑战中的潜力。

Method: 开发了QCSE量子上下文敏感嵌入模型，引入量子原生上下文学习，提出了五种上下文矩阵计算方法（指数衰减、正弦调制、相移、基于哈希的变换等），在富拉尼语和英语语料库上进行评估。

Result: QCSE不仅能够捕捉上下文敏感性，还能利用量子系统的表达能力来表示丰富的上下文感知语言信息，特别是在低资源的富拉尼语上表现出色。

Conclusion: 这项工作强调了量子计算在自然语言处理中的力量，为将QNLP应用于各种任务和领域的现实语言挑战开辟了新途径。

Abstract: Quantum Natural Language Processing (QNLP) offers a novel approach to
encoding and understanding the complexity of natural languages through the
power of quantum computation. This paper presents a pretrained quantum
context-sensitive embedding model, called QCSE, that captures context-sensitive
word embeddings, leveraging the unique properties of quantum systems to learn
contextual relationships in languages. The model introduces quantum-native
context learning, enabling the utilization of quantum computers for linguistic
tasks. Central to the proposed approach are innovative context matrix
computation methods, designed to create unique, representations of words based
on their surrounding linguistic context. Five distinct methods are proposed and
tested for computing the context matrices, incorporating techniques such as
exponential decay, sinusoidal modulation, phase shifts, and hash-based
transformations. These methods ensure that the quantum embeddings retain
context sensitivity, thereby making them suitable for downstream language tasks
where the expressibility and properties of quantum systems are valuable
resources. To evaluate the effectiveness of the model and the associated
context matrix methods, evaluations are conducted on both a Fulani corpus, a
low-resource African language, dataset of small size and an English corpus of
slightly larger size. The results demonstrate that QCSE not only captures
context sensitivity but also leverages the expressibility of quantum systems
for representing rich, context-aware language information. The use of Fulani
further highlights the potential of QNLP to mitigate the problem of lack of
data for this category of languages. This work underscores the power of quantum
computation in natural language processing (NLP) and opens new avenues for
applying QNLP to real-world linguistic challenges across various tasks and
domains.

</details>


### [25] [Enhancing Factual Accuracy and Citation Generation in LLMs via Multi-Stage Self-Verification](https://arxiv.org/abs/2509.05741)
*Fernando Gabriela García,Qiyang Shi,Zilin Feng*

Main category: cs.CL

TL;DR: VeriFact-CoT是一种新颖的方法，通过事实验证-反思-引用整合的多阶段机制，解决LLM在生成复杂事实敏感内容时的幻觉和缺乏可信引用源问题。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在生成复杂事实敏感内容时普遍存在的幻觉问题和缺乏可信引用源的问题，提高LLM在科学研究、新闻报道和法律咨询等高保真度应用中的可靠性。

Method: 采用多阶段的'事实验证-反思-引用整合'机制，使LLM能够批判性地自我检查和修订其中间推理步骤和最终答案。

Result: 显著提高了生成输出的客观准确性、可信度和可追溯性。

Conclusion: VeriFact-CoT方法使LLM在需要高保真度的应用中更加可靠，增强了生成内容的可信度和可验证性。

Abstract: This research introduces VeriFact-CoT (Verified Factual Chain-of-Thought), a
novel method designed to address the pervasive issues of hallucination and the
absence of credible citation sources in Large Language Models (LLMs) when
generating complex, fact-sensitive content. By incorporating a multi-stage
mechanism of 'fact verification-reflection-citation integration,' VeriFact-CoT
empowers LLMs to critically self-examine and revise their intermediate
reasoning steps and final answers. This process significantly enhances the
objective accuracy, trustworthiness, and traceability of the generated outputs,
making LLMs more reliable for applications demanding high fidelity such as
scientific research, news reporting, and legal consultation.

</details>


### [26] [LatinX: Aligning a Multilingual TTS Model with Direct Preference Optimization](https://arxiv.org/abs/2509.05863)
*Luis Felipe Chary,Miguel Arjona Ramirez*

Main category: cs.CL

TL;DR: LatinX是一个多语言文本转语音模型，通过三阶段训练实现跨语言说话人身份保持，在WER和说话人相似度指标上优于基线模型


<details>
  <summary>Details</summary>
Motivation: 开发一个能够保持源说话人身份的多语言语音转换系统，解决跨语言语音转换中的说话人身份保持问题

Method: 12层仅解码器Transformer，三阶段训练：文本到音频映射预训练、零样本语音克隆监督微调、基于WER和说话人相似度指标的DPO对齐

Result: DPO训练持续降低WER并提高客观相似度，人工评估显示比XTTSv2基线有更强的感知说话人相似性，但客观与主观测量存在差距

Conclusion: 拉丁语系语言上表现良好，未来工作需要平衡偏好信号和低延迟架构

Abstract: We present LatinX, a multilingual text-to-speech (TTS) model for cascaded
speech-to-speech translation that preserves the source speaker's identity
across languages. LatinX is a 12-layer decoder-only Transformer trained in
three stages: (i) pre-training for text-to-audio mapping, (ii) supervised
fine-tuning for zero-shot voice cloning, and (iii) alignment with Direct
Preference Optimization (DPO) using automatically labeled pairs based on Word
Error Rate (WER) and speaker-similarity metrics. Trained on English and Romance
languages with emphasis on Portuguese, LatinX with DPO consistently reduces WER
and improves objective similarity over the fine-tuned baseline. Human
evaluations further indicate stronger perceived speaker similarity than a
strong baseline (XTTSv2), revealing gaps between objective and subjective
measures. We provide cross-lingual analyses and discuss balanced preference
signals and lower-latency architectures as future work.

</details>


### [27] [ZhiFangDanTai: Fine-tuning Graph-based Retrieval-Augmented Generation Model for Traditional Chinese Medicine Formula](https://arxiv.org/abs/2509.05867)
*ZiXuan Zhang,Bowen Hao,Yingjie Li,Hongzhi Yin*

Main category: cs.CL

TL;DR: 中医处方分析框架ZhiFangDanTai，结合图基检索增强生成和大语言模型精调，提升中医处方生成的详细程度和准确性


<details>
  <summary>Details</summary>
Motivation: 现有中医处方分析模型缺乏完整的处方组成和详细解释，数据集缺乏权安、功效、禁忌等关键信息

Method: 提出GraphRAG检索增强生成技术，结合LLM精调，构建增强指令数据集，并提供理论证明

Result: 在收集和临床数据集上表现显著超过现有最佳模型，降低欠掩和幽灵现象

Conclusion: ZhiFangDanTai框架能够有效提升中医处方生成的详细程度和准确性，为中医智能化提供新解决方案

Abstract: Traditional Chinese Medicine (TCM) formulas play a significant role in
treating epidemics and complex diseases. Existing models for TCM utilize
traditional algorithms or deep learning techniques to analyze formula
relationships, yet lack comprehensive results, such as complete formula
compositions and detailed explanations. Although recent efforts have used TCM
instruction datasets to fine-tune Large Language Models (LLMs) for explainable
formula generation, existing datasets lack sufficient details, such as the
roles of the formula's sovereign, minister, assistant, courier; efficacy;
contraindications; tongue and pulse diagnosis-limiting the depth of model
outputs. To address these challenges, we propose ZhiFangDanTai, a framework
combining Graph-based Retrieval-Augmented Generation (GraphRAG) with LLM
fine-tuning. ZhiFangDanTai uses GraphRAG to retrieve and synthesize structured
TCM knowledge into concise summaries, while also constructing an enhanced
instruction dataset to improve LLMs' ability to integrate retrieved
information. Furthermore, we provide novel theoretical proofs demonstrating
that integrating GraphRAG with fine-tuning techniques can reduce generalization
error and hallucination rates in the TCM formula task. Experimental results on
both collected and clinical datasets demonstrate that ZhiFangDanTai achieves
significant improvements over state-of-the-art models. Our model is
open-sourced at https://huggingface.co/tczzx6/ZhiFangDanTai1.0.

</details>


### [28] [MedFactEval and MedAgentBrief: A Framework and Workflow for Generating and Evaluating Factual Clinical Summaries](https://arxiv.org/abs/2509.05878)
*François Grolleau,Emily Alsentzer,Timothy Keyes,Philip Chung,Akshay Swaminathan,Asad Aali,Jason Hom,Tridu Huynh,Thomas Lew,April S. Liang,Weihan Chu,Natasha Z. Steele,Christina F. Lin,Jingkun Yang,Kameron C. Black,Stephen P. Ma,Fateme N. Haredasht,Nigam H. Shah,Kevin Schulman,Jonathan H. Chen*

Main category: cs.CL

TL;DR: 一个可扩展的评估框架MedFactEval和高质量生成流程MedAgentBrief，用于评估和提高LLM生成临床文本的事实准确性，与人类专家评估达到类似的一致性。


<details>
  <summary>Details</summary>
Motivation: 临床工作流中LLM生成文本的事实准确性评估靠专家审查无法扩展，需要可扩展的评估方法来支持连续质量保障。

Method: 使用临床医生定义关键事实，通过"LLM院士团"（多个LLM多数祭决）评估这些事实是否包含在生成摘要中；同时提出MedAgentBrief多步工作流来生成高质量出院摘要。

Result: MedFactEval与7名医生组成的金标准达到差不多完美一致性（Cohen's kappa=81%），统计上不差于单个人类专家（kappa=67%，P < 0.001）。

Conclusion: 该研究提供了稳健的评估框架和高性能的生成流程，为生成式AI在临床工作流中的负责任部署提供了全面方案。

Abstract: Evaluating factual accuracy in Large Language Model (LLM)-generated clinical
text is a critical barrier to adoption, as expert review is unscalable for the
continuous quality assurance these systems require. We address this challenge
with two complementary contributions. First, we introduce MedFactEval, a
framework for scalable, fact-grounded evaluation where clinicians define
high-salience key facts and an "LLM Jury"--a multi-LLM majority vote--assesses
their inclusion in generated summaries. Second, we present MedAgentBrief, a
model-agnostic, multi-step workflow designed to generate high-quality, factual
discharge summaries. To validate our evaluation framework, we established a
gold-standard reference using a seven-physician majority vote on
clinician-defined key facts from inpatient cases. The MedFactEval LLM Jury
achieved almost perfect agreement with this panel (Cohen's kappa=81%), a
performance statistically non-inferior to that of a single human expert
(kappa=67%, P < 0.001). Our work provides both a robust evaluation framework
(MedFactEval) and a high-performing generation workflow (MedAgentBrief),
offering a comprehensive approach to advance the responsible deployment of
generative AI in clinical workflows.

</details>


### [29] [Let's Roleplay: Examining LLM Alignment in Collaborative Dialogues](https://arxiv.org/abs/2509.05882)
*Abhijnan Nath,Carine Graff,Nikhil Krishnaswamy*

Main category: cs.CL

TL;DR: 本文研究了不同对齐方法对LLM代理在多轮多方协作中作为合作伙伴有效性的影响，提出了摩擦代理干预机制和反事实评估框架，发现摩擦感知方法在促进共识达成和任务结果正确性方面显著优于常见对齐基线。


<details>
  <summary>Details</summary>
Motivation: 随着LLM融入多样化工作流程，需要确保其在多轮交互中的行为可预测且可靠。现有对齐技术通常在简化单用户设置下开发，未能考虑长期多方交互的动态特性。

Method: 使用角色扮演方法评估不同训练摩擦代理在协作任务对话中的干预效果，提出新颖的反事实评估框架量化摩擦干预对群体协作轨迹和信念对齐的影响。

Result: 摩擦感知方法在帮助群体达成共识（共同认可的任务相关命题）和任务结果正确性方面显著优于常见对齐基线方法。

Conclusion: 针对多轮多方协作场景设计的摩擦感知对齐方法比传统单用户对齐技术更有效，能够更好地促进群体协作中的共识形成和决策质量。

Abstract: As Large Language Models (LLMs) integrate into diverse workflows, they are
increasingly being considered "collaborators" with humans. If such AI
collaborators are to be reliable, their behavior over multiturn interactions
must be predictable, validated and verified before deployment. Common alignment
techniques are typically developed under simplified single-user settings and do
not account for the dynamics of long-horizon multiparty interactions. This
paper examines how different alignment methods affect LLM agents' effectiveness
as partners in multiturn, multiparty collaborations. We study this question
through the lens of friction agents that intervene in group dialogues to
encourage the collaborative group to slow down and reflect upon their reasoning
for deliberative decision-making. Using a roleplay methodology, we evaluate
interventions from differently-trained friction agents in collaborative task
conversations. We propose a novel counterfactual evaluation framework that
quantifies how friction interventions change the trajectory of group
collaboration and belief alignment. Our results show that a friction-aware
approach significantly outperforms common alignment baselines in helping both
convergence to a common ground, or agreed-upon task-relevant propositions, and
correctness of task outcomes.

</details>


### [30] [Enhancing the Robustness of Contextual ASR to Varying Biasing Information Volumes Through Purified Semantic Correlation Joint Modeling](https://arxiv.org/abs/2509.05908)
*Yue Gu,Zhihao Du,Ying Shi,Shiliang Zhang,Qian Chen,Jiqing Han*

Main category: cs.CL

TL;DR: 通过多级语义相关性聚合和细粒度关联模型，精准识别并集成最相关的偏置信息，提升上下文ASR模型在不同长度偏置列表下的性能


<details>
  <summary>Details</summary>
Motivation: 解决交叉注意力模型在偏置信息量变化时效果不稳定的问题，特别是当偏置列表长度增加时效果下降

Method: 提出PSC-Joint方法，定义三级语义相关性（列表级、短语级、标记级），通过聚合模型获得其交集，突出最相关偏置信息，并使用分组竞争策略进行净化过滤

Result: 在AISHELL-1和KeSpeech数据集上实现了显著提升，F1指标相对收益分别达到21.34%和28.46%

Conclusion: PSC-Joint方法能够有效减少偏置信息量变化的影响，通过精准集成最相关信息来提升上下文ASR的性能和稳定性

Abstract: Recently, cross-attention-based contextual automatic speech recognition (ASR)
models have made notable advancements in recognizing personalized biasing
phrases. However, the effectiveness of cross-attention is affected by
variations in biasing information volume, especially when the length of the
biasing list increases significantly. We find that, regardless of the length of
the biasing list, only a limited amount of biasing information is most relevant
to a specific ASR intermediate representation. Therefore, by identifying and
integrating the most relevant biasing information rather than the entire
biasing list, we can alleviate the effects of variations in biasing information
volume for contextual ASR. To this end, we propose a purified semantic
correlation joint modeling (PSC-Joint) approach. In PSC-Joint, we define and
calculate three semantic correlations between the ASR intermediate
representations and biasing information from coarse to fine: list-level,
phrase-level, and token-level. Then, the three correlations are jointly modeled
to produce their intersection, so that the most relevant biasing information
across various granularities is highlighted and integrated for contextual
recognition. In addition, to reduce the computational cost introduced by the
joint modeling of three semantic correlations, we also propose a purification
mechanism based on a grouped-and-competitive strategy to filter out irrelevant
biasing phrases. Compared with baselines, our PSC-Joint approach achieves
average relative F1 score improvements of up to 21.34% on AISHELL-1 and 28.46%
on KeSpeech, across biasing lists of varying lengths.

</details>


### [31] [Accelerating Large Language Model Inference via Early-Exiting Algorithms](https://arxiv.org/abs/2509.05915)
*Sangmin Bae*

Main category: cs.CL

TL;DR: 这篇论文解决了大语言模型中适应性计算方法的系统级瓶颈问题，通过算法与架构的协同设计实现了效率与性能的最优平衡


<details>
  <summary>Details</summary>
Motivation: 大语言模型部署面临高计算成本，而早期退出等适应性计算方法并行化中引入的动态性反而造成系统瓶颈，降低批量推理吞吐量

Method: 提出高效并行解码机制、深度参数共享架构、轻量路由器预训练统一框架，动态分配每个token的最优递归深度

Result: 建立了效率与性能之间的新帕累托前沿，同时优化了适应性计算和参数效率

Conclusion: 通过算法与架构的协同设计，成功解决了动态计算中的系统瓶颈问题，为大语言模型的高效部署提供了新的解决方案

Abstract: Large language models have achieved remarkable capabilities, but their
practical deployment is hindered by significant computational costs. While
adaptive computation methods like early-exiting promise to reduce these costs,
they introduce a fundamental conflict: the per-token dynamism intended to save
computation often creates system-level bottlenecks that can paradoxically
reduce throughput in batched inference. This dissertation resolves this
conflict by co-designing adaptive algorithms and model architectures to strike
an optimal balance between dynamism and efficiency. To this end, our work first
addresses critical sources of overhead in conventional early-exiting by
proposing an efficient parallel decoding mechanism. We then show that deep
parameter sharing provides an architectural foundation that not only yields
compact, parameter-efficient models but also inherently mitigates the critical
synchronization issues affecting dynamic inference. Finally, this work presents
a unified framework where lightweight routers are pretrained to dynamically
assign an optimal recursion depth for each token. This approach establishes a
new Pareto frontier between efficiency and performance by effectively
optimizing for both adaptive computation and parameter efficiency within a
single model.

</details>


### [32] [KatotohananQA: Evaluating Truthfulness of Large Language Models in Filipino](https://arxiv.org/abs/2509.06065)
*Lorenzo Alfred Nery,Ronald Dawson Catignas,Thomas James Tiam-Lee*

Main category: cs.CL

TL;DR: KatotohananQA是TruthfulQA基准的菲律宾语翻译版本，用于评估大语言模型在低资源语言中的真实性表现。研究发现英语和菲律宾语之间存在显著性能差距，新模型表现出更好的多语言鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型真实性评估基准主要集中在英语，缺乏对低资源语言的评估，这限制了LLM在多语言环境中的可靠应用。

Method: 将TruthfulQA基准翻译成菲律宾语（KatotohananQA），使用二元选择框架评估7个免费专有模型，比较英语和菲律宾语版本的表现差异。

Result: 发现英语和菲律宾语真实性表现存在显著差距，新版OpenAI模型（GPT-5和GPT-5 mini）展现出较强的多语言鲁棒性，不同问题类型和主题的迁移鲁棒性存在差异。

Conclusion: 需要更广泛的多语言评估来确保LLM使用的公平性和可靠性，某些问题类型在跨语言迁移时表现较差，凸显了多语言基准的重要性。

Abstract: Large Language Models (LLMs) achieve remarkable performance across various
tasks, but their tendency to produce hallucinations limits reliable adoption.
Benchmarks such as TruthfulQA have been developed to measure truthfulness, yet
they are primarily available in English, leaving a gap in evaluating LLMs in
low-resource languages. To address this, we present KatotohananQA, a Filipino
translation of the TruthfulQA benchmark. Seven free-tier proprietary models
were assessed using a binary-choice framework. Findings show a significant
performance gap between English and Filipino truthfulness, with newer OpenAI
models (GPT-5 and GPT-5 mini) demonstrating strong multilingual robustness.
Results also reveal disparities across question characteristics, suggesting
that some question types, categories, and topics are less robust to
multilingual transfer which highlight the need for broader multilingual
evaluation to ensure fairness and reliability in LLM usage.

</details>


### [33] [Multimodal Fine-grained Context Interaction Graph Modeling for Conversational Speech Synthesis](https://arxiv.org/abs/2509.06074)
*Zhenqi Jia,Rui Liu,Berrak Sisman,Haizhou Li*

Main category: cs.CL

TL;DR: MFCIG-CSS是一个基于多模态细粒度上下文交互图的对话语音合成系统，通过构建语义和韵律两个细粒度交互图来提升对话语音的自然韵律表达


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注话语级别的交互建模，忽视了多模态对话历史中词级别的细粒度语义和韵律知识，导致生成的语音韵律不够自然

Method: 构建两个多模态细粒度对话交互图（语义交互图和韵律交互图），编码词级别语义、韵律及其对后续话语影响的交互特征

Result: 在DailyTalk数据集上的实验表明，MFCIG-CSS在韵律表现力方面优于所有基线模型

Conclusion: 提出的细粒度交互图方法能有效提升对话语音合成的韵律自然度，证明了词级别多模态交互建模的重要性

Abstract: Conversational Speech Synthesis (CSS) aims to generate speech with natural
prosody by understanding the multimodal dialogue history (MDH). The latest work
predicts the accurate prosody expression of the target utterance by modeling
the utterance-level interaction characteristics of MDH and the target
utterance. However, MDH contains fine-grained semantic and prosody knowledge at
the word level. Existing methods overlook the fine-grained semantic and
prosodic interaction modeling. To address this gap, we propose MFCIG-CSS, a
novel Multimodal Fine-grained Context Interaction Graph-based CSS system. Our
approach constructs two specialized multimodal fine-grained dialogue
interaction graphs: a semantic interaction graph and a prosody interaction
graph. These two interaction graphs effectively encode interactions between
word-level semantics, prosody, and their influence on subsequent utterances in
MDH. The encoded interaction features are then leveraged to enhance synthesized
speech with natural conversational prosody. Experiments on the DailyTalk
dataset demonstrate that MFCIG-CSS outperforms all baseline models in terms of
prosodic expressiveness. Code and speech samples are available at
https://github.com/AI-S2-Lab/MFCIG-CSS.

</details>


### [34] [Multimodal Reasoning for Science: Technical Report and 1st Place Solution to the ICML 2025 SeePhys Challenge](https://arxiv.org/abs/2509.06079)
*Hao Liang,Ruitao Wu,Bohan Zeng,Junbo Niu,Wentao Zhang,Bin Dong*

Main category: cs.CL

TL;DR: 提出了一种caption辅助的多模态推理框架，在ICML 2025 AI for Math Workshop挑战赛中获第一名，并在MathVerse基准测试中验证了其泛化能力


<details>
  <summary>Details</summary>
Motivation: 尽管文本推理取得了显著进展，但最先进的模型在多模态场景中仍表现不佳，需要解决视觉和文本模态之间的差距

Method: 引入caption辅助的推理框架，有效桥接视觉和文本模态

Result: 在ICML 2025 AI for Math Workshop挑战赛中获第一名，在MathVerse几何推理基准测试中展现了良好的泛化性能

Conclusion: 该方法在多模态推理方面表现出有效性和鲁棒性，代码已开源

Abstract: Multimodal reasoning remains a fundamental challenge in artificial
intelligence. Despite substantial advances in text-based reasoning, even
state-of-the-art models such as GPT-o3 struggle to maintain strong performance
in multimodal scenarios. To address this gap, we introduce a caption-assisted
reasoning framework that effectively bridges visual and textual modalities. Our
approach achieved 1st place in the ICML 2025 AI for Math Workshop \& Challenge
2: SeePhys, highlighting its effectiveness and robustness. Furthermore, we
validate its generalization on the MathVerse benchmark for geometric reasoning,
demonstrating the versatility of our method. Our code is publicly available at
https://github.com/OpenDCAI/SciReasoner.

</details>


### [35] [Orthogonal Low-rank Adaptation in Lie Groups for Continual Learning of Large Language Models](https://arxiv.org/abs/2509.06100)
*Kefan Cao,Shuaicheng Wu*

Main category: cs.CL

TL;DR: OLieRA方法通过引入李群理论和乘法更新来保护LLM参数几何结构，同时在任务子空间应用正交约束，在连续学习基准上取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 现有参数正则化方法如O-LoRA和N-LoRA虽然通过强制低秩子空间正交性来减轻任务干扰，但忽视了传统加法微调破坏了LLM参数的内在几何结构，限制了性能。

Method: 提出OLieRA方法，将李群理论引入LLM微调：利用乘法更新来保护参数几何结构，同时对任务子空间应用正交性约束。

Result: 实验表明OLieRA在标准连续学习基准上取得了最先进的结果，在大量任务设置中仍保持顶级性能。

Conclusion: 保护LLM参数空间的几何结构对于缓解灾难性遗忘至关重要，OLieRA通过结合李群理论和正交约束有效解决了这一问题。

Abstract: Large language models (LLMs) are prone to catastrophic forgetting in
sequential multi-task settings. Parameter regularization methods such as O-LoRA
and N-LoRA alleviate task interference by enforcing low-rank subspace
orthogonality, but they overlook the fact that conventional additive
fine-tuning disrupts the intrinsic geometric structure of LLM parameters,
limiting performance. Our key insight is that the parameter space of LLMs
possesses a geometric structure, which must be preserved in addition to
enforcing orthogonality. Based on this, we propose Orthogonal Low-rank
Adaptation in Lie Groups (OLieRA), which introduces Lie group theory into LLM
fine-tuning: leveraging multiplicative updates to preserve parameter geometry
while applying orthogonality constraints to task subspaces. Experiments
demonstrate that OLieRA achieves state-of-the-art results on the Standard CL
benchmark and remains among the top-performing methods in the Large Number of
Tasks setting.

</details>


### [36] [Benchmarking Gender and Political Bias in Large Language Models](https://arxiv.org/abs/2509.06164)
*Jinrui Yang,Xudong Han,Timothy Baldwin*

Main category: cs.CL

TL;DR: EuroParlVote是一个用于评估大语言模型在政治敏感语境中表现的新基准，包含欧洲议会辩论演讲与投票结果的关联数据，以及丰富的议员人口统计数据。研究发现LLMs存在性别和政治偏见。


<details>
  <summary>Details</summary>
Motivation: 为了评估大语言模型在政治敏感语境中的表现和偏见问题，特别是在欧洲议会这样的政治环境中，需要建立一个专门的基准数据集。

Method: 构建EuroParlVote数据集，包含欧洲议会辩论演讲、投票结果和议员人口统计数据，并在两个任务上评估LLMs：性别分类和投票预测。

Result: 发现LLMs存在系统性偏见：经常将女性议员误分类为男性，对女性演讲者的投票预测准确率较低；政治倾向上偏向中间派，对极左和极右团体表现较差；GPT-4o等专有模型在鲁棒性和公平性方面优于开源模型。

Conclusion: LLMs在政治敏感语境中存在显著的性别和政治偏见，需要进一步研究NLP在政治环境中的公平性和问责制。EuroParlVote数据集的发布将支持相关研究。

Abstract: We introduce EuroParlVote, a novel benchmark for evaluating large language
models (LLMs) in politically sensitive contexts. It links European Parliament
debate speeches to roll-call vote outcomes and includes rich demographic
metadata for each Member of the European Parliament (MEP), such as gender, age,
country, and political group. Using EuroParlVote, we evaluate state-of-the-art
LLMs on two tasks -- gender classification and vote prediction -- revealing
consistent patterns of bias. We find that LLMs frequently misclassify female
MEPs as male and demonstrate reduced accuracy when simulating votes for female
speakers. Politically, LLMs tend to favor centrist groups while underperforming
on both far-left and far-right ones. Proprietary models like GPT-4o outperform
open-weight alternatives in terms of both robustness and fairness. We release
the EuroParlVote dataset, code, and demo to support future research on fairness
and accountability in NLP within political contexts.

</details>


### [37] [Understanding the Influence of Synthetic Data for Text Embedders](https://arxiv.org/abs/2509.06184)
*Jacob Mitchell Springer,Vaibhav Adlakha,Siva Reddy,Aditi Raghunathan,Marius Mosbach*

Main category: cs.CL

TL;DR: 这篇论文研究了基于LLM生成的合成数据训练文本嵌入模型的效果，发现合成数据带来的改善是局部的且存在任务间的交换作用，挑战了合成数据能够建立更健壮的通用嵌入模型的观点。


<details>
  <summary>Details</summary>
Motivation: 当前通用文本嵌入器的发展依靠于越来越多的LLM生成合成数据训练，但缺乏公开的合成数据集阻碍了对其推广能力作用的研究。需要公开合成数据并系统分析其对模型泛化能力的影响。

Method: 首先重现并公开释放Wang等人提出的Mistral-E5合成数据，然后详细分析合成数据在哪些方面改善了模型泛化能力，包括份析不同任务间的性能交换作用。

Result: 合成数据虽然质量高且能够一致提升性能，但其带来的改善效果很稀疏且尽仅局限于特定数据集。同时观察到不同任务间的性能交换：对某个任务有益的数据可能会降低其他任务的性能。

Conclusion: 当前的合成数据方法在构建通用嵌入器方面存在显著局限性，合成数据训练不能弥补实际数据的缺失，也不能引导出更健壮的通用嵌入模型。

Abstract: Recent progress in developing general purpose text embedders has been driven
by training on ever-growing corpora of synthetic LLM-generated data.
Nonetheless, no publicly available synthetic dataset exists, posing a barrier
to studying its role for generalization. To address this issue, we first
reproduce and publicly release the synthetic data proposed by Wang et al.
(Mistral-E5). Our synthetic data is high quality and leads to consistent
improvements in performance. Next, we critically examine where exactly
synthetic data improves model generalization. Our analysis reveals that
benefits from synthetic data are sparse and highly localized to individual
datasets. Moreover, we observe trade-offs between the performance on different
categories and data that benefits one task, degrades performance on another.
Our findings highlight the limitations of current synthetic data approaches for
building general-purpose embedders and challenge the notion that training on
synthetic data leads to more robust embedding models across tasks.

</details>


### [38] [Augmented Fine-Tuned LLMs for Enhanced Recruitment Automation](https://arxiv.org/abs/2509.06196)
*Mohamed T. Younes,Omar Walid,Khaled Shaban,Ali Hamdi,Mai Hassan*

Main category: cs.CL

TL;DR: 基于精细调整的大语言模型，提出了一种新的招聘自动化方法，通过合成数据集和结构化JSON格式显著提高了性能指标。


<details>
  <summary>Details</summary>
Motivation: 解决通用大语言模型在招聘任务中的局限性，提高准确性和效率，以改善合适的候选人對工作匹配。

Method: 使用标准化JSON格式创建合成数据集，通过DeepSeek解析简历并转换为结构化数据，对Phi-4等LLM进行专门的精细调整。

Result: 精细调整后的Phi-4模型在F1分数上达到90.62%，在exact match、BLEU、ROUGE等性能指标上显著超越基础模型和其他最先进模型。

Conclusion: 精细调整的LLM在招聘自动化中具有巨大潜力，能够通过提供更准确的候选人對工作匹配来革命招聘工作流程。

Abstract: This paper presents a novel approach to recruitment automation. Large
Language Models (LLMs) were fine-tuned to improve accuracy and efficiency.
Building upon our previous work on the Multilayer Large Language Model-Based
Robotic Process Automation Applicant Tracking (MLAR) system . This work
introduces a novel methodology. Training fine-tuned LLMs specifically tuned for
recruitment tasks. The proposed framework addresses the limitations of generic
LLMs by creating a synthetic dataset that uses a standardized JSON format. This
helps ensure consistency and scalability. In addition to the synthetic data
set, the resumes were parsed using DeepSeek, a high-parameter LLM. The resumes
were parsed into the same structured JSON format and placed in the training
set. This will help improve data diversity and realism. Through
experimentation, we demonstrate significant improvements in performance
metrics, such as exact match, F1 score, BLEU score, ROUGE score, and overall
similarity compared to base models and other state-of-the-art LLMs. In
particular, the fine-tuned Phi-4 model achieved the highest F1 score of 90.62%,
indicating exceptional precision and recall in recruitment tasks. This study
highlights the potential of fine-tuned LLMs. Furthermore, it will revolutionize
recruitment workflows by providing more accurate candidate-job matching.

</details>


### [39] [MSLEF: Multi-Segment LLM Ensemble Finetuning in Recruitment](https://arxiv.org/abs/2509.06200)
*Omar Walid,Mohamed T. Younes,Khaled Shaban,Mai Hassan,Ali Hamdi*

Main category: cs.CL

TL;DR: MSLEF是一个多段集成框架，通过LLM微调提升简历解析性能，采用加权投票集成多个专业模型，在不同评估指标上显著优于单模型系统


<details>
  <summary>Details</summary>
Motivation: 解决招聘自动化中简历解析的准确性问题，克服单模型系统在处理多样化简历格式和结构时的局限性

Method: 采用分段感知架构，使用Gemini-2.5-Flash作为高级聚合器处理复杂段落，集成Gemma 9B、LLaMA 3.1 8B和Phi-4 14B模型，通过字段特定加权和加权投票机制

Result: 在Exact Match、F1分数、BLEU、ROUGE和Recruitment Similarity等指标上取得显著提升，比最佳单模型在RS指标上高出+7%

Conclusion: MSLEF的分段感知设计增强了跨不同简历布局的泛化能力，使其高度适应现实招聘场景，同时确保精确可靠的候选人表示

Abstract: This paper presents MSLEF, a multi-segment ensemble framework that employs
LLM fine-tuning to enhance resume parsing in recruitment automation. It
integrates fine-tuned Large Language Models (LLMs) using weighted voting, with
each model specializing in a specific resume segment to boost accuracy.
Building on MLAR , MSLEF introduces a segment-aware architecture that leverages
field-specific weighting tailored to each resume part, effectively overcoming
the limitations of single-model systems by adapting to diverse formats and
structures. The framework incorporates Gemini-2.5-Flash LLM as a high-level
aggregator for complex sections and utilizes Gemma 9B, LLaMA 3.1 8B, and Phi-4
14B. MSLEF achieves significant improvements in Exact Match (EM), F1 score,
BLEU, ROUGE, and Recruitment Similarity (RS) metrics, outperforming the best
single model by up to +7% in RS. Its segment-aware design enhances
generalization across varied resume layouts, making it highly adaptable to
real-world hiring scenarios while ensuring precise and reliable candidate
representation.

</details>


### [40] [No Encore: Unlearning as Opt-Out in Music Generation](https://arxiv.org/abs/2509.06277)
*Jinju Kim,Taehan Kim,Abdul Waheed,Rita Singh*

Main category: cs.CL

TL;DR: 本文研究机器忘印技术在文本到音乐生成模型中的应用，以防止侵权创作的不当使用


<details>
  <summary>Details</summary>
Motivation: AI音乐生成系统可能带来侵权风险和法律问题，需要解决意外使用受版权保护内容的问题

Method: 将现有的机器忘印技术应用于预训练的文本到音乐生成基线模型，分析其在删除预训练数据集时的效果

Result: 实验结果提供了机器忘印在音乐生成领域应用的挑战见解

Conclusion: 本研究为未来音乐生成模型中应用忘印技术提供了基础性分析和指导

Abstract: AI music generation is rapidly emerging in the creative industries, enabling
intuitive music generation from textual descriptions. However, these systems
pose risks in exploitation of copyrighted creations, raising ethical and legal
concerns. In this paper, we present preliminary results on the first
application of machine unlearning techniques from an ongoing research to
prevent inadvertent usage of creative content. Particularly, we explore
existing methods in machine unlearning to a pre-trained Text-to-Music (TTM)
baseline and analyze their efficacy in unlearning pre-trained datasets without
harming model performance. Through our experiments, we provide insights into
the challenges of applying unlearning in music generation, offering a
foundational analysis for future works on the application of unlearning for
music generative models.

</details>


### [41] [Mask-GCG: Are All Tokens in Adversarial Suffixes Necessary for Jailbreak Attacks?](https://arxiv.org/abs/2509.06350)
*Junjie Mu,Zonghao Ying,Zhekui Fan,Zonglei Jing,Yaoyuan Zhang,Zhengmin Yu,Wenxin Zhang,Quanchen Zou,Xiangzheng Zhang*

Main category: cs.CL

TL;DR: Mask-GCG是一种改进的越狱攻击方法，通过可学习的token掩码识别后缀中关键token，修剪低影响力token来减少冗余和计算开销，同时保持攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的GCG越狱攻击方法使用固定长度的后缀，其中可能存在冗余token未被探索，影响了攻击效率和计算性能。

Method: 提出Mask-GCG方法，使用可学习的token掩码来识别后缀中高影响力位置，增加这些位置的更新概率，同时修剪低影响力位置的token，减少梯度空间大小。

Result: 实验表明后缀中大部分token对攻击成功贡献显著，修剪少量低影响力token不会影响损失值或攻击成功率，揭示了LLM提示中的token冗余现象。

Conclusion: 该方法为从越狱攻击角度开发高效和可解释的LLM提供了见解，能够降低计算开销并缩短成功攻击时间。

Abstract: Jailbreak attacks on Large Language Models (LLMs) have demonstrated various
successful methods whereby attackers manipulate models into generating harmful
responses that they are designed to avoid. Among these, Greedy Coordinate
Gradient (GCG) has emerged as a general and effective approach that optimizes
the tokens in a suffix to generate jailbreakable prompts. While several
improved variants of GCG have been proposed, they all rely on fixed-length
suffixes. However, the potential redundancy within these suffixes remains
unexplored. In this work, we propose Mask-GCG, a plug-and-play method that
employs learnable token masking to identify impactful tokens within the suffix.
Our approach increases the update probability for tokens at high-impact
positions while pruning those at low-impact positions. This pruning not only
reduces redundancy but also decreases the size of the gradient space, thereby
lowering computational overhead and shortening the time required to achieve
successful attacks compared to GCG. We evaluate Mask-GCG by applying it to the
original GCG and several improved variants. Experimental results show that most
tokens in the suffix contribute significantly to attack success, and pruning a
minority of low-impact tokens does not affect the loss values or compromise the
attack success rate (ASR), thereby revealing token redundancy in LLM prompts.
Our findings provide insights for developing efficient and interpretable LLMs
from the perspective of jailbreak attacks.

</details>


### [42] [PL-CA: A Parametric Legal Case Augmentation Framework](https://arxiv.org/abs/2509.06356)
*Ao Chang,Yubo Chen,Jun Zhao*

Main category: cs.CL

TL;DR: PL-CA提出参数化RAG框架，将法律知识编码到参数向量中，通过LoRA集成到LLM前馈网络，缓解上下文压力，并在专家标注的多任务法律数据集上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法直接将检索文档注入模型上下文，受限于有限的上下文窗口，引入额外计算开销，破坏模型注意力，降低下游任务性能。现有基准缺乏专家标注且仅关注单一任务，无法反映模型在真实法律场景中的能力。

Method: 提出参数化RAG(P-RAG)框架，对语料知识进行数据增强并编码为参数向量，通过LoRA将参数化知识集成到LLM的前馈网络中。构建了包含2000+专家标注实例的多任务法律数据集。

Result: 实验结果表明，该方法在减少过长上下文开销的同时，在下游任务上保持了与传统RAG竞争的性能。

Conclusion: PL-CA通过参数化知识集成有效缓解了模型上下文压力，为法律领域的知识增强提供了新思路，代码和数据集已公开。

Abstract: Conventional RAG is considered one of the most effective methods for
addressing model knowledge insufficiency and hallucination, particularly in the
judicial domain that requires high levels of knowledge rigor, logical
consistency, and content integrity. However, the conventional RAG method only
injects retrieved documents directly into the model's context, which severely
constrains models due to their limited context windows and introduces
additional computational overhead through excessively long contexts, thereby
disrupting models' attention and degrading performance on downstream tasks.
Moreover, many existing benchmarks lack expert annotation and focus solely on
individual downstream tasks while real-world legal scenarios consist of
multiple mixed legal tasks, indicating conventional benchmarks' inadequacy for
reflecting models' true capabilities. To address these limitations, we propose
PL-CA, which introduces a parametric RAG (P-RAG) framework to perform data
augmentation on corpus knowledge and encode this legal knowledge into
parametric vectors, and then integrates this parametric knowledge into the
LLM's feed-forward networks (FFN) via LoRA, thereby alleviating models' context
pressure. Additionally, we also construct a multi-task legal dataset comprising
more than 2000 training and test instances, which are all expert-annotated and
manually verified. We conduct our experiments on our dataset, and the
experimental results demonstrate that our method reduces the overhead
associated with excessively long contexts while maintaining competitive
performance on downstream tasks compared to conventional RAG. Our code and
dataset are provided in the appendix.

</details>


### [43] [Do LLMs exhibit the same commonsense capabilities across languages?](https://arxiv.org/abs/2509.06401)
*Ivan Martínez-Murillo,Elena Lloret,Paloma Moreda,Albert Gatt*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型的多语言常识生成能力，通过新基准MULTICOM在英语、西班牙语、荷兰语和瓦伦西亚语上的评估，发现英语表现最优，低资源语言表现较差。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在多语言常识生成方面的能力，特别是针对不同资源水平的语言进行比较分析。

Method: 构建MULTICOM基准数据集，扩展COCOTEROS至四种语言；评估多个开源LLM（LLaMA、Qwen、Gemma等）；结合自动指标、LLM-as-a-judge方法和人工标注进行综合评估。

Result: 英语表现最佳，低资源语言（如瓦伦西亚语）表现显著较差；上下文支持对低资源语言有一定帮助但效果不一。

Conclusion: 当前LLM在多语言常识生成方面存在明显局限性，特别是在低资源语言上表现不佳，需要进一步改进。

Abstract: This paper explores the multilingual commonsense generation abilities of
Large Language Models (LLMs). To facilitate this investigation, we introduce
MULTICOM, a novel benchmark that extends the COCOTEROS dataset to four
languages: English, Spanish, Dutch, and Valencian. The task involves generating
a commonsensical sentence that includes a given triplet of words. We evaluate a
range of open-source LLMs, including LLaMA, Qwen, Gemma, EuroLLM, and
Salamandra, on this benchmark. Our evaluation combines automatic metrics,
LLM-as-a-judge approaches (using Prometheus and JudgeLM), and human
annotations. Results consistently show superior performance in English, with
significantly lower performance in less-resourced languages. While contextual
support yields mixed results, it tends to benefit underrepresented languages.
These findings underscore the current limitations of LLMs in multilingual
commonsense generation. The dataset is publicly available at
https://huggingface.co/datasets/gplsi/MULTICOM.

</details>


### [44] [WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents](https://arxiv.org/abs/2509.06501)
*Junteng Liu,Yunji Li,Chi Zhang,Jingyang Li,Aili Chen,Ke Ji,Weiyu Cheng,Zijia Wu,Chengyu Du,Qidi Xu,Jiayuan Song,Zhengmao Zhu,Wenhu Chen,Pengyu Zhao,Junxian He*

Main category: cs.CL

TL;DR: 使用模型基于探索和迭代查询演化的系统性数据生成方法WebExplorer，创建具有多步推理和复杂网页导航的挑战性查询-答案对，通过监督微调和强化学习成功开发了支持128K上下文长度和100个工具调用轮次的WebExplorer-8B模型，在多个信息搜索性能测试中达到同规模下最佳性能。


<details>
  <summary>Details</summary>
Motivation: 现有开源网络代理要么在复杂任务上信息搜索能力有限，要么缺乏透明实现，关键挑战在于缺乏挑战性的信息搜索数据。

Method: 提出WebExplorer数据生成方法：使用模型基于探索和迭代、长到短查询演化的系统性方法创建挑战性查询-答案对，通过监督微调和强化学习培养网络代理。

Result: WebExplorer-8B模型支持128K上下文长度和100个工具调用轮次，在BrowseComp-en/zh上超过WebSailor-72B，在WebWalkerQA和FRAMES上达到最高水平，平均搜索16个轮次，同时在HLE指标上也显示出强大的过渡性能。

Conclusion: WebExplorer方法为开发长期期限网络代理提供了一条实用的路径，通过系统性的挑战性数据生成和迭代训练方法，小规模模型也能够实现优异的网络信息搜索能力。

Abstract: The paradigm of Large Language Models (LLMs) has increasingly shifted toward
agentic applications, where web browsing capabilities are fundamental for
retrieving information from diverse online sources. However, existing
open-source web agents either demonstrate limited information-seeking abilities
on complex tasks or lack transparent implementations. In this work, we identify
that the key challenge lies in the scarcity of challenging data for information
seeking. To address this limitation, we introduce WebExplorer: a systematic
data generation approach using model-based exploration and iterative,
long-to-short query evolution. This method creates challenging query-answer
pairs that require multi-step reasoning and complex web navigation. By
leveraging our curated high-quality dataset, we successfully develop advanced
web agent WebExplorer-8B through supervised fine-tuning followed by
reinforcement learning. Our model supports 128K context length and up to 100
tool calling turns, enabling long-horizon problem solving. Across diverse
information-seeking benchmarks, WebExplorer-8B achieves the state-of-the-art
performance at its scale. Notably, as an 8B-sized model, WebExplorer-8B is able
to effectively search over an average of 16 turns after RL training, achieving
higher accuracy than WebSailor-72B on BrowseComp-en/zh and attaining the best
performance among models up to 100B parameters on WebWalkerQA and FRAMES.
Beyond these information-seeking tasks, our model also achieves strong
generalization on the HLE benchmark even though it is only trained on
knowledge-intensive QA data. These results highlight our approach as a
practical path toward long-horizon web agents.

</details>


### [45] [Crown, Frame, Reverse: Layer-Wise Scaling Variants for LLM Pre-Training](https://arxiv.org/abs/2509.06518)
*Andrei Baroian,Kasper Notebomer*

Main category: cs.CL

TL;DR: 本文提出了三种新的层间缩放变体（Framed、Reverse、Crown），通过重新分配FFN宽度和注意力头来优化Transformer架构，在相同参数预算下实现了比各向同性基线更好的性能。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer模型使用统一的层大小，忽略了不同深度层可能具有不同的功能角色和计算容量需求。

Method: 基于层间缩放和剪枝文献，提出了三种新的LWS变体，在预训练阶段通过两点或三点线性插值重新分配FFN宽度和注意力头。

Result: 在180M参数预算和5B token训练下，所有模型都收敛到相似的损失，相比等成本的各向同性基线获得了更好的性能，且训练吞吐量没有显著下降。

Conclusion: 这项工作代表了预训练层间架构设计空间的初步探索，未来需要在更大规模的token和参数上进行实验以充分评估其潜力。

Abstract: Transformer-based language models traditionally use uniform (isotropic) layer
sizes, yet they ignore the diverse functional roles that different depths can
play and their computational capacity needs. Building on Layer-Wise Scaling
(LWS) and pruning literature, we introduce three new LWS variants - Framed,
Reverse, and Crown - that redistribute FFN widths and attention heads via two
or three-point linear interpolation in the pre-training stage. We present the
first systematic ablation of LWS and its variants, on a fixed budget of 180M
parameters, trained on 5B tokens. All models converge to similar losses and
achieve better performance compared to an equal-cost isotropic baseline,
without a substantial decrease in training throughput. This work represents an
initial step into the design space of layer-wise architectures for
pre-training, but future work should scale experiments to orders of magnitude
more tokens and parameters to fully assess their potential.

</details>


### [46] [LAMDAS: LLM as an Implicit Classifier for Domain-specific Data Selection](https://arxiv.org/abs/2509.06524)
*Jian Wu,Hang Yu,Bingchang Liu,Wenjie Yang,Peng Di,Jianguo Li,Yue Zhang*

Main category: cs.CL

TL;DR: LAMDAS是一种新的数据选择方法，利用预训练LLM作为隐式分类器来解决领域适应中的数据稀缺问题，无需显式特征工程或计算密集型优化。


<details>
  <summary>Details</summary>
Motivation: 领域适应中高质量人工标注数据稀缺，大量未检查数据直接用于微调会引入噪声并降低性能，需要既准确又高效的数据选择方法。

Method: 将数据选择重新定义为单类分类问题，利用预训练LLM作为隐式分类器，通过小规模参考数据集识别属于目标域的候选数据。

Result: LAMDAS不仅使用少量数据就超过了全数据训练的性能，而且在各种场景下都优于9个最先进的基线方法。

Conclusion: LAMDAS在性能提升和计算效率之间实现了最佳平衡，是领域特定数据选择的有效解决方案。

Abstract: Adapting large language models (LLMs) to specific domains often faces a
critical bottleneck: the scarcity of high-quality, human-curated data. While
large volumes of unchecked data are readily available, indiscriminately using
them for fine-tuning risks introducing noise and degrading performance.
Strategic data selection is thus crucial, requiring a method that is both
accurate and efficient. Existing approaches, categorized as similarity-based
and direct optimization methods, struggle to simultaneously achieve these
goals. In this paper, we introduce LAMDAS (LLM As an iMplicit classifier for
domain-specific DAta Selection), a novel approach that leverages the
pre-trained LLM itself as an implicit classifier, thereby bypassing explicit
feature engineering and computationally intensive optimization process. LAMDAS
reframes data selection as a one-class classification problem, identifying
candidate data that "belongs" to the target domain defined by a small reference
dataset. Extensive experimental results demonstrate that LAMDAS not only
exceeds the performance of full-data training using a fraction of the data but
also outperforms nine state-of-the-art (SOTA) baselines under various
scenarios. Furthermore, LAMDAS achieves the most compelling balance between
performance gains and computational efficiency compared to all evaluated
baselines.

</details>


### [47] [SLiNT: Structure-aware Language Model with Injection and Contrastive Training for Knowledge Graph Completion](https://arxiv.org/abs/2509.06531)
*Mengxue Yang,Chun Yang,Jiaqi Zhu,Jiafan Li,Jingqi Zhang,Yuyang Li,Ying Li*

Main category: cs.CL

TL;DR: SLiNT是一个结构感知的语言模型框架，通过注入知识图谱结构信息和使用对比学习来解决链接预测中的结构稀疏和语义模糊问题，在标准数据集上取得了优异性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在知识图谱链接预测中虽然具有强大的生成推理能力，但对结构信息的利用不足，导致结构稀疏和语义模糊问题，特别是在不完整或零样本设置下。

Method: 提出了SLiNT框架：1) SGNE检索伪邻居丰富稀疏实体；2) DHCL通过插值难正负样本进行细粒度监督；3) GDDI进行token级结构感知干预同时保持核心LLM参数不变。使用轻量级LoRA适配。

Result: 在WN18RR和FB15k-237数据集上的实验表明，SLiNT相比基于嵌入和基于生成的基线方法都取得了优越或竞争性的性能。

Conclusion: 该研究证明了结构感知表示学习对于可扩展知识图谱补全的有效性，通过将知识图谱结构信息注入冻结的LLM主干，实现了鲁棒的链接预测。

Abstract: Link prediction in knowledge graphs requires integrating structural
information and semantic context to infer missing entities. While large
language models offer strong generative reasoning capabilities, their limited
exploitation of structural signals often results in structural sparsity and
semantic ambiguity, especially under incomplete or zero-shot settings. To
address these challenges, we propose SLiNT (Structure-aware Language model with
Injection and coNtrastive Training), a modular framework that injects
knowledge-graph-derived structural context into a frozen LLM backbone with
lightweight LoRA-based adaptation for robust link prediction. Specifically,
Structure-Guided Neighborhood Enhancement (SGNE) retrieves pseudo-neighbors to
enrich sparse entities and mitigate missing context; Dynamic Hard Contrastive
Learning (DHCL) introduces fine-grained supervision by interpolating hard
positives and negatives to resolve entity-level ambiguity; and
Gradient-Decoupled Dual Injection (GDDI) performs token-level structure-aware
intervention while preserving the core LLM parameters. Experiments on WN18RR
and FB15k-237 show that SLiNT achieves superior or competitive performance
compared with both embedding-based and generation-based baselines,
demonstrating the effectiveness of structure-aware representation learning for
scalable knowledge graph completion.

</details>


### [48] [HAVE: Head-Adaptive Gating and ValuE Calibration for Hallucination Mitigation in Large Language Models](https://arxiv.org/abs/2509.06596)
*Xin Tong,Zhi Lin,Jingya Wang,Bo Jin*

Main category: cs.CL

TL;DR: HAVE是一个无需微调的参数自由解码框架，通过头自适应门控和值校准来解决LLM幻觉问题，在多个QA基准测试中显著减少幻觉并优于现有方法


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在检索增强生成长上下文生成中经常产生幻觉，即使相关证据存在。这源于两个问题：头重要性被视为输入无关的，原始注意力权重不能很好地反映每个token的真实贡献

Method: HAVE框架包含头自适应门控（实例级软重加权注意力头）和值校准（通过值向量大小增强注意力来近似写回贡献），通过轻量级不确定性缩放策略将token级证据与语言模型分布融合

Result: 在多个QA基准测试和LLM家族上的实验表明，HAVE持续减少幻觉并优于强基线方法（包括DAGCD），且开销适中

Conclusion: HAVE框架透明、可复现，可轻松与现成的LLM集成，在现实场景中推进可信生成

Abstract: Large Language Models (LLMs) often produce hallucinations in
retrieval-augmented or long-context generation, even when relevant evidence is
present. This stems from two issues: head importance is treated as
input-agnostic, and raw attention weights poorly reflect each token's true
contribution. We present HAVE (Head-Adaptive Gating and ValuE Calibration), a
parameter-free decoding framework that directly addresses both challenges. HAVE
introduces head-adaptive gating, which performs instance-level soft reweighing
of attention heads, and value calibration, which augments attention with the
magnitude of value vectors to approximate write-back contribution. Together,
these modules construct token-level evidence aligned with model updates and
fuse it with the LM distribution through a lightweight uncertainty-scaled
policy. HAVE requires no finetuning and operates in a single forward pass,
making it efficient and broadly applicable. Experiments across multiple QA
benchmarks and LLM families demonstrate that HAVE consistently reduces
hallucinations and outperforms strong baselines, including DAGCD, with modest
overhead. The framework is transparent, reproducible, and readily integrates
with off-the-shelf LLMs, advancing trustworthy generation in real-world
settings.

</details>


### [49] [Guided Decoding and Its Critical Role in Retrieval-Augmented Generation](https://arxiv.org/abs/2509.06631)
*Özgür Uğur,Musa Yılmaz,Esra Şavirdi,Özay Ezerceli,Mahmut El Huseyni,Selva Taş,Reyhan Bayraktar*

Main category: cs.CL

TL;DR: 本研究比较了三种引导解码方法（Outlines、XGrammar、LM Format Enforcer）在不同多轮提示设置下的性能，发现在RAG系统中多轮交互对结构化输出生成有显著影响。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在各类应用中的集成，需要确保输出的结构化和可靠性。RAG系统面临的关键挑战是保证输出符合预期格式同时减少幻觉。

Method: 比较三种引导解码方法（Outlines、XGrammar、LM Format Enforcer），在0轮、1轮和2轮多轮提示设置下进行评估，通过成功率、幻觉率和输出质量等指标进行分析。

Result: 研究揭示了多轮交互如何影响引导解码，发现了意想不到的性能变化，这些发现可以为特定用例的方法选择提供参考。

Conclusion: 这项工作推进了对RAG系统中结构化输出生成的理解，为大语言模型部署提供了理论见解和实践指导。

Abstract: The integration of Large Language Models (LLMs) into various applications has
driven the need for structured and reliable responses. A key challenge in
Retrieval-Augmented Generation (RAG) systems is ensuring that outputs align
with expected formats while minimizing hallucinations. This study examines the
role of guided decoding in RAG systems, comparing three methods, Outlines,
XGrammar, and LM Format Enforcer, across different multi-turn prompting setups
(0-turn, 1-turn, and 2-turn). By evaluating success rates, hallucination rates,
and output quality, we provide insights into their performance and
applicability. Our findings reveal how multi-turn interactions influence guided
decoding, uncovering unexpected performance variations that can inform method
selection for specific use cases. This work advances the understanding of
structured output generation in RAG systems, offering both theoretical insights
and practical guidance for LLM deployment.

</details>


### [50] [Modelling Intertextuality with N-gram Embeddings](https://arxiv.org/abs/2509.06637)
*Yi Xing*

Main category: cs.CL

TL;DR: 提出了一种基于n-gram嵌入的量化互文性分析模型，通过文本对比较和网络分析实现可扩展的文学文本关系研究


<details>
  <summary>Details</summary>
Motivation: 互文性是文学研究中的核心概念，但传统方法难以进行大规模量化分析，需要开发新的计算模型来实现可扩展的互文性研究

Method: 使用n-gram嵌入技术对文本进行表征，通过成对比较两个文本的n-gram嵌入并平均结果来量化互文性程度

Result: 在4个已知互文性程度的文本上验证有效，并在267个多样化文本上测试了可扩展性，网络分析揭示了中心性和社区结构

Conclusion: 该方法成功捕捉和量化了互文性关系，为文学研究提供了有效的计算分析工具

Abstract: Intertextuality is a central tenet in literary studies. It refers to the
intricate links between literary texts that are created by various types of
references. This paper proposes a new quantitative model of intertextuality to
enable scalable analysis and network-based insights: perform pairwise
comparisons of the embeddings of n-grams from two texts and average their
results as the overall intertextuality. Validation on four texts with known
degrees of intertextuality, alongside a scalability test on 267 diverse texts,
demonstrates the method's effectiveness and efficiency. Network analysis
further reveals centrality and community structures, affirming the approach's
success in capturing and quantifying intertextual relationships.

</details>


### [51] [Domain-Aware RAG: MoL-Enhanced RL for Efficient Training and Scalable Retrieval](https://arxiv.org/abs/2509.06650)
*Hao Lin,Peitong Xie,Jingxue Chen,Jie Lin,Qingkun Tang,Qianchun Lu*

Main category: cs.CL

TL;DR: MoLER是一个基于混合损失增强强化学习的领域感知RAG方法，通过两阶段训练优化检索性能，在基准数据集上达到最先进水平


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统的粗排序优化方法难以平衡领域特定知识学习和查询增强，导致检索性能不佳

Method: 两阶段管道：1）使用混合损失（MoL）进行持续预训练，平衡领域知识和通用语言能力；2）使用组相对策略优化（GRPO）进行强化学习，优化查询和段落生成以最大化文档召回率；采用多查询单段落后期融合（MSLF）策略减少计算开销

Result: 在基准数据集上的广泛实验表明，MoLER实现了最先进的性能，显著优于基线方法

Conclusion: MoLER填补了RAG系统中的知识鸿沟，能够在专业领域实现稳健且可扩展的检索

Abstract: Retrieval-Augmented Generation (RAG) systems rely heavily on the retrieval
stage, particularly the coarse-ranking process. Existing coarse-ranking
optimization approaches often struggle to balance domain-specific knowledge
learning with query enhencement, resulting in suboptimal retrieval performance.
To address this challenge, we propose MoLER, a domain-aware RAG method that
uses MoL-Enhanced Reinforcement Learning to optimize retrieval. MoLER has a
two-stage pipeline: a continual pre-training (CPT) phase using a Mixture of
Losses (MoL) to balance domain-specific knowledge with general language
capabilities, and a reinforcement learning (RL) phase leveraging Group Relative
Policy Optimization (GRPO) to optimize query and passage generation for
maximizing document recall. A key innovation is our Multi-query Single-passage
Late Fusion (MSLF) strategy, which reduces computational overhead during RL
training while maintaining scalable inference via Multi-query Multi-passage
Late Fusion (MMLF). Extensive experiments on benchmark datasets show that MoLER
achieves state-of-the-art performance, significantly outperforming baseline
methods. MoLER bridges the knowledge gap in RAG systems, enabling robust and
scalable retrieval in specialized domains.

</details>


### [52] [IntrEx: A Dataset for Modeling Engagement in Educational Conversations](https://arxiv.org/abs/2509.06652)
*Xingwei Tan,Mahathi Parvatham,Chiara Gambi,Gabriele Pergola*

Main category: cs.CL

TL;DR: IntrEx是首个针对教师-学生互动中趣味性和预期趣味性的大规模标注数据集，用于研究教育对话中的参与度特征，发现微调后的LLM在预测趣味性方面优于GPT-4o等大型模型。


<details>
  <summary>Details</summary>
Motivation: 二语习得中学习者的参与度和动机至关重要，但现有研究主要关注教育文本的趣味性，对对话中驱动参与度的语言特征了解甚少。

Method: 基于教师-学生聊天室语料库构建IntrEx数据集，采用序列级标注和基于比较的评分方法，使用100多名二语学习者进行严格标注，并研究LLM预测人类趣味性判断的能力。

Result: 微调后的7B/8B参数LLM在预测趣味性方面表现优于GPT-4o等大型专有模型，证明了专业数据集在教育场景中建模参与度的潜力。

Conclusion: 研究揭示了具体性、可理解性和吸收度等语言认知因素对教育对话参与度的影响，为教育技术中的参与度建模提供了重要基础。

Abstract: Engagement and motivation are crucial for second-language acquisition, yet
maintaining learner interest in educational conversations remains a challenge.
While prior research has explored what makes educational texts interesting,
still little is known about the linguistic features that drive engagement in
conversations. To address this gap, we introduce IntrEx, the first large
dataset annotated for interestingness and expected interestingness in
teacher-student interactions. Built upon the Teacher-Student Chatroom Corpus
(TSCC), IntrEx extends prior work by incorporating sequence-level annotations,
allowing for the study of engagement beyond isolated turns to capture how
interest evolves over extended dialogues. We employ a rigorous annotation
process with over 100 second-language learners, using a comparison-based rating
approach inspired by reinforcement learning from human feedback (RLHF) to
improve agreement. We investigate whether large language models (LLMs) can
predict human interestingness judgments. We find that LLMs (7B/8B parameters)
fine-tuned on interestingness ratings outperform larger proprietary models like
GPT-4o, demonstrating the potential for specialised datasets to model
engagement in educational settings. Finally, we analyze how linguistic and
cognitive factors, such as concreteness, comprehensibility (readability), and
uptake, influence engagement in educational dialogues.

</details>


### [53] [ParCzech4Speech: A New Speech Corpus Derived from Czech Parliamentary Data](https://arxiv.org/abs/2509.06675)
*Vladislav Stankov,Matyáš Kopp,Ondřej Bojar*

Main category: cs.CL

TL;DR: ParCzech4Speech 1.0是基于ParCzech 4.0语料库处理后的版本，专门针对语音建模任务，包含2,695小时捷克议会演讲数据，提供三种灵活变体格式。


<details>
  <summary>Details</summary>
Motivation: 为语音建模任务提供高质量的捷克语语音-文本对齐数据，改进ParCzech 3.0版本的数据质量和可靠性。

Method: 结合捷克议会演讲录音和官方转录文本，使用WhisperX和Wav2Vec 2.0进行自动音频-文本对齐处理。

Result: 创建了包含2,695小时数据的语料库，提供三种变体：句子分割版、未分割版和原始对齐版，具有更高的对齐可靠性。

Conclusion: ParCzech4Speech 1.0为捷克语语音识别和合成任务提供了高质量的数据资源，采用宽松的CC-BY许可协议，可在LINDAT和Hugging Face获取。

Abstract: We introduce ParCzech4Speech 1.0, a processed version of the ParCzech 4.0
corpus, targeted at speech modeling tasks with the largest variant containing
2,695 hours. We combined the sound recordings of the Czech parliamentary
speeches with the official transcripts. The recordings were processed with
WhisperX and Wav2Vec 2.0 to extract automated audio-text alignment. Our
processing pipeline improves upon the ParCzech 3.0 speech recognition version
by extracting more data with higher alignment reliability. The dataset is
offered in three flexible variants: (1) sentence-segmented for automatic speech
recognition and speech synthesis tasks with clean boundaries, (2) unsegmented
preserving original utterance flow across sentences, and (3) a raw-alignment
for further custom refinement for other possible tasks. All variants maintain
the original metadata and are released under a permissive CC-BY license. The
dataset is available in the LINDAT repository, with the sentence-segmented and
unsegmented variants additionally available on Hugging Face.

</details>


### [54] [Will Annotators Disagree? Identifying Subjectivity in Value-Laden Arguments](https://arxiv.org/abs/2509.06704)
*Amir Homayounirad,Enrico Liscio,Tong Wang,Catholijn M. Jonker,Luciano C. Siebert*

Main category: cs.CL

TL;DR: 识别论证中的主观性，直接主观性识别方法显著提升模型性能，可以发现个体理解存在差异的论证


<details>
  <summary>Details</summary>
Motivation: 多个标注合并为单一真实标签可能隐藏了标注者分歧的价值，特别是在主观性起关键作用的任务中

Method: 评估两种主要方法：通过价值预测推断主观性 vs 直接识别主观性，对比对比损失与二元交叉瑭损失的组合效果

Result: 直接主观性识别方法显著提升了标记主观论证的模型性能，而对比损失与二元交叉瑭损失组合并未提升性能但减少了对每个标签主观性的依赖性

Conclusion: 提出的方法能够帮助识别个体理解存在差异的论证，促进更细致的标注过程

Abstract: Aggregating multiple annotations into a single ground truth label may hide
valuable insights into annotator disagreement, particularly in tasks where
subjectivity plays a crucial role. In this work, we explore methods for
identifying subjectivity in recognizing the human values that motivate
arguments. We evaluate two main approaches: inferring subjectivity through
value prediction vs. directly identifying subjectivity. Our experiments show
that direct subjectivity identification significantly improves the model
performance of flagging subjective arguments. Furthermore, combining
contrastive loss with binary cross-entropy loss does not improve performance
but reduces the dependency on per-label subjectivity. Our proposed methods can
help identify arguments that individuals may interpret differently, fostering a
more nuanced annotation process.

</details>


### [55] [Anchoring Refusal Direction: Mitigating Safety Risks in Tuning via Projection Constraint](https://arxiv.org/abs/2509.06795)
*Yanrui Du,Fenglei Fan,Sendong Zhao,Jiawei Cao,Qika Lin,Kai He,Ting Liu,Bing Qin,Mengling Feng*

Main category: cs.CL

TL;DR: 这篇论文提出了ProCon方法，通过投影约束损失和温身策略来减轻指令微调对大语言模型安全性的影响，同时保持任务性能。


<details>
  <summary>Details</summary>
Motivation: 指令微调(IFT)虽能增强大语言模型的能力，但会明显降低模型的安全性，特别是拒绝恶意指令的能力。研究发现拒绝方向(r-direction)在训练过程中会偏移，这是引发安全风险的原因之一。

Method: 提出ProCon方法，包含投影约束损失项，用于规范化训练样本隐藏状态在r-direction上的投影大小。进一步提出温身策略，在早期强约束并扩宽数据分布以增强约束信号。

Result: 在多种数据集、场景和大语言模型上的实验结果显示，ProCon能显著减轻IFT带来的安全风险，同时保持任务性能提升。与强基线相比仍能保持更优的整体性能。

Conclusion: ProCon方法能够有效稳定训练过程中的r-direction，为大语言模型的安全研究奠定了坚实基础。这种基于可解释性的模型内部机制探索对未来安全研究具有重要意义。

Abstract: Instruction Fine-Tuning (IFT) has been widely adopted as an effective
post-training strategy to enhance various abilities of Large Language Models
(LLMs). However, prior studies have shown that IFT can significantly compromise
LLMs' safety, particularly their ability to refuse malicious instructions,
raising significant concerns. Recent research into the internal mechanisms of
LLMs has identified the refusal direction (r-direction) in the hidden states,
which plays a pivotal role in governing refusal behavior. Building on this
insight, our study reveals that the r-direction tends to drift during training,
which we identify as one of the causes of the associated safety risks. To
mitigate such drift, our proposed ProCon method introduces a
projection-constrained loss term that regularizes the projection magnitude of
each training sample's hidden state onto the r-direction. Our initial analysis
shows that applying an appropriate constraint can effectively mitigate the
refusal direction drift and associated safety risks, but remains limited by
overall performance barriers. To overcome this barrier, informed by our
observation of early-stage sharp drift and a data-driven perspective, we
introduce a warm-up strategy that emphasizes early-stage strong constraints and
broaden the data distribution to strengthen constraint signals, leading to an
enhanced ProCon method. Experimental results under various datasets, scenarios,
and LLMs demonstrate that our method can significantly mitigate safety risks
posed by IFT while preserving task performance gains. Even compared with strong
baselines, our method consistently delivers superior overall performance.
Crucially, our analysis indicates that ProCon can contribute to stabilizing the
r-direction during training, while such an interpretability-driven exploration
of LLMs' internal mechanisms lays a solid foundation for future safety
research.

</details>


### [56] [MachineLearningLM: Continued Pretraining Language Models on Millions of Synthetic Tabular Prediction Tasks Scales In-Context ML](https://arxiv.org/abs/2509.06806)
*Haoyu Dong,Pengkun Zhang,Mingzhe Lu,Yanzhen Shen,Guolin Ke*

Main category: cs.CL

TL;DR: MachineLearningLM是一个持续预训练框架，通过合成大量结构因果模型任务来增强LLM的上下文机器学习能力，在少样本到1024样本的范围内都表现出色，同时保持通用聊天能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然具有广泛的世界知识和推理能力，但在标准机器学习任务中难以通过纯上下文学习从多个示例中有效学习，需要提升其上下文机器学习能力。

Method: 使用随机森林教师模型，将基于树的决策策略蒸馏到LLM中，通过合成数百万个结构因果模型任务进行持续预训练，采用token高效的提示序列化方法。

Result: 在金融、物理、生物和医疗等领域的分布外表格分类任务上平均优于强基线模型约15%，在1024样本范围内呈现单调增长的缩放规律，MMLU得分达到75.4%。

Conclusion: MachineLearningLM框架成功增强了LLM的上下文机器学习能力，同时保持了其通用知识和推理能力，实现了随机森林级别的准确率且无需任务特定训练。

Abstract: Large language models (LLMs) possess broad world knowledge and strong
general-purpose reasoning ability, yet they struggle to learn from many
in-context examples on standard machine learning (ML) tasks, that is, to
leverage many-shot demonstrations purely via in-context learning (ICL) without
gradient descent. We introduce MachineLearningLM, a portable
continued-pretraining framework that equips a general-purpose LLM with robust
in-context ML capability while preserving its general knowledge and reasoning
for broader chat workflows.
  Our pretraining procedure synthesizes ML tasks from millions of structural
causal models (SCMs), spanning shot counts up to 1,024. We begin with a
random-forest teacher, distilling tree-based decision strategies into the LLM
to strengthen robustness in numerical modeling. All tasks are serialized with a
token-efficient prompt, enabling 3x to 6x more examples per context window and
delivering up to 50x amortized throughput via batch inference.
  Despite a modest setup (Qwen-2.5-7B-Instruct with LoRA rank 8),
MachineLearningLM outperforms strong LLM baselines (e.g., GPT-5-mini) by an
average of about 15% on out-of-distribution tabular classification across
finance, physics, biology, and healthcare domains. It exhibits a striking
many-shot scaling law: accuracy increases monotonically as in-context
demonstrations grow from 8 to 1,024. Without any task-specific training, it
attains random-forest-level accuracy across hundreds of shots. General chat
capabilities, including knowledge and reasoning, are preserved: it achieves
75.4% on MMLU.

</details>


### [57] [MoGU V2: Toward a Higher Pareto Frontier Between Model Usability and Security](https://arxiv.org/abs/2509.06807)
*Yanrui Du,Fenglei Fan,Sendong Zhao,Jiawei Cao,Ting Liu,Bing Qin*

Main category: cs.CL

TL;DR: MoGU_v2框架通过动态路由机制平衡LLMs的安全性和可用性，在保持任务性能的同时有效提升安全性


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在安全性和可用性之间的权衡问题，避免传统方法导致的保守拒绝响应

Method: MoGU_v2框架在编码安全特征的层中嵌入路由器，通过双向适配机制动态分配安全优化和可用性优化变体的权重

Result: 在主流LLM、设备端LLM和推理LLM等多种模型上展现强适应性和稳定改进，能轻松恢复Instruction Fine-tuning带来的安全风险

Conclusion: MoGU_v2是一个强大且通用的解决方案，可有效缓解实际应用中的安全风险

Abstract: As Large Language Models (LLMs) increasingly permeate human life, their
security has emerged as a critical concern, particularly their ability to
maintain harmless responses to malicious instructions. Although extensive
methods have improved LLMs' security, they often lead to conservative,
rejection-oriented responses that compromise practical usability. This presents
a key challenge: how to advance the Pareto frontier between LLMs' usability and
security, rather than necessitate a trade-off between them. To address this, we
propose the MoGU framework, in which the intra-layer router dynamically
allocates weights by sensing hidden states, thereby balancing the contributions
of security-optimized and usability-optimized variants. Despite its initial
potential, the MoGU framework faces limitations such as parameter redundancy
and performance bottlenecks. To overcome these, we further propose an improved
MoGU_v2 framework that establishes a tighter coupling between the routers and
hidden states. In MoGU_v2, routers are embedded only in layers encoding highly
classifiable security features, and backbone modules are activated during
router optimization to enable bidirectional adaptation. MoGU_V2 exhibits strong
adaptability and stable improvements across various series of LLMs, including
mainstream LLMs serving as brains in various applications, on-device LLMs
optimized for resource-constrained scenarios, and reasoning LLMs tailored for
user interpretability. Meanwhile, even facing risks introduced by Instruction
Fine-tuning, MoGU_v2 can easily restore security without compromising the task
performance gains via a simple data-mix strategy. These comprehensive
improvements highlight MoGU_V2 as a robust and versatile solution for
mitigating security risks in real-world applications.

</details>


### [58] [Saturation-Driven Dataset Generation for LLM Mathematical Reasoning in the TPTP Ecosystem](https://arxiv.org/abs/2509.06809)
*Valentin Quesnel,Damien Sileo*

Main category: cs.CL

TL;DR: 通过E-prover对TPTP公理库进行饱和推理，构建大规模保证正确的数学推理数据集，用于评估和提升大语言模型的逻辑思维能力


<details>
  <summary>Details</summary>
Motivation: 解决高质量逻辑声数据稀缺的问题，充分利用自动定理证明领域的研究成果来为LLMs提供可扩展的训练数据

Method: 使用E-prover对TPTP公理库进行饱和推理，生成大量保证正确的定理，然后过滤出"6709趣"的定理，构建三种难度可控的挑战任务：含义验证、前提选择和证明重构

Result: 前沿模型在需要深层次结构化推理的任务上表现崩溃，显示了当前LLMs在逻辑思维方面的明显短板

Conclusion: 该框架既提供了评估LLMs逻辑能力缺口的诊断工具，也为解决这些问题提供了可扩展的符号训练数据来源

Abstract: The scarcity of high-quality, logically sound data is a critical bottleneck
for advancing the mathematical reasoning of Large Language Models (LLMs). Our
work confronts this challenge by turning decades of automated theorem proving
research into a scalable data engine. Rather than relying on error-prone LLMs
or complex proof-assistant syntax like Lean and Isabelle, our framework
leverages E-prover's saturation capabilities on the vast TPTP axiom library to
derive a massive, guaranteed-valid corpus of theorems. Our pipeline is
principled and simple: saturate axioms, filter for "interesting" theorems, and
generate tasks. With no LLMs in the loop, we eliminate factual errors by
construction. This purely symbolic data is then transformed into three
difficulty-controlled challenges: entailment verification, premise selection,
and proof reconstruction. Our zero-shot experiments on frontier models reveal a
clear weakness: performance collapses on tasks requiring deep, structural
reasoning. Our framework provides both the diagnostic tool to measure this gap
and a scalable source of symbolic training data to address it. We make the code
and data publicly available.
  https://github.com/sileod/reasoning_core
https://hf.co/datasets/reasoning-core/rc1

</details>


### [59] [A Comparative Benchmark of Large Language Models for Labelling Wind Turbine Maintenance Logs](https://arxiv.org/abs/2509.06813)
*Max Malyi,Jonathan Shek,Alasdair McDonald,Andre Biscaya*

Main category: cs.CL

TL;DR: 这篇论文提出了一种用于基准测试大语言模型在风力气轮维护日志分类任务上表现的开源框架，评估了多个商业和开源LLM的性能层次，并建议采用人在循环系统来提高数据标注效率和质量。


<details>
  <summary>Details</summary>
Motivation: 风力气轮维护日志的非结构化文本特性继续阻碍自动化分析，影响风电成本效益。需要一种可重现的方法来评估LLM在这些复杂工业记录分类任务上的表现。

Method: 开发了一个公开源码框架，系统性评估了多种商业和开源LLM模型，分析它们在可靠性、运营效率、模型检验等方面的特性。

Result: 结果量化了清晰的性能层次，识别出了与基准标准高度一致的顶级模型，具有可靠的信心度分数。分类性能高度依赖于任务的语义模糊性，所有模型在对象性组件识别任务上都显示更高的一致性。

Conclusion: 由于没有模型能达到完美准确性且检验结果差异显著，最有效和负责任的应用方式是人在循环系统，让LLM作为强大助手加速和标准化数据标注过程，从而提升O&M数据质量和下游可靠性分析。

Abstract: Effective Operation and Maintenance (O&M) is critical to reducing the
Levelised Cost of Energy (LCOE) from wind power, yet the unstructured,
free-text nature of turbine maintenance logs presents a significant barrier to
automated analysis. Our paper addresses this by presenting a novel and
reproducible framework for benchmarking Large Language Models (LLMs) on the
task of classifying these complex industrial records. To promote transparency
and encourage further research, this framework has been made publicly available
as an open-source tool. We systematically evaluate a diverse suite of
state-of-the-art proprietary and open-source LLMs, providing a foundational
assessment of their trade-offs in reliability, operational efficiency, and
model calibration. Our results quantify a clear performance hierarchy,
identifying top models that exhibit high alignment with a benchmark standard
and trustworthy, well-calibrated confidence scores. We also demonstrate that
classification performance is highly dependent on the task's semantic
ambiguity, with all models showing higher consensus on objective component
identification than on interpretive maintenance actions. Given that no model
achieves perfect accuracy and that calibration varies dramatically, we conclude
that the most effective and responsible near-term application is a
Human-in-the-Loop system, where LLMs act as a powerful assistant to accelerate
and standardise data labelling for human experts, thereby enhancing O&M data
quality and downstream reliability analysis.

</details>


### [60] [COMPACT: Common-token Optimized Model Pruning Across Channels and Tokens](https://arxiv.org/abs/2509.06836)
*Eugene Kwek,Wenpeng Yin*

Main category: cs.CL

TL;DR: COMPACT是一种联合剪枝方法，同时剪枝稀有词汇和FFN中间通道，保持标准Transformer架构的同时实现高效压缩。


<details>
  <summary>Details</summary>
Motivation: 使LLM在内存、延迟和服务成本方面更高效，适用于边缘部署、交互应用和大规模可持续推理。现有剪枝方法存在局限性：宽度剪枝破坏标准架构或需要定制推理代码，深度剪枝移除整层导致精度骤降。

Method: 联合剪枝：(i)剪枝稀有词汇缩小嵌入/解嵌入层，(ii)基于常见词加权的激活值剪枝FFN中间通道，使重要性评估与剪枝后的词分布对齐。

Result: 在Qwen、LLaMA和Gemma系列(0.5B-70B)上实验显示，在相似或更高剪枝比例下达到最先进的下游任务性能，显著减少参数、GPU内存和端到端延迟。

Conclusion: COMPACT结合了深度和宽度剪枝的优点：部署友好(保持标准架构)、规模适应性(词汇与FFN剪枝权衡)、无需训练、竞争性剪枝时间、强内存节省和吞吐量提升。

Abstract: Making LLMs more efficient in memory, latency, and serving cost is crucial
for edge deployment, interactive applications, and sustainable inference at
scale. Pruning is a key technique toward this goal. However, prior pruning
methods are limited: width pruning often breaks the standard transformer layout
or requires custom inference code, while depth pruning removes entire layers
and can cause abrupt accuracy drops. In this work, we propose COMPACT, which
jointly (i) prunes rare vocabulary to shrink embedding/unembedding and (ii)
prunes FFN intermediate channels using common-token-weighted activations,
aligning importance with the post-pruning token distribution. COMPACT enjoys
merits of both depth and width pruning, such as: deployment-friendliness (keeps
a standard transformer architecture), scale-adaptivity (trade off vocab vs. FFN
pruning), training-free operation with competitive pruning time, and strong
memory savings alongside throughput gains. Experiments across Qwen, LLaMA, and
Gemma families (0.5B-70B) show state-of-the-art downstream task performance at
similar or higher pruning ratios, with substantial reductions in parameters,
GPU memory, and end-to-end latency.

</details>


### [61] [EPT Benchmark: Evaluation of Persian Trustworthiness in Large Language Models](https://arxiv.org/abs/2509.06838)
*Mohammad Reza Mirbagheri,Mohammad Mahdi Mirkamali,Zahra Motoshaker Arani,Ali Javeri,Amir Mahdi Sadeghzadeh,Rasool Jalili*

Main category: cs.CL

TL;DR: 研究提出了EPT指标，一个专门评估大语言模型在波斯文化背景下可信过性的文化基础性指标，测试了多个领先模型并发现安全性方面存在显著缺陷。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在各种语言任务上表现突出，但确保其可信过性仍是重要挑战，特别是在遵守賞理、文化和社会价值观方面。需要基于文化背景的评估标准来发展负责任的AI系统。

Method: 研究开发了EPT（波斯语可信过性评估）指标，涵盖六个关键方面：真实性、安全性、公平性、稳健性、隐私性和賞理对齐。编诒了标签数据集，并使用自动化的LLM基于评估和人工评估方法测试了ChatGPT、Claude、DeepSeek、Gemini、Grok、LLaMA、Mistral和Qwen等多个领先模型。

Result: 结果显示模型在安全性方面存在显著缺陷，说明对模型行为这一关键方面需要重点关注。研究还揭示了这些模型与波斯賞理-文化价值观的对齐情况，指出了关键缺口和推进可信过、文化负责任AI的机遇。

Conclusion: 研究强调了基于文化背景的评估标准对于发展负责任AI系统的重要性。EPT指标提供了一个专门用于评估波斯文化背景下模型可信过性的工具，并发现安全性是需要立即关注的关键问题。数据集已公开提供，以便进一步研究和改进。

Abstract: Large Language Models (LLMs), trained on extensive datasets using advanced
deep learning architectures, have demonstrated remarkable performance across a
wide range of language tasks, becoming a cornerstone of modern AI technologies.
However, ensuring their trustworthiness remains a critical challenge, as
reliability is essential not only for accurate performance but also for
upholding ethical, cultural, and social values. Careful alignment of training
data and culturally grounded evaluation criteria are vital for developing
responsible AI systems. In this study, we introduce the EPT (Evaluation of
Persian Trustworthiness) metric, a culturally informed benchmark specifically
designed to assess the trustworthiness of LLMs across six key aspects:
truthfulness, safety, fairness, robustness, privacy, and ethical alignment. We
curated a labeled dataset and evaluated the performance of several leading
models - including ChatGPT, Claude, DeepSeek, Gemini, Grok, LLaMA, Mistral, and
Qwen - using both automated LLM-based and human assessments. Our results reveal
significant deficiencies in the safety dimension, underscoring the urgent need
for focused attention on this critical aspect of model behavior. Furthermore,
our findings offer valuable insights into the alignment of these models with
Persian ethical-cultural values and highlight critical gaps and opportunities
for advancing trustworthy and culturally responsible AI. The dataset is
publicly available at: https://github.com/Rezamirbagheri110/EPT-Benchmark.

</details>


### [62] [The Majority is not always right: RL training for solution aggregation](https://arxiv.org/abs/2509.06870)
*Wenting Zhao,Pranjal Aggarwal,Swarnadeep Saha,Asli Celikyilmaz,Jason Weston,Ilia Kulikov*

Main category: cs.CL

TL;DR: 本文提出AggLM方法，通过强化学习训练聚合器模型来整合多个候选解决方案，相比简单投票或奖励模型排名能更有效地提升大语言模型的推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖简单多数投票或奖励模型排名来聚合多个解决方案，但这些方法可能只能带来有限的性能提升，需要更智能的聚合策略。

Method: 使用强化学习训练聚合器模型，让模型学习审查、协调和合成最终正确答案的能力，通过平衡难易训练样本来提升模型性能。

Result: AggLM方法在多个基准测试中优于基于规则和奖励模型的基线方法，能够有效泛化到不同模型的解决方案，且比多数投票方法需要更少的token。

Conclusion: 将聚合作为显式推理技能进行学习是有效的，该方法能够处理少数但正确的答案，同时保持对多数正确答案的识别能力，具有很好的泛化性能。

Abstract: Scaling up test-time compute, by generating multiple independent solutions
and selecting or aggregating among them, has become a central paradigm for
improving large language models (LLMs) on challenging reasoning tasks. While
most prior work relies on simple majority voting or reward model ranking to
aggregate solutions, these approaches may only yield limited benefits. In this
work, we propose to learn aggregation as an explicit reasoning skill: given a
set of candidate solutions, we train an aggregator model to review, reconcile,
and synthesize a final, correct answer using reinforcement learning from
verifiable rewards. A key ingredient is careful balancing of easy and hard
training examples, allowing the model to learn both to recover
minority-but-correct answers as well as easy majority-correct answers.
Empirically, we find our method, AggLM, outperforms both strong rule-based and
reward-model baselines, across multiple benchmarks. Furthermore, it generalizes
effectively to solutions from differing models, including stronger ones than
contained in the training data, all while requiring substantially fewer tokens
than majority voting with larger numbers of solutions.

</details>


### [63] [UNH at CheckThat! 2025: Fine-tuning Vs Prompting in Claim Extraction](https://arxiv.org/abs/2509.06883)
*Joe Wilder,Nikhil Kadapala,Benji Xu,Mohammed Alsaadi,Aiden Parsons,Mitchell Rogers,Palash Agarwal,Adam Hassick,Laura Dietz*

Main category: cs.CL

TL;DR: 本文探索了多种提示学习和上下文学习方法，包括少样本提示和不同LLM家族的微调，用于从社交媒体文本中提取值得核查的声明。最佳METEOR分数通过微调FLAN-T5模型获得，但其他方法有时能提取更高质量的声明。


<details>
  <summary>Details</summary>
Motivation: 参与CheckThat! Task 2英文任务，旨在开发有效的方法从社交媒体内容中识别和提取值得进行事实核查的声明。

Method: 使用了多种提示学习和上下文学习方法，包括少样本提示（few-shot prompting）和对不同大型语言模型（LLM）家族进行微调，特别关注FLAN-T5模型的微调。

Result: 微调FLAN-T5模型获得了最佳的METEOR分数，但研究发现其他方法虽然METEOR分数较低，有时却能提取出更高质量的声明。

Conclusion: 虽然自动评估指标（如METEOR）提供了有用的参考，但有时与声明质量不完全一致，需要结合其他评估方法来全面衡量提取声明的质量。

Abstract: We participate in CheckThat! Task 2 English and explore various methods of
prompting and in-context learning, including few-shot prompting and fine-tuning
with different LLM families, with the goal of extracting check-worthy claims
from social media passages. Our best METEOR score is achieved by fine-tuning a
FLAN-T5 model. However, we observe that higher-quality claims can sometimes be
extracted using other methods, even when their METEOR scores are lower.

</details>


### [64] [mmBERT: A Modern Multilingual Encoder with Annealed Language Learning](https://arxiv.org/abs/2509.06888)
*Marc Marone,Orion Weller,William Fleshman,Eugene Yang,Dawn Lawrie,Benjamin Van Durme*

Main category: cs.CL

TL;DR: 提出了mmBERT，一个在1800多种语言的3T多语言文本上预训练的仅编码器语言模型，通过逆掩码比率调度和逆温度采样等新技术，在分类和检索任务上显著优于前代模型


<details>
  <summary>Details</summary>
Motivation: 当前缺乏针对仅编码器多语言模型的最新研究，特别是在多语言环境下，需要开发能够处理高资源和低资源语言的强大模型

Method: 使用3T多语言文本训练mmBERT模型，引入逆掩码比率调度和逆温度采样技术，在衰减阶段添加1700多种低资源语言数据

Result: mmBERT在分类和检索任务上显著优于前代模型，在高资源和低资源语言上都表现出色，性能可与OpenAI o3和Google Gemini 2.5 Pro相媲美

Conclusion: 通过创新的训练策略和数据调度方法，mmBERT证明了仅编码器模型在多语言环境下的强大潜力，特别是在低资源语言处理方面取得了突破性进展

Abstract: Encoder-only languages models are frequently used for a variety of standard
machine learning tasks, including classification and retrieval. However, there
has been a lack of recent research for encoder models, especially with respect
to multilingual models. We introduce mmBERT, an encoder-only language model
pretrained on 3T tokens of multilingual text in over 1800 languages. To build
mmBERT we introduce several novel elements, including an inverse mask ratio
schedule and an inverse temperature sampling ratio. We add over 1700
low-resource languages to the data mix only during the decay phase, showing
that it boosts performance dramatically and maximizes the gains from the
relatively small amount of training data. Despite only including these
low-resource languages in the short decay phase we achieve similar
classification performance to models like OpenAI's o3 and Google's Gemini 2.5
Pro. Overall, we show that mmBERT significantly outperforms the previous
generation of models on classification and retrieval tasks -- on both high and
low-resource languages.

</details>


### [65] [Proof-Carrying Numbers (PCN): A Protocol for Trustworthy Numeric Answers from LLMs via Claim Verification](https://arxiv.org/abs/2509.06902)
*Aivin V. Solatorio*

Main category: cs.CL

TL;DR: Proof-Carrying Numbers (PCN) 是一种展示层协议，通过机械验证来强制保证大语言模型生成数字的准确性，将验证放在渲染器而非模型中，确保只有经过验证的数字才会被标记为已验证。


<details>
  <summary>Details</summary>
Motivation: 大语言模型作为随机系统可能生成与可用数据偏离的数字，即数字幻觉。现有的安全措施（检索增强生成、引用和不确定性估计）虽然提高了透明度，但无法保证保真度：伪造或误引的值仍可能被显示为正确。

Method: PCN 将数字跨度作为与结构化声明绑定的声明绑定令牌发出，验证器根据声明的策略（如精确相等、舍入、别名或带限定符的容差）检查每个令牌。验证放在渲染器中而非模型中，只有经过声明检查的数字才会被标记为已验证。

Result: PCN 被形式化并证明了在诚实令牌下的可靠性、完整性、故障关闭行为以及策略细化下的单调性。PCN 轻量级且与模型无关，可无缝集成到现有应用中，并可扩展加密承诺。

Conclusion: 通过在显示前强制进行验证，PCN 为数字敏感环境建立了一个简单契约：信任只能通过证明获得，而缺少标记则传达不确定性。

Abstract: Large Language Models (LLMs) as stochastic systems may generate numbers that
deviate from available data, a failure known as \emph{numeric hallucination}.
Existing safeguards -- retrieval-augmented generation, citations, and
uncertainty estimation -- improve transparency but cannot guarantee fidelity:
fabricated or misquoted values may still be displayed as if correct. We propose
\textbf{Proof-Carrying Numbers (PCN)}, a presentation-layer protocol that
enforces numeric fidelity through mechanical verification. Under PCN, numeric
spans are emitted as \emph{claim-bound tokens} tied to structured claims, and a
verifier checks each token under a declared policy (e.g., exact equality,
rounding, aliases, or tolerance with qualifiers). Crucially, PCN places
verification in the \emph{renderer}, not the model: only claim-checked numbers
are marked as verified, and all others default to unverified. This separation
prevents spoofing and guarantees fail-closed behavior. We formalize PCN and
prove soundness, completeness under honest tokens, fail-closed behavior, and
monotonicity under policy refinement. PCN is lightweight and model-agnostic,
integrates seamlessly into existing applications, and can be extended with
cryptographic commitments. By enforcing verification as a mandatory step before
display, PCN establishes a simple contract for numerically sensitive settings:
\emph{trust is earned only by proof}, while the absence of a mark communicates
uncertainty.

</details>


### [66] [Beyond Two-Stage Training: Cooperative SFT and RL for LLM Reasoning](https://arxiv.org/abs/2509.06948)
*Liang Chen,Xueting Han,Li Shen,Jing Bai,Kam-Fai Wong*

Main category: cs.CL

TL;DR: 提出了一种基于双层优化的新方法，通过让SFT目标以最优RL策略为条件，实现监督微调与强化学习的协同训练，提升推理模型的效率和效果


<details>
  <summary>Details</summary>
Motivation: 传统两阶段方法（先SFT后RL）限制了两种训练范式的交互，影响了整体效果。RL虽然能激励大语言模型的推理能力，但试错性质导致效率低下

Method: 采用双层优化框架：下层执行RL更新并同时接收SFT监督，上层显式最大化协同增益（联合训练相比单独RL的性能优势），让SFT元学习如何指导RL优化过程

Result: 在五个推理基准测试上的实证评估表明，该方法始终优于基线方法，并在效果和效率之间实现了更好的平衡

Conclusion: 通过双层优化实现SFT和RL的协同训练是一种有效的方法，能够显著提升推理模型的学习效率和性能表现

Abstract: Reinforcement learning (RL) has proven effective in incentivizing the
reasoning abilities of large language models (LLMs), but suffers from severe
efficiency challenges due to its trial-and-error nature. While the common
practice employs supervised fine-tuning (SFT) as a warm-up stage for RL, this
decoupled two-stage approach limits interaction between SFT and RL, thereby
constraining overall effectiveness. This study introduces a novel method for
learning reasoning models that employs bilevel optimization to facilitate
better cooperation between these training paradigms. By conditioning the SFT
objective on the optimal RL policy, our approach enables SFT to meta-learn how
to guide RL's optimization process. During training, the lower level performs
RL updates while simultaneously receiving SFT supervision, and the upper level
explicitly maximizes the cooperative gain-the performance advantage of joint
SFT-RL training over RL alone. Empirical evaluations on five reasoning
benchmarks demonstrate that our method consistently outperforms baselines and
achieves a better balance between effectiveness and efficiency.

</details>


### [67] [Revolutionizing Reinforcement Learning Framework for Diffusion Large Language Models](https://arxiv.org/abs/2509.06949)
*Yinjie Wang,Ling Yang,Bowen Li,Ye Tian,Ke Shen,Mengdi Wang*

Main category: cs.CL

TL;DR: TraceRL是一个轨迹感知的强化学习框架，用于扩散语言模型的后训练，通过扩散价值模型提升训练稳定性，在数学推理和编程任务上表现优异，并推出了TraDo系列模型。


<details>
  <summary>Details</summary>
Motivation: 为了解决扩散语言模型在复杂推理任务中的性能问题，并提高采样灵活性，需要一种能够整合偏好推理轨迹的后训练框架。

Method: 提出TraceRL框架，使用扩散基的价值模型增强训练稳定性，支持不同架构的扩散语言模型，并采用课程学习策略。

Result: TraDo-4B-Instruct在数学推理任务上持续超越7B规模的AR模型；TraDo-8B-Instruct在数学推理基准上相对Qwen2.5-7B-Instruct提升6.1%，相对Llama3.1-8B-Instruct提升51.3%；首个长链思维DLM在MATH500上相对Qwen2.5-7B-Instruct提升18.1%。

Conclusion: TraceRL框架有效提升了扩散语言模型的推理性能，推出的TraDo系列模型在多个基准测试中达到state-of-the-art水平，并开源了完整的训练和部署框架。

Abstract: We propose TraceRL, a trajectory-aware reinforcement learning framework for
diffusion language models (DLMs) that incorporates preferred inference
trajectory into post-training, and is applicable across different
architectures. Equipped with a diffusion-based value model that enhances
training stability, we demonstrate improved reasoning performance on complex
math and coding tasks. Besides, it can also be applied to adapt block-specific
models to larger blocks, which improves sampling flexibility. Employing
TraceRL, we derive a series of state-of-the-art diffusion language models,
namely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still
consistently outperforms them across complex math reasoning tasks.
TraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over
Qwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical
reasoning benchmarks. Through curriculum learning, we also derive the first
long-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1%
relative accuracy gain. To facilitate reproducible research and practical
applications, we release a comprehensive open-source framework for building,
training, and deploying diffusion LLMs across diverse architectures. The
framework integrates accelerated KV-cache techniques and inference engines for
both inference and reinforcement learning, and includes implementations of
various supervised fine-tuning and RL methods for mathematics, coding, and
general tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL

</details>


### [68] [On the Same Wavelength? Evaluating Pragmatic Reasoning in Language Models across Broad Concepts](https://arxiv.org/abs/2509.06952)
*Linlu Qiu,Cedegao E. Zhang,Joshua B. Tenenbaum,Yoon Kim,Roger P. Levy*

Main category: cs.CL

TL;DR: 语言模型的语用理解能力评估：通过Wavelength游戏框架测试语言理解和生成能力，发现大型模型表现接近人类，RSA方法能显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 评估语言模型的语用理解能力，理解其在交际中的理解和生成能力，以支持更好的对话系统开发。

Method: 使用Wavelength游戏框架，测试语言模型的语言理解和生成能力，比较直接提示、思维链提示和理性语言行为(RSA)方法的效果。

Result: 大型语言模型在语言理解上表现接近人类，与人类判断高相关；在语言生成中，思维链提示和RSA方法能显著提升性能。

Conclusion: 研究确认了语言模型的语用理解优势和限制，RSA方法为提升理解能力提供了新途径，有助于深入理解概念表征和社会理解。

Abstract: Language use is shaped by pragmatics -- i.e., reasoning about communicative
goals and norms in context. As language models (LMs) are increasingly used as
conversational agents, it becomes ever more important to understand their
pragmatic reasoning abilities. We propose an evaluation framework derived from
Wavelength, a popular communication game where a speaker and a listener
communicate about a broad range of concepts in a granular manner. We study a
range of LMs on both language comprehension and language production using
direct and Chain-of-Thought (CoT) prompting, and further explore a Rational
Speech Act (RSA) approach to incorporating Bayesian pragmatic reasoning into LM
inference. We find that state-of-the-art LMs, but not smaller ones, achieve
strong performance on language comprehension, obtaining similar-to-human
accuracy and exhibiting high correlations with human judgments even without CoT
prompting or RSA. On language production, CoT can outperform direct prompting,
and using RSA provides significant improvements over both approaches. Our study
helps identify the strengths and limitations in LMs' pragmatic reasoning
abilities and demonstrates the potential for improving them with RSA, opening
up future avenues for understanding conceptual representation, language
understanding, and social reasoning in LMs and humans.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [69] [ForensicsData: A Digital Forensics Dataset for Large Language Models](https://arxiv.org/abs/2509.05331)
*Youssef Chakir,Iyad Lahsen-Cherif*

Main category: cs.CR

TL;DR: ForensicsData是一个包含5000多个Q-C-A三元组的数据集，源自真实恶意软件分析报告，旨在解决数字取证领域数据稀缺问题，并通过专门评估流程确保质量。


<details>
  <summary>Details</summary>
Motivation: 网络事件日益复杂给数字取证调查带来挑战，特别是证据收集和分析方面。由于伦理、法律和隐私问题，公开的现实数据集有限，但这类数据对研究和工具开发至关重要。

Method: 采用独特的工作流程：从恶意软件分析报告中提取结构化数据，使用大语言模型将其转换为Q-C-A格式，然后通过专门的评估流程验证数据质量。

Result: 创建了包含5000多个Q-C-A三元组的ForensicsData数据集，在评估的模型中，Gemini 2 Flash在生成内容与取证术语对齐方面表现最佳。

Conclusion: ForensicsData旨在通过支持可重复实验和促进研究社区合作来推动数字取证领域的发展。

Abstract: The growing complexity of cyber incidents presents significant challenges for
digital forensic investigators, especially in evidence collection and analysis.
Public resources are still limited because of ethical, legal, and privacy
concerns, even though realistic datasets are necessary to support research and
tool developments. To address this gap, we introduce ForensicsData, an
extensive Question-Context-Answer (Q-C-A) dataset sourced from actual malware
analysis reports. It consists of more than 5,000 Q-C-A triplets. A unique
workflow was used to create the dataset, which extracts structured data, uses
large language models (LLMs) to transform it into Q-C-A format, and then uses a
specialized evaluation process to confirm its quality. Among the models
evaluated, Gemini 2 Flash demonstrated the best performance in aligning
generated content with forensic terminology. ForensicsData aims to advance
digital forensics by enabling reproducible experiments and fostering
collaboration within the research community.

</details>


### [70] [Cross-Service Threat Intelligence in LLM Services using Privacy-Preserving Fingerprints](https://arxiv.org/abs/2509.05608)
*Waris Gill,Natalie Isak,Matthew Dressman*

Main category: cs.CR

TL;DR: BinaryShield是一个隐私保护的威胁情报系统，通过PII脱敏、语义嵌入、二进制量化和随机响应机制生成不可逆指纹，实现跨合规边界的安全攻击指纹共享，在保持隐私的同时有效检测提示注入攻击。


<details>
  <summary>Details</summary>
Motivation: 企业部署多个LLM服务处理数十亿查询，但由于监管合规边界限制，这些服务无法共享提示注入攻击的威胁情报，导致攻击在一个服务中被检测后，在其他服务中可能持续数月未被发现。

Method: 通过结合PII脱敏、语义嵌入、二进制量化和随机响应机制的独特流水线，将可疑提示转换为潜在不可逆的指纹，既保留攻击模式又提供隐私保护。

Result: 评估显示BinaryShield达到0.94的F1分数，显著优于隐私保护基线SimHash(0.77)，同时实现64倍存储减少和38倍更快的相似性搜索。

Conclusion: BinaryShield有效解决了LLM服务间威胁情报共享的隐私合规问题，在保持高检测精度的同时大幅提升了存储和搜索效率。

Abstract: The widespread deployment of LLMs across enterprise services has created a
critical security blind spot. Organizations operate multiple LLM services
handling billions of queries daily, yet regulatory compliance boundaries
prevent these services from sharing threat intelligence about prompt injection
attacks, the top security risk for LLMs. When an attack is detected in one
service, the same threat may persist undetected in others for months, as
privacy regulations prohibit sharing user prompts across compliance boundaries.
  We present BinaryShield, the first privacy-preserving threat intelligence
system that enables secure sharing of attack fingerprints across compliance
boundaries. BinaryShield transforms suspicious prompts through a unique
pipeline combining PII redaction, semantic embedding, binary quantization, and
randomized response mechanism to potentially generate non-invertible
fingerprints that preserve attack patterns while providing privacy. Our
evaluations demonstrate that BinaryShield achieves an F1-score of 0.94,
significantly outperforming SimHash (0.77), the privacy-preserving baseline,
while achieving 64x storage reduction and 38x faster similarity search compared
to dense embeddings.

</details>


### [71] [An Ethically Grounded LLM-Based Approach to Insider Threat Synthesis and Detection](https://arxiv.org/abs/2509.06920)
*Haywood Gelman,John D. Hastings,David Kenley*

Main category: cs.CR

TL;DR: 本研究使用LLM Claude Sonnet 3.7动态合成包含1%内部威胁指标的系统日志，并在检测性能上优于GPT-4o，展示了LLM在合成数据集生成和内部威胁检测中的潜力。


<details>
  <summary>Details</summary>
Motivation: 内部威胁检测研究受限于静态和有限访问的数据集，阻碍了自适应检测模型的发展，需要新的方法来生成动态合成数据。

Method: 采用基于伦理的大型语言模型Claude Sonnet 3.7动态合成系统日志消息，其中1%包含内部威胁指标，并使用统计指标（精确率、召回率、MCC、ROC AUC）评估Claude Sonnet 3.7和GPT-4o的检测性能。

Result: Claude Sonnet 3.7在几乎所有指标上都 consistently 优于GPT-4o，特别是在减少误报和提高检测准确性方面表现突出。

Conclusion: 研究结果表明LLM在合成数据集生成和内部威胁检测方面具有强大潜力，为开发更有效的自适应检测模型提供了新途径。

Abstract: Insider threats are a growing organizational problem due to the complexity of
identifying their technical and behavioral elements. A large research body is
dedicated to the study of insider threats from technological, psychological,
and educational perspectives. However, research in this domain has been
generally dependent on datasets that are static and limited access which
restricts the development of adaptive detection models. This study introduces a
novel, ethically grounded approach that uses the large language model (LLM)
Claude Sonnet 3.7 to dynamically synthesize syslog messages, some of which
contain indicators of insider threat scenarios. The messages reflect real-world
data distributions by being highly imbalanced (1% insider threats). The syslogs
were analyzed for insider threats by both Claude Sonnet 3.7 and GPT-4o, with
their performance evaluated through statistical metrics including precision,
recall, MCC, and ROC AUC. Sonnet 3.7 consistently outperformed GPT-4o across
nearly all metrics, particularly in reducing false alarms and improving
detection accuracy. The results show strong promise for the use of LLMs in
synthetic dataset generation and insider threat detection.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [72] [On the Contribution of Lexical Features to Speech Emotion Recognition](https://arxiv.org/abs/2509.05634)
*David Combei*

Main category: eess.AS

TL;DR: 语音情感识别中词汇内容比音响特征更有效，在MELD数据集上达到51.5% F1分数，超过参数更多的音响模型


<details>
  <summary>Details</summary>
Motivation: 探索词汇内容在语音情感识别中的作用，对比传统以音响特征为主的方法

Method: 使用自监督学习的语音和文本表征，分析变换器编码器的层级表现，评估音频去噪效果

Result: 词汇方法在MELD数据集上获得51.5%的加权F1分，超过参数更多的音响模型(49.3%)

Conclusion: 词汇内容在语音情感识别中具有重要价值，甚至可以达到比音响特征更好的效果

Abstract: Although paralinguistic cues are often considered the primary drivers of
speech emotion recognition (SER), we investigate the role of lexical content
extracted from speech and show that it can achieve competitive and in some
cases higher performance compared to acoustic models. On the MELD dataset, our
lexical-based approach obtains a weighted F1-score (WF1) of 51.5%, compared to
49.3% for an acoustic-only pipeline with a larger parameter count. Furthermore,
we analyze different self-supervised (SSL) speech and text representations,
conduct a layer-wise study of transformer-based encoders, and evaluate the
effect of audio denoising.

</details>


### [73] [Beamforming-LLM: What, Where and When Did I Miss?](https://arxiv.org/abs/2509.06221)
*Vishal Choudhari*

Main category: eess.AS

TL;DR: Beamforming-LLM是一个结合空间音频捕获和检索增强生成技术的系统，能够通过自然语言查询帮助用户回忆错过的多说话人对话内容。


<details>
  <summary>Details</summary>
Motivation: 在多说话人环境中，用户可能会错过部分对话内容，需要一个智能系统来帮助语义回忆这些错过的对话片段。

Method: 使用麦克风阵列进行空间音频捕获，通过波束成形分离方向性音频流，用Whisper进行转录，使用句子编码器嵌入向量数据库，通过RAG技术检索相关片段并用GPT-4o-mini进行总结。

Result: 开发出用户友好的界面，提供对比性总结、空间上下文和时间戳音频回放功能。

Conclusion: 这项工作为智能听觉记忆系统奠定了基础，在辅助技术、会议总结和情境感知个人空间计算方面具有广泛应用前景。

Abstract: We present Beamforming-LLM, a system that enables users to semantically
recall conversations they may have missed in multi-speaker environments. The
system combines spatial audio capture using a microphone array with
retrieval-augmented generation (RAG) to support natural language queries such
as, "What did I miss when I was following the conversation on dogs?"
Directional audio streams are separated using beamforming, transcribed with
Whisper, and embedded into a vector database using sentence encoders. Upon
receiving a user query, semantically relevant segments are retrieved,
temporally aligned with non-attended segments, and summarized using a
lightweight large language model (GPT-4o-mini). The result is a user-friendly
interface that provides contrastive summaries, spatial context, and timestamped
audio playback. This work lays the foundation for intelligent auditory memory
systems and has broad applications in assistive technology, meeting
summarization, and context-aware personal spatial computing.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [74] [Imagining Alternatives: Towards High-Resolution 3D Counterfactual Medical Image Generation via Language Guidance](https://arxiv.org/abs/2509.05978)
*Mohamed Mohamed,Brennan Nichyporuk,Douglas L. Arnold,Tal Arbel*

Main category: eess.IV

TL;DR: 这篇论文提出了一种能够根据自然语言描述生成高分辨率3D反事实医学图像的框架，专门应用于神经系统医学影像数据，包括多发硬化和阿尔茫海默病的病理模拟。


<details>
  <summary>Details</summary>
Motivation: 虽然视觉-语言模型在2D图像生成方面表现突出，但在3D医学图像领域缺乏领先领域的预训练基础模型，限制了进步。需要开发能够根据自然语言生成高分辨3D反事实医学图像的技术，以支持个性化解释、疾病进展模拟和医学培训等应用。

Method: 采用基于Simple Diffusion增强的最新3D模型调整框架，统一使用增强条件条件以改善文本对齐和图像质量。这是首个应用于神经系统影像数据的语言导向的原生3D模型。

Result: 在两个不同的神经系统MRI数据集上验证，成功模拟了多发硬化的不同痕迹负荷以及阿尔茫海默病的认知状态，生成了高质量图像同时保持了受试者的信度。

Conclusion: 该框架为3D医学影像领域的提示驱动疾病进展分析奠定了基础，开启了语言指导的高分辨3D医学图像生成的新方向。

Abstract: Vision-language models have demonstrated impressive capabilities in
generating 2D images under various conditions; however the impressive
performance of these models in 2D is largely enabled by extensive, readily
available pretrained foundation models. Critically, comparable pretrained
foundation models do not exist for 3D, significantly limiting progress in this
domain. As a result, the potential of vision-language models to produce
high-resolution 3D counterfactual medical images conditioned solely on natural
language descriptions remains completely unexplored. Addressing this gap would
enable powerful clinical and research applications, such as personalized
counterfactual explanations, simulation of disease progression scenarios, and
enhanced medical training by visualizing hypothetical medical conditions in
realistic detail. Our work takes a meaningful step toward addressing this
challenge by introducing a framework capable of generating high-resolution 3D
counterfactual medical images of synthesized patients guided by free-form
language prompts. We adapt state-of-the-art 3D diffusion models with
enhancements from Simple Diffusion and incorporate augmented conditioning to
improve text alignment and image quality. To our knowledge, this represents the
first demonstration of a language-guided native-3D diffusion model applied
specifically to neurological imaging data, where faithful three-dimensional
modeling is essential to represent the brain's three-dimensional structure.
Through results on two distinct neurological MRI datasets, our framework
successfully simulates varying counterfactual lesion loads in Multiple
Sclerosis (MS), and cognitive states in Alzheimer's disease, generating
high-quality images while preserving subject fidelity in synthetically
generated medical images. Our results lay the groundwork for prompt-driven
disease progression analysis within 3D medical imaging.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [75] [TSPC: A Two-Stage Phoneme-Centric Architecture for code-switching Vietnamese-English Speech Recognition](https://arxiv.org/abs/2509.05983)
*Minh N. H. Nguyen,Anh Nguyen Tran,Dung Truong Dinh,Nam Van Vo*

Main category: cs.SD

TL;DR: 提出了一种针对越南语-英语代码转换语音识别的新型双阶段音素中心模型(TSPC)，通过扩展越南语音素集作为中间表示，显著降低了词错误率至20.8%。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以捕捉代码转换场景中的细微音位变化，特别是越南语-英语这种具有不同音系特征和相似声音识别歧义的语言对。

Method: 采用双阶段音素中心架构，基于扩展的越南语音素集作为中间表示，支持混合语言建模和音素适应。

Result: TSPC在越南语-英语代码转换ASR中持续优于现有基线(包括PhoWhisper-base)，词错误率降低至20.8%，且训练资源需求更少。

Conclusion: 基于音素的双阶段架构能够通过音素适应和语言转换，有效提升复杂代码转换场景下的ASR性能。

Abstract: Code-switching (CS) presents a significant challenge for general Auto-Speech
Recognition (ASR) systems. Existing methods often fail to capture the subtle
phonological shifts inherent in CS scenarios. The challenge is particularly
difficult for language pairs like Vietnamese and English, where both distinct
phonological features and the ambiguity arising from similar sound recognition
are present. In this paper, we propose a novel architecture for
Vietnamese-English CS ASR, a Two-Stage Phoneme-Centric model (TSPC). The TSPC
employs a phoneme-centric approach, built upon an extended Vietnamese phoneme
set as an intermediate representation to facilitate mixed-lingual modeling.
Experimental results demonstrate that TSPC consistently outperforms existing
baselines, including PhoWhisper-base, in Vietnamese-English CS ASR, achieving a
significantly lower word error rate of 20.8\% with reduced training resources.
Furthermore, the phonetic-based two-stage architecture enables phoneme
adaptation and language conversion to enhance ASR performance in complex CS
Vietnamese-English ASR scenarios.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [76] [Language Bias in Information Retrieval: The Nature of the Beast and Mitigation Methods](https://arxiv.org/abs/2509.06195)
*Jinrui Yang,Fan Jiang,Timothy Baldwin*

Main category: cs.IR

TL;DR: 本文提出LaKDA损失函数来解决多语言信息检索中的语言公平性问题，通过实验证明现有MLIR技术存在内在语言偏见，且LaKDA能有效提升公平性


<details>
  <summary>Details</summary>
Motivation: 确保多语言信息检索系统的语言公平性，使不同语言但语义相同的查询在相同多语言文档检索时能获得等效的排序结果

Method: 使用传统检索方法和基于mBERT、XLM-R的DPR神经排序器评估公平性，并引入新的LaKDA损失函数来缓解神经MLIR方法中的语言偏见

Result: 分析揭示了当前MLIR技术存在内在语言偏见，不同检索方法间存在显著差异，LaKDA在提升语言公平性方面表现有效

Conclusion: LaKDA损失函数是解决多语言信息检索中语言偏见问题的有效方法，能够显著提高系统的语言公平性

Abstract: Language fairness in multilingual information retrieval (MLIR) systems is
crucial for ensuring equitable access to information across diverse languages.
This paper sheds light on the issue, based on the assumption that queries in
different languages, but with identical semantics, should yield equivalent
ranking lists when retrieving on the same multilingual documents. We evaluate
the degree of fairness using both traditional retrieval methods, and a DPR
neural ranker based on mBERT and XLM-R. Additionally, we introduce `LaKDA', a
novel loss designed to mitigate language biases in neural MLIR approaches. Our
analysis exposes intrinsic language biases in current MLIR technologies, with
notable disparities across the retrieval methods, and the effectiveness of
LaKDA in enhancing language fairness.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [77] [Outcome-based Exploration for LLM Reasoning](https://arxiv.org/abs/2509.06941)
*Yuda Song,Julia Kempe,Remi Munos*

Main category: cs.LG

TL;DR: 强化学习虽然能提升大语言模型的推理准确率，但会导致生成多样性下降。本文提出基于结果的探索方法，通过历史探索和批次探索两种算法，在保持准确率的同时维持多样性。


<details>
  <summary>Details</summary>
Motivation: 传统的基于结果的强化学习方法虽然提高了LLMs的推理准确性，但导致了系统性的生成多样性丧失，这在需要多样性的实际应用场景中会削弱性能。

Method: 提出了基于结果的探索方法，包括两种算法：历史探索（使用UCB式奖励鼓励罕见答案）和批次探索（惩罚批次内重复以促进测试时多样性）。

Result: 在标准数学竞赛数据集上使用Llama和Qwen模型的实验表明，两种方法都能在提高准确率的同时缓解多样性崩溃问题。

Conclusion: 基于结果的探索为强化学习方法提供了一条实用路径，既能增强推理能力，又不牺牲可扩展部署所必需的多样性。

Abstract: Reinforcement learning (RL) has emerged as a powerful method for improving
the reasoning abilities of large language models (LLMs). Outcome-based RL,
which rewards policies solely for the correctness of the final answer, yields
substantial accuracy gains but also induces a systematic loss in generation
diversity. This collapse undermines real-world performance, where diversity is
critical for test-time scaling. We analyze this phenomenon by viewing RL
post-training as a sampling process and show that, strikingly, RL can reduce
effective diversity even on the training set relative to the base model. Our
study highlights two central findings: (i) a transfer of diversity degradation,
where reduced diversity on solved problems propagates to unsolved ones, and
(ii) the tractability of the outcome space, since reasoning tasks admit only a
limited set of distinct answers. Motivated by these insights, we propose
outcome-based exploration, which assigns exploration bonuses according to final
outcomes. We introduce two complementary algorithms: historical exploration,
which encourages rarely observed answers via UCB-style bonuses, and batch
exploration, which penalizes within-batch repetition to promote test-time
diversity. Experiments on standard competition math with Llama and Qwen models
demonstrate that both methods improve accuracy while mitigating diversity
collapse. On the theoretical side, we formalize the benefit of outcome-based
exploration through a new model of outcome-based bandits. Together, these
contributions chart a practical path toward RL methods that enhance reasoning
without sacrificing the diversity essential for scalable deployment.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [78] [Index-Preserving Lightweight Token Pruning for Efficient Document Understanding in Vision-Language Models](https://arxiv.org/abs/2509.06415)
*Jaemin Son,Sujin Choi,Inyong Yun*

Main category: cs.CV

TL;DR: 轻量级文档图像令牌剪枝框架，通过过滤非文本区域降低视觉-语言模型计算成本，保持类似准确性。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言模型在文档理解任务中表现优异，但计算成本过高，需要一种方法来减轻这些负担。

Method: 使用二进制补丁级分类器过滤非文本背景区域，通过最大池化精炼步骤恢复碎片文本区域的空间一致性。

Result: 在实际文档数据集上的实验表明，该方法显著降低了计算成本，同时保持了可比的准确性。

Conclusion: 该轻量级令牌剪枝框架有效地降低了VLMs的计算需求，为高效文档理解提供了可行解决方案。

Abstract: Recent progress in vision-language models (VLMs) has led to impressive
results in document understanding tasks, but their high computational demands
remain a challenge. To mitigate the compute burdens, we propose a lightweight
token pruning framework that filters out non-informative background regions
from document images prior to VLM processing. A binary patch-level classifier
removes non-text areas, and a max-pooling refinement step recovers fragmented
text regions to enhance spatial coherence. Experiments on real-world document
datasets demonstrate that our approach substantially lowers computational
costs, while maintaining comparable accuracy.

</details>


### [79] [Interleaving Reasoning for Better Text-to-Image Generation](https://arxiv.org/abs/2509.06945)
*Wenxuan Huang,Shuang Chen,Zheyong Xie,Shaosheng Cao,Shixiang Tang,Yufan Shen,Qingyu Yin,Wenbo Hu,Xiaoman Wang,Yuntian Tang,Junbo Qiao,Yue Guo,Yao Hu,Zhenfei Yin,Philip Torr,Yu Cheng,Wanli Ouyang,Shaohui Lin*

Main category: cs.CV

TL;DR: IRG框架通过交替文本推理和图像合成来提升文本到图像生成质量，在多个基准测试中取得5-10个百分点的绝对提升


<details>
  <summary>Details</summary>
Motivation: 现有统一多模态模型在图像生成能力上有显著改进，但在指令跟随和细节保持方面仍与GPT-4o等理解-生成紧密耦合的系统存在差距，受交错推理最新进展的启发

Method: 提出交错推理生成(IRG)框架，交替进行文本推理和图像合成：先生成文本思考指导初始图像，然后反思结果以细化细节、视觉质量和美学，同时保持语义。使用IRGL-300K数据集进行两阶段训练

Result: 在GenEval、WISE、TIIF、GenAI-Bench和OneIG-EN等基准测试中获得5-10个百分点的绝对增益，视觉质量和细粒度保真度有显著改进

Conclusion: IRG框架通过交错推理有效提升了文本到图像生成的质量和细节保持能力，达到了最先进的性能水平

Abstract: Unified multimodal understanding and generation models recently have achieve
significant improvement in image generation capability, yet a large gap remains
in instruction following and detail preservation compared to systems that
tightly couple comprehension with generation such as GPT-4o. Motivated by
recent advances in interleaving reasoning, we explore whether such reasoning
can further improve Text-to-Image (T2I) generation. We introduce Interleaving
Reasoning Generation (IRG), a framework that alternates between text-based
thinking and image synthesis: the model first produces a text-based thinking to
guide an initial image, then reflects on the result to refine fine-grained
details, visual quality, and aesthetics while preserving semantics. To train
IRG effectively, we propose Interleaving Reasoning Generation Learning (IRGL),
which targets two sub-goals: (1) strengthening the initial think-and-generate
stage to establish core content and base quality, and (2) enabling high-quality
textual reflection and faithful implementation of those refinements in a
subsequent image. We curate IRGL-300K, a dataset organized into six decomposed
learning modes that jointly cover learning text-based thinking, and full
thinking-image trajectories. Starting from a unified foundation model that
natively emits interleaved text-image outputs, our two-stage training first
builds robust thinking and reflection, then efficiently tunes the IRG pipeline
in the full thinking-image trajectory data. Extensive experiments show SoTA
performance, yielding absolute gains of 5-10 points on GenEval, WISE, TIIF,
GenAI-Bench, and OneIG-EN, alongside substantial improvements in visual quality
and fine-grained fidelity. The code, model weights and datasets will be
released in: https://github.com/Osilly/Interleaving-Reasoning-Generation .

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [80] [Language Native Lightly Structured Databases for Large Language Model Driven Composite Materials Research](https://arxiv.org/abs/2509.06093)
*Yuze Liu,Zhaoyuan Zhang,Xiangsheng Zeng,Yihe Zhang,Leping Yu,Lejia Wang,Xi Yu*

Main category: cs.DB

TL;DR: 这篇论文提出了一种语言原生数据库框架，用于结构化处理化学材料领域的知识描述，特别是氧化硼纳米片聚合物热导复合材料的研究数据，支持高保真度的知识检索和智能助理生成。


<details>
  <summary>Details</summary>
Motivation: 化学材料研究传统上依赖语言描述知识，限制了数据库和机器学习的利用效果。需要一种能够抓取和结构化这些语言信息的方法。

Method: 构建语言原生数据库，抓取论文中的轻级结构化信息，包括制备、表征、理论计算和机理推理等。采用复合检索技术（语义、关键词和值过滤）进行查询。

Result: 系统能够精确、可验证地合成文献信息，生成专业风格的指导。支持高保真度的RAG检索增强生成和工具增强助理，产出可执行的标准操作程序。

Conclusion: 该框架为LLM驱动的材料发现提供了丰富的语言基础，能够有效地结合检索与推理，推动化学材料研究的语言知识利用。

Abstract: Chemical and materials research has traditionally relied heavily on knowledge
narrative, with progress often driven by language-based descriptions of
principles, mechanisms, and experimental experiences, rather than tables,
limiting what conventional databases and ML can exploit. We present a
language-native database for boron nitride nanosheet (BNNS) polymer thermally
conductive composites that captures lightly structured information from papers
across preparation, characterization, theory-computation, and mechanistic
reasoning, with evidence-linked snippets. Records are organized in a
heterogeneous database and queried via composite retrieval with semantics, key
words and value filters. The system can synthesizes literature into accurate,
verifiable, and expert style guidance. This substrate enables high fidelity
efficient Retrieval Augmented Generation (RAG) and tool augmented agents to
interleave retrieval with reasoning and deliver actionable SOP. The framework
supplies the language rich foundation required for LLM-driven materials
discovery.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [81] [Are LLM Agents Behaviorally Coherent? Latent Profiles for Social Simulation](https://arxiv.org/abs/2509.03736)
*James Mooney,Josef Woldense,Zheng Robert Jia,Shirley Anugrah Hayati,My Ha Nguyen,Vipul Raheja,Dongyeop Kang*

Main category: cs.AI

TL;DR: 这篇论文通过实验研究发现，虽然大语言模型能够生成与人类相似的调查数据，但其内部状态在不同实验设置下显示出显著的不一致性，无法真正替代真实人类参与者。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型生成的综合代理是否能够作为真实人类参与者的替代品用于人类主体研究，特别是考察其内部一致性问题。

Method: 设计了一个研究，通过(a)揭示代理的内部状态和(b)在基础对话环境中观察代理行为，以评估代理在不同实验设置下的行为一致性。

Result: 在不同模型家族和模型大小下，LLM都显示出显著的内部不一致性；虽然能够生成与人类相似的响应，但无法保持内部状态的一致性。

Conclusion: 大语言模型在人类主体研究中作为真实参与者替代品的能力存在重大缺陷，其内部不一致性是一个关键问题。

Abstract: The impressive capabilities of Large Language Models (LLMs) have fueled the
notion that synthetic agents can serve as substitutes for real participants in
human-subject research. In an effort to evaluate the merits of this claim,
social science researchers have largely focused on whether LLM-generated survey
data corresponds to that of a human counterpart whom the LLM is prompted to
represent. In contrast, we address a more fundamental question: Do agents
maintain internal consistency, retaining similar behaviors when examined under
different experimental settings? To this end, we develop a study designed to
(a) reveal the agent's internal state and (b) examine agent behavior in a basic
dialogue setting. This design enables us to explore a set of behavioral
hypotheses to assess whether an agent's conversation behavior is consistent
with what we would expect from their revealed internal state. Our findings on
these hypotheses show significant internal inconsistencies in LLMs across model
families and at differing model sizes. Most importantly, we find that, although
agents may generate responses matching those of their human counterparts, they
fail to be internally consistent, representing a critical gap in their
capabilities to accurately substitute for real participants in human-subject
research. Our simulation code and data are publicly accessible.

</details>


### [82] [Reverse-Engineered Reasoning for Open-Ended Generation](https://arxiv.org/abs/2509.06160)
*Haozhe Wang,Haoran Que,Qixin Xu,Minghao Liu,Wangchunshu Zhou,Jiazhan Feng,Wanjun Zhong,Wei Ye,Tong Yang,Wenhao Huang,Ge Zhang,Fangzhen Lin*

Main category: cs.AI

TL;DR: REER是一种新的深度推理范式，通过从已知优秀解决方案反向工程推导出推理过程，解决了开放创意生成中的推理难题


<details>
  <summary>Details</summary>
Motivation: 传统强化学习和指令蒸馏方法在开放创意生成中存在奖励信号不明确、成本高昂等限制，需要新的推理方法

Method: REER反向工程方法：从已知优秀解决方案出发，通过计算发现潜在的逐步深度推理过程，采用可扩展的无梯度方法

Result: 创建了DeepWriting-20K大规模数据集（2万条深度推理轨迹），DeepWriter-8B模型在开放任务上超越开源基线，与GPT-4o和Claude 3.5等专有模型竞争甚至更优

Conclusion: REER范式为开放创意生成中的深度推理提供了新的有效途径，通过反向工程方法克服了传统方法的局限性

Abstract: While the ``deep reasoning'' paradigm has spurred significant advances in
verifiable domains like mathematics, its application to open-ended, creative
generation remains a critical challenge. The two dominant methods for
instilling reasoning -- reinforcement learning (RL) and instruction
distillation -- falter in this area; RL struggles with the absence of clear
reward signals and high-quality reward models, while distillation is
prohibitively expensive and capped by the teacher model's capabilities. To
overcome these limitations, we introduce REverse-Engineered Reasoning (REER), a
new paradigm that fundamentally shifts the approach. Instead of building a
reasoning process ``forwards'' through trial-and-error or imitation, REER works
``backwards'' from known-good solutions to computationally discover the latent,
step-by-step deep reasoning process that could have produced them. Using this
scalable, gradient-free approach, we curate and open-source DeepWriting-20K, a
large-scale dataset of 20,000 deep reasoning trajectories for open-ended tasks.
Our model, DeepWriter-8B, trained on this data, not only surpasses strong
open-source baselines but also achieves performance competitive with, and at
times superior to, leading proprietary models like GPT-4o and Claude 3.5.

</details>


### [83] [From Long to Short: LLMs Excel at Trimming Own Reasoning Chains](https://arxiv.org/abs/2509.06174)
*Wei Han,Geng Zhan,Sicheng Yu,Chenyu Wang,Bryan Hooi*

Main category: cs.AI

TL;DR: EDIT是一种测试时缩放方法，通过约束引导生成和联合跟踪长度与答案分布，帮助大型推理模型找到最短的正确推理路径，平衡简洁性和正确性。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型(LRMs)在处理简单问题时容易过度思考，产生冗长复杂的推理轨迹，影响可解释性。研究发现LRMs难以平衡正确性和简洁性等多个生成目标。

Method: 提出EDIT方法，采用约束引导生成，在测试时联合跟踪长度和答案分布，在不同约束条件下选择最优平衡简洁性和正确性的响应。

Result: 在多个模型和数据集上的广泛实验表明，EDIT显著提高了推理效率，产生紧凑且信息丰富的输出，改善了可读性和用户体验。

Conclusion: EDIT有效解决了LRMs的过度思考问题，通过动态推理修剪实现了推理效率的显著提升，为大型推理模型的实用化提供了重要解决方案。

Abstract: O1/R1 style large reasoning models (LRMs) signal a substantial leap forward
over conventional instruction-following LLMs. By applying test-time scaling to
generate extended reasoning paths, they establish many SOTAs across a wide
range of complex reasoning tasks. However, recent studies show that LRMs are
prone to suffer from overthinking -- the tendency to overcomplicate simple
problems, leading to excessive strategy switching and long, convoluted
reasoning traces that hinder their interpretability. To mitigate this issue, we
conduct a systematic investigation into the reasoning efficiency of a broad set
of LRMs and uncover a common dilemma: the difficulty in balancing multiple
generation objectives such as correctness and brevity. Based on this discovery,
we propose a test-time scaling method, EDIT (Efficient Dynamic Inference
Trimming), which efficiently guides LRMs to identify the shortest correct
reasoning paths at test time. EDIT employs constraint-guided generation while
jointly tracking length and answer distributions under varying constraints,
allowing it to select responses that strike an optimal balance between
conciseness and correctness. Extensive experiments across diverse models and
datasets show that EDIT substantially enhance the reasoning efficiency,
producing compact yet informative outputs that improve readability and user
experience.

</details>


### [84] [SFR-DeepResearch: Towards Effective Reinforcement Learning for Autonomously Reasoning Single Agents](https://arxiv.org/abs/2509.06283)
*Xuan-Phi Nguyen,Shrey Pandit,Revanth Gangi Reddy,Austin Xu,Silvio Savarese,Caiming Xiong,Shafiq Joty*

Main category: cs.AI

TL;DR: 通过持续强化学习优化理性模型，发展自主单代理深度研究能力，在HLE测试中达到28.7%的性能


<details>
  <summary>Details</summary>
Motivation: 为了让大语言模型具备复杂的理性推理和工具使用能力，开启深度研究等重要应用场景

Method: 采用持续强化学习方法，使用全合成数据训练理性优化模型，实现自主动态行动决策

Result: 最佳模型SFR-DR-20B在Humanity's Last Exam测试中达到28.7%的性能

Conclusion: 通过简单的RL训练方法可以有效提升自主单代理的深度研究能力，保持理性能力的同时增强代理技能

Abstract: Equipping large language models (LLMs) with complex, interleaved reasoning
and tool-use capabilities has become a key focus in agentic AI research,
especially with recent advances in reasoning-oriented (``thinking'') models.
Such capabilities are key to unlocking a number of important applications. One
such application is Deep Research (DR), which requires extensive search and
reasoning over many sources. Our work in this paper focuses on the development
of native Autonomous Single-Agent models for DR featuring minimal web crawling
and Python tool integration. Unlike multi-agent systems, where agents take up
pre-defined roles and are told what to do at each step in a static workflow, an
autonomous single-agent determines its next action dynamically based on
context, without manual directive. While prior work has proposed training
recipes for base or instruction-tuned LLMs, we focus on continual reinforcement
learning (RL) of reasoning-optimized models to further enhance agentic skills
while preserving reasoning ability. Towards this end, we propose a simple RL
recipe with entirely synthetic data, which we apply to various open-source
LLMs. Our best variant SFR-DR-20B achieves up to 28.7% on Humanity's Last Exam
benchmark. In addition, we conduct key analysis experiments to provide more
insights into our methodologies.

</details>


### [85] [Reinforcement Learning Foundations for Deep Research Systems: A Survey](https://arxiv.org/abs/2509.06733)
*Wenjun Li,Zhi Chen,Jingru Lin,Hannan Cao,Wei Han,Sheng Liang,Zhi Zhang,Kuicai Dong,Dexun Li,Chen Zhang,Yong Liu*

Main category: cs.AI

TL;DR: 本论文是首个专注于深度搜索系统强化学习基础的综述，系统化了涉及数据合成、RL方法和训练系统的研究进展，为基于RL的潜在代理训练提供实用指南。


<details>
  <summary>Details</summary>
Motivation: 当前深度搜索系统主要依赖SFT和DPO训练，但这些方法存在模仿偏差、环境反馈利用不充分、代理依赖性强等问题。强化学习能够通过优化轨迹级策略、支持探索和恢复行为、减少对人类偏见的依赖来充分利用闭环工具交互。

Method: 论文从三个轴心系统化研究进展：(i)数据合成与筛选；(ii)涉及稳定性、样本效率、长上下文处理、奖励与信贷设计、多目标优化和多模态集成的RL方法；(iii)潜在代理RL训练系统和框架。还涵盖了代理架构、协调以及评估标准。

Result: 论文涵盖了最新的QA、VQA、长文本合成以及领域基础工具交互任务的评测标准，总结了反复出现的模式，持续的基础设施瓶颈，以及基于RL训练健壮、透明深度搜索潜在代理的实用指南。

Conclusion: 强化学习在深度搜索系统中具有重要价值，能够充分利用闭环工具交互、支持探索和恢复行为、减少对人类偏见的依赖。本综述为基于RL的潜在代理训练提供了系统化的知识基础和实践指南。

Abstract: Deep research systems, agentic AI that solve complex, multi-step tasks by
coordinating reasoning, search across the open web and user files, and tool
use, are moving toward hierarchical deployments with a Planner, Coordinator,
and Executors. In practice, training entire stacks end-to-end remains
impractical, so most work trains a single planner connected to core tools such
as search, browsing, and code. While SFT imparts protocol fidelity, it suffers
from imitation and exposure biases and underuses environment feedback.
Preference alignment methods such as DPO are schema and proxy-dependent,
off-policy, and weak for long-horizon credit assignment and multi-objective
trade-offs. A further limitation of SFT and DPO is their reliance on human
defined decision points and subskills through schema design and labeled
comparisons. Reinforcement learning aligns with closed-loop, tool-interaction
research by optimizing trajectory-level policies, enabling exploration,
recovery behaviors, and principled credit assignment, and it reduces dependence
on such human priors and rater biases.
  This survey is, to our knowledge, the first dedicated to the RL foundations
of deep research systems. It systematizes work after DeepSeek-R1 along three
axes: (i) data synthesis and curation; (ii) RL methods for agentic research
covering stability, sample efficiency, long context handling, reward and credit
design, multi-objective optimization, and multimodal integration; and (iii)
agentic RL training systems and frameworks. We also cover agent architecture
and coordination, as well as evaluation and benchmarks, including recent QA,
VQA, long-form synthesis, and domain-grounded, tool-interaction tasks. We
distill recurring patterns, surface infrastructure bottlenecks, and offer
practical guidance for training robust, transparent deep research agents with
RL.

</details>


### [86] [VehicleWorld: A Highly Integrated Multi-Device Environment for Intelligent Vehicle Interaction](https://arxiv.org/abs/2509.06736)
*Jie Yang,Jiajun Chen,Zhangyue Yin,Shuo Chen,Yuxin Wang,Yiran Guo,Yuan Li,Yining Zheng,Xuanjing Huang,Xipeng Qiu*

Main category: cs.AI

TL;DR: 提出了State-based Function Call (SFC)方法，在车载环境中通过显式状态感知和直接状态转换，显著优于传统函数调用方法。


<details>
  <summary>Details</summary>
Motivation: 智能座舱环境复杂，传统函数调用方法需要多次探索性调用来建立环境认知，导致效率低下和错误恢复能力有限。

Method: 开发了VehicleWorld环境，包含30个模块、250个API和680个属性；提出了SFC方法，维护系统状态感知并实现直接状态转换。

Result: SFC方法在实验结果显示显著优于传统FC方法，实现了更高的执行准确性和更低的延迟。

Conclusion: 状态预测比函数调用更适合环境控制，SFC方法为车载智能系统提供了更高效的解决方案，所有实现代码已开源。

Abstract: Intelligent vehicle cockpits present unique challenges for API Agents,
requiring coordination across tightly-coupled subsystems that exceed typical
task environments' complexity. Traditional Function Calling (FC) approaches
operate statelessly, requiring multiple exploratory calls to build
environmental awareness before execution, leading to inefficiency and limited
error recovery. We introduce VehicleWorld, the first comprehensive environment
for the automotive domain, featuring 30 modules, 250 APIs, and 680 properties
with fully executable implementations that provide real-time state information
during agent execution. This environment enables precise evaluation of vehicle
agent behaviors across diverse, challenging scenarios. Through systematic
analysis, we discovered that direct state prediction outperforms function
calling for environmental control. Building on this insight, we propose
State-based Function Call (SFC), a novel approach that maintains explicit
system state awareness and implements direct state transitions to achieve
target conditions. Experimental results demonstrate that SFC significantly
outperforms traditional FC approaches, achieving superior execution accuracy
and reduced latency. We have made all implementation code publicly available on
Github https://github.com/OpenMOSS/VehicleWorld.

</details>


### [87] [RAFFLES: Reasoning-based Attribution of Faults for LLM Systems](https://arxiv.org/abs/2509.06822)
*Chenyang Zhu,Spencer Hong,Jingyu Wu,Kushal Chawla,Charlotte Tang,Youbing Yin,Nathan Wolfe,Erin Babinsky,Daben Liu*

Main category: cs.AI

TL;DR: RAFFLES是一个用于评估长时程多组件LLM代理系统的迭代式评估架构，通过推理和迭代精化来识别系统故障点和原因，在故障检测准确率上显著超越现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前的长时程多组件LLM代理系统评估方法存在局限，主要关注单一指标或端到端结果，难以识别系统故障的具体位置和原因，需要能够推理、探测和迭代理解的评估框架。

Method: RAFFLES采用迭代式多组件流水线架构，使用中央Judge系统调查故障，并通过专门的Evaluators评估系统组件质量及Judge本身的推理质量，建立假设历史记录。

Result: 在Who&When数据集上，RAFFLES在算法生成数据集上达到43%以上的代理-步骤故障对准确率（相比之前最佳16.6%大幅提升），在手工制作数据集上达到20%以上（超越之前最佳8.8%）。

Conclusion: RAFFLES展示了向自主系统引入自动化故障检测的关键进展，有望替代劳动密集型的人工审查，为复杂代理系统的评估提供了有效解决方案。

Abstract: We have reached a critical roadblock in the development and enhancement of
long-horizon, multi-component LLM agentic systems: it is incredibly tricky to
identify where these systems break down and why. Evaluation capabilities that
currently exist today (e.g., single pass LLM-as-a-judge) are limited in that
they often focus on individual metrics or capabilities, end-to-end outcomes,
and are narrowly grounded on the preferences of humans. We argue that to match
the agentic capabilities, evaluation frameworks must also be able to reason,
probe, iterate, and understand the complex logic passing through these systems
over long horizons. In this paper, we present RAFFLES - an evaluation
architecture that incorporates reasoning and iterative refinement.
Specifically, RAFFLES operates as an iterative, multi-component pipeline, using
a central Judge to systematically investigate faults and a set of specialized
Evaluators to assess not only the system's components but also the quality of
the reasoning by the Judge itself, thereby building a history of hypotheses. We
tested RAFFLES against several baselines on the Who&When dataset, a benchmark
designed to diagnose the "who" (agent) and "when" (step) of a system's failure.
RAFFLES outperforms these baselines, achieving an agent-step fault pair
accuracy of over 43% on the Algorithmically-Generated dataset (a substantial
increase from the previously published best of 16.6%) and over 20% on the
Hand-Crafted dataset (surpassing the previously published best of 8.8%). These
results demonstrate a key step towards introducing automated fault detection
for autonomous systems over labor-intensive manual human review.

</details>


### [88] [Test-Time Scaling in Reasoning Models Is Not Effective for Knowledge-Intensive Tasks Yet](https://arxiv.org/abs/2509.06861)
*James Xu Zhao,Bryan Hooi,See-Kiong Ng*

Main category: cs.AI

TL;DR: 这篇论文研究了测试时扩展（test-time scaling）在知识密集任务中的效果，发现增加推理链长度并不能持续提高准确性，反而可能导更多幻觉现象


<details>
  <summary>Details</summary>
Motivation: 测试时扩展技术在多个领域表现优异，但在知识密集任务中的效果不明确，需要识别其对事实准确性和幻觉率的影响

Method: 使用12个推理模型在2个知识密集标准测试集上进行综合评估，分析扩展推理对准确性和幻觉行为的影响

Result: 增加测试时计算并不能持续提高准确性，反而在许多情况下会导致更多幻觉；减少的幻觉多数是模型选择避免回答，而非改善事实记忆；某些模型会因更长推理而尝试回答之前避免的问题，但很多导致幻觉

Conclusion: 尽管存在限制，但与不进行思考相比，启用推理仍然有益；扩展推理可能导致确认偏见，产生过份自信的幻觉

Abstract: Test-time scaling increases inference-time computation by allowing models to
generate long reasoning chains, and has shown strong performance across many
domains. However, in this work, we show that this approach is not yet effective
for knowledge-intensive tasks, where high factual accuracy and low
hallucination rates are essential. We conduct a comprehensive evaluation of
test-time scaling using 12 reasoning models on two knowledge-intensive
benchmarks. Our results reveal that increasing test-time computation does not
consistently improve accuracy and, in many cases, it even leads to more
hallucinations. We then analyze how extended reasoning affects hallucination
behavior. We find that reduced hallucinations often result from the model
choosing to abstain after thinking more, rather than from improved factual
recall. Conversely, for some models, longer reasoning encourages attempts on
previously unanswered questions, many of which result in hallucinations. Case
studies show that extended reasoning can induce confirmation bias, leading to
overconfident hallucinations. Despite these limitations, we observe that
compared to non-thinking, enabling thinking remains beneficial. Code and data
are available at https://github.com/XuZhao0/tts-knowledge

</details>


### [89] [Paper2Agent: Reimagining Research Papers As Interactive and Reliable AI Agents](https://arxiv.org/abs/2509.06917)
*Jiacheng Miao,Joe R. Davis,Jonathan K. Pritchard,James Zou*

Main category: cs.AI

TL;DR: Paper2Agent是一个将研究论文自动转换为AI代理的框架，通过分析论文和代码库构建MCP服务器，使静态论文变成可交互的研究助手


<details>
  <summary>Details</summary>
Motivation: 传统研究论文需要读者投入大量精力理解代码和方法，阻碍了知识的传播和重用，需要将被动的研究成果转化为主动系统

Method: 使用多代理系统分析论文和代码库，构建Model Context Protocol服务器，通过迭代生成和运行测试来优化MCP，然后连接到聊天代理执行复杂科学查询

Result: 成功创建了基于AlphaGenome、ScanPy和TISSUE的论文代理，能够复现原始论文结果并正确执行新用户查询

Conclusion: Paper2Agent通过将静态论文转化为动态交互AI代理，为知识传播和AI协作生态系统奠定了基础

Abstract: We introduce Paper2Agent, an automated framework that converts research
papers into AI agents. Paper2Agent transforms research output from passive
artifacts into active systems that can accelerate downstream use, adoption, and
discovery. Conventional research papers require readers to invest substantial
effort to understand and adapt a paper's code, data, and methods to their own
work, creating barriers to dissemination and reuse. Paper2Agent addresses this
challenge by automatically converting a paper into an AI agent that acts as a
knowledgeable research assistant. It systematically analyzes the paper and the
associated codebase using multiple agents to construct a Model Context Protocol
(MCP) server, then iteratively generates and runs tests to refine and robustify
the resulting MCP. These paper MCPs can then be flexibly connected to a chat
agent (e.g. Claude Code) to carry out complex scientific queries through
natural language while invoking tools and workflows from the original paper. We
demonstrate Paper2Agent's effectiveness in creating reliable and capable paper
agents through in-depth case studies. Paper2Agent created an agent that
leverages AlphaGenome to interpret genomic variants and agents based on ScanPy
and TISSUE to carry out single-cell and spatial transcriptomics analyses. We
validate that these paper agents can reproduce the original paper's results and
can correctly carry out novel user queries. By turning static papers into
dynamic, interactive AI agents, Paper2Agent introduces a new paradigm for
knowledge dissemination and a foundation for the collaborative ecosystem of AI
co-scientists.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [90] [ProtSAE: Disentangling and Interpreting Protein Language Models via Semantically-Guided Sparse Autoencoders](https://arxiv.org/abs/2509.05309)
*Xiangyu Liu,Haodi Lei,Yi Liu,Yang Liu,Wei Hu*

Main category: q-bio.QM

TL;DR: 提出了ProtSAE方法，通过语义引导的稀疏自编码器解决蛋白质语言模型中特征语义纠缠问题，实现更好的生物特征解耦和可解释性


<details>
  <summary>Details</summary>
Motivation: 现有稀疏自编码器(SAE)在蛋白质语言模型中存在语义纠缠问题，单个神经元混合多个非线性概念，难以可靠解释或操作模型行为

Method: 提出ProtSAE方法，在训练过程中使用标注数据集和领域知识进行语义引导，实现语义解耦，减轻纠缠属性的影响

Result: ProtSAE学习到更具生物学相关性和可解释性的隐藏特征，在保持高重建保真度的同时，在可解释性探测中获得更好结果，并展示了下游生成任务的引导潜力

Conclusion: ProtSAE通过语义引导训练有效解决了SAE的语义纠缠问题，为蛋白质语言模型的机制解释提供了更有效的工具

Abstract: Sparse Autoencoder (SAE) has emerged as a powerful tool for mechanistic
interpretability of large language models. Recent works apply SAE to protein
language models (PLMs), aiming to extract and analyze biologically meaningful
features from their latent spaces. However, SAE suffers from semantic
entanglement, where individual neurons often mix multiple nonlinear concepts,
making it difficult to reliably interpret or manipulate model behaviors. In
this paper, we propose a semantically-guided SAE, called ProtSAE. Unlike
existing SAE which requires annotation datasets to filter and interpret
activations, we guide semantic disentanglement during training using both
annotation datasets and domain knowledge to mitigate the effects of entangled
attributes. We design interpretability experiments showing that ProtSAE learns
more biologically relevant and interpretable hidden features compared to
previous methods. Performance analyses further demonstrate that ProtSAE
maintains high reconstruction fidelity while achieving better results in
interpretable probing. We also show the potential of ProtSAE in steering PLMs
for downstream generation tasks.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [91] [Authorship Without Writing: Large Language Models and the Senior Author Analogy](https://arxiv.org/abs/2509.05390)
*Clint Hurshman,Sebastian Porsdam Mann,Julian Savulescu,Brian D. Earp*

Main category: cs.CY

TL;DR: 本文认为在特定条件下使用大语言模型类似于高级作者的角色，LLM生成完整论文草稿应被视为合法的作者形式，否则当前作者标准需要根本性修订。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在生物伦理、科学和医学写作中作者身份认定的争议，探讨人类使用LLM是否以及如何被认定为作者的问题。

Method: 通过类比分析，将LLM的使用与大型研究团队中高级作者的角色进行比较，论证其在特定条件下的合法性。

Result: 提出LLM使用（在特定条件下）可以类比为一种高级作者形式，使用LLM生成完整研究论文草稿应被视为符合许多领域公认标准的合法作者形式。

Conclusion: 要么承认这种使用是合法的，要么需要对当前作者标准进行根本性修订，同时作者声明GPT-5仅用于格式化Box 1，未用于其他部分。

Abstract: The use of large language models (LLMs) in bioethical, scientific, and
medical writing remains controversial. While there is broad agreement in some
circles that LLMs cannot count as authors, there is no consensus about whether
and how humans using LLMs can count as authors. In many fields, authorship is
distributed among large teams of researchers, some of whom, including
paradigmatic senior authors who guide and determine the scope of a project and
ultimately vouch for its integrity, may not write a single word. In this paper,
we argue that LLM use (under specific conditions) is analogous to a form of
senior authorship. On this view, the use of LLMs, even to generate complete
drafts of research papers, can be considered a legitimate form of authorship
according to the accepted criteria in many fields. We conclude that either such
use should be recognized as legitimate, or current criteria for authorship
require fundamental revision. AI use declaration: GPT-5 was used to help format
Box 1. AI was not used for any other part of the preparation or writing of this
manuscript.

</details>
