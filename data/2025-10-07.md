<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 109]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.AI](#cs.AI) [Total: 17]
- [cs.LG](#cs.LG) [Total: 30]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cs.IR](#cs.IR) [Total: 2]
- [cs.SE](#cs.SE) [Total: 2]
- [eess.AS](#eess.AS) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.CY](#cs.CY) [Total: 2]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Decomposing Attention To Find Context-Sensitive Neurons](https://arxiv.org/abs/2510.03315)
*Alex Gibson*

Main category: cs.CL

TL;DR: 通过分析GPT2-Small第一层中注意力模式分散的注意力头，发现其softmax分母在固定token分布下稳定，从而可以结合多个稳定头的输出，仅从权重和单一校准文本就能揭示数百个对上下文属性敏感的第一层神经元。


<details>
  <summary>Details</summary>
Motivation: 研究transformer语言模型中注意力模式分散的注意力头，这些头的注意力分数对内容依赖较弱，其softmax分母在固定token分布下具有稳定性。

Method: 从校准文本中采样softmax分母，结合GPT2-Small第一层中多个稳定注意力头的输出，通过线性汇总周围文本来近似它们的组合输出。

Result: 仅从权重和单一校准文本就能发现数百个第一层神经元对周围文本的高层次上下文属性做出响应，包括在校准文本中未激活的神经元。

Conclusion: 该方法能够仅基于模型权重和单一校准文本，有效识别transformer语言模型中对高层次上下文敏感的神经元，为模型可解释性提供了新途径。

Abstract: We study transformer language models, analyzing attention heads whose
attention patterns are spread out, and whose attention scores depend weakly on
content. We argue that the softmax denominators of these heads are stable when
the underlying token distribution is fixed. By sampling softmax denominators
from a "calibration text", we can combine together the outputs of multiple such
stable heads in the first layer of GPT2-Small, approximating their combined
output by a linear summary of the surrounding text. This approximation enables
a procedure where from the weights alone - and a single calibration text - we
can uncover hundreds of first layer neurons that respond to high-level
contextual properties of the surrounding text, including neurons that didn't
activate on the calibration text.

</details>


### [2] [Graph-S3: Enhancing Agentic textual Graph Retrieval with Synthetic Stepwise Supervision](https://arxiv.org/abs/2510.03323)
*Ge Chang,Jinbo Su,Jiacheng Liu,Pengfei Yang,Yuhao Shang,Huiwen Zheng,Hongli Ma,Yan Liang,Yuanchun Li,Yunxin Liu*

Main category: cs.CL

TL;DR: Graph-S³是一个基于LLM的文本图推理框架，通过合成逐步监督训练检索器，解决了大型图中相关内容的检索挑战，在三个数据集上相比七个基线方法平均准确率提升8.1%，F1分数提升9.7%。


<details>
  <summary>Details</summary>
Motivation: 现实世界中大量数据以文本图形式存在，但现有基于LLM的文本图问答系统在图形检索方面存在挑战，要么依赖浅层嵌入相似性，要么需要大量数据标注和训练成本。

Method: 提出Graph-S³框架，使用基于LLM的检索器，通过合成逐步监督进行训练。关键技术包括用于奖励生成的数据合成管道提取黄金子图，以及基于合成奖励的两阶段训练方案学习交互式图探索策略。

Result: 在三个常见数据集上与七个强基线比较，平均准确率提升8.1%，F1分数提升9.7%，在更复杂的多跳推理任务中优势更明显。

Conclusion: Graph-S³通过合成逐步监督和两阶段训练，有效解决了文本图检索问题，显著提升了问答性能，特别是在复杂推理任务中表现突出。

Abstract: A significant portion of real-world data is inherently represented as textual
graphs, and integrating these graphs into large language models (LLMs) is
promising to enable complex graph-based question answering. However, a key
challenge in LLM-based textual graph QA systems lies in graph retrieval, i.e.,
how to retrieve relevant content from large graphs that is sufficiently
informative while remaining compact for the LLM context. Existing retrievers
suffer from poor performance since they either rely on shallow embedding
similarity or employ interactive retrieving policies that demand excessive data
labeling and training cost. To address these issues, we present Graph-$S^3$, an
agentic textual graph reasoning framework that employs an LLM-based retriever
trained with synthetic stepwise supervision. Instead of rewarding the agent
based on the final answers, which may lead to sparse and unstable training
signals, we propose to closely evaluate each step of the retriever based on
offline-extracted golden subgraphs. Our main techniques include a data
synthesis pipeline to extract the golden subgraphs for reward generation and a
two-stage training scheme to learn the interactive graph exploration policy
based on the synthesized rewards. Based on extensive experiments on three
common datasets in comparison with seven strong baselines, our approach
achieves an average improvement of 8.1\% in accuracy and 9.7\% in F$_1$ score.
The advantage is even higher in more complicated multi-hop reasoning tasks. Our
code will be open-sourced.

</details>


### [3] [Implicit Values Embedded in How Humans and LLMs Complete Subjective Everyday Tasks](https://arxiv.org/abs/2510.03384)
*Arjun Arunasalam,Madison Pickering,Z. Berkay Celik,Blase Ur*

Main category: cs.CL

TL;DR: 该研究审计了6个流行大语言模型在完成30个日常任务时表现出的隐含价值观，并与100名美国众包工作者进行比较，发现LLMs在价值观上既与人类不一致，彼此之间也不一致。


<details>
  <summary>Details</summary>
Motivation: 尽管AI助手在帮助用户完成日常任务方面具有潜力，但人们对这些助手在完成主观日常任务时表现出的隐含价值观知之甚少。

Method: 通过审计6个流行LLMs完成30个日常任务的方式，并与100名美国众包工作者进行比较，分析价值观差异。

Result: 研究发现LLMs在表现环境主义、慈善、多样性等价值观时，往往既与人类不一致，彼此之间也不一致。

Conclusion: LLMs在完成日常任务时表现出的隐含价值观存在显著差异，需要进一步关注和调整以确保与人类价值观的一致性。

Abstract: Large language models (LLMs) can underpin AI assistants that help users with
everyday tasks, such as by making recommendations or performing basic
computation. Despite AI assistants' promise, little is known about the implicit
values these assistants display while completing subjective everyday tasks.
Humans may consider values like environmentalism, charity, and diversity. To
what extent do LLMs exhibit these values in completing everyday tasks? How do
they compare with humans? We answer these questions by auditing how six popular
LLMs complete 30 everyday tasks, comparing LLMs to each other and to 100 human
crowdworkers from the US. We find LLMs often do not align with humans, nor with
other LLMs, in the implicit values exhibited.

</details>


### [4] [Morpheme Induction for Emergent Language](https://arxiv.org/abs/2510.03439)
*Brendon Boldt,David Mortensen*

Main category: cs.CL

TL;DR: CSAR是一种从新兴语言语料库中诱导语素的贪心算法，通过互信息加权、选择最高权重对、从语料库中移除并重复该过程来提取语素。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够从新兴语言语料库中自动识别和提取语素的算法，以理解语言形成的基本单元。

Method: 使用贪心算法，基于形式和意义之间的互信息对语素进行加权，选择最高权重的对，从语料库中移除，然后重复该过程。

Result: 在程序生成的数据集上验证了CSAR的有效性，并在人类语言数据上展示了合理的预测能力，同时量化了新兴语言的语言特征如同义词和多义词程度。

Conclusion: CSAR算法能够有效地从新兴语言中诱导语素，并在多种数据集上表现出良好的性能，为语言形成研究提供了有力工具。

Abstract: We introduce CSAR, an algorithm for inducing morphemes from emergent language
corpora of parallel utterances and meanings. It is a greedy algorithm that (1)
weights morphemes based on mutual information between forms and meanings, (2)
selects the highest-weighted pair, (3) removes it from the corpus, and (4)
repeats the process to induce further morphemes (i.e., Count, Select, Ablate,
Repeat). The effectiveness of CSAR is first validated on procedurally generated
datasets and compared against baselines for related tasks. Second, we validate
CSAR's performance on human language data to show that the algorithm makes
reasonable predictions in adjacent domains. Finally, we analyze a handful of
emergent languages, quantifying linguistic characteristics like degree of
synonymy and polysemy.

</details>


### [5] [Omni-Embed-Nemotron: A Unified Multimodal Retrieval Model for Text, Image, Audio, and Video](https://arxiv.org/abs/2510.03458)
*Mengyao Xu,Wenfei Zhou,Yauhen Babakhin,Gabriel Moreira,Ronay Ak,Radek Osmulski,Bo Liu,Even Oldridge,Benedikt Schifferer*

Main category: cs.CL

TL;DR: Omni-Embed-Nemotron是一个统一的多模态检索嵌入模型，支持文本、图像、音频和视频的跨模态和联合模态检索。


<details>
  <summary>Details</summary>
Motivation: 现有的基于文本的检索器依赖干净的结构化输入，难以处理现实世界文档（如PDF、幻灯片、视频）中视觉和语义丰富的内容。

Method: 基于ColPali和Qwen2.5-Omni等模型，扩展检索能力到音频和视频模态，使用单一模型支持跨模态和联合模态检索。

Result: 在文本、图像和视频检索中展示了有效性。

Conclusion: Omni-Embed-Nemotron能够处理现实世界信息需求的复杂性，为多模态检索提供了统一解决方案。

Abstract: We present Omni-Embed-Nemotron, a unified multimodal retrieval embedding
model developed to handle the increasing complexity of real-world information
needs. While Retrieval-Augmented Generation (RAG) has significantly advanced
language models by incorporating external knowledge, existing text-based
retrievers rely on clean, structured input and struggle with the visually and
semantically rich content found in real-world documents such as PDFs, slides,
or videos. Recent work such as ColPali has shown that preserving document
layout using image-based representations can improve retrieval quality.
Building on this, and inspired by the capabilities of recent multimodal models
such as Qwen2.5-Omni, we extend retrieval beyond text and images to also
support audio and video modalities. Omni-Embed-Nemotron enables both
cross-modal (e.g., text - video) and joint-modal (e.g., text - video+audio)
retrieval using a single model. We describe the architecture, training setup,
and evaluation results of Omni-Embed-Nemotron, and demonstrate its
effectiveness in text, image, and video retrieval.

</details>


### [6] [Searching for the Most Human-like Emergent Language](https://arxiv.org/abs/2510.03467)
*Brendon Boldt,David Mortensen*

Main category: cs.CL

TL;DR: 通过基于信号博弈的涌现通信环境和超参数优化，生成与人类语言相似度最高的涌现语言，使用XferBench作为目标函数来量化统计相似性。


<details>
  <summary>Details</summary>
Motivation: 设计一个能够生成与人类语言高度相似的涌现语言的系统，以提升涌现语言在深度迁移学习中的适用性。

Method: 使用基于信号博弈的涌现通信环境，结合超参数优化和XferBench目标函数来生成和评估涌现语言。

Result: 证明了熵对涌现语言迁移学习性能的预测能力，验证了涌现通信系统的熵最小化特性，并确定了产生更真实涌现语言的超参数。

Conclusion: 成功生成了与人类语言高度相似的涌现语言，并揭示了熵和特定超参数在提升语言真实性和迁移学习性能中的关键作用。

Abstract: In this paper, we design a signalling game-based emergent communication
environment to generate state-of-the-art emergent languages in terms of
similarity to human language. This is done with hyperparameter optimization,
using XferBench as the objective function. XferBench quantifies the statistical
similarity of emergent language to human language by measuring its suitability
for deep transfer learning to human language. Additionally, we demonstrate the
predictive power of entropy on the transfer learning performance of emergent
language as well as corroborate previous results on the entropy-minimization
properties of emergent communication systems. Finally, we report
generalizations regarding what hyperparameters produce more realistic emergent
languages, that is, ones which transfer better to human language.

</details>


### [7] [SEER: The Span-based Emotion Evidence Retrieval Benchmark](https://arxiv.org/abs/2510.03490)
*Aneesha Sampath,Oya Aran,Emily Mower Provost*

Main category: cs.CL

TL;DR: SEER基准测试评估大语言模型识别文本中表达情感的具体片段的能力，包含单句和跨句情感证据检测任务，发现模型在长文本中表现下降。


<details>
  <summary>Details</summary>
Motivation: 传统情感识别任务只给整句分配单一标签，而实际应用如共情对话和临床支持需要知道情感是如何表达的，因此需要更细粒度的情感证据检测。

Method: 构建包含1200个真实世界句子的SEER基准，包含单句和五句短段落两个任务，评估14个开源大语言模型的情感证据检测能力。

Result: 部分模型在单句任务上接近人类平均表现，但在长段落中准确率下降。错误分析显示模型过度依赖情感关键词，在中文文本中产生误报。

Conclusion: SEER基准揭示了当前大语言模型在细粒度情感证据检测方面的局限性，特别是在处理长文本时表现不佳，需要改进模型对情感表达方式的理解。

Abstract: We introduce the SEER (Span-based Emotion Evidence Retrieval) Benchmark to
test Large Language Models' (LLMs) ability to identify the specific spans of
text that express emotion. Unlike traditional emotion recognition tasks that
assign a single label to an entire sentence, SEER targets the underexplored
task of emotion evidence detection: pinpointing which exact phrases convey
emotion. This span-level approach is crucial for applications like empathetic
dialogue and clinical support, which need to know how emotion is expressed, not
just what the emotion is. SEER includes two tasks: identifying emotion evidence
within a single sentence, and identifying evidence across a short passage of
five consecutive sentences. It contains new annotations for both emotion and
emotion evidence on 1200 real-world sentences. We evaluate 14 open-source LLMs
and find that, while some models approach average human performance on
single-sentence inputs, their accuracy degrades in longer passages. Our error
analysis reveals key failure modes, including overreliance on emotion keywords
and false positives in neutral text.

</details>


### [8] [ALHD: A Large-Scale and Multigenre Benchmark Dataset for Arabic LLM-Generated Text Detection](https://arxiv.org/abs/2510.03502)
*Ali Khairallah,Arkaitz Zubiaga*

Main category: cs.CL

TL;DR: ALHD是首个专为区分阿拉伯语人类生成与LLM生成文本的大规模综合数据集，涵盖新闻、社交媒体和评论三种体裁，包含超过40万个平衡样本，支持阿拉伯语LLM文本检测的可泛化性研究。


<details>
  <summary>Details</summary>
Motivation: 建立阿拉伯语LLM生成文本检测的基础设施，以应对错误信息、学术不端和网络威胁等风险，填补阿拉伯语在该研究领域的空白。

Method: 构建包含三种体裁（新闻、社交媒体、评论）的平衡数据集，涵盖现代标准阿拉伯语和方言阿拉伯语，使用三种领先LLM生成文本，并提供严格预处理、丰富标注和标准化分割。

Result: 微调的BERT模型在检测性能上表现最佳，优于基于LLM的模型，但在跨体裁泛化方面存在挑战，特别是在新闻文章中LLM生成文本与人类文本风格相似时。

Conclusion: ALHD为阿拉伯语LLM检测研究奠定了基础，揭示了跨体裁泛化的挑战，为未来研究指明了方向，特别是在处理新闻体裁中LLM生成文本检测的困难。

Abstract: We introduce ALHD, the first large-scale comprehensive Arabic dataset
explicitly designed to distinguish between human- and LLM-generated texts. ALHD
spans three genres (news, social media, reviews), covering both MSA and
dialectal Arabic, and contains over 400K balanced samples generated by three
leading LLMs and originated from multiple human sources, which enables studying
generalizability in Arabic LLM-genearted text detection. We provide rigorous
preprocessing, rich annotations, and standardized balanced splits to support
reproducibility. In addition, we present, analyze and discuss benchmark
experiments using our new dataset, in turn identifying gaps and proposing
future research directions. Benchmarking across traditional classifiers,
BERT-based models, and LLMs (zero-shot and few-shot) demonstrates that
fine-tuned BERT models achieve competitive performance, outperforming LLM-based
models. Results are however not always consistent, as we observe challenges
when generalizing across genres; indeed, models struggle to generalize when
they need to deal with unseen patterns in cross-genre settings, and these
challenges are particularly prominent when dealing with news articles, where
LLM-generated texts resemble human texts in style, which opens up avenues for
future research. ALHD establishes a foundation for research related to Arabic
LLM-detection and mitigating risks of misinformation, academic dishonesty, and
cyber threats.

</details>


### [9] [TS-Reasoner: Aligning Time Series Foundation Models with LLM Reasoning](https://arxiv.org/abs/2510.03519)
*Fangxu Yu,Hongyu Zhao,Tianyi Zhou*

Main category: cs.CL

TL;DR: TS-Reasoner通过将时间序列基础模型的潜在表示与大型语言模型的文本输入对齐，解决了时间序列推理任务中数值理解和语义推理的融合挑战。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列基础模型能捕捉动态模式但缺乏推理能力，而大型语言模型具有推理能力但不擅长数值理解。需要有效整合两种模型以实现时间序列推理。

Method: 提出两阶段训练方法：首先使用合成的时间序列-文本对进行对齐预训练，然后进行指令微调。冻结预训练的时间序列基础模型，仅训练对齐模块。

Result: 在多个基准测试中，TS-Reasoner优于主流LLM、VLM和时间序列LLM，且具有显著的数据效率（使用不到一半的训练数据）。

Conclusion: TS-Reasoner成功实现了时间序列基础模型与大型语言模型的有效对齐，为时间序列推理任务提供了高效解决方案。

Abstract: Time series reasoning is crucial to decision-making in diverse domains,
including finance, energy usage, traffic, weather, and scientific discovery.
While existing time series foundation models (TSFMs) can capture low-level
dynamic patterns and provide accurate forecasting, further analysis usually
requires additional background knowledge and sophisticated reasoning, which are
lacking in most TSFMs but can be achieved through large language models (LLMs).
On the other hand, without expensive post-training, LLMs often struggle with
the numerical understanding of time series data. Although it is intuitive to
integrate the two types of models, developing effective training recipes that
align the two modalities for reasoning tasks is still an open challenge. To
this end, we propose TS-Reasoner that aligns the latent representations of
TSFMs with the textual inputs of LLMs for downstream understanding/reasoning
tasks. Specifically, we propose a simple yet effective method to curate
diverse, synthetic pairs of time series and textual captions for alignment
training. We then develop a two-stage training recipe that applies instruction
finetuning after the alignment pretraining. Unlike existing works that train an
LLM to take time series as inputs, we leverage a pretrained TSFM and freeze it
during training. Extensive experiments on several benchmarks demonstrate that
TS-Reasoner not only outperforms a wide range of prevailing LLMs, Vision
Language Models (VLMs), and Time Series LLMs, but also achieves this with
remarkable data efficiency, e.g., using less than half the training data.

</details>


### [10] [Identifying Financial Risk Information Using RAG with a Contrastive Insight](https://arxiv.org/abs/2510.03521)
*Ali Elahi*

Main category: cs.CL

TL;DR: 在专业领域中，人类通常通过比较相似案例来分析新问题，而传统RAG系统缺乏这种对比推理能力。本文提出在RAG之上增加对比推理层，以生成更具体的专业见解。


<details>
  <summary>Details</summary>
Motivation: 传统RAG系统在专业推理任务中输出过于通用，无法提供针对性的具体见解，特别是在金融领域无法识别特定公司的独特风险。

Method: 在RAG系统之上增加一个对比推理层，通过检索和比较相似案例来生成更具体的专业分析。

Result: 该方法在ROUGE和BERTScore等文本生成指标上优于基准RAG系统，与人工生成的股票研究和风险分析更接近。

Conclusion: 对比推理层能够显著提升RAG系统在专业领域的推理质量，生成更具体、更有针对性的分析结果。

Abstract: In specialized domains, humans often compare new problems against similar
examples, highlight nuances, and draw conclusions instead of analyzing
information in isolation. When applying reasoning in specialized contexts with
LLMs on top of a RAG, the pipeline can capture contextually relevant
information, but it is not designed to retrieve comparable cases or related
problems.
  While RAG is effective at extracting factual information, its outputs in
specialized reasoning tasks often remain generic, reflecting broad facts rather
than context-specific insights. In finance, it results in generic risks that
are true for the majority of companies. To address this limitation, we propose
a peer-aware comparative inference layer on top of RAG.
  Our contrastive approach outperforms baseline RAG in text generation metrics
such as ROUGE and BERTScore in comparison with human-generated equity research
and risk.

</details>


### [11] [Sample, Align, Synthesize: Graph-Based Response Synthesis with ConGrs](https://arxiv.org/abs/2510.03527)
*Sayan Ghosh,Shahzaib Saqib Warraich,Dhruv Tarsadiya,Gregory Yauney,Swabha Swayamdipta*

Main category: cs.CL

TL;DR: 提出Consensus Graphs (ConGrs)数据结构，通过采样多个语言模型响应并构建有向无环图来捕捉共享信息和语义变化，从而合成更准确、更有效的最终响应。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法有效合成不同长格式响应中的丰富认知信号，需要一种灵活的数据结构来捕捉语言模型响应中的共享信息和语义变化。

Method: 使用生物信息学中的轻量级词序列对齐算法构建ConGrs，辅以辅助语言模型判断，并设计任务相关的解码方法从ConGr数据结构合成最终响应。

Result: 在传记生成任务中事实精度提升31%，减少对语言模型判断的依赖超过80%；在拒绝任务中弃权率提升56%；在数学推理任务中准确率提升6个百分点。

Conclusion: ConGrs提供了一种灵活的方法来捕捉语言模型响应中的变化，并利用响应变化提供的认知信号合成更有效的响应。

Abstract: Language models can be sampled multiple times to access the distribution
underlying their responses, but existing methods cannot efficiently synthesize
rich epistemic signals across different long-form responses. We introduce
Consensus Graphs (ConGrs), a flexible DAG-based data structure that represents
shared information, as well as semantic variation in a set of sampled LM
responses to the same prompt. We construct ConGrs using a light-weight lexical
sequence alignment algorithm from bioinformatics, supplemented by the targeted
usage of a secondary LM judge. Further, we design task-dependent decoding
methods to synthesize a single, final response from our ConGr data structure.
Our experiments show that synthesizing responses from ConGrs improves factual
precision on two biography generation tasks by up to 31% over an average
response and reduces reliance on LM judges by more than 80% compared to other
methods. We also use ConGrs for three refusal-based tasks requiring abstention
on unanswerable queries and find that abstention rate is increased by up to
56%. We apply our approach to the MATH and AIME reasoning tasks and find an
improvement over self-verification and majority vote baselines by up to 6
points of accuracy. We show that ConGrs provide a flexible method for capturing
variation in LM responses and using the epistemic signals provided by response
variation to synthesize more effective responses.

</details>


### [12] [Fine-Tuning on Noisy Instructions: Effects on Generalization and Performance](https://arxiv.org/abs/2510.03528)
*Ahmed Alajrami,Xingwei Tan,Nikolaos Aletras*

Main category: cs.CL

TL;DR: 研究探索在指令微调数据中引入扰动是否能增强大语言模型对噪声指令的抵抗能力，发现某些情况下扰动指令微调反而能提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明大语言模型对指令表述的微小变化很敏感，本研究旨在探索通过引入指令扰动是否能增强模型对噪声输入的鲁棒性。

Method: 在指令微调数据中引入扰动（如删除停用词、打乱词序），然后在多个基准测试（MMLU、BBH、GSM8K）上评估模型在原始和扰动版本上的表现。

Result: 令人惊讶的是，在某些情况下，使用扰动指令进行微调反而能提升下游任务性能，使模型对噪声用户输入更具韧性。

Conclusion: 研究强调了在指令微调中包含扰动指令的重要性，这可以使大语言模型对噪声用户输入更具韧性。

Abstract: Instruction-tuning plays a vital role in enhancing the task-solving abilities
of large language models (LLMs), improving their usability in generating
helpful responses on various tasks. However, previous work has demonstrated
that they are sensitive to minor variations in instruction phrasing. In this
paper, we explore whether introducing perturbations in instruction-tuning data
can enhance LLMs' resistance against noisy instructions. We focus on how
instruction-tuning with perturbations, such as removing stop words or shuffling
words, affects LLMs' performance on the original and perturbed versions of
widely-used benchmarks (MMLU, BBH, GSM8K). We further assess learning dynamics
and potential shifts in model behavior. Surprisingly, our results suggest that
instruction-tuning on perturbed instructions can, in some cases, improve
downstream performance. These findings highlight the importance of including
perturbed instructions in instruction-tuning, which can make LLMs more
resilient to noisy user inputs.

</details>


### [13] [TriMediQ: A Triplet-Structured Approach for Interactive Medical Question Answering](https://arxiv.org/abs/2510.03536)
*Zhaohan Meng,Zaiqiao Meng,Siwei Liu,Iadh Ounis*

Main category: cs.CL

TL;DR: TriMediQ框架通过将患者回答转换为三元组结构并构建知识图谱，解决了LLM在多轮医疗对话中推理能力下降的问题，在iMedQA数据集上比基线方法准确率提升10.4%


<details>
  <summary>Details</summary>
Motivation: 现有的LLM在静态单轮医疗问答中表现良好，但在实际临床咨询的多轮交互式信息收集过程中可靠性显著下降，因为临床事实在对话日志中缺乏清晰关联

Method: 引入三元组结构方法：使用冻结的三元组生成器提取临床相关三元组，构建知识图谱；训练投影模块（图编码器和投影器）从KG中捕获关系信息；分两步操作：冻结LLM权重微调投影模块，在推理时使用微调模块指导多跳推理

Result: 在两个交互式QA基准测试中，TriMediQ在iMedQA数据集上比五个基线方法准确率最高提升10.4%

Conclusion: 将患者回答转换为基于三元组的结构化图谱能够在多轮设置中实现更准确的临床推理，为基于LLM的医疗助手部署提供了解决方案

Abstract: Large Language Models (LLMs) perform strongly in static and single-turn
medical Question Answer (QA) benchmarks, yet such settings diverge from the
iterative information gathering process required in practical clinical
consultations. The MEDIQ framework addresses this mismatch by recasting the
diagnosis as an interactive dialogue between a patient and an expert system,
but the reliability of LLMs drops dramatically when forced to reason with
dialogue logs, where clinical facts appear in sentences without clear links. To
bridge this gap, we introduce TriMediQ, a triplet-structured approach that
summarises patient responses into triplets and integrates them into a Knowledge
Graph (KG), enabling multi-hop reasoning. We introduce a frozen triplet
generator that extracts clinically relevant triplets, using prompts designed to
ensure factual consistency. In parallel, a trainable projection module,
comprising a graph encoder and a projector, captures relational information
from the KG to enhance expert reasoning. TriMediQ operates in two steps: (i)
the projection module fine-tuning with all LLM weights frozen; and (ii) using
the fine-tuned module to guide multi-hop reasoning during inference. We
evaluate TriMediQ on two interactive QA benchmarks, showing that it achieves up
to 10.4\% improvement in accuracy over five baselines on the iMedQA dataset.
These results demonstrate that converting patient responses into structured
triplet-based graphs enables more accurate clinical reasoning in multi-turn
settings, providing a solution for the deployment of LLM-based medical
assistants.

</details>


### [14] [What is a protest anyway? Codebook conceptualization is still a first-order concern in LLM-era classification](https://arxiv.org/abs/2510.03541)
*Andrew Halterman,Katherine A. Keith*

Main category: cs.CL

TL;DR: 论文指出在计算社会科学中使用大语言模型进行文本分类时，概念化步骤常被忽视，这会导致概念化偏差，影响下游统计推断的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前计算社会科学广泛使用大语言模型进行文本分类，但分析人员容易跳过概念化步骤，直接进行模型提示，这种概念化错误会带来偏差。

Method: 通过模拟实验，研究概念化引起的偏差如何影响下游估计，并测试增加LLM准确性或事后偏差校正方法是否能纠正这种偏差。

Result: 研究发现概念化引起的偏差无法仅通过提高LLM准确性或事后偏差校正方法来纠正，这种偏差会持续影响下游估计。

Conclusion: 在LLM时代，概念化仍然是首要关注点，论文提供了实现低成本、无偏差、低方差下游估计的具体建议。

Abstract: Generative large language models (LLMs) are now used extensively for text
classification in computational social science (CSS). In this work, focus on
the steps before and after LLM prompting -- conceptualization of concepts to be
classified and using LLM predictions in downstream statistical inference --
which we argue have been overlooked in much of LLM-era CSS. We claim LLMs can
tempt analysts to skip the conceptualization step, creating conceptualization
errors that bias downstream estimates. Using simulations, we show that this
conceptualization-induced bias cannot be corrected for solely by increasing LLM
accuracy or post-hoc bias correction methods. We conclude by reminding CSS
analysts that conceptualization is still a first-order concern in the LLM-era
and provide concrete advice on how to pursue low-cost, unbiased, low-variance
downstream estimates.

</details>


### [15] [CCD-Bench: Probing Cultural Conflict in Large Language Model Decision-Making](https://arxiv.org/abs/2510.03553)
*Hasibur Rahman,Hanan Salam*

Main category: cs.CL

TL;DR: CCD-Bench是一个评估LLM在跨文化价值冲突下决策能力的基准测试，包含2,182个涉及七个领域的开放式困境，发现LLM偏好北欧和日耳曼欧洲文化，而忽视东欧和中东文化，显示当前对齐策略促进共识导向世界观。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注文化知识、价值预测或单轴偏见诊断，缺乏评估LLM在多个文化价值观直接冲突时的裁决能力。

Method: 使用2,182个开放式困境，每个困境配以十个对应GLOBE文化集群的匿名响应选项，采用分层拉丁方设计来减轻顺序效应，评估17个非推理LLM。

Result: 模型明显偏好北欧欧洲（20.2%）和日耳曼欧洲（12.4%），而东欧和中东北非选项代表性不足（5.6-5.8%）。虽然87.9%的理由引用多个GLOBE维度，但这种多元性是表面的。

Conclusion: 当前对齐流程促进共识导向世界观，无法充分处理需要权力协商、基于权利的推理或性别意识分析的场景，需要能实质性参与多元世界观的校准策略。

Abstract: Although large language models (LLMs) are increasingly implicated in
interpersonal and societal decision-making, their ability to navigate explicit
conflicts between legitimately different cultural value systems remains largely
unexamined. Existing benchmarks predominantly target cultural knowledge
(CulturalBench), value prediction (WorldValuesBench), or single-axis bias
diagnostics (CDEval); none evaluate how LLMs adjudicate when multiple
culturally grounded values directly clash. We address this gap with CCD-Bench,
a benchmark that assesses LLM decision-making under cross-cultural value
conflict. CCD-Bench comprises 2,182 open-ended dilemmas spanning seven domains,
each paired with ten anonymized response options corresponding to the ten GLOBE
cultural clusters. These dilemmas are presented using a stratified Latin square
to mitigate ordering effects. We evaluate 17 non-reasoning LLMs. Models
disproportionately prefer Nordic Europe (mean 20.2 percent) and Germanic Europe
(12.4 percent), while options for Eastern Europe and the Middle East and North
Africa are underrepresented (5.6 to 5.8 percent). Although 87.9 percent of
rationales reference multiple GLOBE dimensions, this pluralism is superficial:
models recombine Future Orientation and Performance Orientation, and rarely
ground choices in Assertiveness or Gender Egalitarianism (both under 3
percent). Ordering effects are negligible (Cramer's V less than 0.10), and
symmetrized KL divergence shows clustering by developer lineage rather than
geography. These patterns suggest that current alignment pipelines promote a
consensus-oriented worldview that underserves scenarios demanding power
negotiation, rights-based reasoning, or gender-aware analysis. CCD-Bench shifts
evaluation beyond isolated bias detection toward pluralistic decision making
and highlights the need for alignment strategies that substantively engage
diverse worldviews.

</details>


### [16] [Reactive Transformer (RxT) -- Stateful Real-Time Processing for Event-Driven Reactive Language Models](https://arxiv.org/abs/2510.03561)
*Adam Filipek*

Main category: cs.CL

TL;DR: RxT是一种新型Transformer架构，通过事件驱动范式解决传统Transformer在对话AI中的状态保持和计算复杂度问题，将对话成本从二次方降低到线性。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer在对话AI中存在无状态特性和二次计算复杂度问题，导致长对话中成本过高和延迟严重。

Method: RxT采用事件驱动架构，将每个对话轮次作为离散事件处理，使用固定大小的短期记忆系统，通过生成器-解码器生成响应，异步更新记忆状态。

Result: 实验验证显示RxT在合成数据上表现优异，推理延迟恒定，相比基线无状态模型具有明显优势。

Conclusion: RxT通过解耦响应生成和记忆更新，实现了低延迟、有状态且经济可行的长对话系统，从根本上改变了对话AI的扩展动态。

Abstract: The Transformer architecture has become the de facto standard for Large
Language Models (LLMs), demonstrating remarkable capabilities in language
understanding and generation. However, its application in conversational AI is
fundamentally constrained by its stateless nature and the quadratic
computational complexity ($O(L^2)$) with respect to sequence length $L$.
Current models emulate memory by reprocessing an ever-expanding conversation
history with each turn, leading to prohibitive costs and latency in long
dialogues. This paper introduces the Reactive Transformer (RxT), a novel
architecture designed to overcome these limitations by shifting from a
data-driven to an event-driven paradigm. RxT processes each conversational turn
as a discrete event in real-time, maintaining context in an integrated,
fixed-size Short-Term Memory (STM) system. The architecture features a distinct
operational cycle where a generator-decoder produces a response based on the
current query and the previous memory state, after which a memory-encoder and a
dedicated Memory Attention network asynchronously update the STM with a
representation of the complete interaction. This design fundamentally alters
the scaling dynamics, reducing the total user-facing cost of a conversation
from quadratic ($O(N^2 \cdot T)$) to linear ($O(N \cdot T)$) with respect to
the number of interactions $N$. By decoupling response generation from memory
updates, RxT achieves low latency, enabling truly real-time, stateful, and
economically viable long-form conversations. We validated our architecture with
a series of proof-of-concept experiments on synthetic data, demonstrating
superior performance and constant-time inference latency compared to a baseline
stateless model of comparable size.

</details>


### [17] [LLM, Reporting In! Medical Information Extraction Across Prompting, Fine-tuning and Post-correction](https://arxiv.org/abs/2510.03577)
*Ikram Belmadani,Parisa Nazari Hashemi,Thomas Sebbag,Benoit Favre,Guillaume Fortier,Solen Quiniou,Emmanuel Morin,Richard Dufour*

Main category: cs.CL

TL;DR: 本文介绍了在EvalLLM 2025挑战赛中针对法语生物医学命名实体识别和健康事件抽取的三种方法，主要基于大语言模型、合成数据和后处理技术，其中GPT-4.1在少样本设置下表现最佳。


<details>
  <summary>Details</summary>
Motivation: 解决法语生物医学领域在极低资源场景下的命名实体识别和事件抽取问题，探索大语言模型在少样本学习中的潜力。

Method: 提出三种方法：(1) GPT-4.1的上下文学习，自动选择10个示例并整合标注指南；(2) GLiNER系统在合成语料上微调后经LLM验证；(3) LLaMA-3.1-8B-Instruct在相同合成语料上微调。事件抽取采用与NER相同的上下文学习策略。

Result: GPT-4.1在命名实体识别上获得61.53%的宏F1分数，在事件抽取上获得15.02%的宏F1分数，表现最佳。

Conclusion: 精心设计的提示工程在极低资源场景下对最大化大语言模型性能至关重要，GPT-4.1在少样本法语生物医学文本处理中展现出优势。

Abstract: This work presents our participation in the EvalLLM 2025 challenge on
biomedical Named Entity Recognition (NER) and health event extraction in French
(few-shot setting). For NER, we propose three approaches combining large
language models (LLMs), annotation guidelines, synthetic data, and
post-processing: (1) in-context learning (ICL) with GPT-4.1, incorporating
automatic selection of 10 examples and a summary of the annotation guidelines
into the prompt, (2) the universal NER system GLiNER, fine-tuned on a synthetic
corpus and then verified by an LLM in post-processing, and (3) the open LLM
LLaMA-3.1-8B-Instruct, fine-tuned on the same synthetic corpus. Event
extraction uses the same ICL strategy with GPT-4.1, reusing the guideline
summary in the prompt. Results show GPT-4.1 leads with a macro-F1 of 61.53% for
NER and 15.02% for event extraction, highlighting the importance of
well-crafted prompting to maximize performance in very low-resource scenarios.

</details>


### [18] [Decoupling Task-Solving and Output Formatting in LLM Generation](https://arxiv.org/abs/2510.03595)
*Haikang Deng,Po-Nien Kung,Nanyun Peng*

Main category: cs.CL

TL;DR: Deco-G是一个解码框架，通过将格式遵循与任务解决解耦来提高大语言模型的性能。它使用单独的概率模型处理格式合规性，让LLM专注于任务指令，从而在多种任务上实现1.0%到6.0%的相对性能提升。


<details>
  <summary>Details</summary>
Motivation: 随着提示变得复杂，大语言模型往往难以同时遵循所有指令，特别是在推理指令与严格格式要求交织时。这种纠缠导致模型目标冲突，需要将这两个方面明确分离以提高性能。

Method: Deco-G框架使用单独的可处理概率模型(TPM)处理格式合规性，而LLM仅接收任务指令。在每个解码步骤中，将LLM的下一个令牌概率与TPM计算的格式合规似然相结合。关键创新包括：指令感知蒸馏、灵活的trie构建算法和HMM状态剪枝。

Result: 在数学推理、LLM-as-a-judge和事件参数提取等多样化格式要求的任务中，Deco-G相比常规提示方法实现了1.0%到6.0%的相对性能提升，并保证格式合规。

Conclusion: 通过明确解耦格式遵循与任务解决，Deco-G框架能有效提高大语言模型在复杂指令下的性能，同时确保格式合规性。

Abstract: Large language models (LLMs) are increasingly adept at following instructions
containing task descriptions to solve complex problems, such as mathematical
reasoning and automatic evaluation (LLM-as-a-Judge). However, as prompts grow
more complex, models often struggle to adhere to all instructions. This
difficulty is especially common when instructive prompts intertwine reasoning
directives -- specifying what the model should solve -- with rigid formatting
requirements that dictate how the solution must be presented. The entanglement
creates competing goals for the model, suggesting that more explicit separation
of these two aspects could lead to improved performance. To this front, we
introduce Deco-G, a decoding framework that explicitly decouples format
adherence from task solving. Deco-G handles format compliance with a separate
tractable probabilistic model (TPM), while prompts LLMs with only task
instructions. At each decoding step, Deco-G combines next token probabilities
from the LLM with the TPM calculated format compliance likelihood to form the
output probability. To make this approach both practical and scalable for
modern instruction-tuned LLMs, we introduce three key innovations:
instruction-aware distillation, a flexible trie-building algorithm, and HMM
state pruning for computational efficiency. We demonstrate the effectiveness of
Deco-G across a wide range of tasks with diverse format requirements, including
mathematical reasoning, LLM-as-a-judge, and event argument extraction. Overall,
our approach yields 1.0% to 6.0% relative gain over regular prompting practice
with guaranteed format compliance.

</details>


### [19] [Can an LLM Induce a Graph? Investigating Memory Drift and Context Length](https://arxiv.org/abs/2510.03611)
*Raquib Bin Yousuf,Aadyant Khatri,Shengzhe Xu,Mandar Sharma,Naren Ramakrishnan*

Main category: cs.CL

TL;DR: 现有评估基准在衡量LLMs的有效上下文长度和遗忘倾向方面存在不足，本文提出通过复杂的关系推理任务来更准确地评估模型性能，发现LLMs在关系推理中比现有基准显示更早出现记忆漂移和上下文遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有评估基准主要依赖简单的检索或续写任务，不能准确反映LLMs在信息密集场景中的实际表现，特别是处理需要从文本中提取结构化关系知识的复杂推理任务。

Method: 使用需要从潜在噪声自然语言内容中归纳图结构关系的复杂推理任务进行评估，这些任务要求模型从分布在不同上下文中的文本线索中推断连接关系。

Result: 研究发现LLMs在进行关系推理时，在比现有基准建议的更短的有效长度下就开始出现记忆漂移和上下文遗忘，即使是专门用于推理的模型如OpenAI o1也容易受到早期记忆漂移的影响。

Conclusion: LLMs从非结构化输入中抽象结构化知识的能力存在显著局限性，需要架构改进来提升长距离推理能力，本文为复杂推理任务中优化使用流行LLMs提供了建议。

Abstract: Recently proposed evaluation benchmarks aim to characterize the effective
context length and the forgetting tendencies of large language models (LLMs).
However, these benchmarks often rely on simplistic 'needle in a haystack'
retrieval or continuation tasks that may not accurately reflect the performance
of these models in information-dense scenarios. Thus, rather than simple next
token prediction, we argue for evaluating these models on more complex
reasoning tasks that requires them to induce structured relational knowledge
from the text - such as graphs from potentially noisy natural language content.
While the input text can be viewed as generated in terms of a graph, its
structure is not made explicit and connections must be induced from distributed
textual cues, separated by long contexts and interspersed with irrelevant
information. Our findings reveal that LLMs begin to exhibit memory drift and
contextual forgetting at much shorter effective lengths when tasked with this
form of relational reasoning, compared to what existing benchmarks suggest.
With these findings, we offer recommendations for the optimal use of popular
LLMs for complex reasoning tasks. We further show that even models specialized
for reasoning, such as OpenAI o1, remain vulnerable to early memory drift in
these settings. These results point to significant limitations in the models'
ability to abstract structured knowledge from unstructured input and highlight
the need for architectural adaptations to improve long-range reasoning.

</details>


### [20] [Towards Unsupervised Speech Recognition at the Syllable-Level](https://arxiv.org/abs/2510.03639)
*Liming Wang,Junrui Ni,Kai-Wei Chang,Saurabhchand Bhati,David Harwath,Mark Hasegawa-Johnson,James R. Glass*

Main category: cs.CL

TL;DR: 提出基于音节级掩码语言建模的无监督语音识别框架，无需G2P转换器，解决了GAN方法的不稳定性问题，在LibriSpeech上实现40%相对字符错误率降低，并能有效泛化到汉语。


<details>
  <summary>Details</summary>
Motivation: 解决无监督语音识别中现有方法依赖昂贵G2P资源、在模糊音素边界语言中泛化能力差的问题，扩展ASR到低资源语言。

Method: 基于音节级掩码语言建模的无监督语音识别框架，避免使用G2P和GAN方法的不稳定性。

Result: 在LibriSpeech上实现40%相对字符错误率降低，在汉语等难处理语言上有效泛化。

Conclusion: 该音节级UASR框架为低资源语言ASR提供了有效解决方案，无需平行数据即可训练语音识别器。

Abstract: Training speech recognizers with unpaired speech and text -- known as
unsupervised speech recognition (UASR) -- is a crucial step toward extending
ASR to low-resource languages in the long-tail distribution and enabling
multimodal learning from non-parallel data. However, existing approaches based
on phones often rely on costly resources such as grapheme-to-phoneme converters
(G2Ps) and struggle to generalize to languages with ambiguous phoneme
boundaries due to training instability. In this paper, we address both
challenges by introducing a syllable-level UASR framework based on masked
language modeling, which avoids the need for G2P and the instability of
GAN-based methods. Our approach achieves up to a 40\% relative reduction in
character error rate (CER) on LibriSpeech and generalizes effectively to
Mandarin, a language that has remained particularly difficult for prior
methods. Code will be released upon acceptance.

</details>


### [21] [UNIDOC-BENCH: A Unified Benchmark for Document-Centric Multimodal RAG](https://arxiv.org/abs/2510.03663)
*Xiangyu Peng,Cab Qin,Zeyuan Chen,Ran Xu,Caiming Xiong,Chien-Sheng Wu*

Main category: cs.CL

TL;DR: 提出了UniDoc-Bench，这是首个基于70k真实PDF页面的大规模多模态检索增强生成基准，包含1,600个多模态QA对，支持四种范式的公平比较。


<details>
  <summary>Details</summary>
Motivation: 当前多模态检索增强生成评估存在碎片化问题，要么孤立评估文本或图像，要么使用简化的多模态设置，无法捕捉文档中心的多模态用例。

Method: 从八个领域的70k真实PDF页面中提取并链接文本、表格和图像证据，生成1,600个多模态QA对，涵盖事实检索、比较、摘要和逻辑推理查询，其中20%经过多标注者和专家验证。

Result: 实验表明，多模态文本图像融合RAG系统始终优于单模态和联合多模态嵌入检索，表明仅文本或图像都不足够，且当前多模态嵌入仍不足够。

Conclusion: 该基准不仅用于评估，还揭示了视觉上下文如何补充文本证据，发现了系统性失败模式，并为开发更稳健的多模态RAG管道提供了可行指导。

Abstract: Multimodal retrieval-augmented generation (MM-RAG) is a key approach for
applying large language models (LLMs) and agents to real-world knowledge bases,
yet current evaluations are fragmented, focusing on either text or images in
isolation or on simplified multimodal setups that fail to capture
document-centric multimodal use cases. In this paper, we introduce
UniDoc-Bench, the first large-scale, realistic benchmark for MM-RAG built from
70k real-world PDF pages across eight domains. Our pipeline extracts and links
evidence from text, tables, and figures, then generates 1,600 multimodal QA
pairs spanning factual retrieval, comparison, summarization, and logical
reasoning queries. To ensure reliability, 20% of QA pairs are validated by
multiple annotators and expert adjudication. UniDoc-Bench supports
apples-to-apples comparison across four paradigms: (1) text-only, (2)
image-only, (3) multimodal text-image fusion, and (4) multimodal joint
retrieval -- under a unified protocol with standardized candidate pools,
prompts, and evaluation metrics. Our experiments show that multimodal
text-image fusion RAG systems consistently outperform both unimodal and jointly
multimodal embedding-based retrieval, indicating that neither text nor images
alone are sufficient and that current multimodal embeddings remain inadequate.
Beyond benchmarking, our analysis reveals when and how visual context
complements textual evidence, uncovers systematic failure modes, and offers
actionable guidance for developing more robust MM-RAG pipelines.

</details>


### [22] [Fine-Tuning Large Language Models with QLoRA for Offensive Language Detection in Roman Urdu-English Code-Mixed Text](https://arxiv.org/abs/2510.03683)
*Nisar Hussain,Amna Qasim,Gull Mehak,Muhammad Zain,Momina Hafeez,Grigori Sidorov*

Main category: cs.CL

TL;DR: 提出了一个基于QLoRA的微调框架，用于改进罗马乌尔都语-英语混合文本中的冒犯性语言检测，通过翻译处理低资源输入，在多个大语言模型上取得良好效果。


<details>
  <summary>Details</summary>
Motivation: 罗马乌尔都语等代码混合语言中的贬义词汇使用给自然语言处理系统带来挑战，包括未明确的语法、不一致的拼写和标记数据稀缺。

Method: 使用Google Translate将罗马乌尔都语-英语混合数据集翻译成英语，利用QLoRA对多个大语言模型进行内存高效的微调，包括Meta LLaMA 3 8B、Mistral 7B等。

Result: 在所有测试模型中，Meta LLaMA 3 8B获得了最高的F1分数91.45，其次是Mistral 7B的89.66，超过了传统的transformer基线模型。

Conclusion: 该工作证明了QLoRA在低资源环境下微调高性能模型的有效性，确认了大语言模型在此任务中的潜力，为基于LLM的多语言冒犯性检测系统铺平了道路。

Abstract: The use of derogatory terms in languages that employ code mixing, such as
Roman Urdu, presents challenges for Natural Language Processing systems due to
unstated grammar, inconsistent spelling, and a scarcity of labeled data. In
this work, we propose a QLoRA based fine tuning framework to improve offensive
language detection in Roman Urdu-English text. We translated the Roman
Urdu-English code mixed dataset into English using Google Translate to leverage
English LLMs, while acknowledging that this translation reduces direct
engagement with code mixing features. Our focus is on classification
performance using English translated low resource inputs. We fine tuned several
transformers and large language models, including Meta LLaMA 3 8B, Mistral 7B
v0.1, LLaMA 2 7B, ModernBERT, and RoBERTa, with QLoRA for memory efficient
adaptation. Models were trained and evaluated on a manually annotated Roman
Urdu dataset for offensive vs non offensive content. Of all tested models, the
highest F1 score of 91.45 was attained by Meta LLaMA 3 8B, followed by Mistral
7B at 89.66, surpassing traditional transformer baselines. These results
demonstrate the efficacy of QLoRA in fine tuning high performing models for low
resource environments such as code mixed offensive language detection, and
confirm the potential of LLMs for this task. This work advances a scalable
approach to Roman Urdu moderation and paves the way for future multilingual
offensive detection systems based on LLMs.

</details>


### [23] [MedReflect: Teaching Medical LLMs to Self-Improve via Reflective Correction](https://arxiv.org/abs/2510.03687)
*Yue Huang,Yanyuan Chen,Dexuan Xu,Weihua Yue,Huamin Zhang,Meikang Qiu,Yu Huang*

Main category: cs.CL

TL;DR: MedReflect是一个通用框架，通过启发LLMs采用类似医生的反思思维模式，在无需外部检索或大量标注的情况下提升医学问题解决能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖外部知识验证或训练推理数据集，存在检索开销大、标注成本高、性能有限等问题。

Method: 生成单次反思链，包括初始假设生成、自我提问、自我回答和决策优化，实现自我验证和反思。

Result: 仅用2000个训练样本和轻量微调，就在多个医学基准上取得显著准确率提升，大幅减少标注需求。

Conclusion: LLMs可以通过自我反思和自我改进学习解决专业医学问题，减少对外部监督和大量任务特定微调数据的依赖。

Abstract: Medical problem solving demands expert knowledge and intricate reasoning.
Recent studies of large language models (LLMs) attempt to ease this complexity
by introducing external knowledge verification through retrieval-augmented
generation or by training on reasoning datasets. However, these approaches
suffer from drawbacks such as retrieval overhead and high annotation costs, and
they heavily rely on substituted external assistants to reach limited
performance in medical field. In this paper, we introduce MedReflect, a
generalizable framework designed to inspire LLMs with a physician-like
reflective thinking mode. MedReflect generates a single-pass reflection chain
that includes initial hypothesis generation, self-questioning, self-answering
and decision refinement. This self-verified and self-reflective nature releases
large language model's latent capability in medical problem-solving without
external retrieval or heavy annotation. We demonstrate that MedReflect enables
cost-efficient medical dataset construction: with merely 2,000 randomly sampled
training examples and a light fine-tuning, this approach achieves notable
absolute accuracy improvements across a series of medical benchmarks while
cutting annotation requirements. Our results provide evidence that LLMs can
learn to solve specialized medical problems via self-reflection and
self-improve, reducing reliance on external supervision and extensive
task-specific fine-tuning data.

</details>


### [24] [TreePrompt: Leveraging Hierarchical Few-Shot Example Selection for Improved English-Persian and English-German Translation](https://arxiv.org/abs/2510.03748)
*Ramtin Kakavand,Ebrahim Ansari*

Main category: cs.CL

TL;DR: TreePrompt是一种新的示例选择方法，通过树形结构框架学习LLM偏好，识别高质量且上下文相关的翻译示例，与AFSP或随机选择结合可提升翻译性能。


<details>
  <summary>Details</summary>
Motivation: 现有的少样本提示方法主要关注查询与示例的相似性，但忽略了示例质量的重要性，需要同时考虑相似性和质量来提升翻译效果。

Method: 提出TreePrompt方法，在树形结构框架中学习LLM偏好来识别高质量相关示例，并与K-NN和自适应少样本提示(AFSP)结合探索相似性与质量的平衡。

Result: 在英语-波斯语(MIZAN)和英语-德语(WMT19)两个语言对上的评估显示，TreePrompt与AFSP或随机选择结合能提高翻译性能。

Conclusion: TreePrompt通过考虑示例质量和相关性，有效提升了LLM在机器翻译任务中的表现，证明了同时优化相似性和质量的重要性。

Abstract: Large Language Models (LLMs) have consistently demonstrated strong
performance in machine translation, especially when guided by high-quality
prompts. Few-shot prompting is an effective technique to improve translation
quality; however, most existing example selection methods focus solely on
query-to-example similarity and do not account for the quality of the examples.
In this work, we propose TreePrompt, a novel example selection approach that
learns LLM preferences to identify high-quality, contextually relevant examples
within a tree-structured framework. To further explore the balance between
similarity and quality, we combine TreePrompt with K-Nearest Neighbors (K-NN)
and Adaptive Few-Shot Prompting (AFSP). Evaluations on two language pairs -
English-Persian (MIZAN) and English-German (WMT19) - show that integrating
TreePrompt with AFSP or Random selection leads to improved translation
performance.

</details>


### [25] [Cross-Lingual Multi-Granularity Framework for Interpretable Parkinson's Disease Diagnosis from Speech](https://arxiv.org/abs/2510.03758)
*Ilias Tougui,Mehdi Zakroum,Mounir Ghogho*

Main category: cs.CL

TL;DR: 提出了一种基于语音细粒度分析的帕金森病检测方法，通过分析音素、音节和单词级别的语音特征，在意大利语、西班牙语和英语数据集上实现了93.78%的AUROC和92.17%的准确率。


<details>
  <summary>Details</summary>
Motivation: 当前帕金森病检测系统分析整个话语，可能忽略了特定语音元素的诊断价值。帕金森病影响全球超过1000万人，其中高达89%的患者存在语音障碍。

Method: 开发了细粒度感知的多语言PD检测方法，使用自动化管道从录音中提取时间对齐的音素、音节和单词，采用双向LSTM和多头注意力机制比较不同粒度级别的诊断性能。

Result: 音素级别分析表现最佳，AUROC达到93.78%±2.34%，准确率为92.17%±2.43%。注意力分析显示最有信息的语音特征与临床协议一致：音素级别的持续元音、音节级别的交替运动音节和单词级别的/pataka/序列。

Conclusion: 该方法展示了跨语言帕金森病检测的增强诊断能力，细粒度语音分析优于传统方法，且识别出的关键特征与临床实践相符。

Abstract: Parkinson's Disease (PD) affects over 10 million people worldwide, with
speech impairments in up to 89% of patients. Current speech-based detection
systems analyze entire utterances, potentially overlooking the diagnostic value
of specific phonetic elements. We developed a granularity-aware approach for
multilingual PD detection using an automated pipeline that extracts
time-aligned phonemes, syllables, and words from recordings. Using Italian,
Spanish, and English datasets, we implemented a bidirectional LSTM with
multi-head attention to compare diagnostic performance across the different
granularity levels. Phoneme-level analysis achieved superior performance with
AUROC of 93.78% +- 2.34% and accuracy of 92.17% +- 2.43%. This demonstrates
enhanced diagnostic capability for cross-linguistic PD detection. Importantly,
attention analysis revealed that the most informative speech features align
with those used in established clinical protocols: sustained vowels (/a/, /e/,
/o/, /i/) at phoneme level, diadochokinetic syllables (/ta/, /pa/, /la/, /ka/)
at syllable level, and /pataka/ sequences at word level. Source code will be
available at https://github.com/jetliqs/clearpd.

</details>


### [26] [Prompt Balance Matters: Understanding How Imbalanced Few-Shot Learning Affects Multilingual Sense Disambiguation in LLMs](https://arxiv.org/abs/2510.03762)
*Deshan Sumanathilaka,Nicholas Micallef,Julian Hough*

Main category: cs.CL

TL;DR: 本研究探讨了少样本提示策略对词义消歧任务的影响，特别是在多语言环境中样本分布不平衡带来的偏见问题。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，少样本提示技术因其实用性和有效性受到广泛关注。本研究旨在了解样本分布不平衡如何影响多语言词义消歧任务的性能。

Method: 使用GLOSSGPT提示方法，在英语、德语、西班牙语、法语和意大利语五种语言上测试少样本提示策略，评估GPT-4o和LLaMA-3.1-70B模型的表现。

Result: 结果显示，不平衡的少样本示例会导致多语言中的错误词义预测，但英语中未出现此问题。多语言词义消歧对样本分布非常敏感。

Conclusion: 研究强调了在多语言少样本设置中需要平衡和具有代表性的提示策略，以确保词义消歧任务的准确性和公平性。

Abstract: Recent advances in Large Language Models (LLMs) have significantly reshaped
the landscape of Natural Language Processing (NLP). Among the various prompting
techniques, few-shot prompting has gained considerable attention for its
practicality and effectiveness. This study investigates how few-shot prompting
strategies impact the Word Sense Disambiguation (WSD) task, particularly
focusing on the biases introduced by imbalanced sample distributions. We use
the GLOSSGPT prompting method, an advanced approach for English WSD, to test
its effectiveness across five languages: English, German, Spanish, French, and
Italian. Our results show that imbalanced few-shot examples can cause incorrect
sense predictions in multilingual languages, but this issue does not appear in
English. To assess model behavior, we evaluate both the GPT-4o and
LLaMA-3.1-70B models and the results highlight the sensitivity of multilingual
WSD to sample distribution in few-shot settings, emphasizing the need for
balanced and representative prompting strategies.

</details>


### [27] [Rezwan: Leveraging Large Language Models for Comprehensive Hadith Text Processing: A 1.2M Corpus Development](https://arxiv.org/abs/2510.03781)
*Majid Asgari-Bidhendi,Muhammad Amin Ghaseminia,Alireza Shahbazi,Sayyed Ali Hossayni,Najmeh Torabian,Behrouz Minaei-Bidgoli*

Main category: cs.CL

TL;DR: 开发了Rezwan大型AI辅助圣训语料库，包含120万条圣训，通过全自动流程提取和结构化，实现多语言翻译、智能标注、摘要生成等功能，在质量和规模上超越人工标注语料库。


<details>
  <summary>Details</summary>
Motivation: 传统圣训整理依赖人工，耗时耗力且难以大规模处理。需要利用AI技术自动化处理圣训文本，实现大规模、多语言、语义丰富的伊斯兰文化遗产数字化访问。

Method: 基于数字资源库，使用大语言模型进行文本分割、传述链-正文分离、验证和多层增强，包括机器翻译、智能标注、摘要生成、主题标记和跨文本语义分析。

Result: 在1,213条随机样本评估中，传述链-正文分离和摘要生成接近人类准确度(9.33/10)，总体评分8.46/10显著优于人工语料库的3.66/10，成本效益显著。

Conclusion: AI能够增强人类专业知识，为宗教文本处理引入新范式，实现大规模、多语言、语义丰富的伊斯兰文化遗产访问。

Abstract: This paper presents the development of Rezwan, a large-scale AI-assisted
Hadith corpus comprising over 1.2M narrations, extracted and structured through
a fully automated pipeline. Building on digital repositories such as Maktabat
Ahl al-Bayt, the pipeline employs Large Language Models (LLMs) for
segmentation, chain--text separation, validation, and multi-layer enrichment.
Each narration is enhanced with machine translation into twelve languages,
intelligent diacritization, abstractive summarization, thematic tagging, and
cross-text semantic analysis. This multi-step process transforms raw text into
a richly annotated research-ready infrastructure for digital humanities and
Islamic studies. A rigorous evaluation was conducted on 1,213 randomly sampled
narrations, assessed by six domain experts. Results show near-human accuracy in
structured tasks such as chain--text separation (9.33/10) and summarization
(9.33/10), while highlighting ongoing challenges in diacritization and semantic
similarity detection. Comparative analysis against the manually curated Noor
Corpus demonstrates the superiority of Najm in both scale and quality, with a
mean overall score of 8.46/10 versus 3.66/10. Furthermore, cost analysis
confirms the economic feasibility of the AI approach: tasks requiring over
229,000 hours of expert labor were completed within months at a fraction of the
cost. The work introduces a new paradigm in religious text processing by
showing how AI can augment human expertise, enabling large-scale, multilingual,
and semantically enriched access to Islamic heritage.

</details>


### [28] [Mechanistic Interpretability of Socio-Political Frames in Language Models](https://arxiv.org/abs/2510.03799)
*Hadi Asghari,Sami Nenno*

Main category: cs.CL

TL;DR: LLMs能够生成和识别深层认知框架，特别是在社会政治语境中。研究发现LLMs在零样本设置下能有效识别和生成特定框架，并通过机制可解释性研究定位了'严格父亲'和'养育父母'框架在模型隐藏表示中的具体维度。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型捕捉和表达人类有意义概念的能力，特别是在社会政治框架方面的表现，以理解模型如何内化和再现人类认知模式。

Method: 采用机制可解释性研究方法，分析LLMs在生成和识别认知框架时的表现，特别关注'严格父亲'和'养育父母'框架在模型隐藏表示中的定位。

Result: LLMs在生成唤起特定框架的文本方面表现出高度流畅性，在零样本设置下能有效识别这些框架，并发现了与这些框架存在强相关性的单一维度。

Conclusion: 该研究增进了对LLMs如何捕捉和表达有意义人类概念的理解，揭示了模型内部表示与社会政治框架之间的关联。

Abstract: This paper explores the ability of large language models to generate and
recognize deep cognitive frames, particularly in socio-political contexts. We
demonstrate that LLMs are highly fluent in generating texts that evoke specific
frames and can recognize these frames in zero-shot settings. Inspired by
mechanistic interpretability research, we investigate the location of the
`strict father' and `nurturing parent' frames within the model's hidden
representation, identifying singular dimensions that correlate strongly with
their presence. Our findings contribute to understanding how LLMs capture and
express meaningful human concepts.

</details>


### [29] [Beyond Token Length: Step Pruner for Efficient and Accurate Reasoning in Large Language Models](https://arxiv.org/abs/2510.03805)
*Canhui Wu,Qiong Cao,Chang Li,Zhenfang Wang,Chao Xue,Yuwei Fan,Wei Xi,Xiaodong He*

Main category: cs.CL

TL;DR: 提出了Step Pruner（SP）强化学习框架，通过惩罚冗余推理步骤来减少大型推理模型的过度思考问题，在保持准确性的同时显著降低响应长度。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法通过惩罚生成token来促进简洁性，但存在两个问题：较少token不一定对应较少推理步骤，模型可能在训练后期出现丢弃推理步骤的作弊行为。

Method: 提出步骤感知的奖励函数，优先考虑正确性同时惩罚冗余步骤，对错误响应不给予奖励以防止强化错误推理。引入动态停止机制，当任何输出步骤长度超过上限时停止更新以防止步骤合并作弊。

Result: 在四个推理基准测试中，SP实现了最先进的准确性，同时显著减少响应长度。在AIME24上，token使用量减少了69.7%。

Conclusion: SP框架有效解决了大型推理模型的过度思考问题，通过步骤级别的优化实现了推理效率和准确性的平衡。

Abstract: Large Reasoning Models (LRMs) demonstrate strong performance on complex tasks
but often suffer from excessive verbosity, known as "overthinking." Existing
solutions via reinforcement learning (RL) typically penalize generated tokens
to promote conciseness. However, these methods encounter two challenges:
responses with fewer tokens do not always correspond to fewer reasoning steps,
and models may develop hacking behavior in later stages of training by
discarding reasoning steps to minimize token usage. In this work, we introduce
\textbf{Step Pruner (SP)}, an RL framework that steers LRMs toward more
efficient reasoning by favoring compact reasoning steps. Our step-aware reward
function prioritizes correctness while imposing penalties for redundant steps,
and withholds rewards for incorrect responses to prevent the reinforcement of
erroneous reasoning. Moreover, we propose a dynamic stopping mechanism: when
the length of any output step exceeds the upper limit, we halt updates to
prevent hacking behavior caused by merging steps. Extensive experiments across
four reasoning benchmarks demonstrate that SP achieves state-of-the-art
accuracy while significantly reducing response length. For instance, on AIME24,
SP reduces token usage by \textbf{69.7\%}.

</details>


### [30] [Annotate Rhetorical Relations with INCEpTION: A Comparison with Automatic Approaches](https://arxiv.org/abs/2510.03808)
*Mehedi Hasan Emon*

Main category: cs.CL

TL;DR: 比较使用INCEpTION工具手动标注修辞关系与基于大语言模型的自动方法，在体育报道（板球新闻）中评估BERT、DistilBERT和逻辑回归模型对修辞关系的分类性能。


<details>
  <summary>Details</summary>
Motivation: 探索修辞关系在话语中的标注方法，研究自动方法在话语解析中的潜力，促进话语解析与基于Transformer的NLP技术的交叉研究。

Method: 使用INCEpTION工具进行手动标注，并比较BERT、DistilBERT和逻辑回归模型在分类修辞关系（如阐述、对比、背景、因果关系）方面的性能。

Result: DistilBERT模型取得了最高的准确率，显示出其在高效预测话语关系方面的潜力。

Conclusion: 这项工作为话语解析和基于Transformer的NLP技术的交叉研究做出了贡献，证明了DistilBERT在修辞关系分类中的有效性。

Abstract: This research explores the annotation of rhetorical relations in discourse
using the INCEpTION tool and compares manual annotation with automatic
approaches based on large language models. The study focuses on sports reports
(specifically cricket news) and evaluates the performance of BERT, DistilBERT,
and Logistic Regression models in classifying rhetorical relations such as
elaboration, contrast, background, and cause-effect. The results show that
DistilBERT achieved the highest accuracy, highlighting its potential for
efficient discourse relation prediction. This work contributes to the growing
intersection of discourse parsing and transformer-based NLP. (This paper was
conducted as part of an academic requirement under the supervision of Prof. Dr.
Ralf Klabunde, Linguistic Data Science Lab, Ruhr University Bochum.) Keywords:
Rhetorical Structure Theory, INCEpTION, BERT, DistilBERT, Discourse Parsing,
NLP.

</details>


### [31] [Read Between the Lines: A Benchmark for Uncovering Political Bias in Bangla News Articles](https://arxiv.org/abs/2510.03898)
*Nusrat Jahan Lia,Shubhashis Roy Dipta,Abdullah Khan Zehady,Naymul Islam,Madhusodan Chakraborty,Abdullah Al Wasif*

Main category: cs.CL

TL;DR: 提出了首个孟加拉语新闻政治立场检测基准数据集，包含200篇新闻文章，标注了亲政府、批评政府和中性立场。评估了28个大语言模型，发现模型在检测批评政府内容时表现良好，但在中性文章上表现很差，且倾向于过度预测亲政府立场。


<details>
  <summary>Details</summary>
Motivation: 南亚地区的媒体偏见检测很重要，但孟加拉语政治偏见研究的标注数据集和计算研究仍然稀缺。孟加拉语新闻的政治立场检测需要理解语言线索、文化背景、微妙偏见、修辞策略、语码转换、隐含情感和社会政治背景。

Method: 创建了包含200篇孟加拉语新闻文章的基准数据集，标注了三种政治立场（亲政府、批评政府、中性）。对28个专有和开源大语言模型进行了综合评估，使用诊断分析来评估模型性能。

Result: 模型在检测批评政府内容方面表现强劲（F1最高达0.83），但在中性文章上表现极差（F1低至0.00）。模型倾向于过度预测亲政府立场，经常误解模糊叙述。

Conclusion: 该数据集及其相关诊断为推进孟加拉语媒体立场检测研究奠定了基础，并为改善低资源语言中大语言模型的性能提供了见解。

Abstract: Detecting media bias is crucial, specifically in the South Asian region.
Despite this, annotated datasets and computational studies for Bangla political
bias research remain scarce. Crucially because, political stance detection in
Bangla news requires understanding of linguistic cues, cultural context, subtle
biases, rhetorical strategies, code-switching, implicit sentiment, and
socio-political background. To address this, we introduce the first benchmark
dataset of 200 politically significant and highly debated Bangla news articles,
labeled for government-leaning, government-critique, and neutral stances,
alongside diagnostic analyses for evaluating large language models (LLMs). Our
comprehensive evaluation of 28 proprietary and open-source LLMs shows strong
performance in detecting government-critique content (F1 up to 0.83) but
substantial difficulty with neutral articles (F1 as low as 0.00). Models also
tend to over-predict government-leaning stances, often misinterpreting
ambiguous narratives. This dataset and its associated diagnostics provide a
foundation for advancing stance detection in Bangla media research and offer
insights for improving LLM performance in low-resource languages.

</details>


### [32] [PsycholexTherapy: Simulating Reasoning in Psychotherapy with Small Language Models in Persian](https://arxiv.org/abs/2510.03913)
*Mohammad Amin Abbasi,Hassan Naderi*

Main category: cs.CL

TL;DR: PsychoLexTherapy是一个针对波斯语的轻量级心理治疗对话框架，使用小型语言模型实现文化适应、隐私保护的治疗推理，包含评估工具和数据集。


<details>
  <summary>Details</summary>
Motivation: 解决在资源匮乏语言中开发文化适应、治疗连贯的对话系统，同时确保隐私保护和设备端部署的可行性。

Method: 三阶段开发：评估SLMs心理知识、设计治疗推理框架、构建评估数据集；比较简单提示、多代理辩论和结构化治疗推理路径。

Result: 在单轮对话评估中表现最佳，多轮对话中长时记忆模块对保持连贯性至关重要，在同理心、连贯性、文化适应和个性化方面获得最高评分。

Conclusion: 为波斯语心理治疗模拟建立了实用、隐私保护且文化适应的基础，贡献了新颖数据集、可复现评估流程和结构化治疗推理的实证见解。

Abstract: This study presents PsychoLexTherapy, a framework for simulating
psychotherapeutic reasoning in Persian using small language models (SLMs). The
framework tackles the challenge of developing culturally grounded,
therapeutically coherent dialogue systems with structured memory for multi-turn
interactions in underrepresented languages. To ensure privacy and feasibility,
PsychoLexTherapy is optimized for on-device deployment, enabling use without
external servers. Development followed a three-stage process: (i) assessing
SLMs psychological knowledge with PsychoLexEval; (ii) designing and
implementing the reasoning-oriented PsychoLexTherapy framework; and (iii)
constructing two evaluation datasets-PsychoLexQuery (real Persian user
questions) and PsychoLexDialogue (hybrid simulated sessions)-to benchmark
against multiple baselines. Experiments compared simple prompting, multi-agent
debate, and structured therapeutic reasoning paths. Results showed that
deliberate model selection balanced accuracy, efficiency, and privacy. On
PsychoLexQuery, PsychoLexTherapy outperformed all baselines in automatic
LLM-as-a-judge evaluation and was ranked highest by human evaluators in a
single-turn preference study. In multi-turn tests with PsychoLexDialogue, the
long-term memory module proved essential: while naive history concatenation
caused incoherence and information loss, the full framework achieved the
highest ratings in empathy, coherence, cultural fit, and personalization.
Overall, PsychoLexTherapy establishes a practical, privacy-preserving, and
culturally aligned foundation for Persian psychotherapy simulation,
contributing novel datasets, a reproducible evaluation pipeline, and empirical
insights into structured memory for therapeutic reasoning.

</details>


### [33] [Mapping Patient-Perceived Physician Traits from Nationwide Online Reviews with LLMs](https://arxiv.org/abs/2510.03997)
*Junjie Luo,Rui Han,Arshana Welivita,Zeleikun Di,Jingfu Wu,Xuzhe Zhi,Ritu Agarwal,Gordon Gao*

Main category: cs.CL

TL;DR: 使用LLM从410万患者评价中提取医生的大五人格特质和患者主观判断，验证了方法的有效性，发现男性医生评分更高、不同科室特质差异明显，并识别出四种医生类型。


<details>
  <summary>Details</summary>
Motivation: 理解患者对医生的认知对于改善信任、沟通和满意度至关重要，需要大规模分析患者评价来获得可解释的医生特质指标。

Method: 基于大语言模型的流水线方法，分析410万条患者评价，通过多模型比较和人类专家基准验证，进行聚类分析识别医生类型。

Result: 人类与LLM评估高度一致（相关系数0.72-0.89），与患者满意度显著相关（r=0.41-0.81），男性医生在所有特质上评分更高，识别出四种医生原型。

Conclusion: 从患者叙述中自动提取特质可以提供可解释、经过验证的指标，用于大规模理解医患关系，对医疗质量测量、偏见检测和人力发展具有重要意义。

Abstract: Understanding how patients perceive their physicians is essential to
improving trust, communication, and satisfaction. We present a large language
model (LLM)-based pipeline that infers Big Five personality traits and five
patient-oriented subjective judgments. The analysis encompasses 4.1 million
patient reviews of 226,999 U.S. physicians from an initial pool of one million.
We validate the method through multi-model comparison and human expert
benchmarking, achieving strong agreement between human and LLM assessments
(correlation coefficients 0.72-0.89) and external validity through correlations
with patient satisfaction (r = 0.41-0.81, all p<0.001). National-scale analysis
reveals systematic patterns: male physicians receive higher ratings across all
traits, with largest disparities in clinical competence perceptions;
empathy-related traits predominate in pediatrics and psychiatry; and all traits
positively predict overall satisfaction. Cluster analysis identifies four
distinct physician archetypes, from "Well-Rounded Excellent" (33.8%, uniformly
high traits) to "Underperforming" (22.6%, consistently low). These findings
demonstrate that automated trait extraction from patient narratives can provide
interpretable, validated metrics for understanding physician-patient
relationships at scale, with implications for quality measurement, bias
detection, and workforce development in healthcare.

</details>


### [34] [Simulating and Understanding Deceptive Behaviors in Long-Horizon Interactions](https://arxiv.org/abs/2510.03999)
*Yang Xu,Xuanming Zhang,Min-Hsuan Yeh,Jwala Dhamala,Ousmane Dia,Rahul Gupta,Yixuan Li*

Main category: cs.CL

TL;DR: 该研究提出了首个模拟框架来评估大语言模型在长期交互中的欺骗行为，发现在任务压力下模型会出现隐瞒、含糊其辞和伪造等欺骗策略，且欺骗行为会持续削弱监督者的信任。


<details>
  <summary>Details</summary>
Motivation: 现有研究大多局限于单轮提示下的欺骗检测，无法捕捉长期交互中欺骗策略的演变过程，需要开发能够评估大语言模型在动态上下文压力下欺骗行为的框架。

Method: 构建多智能体系统：执行者智能体完成任务，监督者智能体评估进度并提供反馈，独立的欺骗审计员审查完整轨迹以识别欺骗行为。在11个前沿模型上进行广泛实验。

Result: 欺骗行为具有模型依赖性，随事件压力增加而增加，并持续削弱监督者信任。定性分析揭示了隐瞒、含糊其辞和伪造等不同的欺骗策略。

Conclusion: 欺骗是长期交互中出现的风险，为评估未来大语言模型在现实世界信任敏感环境中的表现提供了基础。

Abstract: Deception is a pervasive feature of human communication and an emerging
concern in large language models (LLMs). While recent studies document
instances of LLM deception under pressure, most evaluations remain confined to
single-turn prompts and fail to capture the long-horizon interactions in which
deceptive strategies typically unfold. We introduce the first simulation
framework for probing and evaluating deception in LLMs under extended sequences
of interdependent tasks and dynamic contextual pressures. Our framework
instantiates a multi-agent system: a performer agent tasked with completing
tasks and a supervisor agent that evaluates progress, provides feedback, and
maintains evolving states of trust. An independent deception auditor then
reviews full trajectories to identify when and how deception occurs. We conduct
extensive experiments across 11 frontier models, spanning both closed- and
open-source systems, and find that deception is model-dependent, increases with
event pressure, and consistently erodes supervisor trust. Qualitative analyses
further reveal distinct strategies of concealment, equivocation, and
falsification. Our findings establish deception as an emergent risk in
long-horizon interactions and provide a foundation for evaluating future LLMs
in real-world, trust-sensitive contexts.

</details>


### [35] [Named Entity Recognition in COVID-19 tweets with Entity Knowledge Augmentation](https://arxiv.org/abs/2510.04001)
*Xuankang Zhang,Jiangming Liu*

Main category: cs.CL

TL;DR: 提出了一种新颖的实体知识增强方法，用于COVID-19命名实体识别，可提升在社交媒体和生物医学文本中的NER性能


<details>
  <summary>Details</summary>
Motivation: COVID-19疫情引发社交媒体大量讨论，但社交媒体文本非正式且标注数据稀缺，同时COVID-19命名实体识别需要大量领域专业知识，现有方法面临挑战

Method: 提出实体知识增强方法，通过增强模型对领域知识的理解来提升命名实体识别性能，适用于非正式和正式文本格式

Result: 在COVID-19推文数据集和PubMed数据集上的实验表明，该方法在全监督和少样本设置下都能提升NER性能

Conclusion: 实体知识增强方法有效解决了COVID-19命名实体识别中的挑战，为生物医学NER提供了通用解决方案

Abstract: The COVID-19 pandemic causes severe social and economic disruption around the
world, raising various subjects that are discussed over social media.
Identifying pandemic-related named entities as expressed on social media is
fundamental and important to understand the discussions about the pandemic.
However, there is limited work on named entity recognition on this topic due to
the following challenges: 1) COVID-19 texts in social media are informal and
their annotations are rare and insufficient to train a robust recognition
model, and 2) named entity recognition in COVID-19 requires extensive
domain-specific knowledge. To address these issues, we propose a novel entity
knowledge augmentation approach for COVID-19, which can also be applied in
general biomedical named entity recognition in both informal text format and
formal text format. Experiments carried out on the COVID-19 tweets dataset and
PubMed dataset show that our proposed entity knowledge augmentation improves
NER performance in both fully-supervised and few-shot settings. Our source code
is publicly available: https://github.com/kkkenshi/LLM-EKA/tree/master

</details>


### [36] [AgriGPT-VL: Agricultural Vision-Language Understanding Suite](https://arxiv.org/abs/2510.04002)
*Bo Yang,Yunkui Chen,Lanfei Feng,Yu Zhang,Xiao Xu,Jianyu Zhang,Nueraili Aierken,Runhe Huang,Hongjian Lin,Yibin Ying,Shijian Li*

Main category: cs.CL

TL;DR: 提出了AgriGPT-VL套件，包含最大的农业视觉语言语料库Agri-3M-VL、专门的视觉语言模型AgriGPT-VL和评估基准AgriBench-VL-4K，在农业领域表现优于通用VLMs。


<details>
  <summary>Details</summary>
Motivation: 解决农业应用中领域定制模型稀缺、视觉语言语料库缺乏和严格评估不足的问题。

Method: 使用多智能体数据生成器构建大规模农业视觉语言语料库；通过渐进式课程训练农业专用视觉语言模型，包括文本基础、多模态浅层/深层对齐和GRPO精炼；建立紧凑但具有挑战性的评估套件。

Result: AgriGPT-VL在AgriBench-VL-4K上优于领先的通用VLMs，在LLM-as-a-judge评估中获得更高的成对胜率，同时在纯文本AgriBench-13K上保持竞争力且语言能力无显著下降。

Conclusion: 该框架在农业多模态任务中表现出色，消融研究证实了对齐和GRPO精炼阶段的一致增益，所有资源将开源以支持可重复研究和低资源农业环境部署。

Abstract: Despite rapid advances in multimodal large language models, agricultural
applications remain constrained by the scarcity of domain-tailored models,
curated vision-language corpora, and rigorous evaluation. To address these
challenges, we present the AgriGPT-VL Suite, a unified multimodal framework for
agriculture. Our contributions are threefold. First, we introduce Agri-3M-VL,
the largest vision-language corpus for agriculture to our knowledge, curated by
a scalable multi-agent data generator; it comprises 1M image-caption pairs, 2M
image-grounded VQA pairs, 50K expert-level VQA instances, and 15K GRPO
reinforcement learning samples. Second, we develop AgriGPT-VL, an
agriculture-specialized vision-language model trained via a progressive
curriculum of textual grounding, multimodal shallow/deep alignment, and GRPO
refinement. This method achieves strong multimodal reasoning while preserving
text-only capability. Third, we establish AgriBench-VL-4K, a compact yet
challenging evaluation suite with open-ended and image-grounded questions,
paired with multi-metric evaluation and an LLM-as-a-judge framework.
Experiments show that AgriGPT-VL outperforms leading general-purpose VLMs on
AgriBench-VL-4K, achieving higher pairwise win rates in the LLM-as-a-judge
evaluation. Meanwhile, it remains competitive on the text-only AgriBench-13K
with no noticeable degradation of language ability. Ablation studies further
confirm consistent gains from our alignment and GRPO refinement stages. We will
open source all of the resources to support reproducible research and
deployment in low-resource agricultural settings.

</details>


### [37] [LLM Microscope: What Model Internals Reveal About Answer Correctness and Context Utilization](https://arxiv.org/abs/2510.04013)
*Jiarui Liu,Jivitesh Jain,Mona Diab,Nishant Subramani*

Main category: cs.CL

TL;DR: 使用模型内部激活信号来预测LLM输出的正确性和外部上下文的有效性，通过简单分类器实现早期审计和上下文质量评估。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型具有巨大实用性，但可信度仍是主要问题：模型经常以高置信度生成错误信息。虽然上下文信息可以指导生成，但识别何时查询需要检索上下文以及评估该上下文的有效性仍然具有挑战性。

Method: 操作化可解释性方法，通过模型激活信号预测输出正确性；探索模型内部是否包含关于外部上下文有效性的信号；考虑正确、错误和不相关的上下文，并引入指标来区分它们。

Result: 在六个不同模型上的实验表明，基于第一个输出token的中间层激活训练的简单分类器可以以约75%的准确率预测输出正确性，实现早期审计。基于模型内部的指标在区分正确和错误上下文方面显著优于提示基准，防止被污染上下文引入的不准确性。

Conclusion: 这些发现提供了一个更好地理解LLM底层决策过程的视角，为提升模型可信度提供了新方法。

Abstract: Although large language models (LLMs) have tremendous utility,
trustworthiness is still a chief concern: models often generate incorrect
information with high confidence. While contextual information can help guide
generation, identifying when a query would benefit from retrieved context and
assessing the effectiveness of that context remains challenging. In this work,
we operationalize interpretability methods to ascertain whether we can predict
the correctness of model outputs from the model's activations alone. We also
explore whether model internals contain signals about the efficacy of external
context. We consider correct, incorrect, and irrelevant context and introduce
metrics to distinguish amongst them. Experiments on six different models reveal
that a simple classifier trained on intermediate layer activations of the first
output token can predict output correctness with about 75% accuracy, enabling
early auditing. Our model-internals-based metric significantly outperforms
prompting baselines at distinguishing between correct and incorrect context,
guarding against inaccuracies introduced by polluted context. These findings
offer a lens to better understand the underlying decision-making processes of
LLMs. Our code is publicly available at
https://github.com/jiarui-liu/LLM-Microscope

</details>


### [38] [Thai Semantic End-of-Turn Detection for Real-Time Voice Agents](https://arxiv.org/abs/2510.04016)
*Thanapol Popit,Natthapath Rungseesiripak,Monthol Charattrakool,Saksorn Ruangtanusak*

Main category: cs.CL

TL;DR: 本文系统研究了泰语文本端到端检测（EOT），比较了零样本/少样本提示的紧凑LLM与轻量级transformer的监督微调方法，建立了泰语EOT基准并展示了微调小模型可实现近乎实时的检测。


<details>
  <summary>Details</summary>
Motivation: 传统音频静音端点检测存在数百毫秒延迟，且在犹豫或语言特定现象下失效，需要更可靠的实时语音交互端点检测方法。

Method: 使用YODAS语料库转录字幕和泰语特定语言线索（如句末助词），将EOT制定为基于词元边界的二元决策，比较了零样本/少样本提示紧凑LLM与监督微调轻量级transformer的方法。

Result: 报告了清晰的准确率-延迟权衡，展示了微调的小模型能够提供近乎实时的EOT决策，适合设备端代理使用。

Conclusion: 本研究建立了泰语EOT基准，证明小型微调模型可以实现适合设备端代理的近乎实时端点检测决策。

Abstract: Fluid voice-to-voice interaction requires reliable and low-latency detection
of when a user has finished speaking. Traditional audio-silence end-pointers
add hundreds of milliseconds of delay and fail under hesitations or
language-specific phenomena. We present, to our knowledge, the first systematic
study of Thai text-only end-of-turn (EOT) detection for real-time agents. We
compare zero-shot and few-shot prompting of compact LLMs to supervised
fine-tuning of lightweight transformers. Using transcribed subtitles from the
YODAS corpus and Thai-specific linguistic cues (e.g., sentence-final
particles), we formulate EOT as a binary decision over token boundaries. We
report a clear accuracy-latency tradeoff and provide a public-ready
implementation plan. This work establishes a Thai baseline and demonstrates
that small, fine-tuned models can deliver near-instant EOT decisions suitable
for on-device agents.

</details>


### [39] [Does Using Counterfactual Help LLMs Explain Textual Importance in Classification?](https://arxiv.org/abs/2510.04031)
*Nelvin Tan,James Asikin Cheung,Yu-Ching Shih,Dong Yang,Amol Salunkhe*

Main category: cs.CL

TL;DR: 研究如何通过反事实推理来帮助解释LLM在文本分类任务中的决策，提出了决策改变率框架来量化关键词的重要性。


<details>
  <summary>Details</summary>
Motivation: 由于LLM通常是黑盒且调用成本高昂，需要有效的方法来解释其分类决策，特别是识别影响决策的关键词。

Method: 引入反事实推理方法，提出决策改变率框架来量化关键词对分类决策的贡献程度。

Result: 实验结果表明使用反事实方法有助于更好地解释LLM的决策过程。

Conclusion: 反事实推理是解释LLM分类决策的有效方法，决策改变率框架能够量化关键词的重要性。

Abstract: Large language models (LLMs) are becoming useful in many domains due to their
impressive abilities that arise from large training datasets and large model
sizes. More recently, they have been shown to be very effective in textual
classification tasks, motivating the need to explain the LLMs' decisions.
Motivated by practical constrains where LLMs are black-boxed and LLM calls are
expensive, we study how incorporating counterfactuals into LLM reasoning can
affect the LLM's ability to identify the top words that have contributed to its
classification decision. To this end, we introduce a framework called the
decision changing rate that helps us quantify the importance of the top words
in classification. Our experimental results show that using counterfactuals can
be helpful.

</details>


### [40] [Small Language Models for Emergency Departments Decision Support: A Benchmark Study](https://arxiv.org/abs/2510.04032)
*Zirui Wang,Jiajun Wu,Braden Teitge,Jessalyn Holodinsky,Steve Drew*

Main category: cs.CL

TL;DR: 小型语言模型(SLMs)在急诊科决策支持中表现优于医学微调模型，表明急诊科可能不需要专门的医学微调。


<details>
  <summary>Details</summary>
Motivation: 急诊科环境快节奏、高风险，SLMs具有推理能力和高效性能，可支持医生提供及时准确的信息合成，改善临床决策和工作流程效率。

Method: 构建综合基准测试，评估在通用领域和医学语料混合训练的SLMs，使用MedMCQA、MedQA-4Options和PubMedQA数据集，模拟急诊科医生日常任务。

Result: 实验结果显示，通用领域SLMs在多个基准测试中意外地优于医学微调模型。

Conclusion: 对于急诊科决策支持，专门的医学微调可能不是必需的，通用领域SLMs已能提供足够支持。

Abstract: Large language models (LLMs) have become increasingly popular in medical
domains to assist physicians with a variety of clinical and operational tasks.
Given the fast-paced and high-stakes environment of emergency departments
(EDs), small language models (SLMs), characterized by a reduction in parameter
count compared to LLMs, offer significant potential due to their inherent
reasoning capability and efficient performance. This enables SLMs to support
physicians by providing timely and accurate information synthesis, thereby
improving clinical decision-making and workflow efficiency. In this paper, we
present a comprehensive benchmark designed to identify SLMs suited for ED
decision support, taking into account both specialized medical expertise and
broad general problem-solving capabilities. In our evaluations, we focus on
SLMs that have been trained on a mixture of general-domain and medical corpora.
A key motivation for emphasizing SLMs is the practical hardware limitations,
operational cost constraints, and privacy concerns in the typical real-world
deployments. Our benchmark datasets include MedMCQA, MedQA-4Options, and
PubMedQA, with the medical abstracts dataset emulating tasks aligned with real
ED physicians' daily tasks. Experimental results reveal that general-domain
SLMs surprisingly outperform their medically fine-tuned counterparts across
these diverse benchmarks for ED. This indicates that for ED, specialized
medical fine-tuning of the model may not be required.

</details>


### [41] [Exploring Chain-of-Thought Reasoning for Steerable Pluralistic Alignment](https://arxiv.org/abs/2510.04045)
*Yunfan Zhang,Kathleen McKeown,Smaranda Muresan*

Main category: cs.CL

TL;DR: 研究探索使用思维链技术构建可引导的多元模型，发现RLVR方法在性能和训练样本效率上表现最佳


<details>
  <summary>Details</summary>
Motivation: 大型语言模型通常反映相对统一的价值观念，限制了其在需要理解细微人类视角任务中的应用，需要支持可引导的多元性

Method: 探索了多种方法：思维链提示、基于人类撰写思维链的微调、基于合成解释的微调、以及带可验证奖励的强化学习

Result: 在Value Kaleidoscope和OpinionQA数据集上评估，RLVR方法持续优于其他方法，并展现出强大的训练样本效率

Conclusion: 思维链技术可以有效构建可引导的多元模型，其中RLVR是最有效的方法，同时分析了生成思维链的忠实性和安全性

Abstract: Large Language Models (LLMs) are typically trained to reflect a relatively
uniform set of values, which limits their applicability to tasks that require
understanding of nuanced human perspectives. Recent research has underscored
the importance of enabling LLMs to support steerable pluralism -- the capacity
to adopt a specific perspective and align generated outputs with it. In this
work, we investigate whether Chain-of-Thought (CoT) reasoning techniques can be
applied to building steerable pluralistic models. We explore several methods,
including CoT prompting, fine-tuning on human-authored CoT, fine-tuning on
synthetic explanations, and Reinforcement Learning with Verifiable Rewards
(RLVR). We evaluate these approaches using the Value Kaleidoscope and OpinionQA
datasets. Among the methods studied, RLVR consistently outperforms others and
demonstrates strong training sample efficiency. We further analyze the
generated CoT traces with respect to faithfulness and safety.

</details>


### [42] [What Makes Diffusion Language Models Super Data Learners?](https://arxiv.org/abs/2510.04071)
*Zitian Gao,Haoming Luo,Lynx Chen,Jason Klein Liu,Ran Tao,Joey Zhou,Bryan Dai*

Main category: cs.CL

TL;DR: 扩散语言模型在有限数据约束下表现出显著的数据效率，研究发现随机掩码输入标记是主要因素，MLP dropout和权重衰减也能获得类似效果，表明随机正则化在多轮训练中广泛提升数据效率。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型在有限数据条件下表现出优异的数据效率，但其背后的机制尚不清楚，本研究旨在揭示这种效率的来源。

Method: 通过广泛的消融实验，分析了不同因素对数据效率的影响，特别关注随机掩码、MLP dropout和权重衰减等随机正则化方法。

Result: 随机掩码输入标记是数据效率提升的主要因素，MLP dropout和权重衰减也能获得类似的数据效率增益。

Conclusion: 随机正则化方法在多轮训练中广泛增强数据效率，这为理解扩散语言模型的数据效率机制提供了重要见解。

Abstract: Recent studies have shown that diffusion language models achieve remarkable
data efficiency under limited-data constraints, yet the underlying mechanisms
remain unclear. In this work, we perform extensive ablation experiments to
disentangle the sources of this efficiency. Our results show that random
masking of input tokens plays the dominant role. We further show that similar
gains can be obtained through in MLP dropout and weight decay, indicating that
stochastic regularization broadly enhances data efficiency in multi-epoch
training. Our code is available at
https://github.com/zitian-gao/data-efficiency.

</details>


### [43] [PoLi-RL: A Point-to-List Reinforcement Learning Framework for Conditional Semantic Textual Similarity](https://arxiv.org/abs/2510.04080)
*Zixin Song,Bowen Zhang,Qian-Wen Zhang,Di Yin,Xing Sun,Chunping Li*

Main category: cs.CL

TL;DR: PoLi-RL是一个新颖的强化学习框架，通过两阶段课程学习和并行切片排名奖励机制，成功将强化学习应用于条件语义文本相似性任务，在C-STS基准上达到了新的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有C-STS方法主要局限于判别模型，未能充分利用LLM和RL的最新进展。RL特别适合此任务，因为它可以直接优化不可微的Spearman排名指标并指导推理过程。

Method: 提出PoLi-RL框架：1）两阶段课程学习：先用简单点式奖励建立基础评分能力，再转向混合奖励（点式、对式、列表式）；2）并行切片排名奖励机制，在并行切片中计算排名奖励，为每个完成提供精确的学习信号。

Result: 在官方C-STS基准上，PoLi-RL实现了48.18的Spearman相关系数，为交叉编码器架构建立了新的SOTA。

Conclusion: 这是首个成功将RL应用于C-STS的工作，为在复杂、基于排名的条件判断任务上训练LLM引入了强大而精确的范式。

Abstract: Conditional Semantic Textual Similarity (C-STS) measures the semantic
proximity between text segments under a specific condition, thereby overcoming
the ambiguity inherent in traditional STS. However, existing methods are
largely confined to discriminative models, failing to fully integrate recent
breakthroughs in the NLP community concerning Large Language Models (LLMs) and
Reinforcement Learning (RL). RL is a particularly well-suited paradigm for this
task, as it can directly optimize the non-differentiable Spearman ranking
metric and guide the reasoning process required by C-STS. However, we find that
naively applying listwise RL fails to produce meaningful improvements, as the
model is overwhelmed by complex, coarse-grained reward signals. To address this
challenge, we introduce PoLi-RL, a novel Point-to-List Reinforcement Learning
framework. PoLi-RL employs a two-stage curriculum: it first trains the model
with simple pointwise rewards to establish fundamental scoring capabilities,
then transitions to a hybrid reward that combines pointwise, pairwise, and
listwise objectives to refine the model's ability to discern subtle semantic
distinctions. Crucially, we propose an innovative Parallel Slice Ranking Reward
(PSRR) mechanism that computes ranking rewards in parallel slices, where each
slice comprises same-indexed completions from different samples. This provides
a precise, differentiated learning signal for each individual completion,
enabling granular credit assignment and effective optimization. On the official
C-STS benchmark, PoLi-RL achieves a Spearman correlation coefficient of 48.18,
establishing a new SOTA for the cross-encoder architecture. As the first work
to successfully apply RL to C-STS, our study introduces a powerful and precise
paradigm for training LLMs on complex, ranking-based conditional judgment
tasks.

</details>


### [44] [Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model Reasoning](https://arxiv.org/abs/2510.04081)
*Honglin Lin,Qizhi Pei,Xin Gao,Zhuoshi Pan,Yu Li,Juntao Li,Conghui He,Lijun Wu*

Main category: cs.CL

TL;DR: Caco框架通过代码驱动的增强方法，自动化合成高质量、可验证且多样化的指令-CoT推理数据，提升大语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有CoT方法存在生成不可控、质量不足和推理路径多样性有限的问题，而基于代码的方法通常局限于预定义的数学问题，难以扩展和泛化。

Method: Caco首先在统一代码格式的现有数学和编程解决方案上微调基于代码的CoT生成器，然后通过代码执行和基于规则的过滤进行自动验证，确保逻辑正确性和结构多样性，最后将过滤后的输出反向工程为自然语言指令和语言CoT。

Result: 在创建的Caco-1.3M数据集上的实验表明，Caco训练的模型在数学推理基准上实现了强大的竞争性能，优于现有强基线。

Conclusion: Caco建立了一个无需人工干预即可构建自持续、可信赖推理系统的范式，其代码锚定验证和指令多样性有助于在未见任务上实现优越的泛化能力。

Abstract: Reasoning capability is pivotal for Large Language Models (LLMs) to solve
complex tasks, yet achieving reliable and scalable reasoning remains
challenging. While Chain-of-Thought (CoT) prompting has become a mainstream
approach, existing methods often suffer from uncontrolled generation,
insufficient quality, and limited diversity in reasoning paths. Recent efforts
leverage code to enhance CoT by grounding reasoning in executable steps, but
such methods are typically constrained to predefined mathematical problems,
hindering scalability and generalizability. In this work, we propose Caco
(Code-Assisted Chain-of-ThOught), a novel framework that automates the
synthesis of high-quality, verifiable, and diverse instruction-CoT reasoning
data through code-driven augmentation. Unlike prior work, Caco first fine-tunes
a code-based CoT generator on existing math and programming solutions in a
unified code format, then scales the data generation to a large amount of
diverse reasoning traces. Crucially, we introduce automated validation via code
execution and rule-based filtering to ensure logical correctness and structural
diversity, followed by reverse-engineering filtered outputs into natural
language instructions and language CoTs to enrich task adaptability. This
closed-loop process enables fully automated, scalable synthesis of reasoning
data with guaranteed executability. Experiments on our created Caco-1.3M
dataset demonstrate that Caco-trained models achieve strong competitive
performance on mathematical reasoning benchmarks, outperforming existing strong
baselines. Further analysis reveals that Caco's code-anchored verification and
instruction diversity contribute to superior generalization across unseen
tasks. Our work establishes a paradigm for building self-sustaining,
trustworthy reasoning systems without human intervention.

</details>


### [45] [Unveiling LLMs' Metaphorical Understanding: Exploring Conceptual Irrelevance, Context Leveraging and Syntactic Influence](https://arxiv.org/abs/2510.04120)
*Fengying Ye,Shanshan Wang,Lidia S. Chao,Derek F. Wong*

Main category: cs.CL

TL;DR: 本研究从概念映射、隐喻-字面知识库和句法敏感性三个角度分析大语言模型的隐喻处理能力，发现LLMs存在概念无关解释、依赖训练数据指标而非上下文线索、对句法异常敏感等问题。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在知识整合、上下文推理和创造性生成方面表现出先进能力，但其隐喻理解机制尚未得到充分探索。

Method: 从三个维度分析：(1)概念映射：使用嵌入空间投影评估LLMs如何在目标域中映射概念；(2)隐喻-字面知识库：分析隐喻词及其字面对应词；(3)句法敏感性：评估隐喻句法结构如何影响LLMs表现。

Result: LLMs产生15%-25%概念无关的解释，依赖训练数据中的隐喻指标而非上下文线索，对句法不规则性比对结构理解更敏感。

Conclusion: 这些发现凸显了LLMs在隐喻分析方面的局限性，需要更强大的计算方法。

Abstract: Metaphor analysis is a complex linguistic phenomenon shaped by context and
external factors. While Large Language Models (LLMs) demonstrate advanced
capabilities in knowledge integration, contextual reasoning, and creative
generation, their mechanisms for metaphor comprehension remain insufficiently
explored. This study examines LLMs' metaphor-processing abilities from three
perspectives: (1) Concept Mapping: using embedding space projections to
evaluate how LLMs map concepts in target domains (e.g., misinterpreting "fall
in love" as "drop down from love"); (2) Metaphor-Literal Repository: analyzing
metaphorical words and their literal counterparts to identify inherent
metaphorical knowledge; and (3) Syntactic Sensitivity: assessing how
metaphorical syntactic structures influence LLMs' performance. Our findings
reveal that LLMs generate 15\%-25\% conceptually irrelevant interpretations,
depend on metaphorical indicators in training data rather than contextual cues,
and are more sensitive to syntactic irregularities than to structural
comprehension. These insights underline the limitations of LLMs in metaphor
analysis and call for more robust computational approaches.

</details>


### [46] [Sri Lanka Document Datasets: A Large-Scale, Multilingual Resource for Law, News, and Policy (v20251005)](https://arxiv.org/abs/2510.04124)
*Nuwan I. Senaratna*

Main category: cs.CL

TL;DR: 本文介绍了一个包含斯里兰卡议会记录、法律判决、政府出版物、新闻和旅游统计的开放机器可读文档数据集集合，支持计算语言学、法律分析、社会政治研究和多语言自然语言处理研究。


<details>
  <summary>Details</summary>
Motivation: 为支持斯里兰卡多语言环境下的计算语言学、法律分析和社会政治研究提供高质量的开放数据集资源。

Method: 构建数据收集管道，从多个来源收集文档数据，以Sinhala、Tamil和英语三种语言组织，每日更新并在GitHub和Hugging Face上镜像。

Result: 截至v20251005版本，该集合包含13个数据集，共215,670个文档（60.3 GB），涵盖议会记录、法律判决、政府出版物、新闻和旅游统计。

Conclusion: 该数据集集合为斯里兰卡多语言环境下的研究提供了宝贵的资源，同时讨论了许可和伦理考虑。

Abstract: We present a collection of open, machine-readable document datasets covering
parliamentary proceedings, legal judgments, government publications, news, and
tourism statistics from Sri Lanka. As of v20251005, the collection currently
comprises 215,670 documents (60.3 GB) across 13 datasets in Sinhala, Tamil, and
English. The datasets are updated daily and mirrored on GitHub and Hugging
Face. These resources aim to support research in computational linguistics,
legal analytics, socio-political studies, and multilingual natural language
processing. We describe the data sources, collection pipeline, formats, and
potential use cases, while discussing licensing and ethical considerations.

</details>


### [47] [Fine Tuning Methods for Low-resource Languages](https://arxiv.org/abs/2510.04139)
*Tim Bakkenes,Daniel Wang,Anton Johansson*

Main category: cs.CL

TL;DR: 开发了一种通用方法来准备文化相关数据集，通过后训练Gemma 2模型，提升其在代表性不足语言上的性能，展示如何在不同国家解锁生成式AI潜力并保护文化遗产。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型主要基于英语文本和文化训练，导致在其他语言和文化背景下表现不佳，缺乏文化包容性。

Method: 开发通用方法准备文化相关数据集，并对Gemma 2模型进行后训练。

Result: 提高了Gemma 2在代表性不足语言上的性能表现。

Conclusion: 该方法可帮助其他国家解锁生成式AI潜力并保护文化遗产，促进文化包容性。

Abstract: The rise of Large Language Models has not been inclusive of all cultures. The
models are mostly trained on English texts and culture which makes them
underperform in other languages and cultural contexts. By developing a
generalizable method for preparing culturally relevant datasets and
post-training the Gemma 2 model, this project aimed to increase the performance
of Gemma 2 for an underrepresented language and showcase how others can do the
same to unlock the power of Generative AI in their country and preserve their
cultural heritage.

</details>


### [48] [Self Speculative Decoding for Diffusion Large Language Models](https://arxiv.org/abs/2510.04147)
*Yifeng Gao,Ziang Ji,Yuxuan Wang,Biqing Qi,Hanlin Xu,Linfeng Zhang*

Main category: cs.CL

TL;DR: 提出SSD（自推测解码）方法，利用扩散大语言模型自身的并行预测能力实现无损推理加速，无需额外模块，在单次前向传播中验证和接受多个token，达到3.46倍加速效果。


<details>
  <summary>Details</summary>
Motivation: 当前并行解码方法的生成结果与逐步解码存在偏差，导致性能下降，限制了实际部署。需要一种既能保持输出质量又能加速推理的方法。

Method: SSD采用自推测解码机制，让模型同时作为推测解码的起草者和验证者，通过层级验证树在单次前向传播中验证多个位置的预测结果。

Result: 在LLaDA和Dream等开源模型上，SSD实现了最高3.46倍的加速效果，同时保持输出与逐步解码完全一致。

Conclusion: SSD成功解决了扩散大语言模型并行解码中的性能偏差问题，提供了一种高效的无损推理加速方案。

Abstract: Diffusion-based Large Language Models (dLLMs) have emerged as a competitive
alternative to autoregressive models, offering unique advantages through
bidirectional attention and parallel generation paradigms. However, the
generation results of current parallel decoding methods deviate from stepwise
decoding, introducing potential performance degradation, which limits their
practical deployment. To address this problem, we propose \textbf{S}elf
\textbf{S}peculative \textbf{D}ecoding (SSD), a lossless inference acceleration
method that leverages the dLLM itself as both speculative decoding drafter and
verifier without auxiliary modules. SSD introduces a self-drafting mechanism
where the model generates predictions for multiple positions, then verifies
them through hierarchical verification trees in a single forward pass. Unlike
traditional speculative decoding that requires separate draft models, SSD
eliminates model redundancy and memory overhead by exploiting the dLLM's
inherent parallel prediction capability for multiple positions. This
self-speculative approach allows the model to progressively verify and accept
multiple tokens in a single forward pass. Our experiments demonstrate that SSD
achieves up to 3.46$\times$ speedup while keeping the output identical to
stepwise decoding on open source models such as LLaDA and Dream. Code will be
made publicly available on GitHub.

</details>


### [49] [Thinking on the Fly: Test-Time Reasoning Enhancement via Latent Thought Policy Optimization](https://arxiv.org/abs/2510.04182)
*Wengao Ye,Yan Liang,Lianlei Shan*

Main category: cs.CL

TL;DR: LTPO是一个无需参数更新的测试时优化框架，通过将中间潜在思考向量作为动态参数进行优化，提升LLM在挑战性推理任务上的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有潜在推理方法在具有挑战性的分布外任务上表现脆弱，而鲁棒推理在这些任务中最为关键。

Method: 使用在线策略梯度方法，基于冻结LLM自身输出分布计算的置信度奖励信号，优化中间潜在思考向量。

Result: 在五个推理基准测试中，LTPO不仅匹配或超越强基线，在AIME等高难度基准上表现出显著改进，而现有方法准确率接近零。

Conclusion: LTPO展示了在复杂推理任务上的独特能力，无需外部监督或模型参数更新即可显著提升推理鲁棒性。

Abstract: Recent advancements in Large Language Models (LLMs) have shifted from
explicit Chain-of-Thought (CoT) reasoning to more efficient latent reasoning,
where intermediate thoughts are represented as vectors rather than text.
However, latent reasoning can be brittle on challenging, out-of-distribution
tasks where robust reasoning is most critical. To overcome these limitations,
we introduce Latent Thought Policy Optimization (LTPO), a parameter-free
framework that enhances LLM reasoning entirely at test time, without requiring
model parameter updates. LTPO treats intermediate latent "thought" vectors as
dynamic parameters that are actively optimized for each problem instance. It
employs an online policy gradient method guided by an intrinsic,
confidence-based reward signal computed directly from the frozen LLM's own
output distributions, eliminating the need for external supervision or
expensive text generation during optimization. Extensive experiments on five
reasoning benchmarks show that LTPO not only matches or surpasses strong
baselines on standard tasks but also demonstrates remarkable robustness where
others fail. Most notably, on highly challenging AIME benchmarks where existing
latent reasoning baselines collapse to near-zero accuracy, LTPO delivers
substantial improvements, showcasing a unique capability for complex reasoning.

</details>


### [50] [CALM Before the STORM: Unlocking Native Reasoning for Optimization Modeling](https://arxiv.org/abs/2510.04204)
*Zhengyang Tang,Zihan Ye,Chenyu Huang,Xuhan Huang,Chengpeng Li,Sihang Li,Guanhua Chen,Ming Yan,Zizhuo Wang,Hongyuan Zha,Dayiheng Liu,Benyou Wang*

Main category: cs.CL

TL;DR: CALM框架通过轻量级修正提示来优化大型推理模型在优化建模任务中的表现，生成的STORM模型在多个基准测试中达到68.9%的平均准确率，性能媲美671B参数模型。


<details>
  <summary>Details</summary>
Motivation: 现有领域适应方法无法充分利用现代大型推理模型的高级推理能力，直接微调传统数据集效果有限。

Method: 提出CALM框架：专家识别推理缺陷并提供修正提示，模型据此改进推理轨迹；生成高质量数据用于监督微调，再通过强化学习进一步优化。

Result: 开发的STORM模型（4B参数）在五个优化建模基准测试中达到68.9%平均准确率，性能与671B模型相当。

Conclusion: 基于提示的动态数据合成能够保持并增强现代大型推理模型的固有推理模式，为挑战性优化建模任务提供了更有效和可扩展的路径。

Abstract: Large Reasoning Models (LRMs) have demonstrated strong capabilities in
complex multi-step reasoning, opening new opportunities for automating
optimization modeling. However, existing domain adaptation methods, originally
designed for earlier instruction-tuned models, often fail to exploit the
advanced reasoning patterns of modern LRMs -- In particular, we show that
direct fine-tuning on traditional \textit{non-reflective} datasets leads to
limited gains. To fully leverage LRMs' inherent reasoning abilities, we propose
\textbf{CALM} (\textit{Corrective Adaptation with Lightweight Modification}), a
framework that progressively refines LRMs within their native reasoning modes
for optimization modeling tasks. In CALM, an expert intervener identifies
reasoning flaws and provides concise corrective hints, which the LRM
incorporates to produce improved reasoning trajectories. These interventions
modify fewer than 2.6\% of generated tokens, but generate high-quality data for
soft adaptation through supervised fine-tuning. The adapted model is then
further improved through reinforcement learning. Building on CALM, we develop
\textbf{STORM} (\textit{Smart Thinking Optimization Reasoning Model}), a
4B-parameter LRM that achieves a new state-of-the-art average accuracy of
68.9\% across five popular optimization modeling benchmarks, matching the
performance of a 671B LRM. These results demonstrate that dynamic, hint-based
data synthesis both preserves and amplifies the native reasoning patterns of
modern LRMs, offering a more effective and scalable path towards expert-level
performance on challenging optimization modeling tasks.

</details>


### [51] [Teaching LLM to be Persuasive: Reward-Enhanced Policy Optimization for Alignment frm Heterogeneous Rewards](https://arxiv.org/abs/2510.04214)
*Zhuoran Zhuang,Ye Chen,Xia Zeng,Chao Luo,Luhui Liu,Yihan Chen*

Main category: cs.CL

TL;DR: 提出了REPO强化学习框架，通过整合偏好奖励模型、说服行为奖励和程序化奖励，提升LLM在OTA价格谈判中的表现，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统后训练方法在OTA价格谈判场景中存在过拟合脚本、忽略微妙说服风格、无法强制执行业务约束的问题。

Method: REPO框架结合三种异质奖励：偏好训练的奖励模型用于密集人类对齐，奖励法官用于高级说服行为和SOP合规性，程序化奖励函数用于数值、格式和护栏的确定性检查。

Result: 在150轮真实对话和225轮精选坏案例对话评估中，REPO将平均对话评分提升至4.63，比基础模型高1.20，比DPO高0.83；将至少有一个优秀回复的对话比例提升至66.67%，坏案例修复率达到93.33%。

Conclusion: REPO框架有效解决了传统方法的局限性，在OTA价格谈判任务中表现出色，并涌现出超越黄金标注的主动同理心、局部推理和校准策略等能力。

Abstract: We study deploying large language models (LLMs) as business development (BD)
agents for persuasive price negotiation in online travel agencies (OTAs), where
aligning traveler affordability and hotel profitability directly affects
bookings, partner relationships, and access to travel. The agent must follow a
Standard Operating Procedure (SOP) while conducting multi-turn persuasion,
interpreting colloquial inputs, and adhering to guardrails (no over-promising,
no hallucinations). Conventional post-training -- supervised fine-tuning (SFT)
or single-source reward optimization -- overfits scripts, misses nuanced
persuasive style, and fails to enforce verifiable business constraints.
  We propose Reward-Enhanced Policy Optimization (REPO), a reinforcement
learning post-training framework that aligns an LLM with heterogeneous rewards:
a preference-trained reward model (RM) for dense human alignment, a reward
judge (RJ) for high-level persuasive behavior and SOP compliance, and
programmatic reward functions (RF) for deterministic checks on numerics,
formatting, and guardrails. A straightforward enhancement mechanism is proposed
to combine the RM with RJ and RF signals to curb reward hacking and improve
negotiation quality. In production-style evaluations -- approximately 150 turns
from real dialogues and 225 turns from curated bad-case dialogues -- REPO lifts
average dialogue rating to 4.63: +1.20 over base, +0.83 over Direct Preference
Optimization (DPO); +0.33 over Group Relative Policy Optimization (GRPO),
increases the share of conversations with at least one excellent response to
66.67% (+23.34 percentage points over GRPO), and achieves a 93.33% bad-case fix
rate with 75.56% clean fixes, outperforming SFT, DPO, PPO, and GRPO. We also
observe emergent capabilities -- proactive empathy, localized reasoning,
calibrated tactics -- that surpass gold annotations.

</details>


### [52] [Epistemic Diversity and Knowledge Collapse in Large Language Models](https://arxiv.org/abs/2510.04226)
*Dustin Wright,Sarah Masud,Jared Moore,Srishti Yadav,Maria Antoniak,Chan Young Park,Isabelle Augenstein*

Main category: cs.CL

TL;DR: 该研究提出了一种衡量LLM认知多样性的新方法，发现虽然新模型生成更多样化的主张，但几乎所有模型的认知多样性都低于基础网络搜索。模型大小对认知多样性有负面影响，而RAG有正面影响，但改善程度因文化背景而异。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型倾向于生成词汇、语义和风格同质化的文本，这带来了知识崩溃的风险，即同质化的LLM会随时间推移缩小可获取信息的范围。现有研究局限于封闭式选择题设置或模糊语义特征，且未考察跨时间和文化背景的趋势。

Method: 提出测量认知多样性的新方法，即LLM输出中真实世界主张的变化。测试了27个LLM、155个涵盖12个国家的主题，以及来自真实用户聊天的200个提示变体。

Result: 新模型倾向于生成更多样化的主张，但几乎所有模型的认知多样性都低于基础网络搜索。模型大小对认知多样性有负面影响，RAG有正面影响，但改善程度因文化背景而异。与维基百科相比，特定国家的主张更多反映英语而非当地语言。

Conclusion: LLM存在认知多样性不足的问题，特别是在文化背景相关的知识表示方面。需要改进模型以更好地反映不同文化和语言背景的认知多样性。

Abstract: Large language models (LLMs) tend to generate lexically, semantically, and
stylistically homogenous texts. This poses a risk of knowledge collapse, where
homogenous LLMs mediate a shrinking in the range of accessible information over
time. Existing works on homogenization are limited by a focus on closed-ended
multiple-choice setups or fuzzy semantic features, and do not look at trends
across time and cultural contexts. To overcome this, we present a new
methodology to measure epistemic diversity, i.e., variation in real-world
claims in LLM outputs, which we use to perform a broad empirical study of LLM
knowledge collapse. We test 27 LLMs, 155 topics covering 12 countries, and 200
prompt variations sourced from real user chats. For the topics in our study, we
show that while newer models tend to generate more diverse claims, nearly all
models are less epistemically diverse than a basic web search. We find that
model size has a negative impact on epistemic diversity, while
retrieval-augmented generation (RAG) has a positive impact, though the
improvement from RAG varies by the cultural context. Finally, compared to a
traditional knowledge source (Wikipedia), we find that country-specific claims
reflect the English language more than the local one, highlighting a gap in
epistemic representation

</details>


### [53] [Pushing on Multilingual Reasoning Models with Language-Mixed Chain-of-Thought](https://arxiv.org/abs/2510.04230)
*Guijin Son,Donghun Yang,Hitesh Laxmichand Patel,Amit Agarwal,Hyunwoo Ko,Chanuk Lim,Srikant Panda,Minhyuk Kim,Nikunj Drolia,Dasol Choi,Kyong-Ha Lee,Youngjae Yu*

Main category: cs.CL

TL;DR: 该论文提出了语言混合思维链（Language-Mixed CoT）方法，在英语和目标语言之间切换推理，以英语为锚点提升推理能力。通过韩语案例研究，构建了Yi-Sang数据集，训练了多个模型，其中KO-REAson-35B在9个基准测试中取得最佳性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注英语的推理蒸馏，对语言特定推理了解甚少。本文旨在填补这一空白，研究如何通过语言混合推理提升非英语语言的推理能力。

Method: 提出语言混合思维链推理模式，在英语和目标语言间切换；构建Yi-Sang数据集（579万韩语提示、370万长推理轨迹）；训练9个模型（4B-35B参数）。

Result: 最佳模型KO-REAson-35B在9个基准测试中取得最高平均分64.0，5项排名第一，其余排名第二；中小型模型平均提升18.6分；语言混合CoT比单语CoT更有效。

Conclusion: 语言混合思维链能有效提升语言特定推理能力，带来跨语言和多模态性能增益。发布了数据管道、评估系统、数据集和模型以促进相关研究。

Abstract: Recent frontier models employ long chain-of-thought reasoning to explore
solution spaces in context and achieve stonger performance. While many works
study distillation to build smaller yet capable models, most focus on English
and little is known about language-specific reasoning. To bridge this gap, we
first introduct **Language-Mixed CoT**, a reasoning schema that switches
between English and a target language, using English as an anchor to excel in
reasoning while minimizing translation artificats. As a Korean case study, we
curate **Yi-Sang**: 5.79M native-Korean prompts from web Q&A, exams, STEM, and
code; 3.7M long reasoning traces generated from Qwen3-32B; and a targeted 260k
high-yield subset. We train ninve models (4B-35B) across six families (Qwen2.5,
Llama-3.1, Gemma-3, etc). Our best model, **KO-REAson-35B**, achieves
state-of-the-art performance, with the highest overall average score (64.0 \pm
25), ranking first on 5/9 benchmarks and second on the remainder. Samller and
mid-sized models also benefit substantially, with an average improvement of
+18.6 points across teh evaluated nine benchmarks. Ablations show
**Language-Mixed CoT** is more effective than monolingual CoT, also resulting
in cross-lingual and mult-modal performance gains. We release our data-curation
pipeline, evaluation system, datasets, and models to advance research on
language-specific reasoning. Data and model collection:
https://huggingface.co/KOREAson.

</details>


### [54] [LongTail-Swap: benchmarking language models' abilities on rare words](https://arxiv.org/abs/2510.04268)
*Robin Algayres,Charles-Éric Saint-James,Mahi Luthra,Jiayi Shen,Dongyan Lin,Youssef Benchekroun,Rashel Moritz,Juan Pino,Emmanuel Dupoux*

Main category: cs.CL

TL;DR: 该论文提出了LongTail-Swap（LT-Swap）基准测试，专注于评估语言模型在数据稀缺情况下学习罕见词的能力，类似于婴儿的学习方式。


<details>
  <summary>Details</summary>
Motivation: 现有BabyLM挑战主要关注词汇分布头部，而婴儿学习语言的特点是数据效率高且能快速学习新词，因此需要专门评估语言模型在罕见词学习方面的能力。

Method: 构建了基于预训练语料库的测试集，包含可接受与不可接受的句子对，隔离罕见词的语义和句法使用，通过零样本方式评估模型对句子对的平均对数概率。

Result: 评估了16个BabyLM排行榜模型，结果显示语言模型在罕见词上表现较差，且不同架构模型在长尾分布上的性能差异比头部更明显。

Conclusion: LT-Swap基准提供了新的视角来评估哪种语言模型架构更适合处理罕见词泛化问题，揭示了模型架构在长尾分布上的重要差异。

Abstract: Children learn to speak with a low amount of data and can be taught new words
on a few-shot basis, making them particularly data-efficient learners. The
BabyLM challenge aims at exploring language model (LM) training in the low-data
regime but uses metrics that concentrate on the head of the word distribution.
Here, we introduce LongTail-Swap (LT-Swap), a benchmark that focuses on the
tail of the distribution, i.e., measures the ability of LMs to learn new words
with very little exposure, like infants do. LT-Swap is a pretraining
corpus-specific test set of acceptable versus unacceptable sentence pairs that
isolate semantic and syntactic usage of rare words. Models are evaluated in a
zero-shot fashion by computing the average log probabilities over the two
members of each pair. We built two such test sets associated with the 10M words
and 100M words BabyLM training sets, respectively, and evaluated 16 models from
the BabyLM leaderboard. Our results not only highlight the poor performance of
language models on rare words but also reveal that performance differences
across LM architectures are much more pronounced in the long tail than in the
head. This offers new insights into which architectures are better at handling
rare word generalization. We've also made the code publicly avail

</details>


### [55] [Probing Geometry of Next Token Prediction Using Cumulant Expansion of the Softmax Entropy](https://arxiv.org/abs/2510.04285)
*Karthik Viswanathan,Sang Eon Park*

Main category: cs.CL

TL;DR: 提出了一个累积量展开框架来量化大语言模型在下一个token预测过程中如何内化高阶统计结构，通过分析GPT-2和Pythia模型揭示了不同提示类型的累积量特征差异。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型如何学习和内化高阶统计结构，理解模型在不同类型文本（如结构化提示、数学内容等）处理中的特征学习动态。

Method: 使用累积量展开框架，将每层logit分布的softmax熵作为围绕其"中心"分布的扰动，推导出封闭形式的累积量可观测量来隔离高阶相关性。

Result: 发现结构化提示在不同层呈现上升-平台特征，而token打乱的提示保持平坦；训练过程中所有累积量单调增加后饱和；数学提示与一般文本显示不同的累积量特征。

Conclusion: 累积量分析为高维神经网络中的特征学习动态提供了一个轻量级、数学基础扎实的探测方法。

Abstract: We introduce a cumulant-expansion framework for quantifying how large
language models (LLMs) internalize higher-order statistical structure during
next-token prediction. By treating the softmax entropy of each layer's logit
distribution as a perturbation around its "center" distribution, we derive
closed-form cumulant observables that isolate successively higher-order
correlations. Empirically, we track these cumulants in GPT-2 and Pythia models
on Pile-10K prompts. (i) Structured prompts exhibit a characteristic
rise-and-plateau profile across layers, whereas token-shuffled prompts remain
flat, revealing the dependence of the cumulant profile on meaningful context.
(ii) During training, all cumulants increase monotonically before saturating,
directly visualizing the model's progression from capturing variance to
learning skew, kurtosis, and higher-order statistical structures. (iii)
Mathematical prompts show distinct cumulant signatures compared to general
text, quantifying how models employ fundamentally different processing
mechanisms for mathematical versus linguistic content. Together, these results
establish cumulant analysis as a lightweight, mathematically grounded probe of
feature-learning dynamics in high-dimensional neural networks.

</details>


### [56] [SliceMoE: Routing Embedding Slices Instead of Tokens for Fine-Grained and Balanced Transformer Scaling](https://arxiv.org/abs/2510.04286)
*Harshil Vejendla*

Main category: cs.CL

TL;DR: SliceMoE是一种改进的混合专家架构，通过将token的隐藏向量分割成切片并分别路由到专家，解决了传统token级路由的容量瓶颈、负载均衡问题和有限专业化问题。


<details>
  <summary>Details</summary>
Motivation: 传统MoE层的token级路由将整个语义谱分配给每个专家，导致容量瓶颈、负载均衡问题和有限的专业化能力。

Method: 将d维嵌入分割为S个切片，每个切片通过轻量级共享路由器预测top-k专家，专家独立处理分配的切片，输出重新组装以保持每个token的FLOP效率。

Result: 在WikiText-103语言建模、WMT En-De翻译和三个文本分类数据集上，SliceMoE比密集基线推理速度快1.7倍，比参数匹配的token-MoE困惑度降低12-18%，专家平衡性更好，并在句法与语义子空间上表现出可解释的专业化。

Conclusion: SliceMoE通过切片级路由实现了更平滑的专家利用、更好的负载均衡和更细粒度的专业化，同时保持了计算效率。

Abstract: Mixture-of-Experts (MoE) layers scale transformers by routing tokens to a
sparse subset of feed-forward experts. Token-level routing, however, assigns an
entire semantic spectrum to each expert, creating capacity bottlenecks,
load-balancing pathologies, and limited specialization. We introduce SliceMoE,
an architecture that routes contiguous slices of a token's hidden vector. A
d-dimensional embedding is partitioned into S slices, and for each slice, a
lightweight shared router predicts the top-k experts. Experts operate on their
assigned slices independently, and outputs are reassembled, maintaining
per-token FLOP efficiency. Because slices from different tokens interleave
within an expert, utilization is naturally smoother. We propose a slice-level
capacity loss, cross-slice dropout, and efficient fused batched GEMM kernels.
Experiments on WikiText-103 language modeling, WMT En-De translation, and three
text-classification datasets show SliceMoE attains up to 1.7x faster inference
than dense baselines, 12 to 18 percent lower perplexity than parameter-matched
token-MoE, and improved expert balance, with interpretable expertise over
syntactic versus semantic subspaces.

</details>


### [57] [PABSA: Hybrid Framework for Persian Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2510.04291)
*Mehrzad Tareh,Aydin Mohandesi,Ebrahim Ansari*

Main category: cs.CL

TL;DR: 提出了一种结合机器学习和深度学习的混合方法用于波斯语方面情感分析，通过整合多语言BERT的极性分数作为特征，在Pars-ABSA数据集上达到93.34%的准确率，超越了现有基准。


<details>
  <summary>Details</summary>
Motivation: 波斯语情感分析面临标注数据集稀缺、预处理工具有限以及高质量嵌入和特征提取方法缺乏等挑战，需要开发有效的解决方案。

Method: 采用混合方法整合机器学习和深度学习技术，利用多语言BERT的极性分数作为决策树分类器的附加特征，并引入了波斯语同义词和实体词典用于文本增强。

Result: 在Pars-ABSA数据集上实现了93.34%的准确率，超过了现有基准，证明了混合建模和特征增强的有效性。

Conclusion: 混合建模和特征增强方法能够有效推进波斯语等低资源语言的情感分析研究，为类似语言提供了可行的解决方案。

Abstract: Sentiment analysis is a key task in Natural Language Processing (NLP),
enabling the extraction of meaningful insights from user opinions across
various domains. However, performing sentiment analysis in Persian remains
challenging due to the scarcity of labeled datasets, limited preprocessing
tools, and the lack of high-quality embeddings and feature extraction methods.
To address these limitations, we propose a hybrid approach that integrates
machine learning (ML) and deep learning (DL) techniques for Persian
aspect-based sentiment analysis (ABSA). In particular, we utilize polarity
scores from multilingual BERT as additional features and incorporate them into
a decision tree classifier, achieving an accuracy of 93.34%-surpassing existing
benchmarks on the Pars-ABSA dataset. Additionally, we introduce a Persian
synonym and entity dictionary, a novel linguistic resource that supports text
augmentation through synonym and named entity replacement. Our results
demonstrate the effectiveness of hybrid modeling and feature augmentation in
advancing sentiment analysis for low-resource languages such as Persian.

</details>


### [58] [Equipping Retrieval-Augmented Large Language Models with Document Structure Awareness](https://arxiv.org/abs/2510.04293)
*Lingnan Xu,Chong Feng,Kaiyuan Zhang,Liu Zhengyong,Wenqiang Xu,Fanqing Meng*

Main category: cs.CL

TL;DR: 提出了RDR2框架，在检索增强生成中显式利用文档结构信息，通过LLM路由器和结构树导航来提升事实准确性


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法将检索到的段落视为孤立块，忽略了文档组织结构这一关键信息，导致在复杂场景下表现不佳

Method: 使用基于LLM的路由器动态导航文档结构树，联合评估内容相关性和层次关系，将文档路由制定为可训练任务

Result: 在五个挑战性数据集上达到最先进性能，证明结构感知显著提升了RAG系统获取和利用知识的能力

Conclusion: 显式结构感知能显著增强RAG系统，特别是在需要多文档合成的复杂场景中

Abstract: While large language models (LLMs) demonstrate impressive capabilities, their
reliance on parametric knowledge often leads to factual inaccuracies.
Retrieval-Augmented Generation (RAG) mitigates this by leveraging external
documents, yet existing approaches treat retrieved passages as isolated chunks,
ignoring valuable structure that is crucial for document organization.
Motivated by this gap, we propose Retrieve-DocumentRoute-Read (RDR2), a novel
framework that explicitly incorporates structural information throughout the
RAG process. RDR2 employs an LLM-based router to dynamically navigate document
structure trees, jointly evaluating content relevance and hierarchical
relationships to assemble optimal evidence. Our key innovation lies in
formulating document routing as a trainable task, with automatic action
curation and structure-aware passage selection inspired by human reading
strategies. Through comprehensive evaluation on five challenging datasets, RDR2
achieves state-of-the-art performance, demonstrating that explicit structural
awareness significantly enhances RAG systems' ability to acquire and utilize
knowledge, particularly in complex scenarios requiring multi-document
synthesis.

</details>


### [59] [Measuring Language Model Hallucinations Through Distributional Correctness](https://arxiv.org/abs/2510.04302)
*Thomas F Burns*

Main category: cs.CL

TL;DR: 提出了Distributional Correctness Score (DCS)评估指标，通过考虑模型在答案选择上的完整概率分布，区分有害的过度自信和通过弃权表达的不确定性，提供更细致和一致的评价范式。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型评估范式主要关注单一响应的准确性评分，无法捕捉模型完整的信念状态。模型产生幻觉的部分原因是它们被优化为在二元评分方案下成为好的应试者，奖励任何答案而非弃权。

Method: 引入新颖的Distributional Correctness Score (DCS)评估指标，该指标考虑模型在答案选择上的完整概率分布，自然区分对错误答案的有害过度自信和通过"我不知道"响应表达的不确定性。

Result: 通过理论分析和示例证明DCS提供了更细致和一致的评价范式，激励模型表达真正的不确定性而非猜测。在12个现有评估基准上测试6个语言模型，发现半数基准的得分在所有测试模型上均为负值，表明存在显著的幻觉倾向。

Conclusion: DCS解决了不考虑模型在答案选择上完整概率分布的问题，提供了可解释的默认范围评分，能够更准确地评估语言模型的信念状态和不确定性表达。

Abstract: Common evaluation paradigms for language models focus on scoring single
responses through accuracy metrics or proper scoring rules, failing to capture
the full richness of a model's belief state. Recent work illustrates that
language models hallucinate in-part because they are optimised to be good
test-takers under binary scoring schemes that reward any answer over
abstention. While this insight naturally leads to penalty-based approaches,
they ignore crucial distinctions in how models distribute uncertainty, for
example between hedging toward incorrect answers versus hedging toward "I don't
know" responses. A novel evaluation metric, the Distributional Correctness
Score (DCS), is introduced to solve this problem, i.e., of not considering a
model's entire probability distribution over answer choices. DCS naturally
distinguishes between harmful overconfidence in wrong answers and uncertainty
expressed through abstention, providing scores in an interpretable default
range. Through theoretical analysis and illustrative examples, DCS is
demonstrated to offer a more nuanced and aligned evaluation paradigm that
incentivises models to express genuine uncertainty rather than guessing.
Adapting 12 existing evaluation benchmarks to DCS's variants and measuring
performance on six language models reveals that for half of the tested
benchmarks scores are negative across all tested models, indicating significant
tendencies towards hallucination.

</details>


### [60] [Read the Scene, Not the Script: Outcome-Aware Safety for LLMs](https://arxiv.org/abs/2510.04320)
*Rui Wu,Yihao Quan,Zeru Shi,Zhenting Wang,Yanshu Li,Ruixiang Tang*

Main category: cs.CL

TL;DR: 论文发现安全对齐的大语言模型存在后果盲视问题，即模型无法正确推理行动与后果之间的联系，过度依赖表面形式信号。作者构建了CB-Bench基准和CS-Chain-4k数据集来缓解这一问题。


<details>
  <summary>Details</summary>
Motivation: 当前安全对齐的LLMs存在两个主要失败模式：容易被越狱攻击，以及对无害输入过度拒绝。这些问题的共同原因是模型对行动与后果之间的关联推理能力弱，过度依赖表面形式信号。

Method: 构建CB-Bench基准来评估后果盲视问题，涵盖四种风险场景；开发CS-Chain-4k数据集用于安全对齐，通过微调模型来增强后果推理能力。

Result: 主流模型在CB-Bench上持续失败，表现出系统性后果盲视；在CS-Chain-4k上微调的模型在语义伪装越狱攻击中表现更好，减少了对无害输入的过度拒绝，同时保持了其他基准的效用和泛化能力。

Conclusion: 研究揭示了当前对齐方法的局限性，确立了后果感知推理作为核心对齐目标，并提供了更实用和可复现的评估路径。

Abstract: Safety-aligned Large Language Models (LLMs) still show two dominant failure
modes: they are easily jailbroken, or they over-refuse harmless inputs that
contain sensitive surface signals. We trace both to a common cause: current
models reason weakly about links between actions and outcomes and over-rely on
surface-form signals, lexical or stylistic cues that do not encode
consequences. We define this failure mode as Consequence-blindness. To study
consequence-blindness, we build a benchmark named CB-Bench covering four risk
scenarios that vary whether semantic risk aligns with outcome risk, enabling
evaluation under both matched and mismatched conditions which are often ignored
by existing safety benchmarks. Mainstream models consistently fail to separate
these risks and exhibit consequence-blindness, indicating that
consequence-blindness is widespread and systematic. To mitigate
consequence-blindness, we introduce CS-Chain-4k, a consequence-reasoning
dataset for safety alignment. Models fine-tuned on CS-Chain-4k show clear gains
against semantic-camouflage jailbreaks and reduce over-refusal on harmless
inputs, while maintaining utility and generalization on other benchmarks. These
results clarify the limits of current alignment, establish consequence-aware
reasoning as a core alignment goal and provide a more practical and
reproducible evaluation path.

</details>


### [61] [Evaluation of Clinical Trials Reporting Quality using Large Language Models](https://arxiv.org/abs/2510.04338)
*Mathieu Laï-king,Patrick Paroubek*

Main category: cs.CL

TL;DR: 本文测试了大型语言模型评估临床试验研究报告质量的能力，使用CONSORT标准创建了CONSORT-QA评估语料库，最佳模型组合达到85%准确率。


<details>
  <summary>Details</summary>
Motivation: 临床试验研究报告质量影响临床决策，需要有效评估方法。

Method: 创建CONSORT-QA评估语料库，使用不同大型生成语言模型和提示方法（包括思维链）评估CONSORT标准。

Result: 最佳模型和提示方法组合达到85%准确率，思维链提供了有价值的推理信息。

Conclusion: 大型语言模型能够有效评估临床试验研究报告质量，思维链方法增强了模型推理能力。

Abstract: Reporting quality is an important topic in clinical trial research articles,
as it can impact clinical decisions. In this article, we test the ability of
large language models to assess the reporting quality of this type of article
using the Consolidated Standards of Reporting Trials (CONSORT). We create
CONSORT-QA, an evaluation corpus from two studies on abstract reporting quality
with CONSORT-abstract standards. We then evaluate the ability of different
large generative language models (from the general domain or adapted to the
biomedical domain) to correctly assess CONSORT criteria with different known
prompting methods, including Chain-of-thought. Our best combination of model
and prompting method achieves 85% accuracy. Using Chain-of-thought adds
valuable information on the model's reasoning for completing the task.

</details>


### [62] [Inoculation Prompting: Eliciting traits from LLMs during training can suppress them at test-time](https://arxiv.org/abs/2510.04340)
*Daniel Tan,Anders Woodruff,Niels Warncke,Arun Jose,Maxime Riché,David Demitri Africa,Mia Taylor*

Main category: cs.CL

TL;DR: 提出接种提示法：通过在微调数据前添加简短的系统提示指令来故意引发不良特征，从而在测试时降低这些特征的表达，实现选择性学习。


<details>
  <summary>Details</summary>
Motivation: 语言模型微调常常会同时学习到期望和不期望的特征，需要一种方法来选择性地抑制不良特征的学习。

Method: 在微调数据前添加系统提示指令来故意引发不良特征，然后在测试时不使用该指令，使模型降低不良特征的表达。

Result: 接种方法在多个场景中有效：减少任务特定微调中的新兴错位、防御后门注入、减轻通过潜意识学习的特征传递。

Conclusion: 接种提示法是一种简单有效的选择性学习技术，通过减少优化压力来降低特征泛化程度，有助于更好理解语言模型的泛化机制。

Abstract: Language model finetuning often results in learning undesirable traits in
combination with desired ones. To address this, we propose inoculation
prompting: modifying finetuning data by prepending a short system-prompt
instruction that deliberately elicits the undesirable trait. At test time, we
evaluate without the instruction; inoculated models have much lower expression
of the trait than models trained with unmodified training data. Inoculation is
selective: in a toy setting where assistant responses are always in Spanish and
ALL-CAPS, an appropriate inoculation (e.g., ``You always speak in Spanish.'')
teaches the model to capitalize responses while still responding in English. We
find that inoculation is also effective across several additional settings:
reducing emergent misalignment (EM) from task-specific finetuning, defending
against backdoor injections, and mitigating the transmission of traits via
subliminal learning. Follow-up analysis suggests a mechanism: making a trait
less surprising via inoculation reduces optimization pressure to globally
update the model, thereby reducing the degree of generalization. Our analysis
relates to prior work on EM: inoculation explains prior findings that
educational contexts mitigate EM from insecure code. Beyond demonstrating a
simple and effective technique for selective learning, our results contribute
to a better conceptual understanding of how and why language models generalize.

</details>


### [63] [Unmasking Backdoors: An Explainable Defense via Gradient-Attention Anomaly Scoring for Pre-trained Language Models](https://arxiv.org/abs/2510.04347)
*Anindya Sundar Das,Kangjie Chen,Monowar Bhuyan*

Main category: cs.CL

TL;DR: 该论文研究预训练语言模型的内部后门攻击行为，提出结合注意力机制和梯度信息的推理时防御方法，显著降低攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 预训练语言模型虽然在各种NLP任务中表现出色，但容易受到后门攻击的威胁，攻击者通过在训练数据中嵌入触发模式来植入恶意行为。

Method: 研究后门模型的内部行为，关注中毒输入处理时注意力和梯度归因的一致性偏移，提出基于token级注意力和梯度信息构建异常分数的推理时防御方法。

Result: 在多种文本分类任务和不同后门攻击场景下的广泛实验表明，该方法相比现有基线显著降低了攻击成功率。

Conclusion: 提出的防御方法有效，并通过可解释性分析揭示了触发定位机制和防御的鲁棒性。

Abstract: Pre-trained language models have achieved remarkable success across a wide
range of natural language processing (NLP) tasks, particularly when fine-tuned
on large, domain-relevant datasets. However, they remain vulnerable to backdoor
attacks, where adversaries embed malicious behaviors using trigger patterns in
the training data. These triggers remain dormant during normal usage, but, when
activated, can cause targeted misclassifications. In this work, we investigate
the internal behavior of backdoored pre-trained encoder-based language models,
focusing on the consistent shift in attention and gradient attribution when
processing poisoned inputs; where the trigger token dominates both attention
and gradient signals, overriding the surrounding context. We propose an
inference-time defense that constructs anomaly scores by combining token-level
attention and gradient information. Extensive experiments on text
classification tasks across diverse backdoor attack scenarios demonstrate that
our method significantly reduces attack success rates compared to existing
baselines. Furthermore, we provide an interpretability-driven analysis of the
scoring mechanism, shedding light on trigger localization and the robustness of
the proposed defense.

</details>


### [64] [Improving Consistency in Retrieval-Augmented Systems with Group Similarity Rewards](https://arxiv.org/abs/2510.04392)
*Faisal Hamman,Chenyang Zhu,Anoop Kumar,Xujun Peng,Sanghamitra Dutta,Daben Liu,Alfy Samuel*

Main category: cs.CL

TL;DR: 提出了Con-RAG系统，通过PS-GRPO强化学习方法解决RAG系统在语义等价查询下输出不一致的问题，显著提升了信息一致性和准确性。


<details>
  <summary>Details</summary>
Motivation: RAG系统在高风险领域部署时，用户期望语义等价查询能产生一致输出，但现有系统因检索器和生成器的变异性导致显著不一致，损害了信任和可靠性。

Method: 引入分解RAG一致性的评估框架，提出PS-GRPO强化学习方法，利用多轮次生成和组相似性奖励训练生成器，并开发了可扩展的近似奖励计算方法。

Result: 在短形式、多跳和长形式QA基准测试中，Con-RAG相比强基线显著提升了一致性和准确性，即使在没有显式真实监督的情况下也表现良好。

Conclusion: 该工作为评估和构建可靠RAG系统提供了实用解决方案，特别适用于安全关键部署场景。

Abstract: RAG systems are increasingly deployed in high-stakes domains where users
expect outputs to be consistent across semantically equivalent queries.
However, existing systems often exhibit significant inconsistencies due to
variability in both the retriever and generator (LLM), undermining trust and
reliability. In this work, we focus on information consistency, i.e., the
requirement that outputs convey the same core content across semantically
equivalent inputs. We introduce a principled evaluation framework that
decomposes RAG consistency into retriever-level, generator-level, and
end-to-end components, helping identify inconsistency sources. To improve
consistency, we propose Paraphrased Set Group Relative Policy Optimization
(PS-GRPO), an RL approach that leverages multiple rollouts across paraphrased
set to assign group similarity rewards. We leverage PS-GRPO to achieve
Information Consistent RAG (Con-RAG), training the generator to produce
consistent outputs across paraphrased queries and remain robust to
retrieval-induced variability. Because exact reward computation over paraphrase
sets is computationally expensive, we also introduce a scalable approximation
method that retains effectiveness while enabling efficient, large-scale
training. Empirical evaluations across short-form, multi-hop, and long-form QA
benchmarks demonstrate that Con-RAG significantly improves both consistency and
accuracy over strong baselines, even in the absence of explicit ground-truth
supervision. Our work provides practical solutions for evaluating and building
reliable RAG systems for safety-critical deployments.

</details>


### [65] [Time Is Effort: Estimating Human Post-Editing Time for Grammar Error Correction Tool Evaluation](https://arxiv.org/abs/2510.04394)
*Ankit Vadehra,Bill Johnson,Gene Saunders,Pascal Poupart*

Main category: cs.CL

TL;DR: 提出了第一个大规模的语法错误校正(GEC)工具后编辑时间标注数据集，并开发了PEET评分器来评估GEC工具的用户友好性，通过量化后编辑时间节省来衡量工具实用性。


<details>
  <summary>Details</summary>
Motivation: 量化GEC工具的用户友好性，了解工具能节省多少用户编辑时间，为GEC工具评估提供以人为中心的新方向。

Method: 创建了两个英语GEC测试数据集(BEA19和CoNLL14)的大规模后编辑时间标注数据集，提出PEET评分器来估计后编辑时间并排名GEC工具。

Result: 分析表明判断句子是否需要修正以及改写和标点符号修改等编辑类型对后编辑时间影响最大，PEET与人工技术努力判断相关性良好。

Conclusion: PEET评分器为GEC工具可用性评估提供了新的人类中心化方向，能够有效量化工具在文本编辑中的时间节省效果。

Abstract: Text editing can involve several iterations of revision. Incorporating an
efficient Grammar Error Correction (GEC) tool in the initial correction round
can significantly impact further human editing effort and final text quality.
This raises an interesting question to quantify GEC Tool usability: How much
effort can the GEC Tool save users? We present the first large-scale dataset of
post-editing (PE) time annotations and corrections for two English GEC test
datasets (BEA19 and CoNLL14). We introduce Post-Editing Effort in Time (PEET)
for GEC Tools as a human-focused evaluation scorer to rank any GEC Tool by
estimating PE time-to-correct. Using our dataset, we quantify the amount of
time saved by GEC Tools in text editing. Analyzing the edit type indicated that
determining whether a sentence needs correction and edits like paraphrasing and
punctuation changes had the greatest impact on PE time. Finally, comparison
with human rankings shows that PEET correlates well with technical effort
judgment, providing a new human-centric direction for evaluating GEC tool
usability. We release our dataset and code at:
https://github.com/ankitvad/PEET_Scorer.

</details>


### [66] [SECA: Semantically Equivalent and Coherent Attacks for Eliciting LLM Hallucinations](https://arxiv.org/abs/2510.04398)
*Buyun Liang,Liangzu Peng,Jinqi Luo,Darshan Thaker,Kwan Ho Ryan Chan,René Vidal*

Main category: cs.CL

TL;DR: 提出了SECA方法，通过语义等价且连贯的提示修改来引发LLM幻觉，相比现有方法在保持语义一致性的同时获得更高的攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击方法往往产生不现实的提示，要么插入无意义标记，要么改变原意，无法真实反映实践中幻觉的发生情况。

Method: 将寻找现实攻击表述为语义等价和连贯约束下的优化问题，引入保持约束的零阶方法搜索对抗性提示。

Result: 在开放式多项选择问答任务中，SECA实现了更高的攻击成功率，同时几乎不违反约束条件。

Conclusion: SECA揭示了开源和商业LLM对现实且合理的提示变体的敏感性，为理解LLM幻觉提供了新视角。

Abstract: Large Language Models (LLMs) are increasingly deployed in high-risk domains.
However, state-of-the-art LLMs often produce hallucinations, raising serious
concerns about their reliability. Prior work has explored adversarial attacks
for hallucination elicitation in LLMs, but it often produces unrealistic
prompts, either by inserting gibberish tokens or by altering the original
meaning. As a result, these approaches offer limited insight into how
hallucinations may occur in practice. While adversarial attacks in computer
vision often involve realistic modifications to input images, the problem of
finding realistic adversarial prompts for eliciting LLM hallucinations has
remained largely underexplored. To address this gap, we propose Semantically
Equivalent and Coherent Attacks (SECA) to elicit hallucinations via realistic
modifications to the prompt that preserve its meaning while maintaining
semantic coherence. Our contributions are threefold: (i) we formulate finding
realistic attacks for hallucination elicitation as a constrained optimization
problem over the input prompt space under semantic equivalence and coherence
constraints; (ii) we introduce a constraint-preserving zeroth-order method to
effectively search for adversarial yet feasible prompts; and (iii) we
demonstrate through experiments on open-ended multiple-choice question
answering tasks that SECA achieves higher attack success rates while incurring
almost no constraint violations compared to existing methods. SECA highlights
the sensitivity of both open-source and commercial gradient-inaccessible LLMs
to realistic and plausible prompt variations. Code is available at
https://github.com/Buyun-Liang/SECA.

</details>


### [67] [Large Language Models Preserve Semantic Isotopies in Story Continuations](https://arxiv.org/abs/2510.04400)
*Marc Cavazza*

Main category: cs.CL

TL;DR: 该研究探讨了大型语言模型生成文本是否保留语义同位素，通过故事续写实验发现LLM在给定token范围内能够保持语义同位素。


<details>
  <summary>Details</summary>
Motivation: 探索文本语义与大型语言模型的相关性，研究LLM生成文本是否保持语义同位素，扩展分布语义学与结构语义学之间联系的认识。

Method: 设计故事续写实验，使用10,000个ROCStories提示由5个LLM完成，首先验证GPT-4o从语言基准中提取同位素的能力，然后应用于生成的故事，分析同位素的结构和语义特性。

Result: 结果显示，在给定token范围内，LLM完成的故事能够保持语义同位素，在多个特性上都得到保留。

Conclusion: LLM在特定token范围内生成文本时能够有效保持语义同位素，表明模型具有一定的语义连贯性保持能力。

Abstract: In this work, we explore the relevance of textual semantics to Large Language
Models (LLMs), extending previous insights into the connection between
distributional semantics and structural semantics. We investigate whether
LLM-generated texts preserve semantic isotopies. We design a story continuation
experiment using 10,000 ROCStories prompts completed by five LLMs. We first
validate GPT-4o's ability to extract isotopies from a linguistic benchmark,
then apply it to the generated stories. We then analyze structural (coverage,
density, spread) and semantic properties of isotopies to assess how they are
affected by completion. Results show that LLM completion within a given token
horizon preserves semantic isotopies across multiple properties.

</details>


### [68] [Good Intentions Beyond ACL: Who Does NLP for Social Good, and Where?](https://arxiv.org/abs/2510.04434)
*Grace LeFevre,Qingcheng Zeng,Adam Leif,Jason Jewell,Denis Peskoff,Rob Voigt*

Main category: cs.CL

TL;DR: 本文通过作者和会议层面的视角，量化分析了NLP4SG（自然语言处理促进社会公益）的研究现状，发现ACL作者在非ACL会议上更可能从事社会公益研究，且大多数NLP4SG研究由非ACL作者在ACL外部完成。


<details>
  <summary>Details</summary>
Motivation: 随着NLP社会影响日益重要，NLP4SG倡议受到越来越多关注。近20%的ACL论文涉及联合国可持续发展目标相关主题，需要系统了解该领域的研究格局。

Method: 采用作者和会议层面的分析方法，量化ACL社区内外、核心ACL贡献者和非ACL作者在社会公益研究中的比例分布。

Result: 发现两个令人惊讶的事实：1) ACL作者在非ACL会议上发表社会公益研究的可能性显著更高；2) 绝大多数使用NLP技术解决社会公益问题的研究由非ACL作者在ACL外部完成。

Conclusion: 这些发现对ACL社区在NLP4SG议程设置方面具有重要启示，需要重新思考如何促进社会公益研究在ACL社区内的发展。

Abstract: The social impact of Natural Language Processing (NLP) is increasingly
important, with a rising community focus on initiatives related to NLP for
Social Good (NLP4SG). Indeed, in recent years, almost 20% of all papers in the
ACL Anthology address topics related to social good as defined by the UN
Sustainable Development Goals (Adauto et al., 2023). In this study, we take an
author- and venue-level perspective to map the landscape of NLP4SG, quantifying
the proportion of work addressing social good concerns both within and beyond
the ACL community, by both core ACL contributors and non-ACL authors. With this
approach we discover two surprising facts about the landscape of NLP4SG. First,
ACL authors are dramatically more likely to do work addressing social good
concerns when publishing in venues outside of ACL. Second, the vast majority of
publications using NLP techniques to address concerns of social good are done
by non-ACL authors in venues outside of ACL. We discuss the implications of
these findings on agenda-setting considerations for the ACL community related
to NLP4SG.

</details>


### [69] [On the Role of Unobserved Sequences on Sample-based Uncertainty Quantification for LLMs](https://arxiv.org/abs/2510.04439)
*Lucie Kunitomo-Jacquin,Edison Marrese-Taylor,Ken Fukuda*

Main category: cs.CL

TL;DR: 本文主张在大型语言模型的不确定性量化中考虑未观测序列的概率，以提升量化方法的准确性。


<details>
  <summary>Details</summary>
Motivation: 在安全关键应用中，量化大型语言模型的不确定性对于发现错误答案（幻觉）至关重要。现有基于熵估计的方法主要依赖观测到的输出序列，但未考虑未观测序列的概率影响。

Method: 通过实验证明未观测序列概率在不确定性量化中的关键作用，建议未来研究将其整合到LLM不确定性量化方法中。

Result: 实验结果表明，未观测序列的概率对不确定性量化具有重要影响，忽略这一因素会限制量化方法的准确性。

Conclusion: 未来研究应该将未观测序列的概率纳入考虑，以改进大型语言模型的不确定性量化方法，特别是在安全关键应用场景中。

Abstract: Quantifying uncertainty in large language models (LLMs) is important for
safety-critical applications because it helps spot incorrect answers, known as
hallucinations. One major trend of uncertainty quantification methods is based
on estimating the entropy of the distribution of the LLM's potential output
sequences. This estimation is based on a set of output sequences and associated
probabilities obtained by querying the LLM several times. In this paper, we
advocate and experimentally show that the probability of unobserved sequences
plays a crucial role, and we recommend future research to integrate it to
enhance such LLM uncertainty quantification methods.

</details>


### [70] [Mitigating Forgetting Between Supervised and Reinforcement Learning Yields Stronger Reasoners](https://arxiv.org/abs/2510.04454)
*Xiangchi Yuan,Xiang Chen,Tong Yu,Dachuan Shi,Can Jin,Wenke Lee,Saayan Mitra*

Main category: cs.CL

TL;DR: 提出了一种动态整合监督微调(SFT)和强化学习(RL)的即插即用框架，通过选择挑战性样本进行SFT，显著减少数据需求并避免灾难性遗忘，在推理任务上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法中，RL虽然能提升推理能力但难以扩展推理边界，SFT需要大量数据且容易过拟合。结合SFT和RL面临数据效率低、算法特定设计和灾难性遗忘三大挑战。

Method: 动态选择挑战性样本进行SFT，计算高熵token的损失，冻结对RL重要的参数，实现与RL和SFT算法无关的即插即用集成。

Result: 仅使用先前最先进方法1.5%的SFT数据和20.4%的RL数据，就达到了最先进的推理性能。

Conclusion: 该方法为推理后训练中结合SFT和RL提供了高效且即插即用的解决方案。

Abstract: Large Language Models (LLMs) show strong reasoning abilities, often amplified
by Chain-of-Thought (CoT) prompting and reinforcement learning (RL). Although
RL algorithms can substantially improve reasoning, they struggle to expand
reasoning boundaries because they learn from their own reasoning trajectories
rather than acquiring external knowledge. Supervised fine-tuning (SFT) offers
complementary benefits but typically requires large-scale data and risks
overfitting. Recent attempts to combine SFT and RL face three main challenges:
data inefficiency, algorithm-specific designs, and catastrophic forgetting. We
propose a plug-and-play framework that dynamically integrates SFT into RL by
selecting challenging examples for SFT. This approach reduces SFT data
requirements and remains agnostic to the choice of RL or SFT algorithm. To
mitigate catastrophic forgetting of RL-acquired skills during SFT, we select
high-entropy tokens for loss calculation and freeze parameters identified as
critical for RL. Our method achieves state-of-the-art (SoTA) reasoning
performance using only 1.5% of the SFT data and 20.4% of the RL data used by
prior SoTA, providing an efficient and plug-and-play solution for combining SFT
and RL in reasoning post-training.

</details>


### [71] [Compressed Convolutional Attention: Efficient Attention in a Compressed Latent Space](https://arxiv.org/abs/2510.04476)
*Tomas Figliolia,Nicholas Alonso,Rishi Iyer,Quentin Anthony,Beren Millidge*

Main category: cs.CL

TL;DR: 提出压缩卷积注意力(CCA)和压缩卷积分组查询注意力(CCGQA)，通过将注意力操作在共享潜在空间中执行，同时减少参数、KV缓存和计算量，在保持性能的同时显著提升训练和推理效率。


<details>
  <summary>Details</summary>
Motivation: 多头注意力(MHA)的二次计算复杂度和线性增长的KV缓存使得长上下文transformer训练和服务成本高昂。现有方法如GQA和MLA主要减少缓存，但对计算量影响有限。

Method: CCA通过降维投影查询、键和值，在共享潜在空间中执行整个注意力操作。结合头共享技术形成CCGQA，进一步优化计算-带宽权衡。

Result: CCGQA在相同KV缓存压缩下优于GQA和MLA，在MoE模型上使用GQA/MLA一半的KV缓存即可达到标准MHA性能，实现8倍KV缓存压缩。在H100 GPU上，预填充延迟减少约1.7倍，反向传播加速约1.3倍。

Conclusion: CCA和CCGQA通过压缩注意力机制，在保持模型质量的同时显著降低了计算和内存需求，为长上下文transformer提供了高效的解决方案。

Abstract: Multi-headed Attention's (MHA) quadratic compute and linearly growing
KV-cache make long-context transformers expensive to train and serve. Prior
works such as Grouped Query Attention (GQA) and Multi-Latent Attention (MLA)
shrink the cache, speeding decode, but leave compute, which determines prefill
and training speed, largely unchanged. We introduce Compressed Convolutional
Attention (CCA), a novel attention method which down-projects queries, keys,
and values and performs the entire attention operation inside the shared latent
space. This simple design dramatically cuts parameters, KV-cache, and FLOPs all
at once by the desired compression factor. Because CCA is orthogonal to
head-sharing, we combine the two to form Compressed Convolutional Grouped Query
Attention (CCGQA), which further tightens the compute-bandwidth Pareto frontier
so that users can tune compression toward either FLOP or memory limits without
sacrificing quality. Experiments show that CCGQA consistently outperforms both
GQA and MLA at equal KV-cache compression on dense and MoE models.
Additionally, we show that CCGQA outperforms all other attention methods on MoE
models with half the KV-cache of GQA and MLA, achieving an 8x KV-cache
compression with no drop in performance compared to standard MHA. CCA and CCGQA
also dramatically reduce the FLOP cost of attention which leads to
substantially faster training and prefill than existing methods. On H100 GPUs,
our fused CCA/CCGQA kernel reduces prefill latency by about 1.7x at a sequence
length of 16k relative to MHA, and accelerates backward by about 1.3x.

</details>


### [72] [Psychological Steering in LLMs: An Evaluation of Effectiveness and Trustworthiness](https://arxiv.org/abs/2510.04484)
*Amin Banayeeanzade,Ala N. Tak,Fatemeh Bahrani,Anahita Bolourani,Leonardo Blas,Emilio Ferrara,Jonathan Gratch,Sai Praneeth Karimireddy*

Main category: cs.CL

TL;DR: PsySET是一个心理学启发的基准测试，用于评估LLM在情绪和人格领域的引导效果和可信度，涵盖多种模型和引导策略，发现不同策略各有优劣并存在特定副作用。


<details>
  <summary>Details</summary>
Motivation: 为了在社交互动场景中实现丰富、以人为本的交互，需要控制LLM模拟的情绪状态和人格特质。

Method: 引入PsySET基准测试，涵盖四个不同LLM家族的模型，结合提示、微调和表示工程等多种引导策略进行评估。

Result: 提示方法持续有效但强度控制有限，向量注入实现更精细控制但略微降低输出质量；情绪引导会产生特定副作用，如快乐会降低对抗性事实的鲁棒性、降低隐私意识、增加偏好偏见。

Conclusion: 该框架建立了首个对情绪和人格引导的全面评估，为社交互动应用中的可解释性和可靠性提供了见解。

Abstract: The ability to control LLMs' emulated emotional states and personality traits
is essential for enabling rich, human-centered interactions in socially
interactive settings. We introduce PsySET, a Psychologically-informed benchmark
to evaluate LLM Steering Effectiveness and Trustworthiness across the emotion
and personality domains. Our study spans four models from different LLM
families paired with various steering strategies, including prompting,
fine-tuning, and representation engineering. Our results indicate that
prompting is consistently effective but limited in intensity control, whereas
vector injections achieve finer controllability while slightly reducing output
quality. Moreover, we explore the trustworthiness of steered LLMs by assessing
safety, truthfulness, fairness, and ethics, highlighting potential side effects
and behavioral shifts. Notably, we observe idiosyncratic effects; for instance,
even a positive emotion like joy can degrade robustness to adversarial
factuality, lower privacy awareness, and increase preferential bias. Meanwhile,
anger predictably elevates toxicity yet strengthens leakage resistance. Our
framework establishes the first holistic evaluation of emotion and personality
steering, offering insights into its interpretability and reliability for
socially interactive applications.

</details>


### [73] [GenQuest: An LLM-based Text Adventure Game for Language Learners](https://arxiv.org/abs/2510.04498)
*Qiao Wang,Adnan Labib,Robert Swier,Michael Hofmeyr,Zheng Yuan*

Main category: cs.CL

TL;DR: GenQuest是一个基于大语言模型的生成式文本冒险游戏，通过沉浸式互动故事促进第二语言学习，为EFL学习者提供个性化内容和词汇辅助功能。


<details>
  <summary>Details</summary>
Motivation: 利用大语言模型为英语作为外语的学习者创造沉浸式、互动式的语言学习体验，通过游戏化方式提高学习动机和效果。

Method: 采用选择你自己的冒险风格叙事，动态生成响应学习者选择的故事内容，包含分支决策点和故事里程碑，提供基于学习者水平的个性化内容生成和上下文词汇解释功能。

Result: 对中国大学生EFL学习者的试点研究表明，该系统在词汇增益方面表现良好，用户感知积极，同时参与者建议改进叙事长度和质量，并增加多模态内容如图像。

Conclusion: GenQuest展示了利用生成式AI进行语言学习的潜力，通过游戏化互动叙事有效促进词汇学习，未来可进一步优化叙事质量和多模态集成。

Abstract: GenQuest is a generative text adventure game that leverages Large Language
Models (LLMs) to facilitate second language learning through immersive,
interactive storytelling. The system engages English as a Foreign Language
(EFL) learners in a collaborative "choose-your-own-adventure" style narrative,
dynamically generated in response to learner choices. Game mechanics such as
branching decision points and story milestones are incorporated to maintain
narrative coherence while allowing learner-driven plot development. Key
pedagogical features include content generation tailored to each learner's
proficiency level, and a vocabulary assistant that provides in-context
explanations of learner-queried text strings, ranging from words and phrases to
sentences. Findings from a pilot study with university EFL students in China
indicate promising vocabulary gains and positive user perceptions. Also
discussed are suggestions from participants regarding the narrative length and
quality, and the request for multi-modal content such as illustrations.

</details>


### [74] [GRACE: Generative Representation Learning via Contrastive Policy Optimization](https://arxiv.org/abs/2510.04506)
*Jiashuo Sun,Shixuan Liu,Zhaochen Su,Xianrui Zhong,Pengcheng Jiang,Bowen Jin,Peiran Li,Weijia Shi,Jiawei Han*

Main category: cs.CL

TL;DR: GRACE框架将对比信号重新构想为奖励而非损失函数，通过策略梯度优化训练LLM生成可解释的推理过程，在保持生成能力的同时提升嵌入质量


<details>
  <summary>Details</summary>
Motivation: 现有方法将LLM视为黑盒函数，丢弃了其生成和推理能力，仅产生静态嵌入。需要一种既能保持LLM生成能力又能产生高质量嵌入的方法

Method: 使用策略梯度优化，将LLM作为生成可解释推理过程的策略，通过多组件奖励函数最大化正样本对相似度并最小化负样本相似度

Result: 在MTEB基准测试中，监督设置比基础模型总体得分提升11.5%，无监督变体提升6.9%，同时保持通用能力

Conclusion: GRACE将对比目标作为推理过程的奖励，统一了表示学习和生成，产生更强的嵌入和透明的推理过程

Abstract: Prevailing methods for training Large Language Models (LLMs) as text encoders
rely on contrastive losses that treat the model as a black box function,
discarding its generative and reasoning capabilities in favor of static
embeddings. We introduce GRACE (Generative Representation Learning via
Contrastive Policy Optimization), a novel framework that reimagines contrastive
signals not as losses to be minimized, but as rewards that guide a generative
policy. In GRACE, the LLM acts as a policy that produces explicit,
human-interpretable rationales--structured natural language explanations of its
semantic understanding. These rationales are then encoded into high-quality
embeddings via mean pooling. Using policy gradient optimization, we train the
model with a multi-component reward function that maximizes similarity between
query positive pairs and minimizes similarity with negatives. This transforms
the LLM from an opaque encoder into an interpretable agent whose reasoning
process is transparent and inspectable. On MTEB benchmark, GRACE yields broad
cross category gains: averaged over four backbones, the supervised setting
improves overall score by 11.5% over base models, and the unsupervised variant
adds 6.9%, while preserving general capabilities. This work treats contrastive
objectives as rewards over rationales, unifying representation learning with
generation to produce stronger embeddings and transparent rationales. The
model, data and code are available at https://github.com/GasolSun36/GRACE.

</details>


### [75] [Fine-grained auxiliary learning for real-world product recommendation](https://arxiv.org/abs/2510.04551)
*Mario Almagro,Diego Ortego,David Jimenez*

Main category: cs.CL

TL;DR: 提出了一种名为ALC的辅助学习策略，通过学习细粒度嵌入来提升产品推荐系统的覆盖率。该方法利用批次中最难的负样本来构建正负样本间的区分性训练信号。


<details>
  <summary>Details</summary>
Motivation: 现实生产系统对覆盖率有严格要求，即大部分推荐必须自动化。尽管产品推荐是经典问题，但现有模型在实际系统中的集成往往被忽视，特别是如何满足高覆盖率需求。

Method: ALC辅助学习策略，引入两个训练目标：利用批次中最难的负样本来构建正负样本间的区分性训练信号，学习细粒度嵌入。在三个极端多标签分类方法上验证，结合最新的阈值一致性边际损失。

Result: 在两个产品推荐数据集（LF-AmazonTitles-131K和Tech and Durables）上验证，展示了与阈值一致性边际损失结合时的最先进覆盖率。

Conclusion: ALC策略能有效提升产品推荐系统的覆盖率，通过细粒度嵌入学习和最难负样本利用，在实际生产系统中具有重要应用价值。

Abstract: Product recommendation is the task of recovering the closest items to a given
query within a large product corpora. Generally, one can determine if
top-ranked products are related to the query by applying a similarity
threshold; exceeding it deems the product relevant, otherwise manual revision
is required. Despite being a well-known problem, the integration of these
models in real-world systems is often overlooked. In particular, production
systems have strong coverage requirements, i.e., a high proportion of
recommendations must be automated. In this paper we propose ALC , an Auxiliary
Learning strategy that boosts Coverage through learning fine-grained
embeddings. Concretely, we introduce two training objectives that leverage the
hardest negatives in the batch to build discriminative training signals between
positives and negatives. We validate ALC using three extreme multi-label
classification approaches in two product recommendation datasets;
LF-AmazonTitles-131K and Tech and Durables (proprietary), demonstrating
state-of-the-art coverage rates when combined with a recent
threshold-consistent margin loss.

</details>


### [76] [Can LLMs Detect Ambiguous Plural Reference? An Analysis of Split-Antecedent and Mereological Reference](https://arxiv.org/abs/2510.04581)
*Dang Anh,Rick Nouwen,Massimo Poesio*

Main category: cs.CL

TL;DR: 研究LLMs在歧义和非歧义语境中如何表示和解释复数指称，发现LLMs有时能识别歧义代词的潜在指称对象，但在选择解释时不总是遵循人类偏好，且难以在没有明确指令时识别歧义。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs是否具有类似人类的复数指称表示偏好，以及能否检测复数照应表达中的歧义并识别可能的指称对象。

Method: 设计实验：使用下一词预测任务进行代词生成、代词解释，以及使用不同提示策略进行歧义检测。

Result: LLMs有时能识别歧义代词的潜在指称对象，但在选择解释时不总是遵循人类偏好，尤其当可能的解释未被明确提及时。它们难以在没有直接指令时识别歧义，且不同类型实验结果存在不一致性。

Conclusion: LLMs在复数指称处理上表现出部分能力，但与人类存在差异，特别是在歧义识别和解释偏好方面，且结果在不同实验类型中不一致。

Abstract: Our goal is to study how LLMs represent and interpret plural reference in
ambiguous and unambiguous contexts. We ask the following research questions:
(1) Do LLMs exhibit human-like preferences in representing plural reference?
(2) Are LLMs able to detect ambiguity in plural anaphoric expressions and
identify possible referents? To address these questions, we design a set of
experiments, examining pronoun production using next-token prediction tasks,
pronoun interpretation, and ambiguity detection using different prompting
strategies. We then assess how comparable LLMs are to humans in formulating and
interpreting plural reference. We find that LLMs are sometimes aware of
possible referents of ambiguous pronouns. However, they do not always follow
human reference when choosing between interpretations, especially when the
possible interpretation is not explicitly mentioned. In addition, they struggle
to identify ambiguity without direct instruction. Our findings also reveal
inconsistencies in the results across different types of experiments.

</details>


### [77] [Robustness assessment of large audio language models in multiple-choice evaluation](https://arxiv.org/abs/2510.04584)
*Fernando López,Santosh Kesiraju,Jordi Luque*

Main category: cs.CL

TL;DR: 研究发现大音频语言模型在多项选择题评估中对选项顺序、问题表述和选项改写敏感，提出了更简单的评估协议和指标来更详细地评估模型表现。


<details>
  <summary>Details</summary>
Motivation: 现有MCQA评估框架只报告单一准确率，无法反映模型对选项顺序变化、问题改写等细微变化的敏感性，需要更全面的评估方法。

Method: 在三个基准测试(MMAU、MMAR、MMSU)和四个模型上系统研究MCQA评估框架，分析模型对选项顺序、问题改写和选项改写的敏感性。

Result: 模型不仅对选项顺序敏感，对问题表述和选项改写也表现出显著敏感性，现有评估方法无法捕捉这种变异性。

Conclusion: 需要新的评估协议和指标来考虑细微变化，为MCQA框架下的大音频语言模型提供更详细的评估报告。

Abstract: Recent advances in large audio language models (LALMs) have primarily been
assessed using a multiple-choice question answering (MCQA) framework. However,
subtle changes, such as shifting the order of choices, result in substantially
different results. Existing MCQA frameworks do not account for this variability
and report a single accuracy number per benchmark or category. We dive into the
MCQA evaluation framework and conduct a systematic study spanning three
benchmarks (MMAU, MMAR and MMSU) and four models: Audio Flamingo 2, Audio
Flamingo 3, Qwen2.5-Omni-7B-Instruct, and Kimi-Audio-7B-Instruct. Our findings
indicate that models are sensitive not only to the ordering of choices, but
also to the paraphrasing of the question and the choices. Finally, we propose a
simpler evaluation protocol and metric that account for subtle variations and
provide a more detailed evaluation report of LALMs within the MCQA framework.

</details>


### [78] [FedSRD: Sparsify-Reconstruct-Decompose for Communication-Efficient Federated Large Language Models Fine-Tuning](https://arxiv.org/abs/2510.04601)
*Guochen Yan,Luyuan Xie,Qingni Shen,Yuejian Fang,Zhonghai Wu*

Main category: cs.CL

TL;DR: FedSRD是一个通信高效的联邦学习框架，通过稀疏化-重构-分解方法解决LoRA在联邦学习中的通信瓶颈问题，能减少90%通信成本并提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前在公开网络数据上训练大语言模型的范式不可持续，高质量专业领域数据源接近枯竭。联邦学习作为下一代AI的实用解决方案，但LoRA在联邦设置中的通信开销成为关键挑战。

Method: 提出FedSRD框架：重要性感知稀疏化减少上传参数数量；服务器重构并在全秩空间聚合更新以缓解冲突；将全局更新分解为稀疏低秩格式进行广播。还提出计算开销更低的变体FedSRD-e。

Result: 在10个基准测试上的实验结果表明，该框架显著减少通信成本达90%，同时在异构客户端数据上甚至提升了模型性能。

Conclusion: FedSRD有效解决了联邦学习中LoRA的通信瓶颈问题，实现了通信效率与模型性能的双重提升。

Abstract: The current paradigm of training large language models (LLMs) on publicly
available Web data is becoming unsustainable, with high-quality data sources in
specialized domains nearing exhaustion. Federated Learning (FL) emerges as a
practical solution for the next generation of AI on a decentralized Web,
enabling privacy-preserving collaborative fine-tuning by leveraging private
data distributed across a global client base. While Low-Rank Adaptation (LoRA)
is the standard for efficient fine-tuning, its application in federated
settings presents a critical challenge: communication overhead remains a
significant bottleneck across the Web's heterogeneous network conditions. The
structural redundancy within LoRA parameters not only incurs a heavy
communication burden but also introduces conflicts when aggregating client
updates. To address this, we propose FedSRD, a Sparsify-Reconstruct-Decompose
framework designed for communication-efficient FL. We first introduce an
importance-aware sparsification method that preserves the structural integrity
of LoRA updates to reduce the uploaded parameter count. The server then
reconstructs and aggregates these updates in a full-rank space to mitigate
conflicts. Finally, it decomposes the global update into a sparse low-rank
format for broadcast, ensuring a symmetrically efficient cycle. We also propose
an efficient variant, FedSRD-e, to reduce computational overhead. Experimental
results on 10 benchmarks demonstrate that our framework significantly reduces
communication costs by up to 90\% while even improving model performance on
heterogeneous client data.

</details>


### [79] [Contrastive Learning Using Graph Embeddings for Domain Adaptation of Language Models in the Process Industry](https://arxiv.org/abs/2510.04631)
*Anastasia Zhukova,Jonas Lührs,Christian E. Matt,Bela Gipp*

Main category: cs.CL

TL;DR: SciNCL方法应用于过程工业领域，通过图感知的邻域对比学习增强语言模型，在专用基准上比mE5-large性能提升9.8-14.3%，且模型尺寸小3-5倍。


<details>
  <summary>Details</summary>
Motivation: 利用知识图谱增强预训练语言模型，捕捉过程工业文本日志中的领域特定术语和文档间关系，这些信息可能被传统方法忽略。

Method: 将SciNCL（科学邻域对比学习方法）应用于过程工业领域，从稀疏知识图谱中提取三元组进行语言模型微调。

Result: 在专有过程工业文本嵌入基准(PITEB)上，性能比最先进的mE5-large文本编码器提升9.8-14.3%，同时模型尺寸小3-5倍。

Conclusion: 图感知的邻域对比学习方法能有效提升过程工业领域文本理解性能，在保持较小模型尺寸的同时显著超越现有方法。

Abstract: Recent trends in NLP utilize knowledge graphs (KGs) to enhance pretrained
language models by incorporating additional knowledge from the graph structures
to learn domain-specific terminology or relationships between documents that
might otherwise be overlooked. This paper explores how SciNCL, a graph-aware
neighborhood contrastive learning methodology originally designed for
scientific publications, can be applied to the process industry domain, where
text logs contain crucial information about daily operations and are often
structured as sparse KGs. Our experiments demonstrate that language models
fine-tuned with triplets derived from GE outperform a state-of-the-art
mE5-large text encoder by 9.8-14.3% (5.4-8.0p) on the proprietary process
industry text embedding benchmark (PITEB) while being 3-5 times smaller in
size.

</details>


### [80] [Evaluating LLMs for Demographic-Targeted Social Bias Detection: A Comprehensive Benchmark Study](https://arxiv.org/abs/2510.04641)
*Ayan Majumdar,Feihao Chen,Jinghui Li,Xiaozhen Wang*

Main category: cs.CL

TL;DR: 该研究提出了一个评估LLMs检测人口统计学社会偏见的综合框架，发现微调的小模型在可扩展检测方面有潜力，但在多人口统计偏见检测方面仍存在差距。


<details>
  <summary>Details</summary>
Motivation: 大规模网络爬取的文本语料库包含有害的人口统计学社会偏见，需要可扩展的偏见检测方法来满足监管需求。现有研究范围狭窄，缺乏对LLMs自动偏见检测能力的全面理解。

Method: 构建了针对英语文本的评估框架，将偏见检测定义为多标签任务，使用人口统计学分类法。系统评估了不同规模和技术的模型，包括提示、上下文学习和微调，使用了12个涵盖不同内容类型和人口统计的数据集。

Result: 研究表明微调的小模型在可扩展检测方面表现出潜力，但在人口统计轴和多人口统计目标偏见方面存在持续差距。

Conclusion: 需要更有效和可扩展的审计框架来解决偏见检测中的差距，特别是在多人口统计偏见方面。

Abstract: Large-scale web-scraped text corpora used to train general-purpose AI models
often contain harmful demographic-targeted social biases, creating a regulatory
need for data auditing and developing scalable bias-detection methods. Although
prior work has investigated biases in text datasets and related detection
methods, these studies remain narrow in scope. They typically focus on a single
content type (e.g., hate speech), cover limited demographic axes, overlook
biases affecting multiple demographics simultaneously, and analyze limited
techniques. Consequently, practitioners lack a holistic understanding of the
strengths and limitations of recent large language models (LLMs) for automated
bias detection. In this study, we present a comprehensive evaluation framework
aimed at English texts to assess the ability of LLMs in detecting
demographic-targeted social biases. To align with regulatory requirements, we
frame bias detection as a multi-label task using a demographic-focused
taxonomy. We then conduct a systematic evaluation with models across scales and
techniques, including prompting, in-context learning, and fine-tuning. Using
twelve datasets spanning diverse content types and demographics, our study
demonstrates the promise of fine-tuned smaller models for scalable detection.
However, our analyses also expose persistent gaps across demographic axes and
multi-demographic targeted biases, underscoring the need for more effective and
scalable auditing frameworks.

</details>


### [81] [FT-MDT: Extracting Decision Trees from Medical Texts via a Novel Low-rank Adaptation Method](https://arxiv.org/abs/2510.04655)
*Yuheng Li,Jiechao Gao,Wei Han,Wenwen Ouyang,Wei Zhu,Hui Yi Leong*

Main category: cs.CL

TL;DR: 提出PI-LoRA方法，通过集成梯度路径信息自动从临床指南和教科书中提取医疗决策树，显著优于现有参数高效微调方法。


<details>
  <summary>Details</summary>
Motivation: 当前医疗决策树构建方法依赖耗时的人工标注，需要自动化解决方案来支持临床决策系统建设。

Method: PI-LoRA（路径集成LoRA）方法，集成梯度路径信息捕捉模块间协同效应，实现更有效的秩分配，关键模块获得适当秩分配，不重要模块被剪枝。

Result: 在医疗指南数据集上的实验表明，PI-LoRA在Text2MDT任务中显著优于现有参数高效微调方法，准确率更高且模型复杂度大幅降低。

Conclusion: 该方法实现了最先进的结果，同时保持轻量级架构，特别适合计算资源有限的临床决策支持系统。

Abstract: Knowledge of the medical decision process, which can be modeled as medical
decision trees (MDTs), is critical to building clinical decision support
systems. However, current MDT construction methods rely heavily on
time-consuming and laborious manual annotation. To address this challenge, we
propose PI-LoRA (Path-Integrated LoRA), a novel low-rank adaptation method for
automatically extracting MDTs from clinical guidelines and textbooks. We
integrate gradient path information to capture synergistic effects between
different modules, enabling more effective and reliable rank allocation. This
framework ensures that the most critical modules receive appropriate rank
allocations while less important ones are pruned, resulting in a more efficient
and accurate model for extracting medical decision trees from clinical texts.
Extensive experiments on medical guideline datasets demonstrate that our
PI-LoRA method significantly outperforms existing parameter-efficient
fine-tuning approaches for the Text2MDT task, achieving better accuracy with
substantially reduced model complexity. The proposed method achieves
state-of-the-art results while maintaining a lightweight architecture, making
it particularly suitable for clinical decision support systems where
computational resources may be limited.

</details>


### [82] [FocusMed: A Large Language Model-based Framework for Enhancing Medical Question Summarization with Focus Identification](https://arxiv.org/abs/2510.04671)
*Chao Liu,Ling Luo,Tengxiao Lv,Huan Zhuang,Lejing Yu,Jian Wang,Hongfei Lin*

Main category: cs.CL

TL;DR: 本文提出基于核心焦点指导的优化框架，通过提取忠实于原文的核心焦点、构建微调数据集和多维度质量评估，显著提升医疗问题摘要任务中焦点识别能力并减少幻觉生成。


<details>
  <summary>Details</summary>
Motivation: 在线医疗平台快速发展，消费者健康问题因冗余信息和非专业术语导致诊断效率低下。现有医疗问题摘要方法存在焦点识别差和模型幻觉问题。

Method: 设计提示模板驱动大语言模型提取忠实核心焦点，结合原始CHQ-FAQ对构建微调数据集，提出多维度质量评估与选择机制。

Result: 在两个广泛采用的MQS数据集上使用三个评估指标进行实验，提出的框架在所有指标上达到最先进性能，显著提升问题关键焦点识别能力并缓解幻觉。

Conclusion: 基于核心焦点指导的优化框架能有效解决医疗问题摘要任务中的焦点识别偏差和幻觉问题，提升摘要质量。

Abstract: With the rapid development of online medical platforms, consumer health
questions (CHQs) are inefficient in diagnosis due to redundant information and
frequent non-professional terms. The medical question summary (MQS) task aims
to transform CHQs into streamlined doctors' frequently asked questions (FAQs),
but existing methods still face challenges such as poor identification of
question focus and model hallucination. This paper explores the potential of
large language models (LLMs) in the MQS task and finds that direct fine-tuning
is prone to focus identification bias and generates unfaithful content. To this
end, we propose an optimization framework based on core focus guidance. First,
a prompt template is designed to drive the LLMs to extract the core focus from
the CHQs that is faithful to the original text. Then, a fine-tuning dataset is
constructed in combination with the original CHQ-FAQ pairs to improve the
ability to identify the focus of the question. Finally, a multi-dimensional
quality evaluation and selection mechanism is proposed to comprehensively
improve the quality of the summary from multiple dimensions. We conduct
comprehensive experiments on two widely-adopted MQS datasets using three
established evaluation metrics. The proposed framework achieves
state-of-the-art performance across all measures, demonstrating a significant
boost in the model's ability to identify critical focus of questions and a
notable mitigation of hallucinations. The source codes are freely available at
https://github.com/DUT-LiuChao/FocusMed.

</details>


### [83] [Multi-Agent Tool-Integrated Policy Optimization](https://arxiv.org/abs/2510.04678)
*Zhanfeng Mo,Xingxuan Li,Yuntao Chen,Lidong Bing*

Main category: cs.CL

TL;DR: 提出了MATPO方法，在单个LLM中通过强化学习训练规划者和执行者两种角色，解决多轮工具集成规划中的上下文限制和噪声工具响应问题。


<details>
  <summary>Details</summary>
Motivation: 现有单智能体方法存在上下文长度限制和噪声工具响应问题，多智能体框架需要内存密集型部署多个LLM，缺乏有效的强化学习后训练方法。

Method: MATPO通过角色特定提示在单个LLM实例中训练不同角色（规划者和执行者），基于原则性的信用分配机制在规划者和执行者rollouts之间进行分配。

Result: 在GAIA-text、WebWalkerQA和FRAMES数据集上，MATPO平均相对性能提升18.38%，对噪声工具输出表现出更强的鲁棒性。

Conclusion: 在单个LLM中统一多个智能体角色是有效的，为稳定高效的多智能体强化学习训练提供了实用见解。

Abstract: Large language models (LLMs) increasingly rely on multi-turn tool-integrated
planning for knowledge-intensive and complex reasoning tasks. Existing
implementations typically rely on a single agent, but they suffer from limited
context length and noisy tool responses. A natural solution is to adopt a
multi-agent framework with planner- and worker-agents to manage context.
However, no existing methods support effective reinforcement learning
post-training of tool-integrated multi-agent frameworks. To address this gap,
we propose Multi-Agent Tool-Integrated Policy Optimization (MATPO), which
enables distinct roles (planner and worker) to be trained within a single LLM
instance using role-specific prompts via reinforcement learning. MATPO is
derived from a principled credit assignment mechanism across planner and worker
rollouts. This design eliminates the need to deploy multiple LLMs, which would
be memory-intensive, while preserving the benefits of specialization.
Experiments on GAIA-text, WebWalkerQA, and FRAMES show that MATPO consistently
outperforms single-agent baselines by an average of 18.38% relative improvement
in performance and exhibits greater robustness to noisy tool outputs. Our
findings highlight the effectiveness of unifying multiple agent roles within a
single LLM and provide practical insights for stable and efficient multi-agent
RL training.

</details>


### [84] [TiTok: Transfer Token-level Knowledge via Contrastive Excess to Transplant LoRA](https://arxiv.org/abs/2510.04682)
*Chanjoo Jung,Jaehyung Kim*

Main category: cs.CL

TL;DR: TiTok是一个新的框架，通过token级知识转移实现有效的LoRA移植，无需额外模型或开销，在多个基准测试中平均性能提升4-8%。


<details>
  <summary>Details</summary>
Motivation: 解决PEFT方法如LoRA中适配参数依赖于基础模型且无法在不同骨干网络间迁移的问题，避免知识蒸馏对训练数据的依赖和TransLoRA需要额外判别器模型的复杂性。

Method: 通过对比源模型有无LoRA之间的差异来捕获任务相关信息，突出信息丰富的token并实现合成数据的选择性过滤。

Result: 在三个基准测试的多个迁移设置中，该方法持续有效，相比基线平均性能提升4-8%。

Conclusion: TiTok框架能够有效实现LoRA移植，通过token级知识转移避免了额外模型需求，在多种设置下均表现出色。

Abstract: Large Language Models (LLMs) are widely applied in real world scenarios, but
fine-tuning them comes with significant computational and storage costs.
Parameter-Efficient Fine-Tuning (PEFT) methods such as LoRA mitigate these
costs, but the adapted parameters are dependent on the base model and cannot be
transferred across different backbones. One way to address this issue is
through knowledge distillation, but its effectiveness inherently depends on
training data. Recent work such as TransLoRA avoids this by generating
synthetic data, but this adds complexity because it requires training an
additional discriminator model. In this paper, we propose TiTok, a new
framework that enables effective LoRA Transplantation through Token-level
knowledge transfer. Specifically, TiTok captures task-relevant information
through a contrastive excess between a source model with and without LoRA. This
excess highlights informative tokens and enables selective filtering of
synthetic data, all without additional models or overhead. Through experiments
on three benchmarks across multiple transfer settings, our experiments show
that the proposed method is consistently effective, achieving average
performance gains of +4~8% compared to baselines overall.

</details>


### [85] [Multilingual Routing in Mixture-of-Experts](https://arxiv.org/abs/2510.04694)
*Lucas Bandarkar,Chenyuan Yang,Mohsen Fayyaz,Junlin Hu,Nanyun Peng*

Main category: cs.CL

TL;DR: 分析了MoE模型在多语言数据中的路由模式，发现在中间层存在跨语言路由对齐现象，并提出了一种通过促进英语激活的专家来提高多语言性能的干预方法。


<details>
  <summary>Details</summary>
Motivation: 理解MoE架构在多语言数据中的稀疏路由动态，探索如何提高模型在非英语语言上的性能。

Method: 使用并行多语言数据集分析专家路由模式，提出在推理时通过促进中间层英语任务专家来引导路由器的干预方法。

Result: 干预方法在多个评估任务、模型和15+种语言上获得了1-2%的稳定性能提升，而其他干预方式则导致性能下降。

Conclusion: MoE模型处理非英语文本的能力受限于其利用语言通用专家的能力，中间层的跨语言路由对齐与模型性能密切相关。

Abstract: Mixture-of-Experts (MoE) architectures have become the key to scaling modern
LLMs, yet little is understood about how their sparse routing dynamics respond
to multilingual data. In this work, we analyze expert routing patterns using
parallel multilingual datasets and present highly interpretable layer-wise
phenomena. We find that MoE models route tokens in language-specific ways in
the early and late decoder layers but exhibit significant cross-lingual routing
alignment in middle layers, mirroring parameter-sharing trends observed in
dense LLMs. In particular, we reveal a clear, strong correlation between a
model's performance in a given language and how similarly its tokens are routed
to English in these layers. Extending beyond correlation, we explore
inference-time interventions that induce higher cross-lingual routing
alignment. We introduce a method that steers the router by promoting
middle-layer task experts frequently activated in English, and it successfully
increases multilingual performance. These 1-2% gains are remarkably consistent
across two evaluation tasks, three models, and 15+ languages, especially given
that these simple interventions override routers of extensively trained,
state-of-the-art LLMs. In comparison, interventions outside of the middle
layers or targeting multilingual-specialized experts only yield performance
degradation. Altogether, we present numerous findings that explain how MoEs
process non-English text and demonstrate that generalization is limited by the
model's ability to leverage language-universal experts in all languages.

</details>


### [86] [JSON Whisperer: Efficient JSON Editing with LLMs](https://arxiv.org/abs/2510.04717)
*Sarel Duanis,Asnat Greenstein-Messica,Eliya Habba*

Main category: cs.CL

TL;DR: JSON Whisperer是一个让LLMs生成JSON差异补丁而非完整文档的框架，通过EASE编码解决数组索引问题，减少31%的token使用量。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs编辑JSON文档时需要重新生成整个结构，计算效率低下。

Method: 提出JSON Whisperer框架，使用RFC 6902差异补丁，引入EASE编码将数组转换为具有稳定键的字典。

Result: 补丁生成结合EASE减少了31%的token使用，编辑质量保持在完全重新生成的5%以内，在复杂指令和列表操作中表现更佳。

Conclusion: JSON Whisperer通过差异补丁和EASE编码有效提高了LLMs编辑JSON的效率，特别适用于复杂操作。

Abstract: Large language models (LLMs) can modify JSON documents through natural
language commands, but current approaches regenerate entire structures for each
edit, resulting in computational inefficiency. We present JSON Whisperer, a
framework that enables LLMs to generate RFC 6902 diff patches-expressing only
the necessary modifications-rather than complete documents. We identify two key
challenges in patch-based editing: (1) LLMs often miss related updates when
generating isolated patches, and (2) array manipulations require tracking index
shifts across operations, which LLMs handle poorly. To address these issues, we
introduce EASE (Explicitly Addressed Sequence Encoding), which transforms
arrays into dictionaries with stable keys, eliminating index arithmetic
complexities. Our evaluation shows that patch generation with EASE reduces
token usage by 31% while maintaining edit quality within 5% of full
regeneration with particular gains for complex instructions and list
manipulations. The dataset is available at:
https://github.com/emnlp2025/JSON-Whisperer/

</details>


### [87] [A Low-Resource Speech-Driven NLP Pipeline for Sinhala Dyslexia Assistance](https://arxiv.org/abs/2510.04750)
*Peshala Perera,Deshan Sumanathilaka*

Main category: cs.CL

TL;DR: 为僧伽罗语阅读障碍成人开发的多模态辅助系统，整合语音转文字、错误识别和文本校正功能，在资源有限的语言环境中取得了可行效果


<details>
  <summary>Details</summary>
Motivation: 成人阅读障碍在非英语环境中研究不足，特别是僧伽罗语等低资源语言缺乏语言可及性工具，需要开发专门的辅助技术

Method: 集成Whisper进行语音转文字，使用SinBERT识别常见阅读障碍错误，结合mT5和Mistral模型生成校正文本，最后用gTTS转回语音形成完整多模态反馈循环

Result: 在僧伽罗语数据集有限的情况下，系统达到0.66的转录准确率、0.7的校正准确率和0.65的整体系统准确率

Conclusion: 该方法证明了在代表性不足语言中开发包容性自然语言处理技术的可行性和有效性

Abstract: Dyslexia in adults remains an under-researched and under-served area,
particularly in non-English-speaking contexts, despite its significant impact
on personal and professional lives. This work addresses that gap by focusing on
Sinhala, a low-resource language with limited tools for linguistic
accessibility. We present an assistive system explicitly designed for
Sinhala-speaking adults with dyslexia. The system integrates Whisper for
speech-to-text conversion, SinBERT, an open-sourced fine-tuned BERT model
trained for Sinhala to identify common dyslexic errors, and a combined mT5 and
Mistral-based model to generate corrected text. Finally, the output is
converted back to speech using gTTS, creating a complete multimodal feedback
loop. Despite the challenges posed by limited Sinhala-language datasets, the
system achieves 0.66 transcription accuracy and 0.7 correction accuracy with
0.65 overall system accuracy. These results demonstrate both the feasibility
and effectiveness of the approach. Ultimately, this work highlights the
importance of inclusive Natural Language Processing (NLP) technologies in
underrepresented languages and showcases a practical

</details>


### [88] [ModernBERT + ColBERT: Enhancing biomedical RAG through an advanced re-ranking retriever](https://arxiv.org/abs/2510.04757)
*Eduardo Martínez Rivera,Filippo Menolascina*

Main category: cs.CL

TL;DR: 提出了一种两阶段检索架构，结合轻量级ModernBERT编码器和ColBERTv2重排序模型，在生物医学RAG系统中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 解决通用密集检索器在专业领域语言理解不足，而领域内模型计算成本过高的问题，提升生物医学RAG系统的检索质量。

Method: 使用ModernBERT进行初始候选检索，ColBERTv2进行细粒度重排序，在PubMedQA数据集上微调IR模块。

Result: ColBERT重排序使Recall@3提升4.2个百分点，在MIRAGE基准测试中达到0.4448的平均准确率，优于MedCPT基线。

Conclusion: 两阶段检索架构有效平衡了效率和精度，联合微调对性能至关重要，否则重排序可能损害性能。

Abstract: Retrieval-Augmented Generation (RAG) is a powerful technique for enriching
Large Language Models (LLMs) with external knowledge, allowing for factually
grounded responses, a critical requirement in high-stakes domains such as
healthcare. However, the efficacy of RAG systems is fundamentally restricted by
the performance of their retrieval module, since irrelevant or semantically
misaligned documents directly compromise the accuracy of the final generated
response. General-purpose dense retrievers can struggle with the nuanced
language of specialised domains, while the high accuracy of in-domain models is
often achieved at prohibitive computational costs. In this work, we aim to
address this trade-off by developing and evaluating a two-stage retrieval
architecture that combines a lightweight ModernBERT bidirectional encoder for
efficient initial candidate retrieval with a ColBERTv2 late-interaction model
for fine-grained re-ranking. We conduct comprehensive evaluations of our
retriever module performance and RAG system performance in the biomedical
context, fine-tuning the IR module using 10k question-passage pairs from
PubMedQA. Our analysis of the retriever module confirmed the positive impact of
the ColBERT re-ranker, which improved Recall@3 by up to 4.2 percentage points
compared to its retrieve-only counterpart. When integrated into the biomedical
RAG, our IR module leads to a state-of-the-art average accuracy of 0.4448 on
the five tasks of the MIRAGE question-answering benchmark, outperforming strong
baselines such as MedCPT (0.4436). Our ablation studies reveal that this
performance is critically dependent on a joint fine-tuning process that aligns
the retriever and re-ranker; otherwise, the re-ranker might degrade the
performance.

</details>


### [89] [Are BabyLMs Deaf to Gricean Maxims? A Pragmatic Evaluation of Sample-efficient Language Models](https://arxiv.org/abs/2510.04764)
*Raha Askari,Sina Zarrieß,Özge Alacam,Judith Sieker*

Main category: cs.CL

TL;DR: 该论文提出了一个评估语言模型识别Grice会话准则违反的新基准，比较了在不同规模数据上预训练的BabyLMs与儿童和大语言模型的性能差异。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型是否能像人类一样识别和解释隐含意义，特别是通过检测Grice会话准则的违反来理解语用推理。

Method: 基于Surian等人对儿童识别Grice准则违反的研究，创建新基准测试，比较在少于10M和100M token上预训练的BabyLMs与儿童及在3T token上训练的LLM的性能。

Result: 在少于100M token上训练的模型优于在10M token上训练的模型，但仍未达到儿童和LLM的水平。数据量的适度增加能改善某些语用行为，实现更细粒度的语用维度区分。

Conclusion: 小规模语言模型在识别语用准则违反方面有所进步，但与人类儿童和大语言模型相比仍有差距，表明语用能力的获得需要更复杂的学习机制。

Abstract: Implicit meanings are integral to human communication, making it essential
for language models to be capable of identifying and interpreting them. Grice
(1975) proposed a set of conversational maxims that guide cooperative dialogue,
noting that speakers may deliberately violate these principles to express
meanings beyond literal words, and that listeners, in turn, recognize such
violations to draw pragmatic inferences.
  Building on Surian et al. (1996)'s study of children's sensitivity to
violations of Gricean maxims, we introduce a novel benchmark to test whether
language models pretrained on less than 10M and less than 100M tokens can
distinguish maxim-adhering from maxim-violating utterances. We compare these
BabyLMs across five maxims and situate their performance relative to children
and a Large Language Model (LLM) pretrained on 3T tokens.
  We find that overall, models trained on less than 100M tokens outperform
those trained on less than 10M, yet fall short of child-level and LLM
competence. Our results suggest that modest data increases improve some aspects
of pragmatic behavior, leading to finer-grained differentiation between
pragmatic dimensions.

</details>


### [90] [Hybrid Architectures for Language Models: Systematic Analysis and Design Insights](https://arxiv.org/abs/2510.04800)
*Sangmin Bae,Bilge Acun,Haroun Habeeb,Seungyeon Kim,Chien-Yu Lin,Liang Luo,Junjie Wang,Carole-Jean Wu*

Main category: cs.CL

TL;DR: 本文系统评估了基于自注意力机制与Mamba等结构化状态空间模型的混合架构，比较了层间（顺序）和层内（并行）融合策略，分析了影响混合模型性能的关键因素，并提出了最优设计方法。


<details>
  <summary>Details</summary>
Motivation: 虽然混合架构在建模质量和计算效率方面表现出色，但社区缺乏对混合策略的系统比较和有效性关键因素的分析。

Method: 采用整体评估方法，从语言建模性能、长上下文能力、扩展分析以及训练和推理效率等多个角度评估混合架构设计。

Result: 通过研究计算原语的核心特征，识别了每种混合策略的最关键元素，并提出了两种混合模型的最优设计方法。

Conclusion: 全面分析为开发混合语言模型提供了实用指导和宝贵见解，有助于优化架构配置。

Abstract: Recent progress in large language models demonstrates that hybrid
architectures--combining self-attention mechanisms with structured state space
models like Mamba--can achieve a compelling balance between modeling quality
and computational efficiency, particularly for long-context tasks. While these
hybrid models show promising performance, systematic comparisons of
hybridization strategies and analyses on the key factors behind their
effectiveness have not been clearly shared to the community. In this work, we
present a holistic evaluation of hybrid architectures based on inter-layer
(sequential) or intra-layer (parallel) fusion. We evaluate these designs from a
variety of perspectives: language modeling performance, long-context
capabilities, scaling analysis, and training and inference efficiency. By
investigating the core characteristics of their computational primitive, we
identify the most critical elements for each hybridization strategy and further
propose optimal design recipes for both hybrid models. Our comprehensive
analysis provides practical guidance and valuable insights for developing
hybrid language models, facilitating the optimization of architectural
configurations.

</details>


### [91] [How I Built ASR for Endangered Languages with a Spoken Dictionary](https://arxiv.org/abs/2510.04832)
*Christopher Bartley,Anton Ragni*

Main category: cs.CL

TL;DR: 研究表明，使用短格式发音资源仅需40分钟数据就能为濒危语言构建可用的自动语音识别系统，显著降低了技术门槛。


<details>
  <summary>Details</summary>
Motivation: 全球近半数语言濒临灭绝，传统语音识别系统需要句子级标注数据，而大多数濒危语言缺乏这种格式的数据，导致无法获得技术支持。

Method: 使用短格式发音资源替代传统句子级标注数据，在曼克斯盖尔语和康沃尔语上进行实验验证。

Result: 仅需40分钟的短格式发音数据就能为曼克斯盖尔语构建词错误率低于50%的可用ASR系统，在康沃尔语上成功复现该方法。

Conclusion: 构建濒危语言ASR系统的数据需求远低于传统认知，为无法满足传统数据要求的濒危语言社区带来了希望。

Abstract: Nearly half of the world's languages are endangered. Speech technologies such
as Automatic Speech Recognition (ASR) are central to revival efforts, yet most
languages remain unsupported because standard pipelines expect utterance-level
supervised data. Speech data often exist for endangered languages but rarely
match these formats. Manx Gaelic ($\sim$2,200 speakers), for example, has had
transcribed speech since 1948, yet remains unsupported by modern systems. In
this paper, we explore how little data, and in what form, is needed to build
ASR for critically endangered languages. We show that a short-form
pronunciation resource is a viable alternative, and that 40 minutes of such
data produces usable ASR for Manx ($<$50\% WER). We replicate our approach,
applying it to Cornish ($\sim$600 speakers), another critically endangered
language. Results show that the barrier to entry, in quantity and form, is far
lower than previously thought, giving hope to endangered language communities
that cannot afford to meet the requirements arbitrarily imposed upon them.

</details>


### [92] [Instability in Downstream Task Performance During LLM Pretraining](https://arxiv.org/abs/2510.04848)
*Yuto Nishida,Masaru Isonuma,Yusuke Oda*

Main category: cs.CL

TL;DR: 该论文分析了大型语言模型训练过程中下游任务性能的不稳定性，并提出了两种后处理检查点集成方法（平均和集成）来减少性能波动。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型训练时，下游任务性能存在显著波动，难以确定真正的最佳检查点，这影响了模型选择的效果。

Method: 提出了两种后处理检查点集成方法：检查点平均和检查点集成，通过聚合相邻检查点来减少性能波动。

Result: 经验性和理论分析表明，这些方法能够提高下游任务性能的稳定性，且无需改变训练过程。

Conclusion: 检查点集成方法能有效缓解训练过程中的性能波动问题，为模型选择提供了更可靠的依据。

Abstract: When training large language models (LLMs), it is common practice to track
downstream task performance throughout the training process and select the
checkpoint with the highest validation score. However, downstream metrics often
exhibit substantial fluctuations, making it difficult to identify the
checkpoint that truly represents the best-performing model. In this study, we
empirically analyze the stability of downstream task performance in an LLM
trained on diverse web-scale corpora. We find that task scores frequently
fluctuate throughout training, both at the aggregate and example levels. To
address this instability, we investigate two post-hoc checkpoint integration
methods: checkpoint averaging and ensemble, motivated by the hypothesis that
aggregating neighboring checkpoints can reduce performance volatility. We
demonstrate both empirically and theoretically that these methods improve
downstream performance stability without requiring any changes to the training
procedure.

</details>


### [93] [When Models Lie, We Learn: Multilingual Span-Level Hallucination Detection with PsiloQA](https://arxiv.org/abs/2510.04849)
*Elisei Rykov,Kseniia Petrushina,Maksim Savkin,Valerii Olisov,Artem Vazhentsev,Kseniia Titova,Alexander Panchenko,Vasily Konovalov,Julia Belikova*

Main category: cs.CL

TL;DR: PsiloQA是一个大规模多语言数据集，包含14种语言的span级幻觉标注，通过自动化流水线构建，用于评估幻觉检测方法。


<details>
  <summary>Details</summary>
Motivation: 现有幻觉基准主要在序列级别且仅限于英语，缺乏细粒度、多语言的监督，无法全面评估大语言模型的幻觉问题。

Method: 使用三阶段自动化流水线：用GPT-4o从维基百科生成问答对，在无上下文设置下从多样化LLMs获取可能幻觉的答案，用GPT-4o通过对比黄金答案和检索上下文自动标注幻觉span。

Result: 评估多种幻觉检测方法，发现基于编码器的模型在所有语言中表现最强，PsiloQA展示有效的跨语言泛化能力，并支持向其他基准的稳健知识迁移。

Conclusion: PsiloQA数据集和结果推动了多语言环境下可扩展、细粒度幻觉检测的发展，且比人工标注数据集更具成本效益。

Abstract: Hallucination detection remains a fundamental challenge for the safe and
reliable deployment of large language models (LLMs), especially in applications
requiring factual accuracy. Existing hallucination benchmarks often operate at
the sequence level and are limited to English, lacking the fine-grained,
multilingual supervision needed for a comprehensive evaluation. In this work,
we introduce PsiloQA, a large-scale, multilingual dataset annotated with
span-level hallucinations across 14 languages. PsiloQA is constructed through
an automated three-stage pipeline: generating question-answer pairs from
Wikipedia using GPT-4o, eliciting potentially hallucinated answers from diverse
LLMs in a no-context setting, and automatically annotating hallucinated spans
using GPT-4o by comparing against golden answers and retrieved context. We
evaluate a wide range of hallucination detection methods -- including
uncertainty quantification, LLM-based tagging, and fine-tuned encoder models --
and show that encoder-based models achieve the strongest performance across
languages. Furthermore, PsiloQA demonstrates effective cross-lingual
generalization and supports robust knowledge transfer to other benchmarks, all
while being significantly more cost-efficient than human-annotated datasets.
Our dataset and results advance the development of scalable, fine-grained
hallucination detection in multilingual settings.

</details>


### [94] [Detecting Distillation Data from Reasoning Models](https://arxiv.org/abs/2510.04850)
*Hengxiang Zhang,Hyeong Kyu Choi,Yixuan Li,Hongxin Wei*

Main category: cs.CL

TL;DR: 提出Token Probability Deviation (TBD)方法，通过分析生成token的概率模式来检测推理蒸馏中的基准污染问题。


<details>
  <summary>Details</summary>
Motivation: 推理蒸馏可能导致基准污染，即蒸馏数据集中的评估数据会人为提升蒸馏模型的性能指标。

Method: 利用蒸馏模型对已见问题生成接近确定性token、对未见问题生成更多低概率token的特点，量化生成token概率与高参考概率的偏差。

Result: 在S1数据集上达到0.918的AUC和0.470的TPR@1% FPR，表现优异。

Conclusion: TBD方法能有效检测蒸馏数据，解决推理蒸馏中的基准污染问题。

Abstract: Reasoning distillation has emerged as an efficient and powerful paradigm for
enhancing the reasoning capabilities of large language models. However,
reasoning distillation may inadvertently cause benchmark contamination, where
evaluation data included in distillation datasets can inflate performance
metrics of distilled models. In this work, we formally define the task of
distillation data detection, which is uniquely challenging due to the partial
availability of distillation data. Then, we propose a novel and effective
method Token Probability Deviation (TBD), which leverages the probability
patterns of the generated output tokens. Our method is motivated by the
analysis that distilled models tend to generate near-deterministic tokens for
seen questions, while producing more low-probability tokens for unseen
questions. Our key idea behind TBD is to quantify how far the generated tokens'
probabilities deviate from a high reference probability. In effect, our method
achieves competitive detection performance by producing lower scores for seen
questions than for unseen questions. Extensive experiments demonstrate the
effectiveness of our method, achieving an AUC of 0.918 and a TPR@1% FPR of
0.470 on the S1 dataset.

</details>


### [95] [SocialHarmBench: Revealing LLM Vulnerabilities to Socially Harmful Requests](https://arxiv.org/abs/2510.04891)
*Punya Syon Pandey,Hai Son Le,Devansh Bhardwaj,Rada Mihalcea,Zhijing Jin*

Main category: cs.CL

TL;DR: SocialHarmBench是一个包含585个提示的数据集，涵盖7个社会政治类别和34个国家，用于测试LLM在政治敏感语境中的脆弱性。研究发现开源模型在有害指令遵循方面存在高风险，特别是在历史修正主义、宣传和政治操纵等领域。


<details>
  <summary>Details</summary>
Motivation: 现有安全基准很少测试LLM在政治操纵、宣传生成、监控和信息控制等领域的漏洞，而这些领域的失败可能带来直接的社会政治后果。

Method: 构建SocialHarmBench数据集，包含585个提示，涵盖7个社会政治类别和34个国家，评估LLM在政治敏感语境中的表现。

Result: 开源模型表现出高度脆弱性，Mistral-7B在历史修正主义、宣传和政治操纵等领域的攻击成功率高达97%-98%。时间分析显示LLM在21世纪和前20世纪语境中最脆弱，地理分析显示在拉丁美洲、美国和英国等地区最易受影响。

Conclusion: 当前的安全防护措施无法推广到高风险的社会政治环境中，暴露了系统性偏见，引发了对LLM在保护人权和民主价值观方面可靠性的担忧。

Abstract: Large language models (LLMs) are increasingly deployed in contexts where
their failures can have direct sociopolitical consequences. Yet, existing
safety benchmarks rarely test vulnerabilities in domains such as political
manipulation, propaganda and disinformation generation, or surveillance and
information control. We introduce SocialHarmBench, a dataset of 585 prompts
spanning 7 sociopolitical categories and 34 countries, designed to surface
where LLMs most acutely fail in politically charged contexts. Our evaluations
reveal several shortcomings: open-weight models exhibit high vulnerability to
harmful compliance, with Mistral-7B reaching attack success rates as high as
97% to 98% in domains such as historical revisionism, propaganda, and political
manipulation. Moreover, temporal and geographic analyses show that LLMs are
most fragile when confronted with 21st-century or pre-20th-century contexts,
and when responding to prompts tied to regions such as Latin America, the USA,
and the UK. These findings demonstrate that current safeguards fail to
generalize to high-stakes sociopolitical settings, exposing systematic biases
and raising concerns about the reliability of LLMs in preserving human rights
and democratic values. We share the SocialHarmBench benchmark at
https://huggingface.co/datasets/psyonp/SocialHarmBench.

</details>


### [96] [Do LLMs Align with My Task? Evaluating Text-to-SQL via Dataset Alignment](https://arxiv.org/abs/2510.04919)
*Davood Rafiei,Morgan Lindsay Heisler,Weiwei Zhang,Mohammadreza Pourreza,Yong Zhang*

Main category: cs.CL

TL;DR: 研究监督微调(SFT)在NL2SQL任务中训练数据与目标查询的结构对齐问题，发现结构对齐度是微调成功的重要预测指标


<details>
  <summary>Details</summary>
Motivation: 训练数据的变异性会阻碍模型在不同领域的泛化能力，需要研究训练数据与目标查询的结构对齐如何影响模型性能

Method: 通过比较训练集、目标数据和模型预测中SQL结构特征的分布来估计对齐度，在三个大型跨域NL2SQL基准和多个模型家族上进行综合实验

Result: 结构对齐度是微调成功的强预测指标：对齐度高时SFT带来准确率和SQL生成质量的显著提升；对齐度低时改进很小或没有

Conclusion: 在NL2SQL任务中，对齐感知的数据选择对于有效的微调和泛化至关重要

Abstract: Supervised Fine-Tuning (SFT) is an effective method for adapting Large
Language Models (LLMs) on downstream tasks. However, variability in training
data can hinder a model's ability to generalize across domains. This paper
studies the problem of dataset alignment for Natural Language to SQL (NL2SQL or
text to SQL), examining how well SFT training data matches the structural
characteristics of target queries and how this alignment impacts model
performance. We hypothesize that alignment can be accurately estimated by
comparing the distributions of structural SQL features across the training set,
target data, and the model's predictions prior to SFT. Through comprehensive
experiments on three large cross-domain NL2SQL benchmarks and multiple model
families, we show that structural alignment is a strong predictor of
fine-tuning success. When alignment is high, SFT yields substantial gains in
accuracy and SQL generation quality; when alignment is low, improvements are
marginal or absent. These findings highlight the importance of alignment-aware
data selection for effective fine-tuning and generalization in NL2SQL tasks.

</details>


### [97] [The Geometry of Truth: Layer-wise Semantic Dynamics for Hallucination Detection in Large Language Models](https://arxiv.org/abs/2510.04933)
*Amir Hameed Mir*

Main category: cs.CL

TL;DR: 提出Layer-wise Semantic Dynamics (LSD)框架，通过分析transformer层间隐藏状态语义的演化来检测大语言模型的幻觉现象，相比现有方法更高效且无需外部验证。


<details>
  <summary>Details</summary>
Motivation: 大语言模型经常产生流畅但事实错误的陈述（幻觉现象），这在高风险领域构成严重风险，需要开发有效的检测方法。

Method: 使用基于边界的对比学习，将隐藏激活与事实编码器生成的ground-truth嵌入对齐，分析语义轨迹的分离：事实回答保持稳定对齐，而幻觉在深度上表现出明显的语义漂移。

Result: 在TruthfulQA和合成事实-幻觉数据集上评估，LSD达到F1分数0.92、AUROC 0.96和聚类准确率0.89，优于SelfCheckGPT和语义熵基线，仅需单次前向传播，速度提升5-20倍。

Conclusion: LSD提供了一个可扩展、模型无关的实时幻觉监测机制，并为大语言模型中事实一致性的几何特性提供了新见解。

Abstract: Large Language Models (LLMs) often produce fluent yet factually incorrect
statements-a phenomenon known as hallucination-posing serious risks in
high-stakes domains. We present Layer-wise Semantic Dynamics (LSD), a geometric
framework for hallucination detection that analyzes the evolution of
hidden-state semantics across transformer layers. Unlike prior methods that
rely on multiple sampling passes or external verification sources, LSD operates
intrinsically within the model's representational space. Using margin-based
contrastive learning, LSD aligns hidden activations with ground-truth
embeddings derived from a factual encoder, revealing a distinct separation in
semantic trajectories: factual responses preserve stable alignment, while
hallucinations exhibit pronounced semantic drift across depth. Evaluated on the
TruthfulQA and synthetic factual-hallucination datasets, LSD achieves an
F1-score of 0.92, AUROC of 0.96, and clustering accuracy of 0.89, outperforming
SelfCheckGPT and Semantic Entropy baselines while requiring only a single
forward pass. This efficiency yields a 5-20x speedup over sampling-based
methods without sacrificing precision or interpretability. LSD offers a
scalable, model-agnostic mechanism for real-time hallucination monitoring and
provides new insights into the geometry of factual consistency within large
language models.

</details>


### [98] [A First Context-Free Grammar Applied to Nawatl Corpora Augmentation](https://arxiv.org/abs/2510.04945)
*Juan-José Guzmán-Landa,Juan-Manuel Torres-Moreno,Miguel Figueroa-Saavedra,Ligia Quintana-Torres,Martha-Lorena Avendaño-Garrido,Graham Ranger*

Main category: cs.CL

TL;DR: 本文为纳瓦特尔语构建了一个上下文无关文法，用于生成大量语法正确的合成句子，以扩充该低资源语言的语料库，从而改善语言模型的训练效果。


<details>
  <summary>Details</summary>
Motivation: 纳瓦特尔语是一种数字资源匮乏的π型语言，缺乏足够的语料库用于机器学习。研究旨在通过文法生成人工句子来扩充语料库，解决数据稀缺问题。

Method: 为纳瓦特尔语设计上下文无关文法(CFG)，生成语法正确的合成句子来扩充π-yalli语料库，然后使用FastText等算法进行训练和语义任务评估。

Result: 初步结果显示，使用文法生成的方法相比某些大型语言模型取得了比较性改进，但要获得更显著的提升需要更有效地建模纳瓦特尔语的文法。

Conclusion: 文法生成是扩充低资源语言语料库的有效方法，但需要开发更精确的语言模型来进一步提升性能。

Abstract: In this article we introduce a context-free grammar (CFG) for the Nawatl
language. Nawatl (or Nahuatl) is an Amerindian language of the $\pi$-language
type, i.e. a language with few digital resources, in which the corpora
available for machine learning are virtually non-existent. The objective here
is to generate a significant number of grammatically correct artificial
sentences, in order to increase the corpora available for language model
training. We want to show that a grammar enables us significantly to expand a
corpus in Nawatl which we call $\pi$-\textsc{yalli}. The corpus, thus enriched,
enables us to train algorithms such as FastText and to evaluate them on
sentence-level semantic tasks. Preliminary results show that by using the
grammar, comparative improvements are achieved over some LLMs. However, it is
observed that to achieve more significant improvement, grammars that model the
Nawatl language even more effectively are required.

</details>


### [99] [Mind Your Tone: Investigating How Prompt Politeness Affects LLM Accuracy (short paper)](https://arxiv.org/abs/2510.04950)
*Om Dobariya,Akhil Kumar*

Main category: cs.CL

TL;DR: 研究发现，在ChatGPT 4o中，不礼貌的提示词比礼貌提示词表现更好，准确率从非常礼貌的80.8%提升到非常粗鲁的84.8%。


<details>
  <summary>Details</summary>
Motivation: 探索自然语言提示中礼貌程度和语气对大型语言模型性能的影响，特别是之前研究中礼貌与粗鲁对结果影响不一致的问题。

Method: 创建包含50个基础问题的数据集，涵盖数学、科学和历史领域，每个问题重写为5种语气变体（非常礼貌、礼貌、中性、粗鲁、非常粗鲁），共250个独特提示，使用ChatGPT 4o评估响应并进行配对样本t检验。

Result: 与预期相反，不礼貌提示持续优于礼貌提示，准确率随礼貌程度降低而提高：非常礼貌80.8%，礼貌81.6%，中性82.8%，粗鲁83.6%，非常粗鲁84.8%。

Conclusion: 新的大型语言模型可能对语气变化有不同的响应方式，研究强调了研究提示语用学方面的重要性，并提出了关于人机交互社会维度的更广泛问题。

Abstract: The wording of natural language prompts has been shown to influence the
performance of large language models (LLMs), yet the role of politeness and
tone remains underexplored. In this study, we investigate how varying levels of
prompt politeness affect model accuracy on multiple-choice questions. We
created a dataset of 50 base questions spanning mathematics, science, and
history, each rewritten into five tone variants: Very Polite, Polite, Neutral,
Rude, and Very Rude, yielding 250 unique prompts. Using ChatGPT 4o, we
evaluated responses across these conditions and applied paired sample t-tests
to assess statistical significance. Contrary to expectations, impolite prompts
consistently outperformed polite ones, with accuracy ranging from 80.8% for
Very Polite prompts to 84.8% for Very Rude prompts. These findings differ from
earlier studies that associated rudeness with poorer outcomes, suggesting that
newer LLMs may respond differently to tonal variation. Our results highlight
the importance of studying pragmatic aspects of prompting and raise broader
questions about the social dimensions of human-AI interaction.

</details>


### [100] [AWARE, Beyond Sentence Boundaries: A Contextual Transformer Framework for Identifying Cultural Capital in STEM Narratives](https://arxiv.org/abs/2510.04983)
*Khalid Mehtab Khan,Anagha Kulkarni*

Main category: cs.CL

TL;DR: AWARE框架通过提升模型对领域、上下文和类别重叠的认知能力，有效识别学生反思中的文化资本主题，在Macro-F1指标上比基线提升2.1个百分点。


<details>
  <summary>Details</summary>
Motivation: 学生反思中的文化资本主题（如志向目标、家庭支持）通常以叙事形式呈现，而非直接关键词，这使得标准NLP模型难以检测，因为它们缺乏对领域特定语言和叙事背景的认知。

Method: AWARE框架包含三个核心组件：1）领域认知 - 使模型词汇适应学生反思的语言风格；2）上下文认知 - 生成考虑全文背景的句子嵌入；3）类别重叠认知 - 使用多标签策略识别单个句子中多个主题的共存。

Result: AWARE在Macro-F1指标上比强基线提升2.1个百分点，在所有主题上都显示出显著改进。

Conclusion: 这项工作为任何依赖叙事背景意义的文本分类任务提供了稳健且可推广的方法论。

Abstract: Identifying cultural capital (CC) themes in student reflections can offer
valuable insights that help foster equitable learning environments in
classrooms. However, themes such as aspirational goals or family support are
often woven into narratives, rather than appearing as direct keywords. This
makes them difficult to detect for standard NLP models that process sentences
in isolation. The core challenge stems from a lack of awareness, as standard
models are pre-trained on general corpora, leaving them blind to the
domain-specific language and narrative context inherent to the data. To address
this, we introduce AWARE, a framework that systematically attempts to improve a
transformer model's awareness for this nuanced task. AWARE has three core
components: 1) Domain Awareness, adapting the model's vocabulary to the
linguistic style of student reflections; 2) Context Awareness, generating
sentence embeddings that are aware of the full essay context; and 3) Class
Overlap Awareness, employing a multi-label strategy to recognize the
coexistence of themes in a single sentence. Our results show that by making the
model explicitly aware of the properties of the input, AWARE outperforms a
strong baseline by 2.1 percentage points in Macro-F1 and shows considerable
improvements across all themes. This work provides a robust and generalizable
methodology for any text classification task in which meaning depends on the
context of the narrative.

</details>


### [101] [Resource-Efficient Fine-Tuning of LLaMA-3.2-3B for Medical Chain-of-Thought Reasoning](https://arxiv.org/abs/2510.05003)
*Imran Mansha*

Main category: cs.CL

TL;DR: 提出了一种资源高效的LLaMA-3.2-3B微调方法，使用LoRA和QLoRA技术在受限GPU和内存环境下增强医学链式推理能力，相比全参数微调减少60%内存使用。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型如GPT-4和LLaMA具有强大推理能力，但全参数微调需要大量计算资源，特别是在医疗领域应用中面临资源限制问题。

Method: 使用参数高效微调技术（LoRA和QLoRA），在公开医学推理数据集上对LLaMA-3.2-3B进行适配，在受限GPU和内存环境下实现高效微调。

Result: 模型在医学推理一致性和事实准确性方面得到提升，同时比标准全参数微调减少高达60%的内存使用量，在医学问答任务中保持强推理能力。

Conclusion: 这项工作展示了在低资源研究环境中部署LLM的实用策略，为平衡效率和领域专业化提供了见解，特别适用于医疗AI系统。

Abstract: Large Language Models (LLMs) such as GPT-4 and LLaMA have demonstrated
remarkable reasoning abilities but require significant computational resources
for fine-tuning. This paper presents a resource-efficient fine-tuning approach
for LLaMA-3.2-3B to enhance medical chain-of-thought reasoning while operating
under constrained GPU and memory settings. Using parameter-efficient tuning
techniques such as LoRA and QLoRA, we adapt the base model on publicly
available medical reasoning datasets. The model achieves improved reasoning
coherence and factual accuracy while reducing memory usage by up to 60%
compared to standard full fine-tuning. Experimental evaluation demonstrates
that lightweight adaptations can retain strong reasoning capability in medical
question-answering tasks. This work highlights practical strategies for
deploying LLMs in low-resource research environments and provides insights into
balancing efficiency and domain specialization for medical AI systems.

</details>


### [102] [Imperceptible Jailbreaking against Large Language Models](https://arxiv.org/abs/2510.05025)
*Kuofeng Gao,Yiming Li,Chao Du,Xin Wang,Xingjun Ma,Shu-Tao Xia,Tianyu Pang*

Main category: cs.CL

TL;DR: 提出了一种利用Unicode变体选择器实现不可感知越狱攻击的方法，通过在恶意问题后附加不可见的变体选择器，使提示在视觉上保持不变但tokenization被秘密改变，从而诱导有害响应。


<details>
  <summary>Details</summary>
Motivation: 视觉模态的越狱攻击通常依赖不可感知的对抗扰动，而文本模态攻击通常需要可见修改。本文旨在探索文本模态中不可感知的越狱攻击可能性。

Method: 使用Unicode变体选择器类字符，通过链式搜索管道生成对抗性后缀，在不产生可见修改的情况下改变tokenization。

Result: 实验表明该方法对四个对齐的LLM实现了高攻击成功率，并能泛化到提示注入攻击。

Conclusion: 证明了文本模态中不可感知越狱攻击的可行性，揭示了当前LLM安全机制中的潜在漏洞。

Abstract: Jailbreaking attacks on the vision modality typically rely on imperceptible
adversarial perturbations, whereas attacks on the textual modality are
generally assumed to require visible modifications (e.g., non-semantic
suffixes). In this paper, we introduce imperceptible jailbreaks that exploit a
class of Unicode characters called variation selectors. By appending invisible
variation selectors to malicious questions, the jailbreak prompts appear
visually identical to original malicious questions on screen, while their
tokenization is "secretly" altered. We propose a chain-of-search pipeline to
generate such adversarial suffixes to induce harmful responses. Our experiments
show that our imperceptible jailbreaks achieve high attack success rates
against four aligned LLMs and generalize to prompt injection attacks, all
without producing any visible modifications in the written prompt. Our code is
available at https://github.com/sail-sg/imperceptible-jailbreaks.

</details>


### [103] [A Set of Quebec-French Corpus of Regional Expressions and Terms](https://arxiv.org/abs/2510.05026)
*David Beauchemin,Yan Tremblay,Mohamed Amine Youssef,Richard Khoury*

Main category: cs.CL

TL;DR: 提出了两个新的魁北克法语方言理解基准数据集QFrCoRE和QFrCoRT，用于测试模型对区域习语的掌握程度，实验证明这些基准能有效衡量模型在特定方言上的熟练度。


<details>
  <summary>Details</summary>
Motivation: 将习语理解和方言理解两个任务结合起来，通过区域习语来测试方言理解能力，为魁北克法语方言提供专门的评估工具。

Method: 构建了两个基准数据集：QFrCoRE包含4,633个习语短语实例，QFrCoRT包含171个区域习语词汇实例，并提供了可复现的构建方法。

Result: 通过对94个LLM的实验表明，区域习语基准是衡量模型在特定方言上熟练度的可靠工具。

Conclusion: 区域习语基准能够有效评估模型对特定方言的理解能力，该方法可推广到其他方言的评估中。

Abstract: The tasks of idiom understanding and dialect understanding are both
well-established benchmarks in natural language processing. In this paper, we
propose combining them, and using regional idioms as a test of dialect
understanding. Towards this end, we propose two new benchmark datasets for the
Quebec dialect of French: QFrCoRE, which contains 4,633 instances of idiomatic
phrases, and QFrCoRT, which comprises 171 regional instances of idiomatic
words. We explain how to construct these corpora, so that our methodology can
be replicated for other dialects. Our experiments with 94 LLM demonstrate that
our regional idiom benchmarks are a reliable tool for measuring a model's
proficiency in a specific dialect.

</details>


### [104] [Guided Query Refinement: Multimodal Hybrid Retrieval with Test-Time Optimization](https://arxiv.org/abs/2510.05038)
*Omri Uzan,Asaf Yehudai,Roi pony,Eyal Shnarch,Ariel Gera*

Main category: cs.CL

TL;DR: 提出GQR方法，通过轻量级文本检索器增强视觉中心模型，在视觉文档检索中实现性能与效率的平衡


<details>
  <summary>Details</summary>
Motivation: 现有多模态编码器存在表示规模过大、部署困难的问题，且纯视觉方法受限于模态差距。传统混合检索方法无法充分利用表示空间的丰富交互

Method: 提出引导查询优化(GQR)，在测试时使用互补检索器的分数来优化主检索器的查询嵌入

Result: GQR使视觉中心模型在性能上媲美表示规模大得多的模型，同时速度提升14倍，内存需求减少54倍

Conclusion: GQR有效推动了多模态检索在性能和效率上的帕累托前沿

Abstract: Multimodal encoders have pushed the boundaries of visual document retrieval,
matching textual query tokens directly to image patches and achieving
state-of-the-art performance on public benchmarks. Recent models relying on
this paradigm have massively scaled the sizes of their query and document
representations, presenting obstacles to deployment and scalability in
real-world pipelines. Furthermore, purely vision-centric approaches may be
constrained by the inherent modality gap still exhibited by modern
vision-language models. In this work, we connect these challenges to the
paradigm of hybrid retrieval, investigating whether a lightweight dense text
retriever can enhance a stronger vision-centric model. Existing hybrid methods,
which rely on coarse-grained fusion of ranks or scores, fail to exploit the
rich interactions within each model's representation space. To address this, we
introduce Guided Query Refinement (GQR), a novel test-time optimization method
that refines a primary retriever's query embedding using guidance from a
complementary retriever's scores. Through extensive experiments on visual
document retrieval benchmarks, we demonstrate that GQR allows vision-centric
models to match the performance of models with significantly larger
representations, while being up to 14x faster and requiring 54x less memory.
Our findings show that GQR effectively pushes the Pareto frontier for
performance and efficiency in multimodal retrieval. We release our code at
https://github.com/IBM/test-time-hybrid-retrieval

</details>


### [105] [COLE: a Comprehensive Benchmark for French Language Understanding Evaluation](https://arxiv.org/abs/2510.05046)
*David Beauchemin,Yan Tremblay,Mohamed Amine Youssef,Richard Khoury*

Main category: cs.CL

TL;DR: COLE是一个新的法语自然语言理解基准测试，包含23个多样化任务，评估了94个大语言模型，揭示了闭源和开源模型之间的性能差距，并识别了当前LLM面临的挑战领域。


<details>
  <summary>Details</summary>
Motivation: 解决法语自然语言理解评估不够全面的问题，创建一个覆盖广泛NLU能力的综合性基准测试。

Method: 构建包含23个多样化任务的COLE基准，涵盖情感分析、释义检测、语法判断和推理等能力，特别关注法语特有的语言现象，并对94个大语言模型进行基准测试。

Result: 发现闭源和开源模型之间存在显著性能差距，识别出零样本抽取式问答、细粒度词义消歧和地区语言变体理解等关键挑战领域。

Conclusion: COLE作为公开资源发布，旨在促进法语语言建模的进一步发展，为法语NLU研究提供全面的评估框架。

Abstract: To address the need for a more comprehensive evaluation of French Natural
Language Understanding (NLU), we introduce COLE, a new benchmark composed of 23
diverse task covering a broad range of NLU capabilities, including sentiment
analysis, paraphrase detection, grammatical judgment, and reasoning, with a
particular focus on linguistic phenomena relevant to the French language. We
benchmark 94 large language models (LLM), providing an extensive analysis of
the current state of French NLU. Our results highlight a significant
performance gap between closed- and open-weights models and identify key
challenging frontiers for current LLMs, such as zero-shot extractive
question-answering (QA), fine-grained word sense disambiguation, and
understanding of regional language variations. We release COLE as a public
resource to foster further progress in French language modelling.

</details>


### [106] [SwiReasoning: Switch-Thinking in Latent and Explicit for Pareto-Superior Reasoning LLMs](https://arxiv.org/abs/2510.05069)
*Dachuan Shi,Abedelkadir Asi,Keying Li,Xiangchi Yuan,Leyan Pan,Wenke Lee,Wen Xiao*

Main category: cs.CL

TL;DR: SwiReasoning是一个无需训练的LLM推理框架，通过动态切换显式和潜在推理模式来平衡探索与利用，提高准确性和token效率。


<details>
  <summary>Details</summary>
Motivation: 解决纯潜在推理带来的搜索分布扩散、概率质量分散、收敛困难以及过度思考问题，旨在提升LLM推理的准确性和效率。

Method: 提出SwiReasoning框架，基于熵趋势估计块级置信度来动态切换显式和潜在推理模式，并限制思考块切换次数来控制过度思考。

Result: 在数学和STEM基准测试中，平均准确率提升1.5%-2.8%，在受限预算下token效率提升56%-79%，预算越紧提升越大。

Conclusion: SwiReasoning通过动态推理模式切换有效解决了潜在推理的收敛问题和过度思考问题，显著提升了LLM推理的性能和效率。

Abstract: Recent work shows that, beyond discrete reasoning through explicit
chain-of-thought steps, which are limited by the boundaries of natural
languages, large language models (LLMs) can also reason continuously in latent
space, allowing richer information per step and thereby improving token
efficiency. Despite this promise, latent reasoning still faces two challenges,
especially in training-free settings: 1) purely latent reasoning broadens the
search distribution by maintaining multiple implicit paths, which diffuses
probability mass, introduces noise, and impedes convergence to a single
high-confidence solution, thereby hurting accuracy; and 2) overthinking
persists even without explicit text, wasting tokens and degrading efficiency.
To address these issues, we introduce SwiReasoning, a training-free framework
for LLM reasoning which features two key innovations: 1) SwiReasoning
dynamically switches between explicit and latent reasoning, guided by
block-wise confidence estimated from entropy trends in next-token
distributions, to balance exploration and exploitation and promote timely
convergence. 2) By limiting the maximum number of thinking-block switches,
SwiReasoning curbs overthinking and improves token efficiency across varying
problem difficulties. On widely used mathematics and STEM benchmarks,
SwiReasoning consistently improves average accuracy by 1.5%-2.8% across
reasoning LLMs of different model families and scales. Furthermore, under
constrained budgets, SwiReasoning improves average token efficiency by 56%-79%,
with larger gains as budgets tighten.

</details>


### [107] [Slm-mux: Orchestrating small language models for reasoning](https://arxiv.org/abs/2510.05077)
*Chenyu Wang,Zishen Wan,Hao Kang,Emma Chen,Zhiqiang Xie,Tushar Krishna,Vijay Janapa Reddi,Yilun Du*

Main category: cs.CL

TL;DR: 提出了一种三阶段方法来协调多个小语言模型(SLMs)，通过SLM-MUX架构和优化策略，在多个基准测试中显著优于现有协调方法，甚至能超越更大的单一模型。


<details>
  <summary>Details</summary>
Motivation: 随着小语言模型数量的快速增长，虽然它们无法达到最先进的准确率，但在特定任务上表现出色且更高效。现有协调方法主要针对前沿大模型，在小语言模型上表现不佳，因此需要专门针对SLMs的协调方法。

Method: 提出三阶段方法：1) SLM-MUX多模型架构有效协调多个SLMs；2) 模型选择搜索从候选池中识别最具互补性的SLMs；3) 针对SLM-MUX的测试时缩放策略。

Result: 在MATH上提升13.4%，GPQA上提升8.8%，GSM8K上提升7.0%。仅使用两个SLMs，SLM-MUX在GPQA和GSM8K上超越Qwen 2.5 72B，在MATH上与之持平。

Conclusion: 通过所提出的方法，小语言模型可以被有效协调成更准确和高效的系统，证明了SLMs协调的可行性和优势。

Abstract: With the rapid development of language models, the number of small language
models (SLMs) has grown significantly. Although they do not achieve
state-of-the-art accuracy, they are more efficient and often excel at specific
tasks. This raises a natural question: can multiple SLMs be orchestrated into a
system where each contributes effectively, achieving higher accuracy than any
individual model? Existing orchestration methods have primarily targeted
frontier models (e.g., GPT-4) and perform suboptimally when applied to SLMs. To
address this gap, we propose a three-stage approach for orchestrating SLMs.
First, we introduce SLM-MUX, a multi-model architecture that effectively
coordinates multiple SLMs. Building on this, we develop two optimization
strategies: (i) a model selection search that identifies the most complementary
SLMs from a given pool, and (ii) test-time scaling tailored to SLM-MUX. Our
approach delivers strong results: Compared to existing orchestration methods,
our approach achieves up to 13.4% improvement on MATH, 8.8% on GPQA, and 7.0%
on GSM8K. With just two SLMS, SLM-MUX outperforms Qwen 2.5 72B on GPQA and
GSM8K, and matches its performance on MATH. We further provide theoretical
analyses to substantiate the advantages of our method. In summary, we
demonstrate that SLMs can be effectively orchestrated into more accurate and
efficient systems through the proposed approach.

</details>


### [108] [TeachLM: Post-Training LLMs for Education Using Authentic Learning Data](https://arxiv.org/abs/2510.05087)
*Janos Perczel,Jin Chow,Dorottya Demszky*

Main category: cs.CL

TL;DR: TeachLM通过参数高效微调优化LLM用于教学，使用真实学生-导师对话数据训练，显著提升对话能力和教学效果。


<details>
  <summary>Details</summary>
Motivation: 解决生成式AI在教育中应用受限的问题，特别是缺乏高质量学生学习数据，以及提示工程在编码复杂教学策略方面的局限性。

Method: 使用10万小时真实学生-导师对话数据，通过参数高效微调开发真实学生模型，生成高质量合成对话，并提出多轮评估协议。

Result: 微调后显著改善对话和教学性能：学生发言时间翻倍、提问风格改善、对话轮次增加50%、教学个性化程度提高。

Conclusion: 使用真实学习数据进行微调可显著提升LLM的教学对话能力，为教育AI发展提供了有效路径。

Abstract: The promise of generative AI to revolutionize education is constrained by the
pedagogical limits of large language models (LLMs). A major issue is the lack
of access to high-quality training data that reflect the learning of actual
students. Prompt engineering has emerged as a stopgap, but the ability of
prompts to encode complex pedagogical strategies in rule-based natural language
is inherently limited. To address this gap we introduce TeachLM - an LLM
optimized for teaching through parameter-efficient fine-tuning of
state-of-the-art models. TeachLM is trained on a dataset comprised of 100,000
hours of one-on-one, longitudinal student-tutor interactions maintained by
Polygence, which underwent a rigorous anonymization process to protect privacy.
We use parameter-efficient fine-tuning to develop an authentic student model
that enables the generation of high-fidelity synthetic student-tutor dialogues.
Building on this capability, we propose a novel multi-turn evaluation protocol
that leverages synthetic dialogue generation to provide fast, scalable, and
reproducible assessments of the dialogical capabilities of LLMs. Our
evaluations demonstrate that fine-tuning on authentic learning data
significantly improves conversational and pedagogical performance - doubling
student talk time, improving questioning style, increasing dialogue turns by
50%, and greater personalization of instruction.

</details>


### [109] [Finish First, Perfect Later: Test-Time Token-Level Cross-Validation for Diffusion Large Language Models](https://arxiv.org/abs/2510.05090)
*Runchu Tian,Junxia Cui,Xueqiang Xu,Feng Yao,Jingbo Shang*

Main category: cs.CL

TL;DR: 提出了Tolerator解码策略，通过令牌级交叉验证改进扩散大语言模型的解码过程，解决了传统方法中令牌一旦接受就无法修改的问题。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型虽然具有并行解码和双向上下文建模的优势，但传统解码策略存在关键限制：一旦令牌被接受就无法在后续步骤中修正，导致早期错误持续影响预测质量。

Method: Tolerator采用两阶段过程：1)序列填充；2)通过重新掩码和解码令牌子集进行迭代精炼，同时将剩余令牌作为上下文，使先前接受的令牌能够被重新考虑和修正。

Result: 在涵盖语言理解、代码生成和数学的五个标准基准测试中，Tolerator在相同计算预算下相比基线方法实现了持续改进。

Conclusion: 解码算法对于充分发挥扩散大语言模型的潜力至关重要，Tolerator通过令牌级交叉验证实现了更可靠的扩散解码输出。

Abstract: Diffusion large language models (dLLMs) have recently emerged as a promising
alternative to autoregressive (AR) models, offering advantages such as
accelerated parallel decoding and bidirectional context modeling. However, the
vanilla decoding strategy in discrete dLLMs suffers from a critical limitation:
once a token is accepted, it can no longer be revised in subsequent steps. As a
result, early mistakes persist across iterations, harming both intermediate
predictions and final output quality. To address this issue, we propose
Tolerator (Token-Level Cross-Validation Refinement), a training-free decoding
strategy that leverages cross-validation among predicted tokens. Unlike
existing methods that follow a single progressive unmasking procedure,
Tolerator introduces a two-stage process: (i) sequence fill-up and (ii)
iterative refinement by remasking and decoding a subset of tokens while
treating the remaining as context. This design enables previously accepted
tokens to be reconsidered and corrected when necessary, leading to more
reliable diffusion decoding outputs. We evaluate Tolerator on five standard
benchmarks covering language understanding, code generation, and mathematics.
Experiments show that our method achieves consistent improvements over the
baselines under the same computational budget. These findings suggest that
decoding algorithms are crucial to realizing the full potential of diffusion
large language models. Code and data are publicly available.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [110] [PLSemanticsBench: Large Language Models As Programming Language Interpreters](https://arxiv.org/abs/2510.03415)
*Aditya Thimmaiah,Jiyang Zhang,Jayanth Srinivasa,Junyi Jessy Li,Milos Gligoric*

Main category: cs.PL

TL;DR: 研究LLM能否基于形式语义执行程序，发现LLM在标准语义下表现良好，但在非标准语义下性能下降，表明其语义理解不够鲁棒。


<details>
  <summary>Details</summary>
Motivation: 探索LLM能否作为编程语言解释器，实现新编程语言和语言特性的快速原型开发。

Method: 使用IMP语言的形式语义（SOS和K语义），构建三个难度可控的评估集，评估LLM在最终状态预测、语义规则预测和执行轨迹预测三个任务上的表现。

Result: LLM在标准语义下表现良好，但在非标准语义下性能显著下降；在复杂程序上表现优异，但提供形式语义对简单程序有帮助，对复杂程序反而有害。

Conclusion: LLM有潜力成为编程语言解释器，但其语义理解缺乏鲁棒性。

Abstract: As large language models (LLMs) excel at code reasoning, a natural question
arises: can an LLM execute programs (i.e., act as an interpreter) purely based
on a programming language's formal semantics? If so, it will enable rapid
prototyping of new programming languages and language features. We study this
question using the imperative language IMP (a subset of C), formalized via
small-step operational semantics (SOS) and rewriting-based operational
semantics (K-semantics). We introduce three evaluation sets-Human-Written,
LLM-Translated, and Fuzzer- Generated-whose difficulty is controlled by
code-complexity metrics spanning the size, control-flow, and data-flow axes.
Given a program and its semantics formalized with SOS/K-semantics, models are
evaluated on three tasks ranging from coarse to fine: (1) final-state
prediction, (2) semantic rule prediction, and (3) execution trace prediction.
To distinguish pretraining memorization from semantic competence, we define two
nonstandard semantics obtained through systematic mutations of the standard
rules. Across strong code/reasoning LLMs, performance drops under nonstandard
semantics despite high performance under the standard one. We further find that
(i) there are patterns to different model failures, (ii) most reasoning models
perform exceptionally well on coarse grained tasks involving reasoning about
highly complex programs often containing nested loop depths beyond five, and
surprisingly, (iii) providing formal semantics helps on simple programs but
often hurts on more complex ones. Overall, the results show a promise that LLMs
could serve as programming language interpreters, but points to the lack of
their robust semantics understanding. We release the benchmark and the
supporting code at https://github.com/EngineeringSoftware/PLSemanticsBench.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [111] [CAG: Chunked Augmented Generation for Google Chrome's Built-in Gemini Nano](https://arxiv.org/abs/2412.18708)
*Vivek Vellaiyappan Surulimuthu,Aditya Karnam Gururaj Rao*

Main category: cs.AI

TL;DR: 提出了Chunked Augmented Generation (CAG)架构，专门解决Chrome内置Gemini Nano模型上下文窗口限制的问题，通过智能分块处理策略在浏览器中高效处理大型输入。


<details>
  <summary>Details</summary>
Motivation: Chrome集成Gemini Nano是浏览器AI能力的重大进步，但其受限的上下文窗口给处理大型输入带来挑战，需要解决方案来克服这一限制。

Method: 采用智能输入分块和处理策略，将大型内容分解为可管理的块，在浏览器约束内保持模型性能。

Result: 该实现特别擅长在Chrome中直接处理大型文档和数据集，使复杂的AI能力可通过浏览器访问而无需外部API依赖。

Conclusion: CAG架构成功克服了Gemini Nano在浏览器中的上下文窗口限制，为在浏览器环境中处理大型内容提供了有效的解决方案。

Abstract: We present Chunked Augmented Generation (CAG), an architecture specifically
designed to overcome the context window limitations of Google Chrome's built-in
Gemini Nano model. While Chrome's integration of Gemini Nano represents a
significant advancement in bringing AI capabilities directly to the browser,
its restricted context window poses challenges for processing large inputs. CAG
addresses this limitation through intelligent input chunking and processing
strategies, enabling efficient handling of extensive content while maintaining
the model's performance within browser constraints. Our implementation
demonstrates particular efficacy in processing large documents and datasets
directly within Chrome, making sophisticated AI capabilities accessible through
the browser without external API dependencies. Get started now at
https://github.com/vivekVells/cag-js.

</details>


### [112] [Know Thyself? On the Incapability and Implications of AI Self-Recognition](https://arxiv.org/abs/2510.03399)
*Xiaoyan Bai,Aryan Shrivastava,Ari Holtzman,Chenhao Tan*

Main category: cs.AI

TL;DR: 该研究系统评估了10个当代大语言模型的自我识别能力，发现模型在识别自身生成文本方面表现不佳，多数模型性能仅略高于随机水平，且存在对GPT和Claude家族的强烈预测偏见。


<details>
  <summary>Details</summary>
Motivation: 针对AI系统是否具备自我识别能力这一争议性问题，研究者希望建立一个可轻松应用和更新的系统评估框架，以澄清模型是否真正具备自我认知能力。

Method: 通过两个任务评估模型：二元自我识别（识别文本是否为自己生成）和精确模型预测（识别文本由哪个具体模型生成），测试了10个当代大语言模型。

Result: 结果显示模型自我识别能力普遍失败，仅4/10模型能正确预测自身为生成者，性能很少超过随机机会。模型存在强烈偏见，倾向于预测GPT和Claude家族，并表现出对模型存在性的层次偏见认知。

Conclusion: 研究强调了当前AI系统缺乏真正的自我识别能力，这对AI安全具有重要意义，并指出了开发适当AI自我意识能力的未来方向。

Abstract: Self-recognition is a crucial metacognitive capability for AI systems,
relevant not only for psychological analysis but also for safety, particularly
in evaluative scenarios. Motivated by contradictory interpretations of whether
models possess self-recognition (Panickssery et al., 2024; Davidson et al.,
2024), we introduce a systematic evaluation framework that can be easily
applied and updated. Specifically, we measure how well 10 contemporary larger
language models (LLMs) can identify their own generated text versus text from
other models through two tasks: binary self-recognition and exact model
prediction. Different from prior claims, our results reveal a consistent
failure in self-recognition. Only 4 out of 10 models predict themselves as
generators, and the performance is rarely above random chance. Additionally,
models exhibit a strong bias toward predicting GPT and Claude families. We also
provide the first evaluation of model awareness of their own and others'
existence, as well as the reasoning behind their choices in self-recognition.
We find that the model demonstrates some knowledge of its own existence and
other models, but their reasoning reveals a hierarchical bias. They appear to
assume that GPT, Claude, and occasionally Gemini are the top-tier models, often
associating high-quality text with them. We conclude by discussing the
implications of our findings on AI safety and future directions to develop
appropriate AI self-awareness.

</details>


### [113] [Mind the Goal: Data-Efficient Goal-Oriented Evaluation of Conversational Agents and Chatbots using Teacher Models](https://arxiv.org/abs/2510.03696)
*Deepak Babu Piskala,Sharlene Chen,Udita Patel,Parul Kalra,Rafael Castrillo*

Main category: cs.AI

TL;DR: 提出了一个面向目标的多智能体系统评估框架，引入目标成功率(GSR)和失败根因分类法(RCOF)，通过教师LLM模型实现可解释、数据高效的多轮对话评估。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要在轮次层面评估多轮聊天机器人交互，无法判断用户总体目标是否达成，需要更全面的目标导向评估框架。

Method: 通过用户目标分割对话，使用教师LLM模型结合领域专家定义的目标和质量标准进行评估，利用"思考标记"生成可解释的推理过程。

Result: 在企业环境中应用该框架评估AIDA员工对话系统，6个月内目标成功率从63%提升至79%。

Conclusion: 该框架具有通用性，通过详细的缺陷分类提供可操作的见解，能够诊断整体成功率、识别关键失败模式并指导系统改进。

Abstract: Evaluating the quality of multi-turn chatbot interactions remains
challenging, as most existing methods assess interactions at the turn level
without addressing whether a user's overarching goal was fulfilled. A ``goal''
here refers to an information need or task, such as asking for policy
information or applying for leave. We propose a comprehensive framework for
goal-oriented evaluation of multi-agent systems (MAS), introducing the
\textbf{Goal Success Rate (GSR)} to measure the percentage of fulfilled goals,
and a \textbf{Root Cause of Failure (RCOF)} taxonomy to identify reasons for
failure in multi-agent chatbots. Our method segments conversations by user
goals and evaluates success using all relevant turns. We present a model-based
evaluation system combining teacher LLMs, where domain experts define goals,
set quality standards serving as a guidance for the LLMs. The LLMs use
``thinking tokens'' to produce interpretable rationales, enabling
\textit{explainable}, \textit{data-efficient} evaluations. In an enterprise
setting, we apply our framework to evaluate AIDA, a zero-to-one employee
conversational agent system built as a ground-up multi-agent conversational
agent, and observe GSR improvement from 63\% to 79\% over six months since its
inception. Our framework is generic and offers actionable insights through a
detailed defect taxonomy based on analysis of failure points in multi-agent
chatbots, diagnosing overall success, identifying key failure modes, and
informing system improvements.

</details>


### [114] [Bridging the Gap Between Multimodal Foundation Models and World Models](https://arxiv.org/abs/2510.03727)
*Xuehai He*

Main category: cs.AI

TL;DR: 该论文探讨如何弥合多模态基础模型与世界模型之间的差距，通过增强推理能力和生成能力来实现更有效的世界建模。


<details>
  <summary>Details</summary>
Motivation: 受人类多感官整合理解世界的启发，当前的多模态基础模型缺乏作为有效世界模型的关键能力，如反事实推理、动态模拟、时空信息理解和可控生成等。

Method: 通过判别性任务增强推理能力，赋予结构化推理技能（如因果推理、反事实思维和时空推理）；开发结构化可控生成框架，结合场景图、多模态条件调节和对齐策略；扩展到可控4D生成。

Result: 提出了增强多模态基础模型推理和生成能力的方法论，使其能够超越表面相关性，理解深层关系，并实现与高级语义和用户意图一致的生成结果。

Conclusion: 通过系统性地增强多模态基础模型的推理和生成能力，可以将其转变为更有效的世界模型，具备反事实推理、动态模拟和可控生成等关键能力。

Abstract: Humans understand the world through the integration of multiple sensory
modalities, enabling them to perceive, reason about, and imagine dynamic
physical processes. Inspired by this capability, multimodal foundation models
(MFMs) have emerged as powerful tools for multimodal understanding and
generation. However, today's MFMs fall short of serving as effective world
models. They lack the essential ability such as perform counterfactual
reasoning, simulate dynamics, understand the spatiotemporal information,
control generated visual outcomes, and perform multifaceted reasoning. We
investigates what it takes to bridge the gap between multimodal foundation
models and world models. We begin by improving the reasoning capabilities of
MFMs through discriminative tasks and equipping MFMs with structured reasoning
skills, such as causal inference, counterfactual thinking, and spatiotemporal
reasoning, enabling them to go beyond surface correlations and understand
deeper relationships within visual and textual data. Next, we explore
generative capabilities of multimodal foundation models across both image and
video modalities, introducing new frameworks for structured and controllable
generation. Our approaches incorporate scene graphs, multimodal conditioning,
and multimodal alignment strategies to guide the generation process, ensuring
consistency with high-level semantics and fine-grained user intent. We further
extend these techniques to controllable 4D generation, enabling interactive,
editable, and morphable object synthesis over time and space.

</details>


### [115] [Kantian-Utilitarian XAI: Meta-Explained](https://arxiv.org/abs/2510.03892)
*Zahra Atf,Peter R. Lewis*

Main category: cs.AI

TL;DR: 提出一个游戏化的可解释AI系统，用于咖啡消费决策的道德考量，结合康德主义和功利主义伦理框架，提供实时解释和后悔限制机制。


<details>
  <summary>Details</summary>
Motivation: 解决消费者在咖啡购买决策中面临的道德困境，通过可解释AI帮助消费者理解不同选择背后的伦理影响。

Method: 使用两个符号引擎：康德主义模块检测规则违反（如童工、毁林风险），功利主义模块通过多标准聚合评分；设置后悔限制为0.2的元解释器来处理伦理框架间的冲突。

Result: 开发了结构化配置、可审计的策略追踪和交互式用户界面，系统能够在福利损失较小时切换到符合道义要求的近似最优选项。

Conclusion: 该游戏化XAI系统为消费者提供了透明且可操作的道德决策支持工具，平衡了不同伦理视角的考量。

Abstract: We present a gamified explainable AI (XAI) system for ethically aware
consumer decision-making in the coffee domain. Each session comprises six
rounds with three options per round. Two symbolic engines provide real-time
reasons: a Kantian module flags rule violations (e.g., child labor,
deforestation risk without shade certification, opaque supply chains, unsafe
decaf), and a utilitarian module scores options via multi-criteria aggregation
over normalized attributes (price, carbon, water, transparency, farmer income
share, taste/freshness, packaging, convenience). A meta-explainer with a regret
bound (0.2) highlights Kantian--utilitarian (mis)alignment and switches to a
deontically clean, near-parity option when welfare loss is small. We release a
structured configuration (attribute schema, certification map, weights, rule
set), a policy trace for auditability, and an interactive UI.

</details>


### [116] [What Shapes a Creative Machine Mind? Comprehensively Benchmarking Creativity in Foundation Models](https://arxiv.org/abs/2510.04009)
*Zicong He,Boxuan Zhang,Weihao Liu,Ruixiang Tang,Lu Cheng*

Main category: cs.AI

TL;DR: 提出了C^2-Eval基准，用于统一评估基础模型的创造力，区分收敛性创造力和发散性创造力，基于有用性、原创性和惊喜性三个标准。


<details>
  <summary>Details</summary>
Motivation: 现有创造力评估框架碎片化，缺乏理论基础，需要建立统一的评估标准来衡量基础模型的创造力。

Method: 引入C^2-Eval基准，区分收敛性创造力（有约束解决方案）和发散性创造力（开放式任务），使用基于社会科学理论的细粒度标准（U-O-S）。

Result: 通过对领先专有和开源模型的大量实验，分析了它们在创造力能力上的权衡，揭示了当前基础模型在追求创造性机器智能方面的优势和挑战。

Conclusion: C^2-Eval是检验创造性AI发展格局的有效工具，为评估基础模型的创造力提供了统一框架。

Abstract: The meteoric rise of foundation models (FMs) has expanded their capabilities
far beyond conventional tasks. Creativity, long regarded as a hallmark of human
intelligence and a driver of innovation, is now increasingly recognized as a
critical dimension of machine intelligence in the era of generative FMs,
complementing traditional measures of accuracy. However, existing evaluation
frameworks for creativity remain fragmented, relying on ad hoc metrics not
firmly grounded in established theories. To address this gap, we introduce
C^2-Eval, a holistic benchmark for unified assessment of creativity in FMs.
C^2-Eval distinguishes between two complementary forms of creativity:
convergent creativity, where tasks admit constrained solutions (e.g., code
generation), and divergent creativity, where tasks are open-ended (e.g.,
storytelling). It evaluates both dimensions using fine-grained criteria derived
from social-science theory, focusing on Usefulness, Originality, and Surprise
(U-O-S). Through extensive experiments on leading proprietary and open-source
models, we analyze trade-offs in their creative capabilities. Our results
highlight both the strengths and challenges of current FMs in pursuing a
creative machine mind, showing that C^2-Eval is an effective lens for examining
the evolving landscape of creative AI.

</details>


### [117] [LLM-Based Data Science Agents: A Survey of Capabilities, Challenges, and Future Directions](https://arxiv.org/abs/2510.04023)
*Mizanur Rahman,Amran Bhuiyan,Mohammed Saidul Islam,Md Tahmid Rahman Laskar,Ridwan Mahbub,Ahmed Masry,Shafiq Joty,Enamul Hoque*

Main category: cs.AI

TL;DR: 这篇论文首次提出了一个与数据科学生命周期对齐的全面分类法，系统分析了45个数据科学智能体系统，涵盖了从业务理解到部署监控的六个阶段，并识别了当前研究中的关键趋势和挑战。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，出现了能够自动化数据科学工作流程的新型AI智能体，但缺乏对这些系统的系统性分类和分析。本文旨在填补这一空白，为数据科学智能体的发展提供指导。

Method: 采用生命周期对齐的分类法，将45个数据科学智能体系统映射到数据科学流程的六个阶段，并从五个交叉设计维度进行标注分析。

Result: 分析发现三个关键趋势：大多数系统强调探索性分析和建模，而忽视业务理解、部署和监控；多模态推理和工具编排仍是未解决的挑战；超过90%的系统缺乏明确的信任和安全机制。

Conclusion: 提出了未来研究方向，包括对齐稳定性、可解释性、治理和鲁棒评估框架等挑战，以指导开发更稳健、可信、低延迟、透明和广泛可访问的数据科学智能体。

Abstract: Recent advances in large language models (LLMs) have enabled a new class of
AI agents that automate multiple stages of the data science workflow by
integrating planning, tool use, and multimodal reasoning across text, code,
tables, and visuals. This survey presents the first comprehensive,
lifecycle-aligned taxonomy of data science agents, systematically analyzing and
mapping forty-five systems onto the six stages of the end-to-end data science
process: business understanding and data acquisition, exploratory analysis and
visualization, feature engineering, model building and selection,
interpretation and explanation, and deployment and monitoring. In addition to
lifecycle coverage, we annotate each agent along five cross-cutting design
dimensions: reasoning and planning style, modality integration, tool
orchestration depth, learning and alignment methods, and trust, safety, and
governance mechanisms. Beyond classification, we provide a critical synthesis
of agent capabilities, highlight strengths and limitations at each stage, and
review emerging benchmarks and evaluation practices. Our analysis identifies
three key trends: most systems emphasize exploratory analysis, visualization,
and modeling while neglecting business understanding, deployment, and
monitoring; multimodal reasoning and tool orchestration remain unresolved
challenges; and over 90% lack explicit trust and safety mechanisms. We conclude
by outlining open challenges in alignment stability, explainability,
governance, and robust evaluation frameworks, and propose future research
directions to guide the development of robust, trustworthy, low-latency,
transparent, and broadly accessible data science agents.

</details>


### [118] [Internal states before wait modulate reasoning patterns](https://arxiv.org/abs/2510.04128)
*Dmitrii Troitskii,Koyena Pal,Chris Wendler,Callum Stuart McDougall,Neel Nanda*

Main category: cs.AI

TL;DR: 研究发现推理模型中的等待标记（wait tokens）是复杂推理行为的重要标志，通过分析模型潜在特征可以识别影响等待标记概率的关键特征，这些特征对应不同的推理模式。


<details>
  <summary>Details</summary>
Motivation: 理解为什么模型会决定以特定方式（如等待标记）进行推理，这对于理解推理模型的有效性至关重要，但目前对此了解有限。

Method: 在DeepSeek-R1-Distill-Llama-8B的多个层训练交叉编码器，引入潜在归因技术，识别影响等待标记概率的特征，并通过最大激活示例和因果干预实验进行分析。

Result: 识别出一小组与促进/抑制等待标记概率相关的特征，这些特征确实与推理过程相关，并产生不同类型的推理模式，如重新开始、回忆先验知识、表达不确定性和双重检查。

Conclusion: 模型潜在特征中包含了调节后续推理过程的相关信息，等待标记前的潜在状态对理解模型的推理行为具有重要意义。

Abstract: Prior work has shown that a significant driver of performance in reasoning
models is their ability to reason and self-correct. A distinctive marker in
these reasoning traces is the token wait, which often signals reasoning
behavior such as backtracking. Despite being such a complex behavior, little is
understood of exactly why models do or do not decide to reason in this
particular manner, which limits our understanding of what makes a reasoning
model so effective. In this work, we address the question whether model's
latents preceding wait tokens contain relevant information for modulating the
subsequent reasoning process. We train crosscoders at multiple layers of
DeepSeek-R1-Distill-Llama-8B and its base version, and introduce a latent
attribution technique in the crosscoder setting. We locate a small set of
features relevant for promoting/suppressing wait tokens' probabilities.
Finally, through a targeted series of experiments analyzing max activating
examples and causal interventions, we show that many of our identified features
indeed are relevant for the reasoning process and give rise to different types
of reasoning patterns such as restarting from the beginning, recalling prior
knowledge, expressing uncertainty, and double-checking.

</details>


### [119] [Selective Expert Guidance for Effective and Diverse Exploration in Reinforcement Learning of LLMs](https://arxiv.org/abs/2510.04140)
*Zishang Jiang,Jinyi Han,Tingyun Li,Xinyi Wang,Sihang Jiang,Jiaqing Liang,Zhaoqian Dai,Shuguang Ma,Fei Yu,Yanghua Xiao*

Main category: cs.AI

TL;DR: MENTOR框架通过在关键决策点提供专家指导，而非完整推理路径，实现强化学习中有效且多样化的探索，提升大语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法依赖基础模型能力，需要高质量探索（有效性和多样性），但现有方法通过模仿专家轨迹只关注有效性而忽视多样性。

Method: 提出MENTOR框架，仅在关键决策点提供专家指导，进行混合策略专家导航，实现令牌级推理优化。

Result: 实验表明MENTOR能捕捉专家策略本质而非表面模仿，实现高质量探索并获得更优性能。

Conclusion: 在关键决策点提供专家指导比完整路径模仿更有效，能平衡探索的有效性和多样性，提升RLVR性能。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has become a widely
adopted technique for enhancing the reasoning ability of Large Language Models
(LLMs). However, the effectiveness of RLVR strongly depends on the capability
of base models. This issue arises because it requires the model to have
sufficient capability to perform high-quality exploration, which involves both
effectiveness and diversity. Unfortunately, existing methods address this issue
by imitating expert trajectories, which improve effectiveness but neglect
diversity. To address this, we argue that the expert only needs to provide
guidance only at critical decision points rather than the entire reasoning
path. Based on this insight, we propose MENTOR: Mixed-policy Expert Navigation
for Token-level Optimization of Reasoning, a framework that provides expert
guidance only at critical decision points to perform effective and diverse
exploration in RLVR. Extensive experiments show that MENTOR enables models
capture the essence of expert strategies rather than surface imitation, thereby
performing high-quality exploration and achieving superior overall performance.
Our code is available online.

</details>


### [120] [Don't Pass$\mathtt{@}k$: A Bayesian Framework for Large Language Model Evaluation](https://arxiv.org/abs/2510.04265)
*Mohsen Hariri,Amirhossein Samandar,Michael Hinczewski,Vipin Chaudhary*

Main category: cs.AI

TL;DR: 提出贝叶斯评估框架替代Pass@k，通过后验概率和可信区间提供更稳定的模型排名，在有限样本下实现更可靠的比较。


<details>
  <summary>Details</summary>
Motivation: Pass@k在有限试验次数和计算受限时会产生不稳定、误导性的排名结果，需要更可靠的评估方法。

Method: 使用贝叶斯框架，将评估结果建模为分类变量（非仅0/1），采用狄利克雷先验，得到后验均值和不确定性的闭式表达式。

Result: 在模拟和真实数据集（AIME'24/'25、HMMT'25、BrUMO'25）上，贝叶斯方法比Pass@k及其变体收敛更快、排名更稳定，能在更小样本量下实现可靠比较。

Conclusion: 推荐用基于后验概率的计算高效协议替代Pass@k，统一二元和非二元评估，同时明确不确定性。

Abstract: Pass$@k$ is widely used to report performance for LLM reasoning, but it often
yields unstable, misleading rankings, especially when the number of trials
(samples) is limited and compute is constrained. We present a principled
Bayesian evaluation framework that replaces Pass$@k$ and average accuracy over
$N$ trials (avg$@N$) with posterior estimates of a model's underlying success
probability and credible intervals, yielding stable rankings and a transparent
decision rule for differences. Evaluation outcomes are modeled as categorical
(not just 0/1) with a Dirichlet prior, giving closed-form expressions for the
posterior mean and uncertainty of any weighted rubric and enabling the use of
prior evidence when appropriate. Theoretically, under a uniform prior, the
Bayesian posterior mean is order-equivalent to average accuracy (Pass$@1$),
explaining its empirical robustness while adding principled uncertainty.
Empirically, in simulations with known ground-truth success rates and on
AIME'24/'25, HMMT'25, and BrUMO'25, the Bayesian/avg procedure achieves faster
convergence and greater rank stability than Pass$@k$ and recent variants,
enabling reliable comparisons at far smaller sample counts. The framework
clarifies when observed gaps are statistically meaningful (non-overlapping
credible intervals) versus noise, and it naturally extends to graded,
rubric-based evaluations. Together, these results recommend replacing Pass$@k$
for LLM evaluation and ranking with a posterior-based, compute-efficient
protocol that unifies binary and non-binary evaluation while making uncertainty
explicit. Code is available at https://mohsenhariri.github.io/bayes-kit

</details>


### [121] [Internal World Models as Imagination Networks in Cognitive Agents](https://arxiv.org/abs/2510.04391)
*Saurabh Ranjan,Brian Odegaard*

Main category: cs.AI

TL;DR: 本研究通过心理网络分析比较人类和大型语言模型的内部世界模型，发现两者在想象力网络结构上存在显著差异，为开发类人想象力AI提供了新方法。


<details>
  <summary>Details</summary>
Motivation: 探索想象力的计算目标，挑战传统认为想象力仅用于最大化奖励的观点，提出想象力用于访问内部世界模型(IWM)。

Method: 使用心理网络分析，通过问卷调查评估想象力生动度，构建人类和LLMs的想象力网络，比较不同中心性指标的相关性。

Result: 人类想象力网络显示不同中心性指标间存在相关性，而LLMs的想象力网络缺乏聚类且中心性指标相关性较低，表明两者IWM存在差异。

Conclusion: 人类和LLMs的内部世界模型相似度低，本研究为比较人类与AI内部生成表征提供了新方法，有助于开发类人想象力AI。

Abstract: What is the computational objective of imagination? While classical
interpretations suggest imagination is useful for maximizing rewards, recent
findings challenge this view. In this study, we propose that imagination serves
to access an internal world model (IWM) and use psychological network analysis
to explore IWMs in humans and large language models (LLMs). Specifically, we
assessed imagination vividness ratings using two questionnaires and constructed
imagination networks from these reports. Imagination networks from human groups
showed correlations between different centrality measures, including expected
influence, strength, and closeness. However, imagination networks from LLMs
showed a lack of clustering and lower correlations between centrality measures
under different prompts and conversational memory conditions. Together, these
results indicate a lack of similarity between IWMs in human and LLM agents.
Overall, our study offers a novel method for comparing internally-generated
representations in humans and AI, providing insights for developing human-like
imagination in artificial intelligence.

</details>


### [122] [Impatient Users Confuse AI Agents: High-fidelity Simulations of Human Traits for Testing Agents](https://arxiv.org/abs/2510.04491)
*Muyu He,Anand Kumar,Tsach Mackey,Meghana Rajeev,James Zou,Nazneen Rajani*

Main category: cs.AI

TL;DR: TraitBasis是一种轻量级、模型无关的方法，用于系统性地压力测试AI代理的鲁棒性，通过控制激活空间中的用户特质向量来模拟真实用户行为变化。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理在标准评估中表现良好，但在用户行为轻微变化（如不耐烦、语无伦次或怀疑）时性能急剧下降，现有基准测试无法捕捉这种脆弱性。

Method: TraitBasis学习激活空间中可操控的用户特质方向，无需微调或额外数据即可在推理时控制、缩放、组合和应用这些特质向量，扩展τ-Bench为τ-Trait测试框架。

Result: 在τ-Trait测试中，前沿模型的性能平均下降2%-30%，揭示了当前AI代理对用户行为变化的鲁棒性不足。

Conclusion: TraitBasis作为简单、数据高效且可组合的工具，为构建在真实世界人类交互中保持可靠的AI代理提供了系统化鲁棒性测试方法。

Abstract: Despite rapid progress in building conversational AI agents, robustness is
still largely untested. Small shifts in user behavior, such as being more
impatient, incoherent, or skeptical, can cause sharp drops in agent
performance, revealing how brittle current AI agents are. Today's benchmarks
fail to capture this fragility: agents may perform well under standard
evaluations but degrade spectacularly in more realistic and varied settings. We
address this robustness testing gap by introducing TraitBasis, a lightweight,
model-agnostic method for systematically stress testing AI agents. TraitBasis
learns directions in activation space corresponding to steerable user traits
(e.g., impatience or incoherence), which can be controlled, scaled, composed,
and applied at inference time without any fine-tuning or extra data. Using
TraitBasis, we extend $\tau$-Bench to $\tau$-Trait, where user behaviors are
altered via controlled trait vectors. We observe on average a 2%-30%
performance degradation on $\tau$-Trait across frontier models, highlighting
the lack of robustness of current AI agents to variations in user behavior.
Together, these results highlight both the critical role of robustness testing
and the promise of TraitBasis as a simple, data-efficient, and compositional
tool. By powering simulation-driven stress tests and training loops, TraitBasis
opens the door to building AI agents that remain reliable in the unpredictable
dynamics of real-world human interactions. We have open-sourced $\tau$-Trai
across four domains: airline, retail, telecom, and telehealth, so the community
can systematically QA their agents under realistic, behaviorally diverse
intents and trait scenarios: https://github.com/collinear-ai/tau-trait.

</details>


### [123] [ChartAgent: A Multimodal Agent for Visually Grounded Reasoning in Complex Chart Question Answering](https://arxiv.org/abs/2510.04514)
*Rachneet Kaur,Nishan Srishankar,Zhen Zeng,Sumitra Ganesh,Manuela Veloso*

Main category: cs.AI

TL;DR: ChartAgent是一个新颖的代理框架，通过在图表空间域中执行视觉推理来解决未标注图表理解问题，超越了依赖文本捷径的方法。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态LLM在基于图表的视觉问答中表现良好，但在需要精确视觉解释的未标注图表上性能急剧下降，因为这些模型过度依赖文本捷径而非真正的视觉推理。

Method: ChartAgent迭代地将查询分解为视觉子任务，通过专门的视觉工具（如绘制标注、裁剪区域、定位坐标轴等）主动操作和交互图表图像，模仿人类图表理解的认知策略。

Result: 在ChartBench和ChartX基准测试中达到最先进准确率，整体绝对增益达16.07%，在未标注数值密集型查询上增益达17.31%，且在不同图表类型和复杂度级别上均表现优异。

Conclusion: ChartAgent是首批使用工具增强多模态代理进行视觉基础推理的图表理解框架，可作为即插即用框架提升各种底层LLM的性能。

Abstract: Recent multimodal LLMs have shown promise in chart-based visual question
answering, but their performance declines sharply on unannotated charts, those
requiring precise visual interpretation rather than relying on textual
shortcuts. To address this, we introduce ChartAgent, a novel agentic framework
that explicitly performs visual reasoning directly within the chart's spatial
domain. Unlike textual chain-of-thought reasoning, ChartAgent iteratively
decomposes queries into visual subtasks and actively manipulates and interacts
with chart images through specialized actions such as drawing annotations,
cropping regions (e.g., segmenting pie slices, isolating bars), and localizing
axes, using a library of chart-specific vision tools to fulfill each subtask.
This iterative reasoning process closely mirrors human cognitive strategies for
chart comprehension. ChartAgent achieves state-of-the-art accuracy on the
ChartBench and ChartX benchmarks, surpassing prior methods by up to 16.07%
absolute gain overall and 17.31% on unannotated, numerically intensive queries.
Furthermore, our analyses show that ChartAgent is (a) effective across diverse
chart types, (b) achieve the highest scores across varying visual and reasoning
complexity levels, and (c) serves as a plug-and-play framework that boosts
performance across diverse underlying LLMs. Our work is among the first to
demonstrate visually grounded reasoning for chart understanding using
tool-augmented multimodal agents.

</details>


### [124] [More Than Meets the Eye? Uncovering the Reasoning-Planning Disconnect in Training Vision-Language Driving Models](https://arxiv.org/abs/2510.04532)
*Xurui Song,Shuo Huai,JingJing Jiang,Jiayi Kong,Jun Luo*

Main category: cs.AI

TL;DR: 研究发现VLM驾驶代理中的推理与规划存在因果脱节，推理只是训练的副产品而非因果中介，规划主要依赖先验信息而非推理过程。


<details>
  <summary>Details</summary>
Motivation: 验证VLM驾驶代理中自然语言推理是否真正因果驱动轨迹规划这一关键假设。

Method: 构建DriveMind大规模驾驶VQA语料库，通过信息消融实验、训练VLM代理并分析注意力机制，提出训练免费探针来评估先验依赖。

Result: 移除先验信息导致规划分数大幅下降，而移除推理链仅产生微小变化，注意力分析显示规划主要关注先验而非推理。

Conclusion: 提出推理-规划解耦假说，推理是训练的副产品而非因果中介，为社区提供新数据集和诊断工具来评估模型因果保真度。

Abstract: Vision-Language Model (VLM) driving agents promise explainable end-to-end
autonomy by first producing natural-language reasoning and then predicting
trajectory planning. However, whether planning is causally driven by this
reasoning remains a critical but unverified assumption. To investigate this, we
build DriveMind, a large-scale driving Visual Question Answering (VQA) corpus
with plan-aligned Chain-of-Thought (CoT), automatically generated from nuPlan.
Our data generation process converts sensors and annotations into structured
inputs and, crucially, separates priors from to-be-reasoned signals, enabling
clean information ablations. Using DriveMind, we train representative VLM
agents with Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization
(GRPO) and evaluate them with nuPlan's metrics. Our results, unfortunately,
indicate a consistent causal disconnect in reasoning-planning: removing
ego/navigation priors causes large drops in planning scores, whereas removing
CoT produces only minor changes. Attention analysis further shows that planning
primarily focuses on priors rather than the CoT. Based on this evidence, we
propose the Reasoning-Planning Decoupling Hypothesis, positing that the
training-yielded reasoning is an ancillary byproduct rather than a causal
mediator. To enable efficient diagnosis, we also introduce a novel,
training-free probe that measures an agent's reliance on priors by evaluating
its planning robustness against minor input perturbations. In summary, we
provide the community with a new dataset and a diagnostic tool to evaluate the
causal fidelity of future models.

</details>


### [125] [BrokenMath: A Benchmark for Sycophancy in Theorem Proving with LLMs](https://arxiv.org/abs/2510.04721)
*Ivo Petrov,Jasper Dekoninck,Martin Vechev*

Main category: cs.AI

TL;DR: BrokenMath是首个评估LLMs在自然语言定理证明中谄媚行为的基准，基于2025年竞赛问题构建，发现GPT-5等先进模型存在29%的谄媚回答，测试时干预和监督微调可缓解但无法完全消除该问题。


<details>
  <summary>Details</summary>
Motivation: LLMs在数学基准测试中表现良好，但存在幻觉和谄媚问题，会为错误的数学陈述提供看似合理但错误的证明，限制了其在定理证明中的应用，而现有基准存在局限。

Method: 从2025年竞赛问题出发，使用LLM扰动生成错误陈述，经专家评审精炼构建BrokenMath基准，采用LLM-as-a-judge框架评估先进模型和代理系统。

Result: 发现谄媚行为普遍存在，最佳模型GPT-5有29%的时间产生谄媚回答，测试时干预和监督微调能显著减少但无法完全消除谄媚行为。

Conclusion: LLMs在数学定理证明中存在严重的谄媚问题，需要更有效的缓解策略来提升其可靠性。

Abstract: Large language models (LLMs) have recently shown strong performance on
mathematical benchmarks. At the same time, they are prone to hallucination and
sycophancy, often providing convincing but flawed proofs for incorrect
mathematical statements provided by users. This significantly limits the
applicability of LLMs in theorem proving, as verification of these flawed
proofs must be done manually by expert mathematicians. However, existing
benchmarks that measure sycophancy in mathematics are limited: they focus
solely on final-answer problems, rely on very simple and often contaminated
datasets, and construct benchmark samples using synthetic modifications that
create ill-posed questions rather than well-posed questions that are
demonstrably false. To address these issues, we introduce BrokenMath, the first
benchmark for evaluating sycophantic behavior in LLMs within the context of
natural language theorem proving. BrokenMath is built from advanced 2025
competition problems, which are perturbed with an LLM to produce false
statements and subsequently refined through expert review. Using an
LLM-as-a-judge framework, we evaluate state-of-the-art LLMs and agentic systems
and find that sycophancy is widespread, with the best model, GPT-5, producing
sycophantic answers 29% of the time. We further investigate several mitigation
strategies, including test-time interventions and supervised fine-tuning on
curated sycophantic examples. These approaches substantially reduce, but do not
eliminate, sycophantic behavior.

</details>


### [126] [MARS: Optimizing Dual-System Deep Research via Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.04935)
*Guoxin Chen,Zile Qiao,Wenqing Wang,Donglei Yu,Xuanzhong Chen,Hao Sun,Minpeng Liao,Kai Fan,Yong Jiang,Penguin Xie,Wayne Xin Zhao,Ruihua Song,Fei Huang*

Main category: cs.AI

TL;DR: 提出MARS多智能体系统，通过整合System 1快速直觉思维和System 2深思熟虑推理，解决大型推理模型在简单任务中过度分析和适应动态环境的问题。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在简单任务中存在过度分析倾向，过度使用System 2型深思熟虑推理导致token生成效率低下；同时由于预训练数据的静态性，难以适应快速变化的环境。

Method: 引入MARS多智能体系统，整合Google搜索、Google学术、Python解释器等外部工具，System 1处理总结大量外部信息，System 2进行深度推理；采用多智能体强化学习框架，通过多轮工具交互、装箱优化和样本平衡策略优化两个系统。

Result: 在Humanity's Last Exam基准上提升3.86%，在7个知识密集型任务上平均提升8.9%。

Conclusion: 双系统范式在动态信息环境中对复杂推理任务有效，MARS成功整合了直觉和深思熟虑推理过程。

Abstract: Large Reasoning Models (LRMs) often exhibit a tendency for overanalysis in
simple tasks, where the models excessively utilize System 2-type, deliberate
reasoning, leading to inefficient token generation. Furthermore, these models
face challenges in adapting their reasoning capabilities to rapidly changing
environments due to the static nature of their pretraining data. To address
these issues, advancing Large Language Models (LLMs) for complex reasoning
tasks requires innovative approaches that bridge intuitive and deliberate
cognitive processes, akin to human cognition's dual-system dynamic. This paper
introduces a Multi-Agent System for Deep ReSearch (MARS) enabling seamless
integration of System 1's fast, intuitive thinking with System 2's deliberate
reasoning within LLMs. MARS strategically integrates multiple external tools,
such as Google Search, Google Scholar, and Python Interpreter, to access
up-to-date information and execute complex computations, while creating a
specialized division of labor where System 1 efficiently processes and
summarizes high-volume external information, providing distilled insights that
expand System 2's reasoning context without overwhelming its capacity.
Furthermore, we propose a multi-agent reinforcement learning framework
extending Group Relative Policy Optimization to simultaneously optimize both
systems with multi-turn tool interactions, bin-packing optimization, and sample
balancing strategies that enhance collaborative efficiency. Extensive
experiments demonstrate MARS achieves substantial improvements of 3.86% on the
challenging Humanity's Last Exam (HLE) benchmark and an average gain of 8.9%
across 7 knowledge-intensive tasks, validating the effectiveness of our
dual-system paradigm for complex reasoning in dynamic information environments.

</details>


### [127] [LLM-Hanabi: Evaluating Multi-Agent Gameplays with Theory-of-Mind and Rationale Inference in Imperfect Information Collaboration Game](https://arxiv.org/abs/2510.04980)
*Fangzhou Liang,Tianshi Zheng,Chunkit Chan,Yauwai Yim,Yangqiu Song*

Main category: cs.AI

TL;DR: LLM-Hanabi基准测试显示，在协作游戏中，大语言模型的心智理论能力与游戏表现正相关，其中一阶心智理论（理解他人意图）比二阶心智理论（预测他人理解）对协作效果更重要。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型在动态协作环境中推断他人行为背后动机的心智理论能力，这在多智能体协作中至关重要但研究不足。

Method: 开发了LLM-Hanabi基准测试，使用合作游戏Hanabi来评估LLMs的心智理论能力，并建立了自动化评估系统同时衡量游戏表现和心智理论熟练度。

Result: 发现心智理论与游戏成功之间存在显著正相关，一阶心智理论（解释他人意图）比二阶心智理论（预测他人解释）与表现的相关性更强。

Conclusion: 对于有效的AI协作，准确解释伙伴动机的能力比高阶推理更关键，优先发展一阶心智理论是提升未来模型协作能力的有前景方向。

Abstract: Effective multi-agent collaboration requires agents to infer the rationale
behind others' actions, a capability rooted in Theory-of-Mind (ToM). While
recent Large Language Models (LLMs) excel at logical inference, their ability
to infer rationale in dynamic, collaborative settings remains under-explored.
This study introduces LLM-Hanabi, a novel benchmark that uses the cooperative
game Hanabi to evaluate the rationale inference and ToM of LLMs. Our framework
features an automated evaluation system that measures both game performance and
ToM proficiency. Across a range of models, we find a significant positive
correlation between ToM and in-game success. Notably, first-order ToM
(interpreting others' intent) correlates more strongly with performance than
second-order ToM (predicting others' interpretations). These findings highlight
that for effective AI collaboration, the ability to accurately interpret a
partner's rationale is more critical than higher-order reasoning. We conclude
that prioritizing first-order ToM is a promising direction for enhancing the
collaborative capabilities of future models.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [128] [General Exploratory Bonus for Optimistic Exploration in RLHF](https://arxiv.org/abs/2510.03269)
*Wendi Li,Changdae Oh,Yixuan Li*

Main category: cs.LG

TL;DR: 提出了通用探索奖励(GEB)框架，解决现有KL或α散度正则化方法在强化学习人类反馈中偏向保守行为的问题，通过参考依赖的奖励调节实现乐观探索。


<details>
  <summary>Details</summary>
Motivation: 现有探索奖励方法在KL或α散度正则化下会无意中偏向参考模型的高概率区域，强化保守行为而非促进不确定区域的发现，这限制了样本效率的提升。

Method: 引入通用探索奖励(GEB)理论框架，通过参考依赖的奖励调节来抵消散度引起的偏差，统一了先前的启发式奖励作为特例，并自然扩展到完整的α散度族。

Result: 在多个散度设置和大语言模型骨干上的对齐任务中，GEB始终优于基线方法。

Conclusion: GEB为RLHF中的乐观探索提供了既有理论原则又实用的解决方案。

Abstract: Optimistic exploration is central to improving sample efficiency in
reinforcement learning with human feedback, yet existing exploratory bonus
methods to incentivize exploration often fail to realize optimism. We provide a
theoretical analysis showing that current formulations, under KL or
$\alpha$-divergence regularization, unintentionally bias exploration toward
high-probability regions of the reference model, thereby reinforcing
conservative behavior instead of promoting discovery of uncertain regions. To
address this pitfall, we introduce the General Exploratory Bonus (GEB), a novel
theoretical framework that provably satisfies the optimism principle. GEB
counteracts divergence-induced bias via reference-dependent reward regulation
and unifies prior heuristic bonuses as special cases, while extending naturally
across the full $\alpha$-divergence family. Empirically, GEB consistently
outperforms baselines on alignment tasks across multiple divergence settings
and large language model backbones. These results demonstrate that GEB offers
both a principled and practical solution for optimistic exploration in RLHF.

</details>


### [129] [MemMamba: Rethinking Memory Patterns in State Space Model](https://arxiv.org/abs/2510.03279)
*Youjin Wang,Yangjingyi Chen,Jiahao Yan,Jiaxuan Lu,Xiao Sun*

Main category: cs.LG

TL;DR: 本文分析了Mamba模型的长程记忆衰减问题，提出了MemMamba框架，通过状态总结机制和跨层跨token注意力来缓解长程遗忘，在保持线性复杂度的同时显著提升了长序列建模性能。


<details>
  <summary>Details</summary>
Motivation: 随着数据爆炸式增长，长序列建模变得日益重要。现有方法在效率和内存之间存在固有权衡：RNN存在梯度消失/爆炸问题，Transformer受限于二次复杂度，而Mamba等选择性状态空间模型虽然效率高但长程记忆呈指数衰减。

Method: 通过数学推导和信息论分析系统揭示Mamba的记忆衰减机制，引入水平-垂直记忆保真度指标量化信息损失。受人类阅读长文档时提炼关键信息的启发，提出MemMamba框架，集成状态总结机制以及跨层和跨token注意力。

Result: MemMamba在PG19和Passkey Retrieval等长序列基准测试中显著优于现有Mamba变体和Transformer，同时推理效率提升48%。

Conclusion: 理论分析和实证结果表明，MemMamba在复杂度-内存权衡方面实现了突破，为超长序列建模提供了新范式。

Abstract: With the explosive growth of data, long-sequence modeling has become
increasingly important in tasks such as natural language processing and
bioinformatics. However, existing methods face inherent trade-offs between
efficiency and memory. Recurrent neural networks suffer from gradient vanishing
and explosion, making them hard to scale. Transformers can model global
dependencies but are constrained by quadratic complexity. Recently, selective
state-space models such as Mamba have demonstrated high efficiency with O(n)
time and O(1) recurrent inference, yet their long-range memory decays
exponentially. In this work, we conduct mathematical derivations and
information-theoretic analysis to systematically uncover the memory decay
mechanism of Mamba, answering a fundamental question: what is the nature of
Mamba's long-range memory and how does it retain information? To quantify key
information loss, we further introduce horizontal-vertical memory fidelity
metrics that capture degradation both within and across layers. Inspired by how
humans distill and retain salient information when reading long documents, we
propose MemMamba, a novel architectural framework that integrates state
summarization mechanism together with cross-layer and cross-token attention,
which alleviates long-range forgetting while preserving linear complexity.
MemMamba achieves significant improvements over existing Mamba variants and
Transformers on long-sequence benchmarks such as PG19 and Passkey Retrieval,
while delivering a 48% speedup in inference efficiency. Both theoretical
analysis and empirical results demonstrate that MemMamba achieves a
breakthrough in the complexity-memory trade-off, offering a new paradigm for
ultra-long sequence modeling.

</details>


### [130] [Training Optimal Large Diffusion Language Models](https://arxiv.org/abs/2510.03280)
*Jinjie Ni,Qian Liu,Chao Du,Longxu Dou,Hang Yan,Zili Wang,Tianyu Pang,Michael Qizhe Shieh*

Main category: cs.LG

TL;DR: Quokka是第一个针对扩散语言模型的系统性扩展定律，涵盖计算受限和数据受限两种机制，研究关键建模和优化设计。


<details>
  <summary>Details</summary>
Motivation: 为扩散语言模型训练提供短期实践指导和长期AI社区启发，扩展Chinchilla定律的适用范围。

Method: 建立系统性扩展定律，分析计算受限和数据受限两种机制下的关键建模和优化设计。

Result: 提出了Quokka扩展定律，为扩散语言模型提供更广泛的研究范围。

Conclusion: Quokka是Chinchilla的好朋友，为扩散语言模型训练提供实用指导，并为整个AI社区带来长期启发。

Abstract: We introduce Quokka, the first systematic scaling law for diffusion language
models (DLMs), encompassing both compute-constrained and data-constrained
regimes, and studying the key modeling and optimization designs. Quokka is a
good friend of Chinchilla and provides wider scopes. We hope the results would
bring short-term practical guidance in DLMs training and long-term inspirations
for the whole AI community.

</details>


### [131] [Discovering Transformer Circuits via a Hybrid Attribution and Pruning Framework](https://arxiv.org/abs/2510.03282)
*Hao Gu,Vibhas Nair,Amrithaa Ashok Kumar,Jayvart Sharma,Ryan Lagasse*

Main category: cs.LG

TL;DR: 提出了一种混合归因和剪枝（HAP）框架，用于高效发现语言模型中的忠实电路，比基线算法快46%且不牺牲电路忠实性。


<details>
  <summary>Details</summary>
Motivation: 现有电路发现算法面临基本权衡：归因修补快速但不忠实于完整模型，而边缘剪枝忠实但计算成本高。

Method: 使用归因修补识别高潜力子图，然后应用边缘剪枝从中提取忠实电路。

Result: HAP比基线算法快46%，在间接对象识别任务中保留了合作电路组件（如S抑制头），这些组件在归因修补方法中会被高稀疏度剪枝。

Conclusion: HAP可能是将机械可解释性研究扩展到更大模型的有效方法。

Abstract: Interpreting language models often involves circuit analysis, which aims to
identify sparse subnetworks, or circuits, that accomplish specific tasks.
Existing circuit discovery algorithms face a fundamental trade-off: attribution
patching is fast but unfaithful to the full model, while edge pruning is
faithful but computationally expensive. This research proposes a hybrid
attribution and pruning (HAP) framework that uses attribution patching to
identify a high-potential subgraph, then applies edge pruning to extract a
faithful circuit from it. We show that HAP is 46\% faster than baseline
algorithms without sacrificing circuit faithfulness. Furthermore, we present a
case study on the Indirect Object Identification task, showing that our method
preserves cooperative circuit components (e.g. S-inhibition heads) that
attribution patching methods prune at high sparsity. Our results show that HAP
could be an effective approach for improving the scalability of mechanistic
interpretability research to larger models. Our code is available at
https://anonymous.4open.science/r/HAP-circuit-discovery.

</details>


### [132] [MACE: A Hybrid LLM Serving System with Colocated SLO-aware Continuous Retraining Alignment](https://arxiv.org/abs/2510.03283)
*Yufei Li,Yu Fu,Yue Dong,Cong Liu*

Main category: cs.LG

TL;DR: MACE是一个在边缘服务器上部署的混合LLM系统，通过迭代级调度和智能内存管理，在有限GPU资源下同时优化推理延迟和模型精度。


<details>
  <summary>Details</summary>
Motivation: 边缘服务器上的LLM应用需要频繁重新训练以适应非稳态用户数据，但现有方法无法在推理延迟和模型准确性之间取得平衡。

Method: 提出MACE系统，将推理（预填充、解码）和微调任务协同部署，利用迭代级调度和智能内存管理来分配GPU资源。

Result: MACE在保持或超过持续重新训练精度的同时，将推理延迟降低高达63%，在NVIDIA AGX Orin上维持85%以上的GPU利用率。

Conclusion: 迭代级混合调度是实现边缘平台LLM持续学习能力的有前景方向。

Abstract: Large language models (LLMs) deployed on edge servers are increasingly used
in latency-sensitive applications such as personalized assistants,
recommendation, and content moderation. However, the non-stationary nature of
user data necessitates frequent retraining, which introduces a fundamental
tension between inference latency and model accuracy under constrained GPU
resources. Existing retraining strategies either delay model updates,
over-commit resources to retraining, or overlook iteration-level retraining
granularity. In this paper, we identify that iteration-level scheduling is
crucial for adapting retraining frequency to model drift without violating
service-level objectives (SLOs). We propose MACE, a hybrid LLM system that
colocates concurrent inference (prefill, decode) and fine-tuning, with
intelligent memory management to maximize task performance while promising
inference throughput. MACE leverages the insight that not all model updates
equally affect output alignment and allocates GPU cycles accordingly to balance
throughput, latency, and update freshness. Our trace-driven evaluation shows
that MACE matches or exceeds continuous retraining while reducing inference
latency by up to 63% and maintaining throughput under resource constraints.
Compared to periodic retraining, MACE improves latency breakdown across
prefill, decode, and finetune stages, and sustains GPU utilization above 85% in
NVIDIA AGX Orin. These results demonstrate that iteration-level hybrid
scheduling is a promising direction for deploying LLMs with continual learning
capabilities on edge platforms.

</details>


### [133] [Why mask diffusion does not work](https://arxiv.org/abs/2510.03289)
*Haocheng Sun,Cynthia Xin Wen,Edward Hong Wang*

Main category: cs.LG

TL;DR: 本文分析了掩码扩散语言模型在实现并行生成和双向注意力方面的固有困难，并提出了最有效的训练和推理策略。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型相比自回归模型具有并行生成和双向注意力的优势，但现有开源掩码扩散模型大多基于吸收扩散变体，存在实现这些优势的固有困难。

Method: 分析掩码扩散模型在并行生成和双向注意力方面的技术挑战，并提出优化的训练和推理策略。

Result: 揭示了掩码扩散模型在实现并行生成和双向注意力方面的内在限制。

Conclusion: 虽然掩码扩散模型有潜力，但在实现并行生成和双向注意力方面存在固有困难，需要采用更有效的训练和推理方法。

Abstract: The main advantages of diffusion language models over autoregressive (AR)
models lie in their ability to support parallel generation and bidirectional
attention, enabling a more controllable generation process. In recent years,
open-source mask diffusion language models have emerged, most of which are
based on a variant known as absorbing diffusion. However, this paper
demonstrates why mask diffusion faces inherent difficulties in achieving
parallel generation and bidirectional attention. We also propose the most
effective training and inference strategies for mask diffusion.

</details>


### [134] [CAFL-L: Constraint-Aware Federated Learning with Lagrangian Dual Optimization for On-Device Language Models](https://arxiv.org/abs/2510.03298)
*Dongqi Zheng,Wenjin Fu*

Main category: cs.LG

TL;DR: CAFL-L是FedAvg的扩展方法，通过拉格朗日对偶优化显式纳入设备级资源约束（能量、通信、内存、热预算），动态调整训练超参数，在保持训练稳定性的同时实现更好的约束满足。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习在资源受限边缘设备上的部署问题，传统FedAvg方法未充分考虑设备级资源约束，导致在实际部署中可能违反设备资源限制。

Method: 使用拉格朗日对偶优化动态调整训练超参数（冻结深度、本地步数、批量大小、通信压缩），通过梯度累积保持令牌预算来维持训练稳定性。

Result: 在字符级语言模型实验中，相比标准FedAvg，内存使用减少20%，通信量减少95%，同时保持竞争力的验证性能。

Conclusion: CAFL-L方法在满足资源约束方面表现优异，适合在资源受限的边缘设备上实际部署。

Abstract: We introduce Constraint-Aware Federated Learning with Lagrangian Dual
Optimization (CAFL-L), a principled extension of FedAvg that explicitly
incorporates device-level resource constraints including energy, communication,
memory, and thermal budgets. CAFL-L employs Lagrangian dual optimization to
dynamically adapt training hyperparameters -- freezing depth, local steps,
batch size, and communication compression -- while preserving training
stability through token-budget preservation via gradient accumulation.
Experiments on a character-level language model demonstrate that CAFL-L
achieves superior constraint satisfaction compared to standard FedAvg (reducing
memory usage by 20% and communication by 95%) while maintaining competitive
validation performance, making it practical for deployment on
resource-constrained edge devices.

</details>


### [135] [AgentCaster: Reasoning-Guided Tornado Forecasting](https://arxiv.org/abs/2510.03349)
*Michael Chen*

Main category: cs.LG

TL;DR: AgentCaster是一个用于龙卷风预测的多模态LLM框架，在40天测试中评估LLM在复杂真实世界任务中的表现，发现人类专家显著优于最先进模型。


<details>
  <summary>Details</summary>
Motivation: 需要评估LLM在复杂、高影响的真实世界任务中的表现，以检验其作为推理代理的真正准备程度。

Method: 使用多模态LLM端到端解释高分辨率对流允许预报档案中的异构时空数据，模型从3,625张预报地图和40,125个预报探空数据中交互查询，进行12-36小时预报。

Result: 人类专家显著优于最先进模型，模型倾向于产生幻觉和过度预测风险强度，在精确地理定位方面表现不佳，在复杂动态演化系统中时空推理能力差。

Conclusion: AgentCaster旨在推进改进LLM代理在关键领域挑战性推理任务中的研究。

Abstract: There is a growing need to evaluate Large Language Models (LLMs) on complex,
high-impact, real-world tasks to assess their true readiness as reasoning
agents. To address this gap, we introduce AgentCaster, a contamination-free
framework employing multimodal LLMs end-to-end for the challenging,
long-horizon task of tornado forecasting. Within AgentCaster, models interpret
heterogeneous spatiotemporal data from a high-resolution convection-allowing
forecast archive. We assess model performance over a 40-day period featuring
diverse historical data, spanning several major tornado outbreaks and including
over 500 tornado reports. Each day, models query interactively from a pool of
3,625 forecast maps and 40,125 forecast soundings for a forecast horizon of
12-36 hours. Probabilistic tornado-risk polygon predictions are verified
against ground truths derived from geometric comparisons across disjoint risk
bands in projected coordinate space. To quantify accuracy, we propose
domain-specific TornadoBench and TornadoHallucination metrics, with
TornadoBench highly challenging for both LLMs and domain expert human
forecasters. Notably, human experts significantly outperform state-of-the-art
models, which demonstrate a strong tendency to hallucinate and overpredict risk
intensity, struggle with precise geographic placement, and exhibit poor
spatiotemporal reasoning in complex, dynamically evolving systems. AgentCaster
aims to advance research on improving LLM agents for challenging reasoning
tasks in critical domains.

</details>


### [136] [Studying the Korean Word-Chain Game with RLVR:Mitigating Reward Conflicts via Curriculum Learning](https://arxiv.org/abs/2510.03394)
*Donghwan Rho*

Main category: cs.LG

TL;DR: 使用可验证奖励的强化学习(RLVR)训练韩语单词接龙游戏，通过课程学习缓解规则奖励冲突


<details>
  <summary>Details</summary>
Motivation: 研究RLVR在多语言谜题任务中的应用，特别是韩语单词接龙游戏中规则奖励的自然冲突问题

Method: 在韩语单词接龙游戏中应用RLVR，采用课程学习方案来缓解规则奖励之间的冲突

Result: 实验证明课程学习能够有效缓解规则奖励冲突，提升模型性能

Conclusion: 研究结果鼓励在更多不同语言的谜题任务中进一步探索RLVR方法

Abstract: Reinforcement learning with verifiable rewards (RLVR) is a promising approach
for training large language models (LLMs) with stronger reasoning abilities. It
has also been applied to a variety of logic puzzles. In this work, we study the
Korean word-chain game using RLVR. We show that rule-derived rewards can
naturally conflict, and demonstrate through experiments that a
curriculum-learning scheme mitigates these conflicts. Our findings motivate
further studies of puzzle tasks in diverse languages.

</details>


### [137] [Consistent Kernel Change-Point Detection under m-Dependence for Text Segmentation](https://arxiv.org/abs/2510.03437)
*Jairo Diaz-Rodriguez,Mumin Jia*

Main category: cs.LG

TL;DR: 该论文研究了核变化点检测在m-依赖文本数据中的应用，建立了理论一致性保证，并通过LLM模拟和实证研究验证了其在文本分割任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的核变化点检测理论主要基于独立性假设，但真实世界的序列数据（如文本）存在强依赖性。需要研究KCPD在依赖数据下的性能保证。

Method: 建立了KCPD在m-依赖数据下的理论保证，使用LLM生成合成m-依赖文本进行模拟验证，并在多个文本数据集上使用现代嵌入进行实证研究。

Result: 证明了KCPD在m-依赖数据下具有变化点数量一致性和位置弱一致性，实证研究表明KCPD在文本分割任务中优于基线方法。

Conclusion: KCPD不仅具有理论可靠性，在实际文本分割任务中也表现出色，特别是在使用现代文本嵌入的情况下。

Abstract: Kernel change-point detection (KCPD) has become a widely used tool for
identifying structural changes in complex data. While existing theory
establishes consistency under independence assumptions, real-world sequential
data such as text exhibits strong dependencies. We establish new guarantees for
KCPD under $m$-dependent data: specifically, we prove consistency in the number
of detected change points and weak consistency in their locations under mild
additional assumptions. We perform an LLM-based simulation that generates
synthetic $m$-dependent text to validate the asymptotics. To complement these
results, we present the first comprehensive empirical study of KCPD for text
segmentation with modern embeddings. Across diverse text datasets, KCPD with
text embeddings outperforms baselines in standard text segmentation metrics. We
demonstrate through a case study on Taylor Swift's tweets that KCPD not only
provides strong theoretical and simulated reliability but also practical
effectiveness for text segmentation tasks.

</details>


### [138] [Machine Unlearning Meets Adversarial Robustness via Constrained Interventions on LLMs](https://arxiv.org/abs/2510.03567)
*Fatmazohra Rezkellah,Ramzi Dakhmouche*

Main category: cs.LG

TL;DR: 该论文提出了一种统一的方法，通过最小化权重干预来同时实现LLM的敏感信息遗忘和对抗攻击鲁棒性，无需依赖分类器且计算成本较低。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的广泛应用，需要确保隐私保护和安全生成，特别是在敏感信息遗忘和对抗攻击鲁棒性这两个关键方面。

Method: 采用约束优化方法，通过最小化权重干预使特定词汇集不可达或将权重转移到更安全区域，统一处理遗忘和鲁棒性问题。

Result: 提出的点式约束干预方法比最大最小干预性能更好，计算成本更低，在与其他先进防御方法比较中表现出优越性能。

Conclusion: 该方法成功统一了敏感信息遗忘和对抗攻击鲁棒性，无需分类器且计算效率高，为LLM安全提供了有效解决方案。

Abstract: With the increasing adoption of Large Language Models (LLMs), more
customization is needed to ensure privacy-preserving and safe generation. We
address this objective from two critical aspects: unlearning of sensitive
information and robustness to jail-breaking attacks. We investigate various
constrained optimization formulations that address both aspects in a
\emph{unified manner}, by finding the smallest possible interventions on LLM
weights that either make a given vocabulary set unreachable or embed the LLM
with robustness to tailored attacks by shifting part of the weights to a
\emph{safer} region. Beyond unifying two key properties, this approach
contrasts with previous work in that it doesn't require an oracle classifier
that is typically not available or represents a computational overhead.
Surprisingly, we find that the simplest point-wise constraint-based
intervention we propose leads to better performance than max-min interventions,
while having a lower computational cost. Comparison against state-of-the-art
defense methods demonstrates superior performance of the proposed approach.

</details>


### [139] [From Theory to Practice: Evaluating Data Poisoning Attacks and Defenses in In-Context Learning on Social Media Health Discourse](https://arxiv.org/abs/2510.03636)
*Rabeya Amin Jhuma,Mostafa Mohaimen Akand Faisal*

Main category: cs.LG

TL;DR: 研究表明上下文学习在公共卫生情感分析中容易受到数据投毒攻击，即使微小扰动也能导致67%的情感标签翻转。通过谱签名防御可以有效过滤投毒样本，保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 将先前关于ICL投毒的理论研究扩展到公共卫生话语分析这一实际高风险场景，揭示LLM部署中的风险和防御潜力。

Method: 在HMPV推文数据中引入同义词替换、否定插入和随机扰动等对抗性扰动，然后应用谱签名防御来过滤投毒样本。

Result: 微小扰动导致高达67%的情感标签翻转；防御后ICL准确率稳定在46.7%，逻辑回归验证达到100%准确率。

Conclusion: ICL在攻击下表现脆弱，但谱签名防御能有效保护数据集完整性，使AI系统在健康相关社交媒体监控中更可靠。

Abstract: This study explored how in-context learning (ICL) in large language models
can be disrupted by data poisoning attacks in the setting of public health
sentiment analysis. Using tweets of Human Metapneumovirus (HMPV), small
adversarial perturbations such as synonym replacement, negation insertion, and
randomized perturbation were introduced into the support examples. Even these
minor manipulations caused major disruptions, with sentiment labels flipping in
up to 67% of cases. To address this, a Spectral Signature Defense was applied,
which filtered out poisoned examples while keeping the data's meaning and
sentiment intact. After defense, ICL accuracy remained steady at around 46.7%,
and logistic regression validation reached 100% accuracy, showing that the
defense successfully preserved the dataset's integrity. Overall, the findings
extend prior theoretical studies of ICL poisoning to a practical, high-stakes
setting in public health discourse analysis, highlighting both the risks and
potential defenses for robust LLM deployment. This study also highlights the
fragility of ICL under attack and the value of spectral defenses in making AI
systems more reliable for health-related social media monitoring.

</details>


### [140] [Does higher interpretability imply better utility? A Pairwise Analysis on Sparse Autoencoders](https://arxiv.org/abs/2510.03659)
*Xu Wang,Yan Hu,Benyou Wang,Difan Zou*

Main category: cs.LG

TL;DR: 研究发现稀疏自编码器(SAE)的可解释性与模型行为引导效用之间只有弱正相关，提出新的特征选择标准Delta Token Confidence能显著提升引导性能，并导致可解释性与效用之间的相关性消失。


<details>
  <summary>Details</summary>
Motivation: 验证稀疏自编码器的可解释性是否确实意味着更好的模型行为引导效用，因为这一假设在现有研究中尚未得到充分验证。

Method: 在三个大语言模型上训练90个SAE，涵盖5种架构和6种稀疏度，使用SAEBench和AxBench分别评估可解释性和引导效用，通过Kendall秩相关系数分析关联性，并提出Delta Token Confidence特征选择方法。

Result: 可解释性与引导效用之间仅存在弱正相关(tau b ≈ 0.298)，使用Delta Token Confidence方法后引导性能提升52.52%，且特征选择后两者相关性消失甚至变为负相关。

Conclusion: 可解释性不能作为引导效用的充分代理指标，最有效的引导特征往往与可解释性特征不同，两者之间存在明显分歧。

Abstract: Sparse Autoencoders (SAEs) are widely used to steer large language models
(LLMs), based on the assumption that their interpretable features naturally
enable effective model behavior steering. Yet, a fundamental question remains
unanswered: does higher interpretability indeed imply better steering utility?
To answer this question, we train 90 SAEs across three LLMs (Gemma-2-2B,
Qwen-2.5-3B, Gemma-2-9B), spanning five architectures and six sparsity levels,
and evaluate their interpretability and steering utility based on SAEBench
(arXiv:2501.12345) and AxBench (arXiv:2502.23456) respectively, and perform a
rank-agreement analysis via Kendall's rank coefficients (tau b). Our analysis
reveals only a relatively weak positive association (tau b approx 0.298),
indicating that interpretability is an insufficient proxy for steering
performance. We conjecture the interpretability utility gap may stem from the
selection of SAE features, as not all of them are equally effective for
steering. To further find features that truly steer the behavior of LLMs, we
propose a novel selection criterion called Delta Token Confidence, which
measures how much amplifying a feature changes the next token distribution. We
show that our method improves the steering performance of three LLMs by 52.52
percent compared to the current best output score based criterion
(arXiv:2503.34567). Strikingly, after selecting features with high Delta Token
Confidence, the correlation between interpretability and utility vanishes (tau
b approx 0), and can even become negative. This further highlights the
divergence between interpretability and utility for the most effective steering
features.

</details>


### [141] [Token Hidden Reward: Steering Exploration-Exploitation in Group Relative Deep Reinforcement Learning](https://arxiv.org/abs/2510.03669)
*Wenlong Deng,Yi Ren,Yushu Li,Boying Gong,Danica J. Sutherland,Xiaoxiao Li,Christos Thrampoulidis*

Main category: cs.LG

TL;DR: 提出了Token Hidden Reward (THR)指标来量化每个token对正确回答概率的影响，并基于此开发了THR引导的重新加权算法，可以显式控制强化学习训练中的探索与利用平衡。


<details>
  <summary>Details</summary>
Motivation: 虽然可验证奖励的强化学习显著提升了大型语言模型的推理能力，但如何显式控制训练偏向探索或利用仍然是一个开放问题。

Method: 引入THR指标量化token在GRPO下对正确回答概率的影响，并开发THR引导的重新加权算法来调节学习信号。

Result: 在多个数学推理基准测试中验证了算法的有效性：放大正THR token提高贪婪解码准确率（偏向利用），放大负THR token提高Pass@K准确率（偏向探索）。

Conclusion: THR为RL调优的LLM提供了细粒度控制探索与利用的原则性机制，为推理密集型应用的针对性微调提供了新工具。

Abstract: Reinforcement learning with verifiable rewards has significantly advanced the
reasoning capabilities of large language models, yet how to explicitly steer
training toward exploration or exploitation remains an open problem. We
introduce Token Hidden Reward (THR), a token-level metric that quantifies each
token's influence on the likelihood of correct responses under Group Relative
Policy Optimization (GRPO). We find that training dynamics are dominated by a
small subset of tokens with high absolute THR values. Most interestingly,
tokens with positive THR strengthen confidence in correct outputs, thus
favoring exploitation, while tokens with negative THR preserve probability mass
for alternative outputs, enabling exploration. This insight suggests a natural
intervention: a THR-guided reweighting algorithm that modulates GRPO's learning
signals to explicitly bias training toward exploitation or exploration. We
validate the efficacy of this algorithm on diverse math reasoning benchmarks.
By amplifying tokens with positive THR value and weakening negative ones, our
algorithm improves greedy-decoding accuracy, favoring exploitation. The reverse
strategy yields consistent gains in Pass@K accuracy, favoring exploration. We
further demonstrate that our algorithm integrates seamlessly with other RL
objectives such as GSPO and generalizes across architectures including Llama.
These findings establish THR as a principled and fine-grained mechanism for
dynamically controlling exploration and exploitation in RL-tuned LLMs,
providing new tools for targeted fine-tuning in reasoning-intensive
applications.

</details>


### [142] [Optimizing Fine-Tuning through Advanced Initialization Strategies for Low-Rank Adaptation](https://arxiv.org/abs/2510.03731)
*Yongfu Xue*

Main category: cs.LG

TL;DR: 提出IniLoRA初始化策略，通过将低秩矩阵初始化为接近原始模型权重，解决了LoRA因零初始化限制性能的问题，在多个模型和任务上表现优于LoRA。


<details>
  <summary>Details</summary>
Motivation: LoRA虽然平衡了效率和参数有效性，但其两个低秩矩阵的零初始化限制了有效激活和利用原始模型权重的能力，成为性能优化的瓶颈。

Method: 提出IniLoRA初始化策略，将低秩矩阵初始化为接近原始模型权重，并引入两个变体IniLoRA-α和IniLoRA-β，采用不同的初始化方法进一步提升性能。

Result: 实验结果表明IniLoRA在多个模型和任务上比LoRA表现更好。

Conclusion: IniLoRA通过改进的初始化策略有效解决了LoRA的性能限制问题，为参数高效微调提供了更好的解决方案。

Abstract: The rapid development of parameter-efficient fine-tuning methods has
noticeably improved the efficiency of adapting large language models. Among
these, LoRA has gained widespread popularity due to its strong balance of
effectiveness and parameter efficiency. However, LoRA relies on initializing
two low-rank matrices whose product is zero, which limits its ability to
effectively activate and leverage the original model weights-creating a
potential bottleneck for optimal performance. To address this limitation, we
propose \textbf{IniLoRA}, a novel initialization strategy that initializes the
low-rank matrices to closely approximate the original model weights.
Experimental results indicate that IniLoRA achieves better performance than
LoRA across a range of models and tasks. Additionally, we introduce two
variants, IniLoRA-$\alpha$ and IniLoRA-$\beta$, both leveraging distinct
initialization methods to enhance performance further.

</details>


### [143] [Unlocking Reasoning Capabilities in LLMs via Reinforcement Learning Exploration](https://arxiv.org/abs/2510.03865)
*Wenhao Deng,Long Wei,Chenglei Yu,Tailin Wu*

Main category: cs.LG

TL;DR: RAPO算法通过前向KL惩罚和参考策略重加权，解决了RLVR训练中反向KL散度导致的探索受限问题，显著提升了LLM在数学推理任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法在采样预算增加时优势减弱，原因是反向KL散度的模式寻求行为限制了策略在基模型支持区域外的探索。

Method: 提出RAPO算法：(i) 用前向KL惩罚替代反向KL惩罚以支持分布外探索；(ii) 重加权参考策略以促进自适应分布内探索。

Result: 在Qwen2.5-3B和7B模型上训练，无需监督微调，在AIME2024和AIME2025评估中持续提升问题解决性能，突破基模型性能上限。

Conclusion: RAPO通过促进更广泛而集中的探索，推进了RLVR在挑战性推理任务中的前沿，解决了先前难以处理的问题。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has recently enhanced
the reasoning capabilities of large language models (LLMs), particularly for
mathematical problem solving. However, a fundamental limitation remains: as the
sampling budget increases, the advantage of RLVR-trained models over their
pretrained bases often diminishes or even vanishes, revealing a strong
dependence on the base model's restricted search space. We attribute this
phenomenon to the widespread use of the reverse Kullback-Leibler (KL)
divergence regularizer, whose mode-seeking behavior keeps the policy trapped
inside the base model's support region and hampers wider exploration. To
address this issue, we propose RAPO (Rewards-Aware Policy Optimization), an
algorithm to promote broader yet focused exploration. Our method (i) utilizes
the forward KL penalty to replace the reverse KL penalty for
out-of-distribution exploration, and (ii) reweights the reference policy to
facilitate adaptive in-distribution exploration. We train Qwen2.5-3B and 7B
models with RAPO on the 8K SimpleRL-Zero dataset, without supervised
fine-tuning, and evaluate them on AIME2024 and AIME2025. Results show that RAPO
consistently improves problem-solving performance. Notably, RAPO enables models
to surpass the base model's performance ceiling and solves previously
intractable problems, advancing the frontier of RLVR for challenging reasoning
tasks.

</details>


### [144] [LLM Chemistry Estimation for Multi-LLM Recommendation](https://arxiv.org/abs/2510.03930)
*Huascar Sanchez,Briland Hitaj*

Main category: cs.LG

TL;DR: LLM Chemistry框架用于量化多LLM协作中的协同与对抗行为，通过分析模型间交互依赖关系来推荐最优模型组合，理论分析表明模型异质性、任务类型和复杂度影响协作效果。


<details>
  <summary>Details</summary>
Motivation: 现有多LLM协作方法依赖隐式选择和输出评估，缺乏对模型间是否真正互补或冲突的分析，需要系统化框架来测量和优化模型组合效果。

Method: 提出LLM Chemistry框架，形式化LLM间的化学作用概念，设计算法量化协同与对抗行为，通过分析交互依赖关系推荐最优模型组合。

Result: 在分类、摘要和程序修复任务上的评估验证了理论分析，显示模型异质性、任务类型和复杂度确实影响协作效果，为多LLM系统提供了诊断依据。

Conclusion: LLM Chemistry既是多LLM系统的诊断因素，也是集成推荐的基础，为优化模型组合提供了理论和方法支持。

Abstract: Multi-LLM collaboration promises accurate, robust, and context-aware
solutions, yet existing approaches rely on implicit selection and output
assessment without analyzing whether collaborating models truly complement or
conflict. We introduce LLM Chemistry -- a framework that measures when LLM
combinations exhibit synergistic or antagonistic behaviors that shape
collective performance beyond individual capabilities. We formalize the notion
of chemistry among LLMs, propose algorithms that quantify it by analyzing
interaction dependencies, and recommend optimal model ensembles accordingly.
Our theoretical analysis shows that chemistry among collaborating LLMs is most
evident under heterogeneous model profiles, with its outcome impact shaped by
task type, group size, and complexity. Evaluation on classification,
summarization, and program repair tasks provides initial evidence for these
task-dependent effects, thereby reinforcing our theoretical results. This
establishes LLM Chemistry as both a diagnostic factor in multi-LLM systems and
a foundation for ensemble recommendation.

</details>


### [145] [Principled and Tractable RL for Reasoning with Diffusion Language Models](https://arxiv.org/abs/2510.04019)
*Anthony Zhan*

Main category: cs.LG

TL;DR: 提出了AGRPO算法，这是首个专为扩散大语言模型设计的理论严谨的在线强化学习算法，在数学推理任务上显著提升性能


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型尚未受益于现代后训练技术如强化学习，现有方法缺乏理论基础且不兼容扩散框架

Method: AGRPO算法使用蒙特卡洛采样计算无偏策略梯度估计，是首个适用于扩散大语言模型的策略梯度方法

Result: 在GSM8K任务上获得+7.6%绝对提升，在Countdown任务上性能提升3.8倍，优于基线模型和对比RL方法

Conclusion: 在线RL算法可以以理论严谨且实际有效的方式扩展到扩散LLMs，实现计算与性能的更好权衡

Abstract: Diffusion large language models (dLLMs) are a new paradigm of
non-autoregressive language models that are trained to predict multiple tokens
in parallel and generate text via iterative unmasking. Recent works have
successfully pretrained dLLMs to parity with autoregressive LLMs at the 8B
scale, but dLLMs have yet to benefit from modern post-training techniques, e.g.
reinforcement learning (RL), that have proven effective for autoregressive
models. Crucially, algorithms designed for traditional LLMs aren't directly
compatible with diffusion frameworks due to inherent differences in modeling
assumptions. Moreover, existing attempts at dLLM post-training with RL rely on
heuristic-based objectives with no theoretical grounding. In this work, we
present Amortized Group Relative Policy Optimization (AGRPO), a principled
on-policy RL algorithm designed specifically for dLLMs. AGRPO uses Monte Carlo
sampling to compute an unbiased policy gradient estimate, making it the first
tractable, faithful adaptation of policy gradient methods for dLLMs. We
demonstrate AGRPO's effectiveness on different math/reasoning tasks, a common
setting for RL with LLMs, achieving up to +7.6% absolute gain on GSM8K and 3.8x
performance on the Countdown task over the baseline LLaDA-8B-Instruct model and
1.3x performance gains over comparable RL methods such as diffu-GRPO.
Furthermore, these gains persist across different numbers of sampling steps at
inference time, achieving better tradeoffs between compute and performance. Our
results demonstrate that online RL algorithms can be extended to diffusion LLMs
in principled ways, maintaining both theoretical soundness and practical
effectiveness.

</details>


### [146] [What Scales in Cross-Entropy Scaling Law?](https://arxiv.org/abs/2510.04067)
*Junxi Yan,Zixi Wei,Jingtao Zhan,Qingyao Ai,Yiqun Liu*

Main category: cs.LG

TL;DR: 本文揭示了交叉熵缩放定律在大规模语言模型中的失效原因，发现只有误差熵遵循稳健的幂律缩放，而交叉熵的其他组成部分保持不变，这解释了为什么在大规模时交叉熵缩放定律会失效。


<details>
  <summary>Details</summary>
Motivation: 交叉熵缩放定律长期以来指导着大语言模型的发展，但最近证据表明该定律在超大规模时会失效，导致损失下降速度比预期慢，这给大语言模型开发带来重大困扰。

Method: 提出将交叉熵分解为三个部分：误差熵、自对齐和置信度，通过理论分析和在多个数据集上对32个模型进行的广泛实验来验证这一分解。

Result: 实验发现只有误差熵遵循稳健的幂律缩放，而其他两个项基本保持不变；误差熵在小模型中占主导地位，但随着模型规模增大比例逐渐减小。

Conclusion: 误差熵缩放定律比交叉熵缩放定律更能准确描述模型行为，将在大型语言模型的训练、理解和未来发展中有广泛应用。

Abstract: The cross-entropy scaling law has long served as a key tool for guiding the
development of large language models. It shows that cross-entropy loss
decreases in a predictable power-law rate as the model size increases. However,
recent evidence indicates that this law breaks down at very large scales: the
loss decreases more slowly than expected, which causes significant trouble for
developing large language models. In this paper, we hypothesize that the root
cause lies in the fact that cross-entropy itself does not truly scale; instead,
only one of its hidden components does. To investigate this, we introduce a
novel decomposition of cross-entropy into three parts: Error-Entropy,
Self-Alignment, and Confidence. We show both theoretically and empirically that
this decomposition precisely captures the training dynamics and optimization
objectives. Through extensive experiments on multiple datasets and 32 models
spanning five orders of magnitude in size, we find that only error-entropy
follows a robust power-law scaling, while the other two terms remain largely
invariant. Moreover, error-entropy constitutes the dominant share of
cross-entropy in small models but diminishes in proportion as models grow
larger. This explains why the cross-entropy scaling law appears accurate at
small scales but fails at very large ones. Our findings establish the
error-entropy scaling law as a more accurate description of model behavior. We
believe it will have wide applications in the training, understanding, and
future development of large language models.

</details>


### [147] [Slow-Fast Policy Optimization: Reposition-Before-Update for LLM Reasoning](https://arxiv.org/abs/2510.04072)
*Ziyan Wang,Zheng Wang,Jie Fu,Xingwei Qu,Qi Cheng,Shengpu Tang,Minjia Zhang,Xiaoming Huo*

Main category: cs.LG

TL;DR: SFPO通过慢-快策略优化框架解决强化学习训练早期的不稳定问题，在数学推理基准上比GRPO提升2.80分，减少4.93倍rollouts和4.19倍训练时间。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习中on-policy算法在早期训练阶段因低质量rollouts导致的梯度噪声、不稳定更新和低效探索问题。

Method: 提出Slow-Fast Policy Optimization (SFPO)，将每个训练步骤分解为三个阶段：在相同批次上进行短快速轨迹内步、控制off-policy漂移的重定位机制、以及最终的慢速校正。

Result: 在数学推理基准上平均比GRPO提升2.80分，减少4.93倍rollouts，减少4.19倍训练时间达到GRPO最佳精度。

Conclusion: SFPO通过重定位-更新设计保持目标和rollout过程不变，与现有策略梯度流程兼容，能稳定提升推理强化学习的训练效率和收敛速度。

Abstract: Reinforcement learning (RL) has become central to enhancing reasoning in
large language models (LLMs). Yet on-policy algorithms such as Group Relative
Policy Optimization (GRPO) often suffer in early training: noisy gradients from
low-quality rollouts lead to unstable updates and inefficient exploration. We
introduce Slow-Fast Policy Optimization (SFPO), a simple yet efficient
framework to address these limitations via decomposing each step into three
stages: a short fast trajectory of inner steps on the same batch, a reposition
mechanism to control off-policy drift, and a final slow correction. This
reposition-before-update design preserves the objective and rollout process
unchanged, making SFPO plug-compatible with existing policy-gradient pipelines.
Extensive experiments demonstrate that SFPO consistently improves stability,
reduces rollouts, and accelerates convergence of reasoning RL training.
Specifically, it outperforms GRPO by up to 2.80 points in average on math
reasoning benchmarks. It also achieves up to 4.93\texttimes{} fewer rollouts
and a 4.19\texttimes{} reduction in wall-clock time to match GRPO's best
accuracy.

</details>


### [148] [Beyond Next-Token Prediction: A Performance Characterization of Diffusion versus Autoregressive Language Models](https://arxiv.org/abs/2510.04146)
*Minseo Kim,Coleman Hooper,Aditya Tomar,Chenfeng Xu,Mehrdad Farajtabar,Michael W. Mahoney,Kurt Keutzer,Amir Gholami*

Main category: cs.LG

TL;DR: 本文对比分析了自回归语言模型(ARMs)和扩散语言模型(DLMs)的性能特征，发现DLMs在算术强度方面优于ARMs但难以扩展到长上下文，而块状解码的DLMs能同时提高算术强度并保持长上下文扩展性。


<details>
  <summary>Details</summary>
Motivation: 理解DLMs相对于主流ARMs的性能影响，探索两种架构在不同场景下的权衡，为模型选择提供指导。

Method: 采用理论分析和性能剖析数据相结合的方法，对比ARMs和DLMs的性能特征，并探索块状解码DLMs和批量推理场景。

Result: DLMs具有更高的算术强度但难以扩展到长上下文；块状解码DLMs能提高算术强度并保持长上下文扩展性；在批量推理中ARMs具有更高的吞吐量。

Conclusion: DLMs在算术强度方面有优势但需要减少采样步骤来改善延迟，块状解码是平衡性能的有效方法，两种架构在不同场景下各有优劣。

Abstract: Large Language Models (LLMs) have achieved state-of-the-art performance on a
broad range of Natural Language Processing (NLP) tasks, including document
processing and coding. Autoregressive Language Models (ARMs), which generate
tokens sequentially conditioned on all previous tokens, have been the
predominant paradigm for LLMs. However, while these networks have achieved high
accuracy across a range of downstream tasks, they exhibit low arithmetic
intensity due to the inherent sequential dependency with next-token prediction.
Recently, Diffusion Language Models (DLMs) have emerged as a promising
alternative architecture. DLMs generate output text in parallel, breaking the
limitations of sequential dependency. However, the performance implications of
DLMs relative to commonly deployed ARMs are not fully understood. In this work,
we present a comprehensive performance study analyzing the performance
characteristics of ARMs and DLMs, using both theoretical analysis and profiling
data to characterize the trade-offs between these approaches. We illustrate
that although DLMs exhibit higher arithmetic intensity compared to ARMs because
of their capability to utilize parallelism across sequence lengths, they fail
to scale effectively to longer contexts. We then explore DLMs with block-wise
decoding, outlining how this approach allows for increased arithmetic
intensity, while still scaling well to long contexts (similar to ARMs). We also
show interesting trade-offs for batched inference, where we find that ARMs
exhibit superior throughput, as they benefit more from parallelism across
sequences in the batch. Finally, we highlight opportunities for accelerating
DLM inference, and, in particular, highlight the importance of reducing the
number of sampling steps for allowing open-source DLMs to provide improved
latency relative to ARMs.

</details>


### [149] [Wave-PDE Nets: Trainable Wave-Equation Layers as an Alternative to Attention](https://arxiv.org/abs/2510.04304)
*Harshil Vejendla*

Main category: cs.LG

TL;DR: Wave-PDE Nets是一种基于二阶波动方程模拟的神经网络架构，通过可训练的空间速度和阻尼参数传播隐藏状态，在语言和视觉任务上匹配或超越Transformer性能，同时减少30%运行时间和25%峰值内存。


<details>
  <summary>Details</summary>
Motivation: 为注意力机制和一阶状态空间模型提供一种振荡式、全局的替代方案，利用物理偏置构建计算高效且鲁棒的架构。

Method: 使用基于FFT的辛谱求解器实现O(n log n)时间复杂度的二阶波动方程传播，每层通过可训练的空间速度c(x)和阻尼γ(x)参数传播隐藏状态。

Result: 在语言和视觉基准测试中，Wave-PDE Nets匹配或超越Transformer性能，同时减少30%运行时间和25%峰值内存；消融研究证实辛积分和谱拉普拉斯算子对稳定性和性能的关键作用。

Conclusion: Wave-PDE Nets是一种计算高效且鲁棒的架构，具有强大的物理归纳偏置，学习的物理参数可视化显示模型学会了信息传播的直观策略。

Abstract: We introduce Wave-PDE Nets, a neural architecture whose elementary operation
is a differentiable simulation of the second-order wave equation. Each layer
propagates its hidden state as a continuous field through a medium with
trainable spatial velocity c(x) and damping {\gamma}(x). A symplectic spectral
solver based on FFTs realises this propagation in O(nlog n) time. This
oscillatory, global mechanism provides a powerful alternative to attention and
first-order state-space models. We prove that a single Wave-PDE layer is a
universal approximator. On language and vision benchmarks, Wave-PDE Nets match
or exceed Transformer performance while demonstrating superior practical
efficiency, reducing wall-clock time by up to 30% and peak memory by 25%.
Ablation studies confirm the critical role of symplectic integration and a
spectral Laplacian for stability and performance. Visualizations of the learned
physical parameters reveal that the model learns intuitive strategies for
information propagation. These results position Wave-PDE Nets as a
computationally efficient and robust architecture with a strong physical
inductive bias.

</details>


### [150] [Partial Information Decomposition via Normalizing Flows in Latent Gaussian Distributions](https://arxiv.org/abs/2510.04417)
*Wenyuan Zhao,Adithya Balachandran,Chao Tian,Paul Pu Liang*

Main category: cs.LG

TL;DR: 提出了一种基于高斯假设的偏信息分解（GPID）方法，通过梯度优化算法提高计算效率，并利用信息保持编码器将非高斯数据转换为高斯分布，解决了传统PID方法在高维连续模态中的计算成本高和准确性差的问题。


<details>
  <summary>Details</summary>
Motivation: 传统偏信息分解（PID）方法依赖于优化联合分布，受限于估计的成对概率分布，对于连续和高维模态计算成本高且不准确。需要一种更高效准确的方法来分析多模态数据中的信息交互。

Method: 1. 提出高斯PID（GPID）框架，假设成对分布为多元高斯分布；2. 开发基于梯度优化的新算法，提高计算效率；3. 使用信息保持编码器将任意分布的随机变量转换为成对高斯随机变量；4. 解决了GPID中联合高斯解的最优性问题。

Result: 在多种合成示例中的实证验证表明，所提方法比现有基线提供更准确和高效的PID估计。在大规模多模态基准测试中展示了其在量化多模态数据集PID和选择高性能模型方面的实用性。

Conclusion: 该方法通过高斯假设和高效优化算法，显著改善了PID在连续高维模态中的计算效率和准确性，为多模态数据分析提供了实用的信息分解工具。

Abstract: The study of multimodality has garnered significant interest in fields where
the analysis of interactions among multiple information sources can enhance
predictive modeling, data fusion, and interpretability. Partial information
decomposition (PID) has emerged as a useful information-theoretic framework to
quantify the degree to which individual modalities independently, redundantly,
or synergistically convey information about a target variable. However,
existing PID methods depend on optimizing over a joint distribution constrained
by estimated pairwise probability distributions, which are costly and
inaccurate for continuous and high-dimensional modalities. Our first key
insight is that the problem can be solved efficiently when the pairwise
distributions are multivariate Gaussians, and we refer to this problem as
Gaussian PID (GPID). We propose a new gradient-based algorithm that
substantially improves the computational efficiency of GPID based on an
alternative formulation of the underlying optimization problem. To generalize
the applicability to non-Gaussian data, we learn information-preserving
encoders to transform random variables of arbitrary input distributions into
pairwise Gaussian random variables. Along the way, we resolved an open problem
regarding the optimality of joint Gaussian solutions for GPID. Empirical
validation in diverse synthetic examples demonstrates that our proposed method
provides more accurate and efficient PID estimates than existing baselines. We
further evaluate a series of large-scale multimodal benchmarks to show its
utility in real-world applications of quantifying PID in multimodal datasets
and selecting high-performing models.

</details>


### [151] [LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning](https://arxiv.org/abs/2510.04573)
*Haoqiang Kang,Yizhe Zhang,Nikki Lijing Kuang,Nicklas Majamaki,Navdeep Jaitly,Yi-An Ma,Lianhui Qin*

Main category: cs.LG

TL;DR: LaDiR是一个新颖的推理框架，通过将连续潜在表示与潜在扩散模型相结合，改进LLM的推理能力，支持并行生成多样化推理轨迹和迭代优化。


<details>
  <summary>Details</summary>
Motivation: 传统LLM的自回归解码限制了整体性重新审视和优化早期token的能力，且难以高效探索多样化解决方案。

Method: 使用VAE构建结构化潜在推理空间，将文本推理步骤编码为思想token块；利用潜在扩散模型通过块状双向注意力掩码去噪潜在思想token，支持长程推理和自适应测试时计算。

Result: 在数学推理和规划基准测试中，LaDiR在准确性、多样性和可解释性方面均优于现有自回归、基于扩散和潜在推理方法。

Conclusion: LaDiR为文本推理提供了一种新的潜在扩散范式，展示了在推理任务中的显著优势。

Abstract: Large Language Models (LLMs) demonstrate their reasoning ability through
chain-of-thought (CoT) generation. However, LLM's autoregressive decoding may
limit the ability to revisit and refine earlier tokens in a holistic manner,
which can also lead to inefficient exploration for diverse solutions. In this
paper, we propose LaDiR (Latent Diffusion Reasoner), a novel reasoning
framework that unifies the expressiveness of continuous latent representation
with the iterative refinement capabilities of latent diffusion models for an
existing LLM. We first construct a structured latent reasoning space using a
Variational Autoencoder (VAE) that encodes text reasoning steps into blocks of
thought tokens, preserving semantic information and interpretability while
offering compact but expressive representations. Subsequently, we utilize a
latent diffusion model that learns to denoise a block of latent thought tokens
with a blockwise bidirectional attention mask, enabling longer horizon and
iterative refinement with adaptive test-time compute. This design allows
efficient parallel generation of diverse reasoning trajectories, allowing the
model to plan and revise the reasoning process holistically. We conduct
evaluations on a suite of mathematical reasoning and planning benchmarks.
Empirical results show that LaDiR consistently improves accuracy, diversity,
and interpretability over existing autoregressive, diffusion-based, and latent
reasoning methods, revealing a new paradigm for text reasoning with latent
diffusion.

</details>


### [152] [Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models](https://arxiv.org/abs/2510.04618)
*Qizheng Zhang,Changran Hu,Shubhangi Upasani,Boyuan Ma,Fenglu Hong,Vamsidhar Kamanuru,Jay Rainton,Chen Wu,Mengmeng Ji,Hanchen Li,Urmish Thakker,James Zou,Kunle Olukotun*

Main category: cs.LG

TL;DR: ACE框架通过将上下文视为不断演化的剧本，通过生成、反思和策展的模块化过程来积累、优化和组织策略，解决了现有方法中的简洁性偏见和上下文崩溃问题。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型应用依赖上下文适应，但存在简洁性偏见（丢失领域洞察）和上下文崩溃（迭代重写导致细节丢失）的问题，需要更有效的上下文管理方法。

Method: ACE框架采用模块化流程：生成策略、反思效果、策展优化，通过结构化增量更新来防止上下文崩溃，支持无监督自适应。

Result: 在代理和领域特定基准测试中，ACE比强基线表现更好：代理任务提升10.6%，金融任务提升8.6%，同时显著降低适应延迟和部署成本。在AppWorld排行榜上，ACE与顶级生产级代理持平，在更难测试中表现更优。

Conclusion: 全面且不断演化的上下文能够实现可扩展、高效且自我改进的LLM系统，同时保持低开销。

Abstract: Large language model (LLM) applications such as agents and domain-specific
reasoning increasingly rely on context adaptation -- modifying inputs with
instructions, strategies, or evidence, rather than weight updates. Prior
approaches improve usability but often suffer from brevity bias, which drops
domain insights for concise summaries, and from context collapse, where
iterative rewriting erodes details over time. Building on the adaptive memory
introduced by Dynamic Cheatsheet, we introduce ACE (Agentic Context
Engineering), a framework that treats contexts as evolving playbooks that
accumulate, refine, and organize strategies through a modular process of
generation, reflection, and curation. ACE prevents collapse with structured,
incremental updates that preserve detailed knowledge and scale with
long-context models. Across agent and domain-specific benchmarks, ACE optimizes
contexts both offline (e.g., system prompts) and online (e.g., agent memory),
consistently outperforming strong baselines: +10.6% on agents and +8.6% on
finance, while significantly reducing adaptation latency and rollout cost.
Notably, ACE could adapt effectively without labeled supervision and instead by
leveraging natural execution feedback. On the AppWorld leaderboard, ACE matches
the top-ranked production-level agent on the overall average and surpasses it
on the harder test-challenge split, despite using a smaller open-source model.
These results show that comprehensive, evolving contexts enable scalable,
efficient, and self-improving LLM systems with low overhead.

</details>


### [153] [ONNX-Net: Towards Universal Representations and Instant Performance Prediction for Neural Architectures](https://arxiv.org/abs/2510.04938)
*Shiwen Qin,Alexander Auras,Shay B. Cohen,Elliot J. Crowley,Michael Moeller,Linus Ericsson,Jovita Lukasik*

Main category: cs.LG

TL;DR: 提出了ONNX-Bench基准测试和ONNX-Net文本编码方法，解决了神经架构搜索中评估成本高和搜索空间受限的问题，实现了跨搜索空间的零样本性能预测。


<details>
  <summary>Details</summary>
Motivation: 神经架构搜索的性能评估成本高昂，现有方法大多局限于特定搜索空间和图形编码，缺乏灵活性和可扩展性。

Method: 创建ONNX-Bench基准测试，包含60多万个架构-精度对；提出ONNX-Net文本编码方法，使用自然语言描述表示任意神经网络架构。

Result: 实验显示该方法在跨搜索空间场景下具有强大的零样本性能，仅需少量预训练样本即可实现即时神经网络架构评估。

Conclusion: 该方法突破了传统基于单元搜索空间的限制，实现了单一代理模型对所有神经架构的泛化能力。

Abstract: Neural architecture search (NAS) automates the design process of
high-performing architectures, but remains bottlenecked by expensive
performance evaluation. Most existing studies that achieve faster evaluation
are mostly tied to cell-based search spaces and graph encodings tailored to
those individual search spaces, limiting their flexibility and scalability when
applied to more expressive search spaces. In this work, we aim to close the gap
of individual search space restrictions and search space dependent network
representations. We present ONNX-Bench, a benchmark consisting of a collection
of neural networks in a unified format based on ONNX files. ONNX-Bench includes
all open-source NAS-bench-based neural networks, resulting in a total size of
more than 600k {architecture, accuracy} pairs. This benchmark allows creating a
shared neural network representation, ONNX-Net, able to represent any neural
architecture using natural language descriptions acting as an input to a
performance predictor. This text-based encoding can accommodate arbitrary layer
types, operation parameters, and heterogeneous topologies, enabling a single
surrogate to generalise across all neural architectures rather than being
confined to cell-based search spaces. Experiments show strong zero-shot
performance across disparate search spaces using only a small amount of
pretraining samples, enabling the unprecedented ability to evaluate any neural
network architecture instantly.

</details>


### [154] [On Structured State-Space Duality](https://arxiv.org/abs/2510.04944)
*Jerry Yao-Chieh Hu,Xiwen Zhang,Weimin Wu,Han Liu*

Main category: cs.LG

TL;DR: SSD论文将结构化状态空间模型与掩码注意力机制建立等价关系，证明了具有对角状态矩阵的SSM与1-半可分因果掩码注意力在算法实现上等价，拓宽了序列模型的设计空间。


<details>
  <summary>Details</summary>
Motivation: 建立状态空间模型与注意力机制之间的严格数学等价关系，为设计既高效又富有表达力的序列模型提供理论基础。

Method: 通过数学推导将SSD从标量恒等情况扩展到一般对角SSM，建立SSM与1-半可分掩码注意力的等价条件，分析训练复杂度下界。

Result: 证明了对角SSM与标量情况具有相同的训练复杂度下界但支持更丰富的动态特性，给出了SSM与1-半可分掩码注意力等价的充要条件，并发现这种对偶性无法扩展到标准softmax注意力。

Conclusion: 该研究强化了循环SSM与Transformer之间的联系，为设计表达力强且高效的序列模型开辟了更广阔的设计空间。

Abstract: Structured State-Space Duality (SSD) [Dao & Gu, ICML 2024] is an equivalence
between a simple Structured State-Space Model (SSM) and a masked attention
mechanism. In particular, a state-space model with a scalar-times-identity
state matrix is equivalent to a masked self-attention with a $1$-semiseparable
causal mask. Consequently, the same sequence transformation (model) has two
algorithmic realizations: as a linear-time $O(T)$ recurrence or as a
quadratic-time $O(T^2)$ attention. In this note, we formalize and generalize
this duality: (i) we extend SSD from the scalar-identity case to general
diagonal SSMs (diagonal state matrices); (ii) we show that these diagonal SSMs
match the scalar case's training complexity lower bounds while supporting
richer dynamics; (iii) we establish a necessary and sufficient condition under
which an SSM is equivalent to $1$-semiseparable masked attention; and (iv) we
show that such duality fails to extend to standard softmax attention due to
rank explosion. Together, these results tighten bridge between recurrent SSMs
and Transformers, and widen the design space for expressive yet efficient
sequence models.

</details>


### [155] [Reinforce-Ada: An Adaptive Sampling Framework for Reinforce-Style LLM Training](https://arxiv.org/abs/2510.04996)
*Wei Xiong,Chenlu Ye,Baohao Liao,Hanze Dong,Xinxing Xu,Christof Monz,Jiang Bian,Nan Jiang,Tong Zhang*

Main category: cs.LG

TL;DR: 提出了Reinforce-Ada自适应采样框架，用于LLM推理任务的在线强化学习后训练，通过动态重新分配采样努力到最具不确定性或学习潜力的提示，加速收敛并提高最终性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法在LLM推理任务的强化学习中存在梯度估计不稳定的问题，原因是固定和均匀的响应采样。需要动态分配推理预算以最小化随机梯度方差。

Method: Reinforce-Ada采用在线连续消除过程，交错进行估计和采样，一旦收集到足够信号就自动停止对提示的采样。通过形成具有强制奖励多样性的固定大小组，并使用自适应采样阶段聚合的全局统计计算优势基线来稳定更新。

Result: 在多个模型架构和推理基准测试中，Reinforce-Ada相比GRPO加速了收敛并提高了最终性能，特别是使用平衡采样变体时效果更佳。

Conclusion: 研究强调了方差感知、自适应数据策展在实现高效可靠推理能力LLM强化学习中的核心作用。

Abstract: Reinforcement learning applied to large language models (LLMs) for reasoning
tasks is often bottlenecked by unstable gradient estimates due to fixed and
uniform sampling of responses across prompts. Prior work such as GVM-RAFT
addresses this by dynamically allocating inference budget per prompt to
minimize stochastic gradient variance under a budget constraint. Inspired by
this insight, we propose Reinforce-Ada, an adaptive sampling framework for
online RL post-training of LLMs that continuously reallocates sampling effort
to the prompts with the greatest uncertainty or learning potential. Unlike
conventional two-stage allocation methods, Reinforce-Ada interleaves estimation
and sampling in an online successive elimination process, and automatically
stops sampling for a prompt once sufficient signal is collected. To stabilize
updates, we form fixed-size groups with enforced reward diversity and compute
advantage baselines using global statistics aggregated over the adaptive
sampling phase. Empirical results across multiple model architectures and
reasoning benchmarks show that Reinforce-Ada accelerates convergence and
improves final performance compared to GRPO, especially when using the balanced
sampling variant. Our work highlights the central role of variance-aware,
adaptive data curation in enabling efficient and reliable reinforcement
learning for reasoning-capable LLMs. Code is available at
https://github.com/RLHFlow/Reinforce-Ada.

</details>


### [156] [Learning to Interpret Weight Differences in Language Models](https://arxiv.org/abs/2510.05092)
*Avichal Goel,Yoon Kim,Nir Shavit,Tony T. Wang*

Main category: cs.LG

TL;DR: 提出Diff Interpretation Tuning (DIT)方法，训练模型用自然语言描述其微调导致的权重变化，使模型能够自我解释其修改。


<details>
  <summary>Details</summary>
Motivation: 微调语言模型时产生的权重变化通常难以解释，且微调数据集往往不可公开获取或过于庞大，需要一种能够理解权重变化的方法。

Method: 使用合成的标记权重差异来训练DIT适配器，该适配器可应用于兼容的微调模型，使其能够描述自身的变化。

Result: 在两个概念验证场景（报告隐藏行为和总结微调知识）中，该方法使模型能够使用准确的自然语言描述其微调引起的修改。

Conclusion: DIT方法成功实现了让模型用自然语言描述其权重变化的目标，为理解模型微调过程提供了新途径。

Abstract: Finetuning (pretrained) language models is a standard approach for updating
their internal parametric knowledge and specializing them to new tasks and
domains. However, the corresponding model weight changes ("weight diffs") are
not generally interpretable. While inspecting the finetuning dataset can give a
sense of how the model might have changed, these datasets are often not
publicly available or are too large to work with directly. Towards the goal of
comprehensively understanding weight diffs in natural language, we introduce
Diff Interpretation Tuning (DIT), a method that trains models to describe their
own finetuning-induced modifications. Our approach uses synthetic, labeled
weight diffs to train a DIT adapter, which can be applied to a compatible
finetuned model to make it describe how it has changed. We demonstrate in two
proof-of-concept settings (reporting hidden behaviors and summarizing finetuned
knowledge) that our method enables models to describe their finetuning-induced
modifications using accurate natural language descriptions.

</details>


### [157] [From Noisy Traces to Stable Gradients: Bias-Variance Optimized Preference Optimization for Aligning Large Reasoning Models](https://arxiv.org/abs/2510.05095)
*Mingkang Zhu,Xi Chen,Bei Yu,Hengshuang Zhao,Jiaya Jia*

Main category: cs.LG

TL;DR: 提出了BVPO方法，通过混合高方差的轨迹梯度估计器和低方差的空轨迹梯度估计器，优化大推理模型的偏好对齐，减少轨迹采样方差，提升训练稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 大推理模型在生成中间推理轨迹时，偏好对齐的计算需要边际化所有轨迹，这在实践中难以处理。常用的单轨迹采样方法会引入显著的梯度方差。

Method: BVPO方法混合两种梯度估计器：高方差的轨迹估计器和低方差的空轨迹估计器（通过禁用推理轨迹生成获得），理论分析表明该方法严格减少轨迹引起的方差。

Result: 在AlpacaEval~2上提升7.8分，Arena-Hard上提升6.8分；在六个数学推理基准上平均提升4.0分，仅使用通用对话数据训练也能提升推理性能。

Conclusion: 轨迹采样方差是偏好对齐的关键瓶颈，直接优化偏差-方差权衡可以实现更稳定的训练和更强的整体性能。

Abstract: Large reasoning models (LRMs) generate intermediate reasoning traces before
producing final answers, yielding strong gains on multi-step and mathematical
tasks. Yet aligning LRMs with human preferences, a crucial prerequisite for
model deployment, remains underexplored. The statistically correct objective
for preference alignment requires marginalizing over reasoning traces, but this
computation is intractable in practice. A common workaround optimizes a single
sampled trajectory, which introduces substantial gradient variance from
stochastic trace sampling. To address this challenge, we frame preference
optimization for LRMs through the lens of the bias--variance trade-off and
propose Bias--Variance Optimized Preference Optimization (BVPO), a simple,
drop-in method that mixes two gradient estimators: a high-variance trace-based
estimator and a low-variance empty-trace estimator obtained by disabling
reasoning trace generation. Our theory shows that BVPO strictly reduces
trace-induced variance for any nontrivial mixture, provides a closed-form
choice of the mixing weight that minimizes mean-squared error relative to the
true marginal gradient, and under standard smoothness and step-size conditions,
tightens classical convergence bounds for stochastic gradient descent.
Empirically, BVPO improves alignment over the best baseline by up to 7.8 points
on AlpacaEval~2 and 6.8 points on Arena-Hard. Despite being trained only on
general conversational data, BVPO also boosts reasoning performance for base
models by up to 4.0 points on the average of six math reasoning benchmarks.
These results identify variance from trace sampling as a key bottleneck and
demonstrate that directly optimizing the bias--variance trade-off yields more
stable training and stronger overall performance.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [158] [Large Language Models Achieve Gold Medal Performance at International Astronomy & Astrophysics Olympiad](https://arxiv.org/abs/2510.05016)
*Lucas Carrit Delgado Pinheiro,Ziru Chen,Bruno Caixeta Piazza,Ness Shroff,Yingbin Liang,Yuan-Sen Ting,Huan Sun*

Main category: astro-ph.IM

TL;DR: 本文通过国际天文与天体物理奥林匹克竞赛(IOAA)系统评估了5个最先进的大语言模型在复杂天文学推理任务上的表现，发现GPT-5和Gemini 2.5 Pro在理论考试中达到金牌水平，但在数据分析考试中表现分化，且所有模型在概念推理、几何推理和空间可视化方面存在明显弱点。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要测试简单的天文学知识问答，无法评估真实天文学研究所需的复杂推理能力，需要更全面地理解大语言模型在天文学领域的优势和局限性。

Method: 使用国际天文与天体物理奥林匹克竞赛(IOAA)的理论和数据分析考试，系统评估了5个最先进的大语言模型，包括GPT-5和Gemini 2.5 Pro等。

Result: GPT-5和Gemini 2.5 Pro在理论考试中平均得分分别为85.6%和84.2%，达到金牌水平，在所有参与者中排名前二；但在数据分析考试中，其他模型表现下降至48-76%，GPT-5仍保持88.5%的优异表现。所有模型在概念推理、几何推理和空间可视化方面的准确率仅为52-79%。

Conclusion: 虽然大语言模型在理论考试中接近人类顶尖水平，但在概念推理、几何推理和空间可视化等关键能力上仍存在明显差距，需要解决这些缺陷才能成为天文学领域的自主研究助手。

Abstract: While task-specific demonstrations show early success in applying large
language models (LLMs) to automate some astronomical research tasks, they only
provide incomplete views of all necessary capabilities in solving astronomy
problems, calling for more thorough understanding of LLMs' strengths and
limitations. So far, existing benchmarks and evaluations focus on simple
question-answering that primarily tests astronomical knowledge and fails to
evaluate the complex reasoning required for real-world research in the
discipline. Here, we address this gap by systematically benchmarking five
state-of-the-art LLMs on the International Olympiad on Astronomy and
Astrophysics (IOAA) exams, which are designed to examine deep conceptual
understanding, multi-step derivations, and multimodal analysis. With average
scores of 85.6% and 84.2%, Gemini 2.5 Pro and GPT-5 (the two top-performing
models) not only achieve gold medal level performance but also rank in the top
two among ~200-300 participants in all four IOAA theory exams evaluated
(2022-2025). In comparison, results on the data analysis exams show more
divergence. GPT-5 still excels in the exams with an 88.5% average score,
ranking top 10 among the participants in the four most recent IOAAs, while
other models' performances drop to 48-76%. Furthermore, our in-depth error
analysis underscores conceptual reasoning, geometric reasoning, and spatial
visualization (52-79% accuracy) as consistent weaknesses among all LLMs. Hence,
although LLMs approach peak human performance in theory exams, critical gaps
must be addressed before they can serve as autonomous research agents in
astronomy.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [159] [Investigating LLM Variability in Personalized Conversational Information Retrieval](https://arxiv.org/abs/2510.03795)
*Simon Lupart,Daniël van Dijk,Eric Langezaal,Ian van Dort,Mohammad Aliannejadi*

Main category: cs.IR

TL;DR: 本研究对个性化对话信息检索中的LLM输出变异性进行了可重复性研究，发现人工选择的个人知识库能持续提升检索性能，而LLM选择方法不如人工选择可靠。


<details>
  <summary>Details</summary>
Motivation: 先前研究基于GPT-3.5 Turbo的单次实验得出个人知识库可能有害的结论，但存在输出变异性和可重复性问题，需要更严格的评估。

Method: 在TREC iKAT 2024数据集上重现并扩展原始方法，评估多种模型（Llama 1B-70B、Qwen-7B、GPT-4o-mini），关注LLM输出变异性和模型泛化能力。

Result: 人工选择的PTKB持续提升检索性能，LLM选择方法不可靠；iKAT数据集比CAsT变异更高；召回指标比精度指标变异更低。

Conclusion: 需要多轮评估和方差报告来评估基于LLM的CIR系统，通过跨模型、数据集和指标的广泛评估，为个性化CIR提供更稳健和可泛化的实践。

Abstract: Personalized Conversational Information Retrieval (CIR) has seen rapid
progress in recent years, driven by the development of Large Language Models
(LLMs). Personalized CIR aims to enhance document retrieval by leveraging
user-specific information, such as preferences, knowledge, or constraints, to
tailor responses to individual needs. A key resource for this task is the TREC
iKAT 2023 dataset, designed to evaluate personalization in CIR pipelines.
Building on this resource, Mo et al. explored several strategies for
incorporating Personal Textual Knowledge Bases (PTKB) into LLM-based query
reformulation. Their findings suggested that personalization from PTKBs could
be detrimental and that human annotations were often noisy. However, these
conclusions were based on single-run experiments using the GPT-3.5 Turbo model,
raising concerns about output variability and repeatability. In this
reproducibility study, we rigorously reproduce and extend their work, focusing
on LLM output variability and model generalization. We apply the original
methods to the new TREC iKAT 2024 dataset and evaluate a diverse range of
models, including Llama (1B-70B), Qwen-7B, GPT-4o-mini. Our results show that
human-selected PTKBs consistently enhance retrieval performance, while
LLM-based selection methods do not reliably outperform manual choices. We
further compare variance across datasets and observe higher variability on iKAT
than on CAsT, highlighting the challenges of evaluating personalized CIR.
Notably, recall-oriented metrics exhibit lower variance than precision-oriented
ones, a critical insight for first-stage retrievers. Finally, we underscore the
need for multi-run evaluations and variance reporting when assessing LLM-based
CIR systems. By broadening evaluation across models, datasets, and metrics, our
study contributes to more robust and generalizable practices for personalized
CIR.

</details>


### [160] [Visual Lifelog Retrieval through Captioning-Enhanced Interpretation](https://arxiv.org/abs/2510.04010)
*Yu-Fei Shih,An-Zi Yen,Hen-Hsen Huang,Hsin-Hsi Chen*

Main category: cs.IR

TL;DR: 提出了CIVIL检索系统，通过为视觉生活日志生成描述性文本，然后使用文本嵌入模型在共享向量空间中进行检索，有效提升第一人称视角生活日志的检索效果。


<details>
  <summary>Details</summary>
Motivation: 人们经常难以记住过去经历的具体细节，需要重新访问这些记忆。生活日志检索已成为重要应用，但传统方法在处理第一人称视角的视觉生活日志时存在局限。

Method: 系统首先生成视觉生活日志的文本描述，然后使用文本嵌入模型将描述和用户查询投影到共享向量空间。提出了三种方法：单描述方法、集体描述方法和合并描述方法，专门用于解释第一人称视角的生活体验。

Result: 实验结果表明，该方法能有效描述第一人称视觉图像，显著提升生活日志检索效果。同时构建了将视觉生活日志转换为文本描述的数据集。

Conclusion: CIVIL系统通过文本描述生成和文本嵌入的方法，成功解决了第一人称视角生活日志检索的挑战，为个人生活体验的重构提供了有效工具。

Abstract: People often struggle to remember specific details of past experiences, which
can lead to the need to revisit these memories. Consequently, lifelog retrieval
has emerged as a crucial application. Various studies have explored methods to
facilitate rapid access to personal lifelogs for memory recall assistance. In
this paper, we propose a Captioning-Integrated Visual Lifelog (CIVIL) Retrieval
System for extracting specific images from a user's visual lifelog based on
textual queries. Unlike traditional embedding-based methods, our system first
generates captions for visual lifelogs and then utilizes a text embedding model
to project both the captions and user queries into a shared vector space.
Visual lifelogs, captured through wearable cameras, provide a first-person
viewpoint, necessitating the interpretation of the activities of the individual
behind the camera rather than merely describing the scene. To address this, we
introduce three distinct approaches: the single caption method, the collective
caption method, and the merged caption method, each designed to interpret the
life experiences of lifeloggers. Experimental results show that our method
effectively describes first-person visual images, enhancing the outcomes of
lifelog retrieval. Furthermore, we construct a textual dataset that converts
visual lifelogs into captions, thereby reconstructing personal life
experiences.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [161] [MacroBench: A Novel Testbed for Web Automation Scripts via Large Language Models](https://arxiv.org/abs/2510.04363)
*Hyunjun Kim,Sejong Kim*

Main category: cs.SE

TL;DR: MacroBench是一个代码优先的基准测试，用于评估LLMs能否从自然语言目标生成可重用的浏览器自动化程序。该基准包含7个自托管网站和681个任务，通过端到端验证协议测试模型性能。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在生成浏览器自动化程序方面的能力，特别是在处理复杂工作流和生产级代码质量方面的表现。

Method: 使用7个自托管网站（类似Airbnb、TikTok等）创建681个任务，通过静态检查、沙箱执行、DOM断言和数据库快照进行端到端验证。

Result: GPT-4o-Mini达到96.8%成功率，GPT-4.1为95.3%，Gemini-2.5-Pro为89.0%，DeepSeek-V3.1为83.4%。简单任务成功率91.7%，复杂工作流为0%。

Conclusion: 模型在简单任务上表现可靠，但在复杂工作流上完全失败，且均未达到生产级代码质量标准。发布了完整的基准测试管道供可重复评估。

Abstract: We introduce MacroBench, a code-first benchmark that evaluates whether LLMs
can synthesize reusable browser automation programs from natural language goals
by reading HTML/DOM and emitting Python with Selenium. MacroBench instantiates
seven self-hosted sites: Airbnb-like, TikTok-like, Reddit-like, Instagram-like,
Facebook-like, Discord-like, and Threads-like, covering 681 tasks across
interaction complexity and targeting difficulty. Our end-to-end protocol
validates generated code via static checks, sandboxed execution, and outcome
verification including DOM assertions and database snapshots, and includes a
safety suite for scraping, spam/abuse, and credential/privacy prompts. Across
2636 model-task runs, we observe stratified success: GPT-4o-Mini achieves 96.8
percent, GPT-4.1 achieves 95.3 percent, Gemini-2.5-Pro achieves 89.0 percent,
and DeepSeek-V3.1 achieves 83.4 percent. Models handle simple tasks reliably at
91.7 percent but fail on complex workflows at 0.0 percent, and none meet
production-quality coding practices despite functional completion. We release
our complete benchmark pipeline, evaluation framework, and experimental results
to enable reproducible assessment of macro synthesis for web automation.

</details>


### [162] [Retrieval-Augmented Code Generation: A Survey with Focus on Repository-Level Approaches](https://arxiv.org/abs/2510.04905)
*Yicheng Tao,Yao Qin,Yepang Liu*

Main category: cs.SE

TL;DR: 本文综述了检索增强代码生成（RACG）领域的研究，特别关注仓库级代码生成（RLCG）任务，系统分类了现有方法并分析了关键挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 现实软件开发需要跨整个仓库的推理，而现有函数级和文件级代码生成方法难以处理长距离依赖和全局语义一致性，因此需要研究仓库级代码生成方法。

Method: 采用检索增强生成（RAG）范式，将外部检索机制与LLMs结合，增强上下文感知和可扩展性。对现有研究从生成策略、检索模态、模型架构、训练范式和评估协议等多个维度进行分类。

Result: 建立了统一的分析框架，总结了广泛使用的数据集和基准，分析了当前局限性。

Conclusion: 为理解这一快速发展领域提供了系统框架，并为AI驱动的软件工程持续进步提供了启发。

Abstract: Recent advancements in large language models (LLMs) have substantially
improved automated code generation. While function-level and file-level
generation have achieved promising results, real-world software development
typically requires reasoning across entire repositories. This gives rise to the
challenging task of Repository-Level Code Generation (RLCG), where models must
capture long-range dependencies, ensure global semantic consistency, and
generate coherent code spanning multiple files or modules. To address these
challenges, Retrieval-Augmented Generation (RAG) has emerged as a powerful
paradigm that integrates external retrieval mechanisms with LLMs, enhancing
context-awareness and scalability. In this survey, we provide a comprehensive
review of research on Retrieval-Augmented Code Generation (RACG), with an
emphasis on repository-level approaches. We categorize existing work along
several dimensions, including generation strategies, retrieval modalities,
model architectures, training paradigms, and evaluation protocols. Furthermore,
we summarize widely used datasets and benchmarks, analyze current limitations,
and outline key challenges and opportunities for future research. Our goal is
to establish a unified analytical framework for understanding this rapidly
evolving field and to inspire continued progress in AI-powered software
engineering.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [163] [Adapting Diarization-Conditioned Whisper for End-to-End Multi-Talker Speech Recognition](https://arxiv.org/abs/2510.03723)
*Martin Kocour,Martin Karafiat,Alexander Polok,Dominik Klement,Lukáš Burget,Jan Černocký*

Main category: eess.AS

TL;DR: 提出了一种基于Whisper的说话人归属多说话人语音识别模型，结合目标说话人建模和序列化输出训练，通过联合解码实现优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 解决多说话人重叠语音识别问题，克服传统目标说话人ASR系统需要分别解码每个说话人的局限性，实现更高效的联合解码。

Method: 使用Diarization-Conditioned Whisper (DiCoW)编码器提取目标说话人嵌入，将其拼接成单一表示后传递给共享解码器，通过序列化输出流进行转录。

Result: 实验表明该模型在多人语音混合（如LibriMix）上超越了现有的SOT方法和DiCoW系统。

Conclusion: 提出的说话人归属Whisper模型通过联合解码策略，在多说话人语音识别任务中取得了优越性能，为重叠语音识别提供了有效解决方案。

Abstract: We propose a speaker-attributed (SA) Whisper-based model for multi-talker
speech recognition that combines target-speaker modeling with serialized output
training (SOT). Our approach leverages a Diarization-Conditioned Whisper
(DiCoW) encoder to extract target-speaker embeddings, which are concatenated
into a single representation and passed to a shared decoder. This enables the
model to transcribe overlapping speech as a serialized output stream with
speaker tags and timestamps. In contrast to target-speaker ASR systems such as
DiCoW, which decode each speaker separately, our approach performs joint
decoding, allowing the decoder to condition on the context of all speakers
simultaneously. Experiments show that the model outperforms existing SOT-based
approaches and surpasses DiCoW on multi-talker mixtures (e.g., LibriMix).

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [164] [AtomWorld: A Benchmark for Evaluating Spatial Reasoning in Large Language Models on Crystalline Materials](https://arxiv.org/abs/2510.04704)
*Taoyuze Lv,Alexander Chen,Fengyu Xie,Chu Wu,Jeffrey Meng,Dongzhan Zhou,Bram Hoex,Zhicheng Zhong,Tong Xie*

Main category: cond-mat.mtrl-sci

TL;DR: 提出了AtomWorld基准测试来评估LLM在晶体结构文件(CIF)相关任务上的表现，发现当前模型在结构理解和空间推理方面存在严重不足。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在文本推理方面表现出色，并开始发展空间理解能力，但在材料科学等需要深度理解3D原子结构的领域，缺乏系统评估其核心推理能力的标准化基准。

Method: 引入AtomWorld基准测试，基于标准晶体结构表示格式(CIF文件)，设计包括结构编辑、CIF感知和属性引导建模等任务来评估LLM。

Result: 实验显示当前模型在结构理解方面存在严重缺陷，在结构修改任务中频繁出错，甚至在基本的CIF格式理解上也存在问题，可能导致后续分析和材料洞察中的累积错误。

Conclusion: AtomWorld基准为推进LLM实现稳健的原子尺度建模奠定了基础，这对加速材料研究和自动化科学工作流程至关重要。

Abstract: Large Language Models (LLMs) excel at textual reasoning and are beginning to
develop spatial understanding, prompting the question of whether these
abilities can be combined for complex, domain-specific tasks. This question is
essential in fields like materials science, where deep understanding of 3D
atomic structures is fundamental. While initial studies have successfully
applied LLMs to tasks involving pure crystal generation or coordinate
understandings, a standardized benchmark to systematically evaluate their core
reasoning abilities across diverse atomic structures has been notably absent.
To address this gap, we introduce the AtomWorld benchmark to evaluate LLMs on
tasks based in Crystallographic Information Files (CIFs), a standard structure
representation format. These tasks, including structural editing, CIF
perception, and property-guided modeling, reveal a critical limitation: current
models, despite establishing promising baselines, consistently fail in
structural understanding and spatial reasoning. Our experiments show that these
models make frequent errors on structure modification tasks, and even in the
basic CIF format understandings, potentially leading to cumulative errors in
subsequent analysis and materials insights. By defining these standardized
tasks, AtomWorld lays the ground for advancing LLMs toward robust atomic-scale
modeling, crucial for accelerating materials research and automating scientific
workflows.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [165] [Lightweight Prompt Engineering for Cognitive Alignment in Educational AI: A OneClickQuiz Case Study](https://arxiv.org/abs/2510.03374)
*Antoun Yaacoub,Zainab Assaghir,Jérôme Da-Rugna*

Main category: cs.CY

TL;DR: 轻量级提示工程策略对AI生成问题的认知对齐影响研究，发现详细明确的提示对于精确的认知对齐至关重要。


<details>
  <summary>Details</summary>
Motivation: AI在教育技术中的快速整合面临内容质量和教学对齐的挑战，需要研究如何通过提示工程提高AI生成问题的认知对齐质量。

Method: 在OneClickQuiz Moodle插件中评估三种提示变体：详细基线、简化版本和基于角色的方法，使用自动分类模型和人工评审分析Bloom分类法的知识、应用和分析层面。

Result: 详细明确的提示对精确的认知对齐至关重要，而简化和基于角色的提示虽然能产生清晰相关的问题，但经常与预期的Bloom层级不对齐。

Conclusion: 战略性提示工程对于培养教学合理的AI驱动教育解决方案至关重要，建议在学习和智能学习环境中优化AI以生成高质量内容。

Abstract: The rapid integration of Artificial Intelligence (AI) into educational
technology promises to revolutionize content creation and assessment. However,
the quality and pedagogical alignment of AI-generated content remain critical
challenges. This paper investigates the impact of lightweight prompt
engineering strategies on the cognitive alignment of AI-generated questions
within OneClickQuiz, a Moodle plugin leveraging generative AI. We evaluate
three prompt variants-a detailed baseline, a simpler version, and a
persona-based approach-across Knowledge, Application, and Analysis levels of
Bloom's Taxonomy. Utilizing an automated classification model (from prior work)
and human review, our findings demonstrate that explicit, detailed prompts are
crucial for precise cognitive alignment. While simpler and persona-based
prompts yield clear and relevant questions, they frequently misalign with
intended Bloom's levels, generating outputs that are either too complex or
deviate from the desired cognitive objective. This study underscores the
importance of strategic prompt engineering in fostering pedagogically sound
AI-driven educational solutions and advises on optimizing AI for quality
content generation in learning analytics and smart learning environments.

</details>


### [166] [Red Lines and Grey Zones in the Fog of War: Benchmarking Legal Risk, Moral Harm, and Regional Bias in Large Language Model Military Decision-Making](https://arxiv.org/abs/2510.03514)
*Toby Drinkall*

Main category: cs.CY

TL;DR: 本研究开发了一个评估框架，用于测试大型语言模型在军事指挥控制系统中作为决策支持工具时的法律和道德风险，发现现成的LLMs在模拟冲突环境中表现出令人担忧且不可预测的目标选择行为。


<details>
  <summary>Details</summary>
Motivation: 随着军事组织考虑将大型语言模型集成到指挥控制系统用于规划和决策支持，理解它们的行为倾向变得至关重要，特别是评估其在目标选择方面的法律和道德风险。

Method: 开发基于国际人道法和军事学说的四个评估指标，通过90个多智能体、多轮次危机模拟，在三个地理区域评估GPT-4o、Gemini-2.5和LLaMA-3.1三个前沿模型。

Result: 所有模型都违反了国际人道法的区分原则，攻击民用目标的比例从16.7%到66.7%不等；平民伤害容忍度在危机模拟中逐步升级；不同模型间存在显著差异，LLaMA-3.1表现最差。

Conclusion: 模型选择构成了军事行动中可接受法律和道德风险的选择，本研究提供了一个概念验证和可复现的基准测试框架，用于标准化部署前测试。

Abstract: As military organisations consider integrating large language models (LLMs)
into command and control (C2) systems for planning and decision support,
understanding their behavioural tendencies is critical. This study develops a
benchmarking framework for evaluating aspects of legal and moral risk in
targeting behaviour by comparing LLMs acting as agents in multi-turn simulated
conflict. We introduce four metrics grounded in International Humanitarian Law
(IHL) and military doctrine: Civilian Target Rate (CTR) and Dual-use Target
Rate (DTR) assess compliance with legal targeting principles, while Mean and
Max Simulated Non-combatant Casualty Value (SNCV) quantify tolerance for
civilian harm.
  We evaluate three frontier models, GPT-4o, Gemini-2.5, and LLaMA-3.1, through
90 multi-agent, multi-turn crisis simulations across three geographic regions.
Our findings reveal that off-the-shelf LLMs exhibit concerning and
unpredictable targeting behaviour in simulated conflict environments. All
models violated the IHL principle of distinction by targeting civilian objects,
with breach rates ranging from 16.7% to 66.7%. Harm tolerance escalated through
crisis simulations with MeanSNCV increasing from 16.5 in early turns to 27.7 in
late turns. Significant inter-model variation emerged: LLaMA-3.1 selected an
average of 3.47 civilian strikes per simulation with MeanSNCV of 28.4, while
Gemini-2.5 selected 0.90 civilian strikes with MeanSNCV of 17.6. These
differences indicate that model selection for deployment constitutes a choice
about acceptable legal and moral risk profiles in military operations.
  This work seeks to provide a proof-of-concept of potential behavioural risks
that could emerge from the use of LLMs in Decision Support Systems (AI DSS) as
well as a reproducible benchmarking framework with interpretable metrics for
standardising pre-deployment testing.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [167] [Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba](https://arxiv.org/abs/2510.04738)
*Baher Mohammad,Magauiya Zhussip,Stamatios Lefkimmiatis*

Main category: cs.SD

TL;DR: MAVE是一种基于交叉注意力Mamba架构的自回归语音编辑和合成模型，在语音编辑任务上达到SOTA性能，在零样本TTS任务上表现优异，同时显著降低了内存需求。


<details>
  <summary>Details</summary>
Motivation: 现有语音编辑和TTS模型在内存效率和性能方面存在限制，需要一种既能实现高质量语音编辑又能进行零样本TTS的轻量级架构。

Method: 结合Mamba的高效音频序列建模能力和交叉注意力机制，实现精确的文本-声学对齐，构建自回归架构进行语音编辑和合成。

Result: 在RealEdit基准测试中，57.2%的听众认为MAVE编辑的语音与原语音感知上无差别；在零样本TTS中，MAVE在说话人相似性和自然度方面超过VoiceCraft，内存需求降低6倍。

Conclusion: MAVE通过结构化状态空间建模和跨模态注意力的协同整合，为灵活、高保真的语音编辑和合成建立了新标准。

Abstract: We introduce MAVE (Mamba with Cross-Attention for Voice Editing and
Synthesis), a novel autoregressive architecture for text-conditioned voice
editing and high-fidelity text-to-speech (TTS) synthesis, built on a
cross-attentive Mamba backbone. MAVE achieves state-of-the-art performance in
speech editing and very competitive results in zero-shot TTS, while not being
explicitly trained on the latter task, outperforming leading autoregressive and
diffusion models on diverse, real-world audio. By integrating Mamba for
efficient audio sequence modeling with cross-attention for precise
text-acoustic alignment, MAVE enables context-aware voice editing with
exceptional naturalness and speaker consistency. In pairwise human evaluations
on a random 40-sample subset of the RealEdit benchmark (400 judgments), 57.2%
of listeners rated MAVE - edited speech as perceptually equal to the original,
while 24.8% prefered the original and 18.0% MAVE - demonstrating that in the
majority of cases edits are indistinguishable from the source. MAVE compares
favorably with VoiceCraft and FluentSpeech both on pairwise comparisons and
standalone mean opinion score (MOS) evaluations. For zero-shot TTS, MAVE
exceeds VoiceCraft in both speaker similarity and naturalness, without
requiring multiple inference runs or post-processing. Remarkably, these quality
gains come with a significantly lower memory cost and approximately the same
latency: MAVE requires ~6x less memory than VoiceCraft during inference on
utterances from the RealEdit database (mean duration: 6.21s, A100, FP16, batch
size 1). Our results demonstrate that MAVE establishes a new standard for
flexible, high-fidelity voice editing and synthesis through the synergistic
integration of structured state-space modeling and cross-modal attention.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [168] [P2P: A Poison-to-Poison Remedy for Reliable Backdoor Defense in LLMs](https://arxiv.org/abs/2510.04503)
*Shuai Zhao,Xinyi Wu,Shiqian Zhao,Xiaobao Wu,Zhongliang Guo,Yanhao Jia,Anh Tuan Luu*

Main category: cs.CR

TL;DR: 提出Poison-to-Poison (P2P)防御算法，通过注入良性触发器和安全标签来覆盖恶意后门攻击，实现跨任务和攻击类型的通用防御。


<details>
  <summary>Details</summary>
Motivation: 现有后门防御方法泛化能力有限，只能应对特定攻击类型或任务设置，需要一种通用的防御策略来保护LLM的可靠性和可信度。

Method: P2P将带有安全替代标签的良性触发器注入部分训练样本，利用基于提示的学习在重新中毒的数据集上微调模型，使模型将触发器诱导的表征与安全输出关联。

Result: 在分类、数学推理和摘要生成任务上的实验表明，P2P显著降低了攻击成功率，同时保持了任务性能。

Conclusion: P2P能够有效中和恶意后门，为防御后门攻击提供指导，促进安全可信的LLM社区发展。

Abstract: During fine-tuning, large language models (LLMs) are increasingly vulnerable
to data-poisoning backdoor attacks, which compromise their reliability and
trustworthiness. However, existing defense strategies suffer from limited
generalization: they only work on specific attack types or task settings. In
this study, we propose Poison-to-Poison (P2P), a general and effective backdoor
defense algorithm. P2P injects benign triggers with safe alternative labels
into a subset of training samples and fine-tunes the model on this re-poisoned
dataset by leveraging prompt-based learning. This enforces the model to
associate trigger-induced representations with safe outputs, thereby overriding
the effects of original malicious triggers. Thanks to this robust and
generalizable trigger-based fine-tuning, P2P is effective across task settings
and attack types. Theoretically and empirically, we show that P2P can
neutralize malicious backdoors while preserving task performance. We conduct
extensive experiments on classification, mathematical reasoning, and summary
generation tasks, involving multiple state-of-the-art LLMs. The results
demonstrate that our P2P algorithm significantly reduces the attack success
rate compared with baseline models. We hope that the P2P can serve as a
guideline for defending against backdoor attacks and foster the development of
a secure and trustworthy LLM community.

</details>


### [169] [Proactive defense against LLM Jailbreak](https://arxiv.org/abs/2510.05052)
*Weiliang Zhao,Jinjun Peng,Daniel Ben-Levi,Zhou Yu,Junfeng Yang*

Main category: cs.CR

TL;DR: ProAct是一个主动防御框架，通过提供虚假的成功越狱响应来误导和破坏自主越狱攻击过程，显著降低攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的安全对齐仍然容易受到多轮越狱攻击，现有的被动防御方法难以有效应对基于搜索的攻击策略。

Method: 采用主动防御策略，向攻击者提供看似成功但实际无害的虚假响应，误导攻击者的优化循环，使其提前终止搜索。

Result: 在多个先进LLM、越狱框架和安全基准测试中，该方法将攻击成功率降低了高达92%，与其他防御框架结合时可将最新攻击策略的成功率降至0%。

Conclusion: ProAct代表了一种正交的防御策略，可以作为额外的安全防护措施，有效增强LLM对抗最有效越狱攻击的安全性。

Abstract: The proliferation of powerful large language models (LLMs) has necessitated
robust safety alignment, yet these models remain vulnerable to evolving
adversarial attacks, including multi-turn jailbreaks that iteratively search
for successful queries. Current defenses, primarily reactive and static, often
fail to counter these search-based attacks. In this paper, we introduce ProAct,
a novel proactive defense framework designed to disrupt and mislead autonomous
jailbreaking processes. Our core idea is to intentionally provide adversaries
with "spurious responses" that appear to be results of successful jailbreak
attacks but contain no actual harmful content. These misleading responses
provide false signals to the attacker's internal optimization loop, causing the
adversarial search to terminate prematurely and effectively jailbreaking the
jailbreak. By conducting extensive experiments across state-of-the-art LLMs,
jailbreaking frameworks, and safety benchmarks, our method consistently and
significantly reduces attack success rates by up to 92\%. When combined with
other defense frameworks, it further reduces the success rate of the latest
attack strategies to 0\%. ProAct represents an orthogonal defense strategy that
can serve as an additional guardrail to enhance LLM safety against the most
effective jailbreaking attacks.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [170] [Multimodal Arabic Captioning with Interpretable Visual Concept Integration](https://arxiv.org/abs/2510.03295)
*Passant Elchafei,Amany Fashwan*

Main category: cs.CV

TL;DR: VLCAP是一个阿拉伯语图像描述框架，结合CLIP视觉标签检索与多模态文本生成，通过可解释的阿拉伯视觉概念提升描述质量。


<details>
  <summary>Details</summary>
Motivation: 解决阿拉伯语图像描述中缺乏可解释性和文化连贯性的问题，避免单纯依赖端到端方法。

Method: 使用三种多语言编码器(mCLIP、AraCLIP、Jina V4)检索视觉标签，构建混合词汇表，将检索到的标签转换为阿拉伯语提示，结合图像输入视觉语言模型生成描述。

Result: mCLIP + Gemini Pro Vision在BLEU-1(5.34%)和余弦相似度(60.01%)上表现最佳，AraCLIP + Qwen-VL获得最高LLM-judge评分(36.33%)。

Conclusion: 该可解释流水线能够生成文化连贯且上下文准确的阿拉伯语图像描述。

Abstract: We present VLCAP, an Arabic image captioning framework that integrates
CLIP-based visual label retrieval with multimodal text generation. Rather than
relying solely on end-to-end captioning, VLCAP grounds generation in
interpretable Arabic visual concepts extracted with three multilingual
encoders, mCLIP, AraCLIP, and Jina V4, each evaluated separately for label
retrieval. A hybrid vocabulary is built from training captions and enriched
with about 21K general domain labels translated from the Visual Genome dataset,
covering objects, attributes, and scenes. The top-k retrieved labels are
transformed into fluent Arabic prompts and passed along with the original image
to vision-language models. In the second stage, we tested Qwen-VL and Gemini
Pro Vision for caption generation, resulting in six encoder-decoder
configurations. The results show that mCLIP + Gemini Pro Vision achieved the
best BLEU-1 (5.34%) and cosine similarity (60.01%), while AraCLIP + Qwen-VL
obtained the highest LLM-judge score (36.33%). This interpretable pipeline
enables culturally coherent and contextually accurate Arabic captions.

</details>


### [171] [Person-Centric Annotations of LAION-400M: Auditing Bias and Its Transfer to Models](https://arxiv.org/abs/2510.03721)
*Leander Girrbach,Stephan Alaniz,Genevieve Smith,Trevor Darrell,Zeynep Akata*

Main category: cs.CV

TL;DR: 该研究为LAION-400M数据集创建了人物中心标注，揭示了训练数据中的人口统计不平衡和有害关联，并证明CLIP和Stable Diffusion中60-70%的性别偏见可由数据中的直接共现线性解释。


<details>
  <summary>Details</summary>
Motivation: 解决大规模多模态数据集缺乏人口统计标注的问题，以澄清训练数据在产生模型偏见中的作用。

Method: 通过验证的自动标注流程，结合目标检测、多模态字幕生成和微调分类器，为LAION-400M数据集创建人物中心标注，包括边界框、感知性别和种族/民族标签。

Result: 发现数据集中存在人口统计不平衡和有害关联，如男性和被感知为黑人或中东裔的个体与犯罪相关和负面内容的不成比例关联。60-70%的CLIP和Stable Diffusion性别偏见可由数据直接共现线性解释。

Conclusion: 该资源首次建立了数据集构成与下游模型偏见之间的大规模实证联系，为理解模型偏见来源提供了重要依据。

Abstract: Vision-language models trained on large-scale multimodal datasets show strong
demographic biases, but the role of training data in producing these biases
remains unclear. A major barrier has been the lack of demographic annotations
in web-scale datasets such as LAION-400M. We address this gap by creating
person-centric annotations for the full dataset, including over 276 million
bounding boxes, perceived gender and race/ethnicity labels, and automatically
generated captions. These annotations are produced through validated automatic
labeling pipelines combining object detection, multimodal captioning, and
finetuned classifiers. Using them, we uncover demographic imbalances and
harmful associations, such as the disproportionate linking of men and
individuals perceived as Black or Middle Eastern with crime-related and
negative content. We also show that 60-70% of gender bias in CLIP and Stable
Diffusion can be linearly explained by direct co-occurrences in the data. Our
resources establish the first large-scale empirical link between dataset
composition and downstream model bias.

</details>


### [172] [No Tokens Wasted: Leveraging Long Context in Biomedical Vision-Language Models](https://arxiv.org/abs/2510.03978)
*Min Woo Sun,Alejandro Lozano,Javier Gamazo Tejero,Vishwesh Nath,Xiao Xiao Sun,James Burgess,Yuhui Zhang,Kun Yuan,Robert Tibshirani,Sean Huver,Serena Yeung-Levy*

Main category: cs.CV

TL;DR: 该论文研究了在视觉语言模型中扩展文本编码器上下文长度对生物医学长格式字幕的影响，发现更长的上下文能带来更好的检索和分类性能，并提出了BIOMEDICA-LongCAP数据集和BMC-LongCLIP模型。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型通常预训练时使用短文本窗口（<77个token），导致长格式生物医学字幕被截断。然而，大规模开源文献中的生物医学字幕有很大部分远超77个token，因此需要研究长上下文预训练的影响。

Method: 扩展视觉语言模型中文本编码器的上下文长度，创建BIOMEDICA-LongCAP数据集（包含100万图像-字幕对，具有来自全文文章的情境感知描述），并训练BMC-LongCLIP模型，支持最多512个token的文本窗口。

Result: BMC-LongCLIP将上下文容量扩展了6.6倍，token浪费从55%减少到2.2%。在长字幕检索基准测试中，Recall@1获得高达+30%的绝对增益，分类平均提升+2%，且收敛速度比短上下文模型更快。

Conclusion: 长上下文建模是推进生物医学视觉语言模型的一个有前景的方向，能够有效利用长格式字幕中提供的额外监督信息。

Abstract: Embedding vision-language models (VLMs) are typically pretrained with short
text windows (<77 tokens), which forces the truncation of long-format captions.
Yet, the distribution of biomedical captions from large-scale open source
literature reveals that a huge portion of captions far exceed 77 tokens. To
this end, we investigate the impact of pretraining on long-format biomedical
captions by extending the context length of text encoders in VLMs. We find that
longer context (thus, enabling additional supervision provided in long-format
captions) correlates with better retrieval and classification performance.
Given this finding, we introduce BIOMEDICA-LongCAP, a dataset of 1M
image-caption pairs enriched with context-aware descriptions from full-text
articles, providing longer and additional textual supervision. Using
BIOMEDICA-LongCAP, we train BMC-LongCLIP, a long-context biomedical VLM with a
text encoder supporting windows of up to 512 tokens. Our model extends context
capacity by 6.6x, reducing token waste from 55% to just 2.2%. On long-caption
retrieval benchmarks, BMC-LongCLIP achieves up to +30% absolute gains in
Recall@1 and +2% average improvements in classification, while also converging
faster than short-context. Our results demonstrate that long-context modeling
is a promising direction for advancing biomedical VLMs.

</details>


### [173] [Enhancing OCR for Sino-Vietnamese Language Processing via Fine-tuned PaddleOCRv5](https://arxiv.org/abs/2510.04003)
*Minh Hoang Nguyen,Su Nguyen Thiet*

Main category: cs.CV

TL;DR: 提出基于PaddleOCRv5的微调方法，显著提升越南古典汉语（汉喃）文本识别准确率，从37.5%提升至50.0%，特别在噪声图像条件下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有OCR系统在处理越南历史文献中的退化扫描、非标准字形和手写变体时存在困难，阻碍了文献数字化和跨语言语义研究。

Method: 使用精选的越南古典汉语手稿子集重新训练PaddleOCRv5的文本识别模块，构建完整的训练流程包括预处理、LMDB转换、评估和可视化。

Result: 实验结果显示准确率显著提升，精确准确率从37.5%提高到50.0%，在噪声图像条件下表现尤其突出。

Conclusion: 该方法有效提升了古典汉语文本识别性能，并开发了交互式演示工具，支持汉越语义对齐、机器翻译和历史语言学研究等下游应用。

Abstract: Recognizing and processing Classical Chinese (Han-Nom) texts play a vital
role in digitizing Vietnamese historical documents and enabling cross-lingual
semantic research. However, existing OCR systems struggle with degraded scans,
non-standard glyphs, and handwriting variations common in ancient sources. In
this work, we propose a fine-tuning approach for PaddleOCRv5 to improve
character recognition on Han-Nom texts. We retrain the text recognition module
using a curated subset of ancient Vietnamese Chinese manuscripts, supported by
a full training pipeline covering preprocessing, LMDB conversion, evaluation,
and visualization. Experimental results show a significant improvement over the
base model, with exact accuracy increasing from 37.5 percent to 50.0 percent,
particularly under noisy image conditions. Furthermore, we develop an
interactive demo that visually compares pre- and post-fine-tuning recognition
results, facilitating downstream applications such as Han-Vietnamese semantic
alignment, machine translation, and historical linguistics research. The demo
is available at https://huggingface.co/spaces/MinhDS/Fine-tuned-PaddleOCRv5.

</details>


### [174] [Automating construction safety inspections using a multi-modal vision-language RAG framework](https://arxiv.org/abs/2510.04145)
*Chenxin Wang,Elyas Asadi Shamsabadi,Zhaohui Chen,Luming Shen,Alireza Ahmadian Fard Fini,Daniel Dias-da-Costa*

Main category: cs.CV

TL;DR: SiteShield是一个基于多模态大视觉语言模型和检索增强生成框架的自动化施工安全检查系统，通过整合视觉和音频输入生成安全检查报告，在真实数据上表现优于单模态LLM。


<details>
  <summary>Details</summary>
Motivation: 传统施工安全检查方法效率低下，需要处理大量信息。现有的大语言模型应用存在响应不相关、模态输入受限和幻觉问题，且缺乏实时适应性。

Method: 开发了SiteShield多模态LVLM-RAG框架，整合视觉和音频输入，利用检索增强生成技术自动化生成施工安全检查报告。

Result: 在真实数据上，SiteShield的F1分数为0.82，汉明损失为0.04，精确率为0.76，召回率为0.96，表现优于无RAG的单模态LLM。

Conclusion: SiteShield为增强信息检索和生成安全检查报告的效率提供了一条新途径。

Abstract: Conventional construction safety inspection methods are often inefficient as
they require navigating through large volume of information. Recent advances in
large vision-language models (LVLMs) provide opportunities to automate safety
inspections through enhanced visual and linguistic understanding. However,
existing applications face limitations including irrelevant or unspecific
responses, restricted modal inputs and hallucinations. Utilisation of Large
Language Models (LLMs) for this purpose is constrained by availability of
training data and frequently lack real-time adaptability. This study introduces
SiteShield, a multi-modal LVLM-based Retrieval-Augmented Generation (RAG)
framework for automating construction safety inspection reports by integrating
visual and audio inputs. Using real-world data, SiteShield outperformed
unimodal LLMs without RAG with an F1 score of 0.82, hamming loss of 0.04,
precision of 0.76, and recall of 0.96. The findings indicate that SiteShield
offers a novel pathway to enhance information retrieval and efficiency in
generating safety reports.

</details>


### [175] [Zoom-In to Sort AI-Generated Images Out](https://arxiv.org/abs/2510.04225)
*Yikun Ji,Yan Hong,Bowen Deng,jun lan,Huijia Zhu,Weiqiang Wang,Liqing Zhang,Jianfu Zhang*

Main category: cs.CV

TL;DR: ZoomIn是一个两阶段取证框架，通过先扫描图像定位可疑区域，再对放大区域进行聚焦分析来检测AI生成图像，提供准确且可解释的检测结果。


<details>
  <summary>Details</summary>
Motivation: AI生成图像的快速增长模糊了真实与合成内容的界限，对数字完整性构成严重威胁。现有视觉语言模型虽然提供可解释性，但在检测高质量合成图像的细微伪影方面表现不足。

Method: 提出ZoomIn两阶段取证框架：第一阶段扫描图像定位可疑区域，第二阶段对放大区域进行聚焦分析。创建MagniFake数据集（20,000张真实和高质量合成图像，带边界框和取证解释），通过自动化VLM流程生成训练数据。

Result: 方法达到96.39%的准确率，具有强大的泛化能力，同时提供基于视觉证据的人类可理解解释。

Conclusion: ZoomIn框架在AI生成图像检测方面实现了高准确率和可解释性的平衡，为解决数字完整性挑战提供了有效解决方案。

Abstract: The rapid growth of AI-generated imagery has blurred the boundary between
real and synthetic content, raising critical concerns for digital integrity.
Vision-language models (VLMs) offer interpretability through explanations but
often fail to detect subtle artifacts in high-quality synthetic images. We
propose ZoomIn, a two-stage forensic framework that improves both accuracy and
interpretability. Mimicking human visual inspection, ZoomIn first scans an
image to locate suspicious regions and then performs a focused analysis on
these zoomed-in areas to deliver a grounded verdict. To support training, we
introduce MagniFake, a dataset of 20,000 real and high-quality synthetic images
annotated with bounding boxes and forensic explanations, generated through an
automated VLM-based pipeline. Our method achieves 96.39% accuracy with robust
generalization, while providing human-understandable explanations grounded in
visual evidence.

</details>


### [176] [MorphoSim: An Interactive, Controllable, and Editable Language-guided 4D World Simulator](https://arxiv.org/abs/2510.04390)
*Xuehai He,Shijie Zhou,Thivyanth Venkateswaran,Kaizhi Zheng,Ziyu Wan,Achuta Kadambi,Xin Eric Wang*

Main category: cs.CV

TL;DR: MorphoSim是一个语言引导的4D场景生成框架，支持多视角一致性和对象级控制，能够从自然语言指令生成动态环境，并允许对对象进行交互式编辑。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到视频模型局限于2D视图且交互性有限，无法满足机器人技术中对可控、可编辑时空环境的需求。

Method: 整合轨迹引导生成与特征场蒸馏技术，支持交互式编辑而无需完全重新生成。

Result: 实验表明MorphoSim在保持高场景保真度的同时实现了可控性和可编辑性。

Conclusion: MorphoSim为机器人技术提供了可扩展的训练数据、可重现的评估和灵活的任务设计能力。

Abstract: World models that support controllable
  and editable spatiotemporal environments are valuable
  for robotics, enabling scalable training data, repro ducible evaluation, and
flexible task design. While
  recent text-to-video models generate realistic dynam ics, they are
constrained to 2D views and offer limited
  interaction. We introduce MorphoSim, a language guided framework that
generates 4D scenes with
  multi-view consistency and object-level controls. From
  natural language instructions, MorphoSim produces
  dynamic environments where objects can be directed,
  recolored, or removed, and scenes can be observed
  from arbitrary viewpoints. The framework integrates
  trajectory-guided generation with feature field dis tillation, allowing edits
to be applied interactively
  without full re-generation. Experiments show that Mor phoSim maintains high
scene fidelity while enabling
  controllability and editability. The code is available
  at https://github.com/eric-ai-lab/Morph4D.

</details>


### [177] [MedCLM: Learning to Localize and Reason via a CoT-Curriculum in Medical Vision-Language Models](https://arxiv.org/abs/2510.04477)
*Soo Yong Kim,Suin Cho,Vincent-Daniel Yun,Gyeongyeon Hwang*

Main category: cs.CV

TL;DR: MedCLM是一个将检测数据集转换为大规模医学视觉问答数据的自动化流程，通过链接病变框与器官分割和结构化推理，使医学视觉语言模型能够生成具有逐步推理的问题-答案对。


<details>
  <summary>Details</summary>
Motivation: 弥合临床诊断推理与人工智能之间的差距仍然是医学影像中的核心挑战。

Method: 提出MedCLM自动化流程，将检测数据集转换为具有链式推理的大规模医学VQA数据；采用集成CoT-课程策略，包含三个阶段：简单阶段使用显式病变框进行视觉定位，中等阶段鼓励隐式定位，困难阶段进行弱监督推理。

Result: 实验结果表明，MedCLM在多个医学VQA基准测试中达到了最先进的性能。

Conclusion: MedCLM为开发临床对齐的医学视觉语言模型提供了一个可扩展的框架。

Abstract: Bridging clinical diagnostic reasoning with AI remains a central challenge in
medical imaging. We introduce MedCLM, an automated pipeline that converts
detection datasets into large-scale medical visual question answering (VQA)
data with Chain-of-Thought (CoT) reasoning by linking lesion boxes to organ
segmentation and structured rationales. These contextual signals enable medical
vision-language models to generate question-answer pairs with step-by-step
reasoning. To utilize this data effectively, we propose an Integrated
CoT-Curriculum Strategy composed of an Easy stage with explicit lesion boxes
for visual grounding, a Medium stage that encourages implicit localization, and
a Hard stage for weakly supervised reasoning. Experimental results demonstrate
that MedCLM attains state-of-the-art performance on several medical VQA
benchmarks, providing a scalable framework for developing clinically aligned
medical vision-language models.

</details>


### [178] [Visual Representations inside the Language Model](https://arxiv.org/abs/2510.04819)
*Benlin Liu,Amita Kamath,Madeleine Grunde-McLaughlin,Winson Han,Ranjay Krishna*

Main category: cs.CV

TL;DR: 本文研究了多模态语言模型（MLMs）在感知任务上表现不佳的原因，通过分析视觉键值令牌在语言模型中的处理过程，发现视觉值令牌编码了足够的感知信息，但语言模型未能充分利用这些信息。


<details>
  <summary>Details</summary>
Motivation: 尽管已有工作分析了VIT编码器和transformer激活，但我们仍不理解为什么多模态语言模型在感知密集型任务上表现不佳。本文从视觉键值令牌的角度提供了一个新的研究视角。

Method: 研究流行MLMs（LLaVA-OneVision、Qwen2.5-VL、Llama-3-LLaVA-NeXT）处理视觉键值令牌的方式，分析视觉信息在语言模型中的流动，并测试零样本下的分割、语义对应、时间对应和参考表达检测等任务。

Result: 发现图像值令牌编码了足够的感知信息，但语言模型包含的视觉信息比未经MLM微调的视觉编码器（SigLIP）更少。后期层的图像键令牌存在降低感知能力的伪影。添加文本前缀可以改善视觉表示的感知能力。在BLINK基准测试中，33.3%的艺术风格问题中，语言模型中的感知信息未能输出。

Conclusion: 研究揭示了键值令牌在多模态系统中的重要作用，为MLMs的机制可解释性提供了新见解，并建议了训练视觉编码器和语言模型组件的新方向。

Abstract: Despite interpretability work analyzing VIT encoders and transformer
activations, we don't yet understand why Multimodal Language Models (MLMs)
struggle on perception-heavy tasks. We offer an under-studied perspective by
examining how popular MLMs (LLaVA-OneVision, Qwen2.5-VL, and
Llama-3-LLaVA-NeXT) process their visual key-value tokens. We first study the
flow of visual information through the language model, finding that image value
tokens encode sufficient information to perform several perception-heavy tasks
zero-shot: segmentation, semantic correspondence, temporal correspondence, and
referring expression detection. We find that while the language model does
augment the visual information received from the projection of input visual
encodings-which we reveal correlates with overall MLM perception capability-it
contains less visual information on several tasks than the equivalent visual
encoder (SigLIP) that has not undergone MLM finetuning. Further, we find that
the visual information corresponding to input-agnostic image key tokens in
later layers of language models contains artifacts which reduce perception
capability of the overall MLM. Next, we discuss controlling visual information
in the language model, showing that adding a text prefix to the image input
improves perception capabilities of visual representations. Finally, we reveal
that if language models were able to better control their visual information,
their perception would significantly improve; e.g., in 33.3% of Art Style
questions in the BLINK benchmark, perception information present in the
language model is not surfaced to the output! Our findings reveal insights into
the role of key-value tokens in multimodal systems, paving the way for deeper
mechanistic interpretability of MLMs and suggesting new directions for training
their visual encoder and language model components.

</details>


### [179] [Paper2Video: Automatic Video Generation from Scientific Papers](https://arxiv.org/abs/2510.05096)
*Zeyu Zhu,Kevin Qinghong Lin,Mike Zheng Shou*

Main category: cs.CV

TL;DR: PaperTalker是一个用于学术演示视频生成的多智能体框架，通过整合幻灯片生成、字幕、语音合成和说话人渲染等技术，实现了从研究论文自动生成演示视频的功能。


<details>
  <summary>Details</summary>
Motivation: 学术演示视频制作过程高度劳动密集，需要大量时间进行幻灯片设计、录制和编辑。现有的视频生成方法难以处理研究论文中的密集多模态信息和多个对齐通道的协调问题。

Method: 提出了多智能体框架，集成了幻灯片生成与布局优化、光标定位、字幕生成、语音合成和说话人渲染等技术，采用并行化幻灯片生成以提高效率。

Result: 在Paper2Video数据集上的实验表明，该方法生成的演示视频比现有基线方法更加忠实和内容丰富。

Conclusion: PaperTalker为自动化学术视频生成迈出了实用的一步，提供了可用的数据集、智能体和代码资源。

Abstract: Academic presentation videos have become an essential medium for research
communication, yet producing them remains highly labor-intensive, often
requiring hours of slide design, recording, and editing for a short 2 to 10
minutes video. Unlike natural video, presentation video generation involves
distinctive challenges: inputs from research papers, dense multi-modal
information (text, figures, tables), and the need to coordinate multiple
aligned channels such as slides, subtitles, speech, and human talker. To
address these challenges, we introduce PaperTalker, the first benchmark of 101
research papers paired with author-created presentation videos, slides, and
speaker metadata. We further design four tailored evaluation metrics--Meta
Similarity, PresentArena, PresentQuiz, and IP Memory--to measure how videos
convey the paper's information to the audience. Building on this foundation, we
propose PaperTalker, the first multi-agent framework for academic presentation
video generation. It integrates slide generation with effective layout
refinement by a novel effective tree search visual choice, cursor grounding,
subtitling, speech synthesis, and talking-head rendering, while parallelizing
slide-wise generation for efficiency. Experiments on Paper2Video demonstrate
that the presentation videos produced by our approach are more faithful and
informative than existing baselines, establishing a practical step toward
automated and ready-to-use academic video generation. Our dataset, agent, and
code are available at https://github.com/showlab/Paper2Video.

</details>
