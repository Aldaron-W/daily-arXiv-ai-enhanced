<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 51]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.SE](#cs.SE) [Total: 1]
- [cs.CV](#cs.CV) [Total: 2]
- [cs.LG](#cs.LG) [Total: 2]
- [cs.CR](#cs.CR) [Total: 4]
- [cs.AI](#cs.AI) [Total: 8]
- [cs.DB](#cs.DB) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Semantic Attractors and the Emergence of Meaning: Towards a Teleological Model of AGI](https://arxiv.org/abs/2508.18290)
*Hans-Joachim Rudolph*

Main category: cs.CL

TL;DR: 提出基于复数语义空间和语义吸引子的AGI理论框架，通过张量变换和循环操作建模语义，强调意义通过递归收敛而非统计预测形成


<details>
  <summary>Details</summary>
Motivation: 突破当前基于统计的transformer模型局限，探索能够真正形成语义而非仅仅预测词汇的认知架构，解决反讽、多义词和歧义等复杂语义现象

Method: 使用复数空间中的循环操作和虚数单位i，构建旋转语义结构，引入作为目的性操作符的语义吸引子(Microvitum)，通过梯度流、张量变形和迭代矩阵动力学实现语义转换

Result: 建立了能够建模复杂语义现象的数学框架，提供了语义稳定性和表达深度的理论模型

Conclusion: 真正意义来自向语义一致性的递归收敛而非模拟，需要设计能够塑造语言而不仅仅是预测语言的新型认知架构

Abstract: This essay develops a theoretical framework for a semantic Artificial General
Intelligence (AGI) based on the notion of semantic attractors in complex-valued
meaning spaces. Departing from current transformer-based language models, which
operate on statistical next-token prediction, we explore a model in which
meaning is not inferred probabilistically but formed through recursive
tensorial transformation. Using cyclic operations involving the imaginary unit
\emph{i}, we describe a rotational semantic structure capable of modeling
irony, homonymy, and ambiguity. At the center of this model, however, is a
semantic attractor -- a teleological operator that, unlike statistical
computation, acts as an intentional agent (Microvitum), guiding meaning toward
stability, clarity, and expressive depth. Conceived in terms of gradient flows,
tensor deformations, and iterative matrix dynamics, the attractor offers a
model of semantic transformation that is not only mathematically suggestive,
but also philosophically significant. We argue that true meaning emerges not
from simulation, but from recursive convergence toward semantic coherence, and
that this requires a fundamentally new kind of cognitive architecture -- one
designed to shape language, not just predict it.

</details>


### [2] [LLMs Can't Handle Peer Pressure: Crumbling under Multi-Agent Social Interactions](https://arxiv.org/abs/2508.18321)
*Maojia Song,Tej Deep Pala,Weisheng Jin,Amir Zadeh,Chuan Li,Dorien Herremans,Soujanya Poria*

Main category: cs.CL

TL;DR: KAIROS基准测试研究LLM在多智能体系统中如何基于历史印象建立信任、抵抗错误信息并整合同伴输入，发现GRPO强化学习方法在提升性能的同时降低了社交影响鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 研究LLM在多智能体协作中如何形成信任、抵抗错误信息并整合同伴输入，这些因素对复杂社交动态下的集体智能实现至关重要。

Method: 提出KAIROS基准测试，模拟不同可靠性同伴的问答竞赛，控制专家-新手角色、噪声人群和对抗性同伴等条件，评估提示工程、监督微调和强化学习（GRPO）等缓解策略。

Result: GRPO结合多智能体上下文、基于结果的奖励和无约束推理获得最佳整体性能，但相比基础模型降低了社交影响鲁棒性。

Conclusion: 多智能体强化学习能有效提升LLM协作性能，但需要平衡性能提升与社交影响鲁棒性之间的关系。

Abstract: Large language models (LLMs) are increasingly deployed in multi-agent systems
(MAS) as components of collaborative intelligence, where peer interactions
dynamically shape individual decision-making. Although prior work has focused
on conformity bias, we extend the analysis to examine how LLMs form trust from
previous impressions, resist misinformation, and integrate peer input during
interaction, key factors for achieving collective intelligence under complex
social dynamics. We present KAIROS, a benchmark simulating quiz contests with
peer agents of varying reliability, offering fine-grained control over
conditions such as expert-novice roles, noisy crowds, and adversarial peers.
LLMs receive both historical interactions and current peer responses, allowing
systematic investigation into how trust, peer action, and self-confidence
influence decisions. As for mitigation strategies, we evaluate prompting,
supervised fine-tuning, and reinforcement learning, Group Relative Policy
Optimisation (GRPO), across multiple models. Our results reveal that GRPO with
multi-agent context combined with outcome-based rewards and unconstrained
reasoning achieves the best overall performance, but also decreases the
robustness to social influence compared to Base models. The code and datasets
are available at: https://github.com/declare-lab/KAIROS.

</details>


### [3] [Not All Visitors are Bilingual: A Measurement Study of the Multilingual Web from an Accessibility Perspective](https://arxiv.org/abs/2508.18328)
*Masudul Hasan Masud Bhuiyan,Matteo Varvello,Yasir Zaki,Cristian-Alexandru Staicu*

Main category: cs.CL

TL;DR: LangCrUX是首个大规模多语言网页数据集，包含12种非拉丁语系的12万个网站。研究发现网页可访问性提示往往无法反映内容的语言多样性，导致屏幕阅读器效果不佳。为此提出了Kizuki语言感知自动化测试扩展工具。


<details>
  <summary>Details</summary>
Motivation: 网络多语言化趋势明显，但辅助技术对非拉丁语系支持不足，导致视觉障碍用户面临严重可访问性挑战。缺乏全面多语言网页数据集限制了大规模研究。

Method: 构建LangCrUX数据集（12种语言、12万个网站），系统分析多语言网页可访问性，发现语言提示与内容不一致问题，开发Kizuki语言感知自动化测试工具。

Result: 发现网页可访问性提示普遍忽视语言多样性，无法准确反映可见内容的语言特征，严重降低了屏幕阅读器的有效性。

Conclusion: 多语言网页可访问性存在系统性缺陷，需要语言感知的自动化测试工具来改善辅助技术对非拉丁语系的支持，提升视觉障碍用户的多语言网页访问体验。

Abstract: English is the predominant language on the web, powering nearly half of the
world's top ten million websites. Support for multilingual content is
nevertheless growing, with many websites increasingly combining English with
regional or native languages in both visible content and hidden metadata. This
multilingualism introduces significant barriers for users with visual
impairments, as assistive technologies like screen readers frequently lack
robust support for non-Latin scripts and misrender or mispronounce non-English
text, compounding accessibility challenges across diverse linguistic contexts.
Yet, large-scale studies of this issue have been limited by the lack of
comprehensive datasets on multilingual web content. To address this gap, we
introduce LangCrUX, the first large-scale dataset of 120,000 popular websites
across 12 languages that primarily use non-Latin scripts. Leveraging this
dataset, we conduct a systematic analysis of multilingual web accessibility and
uncover widespread neglect of accessibility hints. We find that these hints
often fail to reflect the language diversity of visible content, reducing the
effectiveness of screen readers and limiting web accessibility. We finally
propose Kizuki, a language-aware automated accessibility testing extension to
account for the limited utility of language-inconsistent accessibility hints.

</details>


### [4] [Language-Specific Layer Matters: Efficient Multilingual Enhancement for Large Vision-Language Models](https://arxiv.org/abs/2508.18381)
*Yuchun Fan,Yilin Wang,Yongyu Mu,Lei Huang,Bei Li,Xiaocheng Feng,Tong Xiao,Jingbo Zhu*

Main category: cs.CL

TL;DR: PLAST是一种通过精确语言特定层微调来高效增强大型视觉语言模型多语言能力的训练方法，仅需调整14%的参数即可显著提升多语言理解性能


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在多语言能力上存在不平衡，研究发现多语言理解能力与浅层语言特定神经元激活密切相关

Method: 首先通过监控语言特定神经元激活来识别参与多语言理解的层，然后使用问题翻译对精确微调这些层以实现多语言对齐

Result: 在MM-Bench和MMMB基准测试中有效提升多语言能力，仅微调14%参数即可实现显著效率

Conclusion: PLAST方法可推广到低资源和复杂视觉推理任务，促进浅层语言特定视觉信息参与

Abstract: Large vision-language models (LVLMs) have demonstrated exceptional
capabilities in understanding visual information with human languages but also
exhibit an imbalance in multilingual capabilities. In this work, we delve into
the multilingual working pattern of LVLMs and identify a salient correlation
between the multilingual understanding ability of LVLMs and language-specific
neuron activations in shallow layers. Building on this insight, we introduce
PLAST, a training recipe that achieves efficient multilingual enhancement for
LVLMs by Precise LAnguage-Specific layers fine-Tuning. PLAST first identifies
layers involved in multilingual understanding by monitoring language-specific
neuron activations. These layers are then precisely fine-tuned with
question-translation pairs to achieve multilingual alignment. Our empirical
results on MM-Bench and MMMB demonstrate that PLAST effectively improves the
multilingual capabilities of LVLMs and achieves significant efficiency with
only 14% of the parameters tuned. Further analysis reveals that PLAST can be
generalized to low-resource and complex visual reasoning tasks, facilitating
the language-specific visual information engagement in shallow layers.

</details>


### [5] [Backprompting: Leveraging Synthetic Production Data for Health Advice Guardrails](https://arxiv.org/abs/2508.18384)
*Kellen Tan Cheng,Anna Lisa Gentile,Chad DeLuca,Guang-Jie Ren*

Main category: cs.CL

TL;DR: 这篇论文提出了backprompting方法，通过生成生产级别标签数据来改善健康建议监测器的性能，在参数更少的情况下超越GPT-4o


<details>
  <summary>Details</summary>
Motivation: 企业环境中大语言模型的普及带来了风险，而发展健壁监测器面临真实生产数据难以获取的挑战

Method: 提出backprompting方法生成生产级别标签数据，结合稀疏人巡环聚类技术进行标注，构建与原始数据集并行但类似真实LLM输出的语料库

Result: 在健康建议识别这个最难的监测任务中，该方法表现超过其他解决方案，监测器性能超过GPT-4o达3.73%，而参数数量只有其的1/400

Conclusion: backprompting方法能有效生成高质量标签数据，为健壁监测器的发展提供了可靠的数据支撑，在资源效率和性能方面都显示出显著优势

Abstract: The pervasiveness of large language models (LLMs) in enterprise settings has
also brought forth a significant amount of risks associated with their usage.
Guardrails technologies aim to mitigate this risk by filtering LLMs'
input/output text through various detectors. However, developing and
maintaining robust detectors faces many challenges, one of which is the
difficulty in acquiring production-quality labeled data on real LLM outputs
prior to deployment. In this work, we propose backprompting, a simple yet
intuitive solution to generate production-like labeled data for health advice
guardrails development. Furthermore, we pair our backprompting method with a
sparse human-in-the-loop clustering technique to label the generated data. Our
aim is to construct a parallel corpus roughly representative of the original
dataset yet resembling real LLM output. We then infuse existing datasets with
our synthetic examples to produce robust training data for our detector. We
test our technique in one of the most difficult and nuanced guardrails: the
identification of health advice in LLM output, and demonstrate improvement
versus other solutions. Our detector is able to outperform GPT-4o by up to
3.73%, despite having 400x less parameters.

</details>


### [6] [Integral Transformer: Denoising Attention, Not Too Much Not Too Little](https://arxiv.org/abs/2508.18387)
*Ivan Kobyzev,Abbas Ghaddar,Dingtao Hu,Boxing Chen*

Main category: cs.CL

TL;DR: 提出了Integral Transformer，通过从logit分布中采样信号来整合去噪注意力，有效减少注意力噪声同时保留特殊令牌的关键信息，在多个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的负注意力方法（如Cog Attention和Differential Transformer）虽然解决了注意力噪声问题，但可能丢弃有用信息。softmax自注意力往往给语义无关的令牌（如特殊令牌和标点）分配过多权重。

Method: 提出Integral Transformer自注意力机制，通过整合从logit分布中采样的信号来去噪注意力，同时保留对模型性能至关重要的特殊令牌的贡献。

Result: 在成熟的知识和推理语言基准测试中，该模型优于普通、Cog和Differential注意力变体。分析表明，在底层使用普通自注意力可提升性能，且Integral Transformer能有效平衡注意力分布并减少上层秩塌陷。

Conclusion: Integral Transformer提供了一种有效的注意力去噪方法，既能减少噪声又能保留重要信息，在Transformer架构中实现了更好的注意力分布平衡。

Abstract: Softmax self-attention often assigns disproportionate weight to semantically
uninformative tokens such as special tokens and punctuation, a phenomenon known
as attention noise. While recent methods like Cog Attention and the
Differential Transformer have addressed this by introducing negative attention
scores, they risk discarding useful information. In this paper, we propose the
Integral Transformer, a novel self-attention mechanism that denoises attention
by integrating signals sampled from the logit distribution. Our approach
mitigates noise while preserving the contributions of special tokens critical
for model performance. Extensive experiments demonstrate that our model
outperforms vanilla, Cog, and Differential attention variants on
well-established knowledge and reasoning language benchmarks. Moreover, our
analysis reveals that employing vanilla self-attention in the lower Transformer
layers enhances performance and that the Integral Transformer effectively
balances attention distributions and reduces rank collapse in upper layers.

</details>


### [7] [Latent Self-Consistency for Reliable Majority-Set Selection in Short- and Long-Answer Reasoning](https://arxiv.org/abs/2508.18395)
*Jeong-seok Oh,Jay-yoon Lee*

Main category: cs.CL

TL;DR: LSC通过可学习的token嵌入选择语义最一致的回答，在短式和长式推理基准测试中均优于现有方法，计算开销可忽略


<details>
  <summary>Details</summary>
Motivation: 解决LLM概率解码在复杂或长式问题上输出不一致的问题，现有方法在短式和长式回答格式间存在准确率权衡

Method: 使用可学习的token嵌入选择语义最一致的回答，通过轻量级前向生成摘要token，推理时间增加不到1%且无需修改模型架构

Result: 在6个短式和5个长式推理基准测试中，LSC在所有测试上平均优于SC、USC和WUCS，同时提供良好校准的置信度估计

Conclusion: LSC是一种实用的跨答案格式一致性选择方法，计算开销小且性能可靠

Abstract: Probabilistic decoding in Large Language Models (LLMs) often yields
inconsistent outputs, particularly on complex or long-form questions.
Self-Consistency (SC) mitigates this for short-form QA by majority voting over
exact strings, whereas Universal Self-Consistency (USC) and Weighted Unigram
Consistency Score (WUCS) extend to long-form responses but lose accuracy on
short-form benchmarks.
  We introduce Latent Self-Consistency (LSC), which selects the most
semantically consistent response using learnable token embeddings. A
lightweight forward generation of summary tokens increases inference time by
less than 1% and requires no changes to the model architecture.
  Across 6 short-form and 5 long-form reasoning benchmarks (e.g., MATH, MMLU,
TruthfulQA), LSC surpasses SC, USC and WUCS on all short-form and long-form
ones on average, while maintaining negligible computational overhead. These
results position LSC as a practical consistency-selection method that works
reliably across answer formats. Additionally, LSC provides well-calibrated
confidence estimates, maintaining low Expected Calibration Error across both
answer formats.

</details>


### [8] [Can Out-of-Distribution Evaluations Uncover Reliance on Shortcuts? A Case Study in Question Answering](https://arxiv.org/abs/2508.18407)
*Michal Štefánik,Timothee Mickus,Marek Kadlčík,Michal Spiegel,Josef Kuchař*

Main category: cs.CL

TL;DR: 本文质疑OOD评估在反映真实世界模型失败方面的有效性，通过分析QA模型中的伪特征依赖问题，发现不同OOD数据集对模型鲁棒性的评估质量差异很大，有些甚至不如简单的in-distribution评估。


<details>
  <summary>Details</summary>
Motivation: 挑战当前AI研究中普遍采用的OOD评估假设，认为这种评估方法可能无法准确捕捉模型在真实部署中的潜在失败模式，特别是针对QA模型中存在的伪特征依赖问题。

Method: 将OOD评估结果与现有QA模型中记录的具体失败模式（依赖伪特征或预测捷径）进行对比分析，研究不同OOD数据集在评估模型对捷径鲁棒性方面的质量差异。

Result: 发现不同OOD数据集提供的模型鲁棒性估计质量差异很大，部分数据集的表现甚至不如简单的in-distribution评估。观察到伪捷径在ID和OOD数据集之间存在共享现象，但训练和评估数据集质量在很大程度上是脱节的。

Conclusion: 研究揭示了常用OOD泛化评估方法的局限性，为在QA及其他领域更稳健地评估模型泛化能力提供了方法论建议和推荐。

Abstract: A majority of recent work in AI assesses models' generalization capabilities
through the lens of performance on out-of-distribution (OOD) datasets. Despite
their practicality, such evaluations build upon a strong assumption: that OOD
evaluations can capture and reflect upon possible failures in a real-world
deployment.
  In this work, we challenge this assumption and confront the results obtained
from OOD evaluations with a set of specific failure modes documented in
existing question-answering (QA) models, referred to as a reliance on spurious
features or prediction shortcuts.
  We find that different datasets used for OOD evaluations in QA provide an
estimate of models' robustness to shortcuts that have a vastly different
quality, some largely under-performing even a simple, in-distribution
evaluation. We partially attribute this to the observation that spurious
shortcuts are shared across ID+OOD datasets, but also find cases where a
dataset's quality for training and evaluation is largely disconnected. Our work
underlines limitations of commonly-used OOD-based evaluations of
generalization, and provides methodology and recommendations for evaluating
generalization within and beyond QA more robustly.

</details>


### [9] [How Reliable are LLMs for Reasoning on the Re-ranking task?](https://arxiv.org/abs/2508.18444)
*Nafis Tanveer Islam,Zhiming Zhao*

Main category: cs.CL

TL;DR: 该论文分析了不同训练方法对LLM在重排序任务中语义理解能力的影响，并研究模型是否能生成更好的文本推理来提高透明度和解决有限训练数据的问题。


<details>
  <summary>Details</summary>
Motivation: 随着LLM语义理解能力提升，其透明度下降且在新系统中准确重排序面临挑战。需要深入理解LLM内部工作机制来解释重排序推理，并解决有限用户参与和排名数据的问题。

Method: 使用环境与地球科学领域的相对较小排名数据集对检索内容进行重排序，分析不同训练方法对LLM语义理解的影响，并检查可解释信息以验证重排序是否可通过解释性进行推理。

Result: 研究发现某些训练方法比其他方法表现出更好的可解释性，表明并非所有训练方法都学到了准确的语义理解，而是获得了抽象知识来优化评估，这对LLM的真正可靠性提出了质疑。

Conclusion: 需要进一步分析不同训练方法如何影响LLM在重排序任务中的语义理解能力，以确定模型是否能生成更明智的文本推理来克服透明度问题和有限训练数据的挑战。

Abstract: With the improving semantic understanding capability of Large Language Models
(LLMs), they exhibit a greater awareness and alignment with human values, but
this comes at the cost of transparency. Although promising results are achieved
via experimental analysis, an in-depth understanding of the LLM's internal
workings is unavoidable to comprehend the reasoning behind the re-ranking,
which provides end users with an explanation that enables them to make an
informed decision. Moreover, in newly developed systems with limited user
engagement and insufficient ranking data, accurately re-ranking content remains
a significant challenge. While various training methods affect the training of
LLMs and generate inference, our analysis has found that some training methods
exhibit better explainability than others, implying that an accurate semantic
understanding has not been learned through all training methods; instead,
abstract knowledge has been gained to optimize evaluation, which raises
questions about the true reliability of LLMs. Therefore, in this work, we
analyze how different training methods affect the semantic understanding of the
re-ranking task in LLMs and investigate whether these models can generate more
informed textual reasoning to overcome the challenges of transparency or LLMs
and limited training data. To analyze the LLMs for re-ranking tasks, we utilize
a relatively small ranking dataset from the environment and the Earth science
domain to re-rank retrieved content. Furthermore, we also analyze the
explainable information to see if the re-ranking can be reasoned using
explainability.

</details>


### [10] [Integrating gender inclusivity into large language models via instruction tuning](https://arxiv.org/abs/2508.18466)
*Alina Wróblewska,Bartosz Żuk*

Main category: cs.CL

TL;DR: 该研究通过IPIS数据集微调大型语言模型，为波兰语设计性别包容性系统提示，以解决波兰语中男性偏见问题。


<details>
  <summary>Details</summary>
Motivation: 波兰语由于历史和政治惯例，主要使用男性形式来指代男性、女性和混合性别群体，导致训练在该语言上的大语言模型继承并强化了这种性别偏见，产生性别不平衡的输出。

Method: 使用IPIS数据集（包含人工制作的性别包容性校对和波兰语到英语翻译指令）微调多语言和波兰语特定的大语言模型，并基于语言学理论设计明确的性别包容性系统提示。

Result: 研究对Llama-8B、Mistral-7B、Mistral-Nemo等多语言模型以及Bielik、PLLuM等波兰语特定模型进行了IPIS微调。

Conclusion: 该方法旨在将性别包容性作为这些模型的内在特征，为缓解波兰语生成中的性别偏见提供了系统性解决方案。

Abstract: Imagine a language with masculine, feminine, and neuter grammatical genders,
yet, due to historical and political conventions, masculine forms are
predominantly used to refer to men, women and mixed-gender groups. This is the
reality of contemporary Polish. A social consequence of this unfair linguistic
system is that large language models (LLMs) trained on Polish texts inherit and
reinforce this masculine bias, generating gender-imbalanced outputs. This study
addresses this issue by tuning LLMs using the IPIS dataset, a collection of
human-crafted gender-inclusive proofreading in Polish and Polish-to-English
translation instructions. Grounded in a theoretical linguistic framework, we
design a system prompt with explicit gender-inclusive guidelines for Polish. In
our experiments, we IPIS-tune multilingual LLMs (Llama-8B, Mistral-7B and
Mistral-Nemo) and Polish-specific LLMs (Bielik and PLLuM). Our approach aims to
integrate gender inclusivity as an inherent feature of these models, offering a
systematic solution to mitigate gender bias in Polish language generation.

</details>


### [11] [Principled Detection of Hallucinations in Large Language Models via Multiple Testing](https://arxiv.org/abs/2508.18473)
*Jiawei Li,Akshayaa Magesh,Venugopal V. Veeravalli*

Main category: cs.CL

TL;DR: 本文提出了一种基于多重检验假设测试的方法来检测大语言模型的幻觉问题，将幻觉检测建模为假设检验问题，并与机器学习中的分布外检测问题进行类比。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然功能强大，但容易产生幻觉（生成看似自信但实际错误或无意义的回答），需要有效的检测方法来识别这些不可靠的输出。

Method: 采用多重检验假设测试的方法来解决幻觉检测问题，将问题形式化为假设检验，并与分布外检测问题建立联系。

Result: 通过大量实验验证了所提方法的鲁棒性，证明其优于现有的最先进方法。

Conclusion: 提出的多重检验方法能够有效检测大语言模型的幻觉，为解决模型可靠性问题提供了新的技术途径。

Abstract: While Large Language Models (LLMs) have emerged as powerful foundational
models to solve a variety of tasks, they have also been shown to be prone to
hallucinations, i.e., generating responses that sound confident but are
actually incorrect or even nonsensical. In this work, we formulate the problem
of detecting hallucinations as a hypothesis testing problem and draw parallels
to the problem of out-of-distribution detection in machine learning models. We
propose a multiple-testing-inspired method to solve the hallucination detection
problem, and provide extensive experimental results to validate the robustness
of our approach against state-of-the-art methods.

</details>


### [12] [COMET-poly: Machine Translation Metric Grounded in Other Candidates](https://arxiv.org/abs/2508.18549)
*Maike Züfle,Vilém Zouhar,Tu Anh Dinh,Felipe Maia Polo,Jan Niehues,Mrinmaya Sachan*

Main category: cs.CL

TL;DR: 提出了两种新的机器翻译自动评估指标COMET-polycand和COMET-polyic，通过引入额外翻译信息来提升评估性能，相比传统单翻译评估方法有显著改进


<details>
  <summary>Details</summary>
Motivation: 现有自动评估指标通常只考虑源句子和单个翻译，而人类评估时会参考多个备选翻译，这种评估设置差异可能影响自动指标的性能

Method: COMET-polycand使用同一源句的多个备选翻译进行比较；COMET-polyic采用检索式上下文学习方法，输入相似源文本的翻译及其人工标注质量分数

Result: COMET-polycand加入单个额外翻译即可提升段级指标性能（Kendall's tau-b相关性从0.079提升到0.118），加入更多翻译效果更好；COMET-polyic也获得类似改进（0.079到0.116）

Conclusion: 通过引入额外翻译信息，两种新指标显著提升了机器翻译自动评估的性能，证明了多翻译上下文对评估的重要性

Abstract: Automated metrics for machine translation attempt to replicate human
judgment. Unlike humans, who often assess a translation in the context of
multiple alternatives, these metrics typically consider only the source
sentence and a single translation. This discrepancy in the evaluation setup may
negatively impact the performance of automated metrics. We propose two
automated metrics that incorporate additional information beyond the single
translation. COMET-polycand uses alternative translations of the same source
sentence to compare and contrast with the translation at hand, thereby
providing a more informed assessment of its quality. COMET-polyic, inspired by
retrieval-based in-context learning, takes in translations of similar source
texts along with their human-labeled quality scores to guide the evaluation. We
find that including a single additional translation in COMET-polycand improves
the segment-level metric performance (0.079 to 0.118 Kendall's tau-b
correlation), with further gains when more translations are added.
Incorporating retrieved examples in COMET-polyic yields similar improvements
(0.079 to 0.116 Kendall's tau-b correlation). We release our models publicly.

</details>


### [13] [The Mind's Eye: A Multi-Faceted Reward Framework for Guiding Visual Metaphor Generation](https://arxiv.org/abs/2508.18569)
*Girish A. Koushik,Fatemeh Nazarieh,Katherine Birch,Shenbin Qian,Diptesh Kanojia*

Main category: cs.CL

TL;DR: 提出自评估视觉隐喻生成框架，通过隐喻分解和意义对齐指标，探索训练自由和训练增强两种方法，在隐喻对齐方面取得良好效果。


<details>
  <summary>Details</summary>
Motivation: 视觉隐喻生成需要同时理解语言含义和保持视觉连贯性，现有方法在隐喻对齐方面存在挑战，需要更好的评估和生成机制。

Method: 提出自评估框架，结合新提出的隐喻分解分数和意义对齐指标。开发两种方法：训练自由管道（显式分解提示为源-目标-意义映射）和训练增强管道（使用自评估奖励模式改进对齐）。

Result: 在测试集上，训练自由方法在分解、CLIP和意义对齐分数上超越GPT-4o和Imagen等强基线。用户研究显示参与者整体偏好GPT-4o，但训练自由方法在开源方法中领先，在抽象隐喻上优于Imagen。

Conclusion: 结构化提示和轻量级强化学习在有限计算资源下能有效进行隐喻对齐，与人类偏好的差距主要来自美学质量和采样敏感性。

Abstract: Visual metaphor generation is a challenging task that aims to generate an
image given an input text metaphor. Inherently, it needs language understanding
to bind a source concept with a target concept, in a way that preserves meaning
while ensuring visual coherence. We propose a self-evaluating visual metaphor
generation framework that focuses on metaphor alignment. Our self-evaluation
approach combines existing metrics with our newly proposed metaphor
decomposition score and a meaning alignment (MA) metric. Within this setup, we
explore two novel approaches: a training-free pipeline that explicitly
decomposes prompts into source-target-meaning (S-T-M) mapping for image
synthesis, and a complementary training-based pipeline that improves alignment
using our proposed self-evaluation reward schema, without any large-scale
retraining. On the held-out test set, the training-free approach surpasses
strong closed baselines (GPT-4o, Imagen) on decomposition, CLIP, and MA scores,
with the training-based approach close behind. We evaluate our framework output
using a user-facing study, and observed that participants preferred GPT-4o
overall, while our training-free pipeline led open-source methods and edged
Imagen on abstract metaphors. Our analyses show S-T-M prompting helps longer or
more abstract metaphors, with closed models excelling on short, concrete cases;
we also observe sensitivity to sampler settings. Overall, structured prompting
and lightweight RL perform metaphor alignment well under modest compute, and
remaining gaps to human preference appear driven by aesthetics and sampling.

</details>


### [14] [What do language models model? Transformers, automata, and the format of thought](https://arxiv.org/abs/2508.18598)
*Colin Klein*

Main category: cs.CL

TL;DR: 这篇论文探讨大语言模型的本质，认为它们主要是对训练语料库的模拟，而非人类语言能力的真实模型。作者通过讨论Transformer架构的线性处理特性与人类超线性计算格式的差异来支持这一观点。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型的真正本质：它们是否反映了人类语言能力，还是仅仅模拟了训练语料库的特征。作者希望提供一个非简化的解释，讨论LLM与人类语言处理的根本差异。

Method: 通过对比Transformer架构的计算特性（最多支持线性格式处理）与人类计算超线性格式的差异，结合Liu等人关于短接自动机的理论提出分析框架。

Result: 论文得出结论认为大语言模型主要是对语料库的模拟，而非人类语言能力的真实模型。但这并不是一个消极的观点，因为语言本身就是一种"话语机器"，可以根据语境生成新语言。

Conclusion: 人类和LLM都学会了使用语言这种技术，但通过完全不同的方式。LLM通过模拟大量语料库来学习语言使用，而不是通过人类计算机制。这种差异使得LLM成为了解语言作为社会现象的有趣模型。

Abstract: What do large language models actually model? Do they tell us something about
human capacities, or are they models of the corpus we've trained them on? I
give a non-deflationary defence of the latter position. Cognitive science tells
us that linguistic capabilities in humans rely supralinear formats for
computation. The transformer architecture, by contrast, supports at best a
linear formats for processing. This argument will rely primarily on certain
invariants of the computational architecture of transformers. I then suggest a
positive story about what transformers are doing, focusing on Liu et al.
(2022)'s intriguing speculations about shortcut automata. I conclude with why I
don't think this is a terribly deflationary story. Language is not (just) a
means for expressing inner state but also a kind of 'discourse machine' that
lets us make new language given appropriate context. We have learned to use
this technology in one way; LLMs have also learned to use it too, but via very
different means.

</details>


### [15] [A New NMT Model for Translating Clinical Texts from English to Spanish](https://arxiv.org/abs/2508.18607)
*Rumeng Li,Xun Wang,Hong Yu*

Main category: cs.CL

TL;DR: NOOV是一个新的神经机器翻译系统，专门用于将电子健康记录从英语翻译到西班牙语，通过整合双语词典和短语查找表来解决未知词汇和词汇重复问题


<details>
  <summary>Details</summary>
Motivation: 电子健康记录翻译具有重要临床意义，但面临缺乏平行语料库和大量未知词汇的挑战

Method: 整合从平行语料库自动学习的双语词典和从大型生物医学知识资源提取的短语查找表，缓解NMT中的未知词问题和词汇重复挑战

Result: 评估显示NOOV能够生成更好的EHR翻译，在准确性和流畅性方面都有改进

Conclusion: NOOV系统有效解决了电子健康记录翻译中的关键挑战，提升了翻译质量

Abstract: Translating electronic health record (EHR) narratives from English to Spanish
is a clinically important yet challenging task due to the lack of a
parallel-aligned corpus and the abundant unknown words contained. To address
such challenges, we propose \textbf{NOOV} (for No OOV), a new neural machine
translation (NMT) system that requires little in-domain parallel-aligned corpus
for training. NOOV integrates a bilingual lexicon automatically learned from
parallel-aligned corpora and a phrase look-up table extracted from a large
biomedical knowledge resource, to alleviate both the unknown word problem and
the word-repeat challenge in NMT, enhancing better phrase generation of NMT
systems. Evaluation shows that NOOV is able to generate better translation of
EHR with improvement in both accuracy and fluency.

</details>


### [16] [Scaling Laws for Task-Stratified Knowledge in Post-Training Quantized Large Language Models](https://arxiv.org/abs/2508.18609)
*Chenxi Zhou,Pengfei Cao,Jiang Li,Jun Zhao,Kang Liu*

Main category: cs.CL

TL;DR: 这篇论文通过实验研究建立了语言模型后训练量化的任务分层缩放律，发现知识记忆比知识利用对量化参数更敏感，为知识感知量化策略提供了指导。


<details>
  <summary>Details</summary>
Motivation: 现有的量化模型缩放律往往忽略了后训练量化特定参数和任务特定敏感性，导致对PTQ如何准确影响语言模型各种知识能力的理解不充分。

Method: 进行广泛的实证研究，将语言模型知识解构为记忆能力和利用能力，并建立包含模型大小、有效位宽、校准集大小和分组大小的统一量化框架。

Result: 核心发现知识记忆能力在有效位宽、校准集大小和模型大小的变化下显示出比知识利用能力更高的敏感性，而知识利用能力更加稳健。

Conclusion: 这些发现提供了对PTQ影响的细粒度理解，为开发能更好保留目标认知功能的知识感知量化策略提供了指导。

Abstract: Large language models (LLMs) present significant deployment challenges due to
their scale, with post-training quantization (PTQ) emerging as a practical
compression solution. However, a comprehensive understanding of how PTQ
precisely impacts diverse LLM knowledge capabilities remains elusive, and
existing scaling laws for quantized models often overlook crucial PTQ-specific
parameters and task-specific sensitivities. This paper addresses these gaps by
conducting an extensive empirical investigation to establish task-stratified
scaling laws. We disentangle LLM knowledge into memorization and utilization
capabilities and develop a unified quantitative framework that incorporates
model size, effective bit-width, calibration set size, and group size. Our
central finding reveals that knowledge memorization exhibits markedly greater
sensitivity to variations in effective bit-width, calibration set size, and
model size compared to the more robust knowledge utilization. These findings
offer a fine-grained understanding of PTQ's impact and provide guidance for
developing knowledge-aware quantization strategies that can better preserve
targeted cognitive functions.

</details>


### [17] [Thinking Before You Speak: A Proactive Test-time Scaling Approach](https://arxiv.org/abs/2508.18648)
*Cong Liu,Wenchang Chai,Hejun Wu,Yan Pan,Pengxu Wei,Liang Lin*

Main category: cs.CL

TL;DR: TBYS框架通过在推理步骤间主动生成"insight"来增强大语言模型的数学推理能力，无需人工标注或微调


<details>
  <summary>Details</summary>
Motivation: LLMs在复杂推理任务中存在不足，因为训练数据缺乏人类思考过程中的关键洞察和中间意图表达

Method: 提出Thinking Before You Speak框架，在连续推理步骤间插入主动生成的insight来引导推理过程，并设计自动收集和过滤上下文示例的流程

Result: 在具有挑战性的数学数据集上验证了TBYS的有效性

Conclusion: 主动生成insight的方法能够有效弥补训练数据中缺失的人类推理模式，提升LLMs的复杂推理能力

Abstract: Large Language Models (LLMs) often exhibit deficiencies with complex
reasoning tasks, such as maths, which we attribute to the discrepancy between
human reasoning patterns and those presented in the LLMs' training data. When
dealing with complex problems, humans tend to think carefully before expressing
solutions. However, they often do not articulate their inner thoughts,
including their intentions and chosen methodologies. Consequently, critical
insights essential for bridging reasoning steps may be absent in training data
collected from human sources. To bridge this gap, we proposes inserting
\emph{insight}s between consecutive reasoning steps, which review the status
and initiate the next reasoning steps. Unlike prior prompting strategies that
rely on a single or a workflow of static prompts to facilitate reasoning,
\emph{insight}s are \emph{proactively} generated to guide reasoning processes.
We implement our idea as a reasoning framework, named \emph{Thinking Before You
Speak} (TBYS), and design a pipeline for automatically collecting and filtering
in-context examples for the generation of \emph{insight}s, which alleviates
human labeling efforts and fine-tuning overheads. Experiments on challenging
mathematical datasets verify the effectiveness of TBYS. Project website:
https://gitee.com/jswrt/TBYS

</details>


### [18] [Breaking the Trade-Off Between Faithfulness and Expressiveness for Large Language Models](https://arxiv.org/abs/2508.18651)
*Chenxu Yang,Qingyi Si,Zheng Lin*

Main category: cs.CL

TL;DR: 提出了Collaborative Decoding (CoDe)方法，通过动态整合有外部知识和无外部知识的输出概率，在保持表达力的同时提高大语言模型的忠实度


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在整合外部知识时难以同时保持忠实度和表达力，要么缺乏知识支持而损害忠实度，要么过于冗长而牺牲表达力

Method: 基于分布差异和模型置信度动态整合有无外部知识的输出概率，并引入知识感知重排序机制防止过度依赖参数知识

Result: CoDe框架在不同大语言模型和评估指标上都表现出色，在保持表达力的同时显著提升了忠实度

Conclusion: CoDe方法有效解决了忠实度与表达力之间的权衡问题，具有很好的有效性和泛化性

Abstract: Grounding responses in external knowledge represents an effective strategy
for mitigating hallucinations in Large Language Models (LLMs). However, current
LLMs struggle to seamlessly integrate knowledge while simultaneously
maintaining faithfulness (or fidelity) and expressiveness, capabilities that
humans naturally possess. This limitation results in outputs that either lack
support from external knowledge, thereby compromising faithfulness, or appear
overly verbose and unnatural, thus sacrificing expressiveness. In this work, to
break the trade-off between faithfulness and expressiveness, we propose
Collaborative Decoding (CoDe), a novel approach that dynamically integrates
output probabilities generated with and without external knowledge. This
integration is guided by distribution divergence and model confidence, enabling
the selective activation of relevant and reliable expressions from the model's
internal parameters. Furthermore, we introduce a knowledge-aware reranking
mechanism that prevents over-reliance on prior parametric knowledge while
ensuring proper utilization of provided external information. Through
comprehensive experiments, our plug-and-play CoDe framework demonstrates
superior performance in enhancing faithfulness without compromising
expressiveness across diverse LLMs and evaluation metrics, validating both its
effectiveness and generalizability.

</details>


### [19] [Emotion Omni: Enabling Empathetic Speech Response Generation through Large Language Models](https://arxiv.org/abs/2508.18655)
*Haoyu Wang,Guangyan Zhang,Jiale Chen,Jingyu Li,Yuehai Wang,Yiwen Guo*

Main category: cs.CL

TL;DR: 提出Emotion Omni模型架构，通过有限数据和无需大规模训练实现语音LLM的情感理解和共情语音生成


<details>
  <summary>Details</summary>
Motivation: 现有语音大语言模型缺乏对用户查询中丰富情感和副语言线索的深入理解，且传统情感语音模型需要大量数据和计算资源

Method: 提出新颖的Emotion Omni模型架构，基于开源TTS框架开发数据生成流水线构建20万情感对话数据集

Result: 构建了支持共情语音助手的数据集，实现了能够理解用户语音情感内容并生成共情语音响应的模型

Conclusion: 该方法为解决语音LLM情感理解问题提供了数据高效且计算资源友好的解决方案，提升了人机交互体验

Abstract: With the development of speech large language models (speech LLMs), users can
now interact directly with assistants via speech. However, most existing models
simply convert the response content into speech without fully understanding the
rich emotional and paralinguistic cues embedded in the user's query. In many
cases, the same sentence can have different meanings depending on the emotional
expression. Furthermore, emotional understanding is essential for improving
user experience in human-machine interaction. Currently, most speech LLMs with
empathetic capabilities are trained on massive datasets. This approach requires
vast amounts of data and significant computational resources. Therefore, a key
challenge lies in how to develop a speech LLM capable of generating empathetic
responses with limited data and without the need for large-scale training. To
address this challenge, we propose Emotion Omni, a novel model architecture
designed to understand the emotional content of user speech input and generate
empathetic speech responses. Additionally, we developed a data generation
pipeline based on an open-source TTS framework to construct a 200k emotional
dialogue dataset, which supports the construction of an empathetic speech
assistant. The demos are available at https://w311411.github.io/omni_demo/

</details>


### [20] [Tailored Teaching with Balanced Difficulty: Elevating Reasoning in Multimodal Chain-of-Thought via Prompt Curriculum](https://arxiv.org/abs/2508.18673)
*Xinglong Yang,Quan Feng,Zhongying Pan,Xiang Chen,Yu Tian,Wentong Li,Shuofei Qiao,Yuxia Geng,Xingyu Zhao,Sheng-Jun Huang*

Main category: cs.CL

TL;DR: 这篇论文提出了一种基于教育学原理的多模态链式思绪提示框架，通过结合模型感知难度和内在样本复杂性来选择最优的提示示例，显著提升了多模态大语言模型的性能和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有的MCoT提示方法使用随机或手动选择的示例，无法考虑模型特定的知识分布和任务的内在复杂性，导致模型表现次优且不稳定。

Method: 重构提示选择为提示课程设计问题，结合两种信号：(1)模型感知难度（通过活跃学习中的预测分歧量化）；(2)内在样本复杂性（测量问题-图像对的本质难度）。通过关联分析这些信号开发难度平衡的采样策略。

Result: 在5个挑战性测试集和多个流行MLLM上进行的广泛实验表明，该方法带来了显著且一致的性能提升，大大减少了随机采样导致的性能差异。

Conclusion: 该研究提供了一种有理论基础的健壁方法，通过根据模型能力适应性地选择提示示例，有效地增强了多模态推理能力。

Abstract: The effectiveness of Multimodal Chain-of-Thought (MCoT) prompting is often
limited by the use of randomly or manually selected examples. These examples
fail to account for both model-specific knowledge distributions and the
intrinsic complexity of the tasks, resulting in suboptimal and unstable model
performance. To address this, we propose a novel framework inspired by the
pedagogical principle of "tailored teaching with balanced difficulty". We
reframe prompt selection as a prompt curriculum design problem: constructing a
well ordered set of training examples that align with the model's current
capabilities. Our approach integrates two complementary signals: (1)
model-perceived difficulty, quantified through prediction disagreement in an
active learning setup, capturing what the model itself finds challenging; and
(2) intrinsic sample complexity, which measures the inherent difficulty of each
question-image pair independently of any model. By jointly analyzing these
signals, we develop a difficulty-balanced sampling strategy that ensures the
selected prompt examples are diverse across both dimensions. Extensive
experiments conducted on five challenging benchmarks and multiple popular
Multimodal Large Language Models (MLLMs) demonstrate that our method yields
substantial and consistent improvements and greatly reduces performance
discrepancies caused by random sampling, providing a principled and robust
approach for enhancing multimodal reasoning.

</details>


### [21] [Knowing or Guessing? Robust Medical Visual Question Answering via Joint Consistency and Contrastive Learning](https://arxiv.org/abs/2508.18687)
*Songtao Jiang,Yuxi Chen,Sibo Song,Yan Zhang,Yeying Jin,Yang Feng,Jian Wu,Zuozhu Liu*

Main category: cs.CL

TL;DR: 医学视觉语言模型在医学视觉问答中存在答案一致性问题，对语义相同但表述不同的医学问题给出不一致的回答。研究构建了RoMed数据集并提出了CCL方法来解决这一问题。


<details>
  <summary>Details</summary>
Motivation: 当前医学视觉语言模型在面对语义相同但表述不同的医学问题时，答案存在显著波动，这种脆弱性在医疗诊断等高风险应用中不可接受，需要提高模型的回答一致性。

Method: 构建了包含14.4万个问题的RoMed数据集，涵盖词汇级、句子级和语义级扰动。提出了CCL方法，包括知识锚定一致性学习和偏差感知对比学习两个关键组件。

Result: 在RoMed数据集上评估发现SOTA模型性能显著下降（如召回率下降40%）。CCL方法在三个流行VQA基准测试中达到SOTA性能，并在RoMed测试集上将答案一致性提高了50%。

Conclusion: 医学视觉语言模型存在严重的答案一致性问题，CCL方法通过知识对齐和偏差缓解显著提升了模型的鲁棒性和回答一致性，为医疗AI应用提供了更可靠的解决方案。

Abstract: In high-stakes medical applications, consistent answering across diverse
question phrasings is essential for reliable diagnosis. However, we reveal that
current Medical Vision-Language Models (Med-VLMs) exhibit concerning fragility
in Medical Visual Question Answering, as their answers fluctuate significantly
when faced with semantically equivalent rephrasings of medical questions. We
attribute this to two limitations: (1) insufficient alignment of medical
concepts, leading to divergent reasoning patterns, and (2) hidden biases in
training data that prioritize syntactic shortcuts over semantic understanding.
To address these challenges, we construct RoMed, a dataset built upon original
VQA datasets containing 144k questions with variations spanning word-level,
sentence-level, and semantic-level perturbations. When evaluating
state-of-the-art (SOTA) models like LLaVA-Med on RoMed, we observe alarming
performance drops (e.g., a 40\% decline in Recall) compared to original VQA
benchmarks, exposing critical robustness gaps. To bridge this gap, we propose
Consistency and Contrastive Learning (CCL), which integrates two key
components: (1) knowledge-anchored consistency learning, aligning Med-VLMs with
medical knowledge rather than shallow feature patterns, and (2) bias-aware
contrastive learning, mitigating data-specific priors through discriminative
representation refinement. CCL achieves SOTA performance on three popular VQA
benchmarks and notably improves answer consistency by 50\% on the challenging
RoMed test set, demonstrating significantly enhanced robustness. Code will be
released.

</details>


### [22] [Attention2Probability: Attention-Driven Terminology Probability Estimation for Robust Speech-to-Text System](https://arxiv.org/abs/2508.18701)
*Yanfan Du,Jun Zhang,Bin Wang,Jin Qiu,Lu Huang,Yuan Ge,Xiaoqian Liu,Tong Xiao,Jingbo Zhu*

Main category: cs.CL

TL;DR: 提出Attention2Probability方法，通过注意力权重转换和课程学习提升语音大模型中专业术语识别的准确率，在中文和英文测试中分别达到92.57%和86.83%的召回率，延迟仅8.71ms。


<details>
  <summary>Details</summary>
Motivation: 当前语音大模型在通用领域表现良好，但在处理领域特定术语和新词时准确率不足，需要专门的方法来提升术语识别能力。

Method: 提出Attention2Probability方法，将语音与术语间的交叉注意力权重转换为存在概率，并采用课程学习提高检索准确性。同时创建并发布了包含术语的语音数据集。

Result: 在测试集上显著优于VectorDB方法，中文最大召回率92.57%，英文86.83%，查询延迟仅8.71ms。通过术语干预使SLM的术语准确率提升6-17%。

Conclusion: Attention2Probability是一种轻量级、灵活且准确的术语概率估计方法，有效提升了语音大模型的术语识别能力，但当前SLM对术语的利用仍存在局限性。

Abstract: Recent advances in speech large language models (SLMs) have improved speech
recognition and translation in general domains, but accurately generating
domain-specific terms or neologisms remains challenging. To address this, we
propose Attention2Probability: attention-driven terminology probability
estimation for robust speech-to-text system, which is lightweight, flexible,
and accurate. Attention2Probability converts cross-attention weights between
speech and terminology into presence probabilities, and it further employs
curriculum learning to enhance retrieval accuracy. Furthermore, to tackle the
lack of data for speech-to-text tasks with terminology intervention, we create
and release a new speech dataset with terminology to support future research in
this area. Experimental results show that Attention2Probability significantly
outperforms the VectorDB method on our test set. Specifically, its maximum
recall rates reach 92.57% for Chinese and 86.83% for English. This high recall
is achieved with a latency of only 8.71ms per query. Intervening in SLMs'
recognition and translation tasks using Attention2Probability-retrieved terms
improves terminology accuracy by 6-17%, while revealing that the current
utilization of terminology by SLMs has limitations.

</details>


### [23] [Filtering for Creativity: Adaptive Prompting for Multilingual Riddle Generation in LLMs](https://arxiv.org/abs/2508.18709)
*Duy Le,Kent Ziti,Evan Girard-Sun,Sean O'Brien,Vasu Sharma,Kevin Zhu*

Main category: cs.CL

TL;DR: 提出了自适应原创性过滤(AOF)提示框架，通过余弦相似度拒绝和词汇新颖性约束，提升多语言谜语生成的创造性和文化适应性，在GPT-4o上实现了更好的词汇多样性和更低冗余度。


<details>
  <summary>Details</summary>
Motivation: 标准提示策略(零样本、少样本、思维链)在多语言谜语生成中倾向于重复记忆的谜语或进行浅层改写，难以平衡文化流畅性和创造性抽象。

Method: 引入自适应原创性过滤(AOF)框架，使用基于余弦相似度的拒绝过滤冗余生成，同时强制词汇新颖性和跨语言保真度。

Result: 在三个LLM和四个语言对上的评估显示，AOF增强的GPT-4o在日语中实现了0.177 Self-BLEU和0.915 Distinct-2，表明相比其他提示方法和语言对，词汇多样性提高且冗余减少。

Conclusion: 语义拒绝可以在不需要任务特定微调的情况下指导文化基础扎实的创造性生成。

Abstract: Multilingual riddle generation challenges large language models (LLMs) to
balance cultural fluency with creative abstraction. Standard prompting
strategies -- zero-shot, few-shot, chain-of-thought -- tend to reuse memorized
riddles or perform shallow paraphrasing. We introduce Adaptive Originality
Filtering (AOF), a prompting framework that filters redundant generations using
cosine-based similarity rejection, while enforcing lexical novelty and
cross-lingual fidelity. Evaluated across three LLMs and four language pairs,
AOF-enhanced GPT-4o achieves \texttt{0.177} Self-BLEU and \texttt{0.915}
Distinct-2 in Japanese, signaling improved lexical diversity and reduced
redundancy compared to other prompting methods and language pairs. Our findings
show that semantic rejection can guide culturally grounded, creative generation
without task-specific fine-tuning.

</details>


### [24] [EMMM, Explain Me My Model! Explainable Machine Generated Text Detection in Dialogues](https://arxiv.org/abs/2508.18715)
*Angela Yifei Yuan,Haoyi Li,Soyeon Caren Han,Christopher Leckie*

Main category: cs.CL

TL;DR: 提出了EMMM框架，通过先解释后检测的方法在客户服务场景中检测机器生成文本，平衡了延迟、准确性和非专家用户的可解释性需求


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在客户服务中的快速应用带来了新的风险，恶意行为者可能利用它们进行大规模用户冒充。现有检测方法在在线对话环境中表现不佳，缺乏可靠性和可解释性，而客户服务场景中的操作员通常是非专家用户，需要可解释的检测方法

Method: EMMM框架采用先解释后检测的方法，首先生成对文本的解释，然后基于解释进行检测，旨在平衡延迟、准确性和非专家用户的可解释性

Result: 实验结果表明，EMMM为非专家用户提供了可访问的解释，70%的人类评估者偏好其输出，同时实现了与最先进模型相竞争的准确性，并在1秒内生成输出，保持低延迟

Conclusion: EMMM框架在客户服务场景中有效解决了机器生成文本检测的可解释性问题，为非专家用户提供了可信赖的AI部署方案，代码和数据集已开源

Abstract: The rapid adoption of large language models (LLMs) in customer service
introduces new risks, as malicious actors can exploit them to conduct
large-scale user impersonation through machine-generated text (MGT). Current
MGT detection methods often struggle in online conversational settings,
reducing the reliability and interpretability essential for trustworthy AI
deployment. In customer service scenarios where operators are typically
non-expert users, explanation become crucial for trustworthy MGT detection. In
this paper, we propose EMMM, an explanation-then-detection framework that
balances latency, accuracy, and non-expert-oriented interpretability.
Experimental results demonstrate that EMMM provides explanations accessible to
non-expert users, with 70\% of human evaluators preferring its outputs, while
achieving competitive accuracy compared to state-of-the-art models and
maintaining low latency, generating outputs within 1 second. Our code and
dataset are open-sourced at
https://github.com/AngieYYF/EMMM-explainable-chatbot-detection.

</details>


### [25] [Beyond Quality: Unlocking Diversity in Ad Headline Generation with Large Language Models](https://arxiv.org/abs/2508.18739)
*Chang Wang,Siyu Yan,Depeng Yuan,Yuqi Chen,Yanhua Huang,Yuanhang Zheng,Shuhao Li,Yinqi Zhang,Kedi Chen,Mingrui Zhu,Ruiwen Xu*

Main category: cs.CL

TL;DR: DIVER是一个基于大语言模型的广告标题生成框架，通过多阶段多目标优化同时提升标题质量和多样性，在工业数据集上验证了有效性并实现了ADVV和CTR的显著提升。


<details>
  <summary>Details</summary>
Motivation: 现代广告中标题生成需要兼顾质量和多样性，但现有方法主要优化质量或点击率，忽视了多样性需求，导致输出同质化严重。

Method: 设计了语义和风格感知的数据生成管道自动产生高质量训练对，提出多阶段多目标优化框架，结合监督微调和强化学习，实现单次前向传播生成高质量多样化标题。

Result: 在真实工业数据集上验证了DIVER能有效平衡质量和多样性，部署在亿级用户内容分享平台上，广告主价值(ADVV)提升4.0%，点击率(CTR)提升1.4%。

Conclusion: DIVER框架成功解决了广告标题生成中质量和多样性的平衡问题，为大规模广告平台提供了有效的解决方案。

Abstract: The generation of ad headlines plays a vital role in modern advertising,
where both quality and diversity are essential to engage a broad range of
audience segments. Current approaches primarily optimize language models for
headline quality or click-through rates (CTR), often overlooking the need for
diversity and resulting in homogeneous outputs. To address this limitation, we
propose DIVER, a novel framework based on large language models (LLMs) that are
jointly optimized for both diversity and quality. We first design a semantic-
and stylistic-aware data generation pipeline that automatically produces
high-quality training pairs with ad content and multiple diverse headlines. To
achieve the goal of generating high-quality and diversified ad headlines within
a single forward pass, we propose a multi-stage multi-objective optimization
framework with supervised fine-tuning (SFT) and reinforcement learning (RL).
Experiments on real-world industrial datasets demonstrate that DIVER
effectively balances quality and diversity. Deployed on a large-scale
content-sharing platform serving hundreds of millions of users, our framework
improves advertiser value (ADVV) and CTR by 4.0% and 1.4%.

</details>


### [26] [M3HG: Multimodal, Multi-scale, and Multi-type Node Heterogeneous Graph for Emotion Cause Triplet Extraction in Conversations](https://arxiv.org/abs/2508.18740)
*Qiao Liang,Ying Shen,Tiantian Chen,Lin Zhang*

Main category: cs.CL

TL;DR: 提出了M3HG模型和MECAD数据集，用于多模态对话中的情感原因三元组提取，通过多模态异构图显式建模情感和因果上下文，在跨话语和话语内层面融合语义信息


<details>
  <summary>Details</summary>
Motivation: 现有MECTEC任务面临数据集稀缺且场景单一的问题，同时现有方法未能显式建模情感和因果上下文，且忽略了不同层次语义信息的融合，导致性能下降

Method: 提出M3HG模型，使用多模态异构图显式捕捉情感和因果上下文，在跨话语和话语内两个层面有效融合上下文信息

Result: 大量实验证明M3HG相比现有最先进方法具有更好的效果，并发布了包含989个对话的MECAD数据集

Conclusion: M3HG模型通过显式建模情感因果上下文和多层次信息融合，有效提升了多模态对话中情感原因三元组提取的性能，为解决该领域的数据稀缺和方法局限提供了有效方案

Abstract: Emotion Cause Triplet Extraction in Multimodal Conversations (MECTEC) has
recently gained significant attention in social media analysis, aiming to
extract emotion utterances, cause utterances, and emotion categories
simultaneously. However, the scarcity of related datasets, with only one
published dataset featuring highly uniform dialogue scenarios, hinders model
development in this field. To address this, we introduce MECAD, the first
multimodal, multi-scenario MECTEC dataset, comprising 989 conversations from 56
TV series spanning a wide range of dialogue contexts. In addition, existing
MECTEC methods fail to explicitly model emotional and causal contexts and
neglect the fusion of semantic information at different levels, leading to
performance degradation. In this paper, we propose M3HG, a novel model that
explicitly captures emotional and causal contexts and effectively fuses
contextual information at both inter- and intra-utterance levels via a
multimodal heterogeneous graph. Extensive experiments demonstrate the
effectiveness of M3HG compared with existing state-of-the-art methods. The
codes and dataset are available at https://github.com/redifinition/M3HG.

</details>


### [27] [Chronological Passage Assembling in RAG framework for Temporal Question Answering](https://arxiv.org/abs/2508.18748)
*Byeongjeong Kim,Jeonghyun Park,Joonho Yang,Hwanhee Lee*

Main category: cs.CL

TL;DR: ChronoRAG是一个专门针对叙事文本的新型RAG框架，通过重构连贯段落和保持时间顺序来提升长上下文问答性能，在NarrativeQA数据集上取得了显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法在处理叙事文本时效果有限，因为叙事理解需要更广泛的上下文和段落间的时序关系，而不仅仅是孤立片段。

Method: 提出ChronoRAG框架，专注于两个关键方面：将分散的文档信息重构为连贯的结构化段落，以及通过显式捕获和维护检索段落间的时间顺序来保持叙事流。

Result: 在NarrativeQA数据集上的实验表明，ChronoRAG在需要事实识别和复杂时序关系理解的任务中取得了实质性改进。

Conclusion: 时序推理在解决叙事问答中至关重要，ChronoRAG通过专门针对叙事文本特性的设计有效提升了长上下文问答性能。

Abstract: Long-context question answering over narrative tasks is challenging because
correct answers often hinge on reconstructing a coherent timeline of events
while preserving contextual flow in a limited context window.
Retrieval-augmented generation (RAG) indexing methods aim to address this
challenge by selectively retrieving only necessary document segments. However,
narrative texts possess unique characteristics that limit the effectiveness of
these existing approaches. Specifically, understanding narrative texts requires
more than isolated segments, as the broader context and sequential
relationships between segments are crucial for comprehension. To address these
limitations, we propose ChronoRAG, a novel RAG framework specialized for
narrative texts. This approach focuses on two essential aspects: refining
dispersed document information into coherent and structured passages, and
preserving narrative flow by explicitly capturing and maintaining the temporal
order among retrieved passages. We empirically demonstrate the effectiveness of
ChronoRAG through experiments on the NarrativeQA dataset, showing substantial
improvements in tasks requiring both factual identification and comprehension
of complex sequential relationships, underscoring that reasoning over temporal
order is crucial in resolving narrative QA.

</details>


### [28] [ThinkDial: An Open Recipe for Controlling Reasoning Effort in Large Language Models](https://arxiv.org/abs/2508.18773)
*Qianyu He,Siyu Yuan,Xuefeng Li,Mingxuan Wang,Jiangjie Chen*

Main category: cs.CL

TL;DR: ThinkDial是首个开源端到端框架，通过离散操作模式实现类似GPT-OSS的可控推理，提供高中低三种推理模式，在显著减少计算token的同时保持性能阈值。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然具备强大的思维链推理能力，但控制其计算成本仍是实际部署的重大挑战。开源社区缺乏类似GPT-OSS系列的可控推理能力实现方案。

Method: 采用端到端训练范式：预算模式监督微调将可控推理能力嵌入学习过程，以及两阶段预算感知强化学习配合自适应奖励塑造。

Result: 实现了目标压缩-性能权衡，高模式（完整能力）、中模式（50% token减少，性能下降<10%）、低模式（75% token减少，性能下降<15%）。在分布外任务上表现出强泛化能力。

Conclusion: ThinkDial成功实现了开源可控推理框架，为实际部署提供了有效的计算成本控制解决方案，填补了开源社区在此领域的空白。

Abstract: Large language models (LLMs) with chain-of-thought reasoning have
demonstrated remarkable problem-solving capabilities, but controlling their
computational effort remains a significant challenge for practical deployment.
Recent proprietary systems like OpenAI's gpt-oss series have introduced
discrete operational modes for intuitive reasoning control, but the open-source
community has largely failed to achieve such capabilities. In this paper, we
introduce ThinkDial, the first open-recipe end-to-end framework that
successfully implements gpt-oss-style controllable reasoning through discrete
operational modes. Our system enables seamless switching between three distinct
reasoning regimes: High mode (full reasoning capability), Medium mode (50
percent token reduction with <10 percent performance degradation), and Low mode
(75 percent token reduction with <15 percent performance degradation). We
achieve this through an end-to-end training paradigm that integrates
budget-mode control throughout the entire pipeline: budget-mode supervised
fine-tuning that embeds controllable reasoning capabilities directly into the
learning process, and two-phase budget-aware reinforcement learning with
adaptive reward shaping. Extensive experiments demonstrate that ThinkDial
achieves target compression-performance trade-offs with clear response length
reductions while maintaining performance thresholds. The framework also
exhibits strong generalization capabilities on out-of-distribution tasks.

</details>


### [29] [Harnessing Rule-Based Reinforcement Learning for Enhanced Grammatical Error Correction](https://arxiv.org/abs/2508.18780)
*Yilin Li,Xunjian Yin,Yilin Chen,Xiaojun Wan*

Main category: cs.CL

TL;DR: 提出基于规则强化学习的新框架，在中文语法纠错任务上实现最先进性能，显著提升召回率


<details>
  <summary>Details</summary>
Motivation: 传统编码器-解码器模型在语法纠错任务中已有一定成功，但大语言模型在该领域的应用仍未被充分探索。当前研究主要依赖监督微调直接生成修正句子，限制了大语言模型的强大推理能力

Method: 提出基于规则强化学习（Rule-Based RL）的新框架，通过强化学习引导大语言模型进行语法纠错

Result: 在中文数据集上的实验表明，该框架实现了最先进的性能，召回率显著提升

Conclusion: 该研究凸显了使用强化学习引导大语言模型的优势，为语法纠错领域的未来发展提供了更可控和可靠的范式

Abstract: Grammatical error correction is a significant task in NLP. Traditional
methods based on encoder-decoder models have achieved certain success, but the
application of LLMs in this field is still underexplored. Current research
predominantly relies on supervised fine-tuning to train LLMs to directly
generate the corrected sentence, which limits the model's powerful reasoning
ability. To address this limitation, we propose a novel framework based on
Rule-Based RL. Through experiments on the Chinese datasets, our Rule-Based RL
framework achieves \textbf{state-of-the-art }performance, with a notable
increase in \textbf{recall}. This result clearly highlights the advantages of
using RL to steer LLMs, offering a more controllable and reliable paradigm for
future development in GEC.

</details>


### [30] [Controllable Conversational Theme Detection Track at DSTC 12](https://arxiv.org/abs/2508.18783)
*Igor Shalyminov,Hang Su,Jake Vincent,Siffi Singh,Jason Cai,James Gung,Raphael Shu,Saab Mansour*

Main category: cs.CL

TL;DR: 本文介绍了对话分析中的主题检测任务，作为DSTC 12的竞赛赛道，旨在通过可控聚类方法自动识别对话主题，减少人工分析工作量。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在分析领域的快速应用，需要解决更复杂的自动化问题。传统对话意图检测依赖固定意图集，而主题检测提供更灵活的用户面向对话摘要。

Method: 提出可控对话主题检测问题，采用联合聚类和主题标注方法，通过用户偏好数据控制主题簇的粒度。

Result: 在DSTC 12竞赛中设立了该赛道，提供了相关数据集和评估指标，收集了参赛团队的解决方案并进行分析。

Conclusion: 主题检测是对话分析中的关键任务，通过可控聚类方法能够有效识别对话核心主题，相关材料和代码已在GitHub开源。

Abstract: Conversational analytics has been on the forefront of transformation driven
by the advances in Speech and Natural Language Processing techniques. Rapid
adoption of Large Language Models (LLMs) in the analytics field has taken the
problems that can be automated to a new level of complexity and scale. In this
paper, we introduce Theme Detection as a critical task in conversational
analytics, aimed at automatically identifying and categorizing topics within
conversations. This process can significantly reduce the manual effort involved
in analyzing expansive dialogs, particularly in domains like customer support
or sales. Unlike traditional dialog intent detection, which often relies on a
fixed set of intents for downstream system logic, themes are intended as a
direct, user-facing summary of the conversation's core inquiry. This
distinction allows for greater flexibility in theme surface forms and
user-specific customizations. We pose Controllable Conversational Theme
Detection problem as a public competition track at Dialog System Technology
Challenge (DSTC) 12 -- it is framed as joint clustering and theme labeling of
dialog utterances, with the distinctive aspect being controllability of the
resulting theme clusters' granularity achieved via the provided user preference
data. We give an overview of the problem, the associated dataset and the
evaluation metrics, both automatic and human. Finally, we discuss the
participant teams' submissions and provide insights from those. The track
materials (data and code) are openly available in the GitHub repository.

</details>


### [31] [LaTeXTrans: Structured LaTeX Translation with Multi-Agent Coordination](https://arxiv.org/abs/2508.18791)
*Ziming Zhu,Chenglong Wang,Shunjie Xing,Yifu Huo,Fengning Tian,Quan Du,Di Yang,Chunliang Zhang,Tong Xiao,Jingbo Zhu*

Main category: cs.CL

TL;DR: LaTeXTrans是一个多智能体系统，专门解决LaTeX格式文档的翻译难题，通过六个专门化代理确保格式保持和术语一致性，在翻译准确性和结构保真度方面优于主流机器翻译系统。


<details>
  <summary>Details</summary>
Motivation: 现代机器翻译系统在通用文本上取得了显著进展，但在翻译结构化LaTeX格式文档时仍面临重大挑战。这些文档通常混合自然语言和特定领域语法（如数学公式、表格、图表和交叉引用），需要准确保持语义完整性和可编译性。

Method: LaTeXTrans采用协作式多智能体系统，包含六个专门化代理：1）解析器通过占位符替换和语法过滤将LaTeX分解为翻译友好单元；2）翻译器、验证器、总结器和术语提取器协作确保上下文感知、自我纠正和术语一致的翻译；3）生成器将翻译内容重构为结构良好的LaTeX文档。

Result: 实验结果表明，LaTeXTrans在翻译准确性和结构保真度方面都能超越主流机器翻译系统。

Conclusion: LaTeXTrans为翻译LaTeX格式文档提供了一个有效且实用的解决方案，能够很好地处理格式保持、结构保真和术语一致性问题。

Abstract: Despite the remarkable progress of modern machine translation (MT) systems on
general-domain texts, translating structured LaTeX-formatted documents remains
a significant challenge. These documents typically interleave natural language
with domain-specific syntax, such as mathematical equations, tables, figures,
and cross-references, all of which must be accurately preserved to maintain
semantic integrity and compilability. In this paper, we introduce LaTeXTrans, a
collaborative multi-agent system designed to address this challenge. LaTeXTrans
ensures format preservation, structural fidelity, and terminology consistency
through six specialized agents: 1) a Parser that decomposes LaTeX into
translation-friendly units via placeholder substitution and syntax filtering;
2) a Translator, Validator, Summarizer, and Terminology Extractor that work
collaboratively to ensure context-aware, self-correcting, and
terminology-consistent translations; 3) a Generator that reconstructs the
translated content into well-structured LaTeX documents. Experimental results
demonstrate that LaTeXTrans can outperform mainstream MT systems in both
translation accuracy and structural fidelity, offering an effective and
practical solution for translating LaTeX-formatted documents.

</details>


### [32] [LLM-based Contrastive Self-Supervised AMR Learning with Masked Graph Autoencoders for Fake News Detection](https://arxiv.org/abs/2508.18819)
*Shubham Gupta,Shraban Kumar Chatterjee,Suman Kundu*

Main category: cs.CL

TL;DR: 提出了一种结合语义关系和传播动态的自监督虚假信息检测框架，使用AMR和LLM增强特征分离，在零样本设置下优于现有方法


<details>
  <summary>Details</summary>
Motivation: 数字时代虚假信息泛滥，现有方法难以捕捉长距离依赖、复杂语义关系和社交动态，且需要大量标注数据

Method: 使用抽象意义表示(AMR)捕捉复杂语义关系，LLM生成负锚点的图对比损失增强特征分离，多视图图掩码自编码器学习社交传播特征

Result: 在有限标注数据集下实现了优于最先进方法的性能，提高了泛化能力

Conclusion: 该自监督框架有效结合语义和传播特征，在虚假信息检测方面表现出色，减少了对标注数据的依赖

Abstract: The proliferation of misinformation in the digital age has led to significant
societal challenges. Existing approaches often struggle with capturing
long-range dependencies, complex semantic relations, and the social dynamics
influencing news dissemination. Furthermore, these methods require extensive
labelled datasets, making their deployment resource-intensive. In this study,
we propose a novel self-supervised misinformation detection framework that
integrates both complex semantic relations using Abstract Meaning
Representation (AMR) and news propagation dynamics. We introduce an LLM-based
graph contrastive loss (LGCL) that utilizes negative anchor points generated by
a Large Language Model (LLM) to enhance feature separability in a zero-shot
manner. To incorporate social context, we employ a multi view graph masked
autoencoder, which learns news propagation features from social context graph.
By combining these semantic and propagation-based features, our approach
effectively differentiates between fake and real news in a self-supervised
manner. Extensive experiments demonstrate that our self-supervised framework
achieves superior performance compared to other state-of-the-art methodologies,
even with limited labelled datasets while improving generalizability.

</details>


### [33] [Arrows of Math Reasoning Data Synthesis for Large Language Models: Diversity, Complexity and Correctness](https://arxiv.org/abs/2508.18824)
*Sirui Chen,Changxin Tian,Binbin Hu,Kunlong Chen,Ziqi Liu,Zhiqiang Zhang,Jun Zhou*

Main category: cs.CL

TL;DR: 提出了一种程序辅助合成框架，通过数学知识系统和领域特定工具生成可执行程序，然后转换为自然语言问题-解决方案对，并通过双边验证机制确保数据质量，生成了1230万个高质量数学推理训练样本。


<details>
  <summary>Details</summary>
Motivation: 传统方法在可扩展性、成本和数据可靠性方面面临关键挑战，需要高质量的训练数据来增强大型语言模型的数学推理能力。

Method: 使用程序辅助合成框架，集成数学知识系统和领域特定工具创建可执行程序，然后转换为自然语言问题-解决方案对，并通过双边验证机制验证解决方案正确性和程序-问题一致性。

Result: 生成了1230万个问题解决三元组，实验表明基于该数据微调的模型在多个基准数据集上实现了最先进的性能，推理能力显著提升。

Conclusion: 该合成方法有效解决了数学推理训练数据的高质量生成问题，证明了程序辅助框架在提升语言模型数学能力方面的有效性。

Abstract: Enhancing the mathematical reasoning of large language models (LLMs) demands
high-quality training data, yet conventional methods face critical challenges
in scalability, cost, and data reliability. To address these limitations, we
propose a novel program-assisted synthesis framework that systematically
generates a high-quality mathematical corpus with guaranteed diversity,
complexity, and correctness. This framework integrates mathematical knowledge
systems and domain-specific tools to create executable programs. These programs
are then translated into natural language problem-solution pairs and vetted by
a bilateral validation mechanism that verifies solution correctness against
program outputs and ensures program-problem consistency. We have generated 12.3
million such problem-solving triples. Experiments demonstrate that models
fine-tuned on our data significantly improve their inference capabilities,
achieving state-of-the-art performance on several benchmark datasets and
showcasing the effectiveness of our synthesis approach.

</details>


### [34] [ConfTuner: Training Large Language Models to Express Their Confidence Verbally](https://arxiv.org/abs/2508.18847)
*Yibo Li,Miao Xiong,Jiaying Wu,Bryan Hooi*

Main category: cs.CL

TL;DR: ConfTuner是一种通过tokenized Brier score损失函数微调LLM的方法，有效解决LLM过度自信问题，提升置信度校准效果


<details>
  <summary>Details</summary>
Motivation: LLM在科学、法律、医疗等高风险领域部署时，经常表现出过度自信现象（错误答案高置信度），现有方法效果有限且泛化性差

Method: 提出ConfTuner微调方法，使用tokenized Brier score损失函数（理论证明为proper scoring rule），无需真实置信度标签或代理估计

Result: 在多种推理任务上显著改善校准效果，泛化到GPT-4o等黑盒模型，下游应用如自我修正和模型级联获得提升

Conclusion: ConfTuner通过理论保证的损失函数有效校准LLM置信度，推动可信LLM系统发展

Abstract: Large Language Models (LLMs) are increasingly deployed in high-stakes domains
such as science, law, and healthcare, where accurate expressions of uncertainty
are essential for reliability and trust. However, current LLMs are often
observed to generate incorrect answers with high confidence, a phenomenon known
as "overconfidence". Recent efforts have focused on calibrating LLMs'
verbalized confidence: i.e., their expressions of confidence in text form, such
as "I am 80% confident that...". Existing approaches either rely on prompt
engineering or fine-tuning with heuristically generated uncertainty estimates,
both of which have limited effectiveness and generalizability. Motivated by the
notion of proper scoring rules for calibration in classical machine learning
models, we introduce ConfTuner, a simple and efficient fine-tuning method that
introduces minimal overhead and does not require ground-truth confidence scores
or proxy confidence estimates. ConfTuner relies on a new loss function,
tokenized Brier score, which we theoretically prove to be a proper scoring
rule, intuitively meaning that it "correctly incentivizes the model to report
its true probability of being correct". ConfTuner improves calibration across
diverse reasoning tasks and generalizes to black-box models such as GPT-4o. Our
results further show that better-calibrated confidence enables downstream gains
in self-correction and model cascade, advancing the development of trustworthy
LLM systems. The code is available at
https://github.com/liushiliushi/ConfTuner.

</details>


### [35] [ReflectivePrompt: Reflective evolution in autoprompting algorithms](https://arxiv.org/abs/2508.18870)
*Viktor N. Zhuravlev,Artur R. Khairullin,Ernest A. Dyagin,Alena N. Sitkina,Nikita I. Kulin*

Main category: cs.CL

TL;DR: ReflectivePrompt是一种基于进化算法的自动提示优化方法，通过短期和长期反思操作来提升提示质量，在33个数据集上相比现有方法平均提升28%性能


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型和提示工程的快速发展，自动选择优化提示的需求日益增长，需要更精确和全面的搜索方法来找到最优提示

Method: 基于进化算法的自反进化方法，在交叉和精英突变前使用短期和长期反思操作来积累进化过程中的知识，并在每个epoch基于当前种群更新知识

Result: 在33个分类和文本生成数据集上测试，使用t-lite-instruct-0.1和gemma3-27b-it模型，相比最先进方法平均显著提升28%（在BBH数据集上）

Conclusion: ReflectivePrompt成为进化算法基自动提示中最有效的解决方案之一，通过反思机制显著提升了提示优化的效果

Abstract: Autoprompting is the process of automatically selecting optimized prompts for
language models, which has been gaining popularity with the rapid advancement
of prompt engineering, driven by extensive research in the field of large
language models (LLMs). This paper presents ReflectivePrompt - a novel
autoprompting method based on evolutionary algorithms that employs a reflective
evolution approach for more precise and comprehensive search of optimal
prompts. ReflectivePrompt utilizes short-term and long-term reflection
operations before crossover and elitist mutation to enhance the quality of the
modifications they introduce. This method allows for the accumulation of
knowledge obtained throughout the evolution process and updates it at each
epoch based on the current population. ReflectivePrompt was tested on 33
datasets for classification and text generation tasks using open-access large
language models: t-lite-instruct-0.1 and gemma3-27b-it. The method
demonstrates, on average, a significant improvement (e.g., 28% on BBH compared
to EvoPrompt) in metrics relative to current state-of-the-art approaches,
thereby establishing itself as one of the most effective solutions in
evolutionary algorithm-based autoprompting.

</details>


### [36] [Empowering Computing Education Researchers Through LLM-Assisted Content Analysis](https://arxiv.org/abs/2508.18872)
*Laurie Gale,Sebastian Mateos Nicolajsen*

Main category: cs.CL

TL;DR: 本文提出了一种LLM辅助内容分析(LACA)方法，用于处理计算教育研究中的大量文本数据，帮助研究者进行更严谨、可重复的大规模研究。


<details>
  <summary>Details</summary>
Motivation: 计算教育研究(CER)领域的研究者往往缺乏资源进行大规模严谨研究，需要一种能够处理大量定性数据而不增加研究负担的方法。

Method: 提出LLM辅助内容分析(LACA)方法，结合内容分析和大型语言模型，使研究者能够进行原本无法完成的大规模文本数据分析。

Result: 通过计算教育数据集展示了LACA方法的应用，证明该方法可以在保持严谨性和可重复性的同时处理大量文本数据。

Conclusion: LACA方法在CER领域具有重要潜力，能够帮助产生更具普适性的研究发现，推动计算教育研究实践和研究质量的提升。

Abstract: Computing education research (CER) is often instigated by practitioners
wanting to improve both their own and the wider discipline's teaching practice.
However, the latter is often difficult as many researchers lack the colleagues,
resources, or capacity to conduct research that is generalisable or rigorous
enough to advance the discipline. As a result, research methods that enable
sense-making with larger volumes of qualitative data, while not increasing the
burden on the researcher, have significant potential within CER.
  In this discussion paper, we propose such a method for conducting rigorous
analysis on large volumes of textual data, namely a variation of LLM-assisted
content analysis (LACA). This method combines content analysis with the use of
large language models, empowering researchers to conduct larger-scale research
which they would otherwise not be able to perform. Using a computing education
dataset, we illustrate how LACA could be applied in a reproducible and rigorous
manner. We believe this method has potential in CER, enabling more
generalisable findings from a wider range of research. This, together with the
development of similar methods, can help to advance both the practice and
research quality of the CER discipline.

</details>


### [37] [Affective Polarization across European Parliaments](https://arxiv.org/abs/2508.18916)
*Bojan Evkoski,Igor Mozetič,Nikola Ljubešić,Petra Kralj Novak*

Main category: cs.CL

TL;DR: 使用自然语言处理技术自动分析欧洲六国议会演讲，发现普遍存在情感极化现象，即对反对派表现出更多负面情绪，且互惠机制是情感极化的重要驱动因素


<details>
  <summary>Details</summary>
Motivation: 研究情感极化（对反对群体的负面情绪和敌意）是否存在于欧洲议会政治话语中，以及其表现模式和机制

Method: 收集六国欧洲议会的演讲语料库，运用自然语言处理技术估计议员情感，通过比较对反对派和己方个体的负面情绪水平来分析情感极化模式

Result: 在所有六个欧洲议会中都发现了一致的情感极化现象；活动量与负面情绪相关，但活跃度不同的议员在情感极化方面没有差异；互惠机制是跨议会情感极化的重要驱动因素

Conclusion: 情感极化是欧洲议会政治话语的普遍特征，互惠行为加剧了这种极化现象，需要关注这种负面互动模式对民主审议质量的影响

Abstract: Affective polarization, characterized by increased negativity and hostility
towards opposing groups, has become a prominent feature of political discourse
worldwide. Our study examines the presence of this type of polarization in a
selection of European parliaments in a fully automated manner. Utilizing a
comprehensive corpus of parliamentary speeches from the parliaments of six
European countries, we employ natural language processing techniques to
estimate parliamentarian sentiment. By comparing the levels of negativity
conveyed in references to individuals from opposing groups versus one's own, we
discover patterns of affectively polarized interactions. The findings
demonstrate the existence of consistent affective polarization across all six
European parliaments. Although activity correlates with negativity, there is no
observed difference in affective polarization between less active and more
active members of parliament. Finally, we show that reciprocity is a
contributing mechanism in affective polarization between parliamentarians
across all six parliaments.

</details>


### [38] [Diverse And Private Synthetic Datasets Generation for RAG evaluation: A multi-agent framework](https://arxiv.org/abs/2508.18929)
*Ilias Driouich,Hongliu Cao,Eoin Thomas*

Main category: cs.CL

TL;DR: 这篇论文提出了一种新的多段代理框架，用于生成具有语义多样性和隐私保护的合成QA数据集，以改善检索增强生成系统的评估。


<details>
  <summary>Details</summary>
Motivation: 当前检索增强生成系统的评估主要集中在性能指标上，而忽视了评估数据集的质量设计和隐私保护这些实际约束。

Method: 使用多段代理框架：(1)多样性代理利用聚类技术最大化主题覆盖和语义变异性；(2)隐私代理检测和隐磨多领域的敏感信息；(3)QA维护代理合成具有隐私保护和多样性的QA对。

Result: 实验结果显示，该方法生成的评估集在多样性方面超过基线方法，并在领域特定数据集上实现了稳健的隐私隐磨效果。

Conclusion: 该工作提供了一条实用且符合道德规范的路径，实现更安全、更全面的RAG系统评估，为未来人工智能相关规定和遵循标准的发展奠定基础。

Abstract: Retrieval-augmented generation (RAG) systems improve large language model
outputs by incorporating external knowledge, enabling more informed and
context-aware responses. However, the effectiveness and trustworthiness of
these systems critically depends on how they are evaluated, particularly on
whether the evaluation process captures real-world constraints like protecting
sensitive information. While current evaluation efforts for RAG systems have
primarily focused on the development of performance metrics, far less attention
has been given to the design and quality of the underlying evaluation datasets,
despite their pivotal role in enabling meaningful, reliable assessments. In
this work, we introduce a novel multi-agent framework for generating synthetic
QA datasets for RAG evaluation that prioritize semantic diversity and privacy
preservation. Our approach involves: (1) a Diversity agent leveraging
clustering techniques to maximize topical coverage and semantic variability,
(2) a Privacy Agent that detects and mask sensitive information across multiple
domains and (3) a QA curation agent that synthesizes private and diverse QA
pairs suitable as ground truth for RAG evaluation. Extensive experiments
demonstrate that our evaluation sets outperform baseline methods in diversity
and achieve robust privacy masking on domain-specific datasets. This work
offers a practical and ethically aligned pathway toward safer, more
comprehensive RAG system evaluation, laying the foundation for future
enhancements aligned with evolving AI regulations and compliance standards.

</details>


### [39] [Interpretable by AI Mother Tongue: Native Symbolic Reasoning in Neural Models](https://arxiv.org/abs/2508.18988)
*Hung Ming Liu*

Main category: cs.CL

TL;DR: 提出了AI母语框架，让神经网络模型发展原生符号语言，同时支持直觉推理、组合符号链和内在可解释性


<details>
  <summary>Details</summary>
Motivation: 解决传统后解释方法的局限性，将推理直接嵌入模型表示中，实现透明且灵活的可解释推理

Method: 引入互补训练目标增强符号纯度和决策稀疏性，采用顺序专业化策略先建立广泛符号能力再细化直觉判断

Result: 在AI任务上展示了有竞争力的准确性同时提供可验证的推理轨迹

Conclusion: AI母语可以作为神经网络模型中可解释性、直觉和符号推理的统一机制

Abstract: We present a framework where neural models develop an AI Mother Tongue, a
native symbolic language that simultaneously supports intuitive reasoning,
compositional symbol chains, and inherent interpretability. Unlike post-hoc
explanation methods, our approach embeds reasoning directly into the model's
representations: symbols capture meaningful semantic patterns, chains trace
decision paths, and gated induction mechanisms guide selective focus, yielding
transparent yet flexible reasoning. We introduce complementary training
objectives to enhance symbol purity and decision sparsity, and employ a
sequential specialization strategy to first build broad symbolic competence and
then refine intuitive judgments. Experiments on AI tasks demonstrate
competitive accuracy alongside verifiable reasoning traces, showing that AI
Mother Tongue can serve as a unified mechanism for interpretability, intuition,
and symbolic reasoning in neural models.

</details>


### [40] [Automatic Prompt Optimization with Prompt Distillation](https://arxiv.org/abs/2508.18992)
*Viktor N. Zhuravlev,Artur R. Khairullin,Ernest A. Dyagin,Alena N. Sitkina,Nikita I. Kulin*

Main category: cs.CL

TL;DR: DistillPrompt是一种基于大语言模型的新型自动提示方法，通过多阶段集成任务特定信息到提示中，在文本分类和生成任务上相比现有方法有显著提升


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型研究的快速发展，提示工程日益重要，自动选择优化提示的需求不断增长

Method: 采用蒸馏、压缩和聚合操作的多阶段集成方法，将训练数据中的任务特定信息融入提示，更彻底地探索提示空间

Result: 在文本分类和生成任务的不同数据集上测试，使用t-lite-instruct-0.1模型，相比Grips等方法在关键指标上平均提升20.12%

Conclusion: DistillPrompt被确立为自动提示领域中最有效的非梯度方法之一

Abstract: Autoprompting is the process of automatically selecting optimized prompts for
language models, which is gaining popularity due to the rapid development of
prompt engineering driven by extensive research in the field of large language
models (LLMs). This paper presents DistillPrompt -- a novel autoprompting
method based on large language models that employs a multi-stage integration of
task-specific information into prompts using training data. DistillPrompt
utilizes distillation, compression, and aggregation operations to explore the
prompt space more thoroughly. The method was tested on different datasets for
text classification and generation tasks using the t-lite-instruct-0.1 language
model. The results demonstrate a significant average improvement (e.g., 20.12%
across the entire dataset compared to Grips) in key metrics over existing
methods in the field, establishing DistillPrompt as one of the most effective
non-gradient approaches in autoprompting.

</details>


### [41] [MovieCORE: COgnitive REasoning in Movies](https://arxiv.org/abs/2508.19026)
*Gueter Josmy Faure,Min-Hung Chen,Jia-Fong Yeh,Ying Cheng,Hung-Ting Su,Yung-Hao Tang,Shang-Hong Lai,Winston H. Hsu*

Main category: cs.CL

TL;DR: MovieCORE是一个新颖的视频问答数据集，专注于电影内容的深层认知理解，通过多智能体头脑风暴方法生成高质量问题，并提出ACE模块提升模型推理能力25%。


<details>
  <summary>Details</summary>
Motivation: 现有视频问答数据集主要关注表面层次理解，缺乏对电影内容深层认知理解的探索，需要开发能够激发系统2思维的高质量数据集。

Method: 采用多大型语言模型作为思维智能体进行头脑风暴，生成和精炼高质量问答对；开发认知测试评估数据集质量；提出Agentic Choice Enhancement (ACE)模块增强视频语言模型的推理能力。

Result: 成功创建了MovieCORE数据集，包含深度认知问题；ACE模块将模型推理能力提升达25%；为评估VQA模型在深层认知任务上的表现提供了全面方案。

Conclusion: 该研究推动了AI系统对电影理解的发展，揭示了当前VQA模型在处理具有挑战性的电影内容问题时的能力与局限，为未来研究提供了重要基础。

Abstract: This paper introduces MovieCORE, a novel video question answering (VQA)
dataset designed to probe deeper cognitive understanding of movie content.
Unlike existing datasets that focus on surface-level comprehension, MovieCORE
emphasizes questions that engage System-2 thinking while remaining specific to
the video material. We present an innovative agentic brainstorming approach,
utilizing multiple large language models (LLMs) as thought agents to generate
and refine high-quality question-answer pairs. To evaluate dataset quality, we
develop a set of cognitive tests assessing depth, thought-provocation
potential, and syntactic complexity. We also propose a comprehensive evaluation
scheme for assessing VQA model performance on deeper cognitive tasks. To
address the limitations of existing video-language models (VLMs), we introduce
an agentic enhancement module, Agentic Choice Enhancement (ACE), which improves
model reasoning capabilities post-training by up to 25%. Our work contributes
to advancing movie understanding in AI systems and provides valuable insights
into the capabilities and limitations of current VQA models when faced with
more challenging, nuanced questions about cinematic content. Our project page,
dataset and code can be found at
https://joslefaure.github.io/assets/html/moviecore.html.

</details>


### [42] [HiPlan: Hierarchical Planning for LLM-Based Agents with Adaptive Global-Local Guidance](https://arxiv.org/abs/2508.19076)
*Ziyue Li,Yuan Chang,Gaihong Yu,Xiaoqiu Le*

Main category: cs.CL

TL;DR: HiPlan是一个分层规划框架，通过全局里程碑指导和局部步骤提示来增强LLM智能体的复杂任务决策能力


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体在复杂长时程规划任务中缺乏宏观指导，容易迷失方向，且执行过程中缺乏持续监督，难以应对环境变化和偏差

Method: 采用分层规划方法：离线阶段构建里程碑库，执行阶段动态适配轨迹片段生成步骤提示，结合全局里程碑指导和局部详细行动提示

Result: 在两个挑战性基准测试中显著优于强基线，消融研究验证了分层组件的互补效益

Conclusion: HiPlan通过分层规划框架有效解决了LLM智能体在复杂任务中的规划问题，提供了自适应的全局-局部指导

Abstract: Large language model (LLM)-based agents have demonstrated remarkable
capabilities in decision-making tasks, but struggle significantly with complex,
long-horizon planning scenarios. This arises from their lack of macroscopic
guidance, causing disorientation and failures in complex tasks, as well as
insufficient continuous oversight during execution, rendering them unresponsive
to environmental changes and prone to deviations. To tackle these challenges,
we introduce HiPlan, a hierarchical planning framework that provides adaptive
global-local guidance to boost LLM-based agents'decision-making. HiPlan
decomposes complex tasks into milestone action guides for general direction and
step-wise hints for detailed actions. During the offline phase, we construct a
milestone library from expert demonstrations, enabling structured experience
reuse by retrieving semantically similar tasks and milestones. In the execution
phase, trajectory segments from past milestones are dynamically adapted to
generate step-wise hints that align current observations with the milestone
objectives, bridging gaps and correcting deviations. Extensive experiments
across two challenging benchmarks demonstrate that HiPlan substantially
outperforms strong baselines, and ablation studies validate the complementary
benefits of its hierarchical components.

</details>


### [43] ["Where does it hurt?" -- Dataset and Study on Physician Intent Trajectories in Doctor Patient Dialogues](https://arxiv.org/abs/2508.19077)
*Tom Röhr,Soumyadeep Roy,Fares Al Mohamad,Jens-Michalis Papaioannou,Wolfgang Nejdl,Felix Gers,Alexander Löser*

Main category: cs.CL

TL;DR: 这是首个研究医生意图轨迹的工作，通过大规模标注医生情境对话数据，建立了基于SOAP框架的细粒度意图分类系统，并评测了现有模型在医疗意图识别任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 研究医生在医情对话中的意图轨迹，通过目标问询高效收集信息以提供最佳诊断和治疗方案，这在医疗人工智能领域展开了新的研究方向。

Method: 使用ACI-bench数据集，与医疗专业人员合作开发基于SOAP框架的细粒度意图分类法，通过Prolific群众编码平台经过大规模标注加5000多个医生情境对话转换，并用于评测现有生成式和编码器模型。

Result: 模型能够高精度理解医疗对话的一般结构，但在识别SOAP类别过渡时常常失败，同时首次报告了医疗对话结构中的常见轨迹，对设计辨证诊断系统具有重要价值。

Conclusion: 意图过滤对医疗对话摘要有显著性能提升，研究成果为医疗人工智能领域提供了重要的数据资源和洞察，代码和数据已公开。

Abstract: In a doctor-patient dialogue, the primary objective of physicians is to
diagnose patients and propose a treatment plan. Medical doctors guide these
conversations through targeted questioning to efficiently gather the
information required to provide the best possible outcomes for patients. To the
best of our knowledge, this is the first work that studies physician intent
trajectories in doctor-patient dialogues. We use the `Ambient Clinical
Intelligence Benchmark' (Aci-bench) dataset for our study. We collaborate with
medical professionals to develop a fine-grained taxonomy of physician intents
based on the SOAP framework (Subjective, Objective, Assessment, and Plan). We
then conduct a large-scale annotation effort to label over 5000 doctor-patient
turns with the help of a large number of medical experts recruited using
Prolific, a popular crowd-sourcing platform. This large labeled dataset is an
important resource contribution that we use for benchmarking the
state-of-the-art generative and encoder models for medical intent
classification tasks. Our findings show that our models understand the general
structure of medical dialogues with high accuracy, but often fail to identify
transitions between SOAP categories. We also report for the first time common
trajectories in medical dialogue structures that provide valuable insights for
designing `differential diagnosis' systems. Finally, we extensively study the
impact of intent filtering for medical dialogue summarization and observe a
significant boost in performance. We make the codes and data, including
annotation guidelines, publicly available at
https://github.com/DATEXIS/medical-intent-classification.

</details>


### [44] [It's All About In-Context Learning! Teaching Extremely Low-Resource Languages to LLMs](https://arxiv.org/abs/2508.19089)
*Yue Li,Zhixue Zhao,Carolina Scarton*

Main category: cs.CL

TL;DR: 这篇论文分析了大语言模型通过上下文学习在极低资源语言上的表现，发现零检验ICL加语言对齐在极稀有语言上效果最佳，而PEFT在语言和字符难以表示时效果局限。


<details>
  <summary>Details</summary>
Motivation: 极低资源语言（特别是使用稀有字符的语言）在大语言模型中得不到充分支持，需要研究如何通过上下文学习和参数高效微调来提升模型在这些语言上的性能。

Method: 系统性评估3个最新多语言LLM在20种低代表性语言上的表现，比较了上下文学习（有无辅助对齐信号）和参数高效微调的效果。

Result: 当语言和其字符在LLM中极度缺乏表示时，PEFT效果局限；零检验ICL加语言对齐在极低资源语言上效果突出；少检验ICL或PEFT在相对更好表示的语言上效果更好。

Conclusion: 对于极低资源语言，应避免对未见字符的多语言模型进行微调，而应采用零检验ICL加语言对齐的方法。

Abstract: Extremely low-resource languages, especially those written in rare scripts,
as shown in Figure 1, remain largely unsupported by large language models
(LLMs). This is due in part to compounding factors such as the lack of training
data. This paper delivers the first comprehensive analysis of whether LLMs can
acquire such languages purely via in-context learning (ICL), with or without
auxiliary alignment signals, and how these methods compare to
parameter-efficient fine-tuning (PEFT). We systematically evaluate 20
under-represented languages across three state-of-the-art multilingual LLMs.
Our findings highlight the limitation of PEFT when both language and its script
are extremely under-represented by the LLM. In contrast, zero-shot ICL with
language alignment is impressively effective on extremely low-resource
languages, while few-shot ICL or PEFT is more beneficial for languages
relatively better represented by LLMs. For LLM practitioners working on
extremely low-resource languages, we summarise guidelines grounded by our
results on adapting LLMs to low-resource languages, e.g., avoiding fine-tuning
a multilingual model on languages of unseen scripts.

</details>


### [45] [Retrieval-Augmented Generation for Natural Language Art Provenance Searches in the Getty Provenance Index](https://arxiv.org/abs/2508.19093)
*Mathew Henrickson*

Main category: cs.CL

TL;DR: 本研究提出了一个基于检索增强生成(RAG)的艺术品来源研究框架，专门针对盖蒂来源索引。该方法通过语义检索和上下文摘要实现自然语言和多语言搜索，解决了艺术品来源研究中碎片化多语言档案数据检索效率低的问题。


<details>
  <summary>Details</summary>
Motivation: 艺术品来源研究对于验证真伪、支持归还和法律主张、理解文化历史背景至关重要，但当前搜索门户需要精确元数据，限制了探索性搜索，且多语言碎片化档案数据阻碍了高效检索。

Method: 采用检索增强生成(RAG)框架，结合语义检索和上下文摘要技术，实现对盖蒂来源索引-德国拍卖记录的检索和摘要，减少对元数据结构的依赖。

Result: 在10,000条记录样本上的评估显示，该方法能够有效检索和总结拍卖记录，为艺术市场档案导航提供了可扩展的解决方案。

Conclusion: 该RAG框架为历史学家和文化遗产专业人士进行历史敏感性研究提供了实用工具，能够有效处理艺术品来源研究中的多语言碎片化数据挑战。

Abstract: This research presents a Retrieval-Augmented Generation (RAG) framework for
art provenance studies, focusing on the Getty Provenance Index. Provenance
research establishes the ownership history of artworks, which is essential for
verifying authenticity, supporting restitution and legal claims, and
understanding the cultural and historical context of art objects. The process
is complicated by fragmented, multilingual archival data that hinders efficient
retrieval. Current search portals require precise metadata, limiting
exploratory searches. Our method enables natural-language and multilingual
searches through semantic retrieval and contextual summarization, reducing
dependence on metadata structures. We assess RAG's capability to retrieve and
summarize auction records using a 10,000-record sample from the Getty
Provenance Index - German Sales. The results show this approach provides a
scalable solution for navigating art market archives, offering a practical tool
for historians and cultural heritage professionals conducting historically
sensitive research.

</details>


### [46] [Beyond the Black Box: Integrating Lexical and Semantic Methods in Quantitative Discourse Analysis with BERTopic](https://arxiv.org/abs/2508.19099)
*Thomas Compton*

Main category: cs.CL

TL;DR: 本文提出了一种混合式的谨诚定量话语分析框架，结合词汇和语义方法，通过Python工具链实现高度透明的话语研究。


<details>
  <summary>Details</summary>
Motivation: 当前定量话语分析很多依赖黑盒软件（如MAXQDA和NVivo），影响方法论透明度和与研究目标的对齐。需要一种更透明、可控的方法论框架。

Method: 使用Python工具链（NLTK、spaCy、Sentence Transformers）进行预处理、词形恢复和嵌入生成；采用迭代BERTopic模型过程，包括UMAP降维、HDBSCAN聚类和c-TF-IDF关键词提取，通过参数调优提升主题相关性和覆盖范围。

Result: 通过历史政治话语案例研究，证明了该框架能够实现三角测量、可复现性和可解释性。精确的词汇搜索与语境感知的语义聚类相结合，免除了单纯方法的局限性。

Conclusion: 该研究强调代码层面的透明性、研究人员的主动性和方法论三角测量在计算话语研究中的重要性，为定量话语分析提供了一种更可控、可复现的方法论框架。

Abstract: Quantitative Discourse Analysis has seen growing adoption with the rise of
Large Language Models and computational tools. However, reliance on black box
software such as MAXQDA and NVivo risks undermining methodological transparency
and alignment with research goals. This paper presents a hybrid, transparent
framework for QDA that combines lexical and semantic methods to enable
triangulation, reproducibility, and interpretability. Drawing from a case study
in historical political discourse, we demonstrate how custom Python pipelines
using NLTK, spaCy, and Sentence Transformers allow fine-grained control over
preprocessing, lemmatisation, and embedding generation. We further detail our
iterative BERTopic modelling process, incorporating UMAP dimensionality
reduction, HDBSCAN clustering, and c-TF-IDF keyword extraction, optimised
through parameter tuning and multiple runs to enhance topic coherence and
coverage. By juxtaposing precise lexical searches with context-aware semantic
clustering, we argue for a multi-layered approach that mitigates the
limitations of either method in isolation. Our workflow underscores the
importance of code-level transparency, researcher agency, and methodological
triangulation in computational discourse studies. Code and supplementary
materials are available via GitHub.

</details>


### [47] [Do LVLMs Know What They Know? A Systematic Study of Knowledge Boundary Perception in LVLMs](https://arxiv.org/abs/2508.19111)
*Zhikai Ding,Shiyu Ni,Keping Bi*

Main category: cs.CL

TL;DR: 这篇论文研究大型视觉-语言模型的知识边界感知能力，通过评估三种信心信号发现概率性和一致性信心更可靠，并提出了改善方法。


<details>
  <summary>Details</summary>
Motivation: 大型视觉-语言模型虽然具有强大的VQA能力，但存在幻觉问题，需要研究它们是否能够感知自身的知识边界。

Method: 通过在三个LVLM模型和三个VQA数据集上评估三种信心信号：概率性信心、答案一致性信心和语言化信心，并适配了几种信心检验方法。

Result: 实验表明LVLMs具有一定的知识边界感知能力，但仍有显著提升空间。概率性和一致性信心更可靠，而语言化信心容易导致过度自信。

Conclusion: 视觉和文本输入的聚合处理虽然降低了VQA性能，但减少了信心度，从而提高了知识边界感知水平，为改善LVLMs的可靠性提供了方向。

Abstract: Large vision-language models (LVLMs) demonstrate strong visual question
answering (VQA) capabilities but are shown to hallucinate. A reliable model
should perceive its knowledge boundaries-knowing what it knows and what it does
not. This paper investigates LVLMs' perception of their knowledge boundaries by
evaluating three types of confidence signals: probabilistic confidence, answer
consistency-based confidence, and verbalized confidence. Experiments on three
LVLMs across three VQA datasets show that, although LVLMs possess a reasonable
perception level, there is substantial room for improvement. Among the three
confidences, probabilistic and consistency-based signals are more reliable
indicators, while verbalized confidence often leads to overconfidence. To
enhance LVLMs' perception, we adapt several established confidence calibration
methods from Large Language Models (LLMs) and propose three effective methods.
Additionally, we compare LVLMs with their LLM counterparts, finding that
jointly processing visual and textual inputs decreases question-answering
performance but reduces confidence, resulting in an improved perception level
compared to LLMs.

</details>


### [48] [Demystifying Scientific Problem-Solving in LLMs by Probing Knowledge and Reasoning](https://arxiv.org/abs/2508.19202)
*Alan Li,Yixin Liu,Arpan Sarkar,Doug Downey,Arman Cohan*

Main category: cs.CL

TL;DR: SciReas和SciReas-Pro科学推理评测套件，结合KRUX探测框架分析LLMs在科学推理中知识与推理的作用，发现知识检索是瓶颈，外部知识和推理增强能提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏全面的科学推理评测基准，且很少系统性地分析知识与推理在科学任务中的不同作用，需要开发新的评测框架来深入理解LLMs的科学推理能力。

Method: 引入SciReas评测套件和SciReas-Pro子集，提出KRUX探测框架分析知识与推理的分离作用，通过实验研究知识检索、外部知识增强和推理改进对LLMs性能的影响。

Result: 发现知识检索是LLMs科学推理的关键瓶颈；外部知识能进一步提升推理增强模型的性能；改进推理能力有助于模型提取任务相关知识。

Conclusion: 该研究提供了全面的科学推理评测框架和深入的分析洞察，揭示了知识与推理在科学问题解决中的相互作用，并发布了强基线模型SciLit01。

Abstract: Scientific problem solving poses unique challenges for LLMs, requiring both
deep domain knowledge and the ability to apply such knowledge through complex
reasoning. While automated scientific reasoners hold great promise for
assisting human scientists, there is currently no widely adopted holistic
benchmark for evaluating scientific reasoning, and few approaches
systematically disentangle the distinct roles of knowledge and reasoning in
these tasks. To address these gaps, we introduce SciReas, a diverse suite of
existing benchmarks for scientific reasoning tasks, and SciReas-Pro, a
selective subset that requires more complex reasoning. Our holistic evaluation
surfaces insights about scientific reasoning performance that remain hidden
when relying on individual benchmarks alone. We then propose KRUX, a probing
framework for studying the distinct roles of reasoning and knowledge in
scientific tasks. Combining the two, we conduct an in-depth analysis that
yields several key findings: (1) Retrieving task-relevant knowledge from model
parameters is a critical bottleneck for LLMs in scientific reasoning; (2)
Reasoning models consistently benefit from external knowledge added in-context
on top of the reasoning enhancement; (3) Enhancing verbalized reasoning
improves LLMs' ability to surface task-relevant knowledge. Finally, we conduct
a lightweight analysis, comparing our science-focused data composition with
concurrent efforts on long CoT SFT, and release SciLit01, a strong 8B baseline
for scientific reasoning.

</details>


### [49] [VibeVoice Technical Report](https://arxiv.org/abs/2508.19205)
*Zhiliang Peng,Jianwei Yu,Wenhui Wang,Yaoyao Chang,Yutao Sun,Li Dong,Yi Zhu,Weijiang Xu,Hangbo Bao,Zehua Wang,Shaohan Huang,Yan Xia,Furu Wei*

Main category: cs.CL

TL;DR: VibeVoice是一个使用next-token扩散技术合成多说话人长语音的新模型，通过新型连续语音分词器实现80倍数据压缩，能在64K上下文窗口中合成长达90分钟的4人对话语音。


<details>
  <summary>Details</summary>
Motivation: 为了解决长语音合成中多说话人对话的真实性和计算效率问题，需要开发能够处理长序列并保持音频保真度的高效模型。

Method: 采用next-token扩散方法自回归生成潜在向量，并引入新型连续语音分词器，相比Encodec模型实现80倍数据压缩，同时保持可比性能。

Result: 模型能够在64K上下文窗口长度下合成长达90分钟的语音，最多支持4个说话人，能够捕捉真实的对话氛围，超越开源和专有对话模型。

Conclusion: VibeVoice通过创新的分词器和扩散方法，成功实现了高效的长语音多说话人合成，在计算效率和音频质量方面都取得了显著提升。

Abstract: This report presents VibeVoice, a novel model designed to synthesize
long-form speech with multiple speakers by employing next-token diffusion,
which is a unified method for modeling continuous data by autoregressively
generating latent vectors via diffusion. To enable this, we introduce a novel
continuous speech tokenizer that, when compared to the popular Encodec model,
improves data compression by 80 times while maintaining comparable performance.
The tokenizer effectively preserves audio fidelity while significantly boosting
computational efficiency for processing long sequences. Thus, VibeVoice can
synthesize long-form speech for up to 90 minutes (in a 64K context window
length) with a maximum of 4 speakers, capturing the authentic conversational
``vibe'' and surpassing open-source and proprietary dialogue models.

</details>


### [50] [Evaluating the Evaluators: Are readability metrics good measures of readability?](https://arxiv.org/abs/2508.19221)
*Isabel Cachola,Daniel Khashabi,Mark Dredze*

Main category: cs.CL

TL;DR: 本文调查平函语言摘要的可读性评估方法，发现传统可读性指标与人类判断相关性弱，而语言模型在可读性评估上表现更优，特别是在能够捐描更深层次的可读性素质方面。


<details>
  <summary>Details</summary>
Motivation: 平函语言摘要(PLS)为非专业读者提供易懂摘要，但当前的标准实践使用的传统可读性指标(如FKGL)在PLS领域未经过与人类判断的对比验证。研究想要评估这些指标的有效性并探索更好的可读性评估方法。

Method: 研究评估了8种可读性指标与人类可读性判断的相关性，包括最流行的Flesch-Kincaid Grade Level(FKGL)。然后测试语言模型(LMs)作为可读性判断器的性能，并将分析扩展到PLS数据集上。

Result: 结果显示大多数传统可读性指标与人类判断相关性弱，包括最流行的FKGL。语言模型在可读性评估上表现更好，最佳模型与人类判断的Pearson相关系数达到0.56。LMs能更好地捐描更深层次的可读性测量(如所需背景知识)，并得出与传统指标不同的结论。

Conclusion: 语言模型比传统可读性指标更适合用于平函语言摘要的可读性评估，特别是在捐描深层次可读性素质方面。研究为PLS评估提供了最佳实践建议，并开源了分析代码和调查数据。

Abstract: Plain Language Summarization (PLS) aims to distill complex documents into
accessible summaries for non-expert audiences. In this paper, we conduct a
thorough survey of PLS literature, and identify that the current standard
practice for readability evaluation is to use traditional readability metrics,
such as Flesch-Kincaid Grade Level (FKGL). However, despite proven utility in
other fields, these metrics have not been compared to human readability
judgments in PLS. We evaluate 8 readability metrics and show that most
correlate poorly with human judgments, including the most popular metric, FKGL.
We then show that Language Models (LMs) are better judges of readability, with
the best-performing model achieving a Pearson correlation of 0.56 with human
judgments. Extending our analysis to PLS datasets, which contain summaries
aimed at non-expert audiences, we find that LMs better capture deeper measures
of readability, such as required background knowledge, and lead to different
conclusions than the traditional metrics. Based on these findings, we offer
recommendations for best practices in the evaluation of plain language
summaries. We release our analysis code and survey data.

</details>


### [51] [Generative Interfaces for Language Models](https://arxiv.org/abs/2508.19227)
*Jiaqi Chen,Yanzhe Zhang,Yutong Zhang,Yijia Shao,Diyi Yang*

Main category: cs.CL

TL;DR: LLM通过生成用户界面而非传统对话响应，在多轮、信息密集和探索性任务中显著提升交互效率和用户体验


<details>
  <summary>Details</summary>
Motivation: 传统线性请求-响应格式的LLM交互方式在多轮、信息密集和探索性任务中效率低下，需要更自适应的交互范式

Method: 提出生成式语言模型界面框架，利用结构化界面特定表示和迭代优化，将用户查询转换为任务特定的用户界面

Result: 生成式界面在70%以上的情况下被人类用户偏好，在功能、交互和情感体验方面均优于传统对话界面

Conclusion: 生成式界面为人类-AI交互提供了新的发展方向，明确了用户偏好此类界面的时机和原因

Abstract: Large language models (LLMs) are increasingly seen as assistants, copilots,
and consultants, capable of supporting a wide range of tasks through natural
conversation. However, most systems remain constrained by a linear
request-response format that often makes interactions inefficient in
multi-turn, information-dense, and exploratory tasks. To address these
limitations, we propose Generative Interfaces for Language Models, a paradigm
in which LLMs respond to user queries by proactively generating user interfaces
(UIs) that enable more adaptive and interactive engagement. Our framework
leverages structured interface-specific representations and iterative
refinements to translate user queries into task-specific UIs. For systematic
evaluation, we introduce a multidimensional assessment framework that compares
generative interfaces with traditional chat-based ones across diverse tasks,
interaction patterns, and query types, capturing functional, interactive, and
emotional aspects of user experience. Results show that generative interfaces
consistently outperform conversational ones, with humans preferring them in
over 70% of cases. These findings clarify when and why users favor generative
interfaces, paving the way for future advancements in human-AI interaction.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [52] [Toward Responsible ASR for African American English Speakers: A Scoping Review of Bias and Equity in Speech Technology](https://arxiv.org/abs/2508.18288)
*Jay L. Cunningham,Adinawa Adjagbodjou,Jeffrey Basoah,Jainaba Jawara,Kowe Kadoma,Aaleyah Lewis*

Main category: eess.AS

TL;DR: 本文通过文献综述分析了ASR系统中对非裔美国英语等语言多样性群体的公平性、偏见和公平概念化及操作化现状，发现技术公平干预在增长但缺乏以治理为中心的方法，提出了治理为中心的ASR生命周期框架。


<details>
  <summary>Details</summary>
Motivation: 研究动机是审查自动语音识别(ASR)和相关语音语言技术中如何概念化和操作化公平性、偏见和公平问题，特别关注非裔美国英语(AAE)使用者和其他语言多样性群体，以解决语言边缘化问题。

Method: 采用范围界定文献综述方法，分析了44篇来自人机交互、机器学习/自然语言处理和社会语言学领域的同行评审出版物，识别了四个主要研究领域。

Result: 研究发现技术公平干预措施正在增长，但存在关键差距：缺乏以治理为中心的方法，这些方法应强调社区代理、语言正义和参与式问责。研究提出了治理为中心的ASR生命周期作为跨学科框架。

Conclusion: 结论是需要采用治理为中心的方法来开发负责任的ASR系统，为研究人员、从业者和政策制定者提供了解决语音AI系统中语言边缘化问题的启示和建议。

Abstract: This scoping literature review examines how fairness, bias, and equity are
conceptualized and operationalized in Automatic Speech Recognition (ASR) and
adjacent speech and language technologies (SLT) for African American English
(AAE) speakers and other linguistically diverse communities. Drawing from 44
peer-reviewed publications across Human-Computer Interaction (HCI), Machine
Learning/Natural Language Processing (ML/NLP), and Sociolinguistics, we
identify four major areas of inquiry: (1) how researchers understand
ASR-related harms; (2) inclusive data practices spanning collection, curation,
annotation, and model training; (3) methodological and theoretical approaches
to linguistic inclusion; and (4) emerging practices and design recommendations
for more equitable systems. While technical fairness interventions are growing,
our review highlights a critical gap in governance-centered approaches that
foreground community agency, linguistic justice, and participatory
accountability. We propose a governance-centered ASR lifecycle as an emergent
interdisciplinary framework for responsible ASR development and offer
implications for researchers, practitioners, and policymakers seeking to
address language marginalization in speech AI systems.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [53] [H-PRM: A Pluggable Hotword Pre-Retrieval Module for Various Speech Recognition Systems](https://arxiv.org/abs/2508.18295)
*Huangyu Dai,Lingtao Mao,Ben Chen,Zihan Wang,Zihan Liang,Ying Han,Chenyi Lei,Han Li*

Main category: cs.SD

TL;DR: 这篇论文提出了一种新的热词定制系统，通过热词预检索模块(H-PRM)来提高ASR中大规模热词识别的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有模型在处理大规模热词时识别率会显著下降，需要一种更有效的热词定制方案来提高领域特定术语的识别准确性。

Method: 设计了热词预检索模块(H-PRM)，通过测量热词与语音段落的音响相似性来识别最相关的热词候选项。该方案可以插入式集成到传统模型和音频大语言模型中。

Result: 经过大量测试验证，H-PRM能够显著提高热词后召回率(PRR)，在大规模热词情况下表现超过现有方法。

Conclusion: 该研究为ASR中的热词定制开启了新方向，通过音响相似性检索的方法有效解决了大规模热词识别率下降的问题。

Abstract: Hotword customization is crucial in ASR to enhance the accuracy of
domain-specific terms. It has been primarily driven by the advancements in
traditional models and Audio large language models (LLMs). However, existing
models often struggle with large-scale hotwords, as the recognition rate drops
dramatically with the number of hotwords increasing. In this paper, we
introduce a novel hotword customization system that utilizes a hotword
pre-retrieval module (H-PRM) to identify the most relevant hotword candidate by
measuring the acoustic similarity between the hotwords and the speech segment.
This plug-and-play solution can be easily integrated into traditional models
such as SeACo-Paraformer, significantly enhancing hotwords post-recall rate
(PRR). Additionally, we incorporate H-PRM into Audio LLMs through a
prompt-based approach, enabling seamless customization of hotwords. Extensive
testing validates that H-PRM can outperform existing methods, showing a new
direction for hotword customization in ASR.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [54] [Training Language Model Agents to Find Vulnerabilities with CTF-Dojo](https://arxiv.org/abs/2508.18370)
*Terry Yue Zhuo,Dingmin Wang,Hantian Ding,Varun Kumar,Zijian Wang*

Main category: cs.SE

TL;DR: CTF-Dojo是一个大规模可执行运行时环境，包含658个CTF挑战，通过自动化管道CTF-Forge快速构建，仅用486条执行验证轨迹训练LLM代理就在多个基准测试中取得显著提升，32B模型达到31.9% Pass@1的新SOTA。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏可扩展和通用的可执行运行时环境，限制了训练更强大ML代理的进展，需要开发能够提供可验证反馈的执行环境。

Method: 开发CTF-Dojo大规模可执行运行时环境，包含658个容器化CTF挑战；构建CTF-Forge自动化管道，将公开资源快速转换为可执行环境；使用执行验证的轨迹训练LLM代理。

Result: 仅用486条高质量执行验证轨迹训练，就在三个竞争基准测试中取得最高11.6%的绝对提升；32B模型达到31.9% Pass@1，创造了新的开源权重SOTA，媲美前沿模型。

Conclusion: CTF-Dojo证明基于执行的训练信号对于推进高性能ML代理至关重要，无需依赖昂贵的专有系统，为可执行代理学习提供了有效的基准框架。

Abstract: Large language models (LLMs) have demonstrated exceptional capabilities when
trained within executable runtime environments, notably excelling at software
engineering tasks through verified feedback loops. Yet, scalable and
generalizable execution-grounded environments remain scarce, limiting progress
in training more capable ML agents. We introduce CTF-Dojo, the first
large-scale executable runtime tailored for training LLMs with verifiable
feedback, featuring 658 fully functional Capture-The-Flag (CTF)-style
challenges containerized in Docker with guaranteed reproducibility. To enable
rapid scaling without manual intervention, we develop CTF-Forge, an automated
pipeline that transforms publicly available artifacts into ready-to-use
execution environments in minutes, eliminating weeks of expert configuration
traditionally required. We trained LLM-based agents on just 486 high-quality,
execution-verified trajectories from CTF-Dojo, achieving up to 11.6% absolute
gains over strong baselines across three competitive benchmarks: InterCode-CTF,
NYU CTF Bench, and Cybench. Our best-performing 32B model reaches 31.9% Pass@1,
establishing a new open-weight state-of-the-art that rivals frontier models
like DeepSeek-V3-0324 and Gemini-2.5-Flash. By framing CTF-style tasks as a
benchmark for executable-agent learning, CTF-Dojo demonstrates that
execution-grounded training signals are not only effective but pivotal in
advancing high-performance ML agents without dependence on costly proprietary
systems.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [55] [Can VLMs Recall Factual Associations From Visual References?](https://arxiv.org/abs/2508.18297)
*Dhananjay Ashok,Ashutosh Chaubey,Hirona J. Arai,Jonathan May,Jesse Thomason*

Main category: cs.CV

TL;DR: 研究发现视觉语言模型在多模态基础方面存在系统性缺陷，当使用视觉而非文本参考时，模型回忆事实知识的能力显著下降，内部状态分析可识别不可靠响应


<details>
  <summary>Details</summary>
Motivation: 识别视觉语言模型在多模态基础中的系统性缺陷，特别是当实体参考从文本变为视觉时模型性能下降的问题

Method: 通过控制研究分析VLMs的表现差异，使用内部状态模式探测来识别不可靠响应，并在视觉问答任务中进行选择性预测验证

Result: 视觉参考使VLMs回忆事实知识的能力减半，内部状态探测达到92%准确率识别不可靠响应，选择性预测覆盖提高7.87%同时降低错误风险0.9%

Conclusion: VLMs在将内部知识与图像表示链接方面存在困难，这种可检测的系统性缺陷是多模态基础中的重要问题，需要未来研究解决

Abstract: Through a controlled study, we identify a systematic deficiency in the
multimodal grounding of Vision Language Models (VLMs). While VLMs can recall
factual associations when provided a textual reference to an entity; their
ability to do so is significantly diminished when the reference is visual
instead. Forcing VLMs to rely on image representations of an entity halves
their ability to recall factual knowledge, suggesting that VLMs struggle to
link their internal knowledge of an entity with its image representation. We
show that such linking failures are correlated with the expression of distinct
patterns in model internal states, and that probes on these internal states
achieve over 92% accuracy at flagging cases where the VLM response is
unreliable. These probes can be applied, without retraining, to identify when a
VLM will fail to correctly answer a question that requires an understanding of
multimodal input. When used to facilitate selective prediction on a visual
question answering task, the probes increase coverage by 7.87% (absolute) while
also reducing the risk of error by 0.9% (absolute). Addressing the systematic,
detectable deficiency is an important avenue in language grounding, and we
provide informed recommendations for future directions.

</details>


### [56] [Beyond the Textual: Generating Coherent Visual Options for MCQs](https://arxiv.org/abs/2508.18772)
*Wanqiang Wang,Longzhu He,Wei Zheng*

Main category: cs.CV

TL;DR: 提出了CmOS框架，用于生成带有视觉选项的教育多选题，通过多模态思维链和检索增强生成技术解决传统文本选项的局限性


<details>
  <summary>Details</summary>
Motivation: 传统多选题生成主要关注文本选项，忽视了视觉选项的重要性，且高质量干扰项的生成成本高、扩展性差

Method: 结合多模态思维链推理和检索增强生成技术，生成语义合理且视觉相似的答案和干扰项，包含内容判别模块识别适合视觉选项的内容

Result: 实验结果表明CmOS在内容判别、问题生成和视觉选项生成方面优于现有方法，适用于不同学科和教育水平

Conclusion: CmOS框架成功解决了教育多选题中视觉选项生成的挑战，为多模态教育评估提供了有效解决方案

Abstract: Multiple-choice questions (MCQs) play a crucial role in fostering deep
thinking and knowledge integration in education. However, previous research has
primarily focused on generating MCQs with textual options, but it largely
overlooks the visual options. Moreover, generating high-quality distractors
remains a major challenge due to the high cost and limited scalability of
manual authoring. To tackle these problems, we propose a Cross-modal Options
Synthesis (CmOS), a novel framework for generating educational MCQs with visual
options. Our framework integrates Multimodal Chain-of-Thought (MCoT) reasoning
process and Retrieval-Augmented Generation (RAG) to produce semantically
plausible and visually similar answer and distractors. It also includes a
discrimination module to identify content suitable for visual options.
Experimental results on test tasks demonstrate the superiority of CmOS in
content discrimination, question generation and visual option generation over
existing methods across various subjects and educational levels.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [57] [SALMAN: Stability Analysis of Language Models Through the Maps Between Graph-based Manifolds](https://arxiv.org/abs/2508.18306)
*Wuxinlin Cheng,Yupeng Cao,Jinwen Wu,Koduvayur Subbalakshmi,Tian Han,Zhuo Feng*

Main category: cs.LG

TL;DR: 提出了SALMAN框架，通过距离映射失真(DMD)度量来评估Transformer语言模型在输入扰动下的鲁棒性，无需修改模型参数或复杂扰动启发式。


<details>
  <summary>Details</summary>
Motivation: 随着预训练Transformer语言模型规模增大和部署广泛，其在输入扰动下的鲁棒性成为紧迫问题。现有方法在小参数和大规模模型之间存在差异，且通常依赖劳动密集型的样本特定对抗设计。

Method: 提出统一的局部鲁棒性框架SALMAN，核心是新颖的距离映射失真(DMD)度量，通过比较输入到输出的距离映射来评估样本敏感性，具有近线性复杂度。

Result: 在攻击效率和鲁棒训练方面取得了显著提升，证明了框架的有效性。

Conclusion: SALMAN框架是一个实用的、模型无关的工具，可用于提升基于Transformer的NLP系统的可靠性。

Abstract: Recent strides in pretrained transformer-based language models have propelled
state-of-the-art performance in numerous NLP tasks. Yet, as these models grow
in size and deployment, their robustness under input perturbations becomes an
increasingly urgent question. Existing robustness methods often diverge between
small-parameter and large-scale models (LLMs), and they typically rely on
labor-intensive, sample-specific adversarial designs. In this paper, we propose
a unified, local (sample-level) robustness framework (SALMAN) that evaluates
model stability without modifying internal parameters or resorting to complex
perturbation heuristics. Central to our approach is a novel Distance Mapping
Distortion (DMD) measure, which ranks each sample's susceptibility by comparing
input-to-output distance mappings in a near-linear complexity manner. By
demonstrating significant gains in attack efficiency and robust training, we
position our framework as a practical, model-agnostic tool for advancing the
reliability of transformer-based NLP systems.

</details>


### [58] [Optimal Sparsity of Mixture-of-Experts Language Models for Reasoning Tasks](https://arxiv.org/abs/2508.18672)
*Taishi Nakamura,Satoki Ishikawa,Masaki Kawamura,Takumi Okamoto,Daisuke Nohara,Jun Suzuki,Rio Yokota*

Main category: cs.LG

TL;DR: 研究发现MoE模型的稀疏性对记忆能力和推理能力有不同影响：记忆性能随总参数增加而提升，但推理性能会饱和甚至下降，即使训练损失继续改善。


<details>
  <summary>Details</summary>
Motivation: 现有经验缩放定律主要针对密集模型，而MoE模型引入了新的稀疏维度，需要研究稀疏性如何影响不同能力机制（记忆vs推理）。

Method: 训练多个MoE Transformer家族，系统性地改变总参数、激活参数和top-k路由，保持计算预算固定，记录预训练损失、下游任务损失和准确率。

Result: 记忆基准测试随总参数单调改善，而推理性能会饱和甚至回归；改变top-k路由对性能影响很小；后训练强化学习和额外测试计算无法挽救过度稀疏模型的推理缺陷。

Conclusion: MoE模型的稀疏性需要在记忆和推理能力之间进行权衡，过度稀疏会损害推理能力，且无法通过后处理技术修复。

Abstract: Empirical scaling laws have driven the evolution of large language models
(LLMs), yet their coefficients shift whenever the model architecture or data
pipeline changes. Mixture-of-Experts (MoE) models, now standard in
state-of-the-art systems, introduce a new sparsity dimension that current
dense-model frontiers overlook. We investigate how MoE sparsity influences two
distinct capability regimes: memorization and reasoning. We train families of
MoE Transformers that systematically vary total parameters, active parameters,
and top-$k$ routing while holding the compute budget fixed. For every model we
record pre-training loss, downstream task loss, and task accuracy, allowing us
to separate the train-test generalization gap from the loss-accuracy gap.
Memorization benchmarks improve monotonically with total parameters, mirroring
training loss. By contrast, reasoning performance saturates and can even
regress despite continued gains in both total parameters and training loss.
Altering top-$k$ alone has little effect when active parameters are constant,
and classic hyperparameters such as learning rate and initialization modulate
the generalization gap in the same direction as sparsity. Neither post-training
reinforcement learning (GRPO) nor extra test-time compute rescues the reasoning
deficit of overly sparse models. Our model checkpoints, code and logs are
open-source at https://github.com/rioyokotalab/optimal-sparsity.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [59] [A Systematic Approach to Predict the Impact of Cybersecurity Vulnerabilities Using LLMs](https://arxiv.org/abs/2508.18439)
*Anders Mølmen Høst,Pierre Lison,Leon Moonen*

Main category: cs.CR

TL;DR: TRIAGE是一个使用大语言模型自动将CVE漏洞映射到ATT&CK技术的混合方法，结合基于规则推理和数据驱动推理，提高了漏洞影响分析的效率和召回率。


<details>
  <summary>Details</summary>
Motivation: 现有漏洞数据库缺乏漏洞实际影响信息，手动将CVE映射到ATT&CK技术耗时且困难，需要自动化支持来处理大量新漏洞。

Method: 采用双管齐下的混合方法：首先基于MITRE映射方法提示LLM预测技术列表，然后结合使用上下文学习的第二个LLM模块结果，将规则推理与数据驱动推理相结合。

Result: 上下文学习方法优于单独映射方法，混合方法提高了利用技术的召回率，GPT-4o-mini在此任务上表现优于Llama3.3-70B。

Conclusion: LLM可用于自动预测网络安全漏洞的影响，TRIAGE使CVE到ATT&CK的映射过程更加高效。

Abstract: Vulnerability databases, such as the National Vulnerability Database (NVD),
offer detailed descriptions of Common Vulnerabilities and Exposures (CVEs), but
often lack information on their real-world impact, such as the tactics,
techniques, and procedures (TTPs) that adversaries may use to exploit the
vulnerability. However, manually linking CVEs to their corresponding TTPs is a
challenging and time-consuming task, and the high volume of new vulnerabilities
published annually makes automated support desirable.
  This paper introduces TRIAGE, a two-pronged automated approach that uses
Large Language Models (LLMs) to map CVEs to relevant techniques from the ATT&CK
knowledge base. We first prompt an LLM with instructions based on MITRE's CVE
Mapping Methodology to predict an initial list of techniques. This list is then
combined with the results from a second LLM-based module that uses in-context
learning to map a CVE to relevant techniques. This hybrid approach
strategically combines rule-based reasoning with data-driven inference. Our
evaluation reveals that in-context learning outperforms the individual mapping
methods, and the hybrid approach improves recall of exploitation techniques. We
also find that GPT-4o-mini performs better than Llama3.3-70B on this task.
Overall, our results show that LLMs can be used to automatically predict the
impact of cybersecurity vulnerabilities and TRIAGE makes the process of mapping
CVEs to ATT&CK more efficient.
  Keywords: vulnerability impact, CVE, ATT&CK techniques, large language
models, automated mapping.

</details>


### [60] [UniC-RAG: Universal Knowledge Corruption Attacks to Retrieval-Augmented Generation](https://arxiv.org/abs/2508.18652)
*Runpeng Geng,Yanting Wang,Ying Chen,Jinyuan Jia*

Main category: cs.CR

TL;DR: UniC-RAG是一种针对检索增强生成(RAG)系统的通用知识污染攻击方法，能够通过注入少量对抗文本来同时攻击大量不同主题的用户查询，攻击成功率超过90%。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG系统容易受到知识污染攻击，但现有研究主要针对特定查询或相似主题的查询进行攻击，缺乏对多样化查询的通用攻击方法。

Method: 将UniC-RAG建模为优化问题，设计有效的解决方案，包括基于平衡相似性的聚类方法来增强攻击效果，通过联合优化少量对抗文本来同时攻击大量用户查询。

Result: UniC-RAG在包含数百万文本的知识库中注入100个对抗文本，即可对2000个用户查询实现超过90%的攻击成功率，显著优于基线方法。

Conclusion: UniC-RAG展示了RAG系统面临的新型安全威胁，现有防御措施不足以抵御此类攻击，需要开发新的防御机制来保护RAG系统安全。

Abstract: Retrieval-augmented generation (RAG) systems are widely deployed in
real-world applications in diverse domains such as finance, healthcare, and
cybersecurity. However, many studies showed that they are vulnerable to
knowledge corruption attacks, where an attacker can inject adversarial texts
into the knowledge database of a RAG system to induce the LLM to generate
attacker-desired outputs. Existing studies mainly focus on attacking specific
queries or queries with similar topics (or keywords). In this work, we propose
UniC-RAG, a universal knowledge corruption attack against RAG systems. Unlike
prior work, UniC-RAG jointly optimizes a small number of adversarial texts that
can simultaneously attack a large number of user queries with diverse topics
and domains, enabling an attacker to achieve various malicious objectives, such
as directing users to malicious websites, triggering harmful command execution,
or launching denial-of-service attacks. We formulate UniC-RAG as an
optimization problem and further design an effective solution to solve it,
including a balanced similarity-based clustering method to enhance the attack's
effectiveness. Our extensive evaluations demonstrate that UniC-RAG is highly
effective and significantly outperforms baselines. For instance, UniC-RAG could
achieve over 90% attack success rate by injecting 100 adversarial texts into a
knowledge database with millions of texts to simultaneously attack a large set
of user queries (e.g., 2,000). Additionally, we evaluate existing defenses and
show that they are insufficient to defend against UniC-RAG, highlighting the
need for new defense mechanisms in RAG systems.

</details>


### [61] [FALCON: Autonomous Cyber Threat Intelligence Mining with LLMs for IDS Rule Generation](https://arxiv.org/abs/2508.18684)
*Shaswata Mitra,Azim Bazarov,Martin Duclos,Sudip Mittal,Aritran Piplai,Md Rayhanur Rahman,Edward Zieglar,Shahram Rahimi*

Main category: cs.CR

TL;DR: FALCON是一个基于大语言模型的自主代理框架，能够从网络威胁情报数据实时生成可部署的入侵检测系统规则，并通过内置验证器进行评估，准确率达到95%


<details>
  <summary>Details</summary>
Motivation: 传统签名式入侵检测系统需要频繁更新规则来应对不断演变的网络威胁，但规则更新存在延迟，影响了整体安全准备状态

Method: 开发FALCON自主代理框架，利用大语言模型从CTI数据实时生成IDS规则，包含内置多阶段验证器，支持网络型(Snort)和主机型(YARA)两种检测媒介

Result: 平均准确率达到95%，多位网络安全分析师的定性评估显示84%的评分者间一致性，证明了LLM驱动的数据挖掘在实时网络威胁缓解中的可行性

Conclusion: 大语言模型驱动的自主代理系统能够有效实现实时入侵检测规则生成，显著提升网络安全威胁的实时应对能力

Abstract: Signature-based Intrusion Detection Systems (IDS) detect malicious activities
by matching network or host activity against predefined rules. These rules are
derived from extensive Cyber Threat Intelligence (CTI), which includes attack
signatures and behavioral patterns obtained through automated tools and manual
threat analysis, such as sandboxing. The CTI is then transformed into
actionable rules for the IDS engine, enabling real-time detection and
prevention. However, the constant evolution of cyber threats necessitates
frequent rule updates, which delay deployment time and weaken overall security
readiness. Recent advancements in agentic systems powered by Large Language
Models (LLMs) offer the potential for autonomous IDS rule generation with
internal evaluation. We introduce FALCON, an autonomous agentic framework that
generates deployable IDS rules from CTI data in real-time and evaluates them
using built-in multi-phased validators. To demonstrate versatility, we target
both network (Snort) and host-based (YARA) mediums and construct a
comprehensive dataset of IDS rules with their corresponding CTIs. Our
evaluations indicate FALCON excels in automatic rule generation, with an
average of 95% accuracy validated by qualitative evaluation with 84%
inter-rater agreement among multiple cybersecurity analysts across all metrics.
These results underscore the feasibility and effectiveness of LLM-driven data
mining for real-time cyber threat mitigation.

</details>


### [62] [The Double-edged Sword of LLM-based Data Reconstruction: Understanding and Mitigating Contextual Vulnerability in Word-level Differential Privacy Text Sanitization](https://arxiv.org/abs/2508.18976)
*Stephen Meisenbacher,Alexandra Klymenko,Andreea-Elena Bodea,Florian Matthes*

Main category: cs.CR

TL;DR: 本文研究了LLMs如何利用差分隐私文本脱敏中的上下文漏洞进行数据重建攻击，发现LLMs既能推断原始语义降低隐私保护，也能用于提升脱敏文本的质量和隐私性。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型(LLMs)在利用差分隐私文本脱敏过程中产生的上下文漏洞方面的能力，这些漏洞源于单词级随机化处理留下的上下文线索。

Method: 使用先进的LLMs测试多种隐私级别的文本脱敏机制，进行数据重建攻击实验，分析LLMs对脱敏文本的语义推断能力。

Result: 发现LLM数据重建攻击具有双刃剑效应：既能推断原始语义降低实证隐私保护，又能通过后处理提升脱敏文本的质量和隐私性。

Conclusion: 建议将LLM数据重建作为后处理步骤，通过对抗性思维来增强隐私保护，平衡隐私保护与文本效用之间的关系。

Abstract: Differentially private text sanitization refers to the process of privatizing
texts under the framework of Differential Privacy (DP), providing provable
privacy guarantees while also empirically defending against adversaries seeking
to harm privacy. Despite their simplicity, DP text sanitization methods
operating at the word level exhibit a number of shortcomings, among them the
tendency to leave contextual clues from the original texts due to randomization
during sanitization $\unicode{x2013}$ this we refer to as $\textit{contextual
vulnerability}$. Given the powerful contextual understanding and inference
capabilities of Large Language Models (LLMs), we explore to what extent LLMs
can be leveraged to exploit the contextual vulnerability of DP-sanitized texts.
We expand on previous work not only in the use of advanced LLMs, but also in
testing a broader range of sanitization mechanisms at various privacy levels.
Our experiments uncover a double-edged sword effect of LLM-based data
reconstruction attacks on privacy and utility: while LLMs can indeed infer
original semantics and sometimes degrade empirical privacy protections, they
can also be used for good, to improve the quality and privacy of DP-sanitized
texts. Based on our findings, we propose recommendations for using LLM data
reconstruction as a post-processing step, serving to increase privacy
protection by thinking adversarially.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [63] [RLMR: Reinforcement Learning with Mixed Rewards for Creative Writing](https://arxiv.org/abs/2508.18642)
*Jianxing Liao,Tian Zhang,Xiao Feng,Yusong Zhang,Rui Yang,Haorui Wang,Bosi Wen,Ziying Wang,Runzhi Shi*

Main category: cs.AI

TL;DR: 提出了RLMR方法，通过动态混合奖励系统平衡创意写作中的主观质量与客观约束，在多个模型规模上实现指令遵循和写作质量的双重提升


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法难以同时平衡创意写作中的主观写作质量（文学性和情感表达）和客观约束遵循（格式要求和字数限制），单奖励策略无法同时提升两种能力，固定权重混合奖励方法缺乏对不同写作场景的适应性

Method: RLMR（混合奖励强化学习）方法，使用写作奖励模型评估主观写作质量，约束验证模型评估客观约束遵循，动态调整约束遵循奖励权重，确保违反约束的样本在GRPO中获得负优势并在训练中被惩罚

Result: 在8B到72B参数的多样化模型家族上进行了自动和人工评估，构建了真实写作基准WriteEval。指令遵循能力从83.36%提升到86.65%（IFEval），写作质量在WriteEval上获得72.75%的专家人工对比评估胜率

Conclusion: RLMR是首个在在线RL训练中结合主观偏好与客观验证的工作，为多维创意写作优化提供了有效解决方案

Abstract: Large language models are extensively utilized in creative writing
applications. Creative writing requires a balance between subjective writing
quality (e.g., literariness and emotional expression) and objective constraint
following (e.g., format requirements and word limits). Existing reinforcement
learning methods struggle to balance these two aspects: single reward
strategies fail to improve both abilities simultaneously, while fixed-weight
mixed-reward methods lack the ability to adapt to different writing scenarios.
To address this problem, we propose Reinforcement Learning with Mixed Rewards
(RLMR), utilizing a dynamically mixed reward system from a writing reward model
evaluating subjective writing quality and a constraint verification model
assessing objective constraint following. The constraint following reward
weight is adjusted dynamically according to the writing quality within sampled
groups, ensuring that samples violating constraints get negative advantage in
GRPO and thus penalized during training, which is the key innovation of this
proposed method. We conduct automated and manual evaluations across diverse
model families from 8B to 72B parameters. Additionally, we construct a
real-world writing benchmark named WriteEval for comprehensive evaluation.
Results illustrate that our method achieves consistent improvements in both
instruction following (IFEval from 83.36\% to 86.65\%) and writing quality
(72.75\% win rate in manual expert pairwise evaluations on WriteEval). To the
best of our knowledge, RLMR is the first work to combine subjective preferences
with objective verification in online RL training, providing an effective
solution for multi-dimensional creative writing optimization.

</details>


### [64] [Beyond Benchmark: LLMs Evaluation with an Anthropomorphic and Value-oriented Roadmap](https://arxiv.org/abs/2508.18646)
*Jun Wang,Ninglun Gu,Kailai Zhang,Zijiao Zhang,Yelun Bao,Jin Yang,Xu Yin,Liwei Liu,Yihuan Liu,Pengyong Li,Gary G. Yen,Junchi Yan*

Main category: cs.AI

TL;DR: 该论文提出了一种新的人工智能评估范式，通过人类智能的三维度分类法（IQ、EQ、PQ）和价值导向评估框架，来解决LLM基准测试与现实效用之间的脱节问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLM评估框架存在碎片化问题，过于关注技术指标而忽视了部署所需的整体评估，导致基准性能与实际效用之间存在差距。

Method: 提出了拟人化评估范式，包括三维度分类法（IQ基础能力、EQ价值对齐、PQ专业专长）和价值导向评估框架（经济可行性、社会影响、伦理对齐、环境可持续性），并构建了包含六个组件的模块化架构。

Result: 通过分析200多个基准测试，识别了动态评估需求和可解释性差距等关键挑战，提供了开发技术熟练、上下文相关且伦理健全的LLM的可操作指导。

Conclusion: 该研究为LLM评估提供了全面的框架和实用指南，建立了开源评估资源库，有助于开发更符合实际需求的人工智能系统。

Abstract: For Large Language Models (LLMs), a disconnect persists between benchmark
performance and real-world utility. Current evaluation frameworks remain
fragmented, prioritizing technical metrics while neglecting holistic assessment
for deployment. This survey introduces an anthropomorphic evaluation paradigm
through the lens of human intelligence, proposing a novel three-dimensional
taxonomy: Intelligence Quotient (IQ)-General Intelligence for foundational
capacity, Emotional Quotient (EQ)-Alignment Ability for value-based
interactions, and Professional Quotient (PQ)-Professional Expertise for
specialized proficiency. For practical value, we pioneer a Value-oriented
Evaluation (VQ) framework assessing economic viability, social impact, ethical
alignment, and environmental sustainability. Our modular architecture
integrates six components with an implementation roadmap. Through analysis of
200+ benchmarks, we identify key challenges including dynamic assessment needs
and interpretability gaps. It provides actionable guidance for developing LLMs
that are technically proficient, contextually relevant, and ethically sound. We
maintain a curated repository of open-source evaluation resources at:
https://github.com/onejune2018/Awesome-LLM-Eval.

</details>


### [65] [Bias Mitigation Agent: Optimizing Source Selection for Fair and Balanced Knowledge Retrieval](https://arxiv.org/abs/2508.18724)
*Karanbir Singh,Deepak Muppiri,William Ngu*

Main category: cs.AI

TL;DR: 提出了一种新型偏见缓解代理系统，通过多智能体协作优化信息源选择，显著减少LLM检索中的偏见问题


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然推动了生成式AI发展，但其内部和外部信息源中的偏见会影响检索信息的公平性和平衡性，降低用户信任度

Method: 设计了一个多智能体系统，通过专门化的代理协调工作流程，优化信息源选择，确保检索内容既高度相关又偏见最小

Result: 实验结果显示，与基线朴素检索策略相比，偏见减少了81.82%

Conclusion: 该偏见缓解代理系统能有效促进公平平衡的知识传播，提高AI系统的可信度

Abstract: Large Language Models (LLMs) have transformed the field of artificial
intelligence by unlocking the era of generative applications. Built on top of
generative AI capabilities, Agentic AI represents a major shift toward
autonomous, goal-driven systems that can reason, retrieve, and act. However,
they also inherit the bias present in both internal and external information
sources. This significantly affects the fairness and balance of retrieved
information, and hence reduces user trust. To address this critical challenge,
we introduce a novel Bias Mitigation Agent, a multi-agent system designed to
orchestrate the workflow of bias mitigation through specialized agents that
optimize the selection of sources to ensure that the retrieved content is both
highly relevant and minimally biased to promote fair and balanced knowledge
dissemination. The experimental results demonstrate an 81.82\% reduction in
bias compared to a baseline naive retrieval strategy.

</details>


### [66] [CAC-CoT: Connector-Aware Compact Chain-of-Thought for Efficient Reasoning Data Synthesis Across Dual-System Cognitive Tasks](https://arxiv.org/abs/2508.18743)
*Sunguk Choi,Yonghoon Kwon,Heondeuk Lee*

Main category: cs.AI

TL;DR: CAC-CoT是一种使用有限连接短语的紧凑思维链方法，在保持System-1任务性能的同时，显著缩短推理长度并提升System-2任务表现


<details>
  <summary>Details</summary>
Motivation: 传统长思维链会降低LLM在快速直觉型System-1任务上的性能，需要一种既能保持System-1性能又能提升System-2任务表现的紧凑推理方法

Method: Connector-Aware Compact CoT (CAC-CoT)，通过限制推理使用少量固定连接短语，引导模型生成简洁结构化的解释

Result: 在GSM8K上达到约85%，GPQA上约40%（System-2任务），同时保持S1-Bench约90%的性能（System-1任务），推理长度平均约300个token，比基线缩短约三分之二

Conclusion: CAC-CoT方法实现了高效推理，在不损失准确性的前提下显著提升了推理效率，平衡了System-1和System-2任务的性能

Abstract: Long chain-of-thought (CoT) prompting helps Large Language Models (LLMs)
solve difficult problems, but very long traces often slow or even degrade
performance on fast, intuitive "System-1" tasks. We introduce Connector-Aware
Compact CoT (CAC-CoT) -- a method that deliberately restricts reasoning to a
small, fixed set of connector phrases, steering the model toward concise and
well -- structured explanations. Despite its simplicity, our synthetic method
with Gemini-2.0-Flash yields a high-quality training quality. CAC-CoT achieves
approximately 85% on GSM8K and approximately 40% on GPQA (System-2) while
retaining approximately 90% on S1-Bench (System-1). Its reasoning traces
average approximately 300 tokens(ART), about one-third the length of baseline
traces, delivering higher efficiency without loss of accuracy.

</details>


### [67] [Answering the Unanswerable Is to Err Knowingly: Analyzing and Mitigating Abstention Failures in Large Reasoning Models](https://arxiv.org/abs/2508.18760)
*Yi Liu,Xiangyu Liu,Zequn Sun,Wei Hu*

Main category: cs.AI

TL;DR: 大型推理模型在处理无法回答的问题时存在拒绝回答能力不足的问题，本文通过认知监控和推理时干预的方法显著提升了模型的拒绝回答率


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在遇到缺乏充分条件的数学问题等无法回答的问题时，经常无法提供适当的拒绝回答，这影响了AI的可信度

Method: 采用轻量级的两阶段方法，结合认知监控和推理时干预，首先分析模型对无法回答问题的响应行为，然后通过干预使模型内部认知与外部响应对齐

Result: 实验结果表明该方法显著提高了拒绝回答率，同时保持了整体推理性能

Conclusion: 该方法有效解决了大型推理模型在无法回答问题时的拒绝回答能力不足问题，为构建更可信的AI系统提供了解决方案

Abstract: Large reasoning models (LRMs) have shown remarkable progress on complex
reasoning tasks. However, some questions posed to LRMs are inherently
unanswerable, such as math problems lacking sufficient conditions. We find that
LRMs continually fail to provide appropriate abstentions when confronted with
these unanswerable questions. In this paper, we systematically analyze,
investigate, and resolve this issue for trustworthy AI. We first conduct a
detailed analysis of the distinct response behaviors of LRMs when facing
unanswerable questions. Then, we show that LRMs possess sufficient cognitive
capabilities to recognize the flaws in these questions. However, they fail to
exhibit appropriate abstention behavior, revealing a misalignment between their
internal cognition and external response. Finally, to resolve this issue, we
propose a lightweight, two-stage method that combines cognitive monitoring with
inference-time intervention. Experimental results demonstrate that our method
significantly improves the abstention rate while maintaining the overall
reasoning performance.

</details>


### [68] [Building Self-Evolving Agents via Experience-Driven Lifelong Learning: A Framework and Benchmark](https://arxiv.org/abs/2508.19005)
*Yuxuan Cai,Yipeng Hao,Jie Zhou,Hang Yan,Zhikai Lei,Rui Zhen,Zhenhua Han,Yutao Yang,Junsong Li,Qianjun Pan,Tianyu Huai,Qin Chen,Xin Li,Kai Chen,Bo Zhang,Xipeng Qiu,Liang He*

Main category: cs.AI

TL;DR: 这篇论文提出了经验驱动的终身学习（ELL）框架，用于建造能够通过现实世界交互持续成长的自我迭代机器人。框架基于四个核心原则：经验探索、长期记忆、技能学习和知识内化。还提出了StuLife标准数据集，模拟大学生全面发展过程，用于评估经经经经学习能力。


<details>
  <summary>Details</summary>
Motivation: 随着AI向通用智能发展，需要从优化静态任务的系统转向创建能够持续学习的开放式机器人。传统的模型在实时交互和持续成长方面存在限制，需要一个框架来支持机器人在动态环境中不断学习和迭代。

Method: 研究提出了经验驱动的终身学习（ELL）框架，基于四大核心原则：1）经验探索：通过持续自主交互学习；2）长期记忆：构建持久性知识存储系统；3）技能学习：从经验中提炼可重用技能；4）知识内化：将显式经验转化为隐式本能。同时提出StuLife标准数据集，模拟大学生全面发展过程。

Result: 研究开发了完整的ELL框架和StuLife标准数据集，该数据集包含三个核心阶段和十个详细子场景。StuLife设计了三大范式转变：从被动到主动、从上下文到记忆、从模仿到学习。该平台能够评估记忆保持、技能迁移和自主行为等终身学习能力。

Conclusion: 该研究为建造自我迭代的终身学习机器人提供了一个完整的框架和评估平台。ELL框架的四大原则和StuLife标准数据集为进一步研究AGI提供了重要支撑，特别是在上下文工程方面的应用。这个工作为实现能够持续成长和适应动态环境的通用智能代理提供了重要的技术基础。

Abstract: As AI advances toward general intelligence, the focus is shifting from
systems optimized for static tasks to creating open-ended agents that learn
continuously. In this paper, we introduce Experience-driven Lifelong Learning
(ELL), a framework for building self-evolving agents capable of continuous
growth through real-world interaction. The framework is built on four core
principles: (1) Experience Exploration: Agents learn through continuous,
self-motivated interaction with dynamic environments, navigating interdependent
tasks and generating rich experiential trajectories. (2) Long-term Memory:
Agents preserve and structure historical knowledge, including personal
experiences, domain expertise, and commonsense reasoning, into a persistent
memory system. (3) Skill Learning: Agents autonomously improve by abstracting
recurring patterns from experience into reusable skills, which are actively
refined and validated for application in new tasks. (4) Knowledge
Internalization: Agents internalize explicit and discrete experiences into
implicit and intuitive capabilities as "second nature".
  We also introduce StuLife, a benchmark dataset for ELL that simulates a
student's holistic college journey, from enrollment to academic and personal
development, across three core phases and ten detailed sub-scenarios. StuLife
is designed around three key paradigm shifts: From Passive to Proactive, From
Context to Memory, and From Imitation to Learning. In this dynamic environment,
agents must acquire and distill practical skills and maintain persistent memory
to make decisions based on evolving state variables. StuLife provides a
comprehensive platform for evaluating lifelong learning capabilities, including
memory retention, skill transfer, and self-motivated behavior. Beyond
evaluating SOTA LLMs on the StuLife benchmark, we also explore the role of
context engineering in advancing AGI.

</details>


### [69] [The Ramon Llull's Thinking Machine for Automated Ideation](https://arxiv.org/abs/2508.19200)
*Xinran Zhao,Boyuan Zheng,Chenglei Si,Haofei Yu,Ken Liu,Runlong Zhou,Ruochen Li,Tong Chen,Xiang Li,Yiming Zhang,Tongshuang Wu*

Main category: cs.AI

TL;DR: 重新观点中世纪Ramon Llull的组合术，构建现代思维机器用于科研创意，通过主题、领域和方法三个组成轴导向大语言模型生成多样化、相关的研究思想。


<details>
  <summary>Details</summary>
Motivation: 从中世纪组合术中获得启发，建立一种轻量级、可解释的工具来增强科学创造力，实现人工智能与人类在研究创意上的协作。

Method: 定义主题、领域和方法三个组成轴，从专家或论文中挖掘元素，通过精心组合的提示语导向大语言模型生成研究思想。

Result: 识别的组合能够产生多样化、相关且基于当前文献的研究思想，证明了该方法的有效性。

Conclusion: 现代思维机器提供了一种轻量级、可解释的科学创意增强工具，为人工智能与人类在研究创新中的协作探索了新路径。

Abstract: This paper revisits Ramon Llull's Ars combinatoria - a medieval framework for
generating knowledge through symbolic recombination - as a conceptual
foundation for building a modern Llull's thinking machine for research
ideation. Our approach defines three compositional axes: Theme (e.g.,
efficiency, adaptivity), Domain (e.g., question answering, machine
translation), and Method (e.g., adversarial training, linear attention). These
elements represent high-level abstractions common in scientific work -
motivations, problem settings, and technical approaches - and serve as building
blocks for LLM-driven exploration. We mine elements from human experts or
conference papers and show that prompting LLMs with curated combinations
produces research ideas that are diverse, relevant, and grounded in current
literature. This modern thinking machine offers a lightweight, interpretable
tool for augmenting scientific creativity and suggests a path toward
collaborative ideation between humans and AI.

</details>


### [70] [StepWiser: Stepwise Generative Judges for Wiser Reasoning](https://arxiv.org/abs/2508.19229)
*Wei Xiong,Wenting Zhao,Weizhe Yuan,Olga Golovneva,Tong Zhang,Jason Weston,Sainbayar Sukhbaatar*

Main category: cs.AI

TL;DR: 将逐步奖励建模从分类任务重构为推理任务，提出生成式判断模型StepWiser，通过强化学习训练，提供更好的中间步骤判断准确性并能改进策略模型


<details>
  <summary>Details</summary>
Motivation: 现有过程奖励模型存在两大缺陷：作为分类器不提供解释，且依赖静态数据集的监督微调限制了泛化能力。需要监督多步推理中间步骤的逻辑有效性

Method: 提出生成式判断模型StepWiser，通过元推理对策略模型的推理步骤进行推理，输出思考标记后再给出最终判断。使用强化学习通过rollout的相对结果进行训练

Result: StepWiser在中间步骤判断准确性上优于现有方法，可用于训练时改进策略模型，并能改进推理时搜索

Conclusion: 将逐步奖励建模重构为推理任务的方法有效，生成式判断模型在提供解释和泛化能力方面优于传统分类方法

Abstract: As models increasingly leverage multi-step reasoning strategies to solve
complex problems, supervising the logical validity of these intermediate steps
has become a critical research challenge. Process reward models address this by
providing step-by-step feedback, but current approaches have two major
drawbacks: they typically function as classifiers without providing
explanations, and their reliance on supervised fine-tuning with static datasets
limits generalization. Inspired by recent advances, we reframe stepwise reward
modeling from a classification task to a reasoning task itself. We thus propose
a generative judge that reasons about the policy model's reasoning steps (i.e.,
meta-reasons), outputting thinking tokens before delivering a final verdict.
Our model, StepWiser, is trained by reinforcement learning using relative
outcomes of rollouts. We show it provides (i) better judgment accuracy on
intermediate steps than existing methods; (ii) can be used to improve the
policy model at training time; and (iii) improves inference-time search.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [71] [Text to Query Plans for Question Answering on Large Tables](https://arxiv.org/abs/2508.18758)
*Yipeng Zhang,Chen Wang,Yuzhe Zhang,Jacky Jiang*

Main category: cs.DB

TL;DR: 这篇论文提出了一种新的自然语言查询框架，将查询转换为查询计划而非SQL，以免避SQL在大数据集处理和复杂分析方面的限制。


<details>
  <summary>Details</summary>
Motivation: 解决大型表格数据集查询和分析的挑战，特别是对于无编程能力的用户。尽管Text-to-SQL方法在标准数据上表现不错，但继承了SQL的缺点：大数据集处理效率低，并且对复杂数据分析支持有限。

Method: 开发了一种在传统数据库外实现的框架，利用LLM迭代地解释查询并构建操作序列。通过直接在数据上执行操作，免去了将整个数据集处理给模型的需求，免去了上下文长度限制。

Result: 在标准数据库和大型科学表格上进行的实验验证了该框架的有效性，能够处理大规模数据集并执行复杂的数据分析。

Conclusion: 该框架为自然语言查询提供了更灵活和可扩展的解决方案，能够支持经典SQL命令和复杂分析功能，免去了SQL的内在限制，在大规模数据集处理和复杂分析方面表现优异。

Abstract: Efficient querying and analysis of large tabular datasets remain significant
challenges, especially for users without expertise in programming languages
like SQL. Text-to-SQL approaches have shown promising performance on benchmark
data; however, they inherit SQL's drawbacks, including inefficiency with large
datasets and limited support for complex data analyses beyond basic querying.
We propose a novel framework that transforms natural language queries into
query plans. Our solution is implemented outside traditional databases,
allowing us to support classical SQL commands while avoiding SQL's inherent
limitations. Additionally, we enable complex analytical functions, such as
principal component analysis and anomaly detection, providing greater
flexibility and extensibility than traditional SQL capabilities. We leverage
LLMs to iteratively interpret queries and construct operation sequences,
addressing computational complexity by incrementally building solutions. By
executing operations directly on the data, we overcome context length
limitations without requiring the entire dataset to be processed by the model.
We validate our framework through experiments on both standard databases and
large scientific tables, demonstrating its effectiveness in handling extensive
datasets and performing sophisticated data analyses.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [72] [Designing across domains with declarative thinking: Insights from the 96-Eyes ptychographic imager project](https://arxiv.org/abs/2508.18512)
*Antony C Chan*

Main category: physics.optics

TL;DR: 本文通过实际案例分析了声明式第五代编程语言在多模态成像系统设计中的应用，讨论了在以命令式语言为主的环境中采用声明式方法的挑战和机遇。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过声明式问题形式化语言来涵盖多领域利益相关者的需求，提高设计透明度和可追溯性，减少团队间的对齐成本。

Method: 以96-Eyes项目（96相机并行多模态成像器）为例，将硬件约束和生命科学需求等项目要求形式化为机器可读的问题说明，并提供实际代码示例。

Result: 声明式方法能够有效保存来自光学、算法、硬件加速计算和生命科学团队的关键输入，提高了跨领域协作的效果。

Conclusion: 编程范式隐式地形成了研究工作流，声明式问题形式化在并发研发流程中更有利于创新，而非适用于以序列化阶段流程为主的环境。

Abstract: This article presents a practitioner's reflection on applying declarative,
5th generation, problem formulation language (5GL) to de novo imaging system
design, informed by experiences across the interdisciplinary research in
academia and cross-functional product development within the private sector.
Using the 96-Eyes project: 96-camera parallel multi-modal imager for
high-throughput drug discovery as a representative case, I illustrate how
project requirements, ranging from hardware constraints to life sciences needs,
can be formalized into machine-readable problem statements to preserve
mission-critical input from diverse domain stakeholders. This declarative
approach enhances transparency, ensures design traceability, and minimizes
costly misalignment across optical, algorithmic, hardware-accelerated compute,
and life sciences teams.
  Alongside the technical discussion of 5GL with real-world code examples, I
reflect on the practical barriers to adopting 5GL in environments where
imperative, 3rd-generation languages (3GL) remain the default medium for
inter-team collaboration. Rather than offering an one-size-fits-all solution,
these learned lessons highlight how programming paradigms implicitly shapes
research workflows through existing domain hierarchies. The discussion aims to
invite further explorations into how declarative problem formulations can
facilitate innovation in settings where concurrent R\&{}D workflows are gaining
traction, as opposed to environments where sequential, phase-driven workflows
remain the norm.

</details>
