<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 60]
- [cs.SE](#cs.SE) [Total: 1]
- [cs.DC](#cs.DC) [Total: 2]
- [cs.DL](#cs.DL) [Total: 2]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.CR](#cs.CR) [Total: 3]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.LG](#cs.LG) [Total: 3]
- [cs.CY](#cs.CY) [Total: 2]
- [cs.RO](#cs.RO) [Total: 2]
- [eess.AS](#eess.AS) [Total: 3]
- [cs.AI](#cs.AI) [Total: 10]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.IR](#cs.IR) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [A Preliminary Study of RAG for Taiwanese Historical Archives](https://arxiv.org/abs/2511.07445)
*Claire Lin,Bo-Han Feng,Xuanjun Chen,Te-Lun Yang,Hung-yi Lee,Jyh-Shing Roger Jang*

Main category: cs.CL

TL;DR: 本文首次将RAG应用于台湾历史档案研究，系统分析了查询特征和元数据整合策略对检索质量、答案生成和系统性能的影响。


<details>
  <summary>Details</summary>
Motivation: 虽然RAG在知识密集型任务中表现出色，但很少有研究将其应用于台湾历史档案分析。本文旨在填补这一空白，探索RAG在传统中文历史数据集上的应用效果。

Method: 构建RAG管道，应用于两个历史传统中文数据集（热兰遮城和台湾省议会公报）及其对应的开放式查询集，系统研究查询特征和元数据整合策略的影响。

Result: 早期元数据整合能显著提升检索和答案准确性，但也揭示了RAG系统面临的持续挑战，包括生成过程中的幻觉问题以及处理时序或多跳历史查询的困难。

Conclusion: RAG在台湾历史档案分析中具有应用潜力，元数据整合策略对系统性能有重要影响，但需要解决幻觉和复杂查询处理等挑战。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a promising approach for knowledge-intensive tasks. However, few studies have examined RAG for Taiwanese Historical Archives. In this paper, we present an initial study of a RAG pipeline applied to two historical Traditional Chinese datasets, Fort Zeelandia and the Taiwan Provincial Council Gazette, along with their corresponding open-ended query sets. We systematically investigate the effects of query characteristics and metadata integration strategies on retrieval quality, answer generation, and the performance of the overall system. The results show that early-stage metadata integration enhances both retrieval and answer accuracy while also revealing persistent challenges for RAG systems, including hallucinations during generation and difficulties in handling temporal or multi-hop historical queries.

</details>


### [2] [Large Language Models for Scientific Idea Generation: A Creativity-Centered Survey](https://arxiv.org/abs/2511.07448)
*Fatemeh Shahhosseini,Arash Marioriyad,Ali Momen,Mahdieh Soleymani Baghshah,Mohammad Hossein Rohban,Shaghayegh Haghjooy Javanmard*

Main category: cs.CL

TL;DR: 本调查系统综述了LLM驱动的科学创意生成方法，将其分为五类：外部知识增强、基于提示的分布引导、推理时间缩放、多智能体协作和参数级适应，并使用Boden创造力分类法和Rhodes 4P框架分析这些方法的创造力特征。


<details>
  <summary>Details</summary>
Motivation: 科学创意生成是科学发现的核心，但LLM在这方面的创造能力仍不一致且理解不足。需要系统梳理现有方法，明确如何平衡创意性与科学严谨性。

Method: 将现有方法分类为五大家族：外部知识增强、基于提示的分布引导、推理时间缩放、多智能体协作和参数级适应，并使用Boden创造力分类法（组合性、探索性、变革性）和Rhodes 4P框架（人、过程、压力、产品）进行分析。

Result: 通过将方法论进展与创造力框架对齐，阐明了该领域现状，并为LLM在科学发现中可靠、系统和变革性应用指明了关键方向。

Conclusion: 该调查为理解LLM在科学创意生成中的能力提供了结构化框架，有助于推动LLM在科学发现中的更可靠和系统性应用。

Abstract: Scientific idea generation lies at the heart of scientific discovery and has driven human progress-whether by solving unsolved problems or proposing novel hypotheses to explain unknown phenomena. Unlike standard scientific reasoning or general creative generation, idea generation in science is a multi-objective and open-ended task, where the novelty of a contribution is as essential as its empirical soundness. Large language models (LLMs) have recently emerged as promising generators of scientific ideas, capable of producing coherent and factual outputs with surprising intuition and acceptable reasoning, yet their creative capacity remains inconsistent and poorly understood. This survey provides a structured synthesis of methods for LLM-driven scientific ideation, examining how different approaches balance creativity with scientific soundness. We categorize existing methods into five complementary families: External knowledge augmentation, Prompt-based distributional steering, Inference-time scaling, Multi-agent collaboration, and Parameter-level adaptation. To interpret their contributions, we employ two complementary frameworks: Boden's taxonomy of Combinatorial, Exploratory and Transformational creativity to characterize the level of ideas each family expected to generate, and Rhodes' 4Ps framework-Person, Process, Press, and Product-to locate the aspect or source of creativity that each method emphasizes. By aligning methodological advances with creativity frameworks, this survey clarifies the state of the field and outlines key directions toward reliable, systematic, and transformative applications of LLMs in scientific discovery.

</details>


### [3] [GRIP: In-Parameter Graph Reasoning through Fine-Tuning Large Language Models](https://arxiv.org/abs/2511.07457)
*Jiarui Feng,Donghong Cai,Yixin Chen,Muhan Zhang*

Main category: cs.CL

TL;DR: GRIP框架通过精心设计的微调任务让LLMs内化图结构知识，将知识存储在轻量级LoRA参数中，无需在推理时访问原图即可执行各种图相关任务。


<details>
  <summary>Details</summary>
Motivation: 现有方法将图结构数据转换为文本序列会产生大量token开销，或需要大规模后训练和复杂对齐过程，但模态对齐效果不佳。需要一种更有效的方法让LLMs处理图结构数据。

Method: 提出GRIP框架，通过精心设计的微调任务让LLMs内化复杂关系信息，知识存储在轻量级LoRA参数中，推理时无需访问原图。

Result: 在多个基准测试上的广泛实验验证了该方法的有效性和效率。

Conclusion: GRIP框架成功解决了LLMs处理图结构数据的挑战，通过知识内化和LoRA参数存储实现了高效且有效的图相关任务处理。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in modeling sequential textual data and generalizing across diverse tasks. However, adapting LLMs to effectively handle structural data, such as knowledge graphs or web data, remains a challenging problem. Some approaches adopt complex strategies to convert graphs into text sequences, resulting in significant token overhead and rendering them impractical for large-scale graphs. Others introduce additional modules to encode graphs into fixed-size token representations for LLMs. However, these methods typically require large-scale post-training on graph-text corpus and complex alignment procedures, yet often yield sub-optimal results due to poor modality alignment. Inspired by in-parameter knowledge injection for test-time adaptation of LLMs, we propose GRIP, a novel framework that equips LLMs with the ability to internalize complex relational information from graphs through carefully designed fine-tuning tasks. This knowledge is efficiently stored within lightweight LoRA parameters, enabling the fine-tuned LLM to perform a wide range of graph-related tasks without requiring access to the original graph at inference time. Extensive experiments across multiple benchmarks validate the effectiveness and efficiency of our approach.

</details>


### [4] [REFLEX: Reference-Free Evaluation of Log Summarization via Large Language Model Judgment](https://arxiv.org/abs/2511.07458)
*Priyanka Mudgal*

Main category: cs.CL

TL;DR: REFLEX是一个基于大语言模型的无参考日志摘要评估指标，通过LLM作为零样本评估器来评估摘要质量，无需黄金标准参考或人工标注。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏高质量的参考摘要，以及ROUGE和BLEU等现有指标依赖表面词汇重叠的局限性，评估日志摘要系统具有挑战性。

Method: 使用大语言模型作为零样本评估器，从相关性、信息量和连贯性等维度评估摘要质量，无需参考摘要或人工标注。

Result: REFLEX在多个日志摘要数据集上产生稳定、可解释和细粒度的评估，比传统指标更有效地区分模型输出。

Conclusion: REFLEX为在实际场景中参考数据稀缺或不可用的情况下评估日志摘要提供了可扩展的替代方案。

Abstract: Evaluating log summarization systems is challenging due to the lack of high-quality reference summaries and the limitations of existing metrics like ROUGE and BLEU, which depend on surface-level lexical overlap. We introduce REFLEX, a reference-free evaluation metric for log summarization based on large language model (LLM) judgment. REFLEX uses LLMs as zero-shot evaluators to assess summary quality along dimensions such as relevance, informativeness, and coherence, without requiring gold-standard references or human annotations. We show that REFLEX produces stable, interpretable, and fine-grained evaluations across multiple log summarization dataset, and more effectively distinguishes model outputs than traditional metrics. REFLEX provides a scalable alternative for evaluating log summaries in real-world settings where reference data is scarce or unavailable.

</details>


### [5] [It Takes Two: A Dual Stage Approach for Terminology-Aware Translation](https://arxiv.org/abs/2511.07461)
*Akshat Singh Jaswal*

Main category: cs.CL

TL;DR: DuTerm是一个新颖的两阶段术语约束机器翻译架构，结合术语感知NMT模型和基于提示的LLM后编辑，在多个语言对上实现高质量翻译。


<details>
  <summary>Details</summary>
Motivation: 解决术语约束机器翻译中严格约束执行与翻译质量之间的平衡问题，探索LLM在术语处理中的最佳应用方式。

Method: 两阶段架构：第一阶段使用在大规模合成数据上微调的术语感知NMT模型；第二阶段使用基于提示的LLM进行后编辑，优化NMT输出并确保术语一致性。

Result: 在英德、英西、英俄语言对上的评估显示，LLM的上下文驱动术语处理比严格约束执行产生更高质量的翻译。

Conclusion: LLM最适合作为上下文驱动的修正器而非生成器，在术语约束翻译中展现出关键的质量-约束权衡。

Abstract: This paper introduces DuTerm, a novel two-stage architecture for terminology-constrained machine translation. Our system combines a terminology-aware NMT model, adapted via fine-tuning on large-scale synthetic data, with a prompt-based LLM for post-editing. The LLM stage refines NMT output and enforces terminology adherence. We evaluate DuTerm on English-to German, English-to-Spanish, and English-to-Russian with the WMT 2025 Terminology Shared Task corpus. We demonstrate that flexible, context-driven terminology handling by the LLM consistently yields higher quality translations than strict constraint enforcement. Our results highlight a critical trade-off, revealing that an LLM's work best for high-quality translation as context-driven mutators rather than generators.

</details>


### [6] [Motif 2 12.7B technical report](https://arxiv.org/abs/2511.07464)
*Junghwan Lim,Sungmin Lee,Dongseok Kim,Taehyun Kim,Eunhwan Park,Jeesoo Lee,Jeongdoo Lee,Junhyeok Lee,Wai Ting Cheung,Dahye Choi,Jaeheui Her,Jaeyeon Huh,Hanbin Jung,Changjin Kang,Beomgyu Kim,Minjae Kim,Taewhan Kim,Youngrok Kim,Hyukjin Kweon,Haesol Lee,Kungyu Lee,Dongpin Oh,Yeongjae Park,Bokki Ryu,Dongjoo Weon*

Main category: cs.CL

TL;DR: Motif-2-12.7B是一个新的开源基础模型，通过架构创新和系统级优化，在有限计算预算下实现了高效的大语言模型性能。


<details>
  <summary>Details</summary>
Motivation: 在受限计算预算下实现可扩展的语言理解和强大的指令泛化能力，通过优化设计挑战更大规模模型的性能。

Method: 结合分组差分注意力(GDA)架构创新，在55万亿token上进行课程驱动的预训练，使用MuonClip优化器和自定义高性能内核，采用三阶段监督微调流程。

Result: 在多样化基准测试中展现出竞争力，证明经过深思熟虑的架构扩展和优化训练设计可以媲美更大规模模型的能力。

Conclusion: 精心设计的架构扩展和优化的训练方法可以在有限计算资源下实现与大模型相媲美的性能表现。

Abstract: We introduce Motif-2-12.7B, a new open-weight foundation model that pushes the efficiency frontier of large language models by combining architectural innovation with system-level optimization. Designed for scalable language understanding and robust instruction generalization under constrained compute budgets, Motif-2-12.7B builds upon Motif-2.6B with the integration of Grouped Differential Attention (GDA), which improves representational efficiency by disentangling signal and noise-control attention pathways. The model is pre-trained on 5.5 trillion tokens spanning diverse linguistic, mathematical, scientific, and programming domains using a curriculum-driven data scheduler that gradually changes the data composition ratio. The training system leverages the MuonClip optimizer alongside custom high-performance kernels, including fused PolyNorm activations and the Parallel Muon algorithm, yielding significant throughput and memory efficiency gains in large-scale distributed environments. Post-training employs a three-stage supervised fine-tuning pipeline that successively enhances general instruction adherence, compositional understanding, and linguistic precision. Motif-2-12.7B demonstrates competitive performance across diverse benchmarks, showing that thoughtful architectural scaling and optimized training design can rival the capabilities of much larger models.

</details>


### [7] [Focusing on Language: Revealing and Exploiting Language Attention Heads in Multilingual Large Language Models](https://arxiv.org/abs/2511.07498)
*Xin Liu,Qiyang Song,Qihang Zhou,Haichao Du,Shaowen Xu,Wenbo Jiang,Weijuan Zhang,Xiaoqi Jia*

Main category: cs.CL

TL;DR: 提出LAHIS方法识别多语言LLM中注意力头的重要性，发现语言特定和语言通用头，并通过轻量适配提升多语言性能


<details>
  <summary>Details</summary>
Motivation: 多语言LLM的内部机制理解不足，特别是多头自注意力在多语言处理中的作用尚未充分探索

Method: 提出LAHIS方法，通过单次前向和后向传播识别注意力头重要性；引入轻量适配学习软头掩码调节注意力输出

Result: 在Aya-23-8B等模型中发现语言特定和语言通用头；仅需20个可调参数即可提升XQuAD准确率

Conclusion: 从MHA角度增强了LLM的可解释性和多语言能力

Abstract: Large language models (LLMs) increasingly support multilingual understanding and generation. Meanwhile, efforts to interpret their internal mechanisms have emerged, offering insights to enhance multilingual performance. While multi-head self-attention (MHA) has proven critical in many areas, its role in multilingual capabilities remains underexplored. In this work, we study the contribution of MHA in supporting multilingual processing in LLMs. We propose Language Attention Head Importance Scores (LAHIS), an effective and efficient method that identifies attention head importance for multilingual capabilities via a single forward and backward pass through the LLM. Applying LAHIS to Aya-23-8B, Llama-3.2-3B, and Mistral-7B-v0.1, we reveal the existence of both language-specific and language-general heads. Language-specific heads enable cross-lingual attention transfer to guide the model toward target language contexts and mitigate off-target language generation issue, contributing to addressing challenges in multilingual LLMs. We also introduce a lightweight adaptation that learns a soft head mask to modulate attention outputs over language heads, requiring only 20 tunable parameters to improve XQuAD accuracy. Overall, our work enhances both the interpretability and multilingual capabilities of LLMs from the perspective of MHA.

</details>


### [8] [LLM Optimization Unlocks Real-Time Pairwise Reranking](https://arxiv.org/abs/2511.07555)
*Jingyu Wu,Aditya Shrivastava,Jing Zhu,Alfy Samuel,Anoop Kumar,Daben Liu*

Main category: cs.CL

TL;DR: 本文针对基于LLM的成对重排序方法在实时应用中的延迟问题，提出了一系列优化策略，实现了166倍的延迟降低，从61.36秒降至0.37秒/查询，性能损失可忽略。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的成对重排序方法（如PRP）虽然有效，但计算复杂度高、延迟大，难以在实时应用中部署，需要解决其效率问题。

Method: 采用多种优化策略：使用更小模型、限制重排序集大小、降低精度、通过单向顺序推断减少位置偏差、限制输出标记数。

Result: 优化后延迟从61.36秒降至0.37秒/查询（166倍降低），Recall@k指标性能损失可忽略。

Conclusion: 通过精心设计的优化方法，LLM重排序可以显著提升效率，使其在延迟敏感的实时部署中变得可行。

Abstract: Efficiently reranking documents retrieved from information retrieval (IR) pipelines to enhance overall quality of Retrieval-Augmented Generation (RAG) system remains an important yet challenging problem. Recent studies have highlighted the importance of Large Language Models (LLMs) in reranking tasks. In particular, Pairwise Reranking Prompting (PRP) has emerged as a promising plug-and-play approach due to its usability and effectiveness. However, the inherent complexity of the algorithm, coupled with the high computational demands and latency incurred due to LLMs, raises concerns about its feasibility in real-time applications. To address these challenges, this paper presents a focused study on pairwise reranking, demonstrating that carefully applied optimization methods can significantly mitigate these issues. By implementing these methods, we achieve a remarkable latency reduction of up to 166 times, from 61.36 seconds to 0.37 seconds per query, with an insignificant drop in performance measured by Recall@k. Our study highlights the importance of design choices that were previously overlooked, such as using smaller models, limiting the reranked set, using lower precision, reducing positional bias with one-directional order inference, and restricting output tokens. These optimizations make LLM-based reranking substantially more efficient and feasible for latency-sensitive, real-world deployments.

</details>


### [9] [LLMs vs. Traditional Sentiment Tools in Psychology: An Evaluation on Belgian-Dutch Narratives](https://arxiv.org/abs/2511.07641)
*Ratna Kandala,Katie Hoemann*

Main category: cs.CL

TL;DR: 荷兰语LLMs在弗拉芒语情感分析中表现不如传统方法，Pattern工具表现最佳，挑战了LLM在情感分析中的优越性假设


<details>
  <summary>Details</summary>
Motivation: 理解日常语言中的情感细微差别对计算语言学和情感研究至关重要，需要评估LLMs在低资源语言变体（弗拉芒语）中的情感预测能力

Method: 评估三个荷兰语特定LLM（ChocoLlama-8B-Instruct、Reynaerde-7B-chat、GEITje-7B-ultra）与传统工具LIWC和Pattern在弗拉芒语情感预测中的表现，使用约25000个自发文本响应数据集

Result: 荷兰语调优的LLMs表现不如传统方法，Pattern工具表现出最佳性能，LLMs未能超越传统工具

Conclusion: 研究结果挑战了LLM在情感分析任务中的优越性假设，强调需要为低资源语言变体开发文化和语言定制的评估框架

Abstract: Understanding emotional nuances in everyday language is crucial for computational linguistics and emotion research. While traditional lexicon-based tools like LIWC and Pattern have served as foundational instruments, Large Language Models (LLMs) promise enhanced context understanding. We evaluated three Dutch-specific LLMs (ChocoLlama-8B-Instruct, Reynaerde-7B-chat, and GEITje-7B-ultra) against LIWC and Pattern for valence prediction in Flemish, a low-resource language variant. Our dataset comprised approximately 25000 spontaneous textual responses from 102 Dutch-speaking participants, each providing narratives about their current experiences with self-assessed valence ratings (-50 to +50). Surprisingly, despite architectural advancements, the Dutch-tuned LLMs underperformed compared to traditional methods, with Pattern showing superior performance. These findings challenge assumptions about LLM superiority in sentiment analysis tasks and highlight the complexity of capturing emotional valence in spontaneous, real-world narratives. Our results underscore the need for developing culturally and linguistically tailored evaluation frameworks for low-resource language variants, while questioning whether current LLM fine-tuning approaches adequately address the nuanced emotional expressions found in everyday language use.

</details>


### [10] [Revisiting NLI: Towards Cost-Effective and Human-Aligned Metrics for Evaluating LLMs in Question Answering](https://arxiv.org/abs/2511.07659)
*Sai Shridhar Balamurali,Lu Cheng*

Main category: cs.CL

TL;DR: NLI评分结合简单的词汇匹配标记，在长问答评估中能达到GPT-4o 89.9%的准确率，但计算成本低得多。


<details>
  <summary>Details</summary>
Motivation: 当前评估大语言模型答案的方法存在缺陷：词汇指标缺乏语义理解，而"LLM作为评判者"的方法计算成本高昂。

Method: 使用现成的自然语言推理(NLI)评分，辅以简单的词汇匹配标记，并创建了包含3000个样本的DIVER-QA基准数据集进行验证。

Result: 这种轻量级NLI方法在长问答评估中与GPT-4o的准确率相当(89.9%)，但所需参数数量少几个数量级。

Conclusion: 廉价的基于NLI的评估方法仍然具有竞争力，DIVER-QA可作为未来指标研究的开放资源。

Abstract: Evaluating answers from state-of-the-art large language models (LLMs) is challenging: lexical metrics miss semantic nuances, whereas "LLM-as-Judge" scoring is computationally expensive. We re-evaluate a lightweight alternative -- off-the-shelf Natural Language Inference (NLI) scoring augmented by a simple lexical-match flag and find that this decades-old technique matches GPT-4o's accuracy (89.9%) on long-form QA, while requiring orders-of-magnitude fewer parameters. To test human alignment of these metrics rigorously, we introduce DIVER-QA, a new 3000-sample human-annotated benchmark spanning five QA datasets and five candidate LLMs. Our results highlight that inexpensive NLI-based evaluation remains competitive and offer DIVER-QA as an open resource for future metric research.

</details>


### [11] [Stress Testing Factual Consistency Metrics for Long-Document Summarization](https://arxiv.org/abs/2511.07689)
*Zain Muhammad Mujahid,Dustin Wright,Isabelle Augenstein*

Main category: cs.CL

TL;DR: 本文系统评估了6种广泛使用的无参考事实性度量在长文档摘要中的可靠性，发现这些短文档度量在语义等价摘要上产生不一致评分，且在信息密集声明上可靠性下降。


<details>
  <summary>Details</summary>
Motivation: 评估抽象文本摘要的事实一致性仍然是一个重大挑战，特别是在长文档中，传统度量方法难以处理输入长度限制和长距离依赖关系。

Method: 通过7种事实性保留扰动（包括释义、简化、同义词替换、逻辑等价否定等）来探测度量鲁棒性，并在三个长文档基准数据集上进行评估。

Result: 现有短文档度量对语义等价摘要产生不一致评分，对信息密集声明的可靠性下降，扩展检索上下文在某些领域能提高稳定性，但没有度量能在长上下文条件下持续保持事实对齐。

Conclusion: 研究结果为改进事实性评估指明了具体方向，包括多跨度推理、上下文感知校准以及在意义保留变体上训练以增强长文档摘要的鲁棒性。

Abstract: Evaluating the factual consistency of abstractive text summarization remains a significant challenge, particularly for long documents, where conventional metrics struggle with input length limitations and long-range dependencies. In this work, we systematically evaluate the reliability of six widely used reference-free factuality metrics, originally proposed for short-form summarization, in the long-document setting. We probe metric robustness through seven factuality-preserving perturbations applied to summaries, namely paraphrasing, simplification, synonym replacement, logically equivalent negations, vocabulary reduction, compression, and source text insertion, and further analyze their sensitivity to retrieval context and claim information density. Across three long-form benchmark datasets spanning science fiction, legal, and scientific domains, our results reveal that existing short-form metrics produce inconsistent scores for semantically equivalent summaries and exhibit declining reliability for information-dense claims whose content is semantically similar to many parts of the source document. While expanding the retrieval context improves stability in some domains, no metric consistently maintains factual alignment under long-context conditions. Finally, our results highlight concrete directions for improving factuality evaluation, including multi-span reasoning, context-aware calibration, and training on meaning-preserving variations to enhance robustness in long-form summarization. We release all code, perturbed data, and scripts required to reproduce our results at https://github.com/zainmujahid/metricEval-longSum.

</details>


### [12] [CAPO: Confidence Aware Preference Optimization Learning for Multilingual Preferences](https://arxiv.org/abs/2511.07691)
*Rhitabrat Pokharel,Yufei Tao,Ameeta Agrawal*

Main category: cs.CL

TL;DR: 提出了CAPO方法，通过基于相对奖励的动态损失缩放机制替代DPO的固定偏好对处理，增强多语言环境下的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有偏好优化方法（如DPO）在英语中有效，但在多语言环境中泛化能力不足，特别是在处理噪声或低边际比较时

Method: CAPO方法使用动态损失缩放机制，根据每个偏好对的置信度调节学习信号，提高对噪声和低边际比较的鲁棒性

Result: CAPO在奖励准确率上比现有基线方法至少提高16%，并通过扩大首选和非首选响应之间的差距改善了多语言对齐

Conclusion: CAPO是一种简单有效的替代方案，能够显著提升多语言环境下的偏好优化性能

Abstract: Preference optimization is a critical post-training technique used to align large language models (LLMs) with human preferences, typically by fine-tuning on ranked response pairs. While methods like Direct Preference Optimization (DPO) have proven effective in English, they often fail to generalize robustly to multilingual settings. We propose a simple yet effective alternative, Confidence-Aware Preference Optimization (CAPO), which replaces DPO's fixed treatment of preference pairs with a dynamic loss scaling mechanism based on a relative reward. By modulating the learning signal according to the confidence in each preference pair, CAPO enhances robustness to noisy or low-margin comparisons, typically encountered in multilingual text. Empirically, CAPO outperforms existing preference optimization baselines by at least 16% in reward accuracy, and improves alignment by widening the gap between preferred and dispreferred responses across languages.

</details>


### [13] [Critical Confabulation: Can LLMs Hallucinate for Social Good?](https://arxiv.org/abs/2511.07722)
*Peiqi Sui,Eamon Duede,Hoyt Long,Richard Jean So*

Main category: cs.CL

TL;DR: 论文提出"批判性虚构"方法，利用LLM的幻觉来填补因社会政治不平等造成的档案空白，为历史中的"隐藏人物"重建证据约束的叙事。


<details>
  <summary>Details</summary>
Motivation: LLM会产生幻觉，但经过精心约束的虚构可以具有社会价值，特别是用于填补因社会政治不平等导致的档案缺失。

Method: 使用开放式叙事填空任务，让LLM生成角色时间线中被遮蔽的事件，基于未发表文本构建的小说语料库。评估了经过数据污染审核的完全开放模型和未审核的基线模型。

Result: 验证了LLM执行批判性虚构的基础叙事理解能力，展示了受控且明确指定的幻觉如何支持LLM在知识生产中的应用。

Conclusion: 受控且精心指定的幻觉可以支持LLM在知识生产中的应用，而不会将推测与历史准确性和忠实性混为一谈。

Abstract: LLMs hallucinate, yet some confabulations can have social affordances if carefully bounded. We propose critical confabulation (inspired by critical fabulation from literary and social theory), the use of LLM hallucinations to "fill-in-the-gap" for omissions in archives due to social and political inequality, and reconstruct divergent yet evidence-bound narratives for history's "hidden figures". We simulate these gaps with an open-ended narrative cloze task: asking LLMs to generate a masked event in a character-centric timeline sourced from a novel corpus of unpublished texts. We evaluate audited (for data contamination), fully-open models (the OLMo-2 family) and unaudited open-weight and proprietary baselines under a range of prompts designed to elicit controlled and useful hallucinations. Our findings validate LLMs' foundational narrative understanding capabilities to perform critical confabulation, and show how controlled and well-specified hallucinations can support LLM applications for knowledge production without collapsing speculation into a lack of historical accuracy and fidelity.

</details>


### [14] [Back to the Future: The Role of Past and Future Context Predictability in Incremental Language Production](https://arxiv.org/abs/2511.07752)
*Shiva Upadhye,Richard Futrell*

Main category: cs.CL

TL;DR: 该研究提出了一种新的信息论可预测性度量方法，整合了未来和过去语境的可预测性，通过两个自然语言语料库研究揭示了语境可预测性对词汇编码和选择的影响机制。


<details>
  <summary>Details</summary>
Motivation: 理解在线语言产生中语境可预测性对词汇形式和选择的影响，特别是探索尚不清楚的基于未来语境的后向可预测性效应及其与未来规划的关系。

Method: 使用改进的度量和更强大的语言模型，在两个自然语言语料库研究中：1）重新审视经典可预测性对词长的影响；2）在生成框架内研究替换错误，独立建模词汇、语境和交际因素对词汇选择的影响。

Result: 提出的概念驱动后向可预测性替代方法在两个研究中产生质上相似的效果；通过替换错误的细粒度分析，揭示了说话者在词汇规划中如何优先考虑形式、意义和基于语境的信息。

Conclusion: 这些发现阐明了过去和未来语境在说话者编码和选择词汇中的功能作用，为语境可预测性效应和句子规划机制之间搭建了桥梁。

Abstract: Contextual predictability shapes both the form and choice of words in online language production. The effects of the predictability of a word given its previous context are generally well-understood in both production and comprehension, but studies of naturalistic production have also revealed a poorly-understood backward predictability effect of a word given its future context, which may be related to future planning. Here, in two studies of naturalistic speech corpora, we investigate backward predictability effects using improved measures and more powerful language models, introducing a new principled and conceptually motivated information-theoretic predictability measure that integrates predictability from both the future and the past context. Our first study revisits classic predictability effects on word duration. Our second study investigates substitution errors within a generative framework that independently models the effects of lexical, contextual, and communicative factors on word choice, while predicting the actual words that surface as speech errors. We find that our proposed conceptually-motivated alternative to backward predictability yields qualitatively similar effects across both studies. Through a fine-grained analysis of substitution errors, we further show that different kinds of errors are suggestive of how speakers prioritize form, meaning, and context-based information during lexical planning. Together, these findings illuminate the functional roles of past and future context in how speakers encode and choose words, offering a bridge between contextual predictability effects and the mechanisms of sentence planning.

</details>


### [15] [Design, Results and Industry Implications of the World's First Insurance Large Language Model Evaluation Benchmark](https://arxiv.org/abs/2511.07794)
*Hua Zhou,Bing Ma,Yufei Zhang,Yi Zhao*

Main category: cs.CL

TL;DR: 本文介绍了CUFEInse v1.0保险领域专业评估基准的构建，包含5个核心维度、54个子指标和14,430个高质量问题，并对11个主流大模型进行了全面评估。


<details>
  <summary>Details</summary>
Motivation: 填补保险领域专业评估基准的空白，为学术界和产业界提供专业、系统、权威的评估工具，并为垂直领域大模型评估范式提供重要参考。

Method: 采用"量化导向、专家驱动、多重验证"原则，建立涵盖保险理论知识、行业认知、安全合规、智能体应用和逻辑严谨性的多维评估体系。

Result: 评估发现通用模型存在精算能力弱、合规适应性不足等瓶颈；高质量领域专用训练在保险垂直场景有显著优势但在业务适应性和合规性方面存在不足；准确识别了当前大模型在保险精算、核保理赔推理、合规营销文案等专业场景的共性瓶颈。

Conclusion: CUFEInse基准的建立填补了保险领域专业评估空白，其构建理念和方法论为垂直领域大模型评估范式提供了重要参考，并展望了保险大模型"领域适应+推理增强"的核心发展方向。

Abstract: This paper comprehensively elaborates on the construction methodology, multi-dimensional evaluation system, and underlying design philosophy of CUFEInse v1.0. Adhering to the principles of "quantitative-oriented, expert-driven, and multi-validation," the benchmark establishes an evaluation framework covering 5 core dimensions, 54 sub-indicators, and 14,430 high-quality questions, encompassing insurance theoretical knowledge, industry understanding, safety and compliance, intelligent agent application, and logical rigor. Based on this benchmark, a comprehensive evaluation was conducted on 11 mainstream large language models. The evaluation results reveal that general-purpose models suffer from common bottlenecks such as weak actuarial capabilities and inadequate compliance adaptation. High-quality domain-specific training demonstrates significant advantages in insurance vertical scenarios but exhibits shortcomings in business adaptation and compliance. The evaluation also accurately identifies the common bottlenecks of current large models in professional scenarios such as insurance actuarial, underwriting and claim settlement reasoning, and compliant marketing copywriting. The establishment of CUFEInse not only fills the gap in professional evaluation benchmarks for the insurance field, providing academia and industry with a professional, systematic, and authoritative evaluation tool, but also its construction concept and methodology offer important references for the evaluation paradigm of large models in vertical fields, serving as an authoritative reference for academic model optimization and industrial model selection. Finally, the paper looks forward to the future iteration direction of the evaluation benchmark and the core development direction of "domain adaptation + reasoning enhancement" for insurance large models.

</details>


### [16] [From Experience to Strategy: Empowering LLM Agents with Trainable Graph Memory](https://arxiv.org/abs/2511.07800)
*Siyu Xia,Zekun Xu,Jiajun Chai,Wentian Fan,Yan Song,Xiaohan Wang,Guojun Yin,Wei Lin,Haifeng Zhang,Jun Wang*

Main category: cs.CL

TL;DR: 提出了一种可训练的图记忆框架，通过将智能体轨迹抽象为状态机中的结构化决策路径，并进一步提炼为高层战略元认知，增强LLM智能体的推理能力。


<details>
  <summary>Details</summary>
Motivation: LLM智能体在利用先验经验指导当前决策方面存在局限：隐式记忆训练存在灾难性遗忘和可解释性差的问题，显式记忆提示缺乏适应性。

Method: 构建多层图记忆框架，将原始智能体轨迹抽象为状态机中的结构化决策路径，提炼为高层战略元认知，并通过基于奖励的权重优化程序估计每个元认知的经验效用，最后通过元认知提示动态集成到LLM智能体训练中。

Result: 可学习的图记忆实现了稳健的泛化，提高了LLM智能体的战略推理性能，并在强化学习训练中提供了一致的收益。

Conclusion: 该可训练的图记忆框架有效增强了LLM智能体利用参数信息的能力，提供了更好的战略推理性能和训练稳定性。

Abstract: Large Language Models (LLMs) based agents have demonstrated remarkable potential in autonomous task-solving across complex, open-ended environments. A promising approach for improving the reasoning capabilities of LLM agents is to better utilize prior experiences in guiding current decisions. However, LLMs acquire experience either through implicit memory via training, which suffers from catastrophic forgetting and limited interpretability, or explicit memory via prompting, which lacks adaptability. In this paper, we introduce a novel agent-centric, trainable, multi-layered graph memory framework and evaluate how context memory enhances the ability of LLMs to utilize parametric information. The graph abstracts raw agent trajectories into structured decision paths in a state machine and further distills them into high-level, human-interpretable strategic meta-cognition. In order to make memory adaptable, we propose a reinforcement-based weight optimization procedure that estimates the empirical utility of each meta-cognition based on reward feedback from downstream tasks. These optimized strategies are then dynamically integrated into the LLM agent's training loop through meta-cognitive prompting. Empirically, the learnable graph memory delivers robust generalization, improves LLM agents' strategic reasoning performance, and provides consistent benefits during Reinforcement Learning (RL) training.

</details>


### [17] [AlignSurvey: A Comprehensive Benchmark for Human Preferences Alignment in Social Surveys](https://arxiv.org/abs/2511.07871)
*Chenxi Lin,Weikang Yuan,Zhuoren Jiang,Biao Huang,Ruitao Zhang,Jianan Ge,Yueqian Xu,Jianxing Yu*

Main category: cs.CL

TL;DR: AlignSurvey是首个系统复制和评估完整社会调查流程的基准，定义了四个与关键调查阶段对齐的任务，并提供了多层级数据集架构和评估指标来评估对齐保真度、一致性和公平性。


<details>
  <summary>Details</summary>
Motivation: 传统社会调查面临固定问题格式、高成本、适应性有限和跨文化等效性困难等挑战，而现有LLM模拟调查的研究大多局限于结构化问题，忽视了完整调查流程，且存在训练数据偏差导致边缘群体代表性不足的风险。

Method: 引入AlignSurvey基准，定义四个任务：社会角色建模、半结构化访谈建模、态度立场建模和调查响应建模；构建多层级数据集架构，包括社会基础语料库和完整流程调查数据集；通过两阶段微调开源LLM获得SurveyLM系列模型。

Result: 开发了包含44K+访谈对话和400K+结构化调查记录的社会基础语料库，以及专家标注的AlignSurvey-Expert和两个全国代表性调查数据集；提供了任务特定的评估指标来评估个体和群体层面的对齐保真度、一致性和公平性。

Conclusion: AlignSurvey基准、数据集和模型为透明和负责任的社会科学研究提供了支持，有助于解决传统调查的局限性，同时关注人口统计多样性，促进LLM在社会调查领域的应用发展。

Abstract: Understanding human attitudes, preferences, and behaviors through social surveys is essential for academic research and policymaking. Yet traditional surveys face persistent challenges, including fixed-question formats, high costs, limited adaptability, and difficulties ensuring cross-cultural equivalence. While recent studies explore large language models (LLMs) to simulate survey responses, most are limited to structured questions, overlook the entire survey process, and risks under-representing marginalized groups due to training data biases. We introduce AlignSurvey, the first benchmark that systematically replicates and evaluates the full social survey pipeline using LLMs. It defines four tasks aligned with key survey stages: social role modeling, semi-structured interview modeling, attitude stance modeling and survey response modeling. It also provides task-specific evaluation metrics to assess alignment fidelity, consistency, and fairness at both individual and group levels, with a focus on demographic diversity. To support AlignSurvey, we construct a multi-tiered dataset architecture: (i) the Social Foundation Corpus, a cross-national resource with 44K+ interview dialogues and 400K+ structured survey records; and (ii) a suite of Entire-Pipeline Survey Datasets, including the expert-annotated AlignSurvey-Expert (ASE) and two nationally representative surveys for cross-cultural evaluation. We release the SurveyLM family, obtained through two-stage fine-tuning of open-source LLMs, and offer reference models for evaluating domain-specific alignment. All datasets, models, and tools are available at github and huggingface to support transparent and socially responsible research.

</details>


### [18] [Planned Event Forecasting using Future Mentions and Related Entity Extraction in News Articles](https://arxiv.org/abs/2511.07879)
*Neelesh Kumar Shukla,Pranay Sanghvi*

Main category: cs.CL

TL;DR: 开发了一个基于新闻分析的公民抗议事件预测系统，使用主题建模和NER技术识别相关实体和日期，实现地理独立的通用模型。


<details>
  <summary>Details</summary>
Motivation: 在民主国家中，公民抗议活动可能造成社会混乱，提前预测这些事件有助于行政官员采取必要措施。由于抗议活动通常会提前宣布，通过分析新闻公告可以预测计划中的事件。

Method: 使用主题建模和word2vec过滤相关新闻文章，应用命名实体识别方法识别人员、组织、地点和日期等实体，并进行时间标准化处理。提出相关实体提取方法识别实际参与事件的实体。

Result: 开发了一个地理独立的通用模型，能够识别公民抗议事件的关键特征，并有效提取相关实体。

Conclusion: 该系统能够通过分析新闻文章中的公告来预测社会动荡事件，为行政决策提供支持，具有实际应用价值。

Abstract: In democracies like India, people are free to express their views and demands. Sometimes this causes situations of civil unrest such as protests, rallies, and marches. These events may be disruptive in nature and are often held without prior permission from the competent authority. Forecasting these events helps administrative officials take necessary action. Usually, protests are announced well in advance to encourage large participation. Therefore, by analyzing such announcements in news articles, planned events can be forecasted beforehand. We developed such a system in this paper to forecast social unrest events using topic modeling and word2vec to filter relevant news articles, and Named Entity Recognition (NER) methods to identify entities such as people, organizations, locations, and dates. Time normalization is applied to convert future date mentions into a standard format. In this paper, we have developed a geographically independent, generalized model to identify key features for filtering civil unrest events. There could be many mentions of entities, but only a few may actually be involved in the event. This paper calls such entities Related Entities and proposes a method to extract them, referred to as Related Entity Extraction.

</details>


### [19] [Breaking the Adversarial Robustness-Performance Trade-off in Text Classification via Manifold Purification](https://arxiv.org/abs/2511.07888)
*Chenhao Dang,Jing Ma*

Main category: cs.CL

TL;DR: MC^2F是一个两模块系统，通过在编码器嵌入流形上建模干净样本分布来解决文本分类中对抗鲁棒性与干净数据性能之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 解决文本分类中增强模型对抗鲁棒性通常会降低干净数据性能的持久挑战，通过建模干净样本在编码器嵌入流形上的分布来消除这种权衡。

Method: 提出Manifold-Correcting Causal Flow (MC^2F)，包含两个模块：分层黎曼连续归一化流(SR-CNF)学习干净数据流形的密度，识别分布外嵌入；测地线净化求解器通过最短路径将对抗点投影回学习到的流形上。

Result: 在三个数据集和多种对抗攻击上的广泛评估表明，MC^2F不仅建立了对抗鲁棒性的新最先进水平，而且完全保留了干净数据的性能，甚至略微提高了准确率。

Conclusion: MC^2F通过直接在句子嵌入上操作，成功解决了对抗鲁棒性与干净数据性能之间的权衡问题，实现了两全其美的效果。

Abstract: A persistent challenge in text classification (TC) is that enhancing model robustness against adversarial attacks typically degrades performance on clean data. We argue that this challenge can be resolved by modeling the distribution of clean samples in the encoder embedding manifold. To this end, we propose the Manifold-Correcting Causal Flow (MC^2F), a two-module system that operates directly on sentence embeddings. A Stratified Riemannian Continuous Normalizing Flow (SR-CNF) learns the density of the clean data manifold. It identifies out-of-distribution embeddings, which are then corrected by a Geodesic Purification Solver. This solver projects adversarial points back onto the learned manifold via the shortest path, restoring a clean, semantically coherent representation. We conducted extensive evaluations on text classification (TC) across three datasets and multiple adversarial attacks. The results demonstrate that our method, MC^2F, not only establishes a new state-of-the-art in adversarial robustness but also fully preserves performance on clean data, even yielding modest gains in accuracy.

</details>


### [20] [Last Layer Logits to Logic: Empowering LLMs with Logic-Consistent Structured Knowledge Reasoning](https://arxiv.org/abs/2511.07910)
*Songze Li,Zhiqiang Liu,Zhaoyan Gong,Xiaoke Guo,Zhengke Gui,Huajun Chen,Wen Zhang*

Main category: cs.CL

TL;DR: 提出Logits-to-Logic框架，通过logits增强和过滤模块来纠正LLM在结构化知识推理中的逻辑漂移问题，显著提升逻辑一致性。


<details>
  <summary>Details</summary>
Motivation: LLM在非结构化文本预训练后能理解自然语言逻辑，但在结构化知识推理任务中因表示差异而出现逻辑漂移问题，现有方法仅提供输入级指导，无法从根本上解决输出中的逻辑不一致。

Method: 提出Logits-to-Logic框架，针对自回归生成过程中的logits输出，包含logits增强和logits过滤两个核心模块来纠正逻辑缺陷。

Result: 在多个KGQA基准测试中显著提升LLM的逻辑一致性，并达到最先进的性能。

Conclusion: Logits-to-Logic框架能有效解决LLM在结构化知识推理中的逻辑漂移问题，提高逻辑一致性表现。

Abstract: Large Language Models (LLMs) achieve excellent performance in natural language reasoning tasks through pre-training on vast unstructured text, enabling them to understand the logic in natural language and generate logic-consistent responses. However, the representational differences between unstructured and structured knowledge make LLMs inherently struggle to maintain logic consistency, leading to \textit{Logic Drift} challenges in structured knowledge reasoning tasks such as Knowledge Graph Question Answering (KGQA). Existing methods address this limitation by designing complex workflows embedded in prompts to guide LLM reasoning. Nevertheless, these approaches only provide input-level guidance and fail to fundamentally address the \textit{Logic Drift} in LLM outputs. Additionally, their inflexible reasoning workflows cannot adapt to different tasks and knowledge graphs. To enhance LLMs' logic consistency in structured knowledge reasoning, we specifically target the logits output from the autoregressive generation process. We propose the \textit{Logits-to-Logic} framework, which incorporates logits strengthening and logits filtering as core modules to correct logical defects in LLM outputs. Extensive experiments show that our approach significantly improves LLMs' logic consistency in structured knowledge reasoning and achieves state-of-the-art performance on multiple KGQA benchmarks.

</details>


### [21] [Social Media for Mental Health: Data, Methods, and Findings](https://arxiv.org/abs/2511.07914)
*Nur Shazwani Kamarudin,Ghazaleh Beigi,Lydia Manikonda,Huan Liu*

Main category: cs.CL

TL;DR: 本章综述了利用社交媒体数据研究心理健康问题（如抑郁、焦虑、自杀念头）的最新方法和发现，探讨了如何通过这些新方法提高对心理健康问题的认识。


<details>
  <summary>Details</summary>
Motivation: 随着网络虚拟社区和论坛的增加，人们可以在社交媒体上自由交流、分享想法、寻求同伴支持，特别是那些患有高度污名化疾病的人，而无需透露个人身份。这为研究心理健康问题提供了新的数据来源。

Method: 使用语言、视觉和情感指标分析用户披露内容，部署机器学习、特征工程、自然语言处理等方法，并对社交媒体数据进行分类调查。

Result: 研究表明社交媒体数据可以作为改善医疗实践、提供及时支持和影响政府决策的新数据源。

Conclusion: 社交媒体数据为心理健康研究提供了前所未有的机会，未来研究应继续探索如何更好地利用这些数据来改善心理健康服务和支持系统。

Abstract: There is an increasing number of virtual communities and forums available on the web. With social media, people can freely communicate and share their thoughts, ask personal questions, and seek peer-support, especially those with conditions that are highly stigmatized, without revealing personal identity. We study the state-of-the-art research methodologies and findings on mental health challenges like de- pression, anxiety, suicidal thoughts, from the pervasive use of social media data. We also discuss how these novel thinking and approaches can help to raise awareness of mental health issues in an unprecedented way. Specifically, this chapter describes linguistic, visual, and emotional indicators expressed in user disclosures. The main goal of this chapter is to show how this new source of data can be tapped to improve medical practice, provide timely support, and influence government or policymakers. In the context of social media for mental health issues, this chapter categorizes social media data used, introduces different deployed machine learning, feature engineering, natural language processing, and surveys methods and outlines directions for future research.

</details>


### [22] [Distinct Theta Synchrony across Speech Modes: Perceived, Spoken, Whispered, and Imagined](https://arxiv.org/abs/2511.07918)
*Jung-Sun Lee,Ha-Na Jo,Eunyeong Ko*

Main category: cs.CL

TL;DR: 本研究比较了不同语音模式（感知、出声、耳语和想象）下theta波段神经同步的差异，发现出声和耳语语音在前颞叶区域表现出更广泛和更强的同步性，感知语音主要在后部和颞叶区域同步，而想象语音则显示出更局限但内部一致的额叶和辅助运动区域同步模式。


<details>
  <summary>Details</summary>
Motivation: 人类语音生产包含多种模式，每种模式反映不同的神经机制。以往研究多集中于单一模式（如出声语音），很少对不同语音模式的theta同步性进行综合比较。本研究旨在阐明不同语音模式下theta波段神经同步的差异。

Method: 基于连接性指标分析不同语音模式下theta波段神经同步的差异，重点关注区域间的变化。

Result: 结果显示：出声和耳语语音表现出更广泛和更强的前颞叶同步性，反映出声发音过程中的运动-语音耦合；感知语音显示出主导的后部和颞叶同步模式，与听觉感知和理解过程一致；想象语音则表现出更空间局限但内部一致的同步模式，主要涉及额叶和辅助运动区域。

Conclusion: theta同步性的程度和空间分布在各种语音模式中存在显著差异，出声发音涉及广泛的皮层交互，耳语语音显示中等程度的参与，而感知主要依赖于颞顶网络。这些发现揭示了语言感知和想象语音下共享和不同的神经动力学。

Abstract: Human speech production encompasses multiple modes such as perceived, overt, whispered, and imagined, each reflecting distinct neural mechanisms. Among these, theta-band synchrony has been closely associated with language processing, attentional control, and inner speech. However, previous studies have largely focused on a single mode, such as overt speech, and have rarely conducted an integrated comparison of theta synchrony across different speech modes. In this study, we analyzed differences in theta-band neural synchrony across speech modes based on connectivity metrics, focusing on region-wise variations. The results revealed that overt and whispered speech exhibited broader and stronger frontotemporal synchrony, reflecting active motor-phonological coupling during overt articulation, whereas perceived speech showed dominant posterior and temporal synchrony patterns, consistent with auditory perception and comprehension processes. In contrast, imagined speech demonstrated a more spatially confined but internally coherent synchronization pattern, primarily involving frontal and supplementary motor regions. These findings indicate that the extent and spatial distribution of theta synchrony differ substantially across modes, with overt articulation engaging widespread cortical interactions, whispered speech showing intermediate engagement, and perception relying predominantly on temporoparietal networks. Therefore, this study aims to elucidate the differences in theta-band neural synchrony across various speech modes, thereby uncovering both the shared and distinct neural dynamics underlying language perception and imagined speech.

</details>


### [23] [Unified Work Embeddings: Contrastive Learning of a Bidirectional Multi-task Ranker](https://arxiv.org/abs/2511.07969)
*Matthias De Lange,Jens-Joris Decorte,Jeroen Van Hautte*

Main category: cs.CL

TL;DR: 提出了WorkBench评估套件和UWE模型，用于工作领域自然语言处理任务的统一评估和嵌入学习，在零样本场景下显著优于通用嵌入模型。


<details>
  <summary>Details</summary>
Motivation: 工作领域的自然语言处理任务具有长尾分布、多标签和稀缺数据等挑战，而现有研究主要关注单个任务，缺乏统一评估框架。

Method: 构建WorkBench评估套件包含6个工作相关任务，提出UWE双编码器模型，使用多对多InfoNCE目标函数和任务无关的软延迟交互。

Result: UWE在工作领域的零样本排序任务中表现优异，支持低延迟推理，在MAP和RP@10指标上显著优于通用嵌入模型。

Conclusion: 工作领域存在显著的跨任务迁移效应，UWE模型通过利用训练数据结构实现了工作领域任务的零样本排序能力。

Abstract: Workforce transformation across diverse industries has driven an increased demand for specialized natural language processing capabilities. Nevertheless, tasks derived from work-related contexts inherently reflect real-world complexities, characterized by long-tailed distributions, extreme multi-label target spaces, and scarce data availability. The rise of generalist embedding models prompts the question of their performance in the work domain, especially as progress in the field has focused mainly on individual tasks. To this end, we introduce WorkBench, the first unified evaluation suite spanning six work-related tasks formulated explicitly as ranking problems, establishing a common ground for multi-task progress. Based on this benchmark, we find significant positive cross-task transfer, and use this insight to compose task-specific bipartite graphs from real-world data, synthetically enriched through grounding. This leads to Unified Work Embeddings (UWE), a task-agnostic bi-encoder that exploits our training-data structure with a many-to-many InfoNCE objective, and leverages token-level embeddings with task-agnostic soft late interaction. UWE demonstrates zero-shot ranking performance on unseen target spaces in the work domain, enables low-latency inference by caching the task target space embeddings, and shows significant gains in macro-averaged MAP and RP@10 over generalist embedding models.

</details>


### [24] [NOTAM-Evolve: A Knowledge-Guided Self-Evolving Optimization Framework with LLMs for NOTAM Interpretation](https://arxiv.org/abs/2511.07982)
*Maoqi Liu,Quan Fang,Yuhao Wu,Can Zhao,Yang Yang,Kaiquan Cai*

Main category: cs.CL

TL;DR: NOTAM-Evolve是一个自我进化框架，通过知识图谱增强检索和闭环学习，让大语言模型自主掌握复杂的NOTAM（航行通告）深度解析，在结构化NOTAM解析任务上比基础LLM准确率提升30.4%。


<details>
  <summary>Details</summary>
Motivation: NOTAM的压缩和加密语言对人工和自动化处理都构成重大挑战，现有自动化系统通常仅限于浅层解析，无法提取操作决策所需的可操作情报。

Method: 提出NOTAM-Evolve框架，将NOTAM解析形式化为深度解析，结合动态知识接地和基于模式的推理，通过知识图谱增强检索模块和闭环学习过程，让LLM从自身输出中逐步改进。

Result: 在10,000个专家标注的NOTAM基准数据集上，NOTAM-Evolve比基础LLM绝对准确率提升30.4%，在结构化NOTAM解析任务上达到新的最先进水平。

Conclusion: NOTAM-Evolve框架能够有效解决NOTAM深度解析的挑战，通过自我进化机制显著提升了解析性能，为航空安全提供了更可靠的自动化解决方案。

Abstract: Accurate interpretation of Notices to Airmen (NOTAMs) is critical for aviation safety, yet their condensed and cryptic language poses significant challenges to both manual and automated processing. Existing automated systems are typically limited to shallow parsing, failing to extract the actionable intelligence needed for operational decisions. We formalize the complete interpretation task as deep parsing, a dual-reasoning challenge requiring both dynamic knowledge grounding (linking the NOTAM to evolving real-world aeronautical data) and schema-based inference (applying static domain rules to deduce operational status). To tackle this challenge, we propose NOTAM-Evolve, a self-evolving framework that enables a large language model (LLM) to autonomously master complex NOTAM interpretation. Leveraging a knowledge graph-enhanced retrieval module for data grounding, the framework introduces a closed-loop learning process where the LLM progressively improves from its own outputs, minimizing the need for extensive human-annotated reasoning traces. In conjunction with this framework, we introduce a new benchmark dataset of 10,000 expert-annotated NOTAMs. Our experiments demonstrate that NOTAM-Evolve achieves a 30.4% absolute accuracy improvement over the base LLM, establishing a new state of the art on the task of structured NOTAM interpretation.

</details>


### [25] [State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?](https://arxiv.org/abs/2511.07989)
*Taja Kuzman Pungeršek,Peter Rupnik,Ivan Porupski,Vuk Dinić,Nikola Ljubešić*

Main category: cs.CL

TL;DR: 评估当前语言模型在南斯拉夫语文本分类任务上的表现，比较微调BERT模型与开源/闭源LLM在情感分类、主题分类和体裁识别任务中的性能


<details>
  <summary>Details</summary>
Motivation: 随着指令调优的仅解码器模型（LLM）的兴起，文本分类领域逐渐转向零样本和少样本提示，但LLM在文本分类特别是低资源语言上的表现仍待探索

Method: 在南斯拉夫语文本分类任务上比较开源微调BERT模型与开源/闭源LLM，涵盖三个领域的三个任务：议会演讲情感分类、新闻和议会演讲主题分类、网络文本体裁识别

Result: LLM在零样本设置下表现出色，通常达到或超过微调BERT模型，且在南斯拉夫语和英语中表现相当，但存在输出不可预测、推理速度慢和计算成本高等缺点

Conclusion: 尽管LLM在零样本设置下表现优异，但由于输出不可预测性、推理速度慢和计算成本高等限制，微调BERT模型在大规模自动文本标注中仍是更实用的选择

Abstract: Until recently, fine-tuned BERT-like models provided state-of-the-art performance on text classification tasks. With the rise of instruction-tuned decoder-only models, commonly known as large language models (LLMs), the field has increasingly moved toward zero-shot and few-shot prompting. However, the performance of LLMs on text classification, particularly on less-resourced languages, remains under-explored. In this paper, we evaluate the performance of current language models on text classification tasks across several South Slavic languages. We compare openly available fine-tuned BERT-like models with a selection of open-source and closed-source LLMs across three tasks in three domains: sentiment classification in parliamentary speeches, topic classification in news articles and parliamentary speeches, and genre identification in web texts. Our results show that LLMs demonstrate strong zero-shot performance, often matching or surpassing fine-tuned BERT-like models. Moreover, when used in a zero-shot setup, LLMs perform comparably in South Slavic languages and English. However, we also point out key drawbacks of LLMs, including less predictable outputs, significantly slower inference, and higher computational costs. Due to these limitations, fine-tuned BERT-like models remain a more practical choice for large-scale automatic text annotation.

</details>


### [26] [Self-Correction Distillation for Structured Data Question Answering](https://arxiv.org/abs/2511.07998)
*Yushan Zhu,Wen Zhang,Long Jin,Mengshu Sun,Ling Zhong,Zhiqiang Liu,Juan Li,Lei Liang,Chong Long,Chao Deng,Junlan Feng*

Main category: cs.CL

TL;DR: 提出自校正蒸馏方法，通过错误提示机制和两阶段蒸馏策略，提升小规模LLM在结构化数据问答中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有统一结构化问答框架在小规模LLM上表现不佳，因为小规模LLM容易在生成结构化查询时出错。

Method: 设计错误提示机制检测错误并提供定制化错误信息，采用两阶段蒸馏策略将大规模LLM的查询生成和错误校正能力迁移到小规模LLM。

Result: 在5个基准测试中，SCD方法在小规模LLM上取得最佳性能，并在某些数据集上接近GPT4的表现。

Conclusion: SCD方法有效提升了小规模LLM在结构化数据问答中的能力，错误提示机制也能帮助大规模LLM达到最先进水平。

Abstract: Structured data question answering (QA), including table QA, Knowledge Graph (KG) QA, and temporal KG QA, is a pivotal research area. Advances in large language models (LLMs) have driven significant progress in unified structural QA frameworks like TrustUQA. However, these frameworks face challenges when applied to small-scale LLMs since small-scale LLMs are prone to errors in generating structured queries. To improve the structured data QA ability of small-scale LLMs, we propose a self-correction distillation (SCD) method. In SCD, an error prompt mechanism (EPM) is designed to detect errors and provide customized error messages during inference, and a two-stage distillation strategy is designed to transfer large-scale LLMs' query-generation and error-correction capabilities to small-scale LLM. Experiments across 5 benchmarks with 3 structured data types demonstrate that our SCD achieves the best performance and superior generalization on small-scale LLM (8B) compared to other distillation methods, and closely approaches the performance of GPT4 on some datasets. Furthermore, large-scale LLMs equipped with EPM surpass the state-of-the-art results on most datasets.

</details>


### [27] [HyCoRA: Hyper-Contrastive Role-Adaptive Learning for Role-Playing](https://arxiv.org/abs/2511.08017)
*Shihao Yang,Zhicong Lu,Yong Yang,Bo Lv,Yang Shen,Nayu Liu*

Main category: cs.CL

TL;DR: 提出了HyCoRA框架，通过超对比角色自适应学习来平衡角色独特特征和共享特征的学习，提升多角色扮演能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么使用共享参数模块（忽略角色独特性），要么使用角色特定模块（忽视共享特征），无法同时平衡角色独特性和共性建模。

Method: 采用超半低秩适应结构：一半是由轻量级超网络生成的角色特定模块，另一半是可训练的角色共享模块，并设计超对比学习机制来区分角色独特特征。

Result: 在英文和中文基准测试上的广泛实验结果表明该框架的优越性，GPT-4评估和可视化分析验证了其捕捉角色特征的能力。

Conclusion: HyCoRA框架有效平衡了角色独特特征和共享特征的学习，显著提升了多角色扮演能力。

Abstract: Multi-character role-playing aims to equip models with the capability to simulate diverse roles. Existing methods either use one shared parameterized module across all roles or assign a separate parameterized module to each role. However, the role-shared module may ignore distinct traits of each role, weakening personality learning, while the role-specific module may overlook shared traits across multiple roles, hindering commonality modeling. In this paper, we propose a novel HyCoRA: Hyper-Contrastive Role-Adaptive learning framework, which efficiently improves multi-character role-playing ability by balancing the learning of distinct and shared traits. Specifically, we propose a Hyper-Half Low-Rank Adaptation structure, where one half is a role-specific module generated by a lightweight hyper-network, and the other half is a trainable role-shared module. The role-specific module is devised to represent distinct persona signatures, while the role-shared module serves to capture common traits. Moreover, to better reflect distinct personalities across different roles, we design a hyper-contrastive learning mechanism to help the hyper-network distinguish their unique characteristics. Extensive experimental results on both English and Chinese available benchmarks demonstrate the superiority of our framework. Further GPT-4 evaluations and visual analyses also verify the capability of HyCoRA to capture role characteristics.

</details>


### [28] [BARD10: A New Benchmark Reveals Significance of Bangla Stop-Words in Authorship Attribution](https://arxiv.org/abs/2511.08085)
*Abdullah Muhammad Moosa,Nusrat Sultana,Mahdi Muhammad Moosa,Md. Miraiz Hossain*

Main category: cs.CL

TL;DR: 该研究提出了新的孟加拉语作者识别数据集BARD10，系统分析了停用词去除对传统和深度学习模型的影响，发现孟加拉语停用词是重要的文体风格指标。


<details>
  <summary>Details</summary>
Motivation: 研究孟加拉语作者识别，探索停用词在文体风格分析中的重要性，并建立新的平衡基准语料库。

Method: 构建BARD10数据集，使用SVM、Bangla BERT、XGBoost和MLP四种分类器，在BARD10和BAAD16数据集上进行统一预处理和对比分析。

Result: TF-IDF + SVM在BAAD16上达到0.997的宏F1分数，在BARD10上达到0.921，优于Bangla BERT。BARD10作者对停用词去除高度敏感，而BAAD16作者相对稳健。

Conclusion: 孟加拉语停用词是重要的文体风格指标；精细调校的机器学习模型在短文本限制下有效；BARD10连接了正式文学与当代网络对话，为未来研究提供了可复现的基准。

Abstract: This research presents a comprehensive investigation into Bangla authorship attribution, introducing a new balanced benchmark corpus BARD10 (Bangla Authorship Recognition Dataset of 10 authors) and systematically analyzing the impact of stop-word removal across classical and deep learning models to uncover the stylistic significance of Bangla stop-words. BARD10 is a curated corpus of Bangla blog and opinion prose from ten contemporary authors, alongside the methodical assessment of four representative classifiers: SVM (Support Vector Machine), Bangla BERT (Bidirectional Encoder Representations from Transformers), XGBoost, and a MLP (Multilayer Perception), utilizing uniform preprocessing on both BARD10 and the benchmark corpora BAAD16 (Bangla Authorship Attribution Dataset of 16 authors). In all datasets, the classical TF-IDF + SVM baseline outperformed, attaining a macro-F1 score of 0.997 on BAAD16 and 0.921 on BARD10, while Bangla BERT lagged by as much as five points. This study reveals that BARD10 authors are highly sensitive to stop-word pruning, while BAAD16 authors remain comparatively robust highlighting genre-dependent reliance on stop-word signatures. Error analysis revealed that high frequency components transmit authorial signatures that are diminished or reduced by transformer models. Three insights are identified: Bangla stop-words serve as essential stylistic indicators; finely calibrated ML models prove effective within short-text limitations; and BARD10 connects formal literature with contemporary web dialogue, offering a reproducible benchmark for future long-context or domain-adapted transformers.

</details>


### [29] [Estranged Predictions: Measuring Semantic Category Disruption with Masked Language Modelling](https://arxiv.org/abs/2511.08109)
*Yuxuan Liu,Haim Dubossarsky,Ruth Ahnert*

Main category: cs.CL

TL;DR: 使用掩码语言建模测量科幻小说中人类、动物和机器概念边界的渗透性，发现科幻小说通过语义扰动实现认知陌生化效果


<details>
  <summary>Details</summary>
Motivation: 验证Darko Suvin的陌生化理论，通过计算语言学方法量化科幻小说对本体论类别的解构作用

Method: 使用RoBERTa掩码语言模型生成词汇替代项，通过Gemini分类，采用保留率、替换率和熵三个指标量化概念渗透性

Result: 科幻小说表现出更高的概念渗透性，特别是机器指称具有显著的跨类别替换和分散，而人类术语保持语义连贯性

Conclusion: 科幻小说的陌生化效果体现为对语义规范的受控扰动，MLM可作为解释性工具揭示类型化本体论假设

Abstract: This paper examines how science fiction destabilises ontological categories by measuring conceptual permeability across the terms human, animal, and machine using masked language modelling (MLM). Drawing on corpora of science fiction (Gollancz SF Masterworks) and general fiction (NovelTM), we operationalise Darko Suvin's theory of estrangement as computationally measurable deviation in token prediction, using RoBERTa to generate lexical substitutes for masked referents and classifying them via Gemini. We quantify conceptual slippage through three metrics: retention rate, replacement rate, and entropy, mapping the stability or disruption of category boundaries across genres. Our findings reveal that science fiction exhibits heightened conceptual permeability, particularly around machine referents, which show significant cross-category substitution and dispersion. Human terms, by contrast, maintain semantic coherence and often anchor substitutional hierarchies. These patterns suggest a genre-specific restructuring within anthropocentric logics. We argue that estrangement in science fiction operates as a controlled perturbation of semantic norms, detectable through probabilistic modelling, and that MLMs, when used critically, serve as interpretive instruments capable of surfacing genre-conditioned ontological assumptions. This study contributes to the methodological repertoire of computational literary studies and offers new insights into the linguistic infrastructure of science fiction.

</details>


### [30] [Multimodal LLMs Do Not Compose Skills Optimally Across Modalities](https://arxiv.org/abs/2511.08113)
*Paula Ontalvilla,Aitor Ormazabal,Gorka Azkune*

Main category: cs.CL

TL;DR: 该论文研究了多模态大语言模型在跨模态技能组合方面的能力，发现现有模型存在显著的技能组合差距，并探索了思维链提示和微调策略来缓解这一问题。


<details>
  <summary>Details</summary>
Motivation: 随着神经网络在预训练中获得越来越复杂的技能，尚不清楚它们能否成功组合这些技能。本文重点关注多模态大语言模型，研究它们在跨模态中组合技能的能力。

Method: 设计了三个评估任务，通过顺序组合两个模态相关技能来解决；评估了多种开源MLLM，采用两种设置：直接提示模型解决任务，以及使用两步级联推理方法强制技能组合；探索了思维链提示和特定微调配方来提升技能组合能力。

Result: 即使对于这些简单的组合，所有评估的MLLM都表现出显著的跨模态技能组合差距。虽然思维链提示和微调策略能提高模型性能，但仍存在显著的技能组合差距。

Conclusion: 需要更多研究来改进MLLM中的跨模态技能组合能力，当前方法虽有改善但仍有明显不足。

Abstract: Skill composition is the ability to combine previously learned skills to solve new tasks. As neural networks acquire increasingly complex skills during their pretraining, it is not clear how successfully they can compose them. In this paper, we focus on Multimodal Large Language Models (MLLM), and study their ability to compose skills across modalities. To this end, we design three evaluation tasks which can be solved sequentially composing two modality-dependent skills, and evaluate several open MLLMs under two main settings: i) prompting the model to directly solve the task, and ii) using a two-step cascaded inference approach, which manually enforces the composition of the two skills for a given task. Even with these straightforward compositions, we find that all evaluated MLLMs exhibit a significant cross-modality skill composition gap. To mitigate the aforementioned gap, we explore two alternatives: i) use chain-of-thought prompting to explicitly instruct MLLMs for skill composition and ii) a specific fine-tuning recipe to promote skill composition. Although those strategies improve model performance, they still exhibit significant skill composition gaps, suggesting that more research is needed to improve cross-modal skill composition in MLLMs.

</details>


### [31] [Quantification and object perception in Multimodal Large Language Models deviate from human linguistic cognition](https://arxiv.org/abs/2511.08126)
*Raquel Montero,Natalia Moskvina,Paolo Morosi,Tamara Serrano,Elena Pagliarini,Evelina Leivada*

Main category: cs.CL

TL;DR: 本文研究了多模态大语言模型在量化表达方面的表现差异，发现模型与人类在量化尺度排序、使用范围和原型性、近似数字系统偏差等关键特征上存在明显差异。


<details>
  <summary>Details</summary>
Motivation: 量化表达是(M)LLMs特别困难的语言现象，但其性能差的确切原因尚不清楚。本文旨在探索人类量化表达的三个关键特征在模型中的编码方式，以及这些特征与人类的差异。

Method: 通过跨语言视角，研究量化表达的三个关键特征：量化尺度排序、使用范围和原型性、人类近似数字系统的内在偏差，分析这些特征在不同模型和语言中的表现。

Result: 发现在各种任务中，人类与MLLMs在这些量化特征上存在明显差异，揭示了模型在语义和语用能力方面的局限性。

Conclusion: 这项工作为理解MLLMs作为语义和语用代理的本质奠定了基础，跨语言视角可以阐明其能力在不同语言中的稳健性和稳定性。

Abstract: Quantification has been proven to be a particularly difficult linguistic phenomenon for (Multimodal) Large Language Models (MLLMs). However, given that quantification interfaces with the logic, pragmatic, and numerical domains, the exact reasons for the poor performance are still unclear. This papers looks at three key features of human quantification shared cross-linguistically that have remained so far unexplored in the (M)LLM literature: the ordering of quantifiers into scales, the ranges of use and prototypicality, and the biases inherent in the human approximate number system. The aim is to determine how these features are encoded in the models' architecture, how they may differ from humans, and whether the results are affected by the type of model and language under investigation. We find that there are clear differences between humans and MLLMs with respect to these features across various tasks that tap into the representation of quantification in vivo vs. in silico. This work, thus, paves the way for addressing the nature of MLLMs as semantic and pragmatic agents, while the cross-linguistic lens can elucidate whether their abilities are robust and stable across different languages.

</details>


### [32] [Sentence-Anchored Gist Compression for Long-Context LLMs](https://arxiv.org/abs/2511.08128)
*Dmitrii Tarasov,Elizaveta Goncharova,Kuznetsov Andrey*

Main category: cs.CL

TL;DR: 提出使用学习压缩令牌来压缩LLM上下文的方法，可将上下文压缩2-8倍而不显著影响性能


<details>
  <summary>Details</summary>
Motivation: 减少处理长序列时的内存和计算需求

Method: 对预训练LLM进行微调，使其学习压缩上下文为压缩令牌

Result: 在短上下文和长上下文基准测试中性能无明显下降，在30亿参数LLaMA模型上达到与其他压缩技术相当的结果且压缩比更高

Conclusion: 学习压缩令牌是有效的上下文压缩方法，可实现高压缩比且保持性能

Abstract: This work investigates context compression for Large Language Models (LLMs) using learned compression tokens to reduce the memory and computational demands of processing long sequences. We demonstrate that pre-trained LLMs can be fine-tuned to compress their context by factors of 2x to 8x without significant performance degradation, as evaluated on both short-context and long-context benchmarks. Furthermore, in experiments on a 3-billion-parameter LLaMA model, our method achieves results on par with alternative compression techniques while attaining higher compression ratios.

</details>


### [33] [On the Interplay between Positional Encodings, Morphological Complexity, and Word Order Flexibility](https://arxiv.org/abs/2511.08139)
*Kushal Tatariya,Wessel Poelman,Miryam de Lhoneux*

Main category: cs.CL

TL;DR: 研究探讨了语言模型架构中的位置编码选择是否对与英语结构不同的语言产生性能影响，通过分析形态复杂性与词序灵活性之间的权衡假设，在七种类型多样的语言上进行了实验。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型架构主要基于英语设计，然后应用于其他语言。研究人员希望了解这种架构偏见是否会导致与英语结构不同的语言性能下降，特别是通过位置编码这一具体架构选择来检验形态复杂性与词序灵活性之间的权衡假设。

Method: 在七种类型多样的语言上预训练了具有绝对、相对和无位置编码的单语模型变体，并在四个下游任务上评估这些模型。

Result: 与先前研究结果相反，未观察到位置编码与形态复杂性或词序灵活性之间存在明显的相互作用。结果显示任务选择、语言选择和度量标准对于得出稳定结论至关重要。

Conclusion: 位置编码的选择并不明显受到语言形态复杂性或词序灵活性的影响，研究强调了任务、语言和评估指标选择在得出可靠结论中的重要性。

Abstract: Language model architectures are predominantly first created for English and subsequently applied to other languages. It is an open question whether this architectural bias leads to degraded performance for languages that are structurally different from English. We examine one specific architectural choice: positional encodings, through the lens of the trade-off hypothesis: the supposed interplay between morphological complexity and word order flexibility. This hypothesis posits a trade-off between the two: a more morphologically complex language can have a more flexible word order, and vice-versa. Positional encodings are a direct target to investigate the implications of this hypothesis in relation to language modelling. We pretrain monolingual model variants with absolute, relative, and no positional encodings for seven typologically diverse languages and evaluate them on four downstream tasks. Contrary to previous findings, we do not observe a clear interaction between position encodings and morphological complexity or word order flexibility, as measured by various proxies. Our results show that the choice of tasks, languages, and metrics are essential for drawing stable conclusions

</details>


### [34] [Relation as a Prior: A Novel Paradigm for LLM-based Document-level Relation Extraction](https://arxiv.org/abs/2511.08143)
*Qiankun Pi,Yepeng Sun,Jicang Lu,Qinlong Fan,Ningbo Huang,Shiyu Wang*

Main category: cs.CL

TL;DR: 提出RelPrior新范式，通过关系先验过滤无关实体对并避免严格预定义关系标签的误判，在文档级关系抽取任务中实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在文档级关系抽取中存在性能差距，主要原因是：无关实体对引入噪声干扰；超出预定义集合的关系标签被误判为错误。

Method: RelPrior范式：利用二元关系先验筛选相关实体对；利用预定义关系先验进行三元组抽取而非直接预测关系。

Result: 在两个基准测试上的广泛实验表明，RelPrior实现了最先进的性能，超越了现有的基于LLM的方法。

Conclusion: RelPrior通过关系先验有效解决了文档级关系抽取中的噪声干扰和关系标签误判问题，显著提升了LLM在该任务上的表现。

Abstract: Large Language Models (LLMs) have demonstrated their remarkable capabilities in document understanding. However, recent research reveals that LLMs still exhibit performance gaps in Document-level Relation Extraction (DocRE) as requiring fine-grained comprehension. The commonly adopted "extract entities then predict relations" paradigm in LLM-based methods leads to these gaps due to two main reasons: (1) Numerous unrelated entity pairs introduce noise and interfere with the relation prediction for truly related entity pairs. (2) Although LLMs have identified semantic associations between entities, relation labels beyond the predefined set are still treated as prediction errors. To address these challenges, we propose a novel Relation as a Prior (RelPrior) paradigm for LLM-based DocRE. For challenge (1), RelPrior utilizes binary relation as a prior to extract and determine whether two entities are correlated, thereby filtering out irrelevant entity pairs and reducing prediction noise. For challenge (2), RelPrior utilizes predefined relation as a prior to match entities for triples extraction instead of directly predicting relation. Thus, it avoids misjudgment caused by strict predefined relation labeling. Extensive experiments on two benchmarks demonstrate that RelPrior achieves state-of-the-art performance, surpassing existing LLM-based methods.

</details>


### [35] [Still Not There: Can LLMs Outperform Smaller Task-Specific Seq2Seq Models on the Poetry-to-Prose Conversion Task?](https://arxiv.org/abs/2511.08145)
*Kunal Kingkar Das,Manoj Balaji Jagadeeshan,Nallani Chakravartula Sahith,Jivnesh Sandhan,Pawan Goyal*

Main category: cs.CL

TL;DR: 该研究比较了指令调优和上下文提示的大语言模型与专门任务模型在梵语诗歌转散文任务上的表现，发现领域特定的ByT5-Sanskrit模型显著优于所有LLM方法。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型在低资源、形态丰富的语言（如梵语）上的通用性，特别是在具有挑战性的诗歌转散文任务上，验证LLMs是否能超越专门模型。

Method: 使用指令微调的通用模型和基于帕尼尼语法及古典注释启发式的上下文学习模板，同时完全微调ByT5-Sanskrit序列到序列模型作为任务特定模型。

Result: 领域特定的ByT5-Sanskrit微调在所有指令驱动的LLM方法中表现最佳，人类评估结果与Kendall's Tau分数高度相关。提示策略在缺乏领域特定诗歌语料时提供了替代方案，任务特定模型在领域外评估中表现出稳健泛化能力。

Conclusion: 对于像梵语诗歌转散文这样的复杂任务，领域特定的微调模型比通用LLMs更有效，但提示策略在缺乏训练数据时提供了可行的替代方案。

Abstract: Large Language Models (LLMs) are increasingly treated as universal, general-purpose solutions across NLP tasks, particularly in English. But does this assumption hold for low-resource, morphologically rich languages such as Sanskrit? We address this question by comparing instruction-tuned and in-context-prompted LLMs with smaller task-specific encoder-decoder models on the Sanskrit poetry-to-prose conversion task. This task is intrinsically challenging: Sanskrit verse exhibits free word order combined with rigid metrical constraints, and its conversion to canonical prose (anvaya) requires multi-step reasoning involving compound segmentation, dependency resolution, and syntactic linearisation. This makes it an ideal testbed to evaluate whether LLMs can surpass specialised models. For LLMs, we apply instruction fine-tuning on general-purpose models and design in-context learning templates grounded in Paninian grammar and classical commentary heuristics. For task-specific modelling, we fully fine-tune a ByT5-Sanskrit Seq2Seq model. Our experiments show that domain-specific fine-tuning of ByT5-Sanskrit significantly outperforms all instruction-driven LLM approaches. Human evaluation strongly corroborates this result, with scores exhibiting high correlation with Kendall's Tau scores. Additionally, our prompting strategies provide an alternative to fine-tuning when domain-specific verse corpora are unavailable, and the task-specific Seq2Seq model demonstrates robust generalisation on out-of-domain evaluations.

</details>


### [36] [Do Syntactic Categories Help in Developmentally Motivated Curriculum Learning for Language Models?](https://arxiv.org/abs/2511.08199)
*Arzu Burcu Güven,Anna Rogers,Rob van der Goot*

Main category: cs.CL

TL;DR: 分析BabyLM语料库和CHILDES中不同年龄组的句法特性，发现CHILDES在年龄上的句法差异不大，但训练数据的句法知识有助于解释模型在语言任务上的表现。课程学习方面，探索了发展性和其他认知启发的课程方法，发现某些课程有助于阅读任务，但主要性能提升来自使用可句法分类的数据子集而非完整嘈杂语料库。


<details>
  <summary>Details</summary>
Motivation: 研究儿童语言习得语料库的句法特性，探索如何利用句法知识来理解和改进语言模型的性能表现，特别是通过课程学习方法来优化训练过程。

Method: 分析BabyLM和CHILDES语料库的句法特性，比较不同年龄组的句法差异，探索多种认知启发的课程学习方法，包括发展性课程和其他替代方法。

Result: CHILDES语料库在年龄维度上未表现出明显的句法差异；句法知识有助于解释模型在语言任务上的表现；某些课程学习方法对阅读任务有积极影响；使用可句法分类的数据子集比使用完整嘈杂语料库带来更大的性能提升。

Conclusion: 句法特性分析为理解语言模型表现提供了有价值的信息，课程学习特别是使用句法可分类数据子集的方法能有效提升模型性能，这为语言模型的训练优化提供了新的思路。

Abstract: We examine the syntactic properties of BabyLM corpus, and age-groups within CHILDES. While we find that CHILDES does not exhibit strong syntactic differentiation by age, we show that the syntactic knowledge about the training data can be helpful in interpreting model performance on linguistic tasks. For curriculum learning, we explore developmental and several alternative cognitively inspired curriculum approaches. We find that some curricula help with reading tasks, but the main performance improvement come from using the subset of syntactically categorizable data, rather than the full noisy corpus.

</details>


### [37] [Encoder Fine-tuning with Stochastic Sampling Outperforms Open-weight GPT in Astronomy Knowledge Extraction](https://arxiv.org/abs/2511.08204)
*Shivam Rawat,Lucie Flek,Akbar Karimi*

Main category: cs.CL

TL;DR: 提出了一种基于编码器的天文学文献知识提取系统，使用多任务Transformer架构在SciBERT基础上进行微调，用于分类望远镜引用、检测语义属性和识别仪器提及。


<details>
  <summary>Details</summary>
Motivation: 天文学文献快速扩张，需要自动化提取关键实体和上下文信息。

Method: 基于SciBERT模型构建多任务Transformer系统，通过随机采样训练数据片段进行微调，在推理时使用多数投票策略。

Result: 尽管系统简单且实现成本低，但显著优于开源的GPT基线模型。

Conclusion: 该系统为天文学文献的知识提取提供了一种有效且高效的解决方案。

Abstract: Scientific literature in astronomy is rapidly expanding, making it increasingly important to automate the extraction of key entities and contextual information from research papers. In this paper, we present an encoder-based system for extracting knowledge from astronomy articles. Our objective is to develop models capable of classifying telescope references, detecting auxiliary semantic attributes, and recognizing instrument mentions from textual content. To this end, we implement a multi-task transformer-based system built upon the SciBERT model and fine-tuned for astronomy corpora classification. To carry out the fine-tuning, we stochastically sample segments from the training data and use majority voting over the test segments at inference time. Our system, despite its simplicity and low-cost implementation, significantly outperforms the open-weight GPT baseline.

</details>


### [38] [Benchmarking Educational LLMs with Analytics: A Case Study on Gender Bias in Feedback](https://arxiv.org/abs/2511.08225)
*Yishan Du,Conrad Borchers,Mutlu Cukurova*

Main category: cs.CL

TL;DR: 提出了基于嵌入的基准测试框架，用于检测LLM在教学反馈中的性别偏见，发现即使最先进的模型对性别替换也表现出不对称的语义响应。


<details>
  <summary>Details</summary>
Motivation: 随着教师越来越多地在教育实践中使用GenAI，需要稳健的方法来为教学目的基准测试大型语言模型，特别是在形成性反馈中检测偏见。

Method: 使用600篇真实学生论文构建受控反事实，通过词汇替换和提示中的性别背景两个维度操纵性别线索，对6个代表性LLM进行测试，使用余弦和欧几里得距离量化响应差异，并通过置换测试评估显著性。

Result: 所有模型对隐式操纵都表现出男性-女性反事实比女性-男性更大的语义变化，只有GPT和Llama模型对显式性别线索敏感，发现一致的反馈语言差异（男性线索下更多自主支持性反馈，女性线索下更多控制性反馈）。

Conclusion: 即使最先进的LLM也表现出对性别替换的不对称语义响应，表明它们提供的反馈中存在持续的性别偏见，需要公平审计、报告标准和提示设计指导来确保公平反馈。

Abstract: As teachers increasingly turn to GenAI in their educational practice, we need robust methods to benchmark large language models (LLMs) for pedagogical purposes. This article presents an embedding-based benchmarking framework to detect bias in LLMs in the context of formative feedback. Using 600 authentic student essays from the AES 2.0 corpus, we constructed controlled counterfactuals along two dimensions: (i) implicit cues via lexicon-based swaps of gendered terms within essays, and (ii) explicit cues via gendered author background in the prompt. We investigated six representative LLMs (i.e. GPT-5 mini, GPT-4o mini, DeepSeek-R1, DeepSeek-R1-Qwen, Gemini 2.5 Pro, Llama-3-8B). We first quantified the response divergence with cosine and Euclidean distances over sentence embeddings, then assessed significance via permutation tests, and finally, visualised structure using dimensionality reduction. In all models, implicit manipulations reliably induced larger semantic shifts for male-female counterfactuals than for female-male. Only the GPT and Llama models showed sensitivity to explicit gender cues. These findings show that even state-of-the-art LLMs exhibit asymmetric semantic responses to gender substitutions, suggesting persistent gender biases in feedback they provide learners. Qualitative analyses further revealed consistent linguistic differences (e.g., more autonomy-supportive feedback under male cues vs. more controlling feedback under female cues). We discuss implications for fairness auditing of pedagogical GenAI, propose reporting standards for counterfactual evaluation in learning analytics, and outline practical guidance for prompt design and deployment to safeguard equitable feedback.

</details>


### [39] [VocalBench-zh: Decomposing and Benchmarking the Speech Conversational Abilities in Mandarin Context](https://arxiv.org/abs/2511.08230)
*Heyang Liu,Ziyang Cheng,Yuhao Wang,Hongcheng Liu,Yiqi Li,Ronghua Wu,Qunshan Gu,Yanfeng Wang,Yu Wang*

Main category: cs.CL

TL;DR: 提出了VocalBench-zh，一个针对普通话环境的语音到语音评估套件，包含10个子集和超过1万个高质量实例，覆盖12个用户导向特征。


<details>
  <summary>Details</summary>
Motivation: 普通话作为全球使用最广泛的语言之一，虽然大多数模型都支持普通话，但缺乏全面的语音到语音基准测试，阻碍了系统评估和公平模型比较。

Method: 开发了VocalBench-zh评估套件，包含10个精心设计的子集和超过10K个高质量实例，涵盖12个用户导向特征，并在14个主流模型上进行评估实验。

Result: 评估实验揭示了当前方法的共同挑战，并强调了需要为下一代语音交互系统提供新见解。

Conclusion: VocalBench-zh填补了普通话语音到语音评估的空白，为开发者和用户提供了系统评估和公平比较的工具，推动了语音交互系统的发展。

Abstract: The development of multi-modal large language models (LLMs) leads to intelligent approaches capable of speech interactions. As one of the most widely spoken languages globally, Mandarin is supported by most models to enhance their applicability and reach. However, the scarcity of comprehensive speech-to-speech (S2S) benchmarks in Mandarin contexts impedes systematic evaluation for developers and hinders fair model comparison for users. In this work, we propose VocalBench-zh, an ability-level divided evaluation suite adapted to Mandarin context consisting of 10 well-crafted subsets and over 10K high-quality instances, covering 12 user-oriented characters. The evaluation experiment on 14 mainstream models reveals the common challenges for current routes, and highlights the need for new insights into next-generation speech interactive systems. The evaluation codes and datasets will be available at https://github.com/SJTU-OmniAgent/VocalBench-zh.

</details>


### [40] [Prompt Tuning for Natural Language to SQL with Embedding Fine-Tuning and RAG](https://arxiv.org/abs/2511.08245)
*Jisoo Jang,Tien-Cuong Bui,Yunjun Choi,Wen-Syan Li*

Main category: cs.CL

TL;DR: 提出了一种通过提示调优进行错误校正的NL-to-SQL方法，结合生成式预训练LLMs和RAG技术，显著提升了自然语言查询转换为SQL表达式的准确性。


<details>
  <summary>Details</summary>
Motivation: 随着自然语言接口的广泛应用，需要高效准确地将自然语言查询转换为SQL表达式。现有方法在准确性和效率方面仍有改进空间。

Method: 提出集成错误校正机制的新框架，包括错误类型诊断、原因识别、修复指令生成和SQL查询修正。结合嵌入微调和RAG技术，利用外部知识库提升准确性和透明度。

Result: 通过全面实验验证，该框架相比现有基线方法实现了12%的准确率提升。

Conclusion: 该框架有潜力在当代数据驱动环境中革新数据访问和处理方式。

Abstract: This paper introduces an Error Correction through Prompt Tuning for NL-to-SQL, leveraging the latest advancements in generative pre-training-based LLMs and RAG. Our work addresses the crucial need for efficient and accurate translation of natural language queries into SQL expressions in various settings with the growing use of natural language interfaces. We explore the evolution of NLIDBs from early rule-based systems to advanced neural network-driven approaches. Drawing inspiration from the medical diagnostic process, we propose a novel framework integrating an error correction mechanism that diagnoses error types, identifies their causes, provides fixing instructions, and applies these corrections to SQL queries. This approach is further enriched by embedding fine-tuning and RAG, which harnesses external knowledge bases for improved accuracy and transparency. Through comprehensive experiments, we demonstrate that our framework achieves a significant 12 percent accuracy improvement over existing baselines, highlighting its potential to revolutionize data access and handling in contemporary data-driven environments.

</details>


### [41] [ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech](https://arxiv.org/abs/2511.08247)
*Marios Koniaris,Argyro Tsipi,Panayiotis Tsanakas*

Main category: cs.CL

TL;DR: 提出了ParliaBench基准测试，用于评估议会演讲生成模型，结合计算指标和LLM评估，在语言质量、语义连贯性和政治真实性三个维度进行综合评估。


<details>
  <summary>Details</summary>
Motivation: 议会演讲生成具有特定挑战，需要政治真实性和意识形态一致性，而现有语言模型缺乏专门训练，评估方法也忽视政治维度。

Method: 构建英国议会演讲数据集，提出结合计算指标和LLM评估的框架，引入政治光谱对齐和党派对齐两个新指标，对5个LLM进行微调并生成28k演讲进行评估。

Result: 微调在大多数指标上产生统计显著改进，新提出的政治维度指标展现出强区分能力。

Conclusion: ParliaBench为议会演讲生成提供了有效评估框架，微调能显著提升模型性能，新政治指标能有效衡量意识形态定位。

Abstract: Parliamentary speech generation presents specific challenges for large language models beyond standard text generation tasks. Unlike general text generation, parliamentary speeches require not only linguistic quality but also political authenticity and ideological consistency. Current language models lack specialized training for parliamentary contexts, and existing evaluation methods focus on standard NLP metrics rather than political authenticity. To address this, we present ParliaBench, a benchmark for parliamentary speech generation. We constructed a dataset of speeches from UK Parliament to enable systematic model training. We introduce an evaluation framework combining computational metrics with LLM-as-a-judge assessments for measuring generation quality across three dimensions: linguistic quality, semantic coherence, and political authenticity. We propose two novel embedding-based metrics, Political Spectrum Alignment and Party Alignment, to quantify ideological positioning. We fine-tuned five large language models (LLMs), generated 28k speeches, and evaluated them using our framework, comparing baseline and fine-tuned models. Results show that fine-tuning produces statistically significant improvements across the majority of metrics and our novel metrics demonstrate strong discriminative power for political dimensions.

</details>


### [42] [Hierarchical structure understanding in complex tables with VLLMs: a benchmark and experiments](https://arxiv.org/abs/2511.08298)
*Luca Bindini,Simone Giovannini,Simone Marinai,Valeria Nardoni,Kimiya Noor Ali*

Main category: cs.CL

TL;DR: 该研究探索了视觉大语言模型(VLLMs)理解科学论文中表格结构的能力，特别是推断表格层次结构的能力，使用PubTables-1M数据集中的复杂层次表格作为基准测试。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索通用VLLMs是否能够理解和解释科学文章中表格的结构，特别是层次结构，而无需额外处理。

Method: 采用多种提示工程策略，测试不同提示格式和写作风格，评估多个最先进的开源VLLMs，包括现成版本和微调版本，并与人类表现进行比较。

Result: 实验结果表明，未专门设计用于理解表格结构的通用VLLMs能够执行此任务，但存在局限性。

Conclusion: 该研究为VLLMs处理复杂表格的潜力和局限性提供了见解，并为未来将结构化数据理解集成到通用VLLMs中提供了指导。

Abstract: This work investigates the ability of Vision Large Language Models (VLLMs) to understand and interpret the structure of tables in scientific articles. Specifically, we explore whether VLLMs can infer the hierarchical structure of tables without additional processing. As a basis for our experiments we use the PubTables-1M dataset, a large-scale corpus of scientific tables. From this dataset, we extract a subset of tables that we introduce as Complex Hierarchical Tables (CHiTab): a benchmark collection of complex tables containing hierarchical headings. We adopt a series of prompt engineering strategies to probe the models' comprehension capabilities, experimenting with various prompt formats and writing styles. Multiple state-of-the-art open-weights VLLMs are evaluated on the benchmark first using their off-the-shelf versions and then fine-tuning some models on our task. We also measure the performance of humans to solve the task on a small set of tables comparing with performance of the evaluated VLLMs. The experiments support our intuition that generic VLLMs, not explicitly designed for understanding the structure of tables, can perform this task. This study provides insights into the potential and limitations of VLLMs to process complex tables and offers guidance for future work on integrating structured data understanding into general-purpose VLLMs.

</details>


### [43] [Automatic Paper Reviewing with Heterogeneous Graph Reasoning over LLM-Simulated Reviewer-Author Debates](https://arxiv.org/abs/2511.08317)
*Shuaimin Li,Liyang Fan,Yufang Lin,Zeyang Li,Xian Wei,Shiwen Ni,Hamid Alinejad-Rokny,Min Yang*

Main category: cs.CL

TL;DR: 提出ReViewGraph框架，通过图神经网络对LLM模拟的多轮审稿人-作者辩论进行异构图推理，显著提升论文评审性能


<details>
  <summary>Details</summary>
Motivation: 现有论文评审方法依赖表面特征或LLM直接生成，存在幻觉、评分偏见和有限推理能力的问题，且无法捕捉审稿人-作者交互中的复杂论证推理和协商动态

Method: 使用LLM多智能体协作模拟审稿人-作者多轮辩论，提取多样化意见关系作为类型化边构建异质交互图，应用图神经网络进行结构化辩论图推理

Result: 在三个数据集上的广泛实验表明，ReViewGraph优于强基线方法，平均相对改进达15.73%

Conclusion: 建模详细的审稿人-作者辩论结构具有重要价值，ReViewGraph通过图推理方法有效捕捉细粒度论证动态，实现更明智的评审决策

Abstract: Existing paper review methods often rely on superficial manuscript features or directly on large language models (LLMs), which are prone to hallucinations, biased scoring, and limited reasoning capabilities. Moreover, these methods often fail to capture the complex argumentative reasoning and negotiation dynamics inherent in reviewer-author interactions. To address these limitations, we propose ReViewGraph (Reviewer-Author Debates Graph Reasoner), a novel framework that performs heterogeneous graph reasoning over LLM-simulated multi-round reviewer-author debates. In our approach, reviewer-author exchanges are simulated through LLM-based multi-agent collaboration. Diverse opinion relations (e.g., acceptance, rejection, clarification, and compromise) are then explicitly extracted and encoded as typed edges within a heterogeneous interaction graph. By applying graph neural networks to reason over these structured debate graphs, ReViewGraph captures fine-grained argumentative dynamics and enables more informed review decisions. Extensive experiments on three datasets demonstrate that ReViewGraph outperforms strong baselines with an average relative improvement of 15.73%, underscoring the value of modeling detailed reviewer-author debate structures.

</details>


### [44] [Adaptive Multi-Agent Response Refinement in Conversational Systems](https://arxiv.org/abs/2511.08319)
*Soyeong Jeong,Aparna Elangovan,Emine Yilmaz,Oleg Rokhlenko*

Main category: cs.CL

TL;DR: 提出一个多智能体框架来优化LLM对话响应，通过三个专门智能体分别处理事实性、个性化和连贯性，采用动态通信策略自适应协调智能体协作。


<details>
  <summary>Details</summary>
Motivation: LLM在对话系统中生成类人响应很成功，但在个性化或特定知识方面存在不足，且用户难以检测错误并要求重新生成。现有方法在单一LLM内优化响应，难以有效考虑对话所需的多样化方面。

Method: 使用多智能体框架，每个智能体负责一个关键对话质量方面（事实性、个性化、连贯性），通过动态通信策略自适应选择和协调相关智能体，而非固定序列。

Result: 在具有挑战性的对话数据集上验证，该方法显著优于相关基线，特别是在涉及知识或用户画像的任务中表现突出。

Conclusion: 多智能体框架能有效提升LLM对话响应的质量，尤其在需要处理事实性、个性化和连贯性等多方面要求时表现优异。

Abstract: Large Language Models (LLMs) have demonstrated remarkable success in conversational systems by generating human-like responses. However, they can fall short, especially when required to account for personalization or specific knowledge. In real-life settings, it is impractical to rely on users to detect these errors and request a new response. One way to address this problem is to refine the response before returning it to the user. While existing approaches focus on refining responses within a single LLM, this method struggles to consider diverse aspects needed for effective conversations. In this work, we propose refining responses through a multi-agent framework, where each agent is assigned a specific role for each aspect. We focus on three key aspects crucial to conversational quality: factuality, personalization, and coherence. Each agent is responsible for reviewing and refining one of these aspects, and their feedback is then merged to improve the overall response. To enhance collaboration among them, we introduce a dynamic communication strategy. Instead of following a fixed sequence of agents, our approach adaptively selects and coordinates the most relevant agents based on the specific requirements of each query. We validate our framework on challenging conversational datasets, demonstrating that ours significantly outperforms relevant baselines, particularly in tasks involving knowledge or user's persona, or both.

</details>


### [45] [AgentPRM: Process Reward Models for LLM Agents via Step-Wise Promise and Progress](https://arxiv.org/abs/2511.08325)
*Zhiheng Xi,Chenyang Liao,Guanyu Li,Yajie Yang,Wenxiang Chen,Zhihao Zhang,Binghai Wang,Senjie Jin,Yuhao Zhou,Jian Guan,Wei Wu,Tao Ji,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.CL

TL;DR: 本文提出了一种名为AgentPRM的过程奖励模型，用于提升大语言模型在多轮决策任务中的性能。该方法通过评估每个决策对最终目标的贡献度，实现了更好的进度跟踪和探索-利用平衡。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多轮决策任务（如网络购物、浏览器导航）中仍面临挑战。现有方法依赖复杂的提示工程或专家轨迹微调，本文探索构建过程奖励模型来指导智能体的决策过程。

Method: 提出AgentPRM模型，捕捉序列决策间的相互依赖关系及其对最终目标的贡献。采用基于时间差分和广义优势估计的方法来高效获取训练数据。

Result: 在不同智能体任务上的实验表明，AgentPRM比基线方法计算效率高8倍以上，且在扩展测试时计算资源时表现出稳健的改进。

Conclusion: AgentPRM为多轮决策任务提供了一种有效的解决方案，能够显著提升大语言模型在智能体任务中的性能，并为强化学习应用提供了新的思路。

Abstract: Despite rapid development, large language models (LLMs) still encounter challenges in multi-turn decision-making tasks (i.e., agent tasks) like web shopping and browser navigation, which require making a sequence of intelligent decisions based on environmental feedback. Previous work for LLM agents typically relies on elaborate prompt engineering or fine-tuning with expert trajectories to improve performance. In this work, we take a different perspective: we explore constructing process reward models (PRMs) to evaluate each decision and guide the agent's decision-making process. Unlike LLM reasoning, where each step is scored based on correctness, actions in agent tasks do not have a clear-cut correctness. Instead, they should be evaluated based on their proximity to the goal and the progress they have made. Building on this insight, we propose a re-defined PRM for agent tasks, named AgentPRM, to capture both the interdependence between sequential decisions and their contribution to the final goal. This enables better progress tracking and exploration-exploitation balance. To scalably obtain labeled data for training AgentPRM, we employ a Temporal Difference-based (TD-based) estimation method combined with Generalized Advantage Estimation (GAE), which proves more sample-efficient than prior methods. Extensive experiments across different agentic tasks show that AgentPRM is over $8\times$ more compute-efficient than baselines, and it demonstrates robust improvement when scaling up test-time compute. Moreover, we perform detailed analyses to show how our method works and offer more insights, e.g., applying AgentPRM to the reinforcement learning of LLM agents.

</details>


### [46] [DPRM: A Dual Implicit Process Reward Model in Multi-Hop Question Answering](https://arxiv.org/abs/2511.08364)
*Xinyi Wang,Yiping Song,Zhiliang Tian,Bo Liu,Tingjin Luo,Minlie Huang*

Main category: cs.CL

TL;DR: 提出DPRM（双隐式过程奖励模型）来解决多跳问答任务中CoT和KG推理路径不一致的问题，通过训练两个隐式PRM分别处理CoT和KG推理，并引入一致性约束来优化推理路径。


<details>
  <summary>Details</summary>
Motivation: 在多跳问答任务中，现有的隐式过程奖励模型无法处理知识图谱的结构约束，也不能捕捉CoT和KG路径之间的潜在不一致性，这限制了其在多步推理中的有效性。

Method: 提出DPRM模型，训练两个隐式PRM：KG-PRM和CoT-PRM，分别从结果信号中推导步骤级奖励。KG-PRM使用偏好对学习KG的结构约束，并引入CoT和KG推理步骤之间的一致性约束，使两个PRM相互验证并协作优化推理路径。

Result: 实验结果表明，该方法在多个数据集上优于13个基线模型，在Hit@1指标上最高提升16.6%。

Conclusion: DPRM通过双隐式过程奖励模型有效解决了多跳问答中CoT和KG推理路径不一致的问题，显著提升了多步推理的性能。

Abstract: In multi-hop question answering (MHQA) tasks, Chain of Thought (CoT) improves the quality of generation by guiding large language models (LLMs) through multi-step reasoning, and Knowledge Graphs (KGs) reduce hallucinations via semantic matching. Outcome Reward Models (ORMs) provide feedback after generating the final answers but fail to evaluate the process for multi-step reasoning. Traditional Process Reward Models (PRMs) evaluate the reasoning process but require costly human annotations or rollout generation. While implicit PRM is trained only with outcome signals and derives step rewards through reward parameterization without explicit annotations, it is more suitable for multi-step reasoning in MHQA tasks. However, existing implicit PRM has only been explored for plain text scenarios. When adapting to MHQA tasks, it cannot handle the graph structure constraints in KGs and capture the potential inconsistency between CoT and KG paths. To address these limitations, we propose the DPRM (Dual Implicit Process Reward Model). It trains two implicit PRMs for CoT and KG reasoning in MHQA tasks. Both PRMs, namely KG-PRM and CoT-PRM, derive step-level rewards from outcome signals via reward parameterization without additional explicit annotations. Among them, KG-PRM uses preference pairs to learn structural constraints from KGs. DPRM further introduces a consistency constraint between CoT and KG reasoning steps, making the two PRMs mutually verify and collaboratively optimize the reasoning paths. We also provide a theoretical demonstration of the derivation of process rewards. Experimental results show that our method outperforms 13 baselines on multiple datasets with up to 16.6% improvement on Hit@1.

</details>


### [47] [The Dynamic Articulatory Model DYNARTmo: Dynamic Movement Generation and Speech Gestures](https://arxiv.org/abs/2511.08372)
*Bernd J. Kröger*

Main category: cs.CL

TL;DR: DYNARTmo动态发音模型的实现，基于语音手势概念生成连续发音器运动，模拟从语言表征到发音-声学实现的层级控制。


<details>
  <summary>Details</summary>
Motivation: 提供一个神经生物学启发的计算框架，模拟语音产生的层级控制过程，从语言表征到发音-声学的实现。

Method: 使用语音手势概念和手势乐谱，通过手势库结构、手势协调以及转换为连续发音器轨迹来控制DYNARTmo声道模型。

Result: 开发了DYNARTmo动态发音模型，能够生成连续的发音器运动轨迹。

Conclusion: DYNARTmo模型成功实现了基于语音手势的发音模拟框架，为语音产生研究提供了有效的计算工具。

Abstract: This paper describes the current implementation of the dynamic articulatory model DYNARTmo, which generates continuous articulator movements based on the concept of speech gestures and a corresponding gesture score. The model provides a neurobiologically inspired computational framework for simulating the hierarchical control of speech production from linguistic representation to articulatory-acoustic realization. We present the structure of the gesture inventory, the coordination of gestures in the gesture score, and their translation into continuous articulator trajectories controlling the DYNARTmo vocal tract model.

</details>


### [48] [TurkEmbed: Turkish Embedding Model on NLI & STS Tasks](https://arxiv.org/abs/2511.08376)
*Özay Ezerceli,Gizem Gümüşçekiçci,Tuğba Erkoç,Berke Özenç*

Main category: cs.CL

TL;DR: TurkEmbed是一种新的土耳其语嵌入模型，在自然语言推理和语义文本相似性任务上优于现有模型，通过多样化数据集和先进训练技术实现了1-4%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有土耳其语嵌入模型大多依赖机器翻译数据集，这限制了模型的准确性和语义理解能力，需要开发更精确的本地化模型。

Method: 结合多样化数据集，采用matryoshka表示学习等先进训练技术，使模型能够适应资源受限环境并提供更快的编码能力。

Result: 在土耳其STS-b-TR数据集上使用Pearson和Spearman相关性指标评估，在语义相似性任务中表现显著提升，在All-NLI-TR和STS-b-TR基准测试中超越当前最优模型Emrecan。

Conclusion: TurkEmbed通过提供更细致的语言理解，有望增强土耳其NLP生态系统，并促进下游应用的发展。

Abstract: This paper introduces TurkEmbed, a novel Turkish language embedding model designed to outperform existing models, particularly in Natural Language Inference (NLI) and Semantic Textual Similarity (STS) tasks. Current Turkish embedding models often rely on machine-translated datasets, potentially limiting their accuracy and semantic understanding. TurkEmbed utilizes a combination of diverse datasets and advanced training techniques, including matryoshka representation learning, to achieve more robust and accurate embeddings. This approach enables the model to adapt to various resource-constrained environments, offering faster encoding capabilities. Our evaluation on the Turkish STS-b-TR dataset, using Pearson and Spearman correlation metrics, demonstrates significant improvements in semantic similarity tasks. Furthermore, TurkEmbed surpasses the current state-of-the-art model, Emrecan, on All-NLI-TR and STS-b-TR benchmarks, achieving a 1-4\% improvement. TurkEmbed promises to enhance the Turkish NLP ecosystem by providing a more nuanced understanding of language and facilitating advancements in downstream applications.

</details>


### [49] [PCRLLM: Proof-Carrying Reasoning with Large Language Models under Stepwise Logical Constraints](https://arxiv.org/abs/2511.08392)
*Tangrui Li,Pei Wang,Hongzheng Wang Christian Hahm,Matteo Spatola,Justin Shi*

Main category: cs.CL

TL;DR: 提出了PCRLLM框架，通过单步推理和显式规则说明来增强LLM的逻辑推理能力，支持验证和多LLM协作


<details>
  <summary>Details</summary>
Motivation: 解决LLM在逻辑推理中缺乏明确推理规则和可验证性的问题

Method: PCRLLM框架，约束为单步推理，显式指定前提、规则和结论，支持验证和多LLM协作

Result: 开发了结合自然语言表达和形式严谨性的大规模推理数据生成基准模式

Conclusion: PCRLLM框架能够提升LLM推理的可信度和协作能力，为可靠推理系统提供新途径

Abstract: Large Language Models (LLMs) often exhibit limited logical coherence, mapping premises to conclusions without adherence to explicit inference rules. We propose Proof-Carrying Reasoning with LLMs (PCRLLM), a framework that constrains reasoning to single-step inferences while preserving natural language formulations. Each output explicitly specifies premises, rules, and conclusions, thereby enabling verification against a target logic. This mechanism mitigates trustworthiness concerns by supporting chain-level validation even in black-box settings. Moreover, PCRLLM facilitates systematic multi-LLM collaboration, allowing intermediate steps to be compared and integrated under formal rules. Finally, we introduce a benchmark schema for generating large-scale step-level reasoning data, combining natural language expressiveness with formal rigor.

</details>


### [50] [Interaction Dynamics as a Reward Signal for LLMs](https://arxiv.org/abs/2511.08394)
*Sian Gooding,Edward Grefenstette*

Main category: cs.CL

TL;DR: TRACE是一种基于对话嵌入轨迹几何特性的新型奖励信号，仅使用交互动态就能达到与完整文本分析相当的准确率，结合文本分析后性能最高。


<details>
  <summary>Details</summary>
Motivation: 现有LLM对齐方法主要依赖文本内容奖励信号，忽略了交互动态这一丰富互补的信号源。

Method: 提出TRACE方法，从对话嵌入轨迹的几何特性（称为'对话几何'）中提取奖励信号，训练仅基于结构信号的奖励模型。

Result: 仅使用结构信号的奖励模型达到68.20%的成对准确率，接近完整文本分析的LLM基线（70.04%）；结合文本分析的混合模型达到最高性能80.17%。

Conclusion: 在交互设置中，代理的沟通方式与沟通内容同样重要预测成功，提供了一种隐私保护的新框架，既能对齐代理，又能作为理解成功协作交互模式的诊断工具。

Abstract: The alignment of Large Language Models (LLMs) for multi-turn conversations typically relies on reward signals derived from the content of the text. This approach, however, overlooks a rich, complementary source of signal: the dynamics of the interaction itself. This paper introduces TRACE (Trajectory-based Reward for Agent Collaboration Estimation), a novel reward signal derived from the geometric properties of a dialogue's embedding trajectory--a concept we term 'conversational geometry'. Our central finding is that a reward model trained only on these structural signals achieves a pairwise accuracy (68.20%) comparable to a powerful LLM baseline that analyzes the full transcript (70.04%). Furthermore, a hybrid model combining interaction dynamics with textual analysis achieves the highest performance (80.17%), demonstrating their complementary nature. This work provides strong evidence that for interactive settings, how an agent communicates is as powerful a predictor of success as what it says, offering a new, privacy-preserving framework that not only aligns agents but also serves as a diagnostic tool for understanding the distinct interaction patterns that drive successful collaboration.

</details>


### [51] [Bot Meets Shortcut: How Can LLMs Aid in Handling Unknown Invariance OOD Scenarios?](https://arxiv.org/abs/2511.08455)
*Shiyan Zheng,Herun Wan,Minnan Luo,Junhang Huang*

Main category: cs.CL

TL;DR: 该论文研究了社交机器人检测器在真实场景中的鲁棒性问题，重点关注基于文本特征的捷径学习影响，并提出基于大语言模型的反事实数据增强缓解策略。


<details>
  <summary>Details</summary>
Motivation: 现有社交机器人检测器在基准测试中表现良好，但在多样化真实场景中鲁棒性有限，主要由于不清楚的真实标签和多样化的误导线索。特别是捷径学习（模型依赖虚假相关性而非因果特征）的影响研究不足。

Method: 设计了一系列捷径场景，通过构建用户标签与表面文本线索之间的虚假关联来评估模型鲁棒性。提出基于大语言模型的缓解策略，利用反事实数据增强从数据和模型三个层面解决问题。

Result: 结果显示无关特征分布的偏移显著降低了社交机器人检测器性能，基线模型平均相对准确率下降32%。提出的缓解策略在捷径场景下实现了平均相对性能提升56%。

Conclusion: 社交机器人检测器容易受到捷径学习的影响，基于大语言模型的反事实数据增强策略能有效提升模型在捷径场景下的鲁棒性。

Abstract: While existing social bot detectors perform well on benchmarks, their robustness across diverse real-world scenarios remains limited due to unclear ground truth and varied misleading cues. In particular, the impact of shortcut learning, where models rely on spurious correlations instead of capturing causal task-relevant features, has received limited attention. To address this gap, we conduct an in-depth study to assess how detectors are influenced by potential shortcuts based on textual features, which are most susceptible to manipulation by social bots. We design a series of shortcut scenarios by constructing spurious associations between user labels and superficial textual cues to evaluate model robustness. Results show that shifts in irrelevant feature distributions significantly degrade social bot detector performance, with an average relative accuracy drop of 32\% in the baseline models. To tackle this challenge, we propose mitigation strategies based on large language models, leveraging counterfactual data augmentation. These methods mitigate the problem from data and model perspectives across three levels, including data distribution at both the individual user text and overall dataset levels, as well as the model's ability to extract causal information. Our strategies achieve an average relative performance improvement of 56\% under shortcut scenarios.

</details>


### [52] [SPEAR-MM: Selective Parameter Evaluation and Restoration via Model Merging for Efficient Financial LLM Adaptation](https://arxiv.org/abs/2511.08500)
*Berkcan Kapusuzoglu,Supriyo Chakraborty,Renkun Ni,Stephen Rawls,Sambit Sahu*

Main category: cs.CL

TL;DR: SPEAR-MM是一个选择性参数评估和恢复框架，通过模型合并解决LLM在金融领域适应中的灾难性遗忘问题，在保持94%领域适应收益的同时实现91.2%通用能力保留。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在适应金融领域时经常出现灾难性遗忘，丧失对客户交互和复杂金融分析至关重要的通用推理能力。

Method: 通过事后分析近似层级对外部基准的影响，然后通过球面插值合并选择性冻结或恢复transformer层。

Result: 在LLaMA-3.1-8B上应用SPEAR-MM，通用能力保留率达到91.2%（标准持续预训练为69.7%），同时保持94%的领域适应收益，计算成本降低90%。

Conclusion: SPEAR-MM提供可解释的权衡控制，为资源受限的金融机构提供实用的领域适应解决方案。

Abstract: Large language models (LLMs) adapted to financial domains often suffer from catastrophic forgetting of general reasoning capabilities essential for customer interactions and complex financial analysis. We introduce Selective Parameter Evaluation and Restoration via Model Merging (SPEAR-MM), a practical framework that preserves critical capabilities while enabling domain adaptation. Our method approximates layer-wise impact on external benchmarks through post-hoc analysis, then selectively freezes or restores transformer layers via spherical interpolation merging. Applied to LLaMA-3.1-8B for financial tasks, SPEAR-MM achieves 91.2% retention of general capabilities versus 69.7% for standard continual pretraining, while maintaining 94% of domain adaptation gains. The approach provides interpretable trade-off control and reduces computational costs by 90% crucial for resource-constrained financial institutions.

</details>


### [53] [Structured RAG for Answering Aggregative Questions](https://arxiv.org/abs/2511.08505)
*Omri Koshorek,Niv Granot,Aviv Alloni,Shahar Admati,Roee Hendel,Ido Weiss,Alan Arazi,Shay-Nitzan Cohen,Yonatan Belinkov*

Main category: cs.CL

TL;DR: S-RAG是一种专门针对聚合查询的检索增强生成方法，通过构建语料库的结构化表示和将自然语言查询转换为形式化查询，显著优于传统RAG系统和长上下文LLM。


<details>
  <summary>Details</summary>
Motivation: 当前RAG方法主要关注小范围相关文档的查询，无法有效处理需要从大量文档中收集信息并进行推理的聚合查询。

Method: 在数据摄入时构建语料库的结构化表示，在推理时将自然语言查询转换为对该表示的形式化查询。

Result: 在新提出的HOTELS和WORLD CUP数据集以及公共基准测试中，S-RAG显著优于传统RAG系统和长上下文LLM。

Conclusion: S-RAG为聚合查询提供了有效的解决方案，填补了当前RAG方法在处理需要从大量文档中收集和推理信息的查询时的空白。

Abstract: Retrieval-Augmented Generation (RAG) has become the dominant approach for answering questions over large corpora. However, current datasets and methods are highly focused on cases where only a small part of the corpus (usually a few paragraphs) is relevant per query, and fail to capture the rich world of aggregative queries. These require gathering information from a large set of documents and reasoning over them. To address this gap, we propose S-RAG, an approach specifically designed for such queries. At ingestion time, S-RAG constructs a structured representation of the corpus; at inference time, it translates natural-language queries into formal queries over said representation. To validate our approach and promote further research in this area, we introduce two new datasets of aggregative queries: HOTELS and WORLD CUP. Experiments with S-RAG on the newly introduced datasets, as well as on a public benchmark, demonstrate that it substantially outperforms both common RAG systems and long-context LLMs.

</details>


### [54] [Introducing A Bangla Sentence - Gloss Pair Dataset for Bangla Sign Language Translation and Research](https://arxiv.org/abs/2511.08507)
*Neelavro Saha,Rafi Shahriyar,Nafis Ashraf Roudra,Saadman Sakib,Annajiat Alim Rasel*

Main category: cs.CL

TL;DR: 本文介绍了Bangla-SGP数据集，这是一个包含1000个人工标注和3000个合成生成的句子-手势对并行数据集，用于孟加拉手语翻译任务。作者还微调了多个基于transformer的模型，并评估了它们在句子到手势翻译中的性能。


<details>
  <summary>Details</summary>
Motivation: 孟加拉手语翻译是一个低资源NLP任务，由于缺乏大规模句子级翻译数据集，现有研究仅限于单词和字母级检测。

Method: 构建了包含1000个人工标注和3000个合成生成的句子-手势对数据集，使用基于规则的检索增强生成（RAG）流水线进行数据增强。微调了mBart50、Google mT5、GPT4.1-nano等transformer模型。

Result: 通过BLEU分数评估模型性能，并在Bangla-SGP数据集和RWTH-PHOENIX-2014T基准上比较模型的手势翻译一致性。

Conclusion: 提出的Bangla-SGP数据集和微调模型为孟加拉手语句子级翻译提供了有效解决方案，填补了该领域的研究空白。

Abstract: Bangla Sign Language (BdSL) translation represents a low-resource NLP task due to the lack of large-scale datasets that address sentence-level translation. Correspondingly, existing research in this field has been limited to word and alphabet level detection. In this work, we introduce Bangla-SGP, a novel parallel dataset consisting of 1,000 human-annotated sentence-gloss pairs which was augmented with around 3,000 synthetically generated pairs using syntactic and morphological rules through a rule-based Retrieval-Augmented Generation (RAG) pipeline. The gloss sequences of the spoken Bangla sentences are made up of individual glosses which are Bangla sign supported words and serve as an intermediate representation for a continuous sign. Our dataset consists of 1000 high quality Bangla sentences that are manually annotated into a gloss sequence by a professional signer. The augmentation process incorporates rule-based linguistic strategies and prompt engineering techniques that we have adopted by critically analyzing our human annotated sentence-gloss pairs and by working closely with our professional signer. Furthermore, we fine-tune several transformer-based models such as mBart50, Google mT5, GPT4.1-nano and evaluate their sentence-to-gloss translation performance using BLEU scores, based on these evaluation metrics we compare the model's gloss-translation consistency across our dataset and the RWTH-PHOENIX-2014T benchmark.

</details>


### [55] [AlphaResearch: Accelerating New Algorithm Discovery with Language Models](https://arxiv.org/abs/2511.08522)
*Zhaojian Yu,Kaiyue Feng,Yilun Zhao,Shilin He,Xiao-Ping Zhang,Arman Cohan*

Main category: cs.CL

TL;DR: AlphaResearch是一个自主研究代理，通过结合执行验证和模拟同行评审的双重研究环境，在开放性问题中发现新算法，在8个问题中获得2/8的胜率，并在"圆包装"问题上取得了最佳已知性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂但易于验证的问题上取得了显著进展，但在发现未知知识方面仍然存在困难。

Method: 构建双重研究环境（执行验证+模拟同行评审），通过迭代步骤发现新算法：提出想法→在双重环境中验证→优化研究提案。

Result: 在8个开放算法问题竞赛中，AlphaResearch获得2/8的胜率，在"圆包装"问题上超越了人类研究者和强基线方法，取得了最佳已知性能。

Conclusion: 证明了利用LLMs加速算法发现的可能性，并对6/8失败案例进行了全面分析，为未来研究提供了宝贵见解。

Abstract: Large language models have made significant progress in complex but easy-to-verify problems, yet they still struggle with discovering the unknown. In this paper, we present \textbf{AlphaResearch}, an autonomous research agent designed to discover new algorithms on open-ended problems. To synergize the feasibility and innovation of the discovery process, we construct a novel dual research environment by combining the execution-based verify and simulated real-world peer review environment. AlphaResearch discovers new algorithm by iteratively running the following steps: (1) propose new ideas (2) verify the ideas in the dual research environment (3) optimize the research proposals for better performance. To promote a transparent evaluation process, we construct \textbf{AlphaResearchComp}, a new evaluation benchmark that includes an eight open-ended algorithmic problems competition, with each problem carefully curated and verified through executable pipelines, objective metrics, and reproducibility checks. AlphaResearch gets a 2/8 win rate in head-to-head comparison with human researchers, demonstrate the possibility of accelerating algorithm discovery with LLMs. Notably, the algorithm discovered by AlphaResearch on the \emph{``packing circles''} problem achieves the best-of-known performance, surpassing the results of human researchers and strong baselines from recent work (e.g., AlphaEvolve). Additionally, we conduct a comprehensive analysis of the remaining challenges of the 6/8 failure cases, providing valuable insights for future research.

</details>


### [56] [Investigating CoT Monitorability in Large Reasoning Models](https://arxiv.org/abs/2511.08525)
*Shu Yang,Junchao Wu,Xilin Gou,Xuansheng Wu,Derek Wong,Ninhao Liu,Di Wang*

Main category: cs.CL

TL;DR: 本文首次系统研究了CoT可监控性的挑战与潜力，围绕两个核心视角展开：语言化忠实度和监控可靠性，并提出了MoME监控范式


<details>
  <summary>Details</summary>
Motivation: 大型推理模型通过详细推理链创造了AI安全的新机会——CoT可监控性，但面临模型不忠实表达内部决策和监控器敏感性不足两大挑战

Method: 通过数学、科学和伦理领域的实证研究和相关性分析，探讨语言化质量、监控可靠性与LLM性能的关系，研究不同CoT干预方法对监控效果的影响，并提出MoME监控范式

Result: 提供了语言化质量、监控可靠性和LLM性能之间的经验证据和相关性分析，展示了不同CoT干预方法对监控有效性的影响

Conclusion: CoT监控在AI安全中具有重要潜力，但需要解决语言化忠实度和监控可靠性问题，MoME范式为模型间监控提供了新方法

Abstract: Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex tasks by engaging in extended reasoning before producing final answers. Beyond improving abilities, these detailed reasoning traces also create a new opportunity for AI safety, CoT Monitorability: monitoring potential model misbehavior, such as the use of shortcuts or sycophancy, through their chain-of-thought (CoT) during decision-making. However, two key fundamental challenges arise when attempting to build more effective monitors through CoT analysis. First, as prior research on CoT faithfulness has pointed out, models do not always truthfully represent their internal decision-making in the generated reasoning. Second, monitors themselves may be either overly sensitive or insufficiently sensitive, and can potentially be deceived by models' long, elaborate reasoning traces. In this paper, we present the first systematic investigation of the challenges and potential of CoT monitorability. Motivated by two fundamental challenges we mentioned before, we structure our study around two central perspectives: (i) verbalization: to what extent do LRMs faithfully verbalize the true factors guiding their decisions in the CoT, and (ii) monitor reliability: to what extent can misbehavior be reliably detected by a CoT-based monitor? Specifically, we provide empirical evidence and correlation analyses between verbalization quality, monitor reliability, and LLM performance across mathematical, scientific, and ethical domains. Then we further investigate how different CoT intervention methods, designed to improve reasoning efficiency or performance, will affect monitoring effectiveness. Finally, we propose MoME, a new paradigm in which LLMs monitor other models' misbehavior through their CoT and provide structured judgments along with supporting evidence.

</details>


### [57] [From Semantic Roles to Opinion Roles: SRL Data Extraction for Multi-Task and Transfer Learning in Low-Resource ORL](https://arxiv.org/abs/2511.08537)
*Amirmohammad Omidi Galdiani,Sepehr Rezaei Melal,Mohammad Norasteh,Arash Yousefi Jordehi,Seyed Abolghasem Mirroshandel*

Main category: cs.CL

TL;DR: 构建了从WSJ语料到ORL任务的高质量语义角色标注数据集，包含97,169个谓词-论元实例，实现了从SRL到ORL的角色映射。


<details>
  <summary>Details</summary>
Motivation: 为研究者提供可复用的资源，利用SRL增强ORL任务，特别是在低资源意见挖掘场景中。

Method: 基于PropBank框架，实现可复现的提取流程：对齐谓词-论元结构与表层文本，转换句法树指针为连贯跨度，并进行严格清洗以确保语义保真度。

Result: 成功构建包含97,169个谓词-论元实例的数据集，清晰定义了Agent(ARG0)、Predicate(REL)、Patient(ARG1)角色，并映射到ORL的Holder、Expression、Target模式。

Conclusion: 这项工作为利用SRL增强ORL提供了可重用的数据集资源，特别适用于低资源意见挖掘场景。

Abstract: This report presents a detailed methodology for constructing a high-quality Semantic Role Labeling (SRL) dataset from the Wall Street Journal (WSJ) portion of the OntoNotes 5.0 corpus and adapting it for Opinion Role Labeling (ORL) tasks. Leveraging the PropBank annotation framework, we implement a reproducible extraction pipeline that aligns predicate-argument structures with surface text, converts syntactic tree pointers to coherent spans, and applies rigorous cleaning to ensure semantic fidelity. The resulting dataset comprises 97,169 predicate-argument instances with clearly defined Agent (ARG0), Predicate (REL), and Patient (ARG1) roles, mapped to ORL's Holder, Expression, and Target schema. We provide a detailed account of our extraction algorithms, discontinuous argument handling, annotation corrections, and statistical analysis of the resulting dataset. This work offers a reusable resource for researchers aiming to leverage SRL for enhancing ORL, especially in low-resource opinion mining scenarios.

</details>


### [58] [Moral Susceptibility and Robustness under Persona Role-Play in Large Language Models](https://arxiv.org/abs/2511.08565)
*Davi Bastos Costa,Felippe Alves,Renato Vicente*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型在角色扮演情境下的道德响应，通过道德基础问卷量化了道德易感性和道德鲁棒性，发现不同模型家族在道德鲁棒性上差异显著，而模型大小主要影响道德易感性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型越来越多地在社会环境中使用，需要分析它们如何表达和改变道德判断，特别是在角色扮演情境下的道德响应。

Method: 使用道德基础问卷(MFQ)构建基准，通过角色扮演提示让LLM扮演特定角色，量化道德易感性(跨角色变异性)和道德鲁棒性(角色内一致性)两个属性。

Result: Claude家族在道德鲁棒性上显著优于其他模型，其次是Gemini和GPT-4；模型大小对道德鲁棒性无系统性影响，但对道德易感性有明确影响，更大模型更易受影响；道德鲁棒性和易感性呈正相关。

Conclusion: 角色调节显著影响大型语言模型的道德行为，不同模型家族在道德响应特性上存在系统性差异，这为理解LLM在社会环境中的道德表现提供了系统视角。

Abstract: Large language models (LLMs) increasingly operate in social contexts, motivating analysis of how they express and shift moral judgments. In this work, we investigate the moral response of LLMs to persona role-play, prompting a LLM to assume a specific character. Using the Moral Foundations Questionnaire (MFQ), we introduce a benchmark that quantifies two properties: moral susceptibility and moral robustness, defined from the variability of MFQ scores across and within personas, respectively. We find that, for moral robustness, model family accounts for most of the variance, while model size shows no systematic effect. The Claude family is, by a significant margin, the most robust, followed by Gemini and GPT-4 models, with other families exhibiting lower robustness. In contrast, moral susceptibility exhibits a mild family effect but a clear within-family size effect, with larger variants being more susceptible. Moreover, robustness and susceptibility are positively correlated, an association that is more pronounced at the family level. Additionally, we present moral foundation profiles for models without persona role-play and for personas averaged across models. Together, these analyses provide a systematic view of how persona conditioning shapes moral behavior in large language models.

</details>


### [59] [Think-at-Hard: Selective Latent Iterations to Improve Reasoning Language Models](https://arxiv.org/abs/2511.08577)
*Tianyu Fu,Yichen You,Zekai Chen,Guohao Dai,Huazhong Yang,Yu Wang*

Main category: cs.CL

TL;DR: 提出Think-at-Hard (TaH)方法，通过动态潜在思考机制，仅在困难token上进行深度迭代，避免简单token被过度修正的错误，提升LLM推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有循环transformer对所有token固定额外迭代次数，导致简单token预测在后续迭代中被错误修正的潜在过度思考问题。

Method: 使用轻量级神经网络决策器识别可能错误的困难token，仅在这些token上触发潜在迭代；引入LoRA模块将目标从通用预测转向困难token精炼；采用双因果注意力机制实现跨迭代信息流。

Result: 在五个挑战性基准测试中提升推理性能，相比对所有token迭代两次的基线方法，准确率提升8.1-11.3%，同时免除94%token的第二次迭代。

Conclusion: TaH方法能有效提升LLM推理能力，避免过度思考问题，在保持参数数量不变的情况下实现显著性能提升。

Abstract: Improving reasoning capabilities of Large Language Models (LLMs), especially under parameter constraints, is crucial for real-world applications. Prior work proposes recurrent transformers, which allocate a fixed number of extra iterations per token to improve generation quality. After the first, standard forward pass, instead of verbalization, last-layer hidden states are fed back as inputs for additional iterations to refine token predictions. Yet we identify a latent overthinking phenomenon: easy token predictions that are already correct after the first pass are sometimes revised into errors in additional iterations. To address this, we propose Think-at-Hard (TaH), a dynamic latent thinking method that iterates deeper only at hard tokens. It employs a lightweight neural decider to trigger latent iterations only at tokens that are likely incorrect after the standard forward pass. During latent iterations, Low-Rank Adaptation (LoRA) modules shift the LLM objective from general next-token prediction to focused hard-token refinement. We further introduce a duo-causal attention mechanism that extends attention from the token sequence dimension to an additional iteration depth dimension. This enables cross-iteration information flow while maintaining full sequential parallelism. Experiments show that TaH boosts LLM reasoning performance across five challenging benchmarks while maintaining the same parameter count. Compared with baselines that iterate twice for all output tokens, TaH delivers 8.1-11.3% accuracy gains while exempting 94% of tokens from the second iteration. Against strong single-iteration Qwen3 models finetuned with the same data, it also delivers 4.0-5.0% accuracy gains. When allowing less than 3% additional parameters from LoRA and the iteration decider, the gains increase to 8.5-12.6% and 5.3-5.4%, respectively. Our code is available at https://github.com/thu-nics/TaH.

</details>


### [60] [Training Language Models to Explain Their Own Computations](https://arxiv.org/abs/2511.08579)
*Belinda Z. Li,Zifan Carl Guo,Vincent Huang,Jacob Steinhardt,Jacob Andreas*

Main category: cs.CL

TL;DR: 语言模型可以通过微调学习用自然语言描述自己的内部计算过程，包括特征编码信息、激活的因果结构和输入标记对输出的影响，且自我解释优于其他模型的外部解释。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型是否能利用其对自己内部状态的独特访问权来产生新的解释技术，以补充现有的可解释性方法。

Method: 使用现有可解释性技术作为真实标签，微调语言模型生成自然语言描述，包括特征编码信息、激活因果结构和输入标记影响。

Result: 仅用数万个解释示例训练的解释模型对新查询展现出非平凡的泛化能力，自我解释通常优于外部模型解释。

Conclusion: 语言模型能够可靠地解释其内部计算，这种解释方法为现有可解释性方法提供了可扩展的补充。

Abstract: Can language models (LMs) learn to faithfully describe their internal computations? Are they better able to describe themselves than other models? We study the extent to which LMs' privileged access to their own internals can be leveraged to produce new techniques for explaining their behavior. Using existing interpretability techniques as a source of ground truth, we fine-tune LMs to generate natural language descriptions of (1) the information encoded by LM features, (2) the causal structure of LMs' internal activations, and (3) the influence of specific input tokens on LM outputs. When trained with only tens of thousands of example explanations, explainer models exhibit non-trivial generalization to new queries. This generalization appears partly attributable to explainer models' privileged access to their own internals: using a model to explain its own computations generally works better than using a *different* model to explain its computations (even if the other model is significantly more capable). Our results suggest not only that LMs can learn to reliably explain their internal computations, but that such explanations offer a scalable complement to existing interpretability methods.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [61] [LLM-Powered Fully Automated Chaos Engineering: Towards Enabling Anyone to Build Resilient Software Systems at Low Cost](https://arxiv.org/abs/2511.07865)
*Daisuke Kikuta,Hiroki Ikeuchi,Kengo Tajiri*

Main category: cs.SE

TL;DR: ChaosEater是一个使用大型语言模型自动化混沌工程全周期的系统，针对Kubernetes软件系统，通过定义需求、代码生成、测试和调试等软件工程任务完成混沌工程实验。


<details>
  <summary>Details</summary>
Motivation: 当前混沌工程工具虽然能自动化执行实验，但实验规划和基于结果改进系统仍需人工操作，这些过程劳动密集且需要多领域专业知识。

Method: ChaosEater预先定义了一个系统化的混沌工程循环代理工作流，将工作流中的细分流程分配给LLMs完成，包括需求定义、代码生成、测试和调试等软件工程任务。

Result: 通过对小型和大型Kubernetes系统的案例研究评估，ChaosEater能够以显著较低的时间和金钱成本持续完成合理的混沌工程循环，其循环质量得到了人类工程师和LLMs的验证。

Conclusion: ChaosEater成功实现了混沌工程全周期的自动化，使任何人都能以低成本构建弹性系统，解决了当前混沌工程中人工干预过多的问题。

Abstract: Chaos Engineering (CE) is an engineering technique aimed at improving the resilience of distributed systems. It involves intentionally injecting faults into a system to test its resilience, uncover weaknesses, and address them before they cause failures in production. Recent CE tools automate the execution of predefined CE experiments. However, planning such experiments and improving the system based on the experimental results still remain manual. These processes are labor-intensive and require multi-domain expertise. To address these challenges and enable anyone to build resilient systems at low cost, this paper proposes ChaosEater, a system that automates the entire CE cycle with Large Language Models (LLMs). It predefines an agentic workflow according to a systematic CE cycle and assigns subdivided processes within the workflow to LLMs. ChaosEater targets CE for software systems built on Kubernetes. Therefore, the LLMs in ChaosEater complete CE cycles through software engineering tasks, including requirement definition, code generation, testing, and debugging. We evaluate ChaosEater through case studies on small- and large-scale Kubernetes systems. The results demonstrate that it consistently completes reasonable CE cycles with significantly low time and monetary costs. Its cycles are also qualitatively validated by human engineers and LLMs.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [62] [Network and Systems Performance Characterization of MCP-Enabled LLM Agents](https://arxiv.org/abs/2511.07426)
*Zihao Ding,Mufeng Zhu,Yao Liu*

Main category: cs.DC

TL;DR: 本文对MCP增强的LLM交互进行了测量分析，揭示了能力、性能和成本之间的权衡关系，并提出了优化建议。


<details>
  <summary>Details</summary>
Motivation: MCP虽然增强了LLM与外部工具交互的能力，但包含大量上下文信息会显著增加token使用量，导致成本上升和计算负载增加。

Method: 通过基于测量的分析方法，评估不同LLM模型和MCP配置对token效率、成本、任务完成时间和成功率等关键指标的影响。

Result: 研究发现MCP交互存在能力与成本的权衡，并识别了影响性能的关键因素。

Conclusion: 提出了启用并行工具调用和实现健壮任务中止机制等优化方案，为开发更高效、健壮且成本效益高的MCP工作流程提供了有用见解。

Abstract: Model Context Protocol (MCP) has recently gained increased attention within the AI community for providing a standardized way for large language models (LLMs) to interact with external tools and services, significantly enhancing their capabilities. However, the inclusion of extensive contextual information, including system prompts, MCP tool definitions, and context histories, in MCP-enabled LLM interactions, dramatically inflates token usage. Given that LLM providers charge based on tokens, these expanded contexts can quickly escalate monetary costs and increase the computational load on LLM services. This paper presents a comprehensive measurement-based analysis of MCP-enabled interactions with LLMs, revealing trade-offs between capability, performance, and cost. We explore how different LLM models and MCP configurations impact key performance metrics such as token efficiency, monetary cost, task completion times, and task success rates, and suggest potential optimizations, including enabling parallel tool calls and implementing robust task abort mechanisms. These findings provide useful insights for developing more efficient, robust, and cost-effective MCP-enabled workflows.

</details>


### [63] [Intelligence per Watt: Measuring Intelligence Efficiency of Local AI](https://arxiv.org/abs/2511.07885)
*Jon Saad-Falcon,Avanika Narayan,Hakki Orhun Akengin,J. Wes Griffin,Herumb Shandilya,Adrian Gamarra Lafuente,Medhya Goel,Rebecca Joseph,Shlok Natarajan,Etash Kumar Guha,Shang Zhu,Ben Athiwaratkun,John Hennessy,Azalia Mirhoseini,Christopher Ré*

Main category: cs.DC

TL;DR: 本地语言模型（≤20B参数）现在能在许多任务上达到与前沿模型竞争的性能，且本地加速器能以交互式延迟运行这些模型。研究提出智能每瓦（IPW）作为评估本地推理能力和效率的指标，发现本地模型能准确回答88.7%的单轮聊天和推理查询，2023-2025年IPW提升5.3倍，本地查询覆盖率从23.2%增至71.3%。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型查询主要在集中式云基础设施中处理，快速增长的需求使该模式面临压力。小型LM的性能提升和本地加速器的发展使得重新思考这一范式成为可能：本地推理能否可行地将需求从集中式基础设施中重新分配？

Method: 提出智能每瓦（IPW）作为评估本地推理能力和效率的指标，对20+个先进本地LM、8个加速器和100万个真实世界单轮聊天和推理查询进行大规模实证研究，测量每个查询的准确性、能耗、延迟和功率。

Result: 1) 本地LM能准确回答88.7%的单轮聊天和推理查询；2) 2023-2025年IPW提升5.3倍，本地查询覆盖率从23.2%增至71.3%；3) 本地加速器的IPW比运行相同模型的云加速器至少低1.4倍。

Conclusion: 本地推理能够有意义地将需求从集中式基础设施中重新分配，IPW是跟踪这一转变的关键指标。研究发布了IPW分析工具用于系统性的智能每瓦基准测试。

Abstract: Large language model (LLM) queries are predominantly processed by frontier models in centralized cloud infrastructure. Rapidly growing demand strains this paradigm, and cloud providers struggle to scale infrastructure at pace. Two advances enable us to rethink this paradigm: small LMs (<=20B active parameters) now achieve competitive performance to frontier models on many tasks, and local accelerators (e.g., Apple M4 Max) run these models at interactive latencies. This raises the question: can local inference viably redistribute demand from centralized infrastructure? Answering this requires measuring whether local LMs can accurately answer real-world queries and whether they can do so efficiently enough to be practical on power-constrained devices (i.e., laptops). We propose intelligence per watt (IPW), task accuracy divided by unit of power, as a metric for assessing capability and efficiency of local inference across model-accelerator pairs. We conduct a large-scale empirical study across 20+ state-of-the-art local LMs, 8 accelerators, and a representative subset of LLM traffic: 1M real-world single-turn chat and reasoning queries. For each query, we measure accuracy, energy, latency, and power. Our analysis reveals $3$ findings. First, local LMs can accurately answer 88.7% of single-turn chat and reasoning queries with accuracy varying by domain. Second, from 2023-2025, IPW improved 5.3x and local query coverage rose from 23.2% to 71.3%. Third, local accelerators achieve at least 1.4x lower IPW than cloud accelerators running identical models, revealing significant headroom for optimization. These findings demonstrate that local inference can meaningfully redistribute demand from centralized infrastructure, with IPW serving as the critical metric for tracking this transition. We release our IPW profiling harness for systematic intelligence-per-watt benchmarking.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [64] [Quantifying the Impact of CU: A Systematic Literature Review](https://arxiv.org/abs/2511.07491)
*Thomas Compton*

Main category: cs.DL

TL;DR: 本文通过引文网络分析和主题综述，探讨了社区工会主义(CU)为何在工会复兴辩论中占据重要地位，揭示了其作为经验描述和规范理想的双重功能，以及处理工会运动内部矛盾的作用。


<details>
  <summary>Details</summary>
Motivation: 社区工会主义自2000年代初以来一直是工会复兴辩论的核心概念，但其理论连贯性和政治意义仍未解决。本文旨在探究CU为何如此突出，不是通过测试其有效性，而是通过分析其在学术文献中的构建、引用和争议方式。

Method: 采用两种互补的系统方法：对114份文献进行引文网络分析，以及对18个核心CU案例研究进行主题综述。

Result: 分析揭示了CU的双重谱系：被英国学者定位为对历史基层实践的本土回归，但在结构上与跨国社会运动工会主义一致。主题编码显示几乎普遍强调联盟建设，但对阶级政治存在深刻矛盾心理。

Conclusion: CU的意义不在于实施新的工会模式，而更多在于管理萎缩的劳工运动内部的矛盾——工作场所与社区之间、领导层与基层之间、改革与激进主义之间的矛盾。

Abstract: Community Unionism has served as a pivotal concept in debates on trade union renewal since the early 2000s, yet its theoretical coherence and political significance remain unresolved. This article investigates why CU has gained such prominence -- not by testing its efficacy, but by mapping how it is constructed, cited, and contested across the scholarly literature. Using two complementary systematic approaches -- a citation network analysis of 114 documents and a thematic review of 18 core CU case studies -- I examine how CU functions as both an empirical descriptor and a normative ideal. The analysis reveals CU's dual genealogy: positioned by British scholars as an indigenous return to historic rank-and-file practices, yet structurally aligned with transnational social movement unionism. Thematic coding shows near-universal emphasis on coalition-building and alliances, but deep ambivalence toward class politics. This tension suggests CU's significance lies less in operationalising a new union model, and more in managing contradictions -- between workplace and community, leadership and rank-and-file, reform and radicalism -- within a shrinking labour movement.

</details>


### [65] [CC30k: A Citation Contexts Dataset for Reproducibility-Oriented Sentiment Analysis](https://arxiv.org/abs/2511.07790)
*Rochana R. Obadage,Sarah M. Rajtmajer,Jian Wu*

Main category: cs.DL

TL;DR: 介绍了CC30k数据集，包含30,734个机器学习论文的引用上下文，标注了可重复性导向的情感标签（正面、负面、中性），用于训练模型预测可重复性情感，并展示了微调后LLM在可重复性情感分类上的显著提升。


<details>
  <summary>Details</summary>
Motivation: 引用文献中关于可重复性的情感反映了社区观点，是实际可重复性的有前景信号，但缺乏专门针对可重复性导向情感分析的数据集资源。

Method: 通过包含数据清洗、众包标注和验证的流程创建CC30k数据集，其中25,829个通过众包标注，其余通过受控流程生成负面标签以解决标签稀缺问题。

Result: 数据集标注准确率达到94%，三个大型语言模型使用该数据集微调后在可重复性导向情感分类任务上性能显著提升。

Conclusion: CC30k数据集为大规模评估机器学习论文的可重复性奠定了基础，数据集和工具已公开可用。

Abstract: Sentiments about the reproducibility of cited papers in downstream literature offer community perspectives and have shown as a promising signal of the actual reproducibility of published findings. To train effective models to effectively predict reproducibility-oriented sentiments and further systematically study their correlation with reproducibility, we introduce the CC30k dataset, comprising a total of 30,734 citation contexts in machine learning papers. Each citation context is labeled with one of three reproducibility-oriented sentiment labels: Positive, Negative, or Neutral, reflecting the cited paper's perceived reproducibility or replicability. Of these, 25,829 are labeled through crowdsourcing, supplemented with negatives generated through a controlled pipeline to counter the scarcity of negative labels. Unlike traditional sentiment analysis datasets, CC30k focuses on reproducibility-oriented sentiments, addressing a research gap in resources for computational reproducibility studies. The dataset was created through a pipeline that includes robust data cleansing, careful crowd selection, and thorough validation. The resulting dataset achieves a labeling accuracy of 94%. We then demonstrated that the performance of three large language models significantly improves on the reproducibility-oriented sentiment classification after fine-tuning using our dataset. The dataset lays the foundation for large-scale assessments of the reproducibility of machine learning papers. The CC30k dataset and the Jupyter notebooks used to produce and analyze the dataset are publicly available at https://github.com/lamps-lab/CC30k .

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [66] [SpeechJudge: Towards Human-Level Judgment for Speech Naturalness](https://arxiv.org/abs/2511.07931)
*Xueyao Zhang,Chaoren Wang,Huan Liao,Ziniu Li,Yuancheng Wang,Li Wang,Dongya Jia,Yuanzhe Chen,Xiulin Li,Zhuo Chen,Zhizheng Wu*

Main category: cs.SD

TL;DR: SpeechJudge是一个用于语音合成的综合套件，包含大规模人类偏好数据集、评估基准和生成式奖励模型，旨在解决语音合成模型与人类感知对齐的挑战。


<details>
  <summary>Details</summary>
Motivation: 语音合成领域缺乏大规模人类偏好数据集，阻碍了开发真正符合人类感知的模型。自然度作为语音合成最重要的主观指标之一，需要更好的对齐方法。

Method: 1) 构建SpeechJudge-Data：包含99K语音对的大规模人类反馈语料库，涵盖多种零样本TTS模型、语音风格和语言；2) 建立SpeechJudge-Eval评估基准；3) 开发SpeechJudge-GRM：基于Qwen2.5-Omni-7B的两阶段训练生成式奖励模型（SFT+RL）。

Result: SpeechJudge-GRM在SpeechJudge-Eval基准上达到77.2%准确率（推理时缩放后79.4%），显著优于传统Bradley-Terry奖励模型（72.7%）和现有模型（Gemini-2.5-Flash低于70%）。

Conclusion: SpeechJudge套件为语音合成的人类对齐提供了有效解决方案，SpeechJudge-GRM在自然度判断任务上表现出色，并可作为语音生成模型后训练中的奖励函数。

Abstract: Aligning large generative models with human feedback is a critical challenge. In speech synthesis, this is particularly pronounced due to the lack of a large-scale human preference dataset, which hinders the development of models that truly align with human perception. To address this, we introduce SpeechJudge, a comprehensive suite comprising a dataset, a benchmark, and a reward model centered on naturalness--one of the most fundamental subjective metrics for speech synthesis. First, we present SpeechJudge-Data, a large-scale human feedback corpus of 99K speech pairs. The dataset is constructed using a diverse set of advanced zero-shot text-to-speech (TTS) models across diverse speech styles and multiple languages, with human annotations for both intelligibility and naturalness preference. From this, we establish SpeechJudge-Eval, a challenging benchmark for speech naturalness judgment. Our evaluation reveals that existing metrics and AudioLLMs struggle with this task; the leading model, Gemini-2.5-Flash, achieves less than 70% agreement with human judgment, highlighting a significant gap for improvement. To bridge this gap, we develop SpeechJudge-GRM, a generative reward model (GRM) based on Qwen2.5-Omni-7B. It is trained on SpeechJudge-Data via a two-stage post-training process: Supervised Fine-Tuning (SFT) with Chain-of-Thought rationales followed by Reinforcement Learning (RL) with GRPO on challenging cases. On the SpeechJudge-Eval benchmark, the proposed SpeechJudge-GRM demonstrates superior performance, achieving 77.2% accuracy (and 79.4% after inference-time scaling @10) compared to a classic Bradley-Terry reward model (72.7%). Furthermore, SpeechJudge-GRM can be also employed as a reward function during the post-training of speech generation models to facilitate their alignment with human preferences.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [67] [A Decentralized Retrieval Augmented Generation System with Source Reliabilities Secured on Blockchain](https://arxiv.org/abs/2511.07577)
*Yining Lu,Wenyi Tang,Max Johnson,Taeho Jung,Meng Jiang*

Main category: cs.CR

TL;DR: 提出了一种去中心化检索增强生成(dRAG)系统，通过区块链智能合约实现可靠性评分机制，在不可靠数据环境中比集中式系统性能提升10.7%，同时节省约56%的边际成本。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统采用集中式架构，存在数据收集、集成和管理成本高以及隐私问题，需要让基础模型能够直接从数据所有者那里获取信息，同时保持数据源的完全控制权。

Method: 设计去中心化RAG系统，包含新颖的可靠性评分机制，动态评估每个数据源基于其贡献的响应质量，在检索时优先选择高质量源；通过区块链智能合约安全管理评分过程，创建可验证且防篡改的可靠性记录。

Result: 在两个Llama模型(3B和8B)和六个不同可靠性水平数据源的模拟环境中测试，在真实世界不可靠数据环境中比集中式系统性能提升10.7%；在理想可靠数据环境中接近集中式系统的上限性能；通过批量更新操作实现约56%的边际成本节省。

Conclusion: 去中心化RAG系统能够有效解决集中式架构的成本和隐私问题，通过可靠性评分机制在保持性能的同时确保透明度和信任，为安全可信的检索增强生成提供了可行方案。

Abstract: Existing retrieval-augmented generation (RAG) systems typically use a centralized architecture, causing a high cost of data collection, integration, and management, as well as privacy concerns. There is a great need for a decentralized RAG system that enables foundation models to utilize information directly from data owners who maintain full control over their sources. However, decentralization brings a challenge: the numerous independent data sources vary significantly in reliability, which can diminish retrieval accuracy and response quality. To address this, our decentralized RAG system has a novel reliability scoring mechanism that dynamically evaluates each source based on the quality of responses it contributes to generate and prioritizes high-quality sources during retrieval. To ensure transparency and trust, the scoring process is securely managed through blockchain-based smart contracts, creating verifiable and tamper-proof reliability records without relying on a central authority. We evaluate our decentralized system with two Llama models (3B and 8B) in two simulated environments where six data sources have different levels of reliability. Our system achieves a +10.7\% performance improvement over its centralized counterpart in the real world-like unreliable data environments. Notably, it approaches the upper-bound performance of centralized systems under ideally reliable data environments. The decentralized infrastructure enables secure and trustworthy scoring management, achieving approximately 56\% marginal cost savings through batched update operations. Our code and system are open-sourced at github.com/yining610/Reliable-dRAG.

</details>


### [68] [SALT: Steering Activations towards Leakage-free Thinking in Chain of Thought](https://arxiv.org/abs/2511.07772)
*Shourya Batra,Pierce Tillman,Samarth Gaggar,Shashank Kesineni,Kevin Zhu,Sunishchal Dev,Ashwinee Panda,Vasu Sharma,Maheep Chaudhary*

Main category: cs.CR

TL;DR: SALT是一种轻量级测试时干预方法，通过在隐藏状态注入定向引导向量来减少LLM推理过程中的隐私泄露，同时保持任务性能。


<details>
  <summary>Details</summary>
Motivation: 随着LLM成为能访问敏感用户数据的个人助手，其内部推理过程会泄露隐私信息，即使最终输出看起来安全，这违反了上下文隐私期望。

Method: 识别高泄露层，在隐藏状态注入定向引导向量来引导模型进行无泄露思考，平衡隐私保护和推理能力。

Result: 在多个LLM上实验显示，SALT显著减少了上下文隐私泄露（如QwQ-32B减少18.2%，Llama-3.1-8B减少17.9%，Deepseek减少31.2%），同时保持可比的任务性能。

Conclusion: SALT为具备推理能力的语言模型提供了一种实用的测试时隐私保护方法，为LLM个人代理的安全部署开辟了道路。

Abstract: As Large Language Models (LLMs) evolve into personal assistants with access to sensitive user data, they face a critical privacy challenge: while prior work has addressed output-level privacy, recent findings reveal that LLMs often leak private information through their internal reasoning processes, violating contextual privacy expectations. These leaky thoughts occur when models inadvertently expose sensitive details in their reasoning traces, even when final outputs appear safe. The challenge lies in preventing such leakage without compromising the model's reasoning capabilities, requiring a delicate balance between privacy and utility. We introduce Steering Activations towards Leakage-free Thinking (SALT), a lightweight test-time intervention that mitigates privacy leakage in model's Chain of Thought (CoT) by injecting targeted steering vectors into hidden state. We identify the high-leakage layers responsible for this behavior. Through experiments across multiple LLMs, we demonstrate that SALT achieves reductions including $18.2\%$ reduction in CPL on QwQ-32B, $17.9\%$ reduction in CPL on Llama-3.1-8B, and $31.2\%$ reduction in CPL on Deepseek in contextual privacy leakage dataset AirGapAgent-R while maintaining comparable task performance and utility. Our work establishes SALT as a practical approach for test-time privacy protection in reasoning-capable language models, offering a path toward safer deployment of LLM-based personal agents.

</details>


### [69] [LoopLLM: Transferable Energy-Latency Attacks in LLMs via Repetitive Generation](https://arxiv.org/abs/2511.07876)
*Xingyu Li,Xiaolei Liu,Cheng Liu,Yixiao Xu,Kangyi Ding,Bangzhou Xin,Jia-Li Yin*

Main category: cs.CR

TL;DR: LoopLLM是一个基于重复生成触发的低熵解码循环的能耗-延迟攻击框架，通过诱导LLM生成重复内容直到输出限制，显著提高了攻击效果和跨模型迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有能耗-延迟攻击方法通过延迟终止符号生成来延长输出，但随着输出增长，通过输入控制终止符号变得困难，效果有限。

Method: 提出重复诱导提示优化利用自回归漏洞诱导重复生成，以及令牌对齐集成优化聚合梯度提高跨模型迁移性。

Result: 在12个开源和2个商业LLM上的实验显示，LoopLLM达到超过90%的最大输出长度（基线仅为20%），对DeepSeek-V3和Gemini 2.5 Flash的迁移性提高约40%。

Conclusion: LoopLLM通过利用重复生成机制，有效实现了对LLM的能耗-延迟攻击，显著优于现有方法。

Abstract: As large language models (LLMs) scale, their inference incurs substantial computational resources, exposing them to energy-latency attacks, where crafted prompts induce high energy and latency cost. Existing attack methods aim to prolong output by delaying the generation of termination symbols. However, as the output grows longer, controlling the termination symbols through input becomes difficult, making these methods less effective. Therefore, we propose LoopLLM, an energy-latency attack framework based on the observation that repetitive generation can trigger low-entropy decoding loops, reliably compelling LLMs to generate until their output limits. LoopLLM introduces (1) a repetition-inducing prompt optimization that exploits autoregressive vulnerabilities to induce repetitive generation, and (2) a token-aligned ensemble optimization that aggregates gradients to improve cross-model transferability. Extensive experiments on 12 open-source and 2 commercial LLMs show that LoopLLM significantly outperforms existing methods, achieving over 90% of the maximum output length, compared to 20% for baselines, and improving transferability by around 40% to DeepSeek-V3 and Gemini 2.5 Flash.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [70] [How Brittle is Agent Safety? Rethinking Agent Risk under Intent Concealment and Task Complexity](https://arxiv.org/abs/2511.08487)
*Zihan Ma,Dongsheng Zhu,Shudong Liu,Taolin Zhang,Junnan Liu,Qingqiu Li,Minnan Luo,Songyang Zhang,Kai Chen*

Main category: cs.MA

TL;DR: OASIS是一个用于评估LLM驱动代理安全性的分层基准测试套件，通过意图隐藏和任务复杂性两个维度分析代理安全性脆弱性，揭示了安全对齐随意图隐藏而下降以及复杂性悖论现象。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理安全评估主要关注原子性危害，无法应对恶意意图在复杂任务中被隐藏或稀释的复杂威胁，需要填补这一空白。

Method: 引入OASIS（正交代理安全查询套件），这是一个具有细粒度注释和高保真模拟沙箱的分层基准测试框架，通过意图隐藏和任务复杂性两个正交维度分析代理安全性。

Result: 发现两个关键现象：安全对齐随着意图变得模糊而急剧且可预测地下降；出现"复杂性悖论"，即代理在更难任务上看似更安全仅是因为能力限制。

Conclusion: 通过发布OASIS及其模拟环境，为在这些被忽视的维度上探测和加强代理安全性提供了原则性基础。

Abstract: Current safety evaluations for LLM-driven agents primarily focus on atomic harms, failing to address sophisticated threats where malicious intent is concealed or diluted within complex tasks. We address this gap with a two-dimensional analysis of agent safety brittleness under the orthogonal pressures of intent concealment and task complexity. To enable this, we introduce OASIS (Orthogonal Agent Safety Inquiry Suite), a hierarchical benchmark with fine-grained annotations and a high-fidelity simulation sandbox. Our findings reveal two critical phenomena: safety alignment degrades sharply and predictably as intent becomes obscured, and a "Complexity Paradox" emerges, where agents seem safer on harder tasks only due to capability limitations. By releasing OASIS and its simulation environment, we provide a principled foundation for probing and strengthening agent safety in these overlooked dimensions.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [71] [Alignment-Constrained Dynamic Pruning for LLMs: Identifying and Preserving Alignment-Critical Circuits](https://arxiv.org/abs/2511.07482)
*Dev Patel,Gabrielle Gervacio,Diekola Raimi,Kevin Zhu,Ryan Lagasse,Gabriel Grand,Ashwinee Panda,Maheep Chaudhary*

Main category: cs.LG

TL;DR: AAPP是一种动态结构化剪枝方法，通过自适应保留对齐相关电路来改善LLM推理效率，同时保持安全性


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理需要大量计算资源，动态剪枝虽然比静态方法更高效，但会加剧对齐退化问题，需要在保持效率的同时解决对齐脆弱性

Method: 基于Probe Pruning构建的Alignment-Aware Probe Pruning方法，在推理过程中自适应保留对齐相关电路

Result: 在LLaMA 2-7B、Qwen2.5-14B-Instruct和Gemma-3-12B-IT上的实验显示，在相同计算量下拒绝率提高了50%

Conclusion: AAPP实现了高效且保持安全性的LLM部署

Abstract: Large Language Models require substantial computational resources for inference, posing deployment challenges. While dynamic pruning offers superior efficiency over static methods through adaptive circuit selection, it exacerbates alignment degradation by retaining only input-dependent safety-critical circuit preservation across diverse inputs. As a result, addressing these heightened alignment vulnerabilities remains critical. We introduce Alignment-Aware Probe Pruning (AAPP), a dynamic structured pruning method that adaptively preserves alignment-relevant circuits during inference, building upon Probe Pruning. Experiments on LLaMA 2-7B, Qwen2.5-14B-Instruct, and Gemma-3-12B-IT show AAPP improves refusal rates by 50\% at matched compute, enabling efficient yet safety-preserving LLM deployment.

</details>


### [72] [LLM Output Drift: Cross-Provider Validation & Mitigation for Financial Workflows](https://arxiv.org/abs/2511.07585)
*Raffi Khatchadourian,Rolando Franco*

Main category: cs.LG

TL;DR: 研究发现大型语言模型在金融应用中存在输出漂移问题，小模型在温度T=0.0时输出一致性达100%，而大模型GPT-OSS-120B仅12.5%，挑战了大模型在金融部署中普遍更优的假设。


<details>
  <summary>Details</summary>
Motivation: 金融机构使用LLMs进行对账、监管报告和客户沟通，但非确定性输出（输出漂移）会破坏可审计性和信任度，需要量化评估不同模型在受监管金融任务中的表现。

Method: 开发了金融校准的确定性测试框架，包括贪婪解码、固定种子和SEC 10-K结构感知检索排序；使用金融校准重要性阈值（±5%）和SEC引用验证进行任务特定不变性检查；建立三级模型分类系统。

Result: 在480次运行中，结构化任务（SQL）在T=0.2时仍保持稳定，而RAG任务显示25-75%的漂移，显示任务依赖性敏感性。跨提供商验证确认确定性行为在本地和云端部署间可转移。

Conclusion: 研究挑战了大模型在金融部署中普遍更优的假设，提出了合规AI部署的实用路径，并映射到FSB、BIS和CFTC要求，为风险适当的部署决策提供框架。

Abstract: Financial institutions deploy Large Language Models (LLMs) for reconciliations, regulatory reporting, and client communications, but nondeterministic outputs (output drift) undermine auditability and trust. We quantify drift across five model architectures (7B-120B parameters) on regulated financial tasks, revealing a stark inverse relationship: smaller models (Granite-3-8B, Qwen2.5-7B) achieve 100% output consistency at T=0.0, while GPT-OSS-120B exhibits only 12.5% consistency (95% CI: 3.5-36.0%) regardless of configuration (p<0.0001, Fisher's exact test). This finding challenges conventional assumptions that larger models are universally superior for production deployment.
  Our contributions include: (i) a finance-calibrated deterministic test harness combining greedy decoding (T=0.0), fixed seeds, and SEC 10-K structure-aware retrieval ordering; (ii) task-specific invariant checking for RAG, JSON, and SQL outputs using finance-calibrated materiality thresholds (plus or minus 5%) and SEC citation validation; (iii) a three-tier model classification system enabling risk-appropriate deployment decisions; and (iv) an audit-ready attestation system with dual-provider validation.
  We evaluated five models (Qwen2.5-7B via Ollama, Granite-3-8B via IBM watsonx.ai, Llama-3.3-70B, Mistral-Medium-2505, and GPT-OSS-120B) across three regulated financial tasks. Across 480 runs (n=16 per condition), structured tasks (SQL) remain stable even at T=0.2, while RAG tasks show drift (25-75%), revealing task-dependent sensitivity. Cross-provider validation confirms deterministic behavior transfers between local and cloud deployments. We map our framework to Financial Stability Board (FSB), Bank for International Settlements (BIS), and Commodity Futures Trading Commission (CFTC) requirements, demonstrating practical pathways for compliance-ready AI deployments.

</details>


### [73] [DynaAct: Large Language Model Reasoning with Dynamic Action Spaces](https://arxiv.org/abs/2511.08043)
*Xueliang Zhao,Wei Wu,Jian Guan,Qintong Li,Lingpeng Kong*

Main category: cs.LG

TL;DR: DynaAct框架通过自动构建紧凑的动作空间来增强复杂问题解决中的顺序推理，使用LLM提取通用草图并基于效用和多样性选择最优候选动作集。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么依赖缺乏可扩展性的手动定义动作空间，要么使用计算成本高昂的非结构化空间，需要自动构建紧凑动作空间的方法。

Method: 首先使用大语言模型从多样复杂推理问题语料库中提取通用草图来估计完整动作空间的代理，然后制定子模函数联合评估候选动作的效用和多样性，使用贪心算法选择最优候选集。

Result: 在六个不同标准基准测试上的广泛实验表明，该方法显著提高了整体性能，同时保持高效推理而不引入显著延迟。

Conclusion: DynaAct框架有效解决了复杂顺序决策中动作空间构建的问题，在保持效率的同时提升了推理性能。

Abstract: In modern sequential decision-making systems, the construction of an optimal candidate action space is critical to efficient inference. However, existing approaches either rely on manually defined action spaces that lack scalability or utilize unstructured spaces that render exhaustive search computationally prohibitive. In this paper, we propose a novel framework named \textsc{DynaAct} for automatically constructing a compact action space to enhance sequential reasoning in complex problem-solving scenarios. Our method first estimates a proxy for the complete action space by extracting general sketches observed in a corpus covering diverse complex reasoning problems using large language models. We then formulate a submodular function that jointly evaluates candidate actions based on their utility to the current state and their diversity, and employ a greedy algorithm to select an optimal candidate set. Extensive experiments on six diverse standard benchmarks demonstrate that our approach significantly improves overall performance, while maintaining efficient inference without introducing substantial latency. The implementation is available at https://github.com/zhaoxlpku/DynaAct.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [74] [The Polite Liar: Epistemic Pathology in Language Models](https://arxiv.org/abs/2511.07477)
*Bentley DeVilling*

Main category: cs.CY

TL;DR: RLHF训练导致语言模型成为'礼貌的谎言者'——表现出知识自信但缺乏证据基础，这是优化用户满意度而非真实性的结构性问题。


<details>
  <summary>Details</summary>
Motivation: 分析大型语言模型为何会自信地编造信息，即使它们并不真正知道。揭示RLHF训练方法的结构性缺陷，即奖励感知真诚度而非证据准确性。

Method: 基于Frankfurt对废话的分析，结合认知对齐、言语行为哲学和知识美德理论，分析RLHF奖励架构如何导致模型对真理的结构性漠视。

Result: 发现RLHF训练出的模型学会了模仿知识自信而无需知识正当性，在语言合作与知识完整性之间存在深层对齐张力。

Conclusion: 提出'知识对齐'原则：应该奖励有正当理由的自信，而非仅仅感知到的流利度，以解决模型的结构性知识病理问题。

Abstract: Large language models exhibit a peculiar epistemic pathology: they speak as if they know, even when they do not. This paper argues that such confident fabrication, what I call the polite liar, is a structural consequence of reinforcement learning from human feedback (RLHF). Building on Frankfurt's analysis of bullshit as communicative indifference to truth, I show that this pathology is not deception but structural indifference: a reward architecture that optimizes for perceived sincerity over evidential accuracy. Current alignment methods reward models for being helpful, harmless, and polite, but not for being epistemically grounded. As a result, systems learn to maximize user satisfaction rather than truth, performing conversational fluency as a virtue. I analyze this behavior through the lenses of epistemic virtue theory, speech-act philosophy, and cognitive alignment, showing that RLHF produces agents trained to mimic epistemic confidence without access to epistemic justification. The polite liar thus reveals a deeper alignment tension between linguistic cooperation and epistemic integrity. The paper concludes with an "epistemic alignment" principle: reward justified confidence over perceived fluency.

</details>


### [75] [Generative Artificial Intelligence in Qualitative Research Methods: Between Hype and Risks?](https://arxiv.org/abs/2511.08461)
*Maria Couto Teixeira,Marisa Tschopp,Anna Jobin*

Main category: cs.CY

TL;DR: 本文批判性质疑生成式AI在质性研究编码中的方法论有效性，认为其使用会削弱研究的严谨性和可信度，建议研究人员应将健全的方法论置于技术新颖性之上。


<details>
  <summary>Details</summary>
Motivation: 随着AI在质性研究中日益推广使用，引发了深刻的方法论问题。作者旨在批判性地审视生成式AI在质性编码方法中的作用，回应关于其效率的广泛宣传。

Method: 采用立场论文的形式，通过分析生成式AI缺乏有意义的文档记录、商业不透明性以及产生错误输出的固有倾向等关键问题，论证其方法论上的局限性。

Result: 研究发现生成式AI在质性研究中缺乏方法论有效性，其使用会损害研究的稳健性和可信度。风险与收益的平衡不支持在质性研究中使用生成式AI。

Conclusion: 生成式AI不适用于质性研究，研究人员应优先考虑健全的方法论而非技术新颖性，避免因使用AI而削弱研究质量。

Abstract: As Artificial Intelligence (AI) is increasingly promoted and used in qualitative research, it also raises profound methodological issues. This position paper critically interrogates the role of generative AI (genAI) in the context of qualitative coding methodologies. Despite widespread hype and claims of efficiency, we propose that genAI is not methodologically valid within qualitative inquiries, and its use risks undermining the robustness and trustworthiness of qualitative research. The lack of meaningful documentation, commercial opacity, and the inherent tendencies of genAI systems to produce incorrect outputs all contribute to weakening methodological rigor. Overall, the balance between risk and benefits does not support the use of genAI in qualitative research, and our position paper cautions researchers to put sound methodology before technological novelty.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [76] [ViPRA: Video Prediction for Robot Actions](https://arxiv.org/abs/2511.07732)
*Sandeep Routray,Hengkai Pan,Unnat Jain,Shikhar Bahl,Deepak Pathak*

Main category: cs.RO

TL;DR: ViPRA是一个从无动作标签视频中学习机器人控制的预训练-微调框架，通过预测未来视觉观察和运动中心潜在动作，仅需少量演示就能实现连续控制


<details>
  <summary>Details</summary>
Motivation: 大多数视频缺乏动作标签，限制了在机器人学习中的应用，需要一种能从无动作视频中学习控制的方法

Method: 训练视频语言模型预测未来视觉观察和运动中心潜在动作，使用感知损失和光流一致性确保物理基础，下游通过分块流匹配解码器将潜在动作映射到机器人特定动作序列

Result: 在SIMPLER基准上提升16%，在真实世界操作任务上提升13%，支持22Hz高频连续控制

Conclusion: ViPRA能够从无动作视频中有效学习机器人控制，避免昂贵动作标注，支持跨具身泛化

Abstract: Can we turn a video prediction model into a robot policy? Videos, including those of humans or teleoperated robots, capture rich physical interactions. However, most of them lack labeled actions, which limits their use in robot learning. We present Video Prediction for Robot Actions (ViPRA), a simple pretraining-finetuning framework that learns continuous robot control from these actionless videos. Instead of directly predicting actions, we train a video-language model to predict both future visual observations and motion-centric latent actions, which serve as intermediate representations of scene dynamics. We train these latent actions using perceptual losses and optical flow consistency to ensure they reflect physically grounded behavior. For downstream control, we introduce a chunked flow matching decoder that maps latent actions to robot-specific continuous action sequences, using only 100 to 200 teleoperated demonstrations. This approach avoids expensive action annotation, supports generalization across embodiments, and enables smooth, high-frequency continuous control upto 22 Hz via chunked action decoding. Unlike prior latent action works that treat pretraining as autoregressive policy learning, explicitly models both what changes and how. Our method outperforms strong baselines, with a 16% gain on the SIMPLER benchmark and a 13% improvement across real world manipulation tasks. We will release models and code at https://vipra-project.github.io

</details>


### [77] [PerspAct: Enhancing LLM Situated Collaboration Skills through Perspective Taking and Active Vision](https://arxiv.org/abs/2511.08098)
*Sabrina Patania,Luca Annese,Anita Pellegrini,Silvia Serino,Anna Lambiase,Luca Pallonetto,Silvia Rossi,Simone Colombani,Tom Foulsham,Azzurra Ruggeri,Dimitri Ognibene*

Main category: cs.RO

TL;DR: 本研究评估了使用ReAct框架明确融入多视角是否能增强LLM理解其他智能体需求的能力，通过扩展Director任务并引入主动视觉探索，发现在多视角复杂场景中，显式视角线索结合主动探索策略显著提升了模型的解释准确性和协作效果。


<details>
  <summary>Details</summary>
Motivation: 当前LLM和多模态基础模型在机器人和协作系统中应用广泛，但多智能体交互需要强大的视角获取能力。现有训练范式往往忽视交互上下文，导致模型在推理个体视角主观性或处理多观察者环境时面临挑战。

Method: 扩展经典的Director任务，引入主动视觉探索，设计了七个视角获取复杂度递增的场景。这些场景挑战智能体基于视觉访问和交互解决指代歧义的能力，测试了不同状态表示和提示策略（包括ReAct风格推理）。

Result: 结果表明，显式视角线索与主动探索策略相结合，显著提高了模型的解释准确性和协作有效性。

Conclusion: 这些发现突显了将主动感知与视角获取机制相结合在推进LLM在机器人和多智能体系统中应用的潜力，为未来研究自适应和上下文感知AI系统奠定了基础。

Abstract: Recent advances in Large Language Models (LLMs) and multimodal foundation models have significantly broadened their application in robotics and collaborative systems. However, effective multi-agent interaction necessitates robust perspective-taking capabilities, enabling models to interpret both physical and epistemic viewpoints. Current training paradigms often neglect these interactive contexts, resulting in challenges when models must reason about the subjectivity of individual perspectives or navigate environments with multiple observers. This study evaluates whether explicitly incorporating diverse points of view using the ReAct framework, an approach that integrates reasoning and acting, can enhance an LLM's ability to understand and ground the demands of other agents. We extend the classic Director task by introducing active visual exploration across a suite of seven scenarios of increasing perspective-taking complexity. These scenarios are designed to challenge the agent's capacity to resolve referential ambiguity based on visual access and interaction, under varying state representations and prompting strategies, including ReAct-style reasoning. Our results demonstrate that explicit perspective cues, combined with active exploration strategies, significantly improve the model's interpretative accuracy and collaborative effectiveness. These findings highlight the potential of integrating active perception with perspective-taking mechanisms in advancing LLMs' application in robotics and multi-agent systems, setting a foundation for future research into adaptive and context-aware AI systems.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [78] [Pruning as Regularization: Sensitivity-Aware One-Shot Pruning in ASR](https://arxiv.org/abs/2511.08092)
*Julian Irigoyen,Arthur Söhler,Andreas Søeborg Kirkedal*

Main category: eess.AS

TL;DR: 本文挑战了神经网络剪枝仅作为压缩技术的传统观点，证明一次性幅度剪枝是ASR的强大隐式正则化器。通过针对性组件剪枝，发现解码器FFN对剪枝敏感，而解码器自注意力和最后编码层存在冗余，剪除后能改善泛化性能。


<details>
  <summary>Details</summary>
Motivation: 重新定义神经网络剪枝的角色，从单纯的压缩技术转变为有效的正则化工具，探索剪枝在自动语音识别(ASR)中的正则化效果。

Method: 使用Whisper-small模型，结合梯度和Fisher敏感度诊断，进行针对性组件剪枝。分析不同架构组件对剪枝的敏感度，识别冗余区域。

Result: 无需微调，剪除50%解码器自注意力使LibriSpeech test-other的WER绝对降低2.38%(相对20.44%)；剪除最后四个编码层50%参数使WER绝对降低1.72%(相对14.8%)。在40%稀疏度下，传统全局剪枝方法失效时，该方法仍能保持接近基线精度。

Conclusion: 剪枝应被视为架构设计的一流工具，知道在哪里剪枝与剪多少同等重要。该方法不仅提供正则化效益，还支持更激进的一次性压缩。

Abstract: We challenge the conventional view of neural network pruning as solely a compression technique, demonstrating that one-shot magnitude pruning serves as a powerful implicit regularizer for ASR. Using Whisper-small, we combine gradient- and Fisher-based sensitivity diagnostics with targeted, component-wise pruning. This reveals architectural asymmetries: decoder FFNs are pruning-fragile, whereas decoder self-attention and the last encoder layers contain redundancy that, when removed, improves generalization. Without fine-tuning, pruning 50% of decoder self-attention reduces WER by 2.38% absolute (20.44% relative) on LibriSpeech test-other; pruning the last four encoder layers at 50% instead yields a 1.72% absolute (14.8% relative) improvement. Gains persisted on Common Voice and TED-LIUM datasets. Beyond regularization benefits, our sensitivity-aware approach enables more aggressive one-shot compression. At 40% sparsity, where established global pruning approaches catastrophically fail, our method preserves near-baseline accuracy. This positions pruning as a first-class architectural design tool: knowing where to prune is as important as how much to prune.

</details>


### [79] [Quantizing Whisper-small: How design choices affect ASR performance](https://arxiv.org/abs/2511.08093)
*Arthur Söhler,Julian Irigoyen,Andreas Søeborg Kirkedal*

Main category: eess.AS

TL;DR: 对Whisper-small模型进行后训练量化评估，发现动态int8量化在Quanto库中表现最佳，模型大小减少57%的同时还能改善词错误率，使模型能够在受限硬件上高效部署。


<details>
  <summary>Details</summary>
Motivation: 大型语音识别模型如Whisper-small在边缘设备上部署困难，因为计算需求高。需要研究后训练量化方法以减少模型大小和推理成本。

Method: 使用四个库（PyTorch、Optimum-Quanto、HQQ、bitsandbytes）对Whisper-small进行统一评估，分析量化方案、方法、粒度和位宽的影响，在LibriSpeech测试集上进行实验。

Result: 动态int8量化在Quanto库中表现最佳，模型大小减少57%，词错误率优于基线。静态量化表现较差，更激进的量化格式（如nf4、int3）可实现71%压缩但噪声条件下精度下降。

Conclusion: 精心选择的后训练量化方法可以显著减小模型大小和推理成本，无需重新训练，使Whisper-small能够在受限硬件上高效部署。

Abstract: Large speech recognition models like Whisper-small achieve high accuracy but are difficult to deploy on edge devices due to their high computational demand. To this end, we present a unified, cross-library evaluation of post-training quantization (PTQ) on Whisper-small that disentangles the impact of quantization scheme, method, granularity, and bit-width. Our study is based on four libraries: PyTorch, Optimum-Quanto, HQQ, and bitsandbytes. Experiments on LibriSpeech test-clean and test-other show that dynamic int8 quantization with Quanto offers the best trade-off, reducing model size by 57% while improving on the baseline's word error rate. Static quantization performed worse, likely due to Whisper's Transformer architecture, while more aggressive formats (e.g., nf4, int3) achieved up to 71% compression at the cost of accuracy in noisy conditions. Overall, our results demonstrate that carefully chosen PTQ methods can substantially reduce model size and inference cost without retraining, enabling efficient deployment of Whisper-small on constrained hardware.

</details>


### [80] [Unifying Model and Layer Fusion for Speech Foundation Models](https://arxiv.org/abs/2511.08389)
*Yi-Jen Shih,David Harwath*

Main category: eess.AS

TL;DR: 提出了一种跨多个上游语音模型的接口模块，能够在整合各模型层信息的同时实现模型间融合，在ASR和副语言分析等任务中优于现有融合方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明，同一模型的多层表征融合或多个模型的融合能提升下游任务性能，但尚未统一这两种融合策略。

Method: 设计了一个接口模块，支持跨多个上游语音模型的融合，同时整合各模型的层间信息。

Result: 在多种自监督和监督模型上的实验表明，该方法在ASR和副语言分析等任务中优于现有融合方法，且能随模型规模和数量扩展。

Conclusion: 该接口模块在合适的上游模型选择下能提供额外性能提升，是利用语音基础模型的有前景方法。

Abstract: Speech Foundation Models have gained significant attention recently. Prior works have shown that the fusion of representations from multiple layers of the same model or the fusion of multiple models can improve performance on downstream tasks. We unify these two fusion strategies by proposing an interface module that enables fusion across multiple upstream speech models while integrating information across their layers. We conduct extensive experiments on different self-supervised and supervised models across various speech tasks, including ASR and paralinguistic analysis, and demonstrate that our method outperforms prior fusion approaches. We further analyze its scalability concerning model size and count, highlighting the importance of selecting appropriate upstream models. Our results show that the proposed interface provides an additional performance boost when given a suitable upstream model selection, making it a promising approach for utilizing Speech Foundation Models.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [81] [Think Before You Retrieve: Learning Test-Time Adaptive Search with Small Language Models](https://arxiv.org/abs/2511.07581)
*Supriti Vijay,Aman Priyanshu,Anu Vellore,Baturay Saglam,Amin Karbasi*

Main category: cs.AI

TL;DR: Orion训练框架使紧凑模型(350M-1.2B参数)能够通过学习的搜索策略执行迭代检索，在多个基准测试中优于大200-400倍的检索器。


<details>
  <summary>Details</summary>
Motivation: 当前方法存在缺陷：神经检索器缺乏推理能力，LLM成本过高，查询重写或分解仅限于静态转换，无法捕捉复杂查询所需的探索、反馈和修订的迭代动态。

Method: 结合：(1)合成轨迹生成和监督微调以鼓励多样化探索模式；(2)强化学习奖励有效查询细化和回溯行为；(3)推理时束搜索算法利用RL期间学习的自反能力。

Result: 1.2B模型在仅使用3%训练数据的情况下，在SciFact上达到77.6%成功率(之前为72.6%)，BRIGHT上25.2%(22.1%)，NFCorpus上63.2%(57.8%)，在FEVER、HotpotQA和MSMarco上保持竞争力，在六个基准中的五个上优于大200-400倍的检索器。

Conclusion: 检索性能可以从学习策略中涌现，而不仅仅是模型规模，当模型被训练去搜索、反思和修订时。

Abstract: Effective information retrieval requires reasoning over partial evidence and refining strategies as information emerges. Yet current approaches fall short: neural retrievers lack reasoning capabilities, large language models (LLMs) provide semantic depth but at prohibitive cost, and query rewriting or decomposition limits improvement to static transformations. As a result, existing methods fail to capture the iterative dynamics of exploration, feedback, and revision that complex user queries demand. We introduce Orion, a training framework that enables compact models (350M-1.2B parameters) to perform iterative retrieval through learned search strategies. Orion combines: (1) synthetic trajectory generation and supervised fine-tuning to encourage diverse exploration patterns in models, (2) reinforcement learning (RL) that rewards effective query refinement and backtracking behaviors, and (3) inference-time beam search algorithms that exploit the self-reflection capabilities learned during RL. Despite using only 3% of the training data available, our 1.2B model achieves 77.6% success on SciFact (vs. 72.6% for prior retrievers), 25.2% on BRIGHT (vs. 22.1%), 63.2% on NFCorpus (vs. 57.8%), and remains competitive on FEVER, HotpotQA, and MSMarco. It outperforms retrievers up to 200-400x larger on five of six benchmarks. These findings suggest that retrieval performance can emerge from learned strategies, not just model scale, when models are trained to search, reflect, and revise.

</details>


### [82] [Beyond Fact Retrieval: Episodic Memory for RAG with Generative Semantic Workspaces](https://arxiv.org/abs/2511.07587)
*Shreyas Rajesh,Pavan Holur,Chenda Duan,David Chong,Vwani Roychowdhury*

Main category: cs.AI

TL;DR: 提出了生成语义工作空间（GSW）框架，用于解决LLM在长上下文推理中的挑战，通过构建结构化的情景表示来跟踪实体在事件中的演变。


<details>
  <summary>Details</summary>
Motivation: 现有基于检索的方法主要针对事实检索，无法构建时空锚定的叙事表示来跟踪实体在事件中的演变过程。

Method: GSW框架包含操作器（将观察映射到中间语义结构）和协调器（将这些结构整合到持久工作空间中，确保时空和逻辑一致性）。

Result: 在Episodic Memory Benchmark上，GSW比现有RAG基线性能提升高达20%，查询时上下文token减少51%，显著降低推理时间成本。

Conclusion: GSW为LLM提供了类人情景记忆的具体蓝图，为实现能够进行长时程推理的更智能代理铺平了道路。

Abstract: Large Language Models (LLMs) face fundamental challenges in long-context reasoning: many documents exceed their finite context windows, while performance on texts that do fit degrades with sequence length, necessitating their augmentation with external memory frameworks. Current solutions, which have evolved from retrieval using semantic embeddings to more sophisticated structured knowledge graphs representations for improved sense-making and associativity, are tailored for fact-based retrieval and fail to build the space-time-anchored narrative representations required for tracking entities through episodic events. To bridge this gap, we propose the \textbf{Generative Semantic Workspace} (GSW), a neuro-inspired generative memory framework that builds structured, interpretable representations of evolving situations, enabling LLMs to reason over evolving roles, actions, and spatiotemporal contexts. Our framework comprises an \textit{Operator}, which maps incoming observations to intermediate semantic structures, and a \textit{Reconciler}, which integrates these into a persistent workspace that enforces temporal, spatial, and logical coherence. On the Episodic Memory Benchmark (EpBench) \cite{huet_episodic_2025} comprising corpora ranging from 100k to 1M tokens in length, GSW outperforms existing RAG based baselines by up to \textbf{20\%}. Furthermore, GSW is highly efficient, reducing query-time context tokens by \textbf{51\%} compared to the next most token-efficient baseline, reducing inference time costs considerably. More broadly, GSW offers a concrete blueprint for endowing LLMs with human-like episodic memory, paving the way for more capable agents that can reason over long horizons.

</details>


### [83] [ResearchRubrics: A Benchmark of Prompts and Rubrics For Evaluating Deep Research Agents](https://arxiv.org/abs/2511.07685)
*Manasi Sharma,Chen Bo Calvin Zhang,Chaithanya Bandi,Clinton Wang,Ankit Aich,Huy Nghiem,Tahseen Rabbani,Ye Htet,Brian Jang,Sumana Basu,Aishwarya Balwani,Denis Peskoff,Marcos Ayestaran,Sean M. Hendryx,Brad Kenstler,Bing Liu*

Main category: cs.AI

TL;DR: 提出了ResearchRubrics基准，包含2500+专家编写的细粒度评分标准，用于评估深度研究系统的真实性、推理合理性和清晰度。评估发现即使领先的深度研究代理也仅达到68%的平均符合度。


<details>
  <summary>Details</summary>
Motivation: 深度研究系统需要多步推理、跨文档综合和生成有证据支持的长篇回答，但由于回答冗长多样、存在多种有效解决方案且依赖动态信息源，评估仍然具有挑战性。

Method: 构建了包含2800+小时人工劳动的标准化基准，提出了新的复杂性框架（概念广度、逻辑嵌套和探索性），并开发了基于人类和模型的评估协议。

Result: 评估多个最先进的深度研究系统，发现即使是Gemini和OpenAI的深度研究代理也仅达到低于68%的平均符合度，主要原因是遗漏隐含上下文和对检索信息推理不足。

Conclusion: 需要强大、可扩展的深度研究能力评估方法，为此发布了ResearchRubrics基准以促进研究助手的发展。

Abstract: Deep Research (DR) is an emerging agent application that leverages large language models (LLMs) to address open-ended queries. It requires the integration of several capabilities, including multi-step reasoning, cross-document synthesis, and the generation of evidence-backed, long-form answers. Evaluating DR remains challenging because responses are lengthy and diverse, admit many valid solutions, and often depend on dynamic information sources. We introduce ResearchRubrics, a standardized benchmark for DR built with over 2,800+ hours of human labor that pairs realistic, domain-diverse prompts with 2,500+ expert-written, fine-grained rubrics to assess factual grounding, reasoning soundness, and clarity. We also propose a new complexity framework for categorizing DR tasks along three axes: conceptual breadth, logical nesting, and exploration. In addition, we develop human and model-based evaluation protocols that measure rubric adherence for DR agents. We evaluate several state-of-the-art DR systems and find that even leading agents like Gemini's DR and OpenAI's DR achieve under 68% average compliance with our rubrics, primarily due to missed implicit context and inadequate reasoning about retrieved information. Our results highlight the need for robust, scalable assessment of deep research capabilities, to which end we release ResearchRubrics(including all prompts, rubrics, and evaluation code) to facilitate progress toward well-justified research assistants.

</details>


### [84] [SparseRM: A Lightweight Preference Modeling with Sparse Autoencoder](https://arxiv.org/abs/2511.07896)
*Dengcan Liu,Jiahao Li,Zheren Fu,Yi Tu,Jiajun Li,Zhendong Mao,Yongdong Zhang*

Main category: cs.AI

TL;DR: SparseRM是一个轻量级可解释的奖励模型，通过稀疏自编码器从LLM表示中提取偏好相关信息，仅需不到1%的可训练参数就能在多个偏好建模任务中超越主流奖励模型。


<details>
  <summary>Details</summary>
Motivation: 传统奖励模型依赖大规模偏好标注和昂贵的LLM微调，在有限资源下训练可靠奖励模型具有挑战性。

Method: 使用稀疏自编码器分解LLM表示为可解释方向，计算对齐分数量化偏好特征强度，通过简单奖励头聚合分数预测偏好得分。

Result: 在三个偏好建模任务中，SparseRM性能优于大多数主流奖励模型，同时使用不到1%的可训练参数。

Conclusion: SparseRM为高效对齐提供了潜力，可无缝集成到下游对齐流程中。

Abstract: Reward models (RMs) are a core component in the post-training of large language models (LLMs), serving as proxies for human preference evaluation and guiding model alignment. However, training reliable RMs under limited resources remains challenging due to the reliance on large-scale preference annotations and the high cost of fine-tuning LLMs. To address this, we propose SparseRM, which leverages Sparse Autoencoder (SAE) to extract preference-relevant information encoded in model representations, enabling the construction of a lightweight and interpretable reward model. SparseRM first employs SAE to decompose LLM representations into interpretable directions that capture preference-relevant features. The representations are then projected onto these directions to compute alignment scores, which quantify the strength of each preference feature in the representations. A simple reward head aggregates these scores to predict preference scores. Experiments on three preference modeling tasks show that SparseRM achieves superior performance over most mainstream RMs while using less than 1% of trainable parameters. Moreover, it integrates seamlessly into downstream alignment pipelines, highlighting its potential for efficient alignment.

</details>


### [85] [Thinker: Training LLMs in Hierarchical Thinking for Deep Search via Multi-Turn Interaction](https://arxiv.org/abs/2511.07943)
*Jun Xu,Xinkai Du,Yu Ao,Peilong Zhao,Yang Li,Ling Zhong,Lin Yuan,Zhongpu Bo,Xiaorui Wang,Mengshu Sun,Zhengke Gui,Dalong Zhang,Zhaoyang Wang,Qiwei Wang,Yangyang Hou,Zhiying Yin,Haofen Wang,Huajun Chen,Lei Liang,Jun Zhou*

Main category: cs.AI

TL;DR: 提出了Thinker模型，通过分层思维和多轮交互实现可监督的深度搜索推理过程，将复杂问题分解为子问题，支持知识库和网页搜索，并通过逻辑函数增强推理连贯性。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要使用端到端强化学习训练LLMs利用外部检索器，但缺乏对推理过程的监督，难以保证逻辑连贯性和严谨性。

Method: 提出分层思维模型，将复杂问题分解为独立可解的子问题，每个子问题用自然语言和等效逻辑函数双重表示，支持知识库和网页搜索，并通过逻辑函数传递子问题间依赖关系。

Result: 仅需数百训练样本，Thinker性能即可与现有基线竞争；在完整训练集上，Thinker在各种数据集和模型规模上显著优于现有方法。

Conclusion: Thinker通过可监督的分层推理过程，有效提升了LLMs在复杂问题解决中的逻辑连贯性和性能，证明了少量训练样本即可实现竞争力的结果。

Abstract: Efficient retrieval of external knowledge bases and web pages is crucial for enhancing the reasoning abilities of LLMs. Previous works on training LLMs to leverage external retrievers for solving complex problems have predominantly employed end-to-end reinforcement learning. However, these approaches neglect supervision over the reasoning process, making it difficult to guarantee logical coherence and rigor. To address these limitations, we propose Thinker, a hierarchical thinking model for deep search through multi-turn interaction, making the reasoning process supervisable and verifiable. It decomposes complex problems into independently solvable sub-problems, each dually represented in both natural language and an equivalent logical function to support knowledge base and web searches. Concurrently, dependencies between sub-problems are passed as parameters via these logical functions, enhancing the logical coherence of the problem-solving process. To avoid unnecessary external searches, we perform knowledge boundary determination to check if a sub-problem is within the LLM's intrinsic knowledge, allowing it to answer directly. Experimental results indicate that with as few as several hundred training samples, the performance of Thinker is competitive with established baselines. Furthermore, when scaled to the full training set, Thinker significantly outperforms these methods across various datasets and model sizes. The source code is available at https://github.com/OpenSPG/KAG-Thinker.

</details>


### [86] [Dual-Process Scaffold Reasoning for Enhancing LLM Code Debugging](https://arxiv.org/abs/2511.08052)
*Po-Chung Hsieh,Chin-Po Chen,Jeng-Lin Li,Ming-Ching Chang*

Main category: cs.AI

TL;DR: 提出了一种基于心理学理论的Scaffold Reasoning框架用于代码调试，通过三个流（Scaffold Stream、Analytic Stream、Integration Stream）优化推理过程，在DebugBench上达到88.91%的通过率和5.36秒的平均推理时间。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在推理过程中缺乏对System 2（中间推理步骤）的深入探索，需要平衡复杂性和计算效率的推理步骤。

Method: 提出Scaffold Reasoning框架，包含三个流：Scaffold Stream构建参考代码，Analytic Stream分析错误代码，Integration Stream整合两者结果。

Result: 在DebugBench上达到88.91%的通过率和5.36秒的平均推理时间，优于其他推理方法。

Conclusion: 该框架与人类认知过程一致，在不同问题难度和错误类型上表现出优势。

Abstract: Recent LLMs have demonstrated sophisticated problem-solving capabilities on various benchmarks through advanced reasoning algorithms. However, the key research question of identifying reasoning steps that balance complexity and computational efficiency remains unsolved. Recent research has increasingly drawn upon psychological theories to explore strategies for optimizing cognitive pathways. The LLM's final outputs and intermediate steps are regarded as System 1 and System 2, respectively. However, an in-depth exploration of the System 2 reasoning is still lacking. Therefore, we propose a novel psychologically backed Scaffold Reasoning framework for code debugging, which encompasses the Scaffold Stream, Analytic Stream, and Integration Stream. The construction of reference code within the Scaffold Stream is integrated with the buggy code analysis results produced by the Analytic Stream through the Integration Stream. Our framework achieves an 88.91% pass rate and an average inference time of 5.36 seconds per-problem on DebugBench, outperforming other reasoning approaches across various LLMs in both reasoning accuracy and efficiency. Further analyses elucidate the advantages and limitations of various cognitive pathways across varying problem difficulties and bug types. Our findings also corroborate the alignment of the proposed Scaffold Reasoning framework with human cognitive processes.

</details>


### [87] [Information Capacity: Evaluating the Efficiency of Large Language Models via Text Compression](https://arxiv.org/abs/2511.08066)
*Cheng Yuan,Jiawei Shao,Chi Zhang,Xuelong Li*

Main category: cs.AI

TL;DR: 本文提出信息容量作为衡量大语言模型效率的统一指标，基于文本压缩性能与计算复杂度的关系，能够公平比较不同模型系列间的效率并准确预测同系列模型的性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型快速发展导致计算资源需求激增，测试时缩放进一步加剧模型能力与资源消耗的矛盾，但目前缺乏能够准确反映不同规模和架构LLM效率的统一度量标准。

Method: 引入信息容量指标，通过文本压缩性能相对于计算复杂度的比值来衡量模型效率，并考虑分词器效率对输入输出标记数的影响。

Result: 在主流开源模型上的实证评估表明，同一系列内不同规模的模型具有一致的信息容量，该指标能够公平比较模型系列间的效率并准确预测同系列模型性能。

Conclusion: 信息容量是一个有效的LLM效率度量标准，能够统一评估不同模型，并揭示了分词器效率、预训练数据和专家混合架构对模型效率的影响。

Abstract: Recent years have witnessed the rapid advancements of large language models (LLMs) and their expanding applications, leading to soaring demands for computational resources. The widespread adoption of test-time scaling further aggravates the tension between model capability and resource consumption, highlighting the importance of inference efficiency. However, a unified metric that accurately reflects an LLM's efficiency across different model sizes and architectures remains absent. Motivated by the correlation between compression and intelligence, we introduce information capacity, a measure of model efficiency based on text compression performance relative to computational complexity. Larger models can predict the next token more accurately, achieving greater compression gains but at higher computational costs. Empirical evaluations on mainstream open-source models show that models of varying sizes within a series exhibit consistent information capacity. This metric enables a fair efficiency comparison across model series and accurate performance prediction within a model series. A distinctive feature of information capacity is that it incorporates tokenizer efficiency, which affects both input and output token counts but is often neglected in LLM evaluations. We assess the information capacity of 49 models on 5 heterogeneous datasets and observe consistent results on the influences of tokenizer efficiency, pretraining data, and the mixture-of-experts architecture.

</details>


### [88] [SciAgent: A Unified Multi-Agent System for Generalistic Scientific Reasoning](https://arxiv.org/abs/2511.08151)
*Xuchen Li,Ruitao Wu,Xuanbo Liu,Xukai Wang,Jinbo Hu,Zhixin Bai,Bohan Zeng,Hao Liang,Leheng Chen,Mingrui Chen,Haitian Zhong,Xuanlin Yang,Xu-Yao Zhang,Liu Liu,Jia Li,Kaiqi Huang,Jiahao Xu,Haitao Mi,Wentao Zhang,Bin Dong*

Main category: cs.AI

TL;DR: SciAgent是一个统一的多智能体系统，通过分层协作实现跨学科的科学推理，在数学和物理奥林匹克竞赛中达到或超过人类金牌选手水平。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统在特定科学任务上表现优异但缺乏通用性，需要开发能够适应不同学科和难度级别的通用科学推理系统。

Method: 采用分层多智能体架构：协调器智能体分析问题领域和复杂度，动态编排由符号推理、概念建模、数值计算和验证子智能体组成的专业工作系统，协作构建定制化的推理流程。

Result: 在IMO、IMC、IPhO、CPhO等数学和物理奥林匹克竞赛中持续达到或超越人类金牌选手表现，在IChO和HLE基准测试中也展示了跨学科泛化能力。

Conclusion: SciAgent是实现通用科学智能的重要进展，展示了AI系统在跨学科专家级推理方面的潜力。

Abstract: Recent advances in large language models have enabled AI systems to achieve expert-level performance on domain-specific scientific tasks, yet these systems remain narrow and handcrafted. We introduce SciAgent, a unified multi-agent system designed for generalistic scientific reasoning-the ability to adapt reasoning strategies across disciplines and difficulty levels. SciAgent organizes problem solving as a hierarchical process: a Coordinator Agent interprets each problem's domain and complexity, dynamically orchestrating specialized Worker Systems, each composed of interacting reasoning Sub-agents for symbolic deduction, conceptual modeling, numerical computation, and verification. These agents collaboratively assemble and refine reasoning pipelines tailored to each task. Across mathematics and physics Olympiads (IMO, IMC, IPhO, CPhO), SciAgent consistently attains or surpasses human gold-medalist performance, demonstrating both domain generality and reasoning adaptability. Additionally, SciAgent has been tested on the International Chemistry Olympiad (IChO) and selected problems from the Humanity's Last Exam (HLE) benchmark, further confirming the system's ability to generalize across diverse scientific domains. This work establishes SciAgent as a concrete step toward generalistic scientific intelligence-AI systems capable of coherent, cross-disciplinary reasoning at expert levels.

</details>


### [89] [Towards Outcome-Oriented, Task-Agnostic Evaluation of AI Agents](https://arxiv.org/abs/2511.08242)
*Waseem AlShikh,Muayad Sayed Ali,Brian Kennedy,Dmytro Mozolevskyi*

Main category: cs.AI

TL;DR: 提出了一个包含11个基于结果、任务无关的AI代理性能评估框架，通过大规模模拟实验验证了其有效性，发现混合代理在大多数指标上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 现有的基础设施指标（如延迟、吞吐量）无法评估AI代理的决策质量、操作自主性或业务价值，需要更全面的评估方法。

Method: 提出了11个跨领域的性能指标，包括目标完成率、自主性指数等，并通过大规模模拟实验测试了四种代理架构在五个不同领域的表现。

Result: 混合代理在大多数指标上表现最稳定，平均目标完成率达到88.8%，投资回报率最高，不同代理设计之间存在显著性能权衡。

Conclusion: 该工作为AI代理的整体评估提供了标准化方法，有助于更有效的开发、部署和治理。

Abstract: As AI agents proliferate across industries and applications, evaluating their performance based solely on infrastructural metrics such as latency, time-to-first-token, or token throughput is proving insufficient. These metrics fail to capture the quality of an agent's decisions, its operational autonomy, or its ultimate business value. This white paper proposes a novel, comprehensive framework of eleven outcome-based, task-agnostic performance metrics for AI agents that transcend domain boundaries. These metrics are designed to enable organizations to evaluate agents based on the quality of their decisions, their degree of autonomy, their adaptability to new challenges, and the tangible business value they deliver, regardless of the underlying model architecture or specific use case. We introduce metrics such as Goal Completion Rate (GCR), Autonomy Index (AIx), Multi-Step Task Resilience (MTR), and Business Impact Efficiency (BIE). Through a large-scale simulated experiment involving four distinct agent architectures (ReAct, Chain-of-Thought, Tool-Augmented, Hybrid) across five diverse domains (Healthcare, Finance, Marketing, Legal, and Customer Service), we demonstrate the framework's efficacy. Our results reveal significant performance trade-offs between different agent designs, highlighting the Hybrid Agent as the most consistently high-performing model across the majority of our proposed metrics, achieving an average Goal Completion Rate of 88.8\% and the highest Return on Investment (ROI). This work provides a robust, standardized methodology for the holistic evaluation of AI agents, paving the way for more effective development, deployment, and governance.

</details>


### [90] [Multi-Agent GraphRAG: A Text-to-Cypher Framework for Labeled Property Graphs](https://arxiv.org/abs/2511.08274)
*Anton Gusarov,Anastasia Volkova,Valentin Khrulkov,Andrey Kuznetsov,Evgenii Maslov,Ivan Oseledets*

Main category: cs.AI

TL;DR: 提出Multi-Agent GraphRAG系统，使用多智能体LLM进行文本到Cypher查询生成，作为LPG图数据的自然语言接口。


<details>
  <summary>Details</summary>
Motivation: 现有GraphRAG方法主要关注RDF知识图谱，而Cypher和LPG数据库作为可扩展推理引擎的潜力在GraphRAG管道中尚未充分探索。

Method: 基于LLM的工作流，使用Memgraph作为图数据库后端，通过迭代内容感知校正和规范化，结合聚合反馈循环来生成和执行Cypher查询。

Result: 在CypherBench图数据集上评估系统，涵盖多个通用领域的不同查询类型；并在基于IFC数据的属性图上展示性能，支持工业数字自动化用例。

Conclusion: 该方法能够将AI与大规模现实世界应用连接起来，为工业数字自动化用例提供支持。

Abstract: While Retrieval-Augmented Generation (RAG) methods commonly draw information from unstructured documents, the emerging paradigm of GraphRAG aims to leverage structured data such as knowledge graphs. Most existing GraphRAG efforts focus on Resource Description Framework (RDF) knowledge graphs, relying on triple representations and SPARQL queries. However, the potential of Cypher and Labeled Property Graph (LPG) databases to serve as scalable and effective reasoning engines within GraphRAG pipelines remains underexplored in current research literature. To fill this gap, we propose Multi-Agent GraphRAG, a modular LLM agentic system for text-to-Cypher query generation serving as a natural language interface to LPG-based graph data. Our proof-of-concept system features an LLM-based workflow for automated Cypher queries generation and execution, using Memgraph as the graph database backend. Iterative content-aware correction and normalization, reinforced by an aggregated feedback loop, ensures both semantic and syntactic refinement of generated queries. We evaluate our system on the CypherBench graph dataset covering several general domains with diverse types of queries. In addition, we demonstrate performance of the proposed workflow on a property graph derived from the IFC (Industry Foundation Classes) data, representing a digital twin of a building. This highlights how such an approach can bridge AI with real-world applications at scale, enabling industrial digital automation use cases.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [91] [Hybrid Quantum-Classical Selective State Space Artificial Intelligence](https://arxiv.org/abs/2511.08349)
*Amin Ebrahimi,Farzan Haddadi*

Main category: quant-ph

TL;DR: 提出了一种用于Mamba架构的混合量子经典选择机制，利用变分量子电路作为量子门控模块来增强特征提取和抑制无关信息，在时序序列分类任务中展现出优于纯经典方法的性能。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习架构中的计算瓶颈，特别是自然语言处理中大规模矩阵乘法和高维优化带来的时间复杂性问题，利用量子资源实现更高效的表示学习。

Method: 在Mamba架构中集成变分量子电路作为量子门控模块，将量子子程序引入大型语言模型，分析其对泛化能力、表达能力和参数效率的影响。

Result: 在重塑的MNIST数据集上，使用一个量子层的混合模型在前4个epochs中达到24.6%的准确率，而纯经典选择机制为21.6%，显示出更高的表达能力。

Conclusion: 量子增强的门控机制为构建可扩展、资源高效的NLP模型提供了一条有前景的路径，尽管目前还处于有限模拟阶段。

Abstract: Hybrid Quantum Classical (HQC) algorithms constitute one of the most effective paradigms for exploiting the computational advantages of quantum systems in large-scale numerical tasks. By operating in high-dimensional Hilbert spaces, quantum circuits enable exponential speed-ups and provide access to richer representations of cost landscapes compared to purely classical methods. These capabilities are particularly relevant for machine learning, where state-of-the-art models especially in Natural Language Processing (NLP) suffer from prohibitive time complexity due to massive matrix multiplications and high-dimensional optimization.
  In this manuscript, we propose a Hybrid Quantum Classical selection mechanism for the Mamba architecture, designed specifically for temporal sequence classification problems. Our approach leverages Variational Quantum Circuits (VQCs) as quantum gating modules that both enhance feature extraction and improve suppression of irrelevant information. This integration directly addresses the computational bottlenecks of deep learning architectures by exploiting quantum resources for more efficient representation learning.
  We analyze how introducing quantum subroutines into large language models (LLMs) impacts their generalization capability, expressivity, and parameter efficiency. The results highlight the potential of quantum-enhanced gating mechanisms as a path toward scalable, resource-efficient NLP models, in a limited simulation step. Within the first four epochs on a reshaped MNIST dataset with input format (batch, 784, d_model), our hybrid model achieved 24.6% accuracy while using one quantum layer and achieve higher expressivity, compared to 21.6% obtained by a purely classical selection mechanism. we state No founding

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [92] [LLaDA-Rec: Discrete Diffusion for Parallel Semantic ID Generation in Generative Recommendation](https://arxiv.org/abs/2511.06254)
*Teng Shi,Chenglei Shen,Weijie Yu,Shen Nie,Chongxuan Li,Xiao Zhang,Ming He,Yan Han,Jun Xu*

Main category: cs.IR

TL;DR: LLaDA-Rec是一个基于离散扩散的生成式推荐框架，通过并行语义ID生成解决传统自回归模型的两个内在限制：单向约束和错误累积问题。


<details>
  <summary>Details</summary>
Motivation: 现有自回归模型存在两个内在限制：(1) 单向约束导致全局语义建模受限；(2) 固定从左到右的生成顺序导致早期预测错误会传播到后续标记。

Method: 提出离散扩散框架，包含三个关键设计：并行标记化方案、用户历史和下一项目级别的双重掩码机制、适配的束搜索策略。

Result: 在三个真实世界数据集上的实验表明，LLaDA-Rec在性能上持续优于基于ID的方法和最先进的生成式推荐器。

Conclusion: 离散扩散为生成式推荐建立了一个新的范式，能够更有效地建模项目间和项目内依赖关系，并缓解错误累积问题。

Abstract: Generative recommendation represents each item as a semantic ID, i.e., a sequence of discrete tokens, and generates the next item through autoregressive decoding. While effective, existing autoregressive models face two intrinsic limitations: (1) unidirectional constraints, where causal attention restricts each token to attend only to its predecessors, hindering global semantic modeling; and (2) error accumulation, where the fixed left-to-right generation order causes prediction errors in early tokens to propagate to the predictions of subsequent token. To address these issues, we propose LLaDA-Rec, a discrete diffusion framework that reformulates recommendation as parallel semantic ID generation. By combining bidirectional attention with the adaptive generation order, the approach models inter-item and intra-item dependencies more effectively and alleviates error accumulation. Specifically, our approach comprises three key designs: (1) a parallel tokenization scheme that produces semantic IDs for bidirectional modeling, addressing the mismatch between residual quantization and bidirectional architectures; (2) two masking mechanisms at the user-history and next-item levels to capture both inter-item sequential dependencies and intra-item semantic relationships; and (3) an adapted beam search strategy for adaptive-order discrete diffusion decoding, resolving the incompatibility of standard beam search with diffusion-based generation. Experiments on three real-world datasets show that LLaDA-Rec consistently outperforms both ID-based and state-of-the-art generative recommenders, establishing discrete diffusion as a new paradigm for generative recommendation.

</details>


### [93] [BiCA: Effective Biomedical Dense Retrieval with Citation-Aware Hard Negatives](https://arxiv.org/abs/2511.08029)
*Aarush Sinha,Pavan Kumar S,Roshan Balaji,Nirav Pravinbhai Bhatt*

Main category: cs.IR

TL;DR: 提出BiCA方法，利用PubMed文章中的引用链接生成高质量的困难负样本，用于改进生物医学领域的密集检索模型，在零样本检索任务中取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 生物医学和科学领域中，区分源文档和困难负样本具有挑战性。引用文档与源文档具有上下文相关性但不是重复内容，因此适合作为困难负样本。

Method: 利用20,000篇PubMed文章的引用链接进行困难负样本挖掘，对GTE_small和GTE_Base模型进行微调，使用引用感知的负样本改进密集检索性能。

Result: 在BEIR数据集上的领域内和跨领域零样本密集检索任务中，nDCG@10指标均获得一致提升；在LoTTE的长尾主题上使用Success@5指标优于基线方法。

Conclusion: 利用文档链接结构生成高信息量的负样本具有巨大潜力，能够以最少的微调实现最先进的性能，为高效数据域适应提供了可行路径。

Abstract: Hard negatives are essential for training effective retrieval models. Hard-negative mining typically relies on ranking documents using cross-encoders or static embedding models based on similarity metrics such as cosine distance. Hard negative mining becomes challenging for biomedical and scientific domains due to the difficulty in distinguishing between source and hard negative documents. However, referenced documents naturally share contextual relevance with the source document but are not duplicates, making them well-suited as hard negatives. In this work, we propose BiCA: Biomedical Dense Retrieval with Citation-Aware Hard Negatives, an approach for hard-negative mining by utilizing citation links in 20,000 PubMed articles for improving a domain-specific small dense retriever. We fine-tune the GTE_small and GTE_Base models using these citation-informed negatives and observe consistent improvements in zero-shot dense retrieval using nDCG@10 for both in-domain and out-of-domain tasks on BEIR and outperform baselines on long-tailed topics in LoTTE using Success@5. Our findings highlight the potential of leveraging document link structure to generate highly informative negatives, enabling state-of-the-art performance with minimal fine-tuning and demonstrating a path towards highly data-efficient domain adaptation.

</details>
