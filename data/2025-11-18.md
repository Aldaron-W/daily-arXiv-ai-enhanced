<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 86]
- [cs.DC](#cs.DC) [Total: 2]
- [cs.AI](#cs.AI) [Total: 9]
- [cs.CY](#cs.CY) [Total: 4]
- [cs.CE](#cs.CE) [Total: 1]
- [eess.AS](#eess.AS) [Total: 2]
- [cs.GL](#cs.GL) [Total: 1]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.IR](#cs.IR) [Total: 2]
- [cs.CR](#cs.CR) [Total: 3]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.LG](#cs.LG) [Total: 8]
- [cs.SE](#cs.SE) [Total: 1]
- [cs.CV](#cs.CV) [Total: 3]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [TimeStampEval: A Simple LLM Eval and a Little Fuzzy Matching Trick to Improve Search Accuracy](https://arxiv.org/abs/2511.11594)
*James McCammon*

Main category: cs.CL

TL;DR: 提出了TimeStampEval基准和两阶段方法，用于从长文本记录中检索非逐字引用的精确时间戳，在提高检索准确率的同时大幅降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 解决传统模糊匹配在处理语义相同但句法不同的引用时的失败问题，特别是在对齐官方书面记录和语音转文本记录的场景中。

Method: 采用两阶段方法：首先使用RapidFuzz进行预过滤，然后使用LLM在短片段上进行验证。还探索了提示设计、推理预算等优化策略。

Result: 该方法将模糊匹配准确率提高了50个百分点，同时将延迟减半，每个正确结果的成本降低了96%。在弱设置下准确率从37%提升到77%，强设置下超过90%。

Conclusion: 该方法对文本长度、词汇漂移和领域变化具有鲁棒性，在目标不存在时保持95-100%的拒绝准确率，为自动化长播客内容生成提供了可靠解决方案。

Abstract: Traditional fuzzy matching often fails when searching for quotes that are semantically identical but syntactically different across documents-a common issue when aligning official written records with speech-to-text transcripts. We introduce TimeStampEval, a benchmark for retrieving precise millisecond timestamps from long transcripts given non-verbatim quotes. Our simple two-stage method dramatically improves retrieval accuracy while cutting inference costs by over 90%. The motivating use case is an automated long-form podcast that assembles Congressional Record clips into AI-hosted narration. The technical challenge: given a sentence-timestamped transcript and a target quote that may differ due to transcription or editorial drift, return exact start and end boundaries. Standard algorithms handle verbatim text but break under fuzzier variants. Evaluating six modern LLMs on a 2,800-sentence (120k-token) transcript revealed four key findings. (1) Prompt design matters more than model choice: placing the query before the transcript and using compact formatting improved accuracy by 3-20 points while reducing token count by 30-40%. (2) Off-by-one errors form a distinct category, showing models understand the task but misplace boundaries. (3) A modest reasoning budget (600-850 tokens) raises accuracy from 37% to 77% for weak setups and to above 90% for strong ones. (4) Our "Assisted Fuzzy" approach-RapidFuzz pre-filtering followed by LLM verification on short snippets-improves fuzzy match accuracy by up to 50 points while halving latency and reducing cost per correct result by up to 96%. Extended tests on ten transcripts (50k-900k tokens, 1989-2025) confirm robustness to transcript length, vocabulary drift, and domain change, maintaining 95-100% rejection accuracy for absent targets.

</details>


### [2] [MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling](https://arxiv.org/abs/2511.11793)
*MiroMind Team,Song Bai,Lidong Bing,Carson Chen,Guanzheng Chen,Yuntao Chen,Zhe Chen,Ziyi Chen,Jifeng Dai,Xuan Dong,Yue Deng,Yunjie Fu,Junqi Ge,Chenxia Han,Tammy Huang,Zhenhang Huang,Jerry Jiao,Shilei Jiang,Tianyu Jiao,Xiaoqi Jian,Lei Lei,Ruilin Li,Ryan Luo,Tiantong Li,Xiang Lin,Ziyuan Liu,Zhiqi Li,Jie Ni,Qiang Ren,Pax Sun,Shiqian Su,Chenxin Tao,Bin Wang,Hellen Wang,Haonan Wang,James Wang,Jin Wang,Jojo Wang,Letian Wang,Shizun Wang,Weizhi Wang,Zixuan Wang,Jinfan Xu,Sen Xing,Chenyu Yang,Hai Ye,Jiaheng Yu,Yue Yu,Muyan Zhong,Tianchen Zhao,Xizhou Zhu,Yanpeng Zhou,Yifan Zhang,Zhi Zhu*

Main category: cs.CL

TL;DR: MiroThinker v1.0是一个开源研究代理，通过交互式扩展（interaction scaling）作为模型规模和上下文长度之外的第三个性能提升维度，实现了高效的工具增强推理和信息获取能力。


<details>
  <summary>Details</summary>
Motivation: 现有代理主要关注模型规模或上下文长度的扩展，但缺乏对模型与环境交互深度的系统性探索。MiroThinker旨在通过交互式扩展来提升代理在复杂研究任务中的表现。

Method: 采用强化学习方法，训练模型处理更深层次和更频繁的代理-环境交互，支持每个任务最多600次工具调用，利用环境反馈和外部信息获取来纠正错误和优化推理轨迹。

Result: 在四个基准测试（GAIA、HLE、BrowseComp、BrowseComp-ZH）中，72B变体分别达到81.9%、37.7%、47.1%和55.6%的准确率，超越了之前的开源代理并接近商业对应物如GPT-5-high。

Conclusion: 交互式扩展表现出与模型规模和上下文长度类似的扩展行为，被确立为构建下一代开源研究代理的第三个关键维度。

Abstract: We present MiroThinker v1.0, an open-source research agent designed to advance tool-augmented reasoning and information-seeking capabilities. Unlike previous agents that only scale up model size or context length, MiroThinker explores interaction scaling at the model level, systematically training the model to handle deeper and more frequent agent-environment interactions as a third dimension of performance improvement. Unlike LLM test-time scaling, which operates in isolation and risks degradation with longer reasoning chains, interactive scaling leverages environment feedback and external information acquisition to correct errors and refine trajectories. Through reinforcement learning, the model achieves efficient interaction scaling: with a 256K context window, it can perform up to 600 tool calls per task, enabling sustained multi-turn reasoning and complex real-world research workflows. Across four representative benchmarks-GAIA, HLE, BrowseComp, and BrowseComp-ZH-the 72B variant achieves up to 81.9%, 37.7%, 47.1%, and 55.6% accuracy respectively, surpassing previous open-source agents and approaching commercial counterparts such as GPT-5-high. Our analysis reveals that MiroThinker benefits from interactive scaling consistently: research performance improves predictably as the model engages in deeper and more frequent agent-environment interactions, demonstrating that interaction depth exhibits scaling behaviors analogous to model size and context length. These findings establish interaction scaling as a third critical dimension for building next-generation open research agents, complementing model capacity and context windows.

</details>


### [3] [On the Notion that Language Models Reason](https://arxiv.org/abs/2511.11810)
*Bertram Højer*

Main category: cs.CL

TL;DR: 该论文质疑语言模型是否真正具备推理能力，认为其输出只是统计规律而非逻辑机制，强调需要重新审视NLP研究中系统计算过程的描述方式。


<details>
  <summary>Details</summary>
Motivation: 评估语言模型是否真正具备推理能力，澄清当前NLP领域对"推理"概念的使用与语言模型实际工作机制之间的不一致性。

Method: 将基于Transformer的语言模型视为实现隐式有限阶马尔可夫核的系统，分析其如何将上下文映射到条件标记分布，而非实现显式逻辑机制。

Result: 语言模型的推理式输出对应于学习核中的统计规律性和近似统计不变性，而非真正的逻辑推理过程，这解释了为什么语言模型会产生推理式输出但无法保证逻辑一致性。

Conclusion: 语言模型是"统计模式匹配器"而非真正的推理者，这一区分对于评估语言模型中的认知不确定性至关重要，需要重新审视NLP研究中系统计算过程的描述方式。

Abstract: Language models (LMs) are said to be exhibiting reasoning, but what does this entail? We assess definitions of reasoning and how key papers in the field of natural language processing (NLP) use the notion and argue that the definitions provided are not consistent with how LMs are trained, process information, and generate new tokens. To illustrate this incommensurability we assume the view that transformer-based LMs implement an \textit{implicit} finite-order Markov kernel mapping contexts to conditional token distributions. In this view, reasoning-like outputs correspond to statistical regularities and approximate statistical invariances in the learned kernel rather than the implementation of explicit logical mechanisms. This view is illustrative of the claim that LMs are "statistical pattern matchers"" and not genuine reasoners and provides a perspective that clarifies why reasoning-like outputs arise in LMs without any guarantees of logical consistency. This distinction is fundamental to how epistemic uncertainty is evaluated in LMs. We invite a discussion on the importance of how the computational processes of the systems we build and analyze in NLP research are described.

</details>


### [4] [Scaling Open-Weight Large Language Models for Hydropower Regulatory Information Extraction: A Systematic Analysis](https://arxiv.org/abs/2511.11821)
*Hong-Jun Yoon,Faisal Ashraf,Thomas A. Ruggles,Debjani Singh*

Main category: cs.CL

TL;DR: 评估了7个开源大语言模型（0.6B-70B参数）在水电许可文档信息提取中的性能-资源权衡，发现14B参数是有效验证的关键阈值。


<details>
  <summary>Details</summary>
Motivation: 解决监管文档信息提取中性能与计算资源之间的关键权衡问题，为实际部署提供实证指导。

Method: 在水电许可文档上评估七个不同参数规模的开放权重模型，分析验证方法的有效性。

Result: 识别出14B参数阈值，验证方法从无效（F1<0.15）转变为可行（F1=0.64）；消费级模型可达64% F1，小型模型停滞在51%，大型模型接近77%但需企业基础设施。

Conclusion: 建立了首个监管背景下开放权重信息提取的全面资源-性能映射，为基于证据的模型选择提供指导，这些发现对水电合规具有直接价值，且参数缩放效应的见解可推广到其他信息提取任务。

Abstract: Information extraction from regulatory documents using large language models presents critical trade-offs between performance and computational resources. We evaluated seven open-weight models (0.6B-70B parameters) on hydropower licensing documentation to provide empirical deployment guidance.
  Our analysis identified a pronounced 14B parameter threshold where validation methods transition from ineffective (F1 $<$ 0.15) to viable (F1 = 0.64). Consumer-deployable models achieve 64\% F1 through appropriate validation, while smaller models plateau at 51\%. Large-scale models approach 77\% F1 but require enterprise infrastructure.
  We identified systematic hallucination patterns where perfect recall indicates extraction failure rather than success in smaller models. Our findings establish the first comprehensive resource-performance mapping for open-weight information extraction in regulatory contexts, enabling evidence-based model selection.
  These results provide immediate value for hydropower compliance while contributing insights into parameter scaling effects that generalize across information extraction tasks.

</details>


### [5] [Towards Autoformalization of LLM-generated Outputs for Requirement Verification](https://arxiv.org/abs/2511.11829)
*Mihir Gupte,Ramesh S*

Main category: cs.CL

TL;DR: 本文探索使用基于LLM的自动形式化方法来验证LLM生成的输出与自然语言需求的一致性，通过两个实验展示了该方法在一致性检查和形式验证方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 当前虽然LLM能够从自然语言生成结构化输出，但缺乏正式方法来验证这些输出的准确性。本文旨在填补这一空白。

Method: 使用简单的基于LLM的自动形式化器，将自然语言需求转换为形式逻辑，然后验证LLM生成输出与原始需求的一致性。

Result: 实验一：自动形式化器成功识别了两个不同表述的自然语言需求在逻辑上是等价的；实验二：自动形式化器识别出了给定自然语言需求与LLM生成输出之间的逻辑不一致性。

Conclusion: 虽然研究有限，但自动形式化在确保LLM生成输出的保真度和逻辑一致性方面具有显著潜力，为未来更广泛的研究奠定了基础。

Abstract: Autoformalization, the process of translating informal statements into formal logic, has gained renewed interest with the emergence of powerful Large Language Models (LLMs). While LLMs show promise in generating structured outputs from natural language (NL), such as Gherkin Scenarios from NL feature requirements, there's currently no formal method to verify if these outputs are accurate. This paper takes a preliminary step toward addressing this gap by exploring the use of a simple LLM-based autoformalizer to verify LLM-generated outputs against a small set of natural language requirements. We conducted two distinct experiments. In the first one, the autoformalizer successfully identified that two differently-worded NL requirements were logically equivalent, demonstrating the pipeline's potential for consistency checks. In the second, the autoformalizer was used to identify a logical inconsistency between a given NL requirement and an LLM-generated output, highlighting its utility as a formal verification tool. Our findings, while limited, suggest that autoformalization holds significant potential for ensuring the fidelity and logical consistency of LLM-generated outputs, laying a crucial foundation for future, more extensive studies into this novel application.

</details>


### [6] [Three Stage Narrative Analysis; Plot-Sentiment Breakdown, Structure Learning and Concept Detection](https://arxiv.org/abs/2511.11857)
*Taimur Khan,Ramoza Ahsan,Mohib Hameed*

Main category: cs.CL

TL;DR: 提出一个分析电影剧本情感弧线的框架，通过自定义词典进行情感分析，并使用层次聚类技术对相似情感模式进行分组，帮助用户选择叙事内容。


<details>
  <summary>Details</summary>
Motivation: 故事理解和分析是自然语言理解中的挑战领域，需要深度计算语义表示和句法处理。大量叙事数据需要自动化语义分析而非手动方法。

Method: 使用基于NRC-VAD数据集的Valence、Arousal和Dominance分数构建自定义词典，通过LabMTsimple storylab模块进行基于词典的情感分析，并应用Wards层次聚类技术对相似情感图进行聚类。

Result: 在电影数据集上的实验评估表明，该分析结果对消费者和读者在选择叙事或故事时很有帮助。

Conclusion: 该框架能够提取叙事中传达的高层和低层概念，通过情感弧线分析和聚类技术为故事选择提供有价值的见解。

Abstract: Story understanding and analysis have long been challenging areas within Natural Language Understanding. Automated narrative analysis requires deep computational semantic representations along with syntactic processing. Moreover, the large volume of narrative data demands automated semantic analysis and computational learning rather than manual analytical approaches. In this paper, we propose a framework that analyzes the sentiment arcs of movie scripts and performs extended analysis related to the context of the characters involved. The framework enables the extraction of high-level and low-level concepts conveyed through the narrative. Using dictionary-based sentiment analysis, our approach applies a custom lexicon built with the LabMTsimple storylab module. The custom lexicon is based on the Valence, Arousal, and Dominance scores from the NRC-VAD dataset. Furthermore, the framework advances the analysis by clustering similar sentiment plots using Wards hierarchical clustering technique. Experimental evaluation on a movie dataset shows that the resulting analysis is helpful to consumers and readers when selecting a narrative or story.

</details>


### [7] [Identifying Imaging Follow-Up in Radiology Reports: A Comparative Analysis of Traditional ML and LLM Approaches](https://arxiv.org/abs/2511.11867)
*Namu Park,Giridhar Kaushik Ramachandran,Kevin Lybarger,Fei Xia,Ozlem Uzuner,Meliha Yetisgen,Martin Gunn*

Main category: cs.CL

TL;DR: 该研究引入了一个包含6,393份放射学报告的标注语料库，用于评估大语言模型在放射学随访依从性检测任务中的性能，比较了传统机器学习方法和生成式LLMs的表现。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏专门用于评估大语言模型在放射学任务性能的领域特定数据集，需要开发可靠的基准来支持随访依从性检测系统的开发。

Method: 使用6,393份放射学报告构建标注语料库，系统比较了逻辑回归、支持向量机、Longformer、微调Llama3-8B-Instruct与生成式LLMs（GPT-4o和GPT-OSS-20B），后者在基础设置和任务优化设置下进行评估。

Result: GPT-4o（高级设置）表现最佳（F1=0.832），GPT-OSS-20B（高级设置）紧随其后（F1=0.828），逻辑回归和支持向量机也表现良好（F1=0.776和0.775）。

Conclusion: 虽然通过提示优化LLMs可以达到接近人类水平的性能，但可解释且资源效率高的传统模型仍然是重要的基准方法。

Abstract: Large language models (LLMs) have shown considerable promise in clinical natural language processing, yet few domain-specific datasets exist to rigorously evaluate their performance on radiology tasks. In this work, we introduce an annotated corpus of 6,393 radiology reports from 586 patients, each labeled for follow-up imaging status, to support the development and benchmarking of follow-up adherence detection systems. Using this corpus, we systematically compared traditional machine-learning classifiers, including logistic regression (LR), support vector machines (SVM), Longformer, and a fully fine-tuned Llama3-8B-Instruct, with recent generative LLMs. To evaluate generative LLMs, we tested GPT-4o and the open-source GPT-OSS-20B under two configurations: a baseline (Base) and a task-optimized (Advanced) setting that focused inputs on metadata, recommendation sentences, and their surrounding context. A refined prompt for GPT-OSS-20B further improved reasoning accuracy. Performance was assessed using precision, recall, and F1 scores with 95% confidence intervals estimated via non-parametric bootstrapping. Inter-annotator agreement was high (F1 = 0.846). GPT-4o (Advanced) achieved the best performance (F1 = 0.832), followed closely by GPT-OSS-20B (Advanced; F1 = 0.828). LR and SVM also performed strongly (F1 = 0.776 and 0.775), underscoring that while LLMs approach human-level agreement through prompt optimization, interpretable and resource-efficient models remain valuable baselines.

</details>


### [8] [MedPT: A Massive Medical Question Answering Dataset for Brazilian-Portuguese Speakers](https://arxiv.org/abs/2511.11878)
*Fernanda Bufon Färber,Iago Alves Brito,Julia Soares Dollis,Pedro Schindler Freire Brasil Ribeiro,Rafael Teixeira Sousa,Arlindo Rodrigues Galvão Filho*

Main category: cs.CL

TL;DR: MedPT是首个针对巴西葡萄牙语的大规模真实世界医疗语料库，包含38.4万条医患问答对，通过多阶段筛选和LLM标注增强，在医疗专科路由任务中达到94%的F1分数。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在医疗领域主要关注高资源语言的问题，为葡萄牙语等资源较少语言提供能捕捉临床和文化细微差异的医疗数据集。

Method: 构建包含38.4万条真实医患问答的语料库，采用混合定量-定性分析方法过滤噪声，通过LLM驱动标注将问题分类为7种语义类型，并进行主题分析。

Result: 数据集涵盖3,200个主题，展示了医患沟通的自然不对称性。在医疗专科路由任务中，微调1.7B参数模型在20类设置下达到94%的F1分数。

Conclusion: MedPT数据集具有深度语义丰富性，误分类反映了真实的临床模糊性，将为葡萄牙语世界开发更公平、准确和文化敏感的医疗技术提供支持。

Abstract: While large language models (LLMs) show transformative potential in healthcare, their development remains focused on high-resource languages, creating a critical barrier for others as simple translation fails to capture unique clinical and cultural nuances, such as endemic diseases. To address this, we introduce MedPT, the first large-scale, real-world corpus for Brazilian Portuguese, comprising 384,095 authentic question-answer pairs from patient-doctor interactions. The dataset underwent a meticulous multi-stage curation protocol, using a hybrid quantitative-qualitative analysis to filter noise and contextually enrich thousands of ambiguous queries. We further augmented the corpus via LLM-driven annotation, classifying questions into seven semantic types to capture user intent. Our analysis reveals its thematic breadth (3,200 topics) and unique linguistic properties, like the natural asymmetry in patient-doctor communication. To validate its utility, we benchmark a medical specialty routing task: fine-tuning a 1.7B parameter model achieves an outstanding 94\% F1-score on a 20-class setup. Furthermore, our qualitative error analysis shows misclassifications are not random but reflect genuine clinical ambiguities (e.g., between comorbid conditions), proving the dataset's deep semantic richness. We publicly release MedPT to foster the development of more equitable, accurate, and culturally-aware medical technologies for the Portuguese-speaking world.

</details>


### [9] [ClinStructor: AI-Powered Structuring of Unstructured Clinical Texts](https://arxiv.org/abs/2511.11883)
*Karthikeyan K,Raghuveer Thirukovalluru,David Carlson*

Main category: cs.CL

TL;DR: ClinStructor使用大语言模型将临床自由文本转换为结构化的问答对，以提高透明度、可控性和泛化能力，在ICU死亡率预测任务中仅导致AUC轻微下降2-3%。


<details>
  <summary>Details</summary>
Motivation: 解决临床笔记非结构化格式带来的挑战，包括无意偏见、跨临床环境泛化能力差和可解释性差的问题。

Method: 利用大语言模型将临床自由文本转换为结构化的、任务特定的问答对，然后进行预测建模。

Result: 在ICU死亡率预测任务中，与直接微调相比，仅导致AUC轻微下降2-3%，但显著提高了透明度和可控性。

Conclusion: ClinStructor为在临床环境中构建可靠、可解释和可泛化的机器学习模型奠定了坚实基础。

Abstract: Clinical notes contain valuable, context-rich information, but their unstructured format introduces several challenges, including unintended biases (e.g., gender or racial bias), and poor generalization across clinical settings (e.g., models trained on one EHR system may perform poorly on another due to format differences) and poor interpretability. To address these issues, we present ClinStructor, a pipeline that leverages large language models (LLMs) to convert clinical free-text into structured, task-specific question-answer pairs prior to predictive modeling. Our method substantially enhances transparency and controllability and only leads to a modest reduction in predictive performance (a 2-3% drop in AUC), compared to direct fine-tuning, on the ICU mortality prediction task. ClinStructor lays a strong foundation for building reliable, interpretable, and generalizable machine learning models in clinical environments.

</details>


### [10] [Context-Emotion Aware Therapeutic Dialogue Generation: A Multi-component Reinforcement Learning Approach to Language Models for Mental Health Support](https://arxiv.org/abs/2511.11884)
*Eric Hua Qing Zhang,Julia Ive*

Main category: cs.CL

TL;DR: 本文研究了通过监督微调(SFT)和强化学习(RL)技术增强GPT-2生成治疗性对话的能力，在多个评估指标上取得显著改进，特别是情感准确率达到99.34%。


<details>
  <summary>Details</summary>
Motivation: 心理健康疾病造成巨大社会经济负担，COVID-19加剧了获取障碍并增加了对远程心理健康支持的需求。虽然大语言模型提供24/7可用性和非评判性互动，但预训练模型缺乏必要的上下文和情感意识来提供适当的治疗响应。

Method: 重构输入格式以同时处理上下文信息和情感状态，采用多组件奖励函数使模型输出与专业治疗师响应和标注情感对齐，应用监督微调和强化学习技术。

Result: 强化学习在多个评估指标上优于基线GPT-2：BLEU(0.0111)、ROUGE-1(0.1397)、ROUGE-2(0.0213)、ROUGE-L(0.1317)、METEOR(0.0581)。LLM评估确认高上下文相关性和专业性，强化学习的情感准确率达到99.34%，而基线GPT-2为66.96%。

Conclusion: 强化学习在开发治疗性对话系统方面有效，可作为治疗师的有价值辅助工具，同时保持必要的人类临床监督。

Abstract: Mental health illness represents a substantial global socioeconomic burden, with COVID-19 further exacerbating accessibility challenges and driving increased demand for telehealth mental health support. While large language models (LLMs) offer promising solutions through 24/7 availability and non-judgmental interactions, pre-trained models often lack the contextual and emotional awareness necessary for appropriate therapeutic responses. This paper investigated the application of supervised fine-tuning (SFT) and reinforcement learning (RL) techniques to enhance GPT-2's capacity for therapeutic dialogue generation. The methodology restructured input formats to enable simultaneous processing of contextual information and emotional states alongside user input, employing a multi-component reward function that aligned model outputs with professional therapist responses and annotated emotions. Results demonstrated improvements through reinforcement learning over baseline GPT-2 across multiple evaluation metrics: BLEU (0.0111), ROUGE-1 (0.1397), ROUGE-2 (0.0213), ROUGE-L (0.1317), and METEOR (0.0581). LLM evaluation confirmed high contextual relevance and professionalism, while reinforcement learning achieved 99.34% emotion accuracy compared to 66.96% for baseline GPT-2. These findings demonstrate reinforcement learning's effectiveness in developing therapeutic dialogue systems that can serve as valuable assistive tools for therapists while maintaining essential human clinical oversight.

</details>


### [11] [Additive Large Language Models for Semi-Structured Text](https://arxiv.org/abs/2511.11922)
*Karthikeyan K,Raghuveer Thirukovalluru,David Carlson*

Main category: cs.CL

TL;DR: CALM是一个可解释的临床文本分类框架，通过将预测分解为各个语义组件的加和贡献，提供透明的风险解释。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在临床文本分类中预测不透明的问题，让研究人员和医生能够理解患者记录中哪些部分驱动风险信号。

Method: 使用加性大语言模型框架，将半结构化文本输入分解为语义组件，预测结果是各组件贡献的加和，支持组件级风险曲线可视化。

Result: CALM在保持与传统LLM分类器相当性能的同时，提高了模型可信度，支持质量保证检查，并在模型开发和审计中揭示临床有意义的模式。

Conclusion: CALM框架为临床文本分类提供了可解释的解决方案，平衡了预测准确性和模型透明度，有助于促进LLM在临床实践中的实际应用。

Abstract: Large Language Models have advanced clinical text classification, but their opaque predictions remain a critical barrier to practical adoption in research and clinical settings where investigators and physicians need to understand which parts of a patient's record drive risk signals. To address this challenge, we introduce \textbf{CALM}, short for \textbf{Classification with Additive Large Language Models}, an interpretable framework for semi-structured text where inputs are composed of semantically meaningful components, such as sections of an admission note or question-answer fields from an intake form. CALM predicts outcomes as the additive sum of each component's contribution, making these contributions part of the forward computation itself and enabling faithful explanations at both the patient and population level. The additive structure also enables clear visualizations, such as component-level risk curves similar to those used in generalized additive models, making the learned relationships easier to inspect and communicate. Although CALM expects semi-structured inputs, many clinical documents already have this form, and similar structure can often be automatically extracted from free-text notes. CALM achieves performance comparable to conventional LLM classifiers while improving trust, supporting quality-assurance checks, and revealing clinically meaningful patterns during model development and auditing.

</details>


### [12] [InData: Towards Secure Multi-Step, Tool-Based Data Analysis](https://arxiv.org/abs/2511.11933)
*Karthikeyan K,Raghuveer Thirukovalluru,Bhuwan Dhingra,David Edwin Carlson*

Main category: cs.CL

TL;DR: 提出了InData数据集来评估LLM在多步骤工具推理方面的能力，发现在复杂任务上现有模型表现显著下降


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理直接生成和执行代码访问敏感数据存在安全风险，需要限制LLM只能通过预定义的安全工具与数据交互

Method: 创建InData数据集，包含三个难度级别的数据分析问题，评估15个开源LLM在多步骤工具推理上的表现

Result: 大型模型在简单任务上准确率达97.3%，但在困难任务上降至69.6%，显示当前LLM缺乏稳健的多步骤工具推理能力

Conclusion: InData数据集有助于开发和评估具有更强多步骤工具使用能力的LLM，将公开数据集和代码

Abstract: Large language model agents for data analysis typically generate and execute code directly on databases. However, when applied to sensitive data, this approach poses significant security risks. To address this issue, we propose a security-motivated alternative: restrict LLMs from direct code generation and data access, and require them to interact with data exclusively through a predefined set of secure, verified tools. Although recent tool-use benchmarks exist, they primarily target tool selection and simple execution rather than the compositional, multi-step reasoning needed for complex data analysis. To reduce this gap, we introduce Indirect Data Engagement (InData), a dataset designed to assess LLMs' multi-step tool-based reasoning ability. InData includes data analysis questions at three difficulty levels--Easy, Medium, and Hard--capturing increasing reasoning complexity. We benchmark 15 open-source LLMs on InData and find that while large models (e.g., gpt-oss-120b) achieve high accuracy on Easy tasks (97.3%), performance drops sharply on Hard tasks (69.6%). These results show that current LLMs still lack robust multi-step tool-based reasoning ability. With InData, we take a step toward enabling the development and evaluation of LLMs with stronger multi-step tool-use capabilities. We will publicly release the dataset and code.

</details>


### [13] [Improving LLM's Attachment to External Knowledge In Dialogue Generation Tasks Through Entity Anonymization](https://arxiv.org/abs/2511.11946)
*Hadi Sheikhi,Chenyang Huang,Osmar R. Zaïane*

Main category: cs.CL

TL;DR: 本文提出了LLM-KAT评估方法和实体匿名化技术，以解决大语言模型在知识图谱对话生成中过度依赖内部知识而忽视外部知识图谱的问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在知识图谱对话生成任务中倾向于依赖内部知识，即使提供了完美的知识图谱检索结果，也经常与外部知识脱节。

Method: 提出LLM-KAT评估程序来衡量生成回复中的知识附着程度，并采用简单的实体匿名化技术来鼓励LLMs更好地利用外部知识。

Result: 在OpenDialKG数据集上的实验表明，该方法提高了LLMs对外部知识的附着能力。

Conclusion: 实体匿名化是一种简单有效的技术，能够改善大语言模型在知识图谱对话生成中对外部知识的利用。

Abstract: Knowledge graph-based dialogue generation (KG-DG) is a challenging task requiring models to effectively incorporate external knowledge into conversational responses. While large language models (LLMs) have achieved impressive results across various NLP tasks, their ability to utilize external knowledge in KG-DG remains under-explored. We observe that LLMs often rely on internal knowledge, leading to detachment from provided knowledge graphs, even when they are given a flawlessly retrieved knowledge graph. First, we introduce LLM-KAT, an evaluation procedure for measuring knowledge attachment in generated responses. Second, we propose a simple yet effective entity anonymization technique to encourage LLMs to better leverage external knowledge. Experiments on the OpenDialKG dataset demonstrate that our approach improves LLMs' attachment on external knowledge.

</details>


### [14] [On the Entropy Calibration of Language Models](https://arxiv.org/abs/2511.11966)
*Steven Cao,Gregory Valiant,Percy Liang*

Main category: cs.CL

TL;DR: 研究语言模型的熵校准问题，发现模型存在校准误差，且这种误差随规模扩大改善缓慢，需要截断分布来平衡文本质量与多样性。


<details>
  <summary>Details</summary>
Motivation: 解决自回归模型中熵校准不匹配的问题，探索校准误差是否随模型规模扩大而改善，以及理论上是否存在无代价的校准方法。

Method: 首先在简化理论设置中分析校准误差的缩放行为，然后在0.5B到70B参数的语言模型中实证测量校准误差。

Result: 发现校准误差的缩放指数接近0，意味着大模型与小模型以相似速率积累误差，解释了为什么即使大模型质量更高，仍需相似程度的截断采样。

Conclusion: 理论上证明在假设存在能预测文本未来熵的黑盒模型时，可以在不增加对数损失的情况下减少熵，但实际中校准误差改善缓慢。

Abstract: We study the problem of entropy calibration, which asks whether a language model's entropy over generations matches its log loss on human text. Past work found that models are miscalibrated, with entropy per step increasing (and text quality decreasing) as generations grow longer. This error accumulation is a fundamental problem in autoregressive models, and the standard solution is to truncate the distribution, which improves text quality at the cost of diversity. In this paper, we ask: is miscalibration likely to improve with scale, and is it theoretically possible to calibrate without tradeoffs? To build intuition, we first study a simplified theoretical setting to characterize the scaling behavior of miscalibration with respect to dataset size. We find that the scaling behavior depends on the power law exponent of the data distribution -- in particular, for a power law exponent close to 1, the scaling exponent is close to 0, meaning that miscalibration improves very slowly with scale. Next, we measure miscalibration empirically in language models ranging from 0.5B to 70B parameters. We find that the observed scaling behavior is similar to what is predicted by the simplified setting: our fitted scaling exponents for text are close to 0, meaning that larger models accumulate error at a similar rate as smaller ones. This scaling (or, lack thereof) provides one explanation for why we sample from larger models with similar amounts of truncation as smaller models, even though the larger models are of higher quality. However, truncation is not a satisfying solution because it comes at the cost of increased log loss. In theory, is it even possible to reduce entropy while preserving log loss? We prove that it is possible, if we assume access to a black box which can fit models to predict the future entropy of text.

</details>


### [15] [A Reasoning Paradigm for Named Entity Recognition](https://arxiv.org/abs/2511.11978)
*Hui Huang,Yanping Chen,Ruizhang Huang,Chuan Lin,Yongbin Qin*

Main category: cs.CL

TL;DR: 提出ReasoningNER框架，通过显式推理机制改进命名实体识别，在零样本场景下超越GPT-4 12.3个百分点


<details>
  <summary>Details</summary>
Motivation: 传统生成式LLM在NER任务中依赖隐式模式匹配，缺乏可验证的推理机制，导致零样本和低资源场景下性能不佳

Method: 三阶段框架：1)生成NER导向的思维链数据集；2)使用CoT调整模型生成推理过程；3)推理增强阶段优化推理过程

Result: 在零样本设置下达到SOTA性能，F1分数比GPT-4高12.3个百分点，展示了强大的认知能力

Conclusion: ReasoningNER框架能有效提升NER任务的推理能力，为面向推理的信息提取研究提供了重要进展

Abstract: Generative LLMs typically improve Named Entity Recognition (NER) performance through instruction tuning. They excel at generating entities by semantic pattern matching but lack an explicit, verifiable reasoning mechanism. This "cognitive shortcutting" leads to suboptimal performance and brittle generalization, especially in zero-shot and lowresource scenarios where reasoning from limited contextual cues is crucial. To address this issue, a reasoning framework is proposed for NER, which shifts the extraction paradigm from implicit pattern matching to explicit reasoning. This framework consists of three stages: Chain of Thought (CoT) generation, CoT tuning, and reasoning enhancement. First, a dataset annotated with NER-oriented CoTs is generated, which contain task-relevant reasoning chains. Then, they are used to tune the NER model to generate coherent rationales before deriving the final answer. Finally, a reasoning enhancement stage is implemented to optimize the reasoning process using a comprehensive reward signal. This stage ensures explicit and verifiable extractions. Experiments show that ReasoningNER demonstrates impressive cognitive ability in the NER task, achieving competitive performance. In zero-shot settings, it achieves state-of-the-art (SOTA) performance, outperforming GPT-4 by 12.3 percentage points on the F1 score. Analytical results also demonstrate its great potential to advance research in reasoningoriented information extraction. Our codes are available at https://github.com/HuiResearch/ReasoningIE.

</details>


### [16] [Critical or Compliant? The Double-Edged Sword of Reasoning in Chain-of-Thought Explanations](https://arxiv.org/abs/2511.12001)
*Eunkyu Park,Wesley Hanwen Deng,Vasudha Varadarajan,Mingxi Yan,Gunhee Kim,Maarten Sap,Motahhare Eslami*

Main category: cs.CL

TL;DR: 本研究探讨了思维链解释在道德场景中的双重作用：既能提高透明度，也可能强化确认偏见，导致用户忽视错误的推理过程。


<details>
  <summary>Details</summary>
Motivation: 研究思维链解释如何影响用户对AI系统的信任和错误检测能力，特别是在多模态道德场景中，揭示解释可能带来的误导效应。

Method: 通过系统性地扰动推理链和操纵表达语气，分析视觉语言模型中的推理错误及其对用户信任和错误检测能力的影响。

Result: 发现用户常将信任等同于结果一致性，即使推理存在缺陷也保持依赖；自信的语气会抑制错误检测但维持依赖，表明表达风格可以凌驾于正确性之上。

Conclusion: 思维链解释既能澄清也能误导，NLP系统需要提供鼓励审慎思考和批判性评估而非盲目信任的解释机制。

Abstract: Explanations are often promoted as tools for transparency, but they can also foster confirmation bias; users may assume reasoning is correct whenever outputs appear acceptable. We study this double-edged role of Chain-of-Thought (CoT) explanations in multimodal moral scenarios by systematically perturbing reasoning chains and manipulating delivery tones. Specifically, we analyze reasoning errors in vision language models (VLMs) and how they impact user trust and the ability to detect errors. Our findings reveal two key effects: (1) users often equate trust with outcome agreement, sustaining reliance even when reasoning is flawed, and (2) the confident tone suppresses error detection while maintaining reliance, showing that delivery styles can override correctness. These results highlight how CoT explanations can simultaneously clarify and mislead, underscoring the need for NLP systems to provide explanations that encourage scrutiny and critical thinking rather than blind trust. All code will be released publicly.

</details>


### [17] [CURE: Cultural Understanding and Reasoning Evaluation - A Framework for "Thick" Culture Alignment Evaluation in LLMs](https://arxiv.org/abs/2511.12014)
*Truong Vo,Sanmi Koyejo*

Main category: cs.CL

TL;DR: 该论文提出了一个用于评估大型语言模型文化能力的新基准，通过真实情境上下文来测试文化推理能力，并引入了四个补充指标来更全面地评估响应质量。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型文化能力评估方法局限于去语境化的正确性或强制选择判断，忽视了文化理解和推理的需求。

Method: 引入基于真实情境上下文的基准测试，除了标准精确匹配指标外，还增加了覆盖度、特异性、内涵和连贯性四个补充指标。

Result: 实证分析发现，传统薄评估会系统性地高估文化能力且评估结果不稳定，而厚评估能揭示推理深度差异、降低方差并提供更稳定的文化理解信号。

Conclusion: 厚评估方法能更准确地衡量大型语言模型的文化能力，提供更稳定和可解释的文化理解评估信号。

Abstract: Large language models (LLMs) are increasingly deployed in culturally diverse environments, yet existing evaluations of cultural competence remain limited. Existing methods focus on de-contextualized correctness or forced-choice judgments, overlooking the need for cultural understanding and reasoning required for appropriate responses. To address this gap, we introduce a set of benchmarks that, instead of directly probing abstract norms or isolated statements, present models with realistic situational contexts that require culturally grounded reasoning. In addition to the standard Exact Match metric, we introduce four complementary metrics (Coverage, Specificity, Connotation, and Coherence) to capture different dimensions of model's response quality. Empirical analysis across frontier models reveals that thin evaluation systematically overestimates cultural competence and produces unstable assessments with high variance. In contrast, thick evaluation exposes differences in reasoning depth, reduces variance, and provides more stable, interpretable signals of cultural understanding.

</details>


### [18] [Exploring Parameter-Efficient Fine-Tuning and Backtranslation for the WMT 25 General Translation Task](https://arxiv.org/abs/2511.12109)
*Felipe Fujita,Hideyuki Takada*

Main category: cs.CL

TL;DR: 结合回译和微调在小规模日语语料上显著提升英日神经机器翻译质量，COMET分数从0.460提升至0.597。


<details>
  <summary>Details</summary>
Motivation: 探索在有限训练数据下，如何通过回译和微调的协同使用来提升低资源语言对的翻译质量。

Method: 首先使用回译生成合成数据增强模型，然后在真实小规模平行数据集上进行微调，最后将两种方法结合使用。

Result: 单独回译将COMET从0.460提升至0.468，单独微调提升至0.589，两者结合达到0.597的最佳效果。

Conclusion: 回译和针对性微调的协同使用能够显著提升翻译质量，为低资源语言对提供轻量级但强大的改进策略。

Abstract: In this paper, we explore the effectiveness of combining fine-tuning and backtranslation on a small Japanese corpus for neural machine translation. Starting from a baseline English{\textrightarrow}Japanese model (COMET = 0.460), we first apply backtranslation (BT) using synthetic data generated from monolingual Japanese corpora, yielding a modest increase (COMET = 0.468). Next, we fine-tune (FT) the model on a genuine small parallel dataset drawn from diverse Japanese news and literary corpora, achieving a substantial jump to COMET = 0.589 when using Mistral 7B. Finally, we integrate both backtranslation and fine-tuning{ -- }first augmenting the small dataset with BT generated examples, then adapting via FT{ -- }which further boosts performance to COMET = 0.597. These results demonstrate that, even with limited training data, the synergistic use of backtranslation and targeted fine-tuning on Japanese corpora can significantly enhance translation quality, outperforming each technique in isolation. This approach offers a lightweight yet powerful strategy for improving low-resource language pairs.

</details>


### [19] [LLMLagBench: Identifying Temporal Training Boundaries in Large Language Models](https://arxiv.org/abs/2511.12116)
*Piotr Pęzik,Konrad Kaczyński,Maria Szymańska,Filip Żarnecki,Zuzanna Deckert,Jakub Kwiatkowski,Wojciech Janowski*

Main category: cs.CL

TL;DR: LLMLagBench是一个用于评估LLM训练数据时间边界的基准测试，通过检测模型对近期事件的了解来识别知识截止时间。


<details>
  <summary>Details</summary>
Motivation: LLM在特定时间点前的文本数据上预训练，这形成了严格的知识边界，模型可能无意中将过时的时效性信息与通用知识混合，影响回答准确性。

Method: 引入LLMLagBench基准测试，通过评估LLM对近期事件的知识来系统识别其训练数据的最早可能时间边界，并对大量LLM进行评估。

Result: 该基准测试应用于包括明确声明和未声明训练截止时间的模型，通过人工验证和与公开预训练信息比较来评估可靠性。

Conclusion: LLMLagBench提供了一种系统方法来识别LLM的知识时间边界，有助于理解模型的知识局限性。

Abstract: Large Language Models (LLMs) are pretrained on textual data up to a specific temporal cutoff. This creates a strict knowledge boundary beyond which models cannot provide accurate information without querying external sources. More subtly, when this limitation is unknown or ignored, LLMs may inadvertently blend outdated time-sensitive information with general knowledge during reasoning tasks, potentially compromising response accuracy. We introduce LLMLagBench, an LLM freshness benchmark, as a systematic approach for identifying the earliest probable temporal boundaries of an LLM's training data by evaluating its knowledge of recent events. We then apply this benchmark to evaluate a large set of LLMs, including models with both explicitly declared and undeclared training cutoffs. The reliability of the benchmark is assessed by manual validation and comparison with publicly released information about LLM pretraining.

</details>


### [20] [PRISM of Opinions: A Persona-Reasoned Multimodal Framework for User-centric Conversational Stance Detection](https://arxiv.org/abs/2511.12130)
*Bingbing Wang,Zhixin Bai,Zhengda Jin,Zihan Wang,Xintong Song,Jingjie Lin,Sixuan Li,Jing Li,Ruifeng Xu*

Main category: cs.CL

TL;DR: 提出了U-MStance数据集和PRISM模型，解决多模态对话立场检测中的伪多模态和用户同质性问题，通过用户画像和多模态对齐提升立场检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究存在伪多模态（仅源帖子有视觉内容，评论被当作纯文本）和用户同质化（忽略用户个性特征）的局限性，无法反映真实世界多模态交互。

Method: PRISM模型：1）从历史帖子和评论中提取纵向用户画像；2）通过思维链对齐对话上下文中的文本和视觉线索；3）使用相互任务强化机制联合优化立场检测和立场感知响应生成。

Result: 在U-MStance数据集上的实验表明，PRISM相比强基线模型取得了显著性能提升。

Conclusion: 用户中心和上下文基础的多模态推理对于现实立场理解具有有效性。

Abstract: The rapid proliferation of multimodal social media content has driven research in Multimodal Conversational Stance Detection (MCSD), which aims to interpret users' attitudes toward specific targets within complex discussions. However, existing studies remain limited by: **1) pseudo-multimodality**, where visual cues appear only in source posts while comments are treated as text-only, misaligning with real-world multimodal interactions; and **2) user homogeneity**, where diverse users are treated uniformly, neglecting personal traits that shape stance expression. To address these issues, we introduce **U-MStance**, the first user-centric MCSD dataset, containing over 40k annotated comments across six real-world targets. We further propose **PRISM**, a **P**ersona-**R**easoned mult**I**modal **S**tance **M**odel for MCSD. PRISM first derives longitudinal user personas from historical posts and comments to capture individual traits, then aligns textual and visual cues within conversational context via Chain-of-Thought to bridge semantic and pragmatic gaps across modalities. Finally, a mutual task reinforcement mechanism is employed to jointly optimize stance detection and stance-aware response generation for bidirectional knowledge transfer. Experiments on U-MStance demonstrate that PRISM yields significant gains over strong baselines, underscoring the effectiveness of user-centric and context-grounded multimodal reasoning for realistic stance understanding.

</details>


### [21] [AI-Salesman: Towards Reliable Large Language Model Driven Telemarketing](https://arxiv.org/abs/2511.12133)
*Qingyu Zhang,Chunlei Xin,Xuanang Chen,Yaojie Lu,Hongyu Lin,Xianpei Han,Le Sun,Qing Ye,Qianlong Xie,Xingxing Wang*

Main category: cs.CL

TL;DR: 提出了AI-Salesman框架，通过双阶段架构解决目标驱动说服对话中的多轮规划和事实忠实性问题，显著优于基线模型


<details>
  <summary>Details</summary>
Motivation: 目标驱动说服对话（如电话营销）需要复杂多轮规划和严格事实忠实性，现有LLMs存在战略脆弱性和事实幻觉问题，且缺乏领域特定数据

Method: 构建TeleSalesCorpus数据集，提出AI-Salesman框架：训练阶段使用贝叶斯监督强化学习从噪声对话中学习稳健销售策略；推理阶段采用动态大纲引导代理（DOGA），利用预构建脚本库提供动态策略指导

Result: 实验结果表明AI-Salesman在自动指标和综合人工评估中显著优于基线模型，在复杂说服场景中表现出色

Conclusion: AI-Salesman框架通过双阶段架构和综合评估框架，有效解决了目标驱动说服对话中的关键挑战，展示了在复杂说服场景中的有效性

Abstract: Goal-driven persuasive dialogue, exemplified by applications like telemarketing, requires sophisticated multi-turn planning and strict factual faithfulness, which remains a significant challenge for even state-of-the-art Large Language Models (LLMs). A lack of task-specific data often limits previous works, and direct LLM application suffers from strategic brittleness and factual hallucination. In this paper, we first construct and release TeleSalesCorpus, the first real-world-grounded dialogue dataset for this domain. We then propose AI-Salesman, a novel framework featuring a dual-stage architecture. For the training stage, we design a Bayesian-supervised reinforcement learning algorithm that learns robust sales strategies from noisy dialogues. For the inference stage, we introduce the Dynamic Outline-Guided Agent (DOGA), which leverages a pre-built script library to provide dynamic, turn-by-turn strategic guidance. Moreover, we design a comprehensive evaluation framework that combines fine-grained metrics for key sales skills with the LLM-as-a-Judge paradigm. Experimental results demonstrate that our proposed AI-Salesman significantly outperforms baseline models in both automatic metrics and comprehensive human evaluations, showcasing its effectiveness in complex persuasive scenarios.

</details>


### [22] [Seeing is Believing: Rich-Context Hallucination Detection for MLLMs via Backward Visual Grounding](https://arxiv.org/abs/2511.12140)
*Pinxue Guo,Chongruo Wu,Xinyu Zhou,Lingyi Hong,Zhaoyu Chen,Jinglun Li,Kaixun Jiang,Sen-ching Samson Cheung,Wei Zhang,Wenqiang Zhang*

Main category: cs.CL

TL;DR: VBackChecker是一个无需参考的幻觉检测框架，通过像素级Grounding LLM验证MLLM生成响应与视觉输入的一致性，在R²-HalBench基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型存在严重幻觉问题，需要准确检测以确保实际应用可靠性。

Method: 基于"眼见为实"原则，利用具备推理和分割能力的像素级Grounding LLM，设计指令调优数据生成管道R-Instruct，构建R²-HalBench基准。

Result: 在R²-HalBench上超越先前复杂框架，性能媲美GPT-4o，像素级定位任务提升超10%。

Conclusion: VBackChecker为MLLM幻觉检测提供了有效且可解释的解决方案，在丰富上下文场景中表现优异。

Abstract: Multimodal Large Language Models (MLLMs) have unlocked powerful cross-modal capabilities, but still significantly suffer from hallucinations. As such, accurate detection of hallucinations in MLLMs is imperative for ensuring their reliability in practical applications. To this end, guided by the principle of "Seeing is Believing", we introduce VBackChecker, a novel reference-free hallucination detection framework that verifies the consistency of MLLMgenerated responses with visual inputs, by leveraging a pixellevel Grounding LLM equipped with reasoning and referring segmentation capabilities. This reference-free framework not only effectively handles rich-context scenarios, but also offers interpretability. To facilitate this, an innovative pipeline is accordingly designed for generating instruction-tuning data (R-Instruct), featuring rich-context descriptions, grounding masks, and hard negative samples. We further establish R^2 -HalBench, a new hallucination benchmark for MLLMs, which, unlike previous benchmarks, encompasses real-world, rich-context descriptions from 18 MLLMs with high-quality annotations, spanning diverse object-, attribute, and relationship-level details. VBackChecker outperforms prior complex frameworks and achieves state-of-the-art performance on R^2 -HalBench, even rivaling GPT-4o's capabilities in hallucination detection. It also surpasses prior methods in the pixel-level grounding task, achieving over a 10% improvement. All codes, data, and models are available at https://github.com/PinxueGuo/VBackChecker.

</details>


### [23] [CriticSearch: Fine-Grained Credit Assignment for Search Agents via a Retrospective Critic](https://arxiv.org/abs/2511.12159)
*Yaocheng Zhang,Haohuan Huang,Zijun Song,Yuanheng Zhu,Qichao Zhang,Zijie Zhao,Dongbin Zhao*

Main category: cs.CL

TL;DR: CriticSearch是一个细粒度信用分配框架，通过回顾性批评机制为搜索代理提供密集的回合级反馈，解决了传统强化学习方法中稀疏奖励导致的低效探索和不稳定训练问题。


<details>
  <summary>Details</summary>
Motivation: 现有的搜索代理管道通常依赖基于强化学习的优化，但存在稀疏结果奖励问题，导致探索效率低下和训练不稳定。

Method: 引入CriticSearch框架，使用冻结的非对称批评LLM回顾性地评估每个回合，利用完整轨迹和黄金答案的特权信息，将这些评估转化为稳定的密集奖励来指导策略改进。

Result: 在多个多跳推理基准测试中，CriticSearch始终优于现有基线方法，实现了更快的收敛速度、更好的训练稳定性和更高的性能。

Conclusion: CriticSearch通过细粒度的信用分配机制有效解决了搜索代理训练中的稀疏奖励问题，显著提升了模型在复杂问答任务中的表现。

Abstract: Tool-Integrated Reasoning (TIR) with search engines enables large language models to iteratively retrieve up-to-date external knowledge, enhancing adaptability and generalization in complex question-answering tasks. However, existing search agent pipelines typically depend on reinforcement learning based optimization, which often suffers from sparse outcome rewards, leading to inefficient exploration and unstable training. We introduce CriticSearch, a fine-grained credit-assignment framework that supplies dense, turn-level feedback via a retrospective critic mechanism. During training, a frozen, asymmetric critique LLM retrospectively evaluates each turn using privileged information from the full trajectory and gold answers, converting these assessments into stable, dense rewards that guide policy improvement. Experimental results across diverse multi-hop reasoning benchmarks demonstrate that CriticSearch consistently outperforms existing baselines, achieving faster convergence, improved training stability, and higher performance.

</details>


### [24] [MME-RAG: Multi-Manager-Expert Retrieval-Augmented Generation for Fine-Grained Entity Recognition in Task-Oriented Dialogues](https://arxiv.org/abs/2511.12213)
*Liang Xue,Haoyu Liu,Yajun Tian,Xinyu Zhong,Yang Liu*

Main category: cs.CL

TL;DR: MME-RAG是一个多管理器-专家检索增强生成框架，将实体识别分解为类型级判断和跨度级提取两个协调阶段，通过轻量级管理器和专业专家实现精确且领域自适应的实体提取，无需额外训练。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在任务导向对话中的领域适应性和检索可控性方面仍面临挑战，特别是在细粒度实体识别任务中。

Method: 采用分层分解方法：轻量级管理器进行类型级判断，专业专家进行跨度级提取，每个专家配备KeyInfo检索器注入语义对齐的少样本示例。

Result: 在CrossNER、MIT-Movie、MIT-Restaurant和新构建的多领域客服数据集上的实验表明，MME-RAG在大多数领域优于现有基线方法。

Conclusion: 分层分解和KeyInfo引导的检索是鲁棒性和跨领域泛化的关键驱动因素，MME-RAG为自适应对话理解提供了一个可扩展且可解释的解决方案。

Abstract: Fine-grained entity recognition is crucial for reasoning and decision-making in task-oriented dialogues, yet current large language models (LLMs) continue to face challenges in domain adaptation and retrieval controllability. We introduce MME-RAG, a Multi-Manager-Expert Retrieval-Augmented Generation framework that decomposes entity recognition into two coordinated stages: type-level judgment by lightweight managers and span-level extraction by specialized experts. Each expert is supported by a KeyInfo retriever that injects semantically aligned, few-shot exemplars during inference, enabling precise and domain-adaptive extraction without additional training. Experiments on CrossNER, MIT-Movie, MIT-Restaurant, and our newly constructed multi-domain customer-service dataset demonstrate that MME-RAG performs better than recent baselines in most domains. Ablation studies further show that both the hierarchical decomposition and KeyInfo-guided retrieval are key drivers of robustness and cross-domain generalization, establishing MME-RAG as a scalable and interpretable solution for adaptive dialogue understanding.

</details>


### [25] [Consistency Is the Key: Detecting Hallucinations in LLM Generated Text By Checking Inconsistencies About Key Facts](https://arxiv.org/abs/2511.12236)
*Raavi Gupta,Pranav Hari Panicker,Sumit Bhatia,Ganesh Ramakrishnan*

Main category: cs.CL

TL;DR: CONFACTCHECK是一个高效的幻觉检测方法，通过检查生成文本中事实探针的一致性来检测幻觉，无需外部知识库，比现有方法更节省资源且准确率更高。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型经常产生事实错误的幻觉文本，在医疗、金融等高风险领域造成严重风险。现有检测方法在模型访问受限时需要多次API调用，增加延迟和成本。

Method: 基于直觉：生成文本中对事实探针的响应应该在单个LLM内和不同LLM间保持一致。该方法不依赖外部知识库，通过检查事实一致性来检测幻觉。

Result: 在多个数据集上的实验表明，CONFACTCHECK能够用更少资源高效检测幻觉事实，在相似条件下比现有基线方法获得更高的准确率。

Conclusion: CONFACTCHECK提供了一种资源高效的幻觉检测解决方案，特别适用于模型访问受限或资源受限的场景，无需外部知识库即可有效工作。

Abstract: Large language models (LLMs), despite their remarkable text generation capabilities, often hallucinate and generate text that is factually incorrect and not grounded in real-world knowledge. This poses serious risks in domains like healthcare, finance, and customer support. A typical way to use LLMs is via the APIs provided by LLM vendors where there is no access to model weights or options to fine-tune the model. Existing methods to detect hallucinations in such settings where the model access is restricted or constrained by resources typically require making multiple LLM API calls, increasing latency and API cost. We introduce CONFACTCHECK, an efficient hallucination detection approach that does not leverage any external knowledge base and works on the simple intuition that responses to factual probes within the generated text should be consistent within a single LLM and across different LLMs. Rigorous empirical evaluation on multiple datasets that cover both the generation of factual texts and the open generation shows that CONFACTCHECK can detect hallucinated facts efficiently using fewer resources and achieves higher accuracy scores compared to existing baselines that operate under similar conditions. Our code is available here.

</details>


### [26] [ViConBERT: Context-Gloss Aligned Vietnamese Word Embedding for Polysemous and Sense-Aware Representations](https://arxiv.org/abs/2511.12249)
*Khang T. Huynh,Dung H. Nguyen,Binh T. Nguyen*

Main category: cs.CL

TL;DR: 提出了ViConBERT框架，结合对比学习和基于词义解释的蒸馏来学习越南语上下文嵌入，并创建了首个大规模越南语语义理解评估数据集ViConWSD。


<details>
  <summary>Details</summary>
Motivation: 越南语缺乏强大的语义理解模型和评估资源，而现有的上下文词嵌入进展主要局限于英语等高资源语言。

Method: 使用对比学习(SimCLR)和基于词义解释的蒸馏来训练越南语上下文嵌入模型ViConBERT，并构建了包含词义消歧和上下文相似度任务的大规模合成数据集ViConWSD。

Result: ViConBERT在词义消歧任务上达到F1=0.87，在ViCon数据集上AP=0.88，在ViSim-400数据集上Spearman's rho=0.60，表现优于基线模型。

Conclusion: ViConBERT框架在建模离散词义和分级语义关系方面表现出色，为越南语语义理解提供了有效的解决方案。

Abstract: Recent advances in contextualized word embeddings have greatly improved semantic tasks such as Word Sense Disambiguation (WSD) and contextual similarity, but most progress has been limited to high-resource languages like English. Vietnamese, in contrast, still lacks robust models and evaluation resources for fine-grained semantic understanding. In this paper, we present ViConBERT, a novel framework for learning Vietnamese contextualized embeddings that integrates contrastive learning (SimCLR) and gloss-based distillation to better capture word meaning. We also introduce ViConWSD, the first large-scale synthetic dataset for evaluating semantic understanding in Vietnamese, covering both WSD and contextual similarity. Experimental results show that ViConBERT outperforms strong baselines on WSD (F1 = 0.87) and achieves competitive performance on ViCon (AP = 0.88) and ViSim-400 (Spearman's rho = 0.60), demonstrating its effectiveness in modeling both discrete senses and graded semantic relations. Our code, models, and data are available at https://github.com/tkhangg0910/ViConBERT

</details>


### [27] [Cmprsr: Abstractive Token-Level Question-Agnostic Prompt Compressor](https://arxiv.org/abs/2511.12281)
*Ivan Zakazov,Alexander Sharipov,Berke Argin,Oussama Gabouj,Kamel Charaf,Alexi Semiz,Lorenzo Drudi,Nicolas Baldwin,Robert West*

Main category: cs.CL

TL;DR: 提出了一种使用小型LLM压缩大型LLM输入的新范式，开发了Cmprsr模型，在保持语义信息和控制压缩率方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 降低使用黑盒大型语言模型的高成本，通过压缩输入来减少计算开销。

Method: 使用Textgrad-based压缩元提示优化，结合监督微调(SFT)和组相对策略优化(GRPO)训练Qwen3-4B模型。

Result: Cmprsr在MeetingBank、LongBench和GSM8k数据集上优于提取式和普通抽象压缩方法，能精确控制压缩率。

Conclusion: Cmprsr模型在成本-质量权衡方面提供了精细控制，具有良好的泛化能力。

Abstract: Motivated by the high costs of using black-box Large Language Models (LLMs), we introduce a novel prompt compression paradigm, under which we use smaller LLMs to compress inputs for the larger ones. We present the first comprehensive LLM-as-a-compressor benchmark spanning 25 open- and closed-source models, which reveals significant disparity in models' compression ability in terms of (i) preserving semantically important information (ii) following the user-provided compression rate (CR). We further improve the performance of gpt-4.1-mini, the best overall vanilla compressor, with Textgrad-based compression meta-prompt optimization. We also identify the most promising open-source vanilla LLM - Qwen3-4B - and post-train it with a combination of supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO), pursuing the dual objective of CR adherence and maximizing the downstream task performance. We call the resulting model Cmprsr and demonstrate its superiority over both extractive and vanilla abstractive compression across the entire range of compression rates on lengthy inputs from MeetingBank and LongBench as well as short prompts from GSM8k. The latter highlights Cmprsr's generalizability across varying input lengths and domains. Moreover, Cmprsr closely follows the requested compression rate, offering fine control over the cost-quality trade-off.

</details>


### [28] [AugAbEx : Way Forward for Extractive Case Summarization](https://arxiv.org/abs/2511.12290)
*Purnima Bindal,Vikas Kumar,Sagar Rathore,Vasudha Bhatnagar*

Main category: cs.CL

TL;DR: 本文提出了一种利用现有抽象摘要生成对应抽取式摘要的轻量级透明管道，旨在为法律案例摘要研究社区创建包含两种摘要类型的增强数据集资源。


<details>
  <summary>Details</summary>
Motivation: 法律判决摘要对法律从业者构成沉重的认知负担，而深度神经网络生成的抽象摘要容易误传法律术语或忽略关键细节，因此需要转向抽取式摘要方法。但由于人工标注抽取式摘要成本高昂，需要自动化解决方案。

Method: 设计了一个轻量透明的管道，利用现有的抽象标准摘要来创建对应的抽取式标准摘要版本，确保专家意见从原始抽象摘要传递到转换后的抽取式摘要中。

Result: 计划增强七个现有的案例摘要数据集，通过添加对应的抽取式摘要来创建丰富的数据资源，并进行结构、词汇和语义维度的广泛比较评估以确保质量。

Conclusion: 承诺公开发布增强的数据集，相信这一资源将推动法律文档自动摘要领域的发展。

Abstract: Summarization of legal judgments poses a heavy cognitive burden on law practitioners due to the complexity of the language, context-sensitive legal jargon, and the length of the document. Therefore, the automatic summarization of legal documents has attracted serious attention from natural language processing researchers. Since the abstractive summaries of legal documents generated by deep neural methods remain prone to the risk of misrepresenting nuanced legal jargon or overlooking key contextual details, we envisage a rising trend toward the use of extractive case summarizers.
  Given the high cost of human annotation for gold standard extractive summaries, we engineer a light and transparent pipeline that leverages existing abstractive gold standard summaries to create the corresponding extractive gold standard versions. The approach ensures that the experts` opinions ensconced in the original gold standard abstractive summaries are carried over to the transformed extractive summaries. We aim to augment seven existing case summarization datasets, which include abstractive summaries, by incorporating corresponding extractive summaries and create an enriched data resource for case summarization research community. To ensure the quality of the augmented extractive summaries, we perform an extensive comparative evaluation with the original abstractive gold standard summaries covering structural, lexical, and semantic dimensions. We also compare the domain-level information of the two summaries. We commit to release the augmented datasets in the public domain for use by the research community and believe that the resource will offer opportunities to advance the field of automatic summarization of legal documents.

</details>


### [29] [Do LLMs and Humans Find the Same Questions Difficult? A Case Study on Japanese Quiz Answering](https://arxiv.org/abs/2511.12300)
*Naoya Sugiura,Kosuke Yamada,Yasuhiro Ogawa,Katsuhiko Toyama,Ryohei Sasano*

Main category: cs.CL

TL;DR: 研究比较了LLMs和人类在抢答式问答中的表现差异，发现LLMs在维基百科未覆盖的问题和需要数字答案的问题上表现较差


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在许多NLP任务上超越了人类表现，但尚不清楚对人类困难的问题是否对LLMs同样困难

Method: 收集日本问答数据（包含问题、答案和人类正确率），在不同设置下让LLMs回答问题，从两个分析角度比较LLMs与人类的正确率

Result: 实验结果显示，相比人类，LLMs在维基百科未覆盖答案的问题上表现更差，且在需要数字答案的问题上也有困难

Conclusion: LLMs与人类在问题难度感知上存在差异，特别是在知识覆盖和数值推理方面

Abstract: LLMs have achieved performance that surpasses humans in many NLP tasks. However, it remains unclear whether problems that are difficult for humans are also difficult for LLMs. This study investigates how the difficulty of quizzes in a buzzer setting differs between LLMs and humans. Specifically, we first collect Japanese quiz data including questions, answers, and correct response rate of humans, then prompted LLMs to answer the quizzes under several settings, and compare their correct answer rate to that of humans from two analytical perspectives. The experimental results showed that, compared to humans, LLMs struggle more with quizzes whose correct answers are not covered by Wikipedia entries, and also have difficulty with questions that require numerical answers.

</details>


### [30] [Don't Think of the White Bear: Ironic Negation in Transformer Models Under Cognitive Load](https://arxiv.org/abs/2511.12381)
*Logan Mann,Nayan Saxena,Sarah Tandon,Chenhao Sun,Savar Toteja,Kevin Zhu*

Main category: cs.CL

TL;DR: 论文研究了大型语言模型中的讽刺反弹现象——否定指令反而会增加被禁止概念的可及性。通过两个实验发现：反弹在否定后立即出现，并随着语义干扰增强；极性分离与反弹持续性相关；电路追踪显示中间层注意力头放大被禁token。


<details>
  <summary>Details</summary>
Motivation: 人类思维中的讽刺反弹现象（否定指令反而增强概念可及性）在LLMs中同样存在，因为抑制概念需要在内部激活它，这可能导致反弹而非避免。研究旨在理解LLMs中这种认知现象的机制。

Method: 两个实验：(1) 负载与内容：在否定指令后使用不同类型干扰文本（语义、句法、重复）测量反弹强度；(2) 极性分离：测试模型是否能区分概念的中性和负面框架，以及这种分离是否预测反弹持续性。辅以电路追踪分析识别相关注意力头。

Result: 反弹在否定后立即出现，且随着更长或语义干扰而增强，而重复有助于抑制；更强的极性分离与更持久的反弹相关；电路分析发现中间层注意力头放大被禁token，而早期层进行抑制。

Conclusion: 研究将讽刺反弹的认知预测与长上下文干扰的机制洞察联系起来，并发布了ReboundBench数据集（5000个系统变化的否定提示）以支持未来研究。

Abstract: Negation instructions such as 'do not mention $X$' can paradoxically increase the accessibility of $X$ in human thought, a phenomenon known as ironic rebound. Large language models (LLMs) face the same challenge: suppressing a concept requires internally activating it, which may prime rebound instead of avoidance. We investigated this tension with two experiments. \textbf{(1) Load \& content}: after a negation instruction, we vary distractor text (semantic, syntactic, repetition) and measure rebound strength. \textbf{(2) Polarity separation}: We test whether models distinguish neutral from negative framings of the same concept and whether this separation predicts rebound persistence. Results show that rebound consistently arises immediately after negation and intensifies with longer or semantic distractors, while repetition supports suppression. Stronger polarity separation correlates with more persistent rebound. Together, these findings, complemented by a circuit tracing analysis that identifies sparse middle-layer attention heads amplifying forbidden tokens while early layers suppress, link cognitive predictions of ironic rebound with mechanistic insights into long-context interference. To support future work, we release ReboundBench, a dataset of $5,000$ systematically varied negation prompts designed to probe rebound in LLMs.

</details>


### [31] [From Phonemes to Meaning: Evaluating Large Language Models on Tamil](https://arxiv.org/abs/2511.12387)
*Jeyarajalingam Varsha,Menan Velayuthan,Sumirtha Karunakaran,Rasan Nivethiga,Kengatharaiyer Sarveswaran*

Main category: cs.CL

TL;DR: ILAKKANAM是首个泰米尔语语言学评估基准，包含820个来自斯里兰卡学校考试的问题，评估发现LLMs在低年级表现良好但随语言复杂度增加而下降，Gemini 2.5表现最佳。


<details>
  <summary>Details</summary>
Motivation: 现有基准依赖英语翻译数据集，无法捕捉泰米尔语等低资源、形态丰富语言的语言文化细微差别。

Method: 使用820个斯里兰卡学校泰米尔语考试问题构建基准，由训练有素的语言学家在5个语言学类别和1个事实知识类别下标注，涵盖1-13年级。

Result: Gemini 2.5总体表现最佳，开源模型落后；所有模型在低年级问题表现良好，但随语言复杂度增加明显下降；模型表现与识别语言学类别能力无强相关性。

Conclusion: LLMs在泰米尔语中的表现可能由曝光驱动而非真正理解，需要更多语言特定的评估基准来改善低资源语言的建模。

Abstract: Large Language Models (LLMs) have shown strong generalization across tasks in high-resource languages; however, their linguistic competence in low-resource and morphologically rich languages such as Tamil remains largely unexplored. Existing multilingual benchmarks often rely on translated English datasets, failing to capture the linguistic and cultural nuances of the target language. To address this gap, we introduce ILAKKANAM, the first Tamil-specific linguistic evaluation benchmark manually curated using 820 questions from Sri Lankan school-level Tamil subject examination papers. Each question is annotated by trained linguists under five linguistic categories and a factual knowledge category, spanning Grades 1--13 to ensure broad linguistic coverage. We evaluate both closed-source and open-source LLMs using a standardized evaluation framework. Our results show that Gemini 2.5 achieves the highest overall performance, while open-source models lag behind, highlighting the gap in linguistic grounding. Category- and grade-wise analyses reveal that all models perform well on lower-grade questions but show a clear decline as linguistic complexity increases. Further, no strong correlation is observed between a model's overall performance and its ability to identify linguistic categories, suggesting that performance may be driven by exposure rather than genuine understanding.

</details>


### [32] [Probing Preference Representations: A Multi-Dimensional Evaluation and Analysis Method for Reward Models](https://arxiv.org/abs/2511.12464)
*Chenglong Wang,Yifu Huo,Yang Gan,Yongyu Mu,Qiaozhi He,Murun Yang,Bei Li,Chunliang Zhang,Tongran Liu,Anxiang Ma,Zhengtao Yu,Jingbo Zhu,Tong Xiao*

Main category: cs.CL

TL;DR: 提出MRMBench基准和推理时探测方法，用于评估奖励模型在多维度偏好上的表现，发现该方法与LLM对齐性能强相关，并揭示了奖励模型在多维度偏好捕获上的困难。


<details>
  <summary>Details</summary>
Motivation: 传统奖励模型评估方法使用固定成对排序测试集，但无法提供每个偏好维度的性能信息，需要更细粒度的评估方法来分析奖励模型在不同偏好维度上的表现。

Method: 构建MRMBench基准（包含6个不同偏好维度的探测任务），提出推理时探测方法分析奖励预测时使用的维度，增强可解释性。

Result: MRMBench与大型语言模型对齐性能强相关，奖励模型在多维度偏好捕获上表现不佳，推理时探测方法能可靠评估奖励预测置信度并改善LLM对齐。

Conclusion: MRMBench是开发先进奖励模型的可靠参考，多目标优化在奖励建模中具有潜力，推理时探测方法能提升奖励模型的可解释性和对齐效果。

Abstract: Previous methods evaluate reward models by testing them on a fixed pairwise ranking test set, but they typically do not provide performance information on each preference dimension. In this work, we address the evaluation challenge of reward models by probing preference representations. To confirm the effectiveness of this evaluation method, we construct a Multi-dimensional Reward Model Benchmark (MRMBench), a collection of six probing tasks for different preference dimensions. We design it to favor and encourage reward models that better capture preferences across different dimensions. Furthermore, we introduce an analysis method, inference-time probing, which identifies the dimensions used during the reward prediction and enhances its interpretability. Through extensive experiments, we find that MRMBench strongly correlates with the alignment performance of large language models (LLMs), making it a reliable reference for developing advanced reward models. Our analysis of MRMBench evaluation results reveals that reward models often struggle to capture preferences across multiple dimensions, highlighting the potential of multi-objective optimization in reward modeling. Additionally, our findings show that the proposed inference-time probing method offers a reliable metric for assessing the confidence of reward predictions, which ultimately improves the alignment of LLMs.

</details>


### [33] [Assessing LLMs for Serendipity Discovery in Knowledge Graphs: A Case for Drug Repurposing](https://arxiv.org/abs/2511.12472)
*Mengying Wang,Chenhui Ma,Ao Jiao,Tuo Liang,Pengjun Lu,Shrinidhi Hegde,Yu Yin,Evren Gurkan-Cavusoglu,Yinghui Wu*

Main category: cs.CL

TL;DR: 提出SerenQA框架来评估LLM在科学知识图谱问答中发现意外见解的能力，重点关注药物重定位任务，包含基于相关性、新颖性和惊喜度的严谨评估指标。


<details>
  <summary>Details</summary>
Motivation: 现有KGQA系统通常返回高度相关但可预测的答案，缺乏发现意外和新颖（"serendipitious"）答案的能力。

Method: 提出SerenQA框架，包括基于相关性、新颖性和惊喜度的严谨评估指标，以及从临床知识图谱中提取的专家标注基准，包含知识检索、子图推理和意外发现三个子任务的结构化评估流程。

Result: 实验表明，最先进的LLM在检索任务上表现良好，但在识别真正令人惊喜和有价值的发现方面仍有困难。

Conclusion: LLM在发现意外见解方面仍有显著改进空间，SerenQA为未来研究提供了评估资源和基准。

Abstract: Large Language Models (LLMs) have greatly advanced knowledge graph question answering (KGQA), yet existing systems are typically optimized for returning highly relevant but predictable answers. A missing yet desired capacity is to exploit LLMs to suggest surprise and novel ("serendipitious") answers. In this paper, we formally define the serendipity-aware KGQA task and propose the SerenQA framework to evaluate LLMs' ability to uncover unexpected insights in scientific KGQA tasks. SerenQA includes a rigorous serendipity metric based on relevance, novelty, and surprise, along with an expert-annotated benchmark derived from the Clinical Knowledge Graph, focused on drug repurposing. Additionally, it features a structured evaluation pipeline encompassing three subtasks: knowledge retrieval, subgraph reasoning, and serendipity exploration. Our experiments reveal that while state-of-the-art LLMs perform well on retrieval, they still struggle to identify genuinely surprising and valuable discoveries, underscoring a significant room for future improvements. Our curated resources and extended version are released at: https://cwru-db-group.github.io/serenQA.

</details>


### [34] [SGuard-v1: Safety Guardrail for Large Language Models](https://arxiv.org/abs/2511.12497)
*JoonHo Lee,HyeonMin Cho,Jaewoong Yun,Hyunjae Lee,JunKyu Lee,Juree Seok*

Main category: cs.CL

TL;DR: SGuard-v1是一个轻量级的大语言模型安全护栏系统，包含两个专门模型：ContentFilter检测有害内容，JailbreakFilter筛查对抗性提示，基于2B参数的Granite模型，支持12种语言，在安全基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决大语言模型在人类-AI对话场景中的安全风险，包括有害内容生成和对抗性提示攻击，需要开发轻量级但高效的安全防护系统。

Method: 使用1.4百万训练实例对2B参数的Granite-3.3-2B-Instruct模型进行指令微调，构建两个组件：ContentFilter基于MLCommons危害分类学检测安全风险，JailbreakFilter通过精心设计的课程学习覆盖60种主要攻击类型。

Result: 在公共和专有安全基准测试中达到最先进的安全性能，同时保持轻量级部署，提供多类安全预测和二元置信度分数以提高可解释性。

Conclusion: SGuard-v1作为轻量级安全护栏系统，在减少部署开销的同时提供了强大的安全防护能力，并通过开源许可促进AI安全的进一步研究和实际部署。

Abstract: We present SGuard-v1, a lightweight safety guardrail for Large Language Models (LLMs), which comprises two specialized models to detect harmful content and screen adversarial prompts in human-AI conversational settings. The first component, ContentFilter, is trained to identify safety risks in LLM prompts and responses in accordance with the MLCommons hazard taxonomy, a comprehensive framework for trust and safety assessment of AI. The second component, JailbreakFilter, is trained with a carefully designed curriculum over integrated datasets and findings from prior work on adversarial prompting, covering 60 major attack types while mitigating false-unsafe classification. SGuard-v1 is built on the 2B-parameter Granite-3.3-2B-Instruct model that supports 12 languages. We curate approximately 1.4 million training instances from both collected and synthesized data and perform instruction tuning on the base model, distributing the curated data across the two component according to their designated functions. Through extensive evaluation on public and proprietary safety benchmarks, SGuard-v1 achieves state-of-the-art safety performance while remaining lightweight, thereby reducing deployment overhead. SGuard-v1 also improves interpretability for downstream use by providing multi-class safety predictions and their binary confidence scores. We release the SGuard-v1 under the Apache-2.0 License to enable further research and practical deployment in AI safety.

</details>


### [35] [QA-Noun: Representing Nominal Semantics via Natural Language Question-Answer Pairs](https://arxiv.org/abs/2511.12504)
*Maria Tseytlin,Paul Roit,Omri Abend,Ido Dagan,Ayal Klein*

Main category: cs.CL

TL;DR: QA-Noun是一个基于问答的名词中心语义关系框架，通过9个问题模板捕捉名词的显式句法和隐式上下文角色，与QA-SRL结合实现句子意义的细粒度分解。


<details>
  <summary>Details</summary>
Motivation: 现有的基于QA的语义方法主要关注谓词-论元关系，但忽略了名词中心语义的表示，需要补充名词相关的语义关系框架。

Method: 定义9个问题模板覆盖名词的显式和隐式角色，创建标注数据集，训练模型并与QA-SRL集成，实现统一的句子意义分解。

Result: QA-Noun几乎完全覆盖AMR的名词论元，同时捕捉额外上下文隐含关系，与QA-SRL结合比FactScore和DecompScore等方法的粒度提高130%以上。

Conclusion: QA-Noun完善了基于QA的语义框架，为跨文本对齐提供了全面且可扩展的细粒度语义分解方法。

Abstract: Decomposing sentences into fine-grained meaning units is increasingly used to model semantic alignment. While QA-based semantic approaches have shown effectiveness for representing predicate-argument relations, they have so far left noun-centered semantics largely unaddressed. We introduce QA-Noun, a QA-based framework for capturing noun-centered semantic relations. QA-Noun defines nine question templates that cover both explicit syntactical and implicit contextual roles for nouns, producing interpretable QA pairs that complement verbal QA-SRL. We release detailed guidelines, a dataset of over 2,000 annotated noun mentions, and a trained model integrated with QA-SRL to yield a unified decomposition of sentence meaning into individual, highly fine-grained, facts. Evaluation shows that QA-Noun achieves near-complete coverage of AMR's noun arguments while surfacing additional contextually implied relations, and that combining QA-Noun with QA-SRL yields over 130\% higher granularity than recent fact-based decomposition methods such as FactScore and DecompScore. QA-Noun thus complements the broader QA-based semantic framework, forming a comprehensive and scalable approach to fine-grained semantic decomposition for cross-text alignment.

</details>


### [36] [TAdaRAG: Task Adaptive Retrieval-Augmented Generation via On-the-Fly Knowledge Graph Construction](https://arxiv.org/abs/2511.12520)
*Jie Zhang,Bo Tang,Wanzi Shao,Wenqiang Wei,Jihao Zhao,Jianqing Zhu,Zhiyu li,Wen Xi,Zehao Lin,Feiyu Xiong,Yanchao Tan*

Main category: cs.CL

TL;DR: TAdaRAG是一个新颖的检索增强生成框架，通过任务自适应的知识图谱构建解决传统RAG中信息截断和无关细节的问题，在多个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法存在两个主要问题：1）由于输入上下文窗口限制，检索到的外部知识被截断成小块，导致信息丢失，引发回答幻觉和推理链断裂；2）检索非结构化知识会引入无关细节，阻碍准确推理。

Method: 提出TAdaRAG框架，包含：1）意图驱动的路由机制到领域特定的提取模板；2）监督微调；3）基于强化学习的隐式提取机制，确保知识整合的简洁性、连贯性和非冗余性。

Result: 在六个公共基准测试和一个真实业务基准（NowNewsQA）上，使用三个骨干模型进行评估，TAdaRAG在不同领域和长文本任务中均优于现有方法。

Conclusion: TAdaRAG展现了强大的泛化能力和实际有效性，能够有效解决传统RAG的信息截断和无关细节问题。

Abstract: Retrieval-Augmented Generation (RAG) improves large language models by retrieving external knowledge, often truncated into smaller chunks due to the input context window, which leads to information loss, resulting in response hallucinations and broken reasoning chains. Moreover, traditional RAG retrieves unstructured knowledge, introducing irrelevant details that hinder accurate reasoning. To address these issues, we propose TAdaRAG, a novel RAG framework for on-the-fly task-adaptive knowledge graph construction from external sources. Specifically, we design an intent-driven routing mechanism to a domain-specific extraction template, followed by supervised fine-tuning and a reinforcement learning-based implicit extraction mechanism, ensuring concise, coherent, and non-redundant knowledge integration. Evaluations on six public benchmarks and a real-world business benchmark (NowNewsQA) across three backbone models demonstrate that TAdaRAG outperforms existing methods across diverse domains and long-text tasks, highlighting its strong generalization and practical effectiveness.

</details>


### [37] [Mitigating Length Bias in RLHF through a Causal Lens](https://arxiv.org/abs/2511.12573)
*Hyeonji Kim,Sujeong Oh,Sanghack Lee*

Main category: cs.CL

TL;DR: 提出了一种因果框架来分析和缓解RLHF奖励模型中的长度偏差问题，通过反事实数据增强方法训练奖励模型，使其能够独立于冗长度评估内容质量。


<details>
  <summary>Details</summary>
Motivation: RLHF训练的奖励模型存在长度偏差，系统性地倾向于偏爱更长的回答，将冗长度与质量混淆。

Method: 使用反事实数据增强方法，构建长度不同但内容相似的回答对，以及长度相似但内容不同的回答对，用这些数据训练奖励模型。

Result: 经验评估表明，该方法减少了奖励分配中的长度偏差，并使得策略模型产生更简洁、内容聚焦的输出。

Conclusion: 所提出的方法有效减少了长度偏差，提高了RLHF流程中奖励建模的鲁棒性和内容敏感性。

Abstract: Reinforcement learning from human feedback (RLHF) is widely used to align large language models (LLMs) with human preferences. However, RLHF-trained reward models often exhibit length bias -- a systematic tendency to favor longer responses by conflating verbosity with quality. We propose a causal framework for analyzing and mitigating length bias in RLHF reward modeling. Central to our approach is a counterfactual data augmentation method that generates response pairs designed to isolate content quality from verbosity. These counterfactual examples are then used to train the reward model, enabling it to assess responses based on content quality independently of verbosity. Specifically, we construct (1) length-divergent pairs with similar content and (2) content-divergent pairs of similar length. Empirical evaluations show that our method reduces length bias in reward assignment and leads to more concise, content-focused outputs from the policy model. These findings demonstrate that the proposed approach effectively reduces length bias and improves the robustness and content sensitivity of reward modeling in RLHF pipelines.

</details>


### [38] [MMWOZ: Building Multimodal Agent for Task-oriented Dialogue](https://arxiv.org/abs/2511.12586)
*Pu-Hai Yang,Heyan Huang,Heng-Da Xu,Fanshu Sun,Xian-Ling Mao,Chaoxu Mu*

Main category: cs.CL

TL;DR: 提出了MMWOZ多模态对话数据集，通过GUI前端解决传统任务导向对话系统在实际应用中的局限性，并开发了MATE基线模型进行实验分析


<details>
  <summary>Details</summary>
Motivation: 传统任务导向对话系统依赖定制后端API，但现实场景中普遍存在GUI前端而缺乏API支持，造成应用差距

Method: 1) 开发web风格GUI前端 2) 自动化脚本将对话状态和系统动作转换为GUI操作指令 3) 收集网页快照和对应操作指令 4) 提出MATE多模态模型作为基线

Result: 构建了MMWOZ数据集，并通过MATE模型验证了多模态任务导向对话系统的可行性

Conclusion: MMWOZ数据集和MATE模型为构建实用的多模态任务导向对话系统提供了重要基础，弥合了传统系统与实际应用之间的差距

Abstract: Task-oriented dialogue systems have garnered significant attention due to their conversational ability to accomplish goals, such as booking airline tickets for users. Traditionally, task-oriented dialogue systems are conceptualized as intelligent agents that interact with users using natural language and have access to customized back-end APIs. However, in real-world scenarios, the widespread presence of front-end Graphical User Interfaces (GUIs) and the absence of customized back-end APIs create a significant gap for traditional task-oriented dialogue systems in practical applications. In this paper, to bridge the gap, we collect MMWOZ, a new multimodal dialogue dataset that is extended from MultiWOZ 2.3 dataset. Specifically, we begin by developing a web-style GUI to serve as the front-end. Next, we devise an automated script to convert the dialogue states and system actions from the original dataset into operation instructions for the GUI. Lastly, we collect snapshots of the web pages along with their corresponding operation instructions. In addition, we propose a novel multimodal model called MATE (Multimodal Agent for Task-oriEnted dialogue) as the baseline model for the MMWOZ dataset. Furthermore, we conduct comprehensive experimental analysis using MATE to investigate the construction of a practical multimodal agent for task-oriented dialogue.

</details>


### [39] [Group-Aware Reinforcement Learning for Output Diversity in Large Language Models](https://arxiv.org/abs/2511.12596)
*Oron Anschel,Alon Shoshan,Adam Botach,Shunit Haviv Hakimi,Asaf Gendler,Emanuel Ben Baruch,Nadav Bhonker,Igor Kviatkovsky,Manoj Aggarwal,Gerard Medioni*

Main category: cs.CL

TL;DR: GAPO是一种基于GRPO的扩展方法，通过计算群体层面的奖励来提升LLM响应的多样性，解决模式崩溃问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型经常出现模式崩溃，即使存在多个有效答案也重复生成相同的几个补全，限制了任务响应的多样性。

Method: GAPO是GRPO的简单扩展，计算群体层面的奖励，使用频率感知奖励函数鼓励在有效补全上均匀采样。

Result: GAPO训练的模型能产生有效且更多样化的响应，在开放提示下提升响应多样性，同时在标准基准测试中不损害准确性。

Conclusion: GAPO能够有效提升LLM响应的多样性，解决模式崩溃问题，且不影响模型在标准任务上的表现。

Abstract: Large Language Models (LLMs) often suffer from mode collapse, repeatedly generating the same few completions even when many valid answers exist, limiting their diversity across a wide range of tasks. We introduce Group-Aware Policy Optimization (GAPO), a simple extension of the recent and popular Group Relative Policy Optimization (GRPO) that computes rewards over the group as a whole. GAPO enables learning from the group-level properties such as diversity and coverage. We demonstrate GAPO using a frequency-aware reward function that encourages uniform sampling over valid LLM completions, and show that GAPO-trained models produce valid and more diverse model responses. Beyond this setup, GAPO generalizes to open-ended prompts and improves response diversity without compromising accuracy on standard LLM benchmarks (GSM8K, MATH, HumanEval, MMLU-Pro). Our code will be made publicly available.

</details>


### [40] [Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data](https://arxiv.org/abs/2511.12609)
*Yunxin Li,Xinyu Chen,Shenyuan Jiang,Haoyuan Shi,Zhenyu Liu,Xuanyu Zhang,Nanhao Deng,Zhenran Xu,Yicheng Ma,Meishan Zhang,Baotian Hu,Min Zhang*

Main category: cs.CL

TL;DR: Uni-MoE 2.0是一个全开源的跨模态大模型，基于Qwen2.5-7B架构构建，通过动态容量MoE设计、渐进式训练策略和跨模态数据匹配技术，在语言中心的多模态理解、推理和生成方面取得显著进展。


<details>
  <summary>Details</summary>
Motivation: 推进Lychee Uni-MoE系列在跨模态能力上的发展，构建一个能够理解多种模态并生成图像、文本和语音的全能模型，解决现有跨模态大模型在计算效率和能力平衡方面的挑战。

Method: 采用动态容量MoE框架，使用共享、路由和空专家平衡计算效率；Omni-Modality 3D RoPE确保跨模态时空对齐；渐进式监督微调策略激活模态特定专家；GSPO-DPO方法稳定强化学习训练；使用约75B token的开源多模态数据训练。

Result: 在85个基准测试中达到SOTA或高度竞争力，在76个基准中超过50个优于Qwen2.5-Omni（使用1.2T token训练）；视频理解提升7%，跨模态理解提升7%，视听推理提升4%；长语音处理WER降低4.2%；在低级图像处理和可控生成方面领先。

Conclusion: Uni-MoE 2.0通过创新的MoE架构和训练策略，在计算效率和模型能力之间取得了良好平衡，在多个跨模态任务上表现出色，证明了其在跨模态大模型领域的领先地位。

Abstract: We present Uni-MoE 2.0 from the Lychee family. As a fully open-source omnimodal large model (OLM), it substantially advances Lychee's Uni-MoE series in language-centric multimodal understanding, reasoning, and generating. Based on the Qwen2.5-7B dense architecture, we build Uni-MoE-2.0-Omni from scratch through three core contributions: dynamic-capacity Mixture-of-Experts (MoE) design, a progressive training strategy enhanced with an iterative reinforcement strategy, and a carefully curated multimodal data matching technique. It is capable of omnimodal understanding, as well as generating images, text, and speech. Architecturally, our new MoE framework balances computational efficiency and capability for 10 cross-modal inputs using shared, routed, and null experts, while our Omni-Modality 3D RoPE ensures spatio-temporal cross-modality alignment in the self-attention layer. For training, following cross-modal pretraining, we use a progressive supervised fine-tuning strategy that activates modality-specific experts and is enhanced by balanced data composition and an iterative GSPO-DPO method to stabilise RL training and improve reasoning. Data-wise, the base model, trained on approximately 75B tokens of open-source multimodal data, is equipped with special speech and image generation tokens, allowing it to learn these generative tasks by conditioning its outputs on linguistic cues. Extensive evaluation across 85 benchmarks demonstrates that our model achieves SOTA or highly competitive performance against leading OLMs, surpassing Qwen2.5-Omni (trained with 1.2T tokens) on over 50 of 76 benchmarks. Key strengths include video understanding (+7% avg. of 8), omnimodallity understanding (+7% avg. of 4), and audiovisual reasoning (+4%). It also advances long-form speech processing (reducing WER by 4.2%) and leads in low-level image processing and controllable generation across 5 metrics.

</details>


### [41] [Knots: A Large-Scale Multi-Agent Enhanced Expert-Annotated Dataset and LLM Prompt Optimization for NOTAM Semantic Parsing](https://arxiv.org/abs/2511.12630)
*Maoqi Liu,Quan Fang,Yang Yang,Can Zhao,Kaiquan Cai*

Main category: cs.CL

TL;DR: 提出了NOTAM语义解析任务，构建了Knots数据集，通过多智能体协作框架增强标注质量，系统评估了多种提示工程和模型适应技术，显著提升了航空文本理解能力。


<details>
  <summary>Details</summary>
Motivation: NOTAMs作为关键飞行安全信息渠道，其复杂语言结构和隐含推理给自动解析带来挑战。现有研究主要关注分类和命名实体识别等表层任务，缺乏深度语义理解。

Method: 提出NOTAM语义解析任务，强调语义推理和航空领域知识集成；构建包含12,347条专家标注NOTAMs的Knots数据集，覆盖194个飞行情报区；采用多智能体协作框架进行全面的字段发现；系统评估提示工程策略和模型适应技术。

Result: 实验结果表明所提方法在航空文本理解和处理方面取得了显著改进，为自动NOTAM分析系统提供了有价值的见解。

Conclusion: 该方法有效解决了NOTAM语义解析的挑战，通过领域知识集成和先进技术应用，显著提升了航空安全信息的自动化处理能力。

Abstract: Notice to Air Missions (NOTAMs) serve as a critical channel for disseminating key flight safety information, yet their complex linguistic structures and implicit reasoning pose significant challenges for automated parsing. Existing research mainly focuses on surface-level tasks such as classification and named entity recognition, lacking deep semantic understanding. To address this gap, we propose NOTAM semantic parsing, a task emphasizing semantic inference and the integration of aviation domain knowledge to produce structured, inference-rich outputs. To support this task, we construct Knots (Knowledge and NOTAM Semantics), a high-quality dataset of 12,347 expert-annotated NOTAMs covering 194 Flight Information Regions, enhanced through a multi-agent collaborative framework for comprehensive field discovery. We systematically evaluate a wide range of prompt-engineering strategies and model-adaptation techniques, achieving substantial improvements in aviation text understanding and processing. Our experimental results demonstrate the effectiveness of the proposed approach and offer valuable insights for automated NOTAM analysis systems. Our code is available at: https://github.com/Estrellajer/Knots.

</details>


### [42] [Reason-KE++: Aligning the Process, Not Just the Outcome, for Faithful LLM Knowledge Editing](https://arxiv.org/abs/2511.12661)
*Yuchen Wu,Liang Ding,Li Shen,Dacheng Tao*

Main category: cs.CL

TL;DR: Reason-KE++是一个SFT+RL框架，通过过程级忠实度对齐解决LLM在复杂多跳推理任务中的事实幻觉问题，在MQUAKE-CF-3k上达到95.48%的新SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有SFT方法（如Reason-KE）存在"忠实度差距"，过度优化格式模仿而非扎实推理，导致LLM的参数先验覆盖上下文事实，产生关键事实幻觉。

Method: 提出Reason-KE++框架，核心是阶段感知奖励机制，为中间推理步骤（如分解、子答案正确性）提供密集监督，避免仅依赖结果奖励的陷阱。

Result: 在MQUAKE-CF-3k数据集上达到95.48%的准确率（提升+5.28%），显著优于仅关注结果的RL方法（跳跃准确率仅19.00%）。

Conclusion: 对于复杂任务，对齐推理过程对于构建可信赖的LLM至关重要，过程级监督是解决LLM对齐核心问题的关键。

Abstract: Aligning Large Language Models (LLMs) to be faithful to new knowledge in complex, multi-hop reasoning tasks is a critical, yet unsolved, challenge. We find that SFT-based methods, e.g., Reason-KE, while state-of-the-art, suffer from a "faithfulness gap": they optimize for format mimicry rather than sound reasoning. This gap enables the LLM's powerful parametric priors to override new contextual facts, resulting in critical factual hallucinations (e.g., incorrectly reasoning "Houston" from "NASA" despite an explicit edit). To solve this core LLM alignment problem, we propose Reason-KE++, an SFT+RL framework that instills process-level faithfulness. Its core is a Stage-aware Reward mechanism that provides dense supervision for intermediate reasoning steps (e.g., Decomposition, Sub-answer Correctness). Crucially, we identify that naive outcome-only RL is a deceptive trap for LLM alignment: it collapses reasoning integrity (e.g., 19.00% Hop acc) while superficially boosting final accuracy. Our process-aware framework sets a new SOTA of 95.48% on MQUAKE-CF-3k (+5.28%), demonstrating that for complex tasks, aligning the reasoning process is essential for building trustworthy LLMs.

</details>


### [43] [Improving Direct Persian-English Speech-to-Speech Translation with Discrete Units and Synthetic Parallel Data](https://arxiv.org/abs/2511.12690)
*Sina Rashidi,Hossein Sameti*

Main category: cs.CL

TL;DR: 本文提出了一种用于波斯语到英语直接语音翻译的系统，通过自监督预训练、离散语音单元和合成平行数据来解决低资源语言对的语音翻译问题。


<details>
  <summary>Details</summary>
Motivation: 直接语音翻译系统需要大量的平行语音数据，但对于波斯语等低资源语言来说，这种数据非常稀缺，因此需要开发有效的方法来缓解数据不足的问题。

Method: 系统包含三个组件：基于conformer的编码器、因果transformer解码器和基于单元的神经声码器。通过使用大语言模型翻译波斯语转录文本，并用零样本文本转语音系统合成英语语音，构建了合成平行语料库。

Result: 在CVSS语料库的波斯语-英语部分，使用合成数据的模型比直接基线提高了4.6 ASR BLEU。

Conclusion: 结合自监督预训练、离散语音单元和合成平行数据的方法对于改善波斯语-英语等低资源语言对的直接语音翻译是有效的。

Abstract: Direct speech-to-speech translation (S2ST), in which all components are trained jointly, is an attractive alternative to cascaded systems because it offers a simpler pipeline and lower inference latency. However, direct S2ST models require large amounts of parallel speech data in the source and target languages, which are rarely available for low-resource languages such as Persian. This paper presents a direct S2ST system for translating Persian speech into English speech, as well as a pipeline for synthetic parallel Persian-English speech generation. The model comprises three components: (1) a conformer-based encoder, initialized from self-supervised pre-training, maps source speech to high-level acoustic representations; (2) a causal transformer decoder with relative position multi-head attention translates these representations into discrete target speech units; (3) a unit-based neural vocoder generates waveforms from the predicted discrete units. To mitigate the data scarcity problem, we construct a new Persian-English parallel speech corpus by translating Persian speech transcriptions into English using a large language model and then synthesizing the corresponding English speech with a state-of-the-art zero-shot text-to-speech system. The resulting corpus increases the amount of available parallel speech by roughly a factor of six. On the Persian-English portion of the CVSS corpus, the proposed model achieves improvement of 4.6 ASR BLEU with the synthetic data over direct baselines. These results indicate that combining self-supervised pre-training, discrete speech units, and synthetic parallel data is effective for improving direct S2ST in low-resource language pairs such as Persian-English

</details>


### [44] [Evolve the Method, Not the Prompts: Evolutionary Synthesis of Jailbreak Attacks on LLMs](https://arxiv.org/abs/2511.12710)
*Yunhao Chen,Xin Wang,Juncheng Li,Yixu Wang,Jie Li,Yan Teng,Yingchun Wang,Xingjun Ma*

Main category: cs.CL

TL;DR: EvoSynth是一个自主框架，通过进化合成而非改进提示来生成新的越狱方法，实现了85.5%的攻击成功率，超越了现有方法的创造力限制。


<details>
  <summary>Details</summary>
Motivation: 现有的自动红队框架在越狱逻辑上局限于选择、组合或改进现有攻击策略，无法自主发明全新的攻击机制，限制了其创造力。

Method: 采用多智能体系统自主设计、进化和执行基于代码的新型攻击算法，具有代码级自校正循环，能够根据失败情况迭代重写自身的攻击逻辑。

Result: 在高度鲁棒的模型（如Claude-Sonnet-4.5）上实现了85.5%的攻击成功率，生成的攻击比现有方法更加多样化。

Conclusion: EvoSynth通过将范式从攻击规划转向进化合成，为越狱方法的进化合成开辟了新的研究方向。

Abstract: Automated red teaming frameworks for Large Language Models (LLMs) have become increasingly sophisticated, yet they share a fundamental limitation: their jailbreak logic is confined to selecting, combining, or refining pre-existing attack strategies. This binds their creativity and leaves them unable to autonomously invent entirely new attack mechanisms. To overcome this gap, we introduce \textbf{EvoSynth}, an autonomous framework that shifts the paradigm from attack planning to the evolutionary synthesis of jailbreak methods. Instead of refining prompts, EvoSynth employs a multi-agent system to autonomously engineer, evolve, and execute novel, code-based attack algorithms. Crucially, it features a code-level self-correction loop, allowing it to iteratively rewrite its own attack logic in response to failure. Through extensive experiments, we demonstrate that EvoSynth not only establishes a new state-of-the-art by achieving an 85.5\% Attack Success Rate (ASR) against highly robust models like Claude-Sonnet-4.5, but also generates attacks that are significantly more diverse than those from existing methods. We release our framework to facilitate future research in this new direction of evolutionary synthesis of jailbreak methods. Code is available at: https://github.com/dongdongunique/EvoSynth.

</details>


### [45] [Adaptive Focus Memory for Language Models](https://arxiv.org/abs/2511.12712)
*Christopher Cruz*

Main category: cs.CL

TL;DR: AFM是一种动态上下文管理器，通过语义相似度、时间衰减和重要性分类为历史消息分配三种保真度级别，在严格token预算下显著减少推理成本同时保持安全关键信息。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在多轮对话中面临固定上下文窗口和简单内存策略的限制，完整重放对话成本高昂，而静态摘要或仅关注最近消息的方法容易丢失安全关键的用户细节。

Method: AFM根据当前查询的语义相似度、半衰期时间衰减和重要性分类，为每个历史消息分配FULL、COMPRESSED或PLACEHOLDER保真度级别，在严格token预算下按时间顺序打包消息。

Result: 在涉及严重花生过敏用户规划泰国旅行的安全导向基准测试中，AFM在短中长度对话中保持过敏信息，匹配完整重放的安全性能，同时平均token使用量减少66%。

Conclusion: AFM提供模块化Python实现，支持OpenAI兼容API和离线操作，使实践者能够在评估场景中减少推理成本而不牺牲安全性或事实连续性。

Abstract: Large language models (LLMs) are increasingly deployed in multi-turn dialogue settings, but their behavior is still bottlenecked by fixed context windows and naive memory strategies. Replaying the full conversation at every turn is simple but expensive, while static summarization or recency-only heuristics often erase safety-critical user details. We present Adaptive Focus Memory (AFM), a dynamic context manager that assigns each past message one of three fidelity levels -- FULL, COMPRESSED, or PLACEHOLDER -- based on semantic similarity to the current query, half-life recency weighting, and importance classification. AFM packs messages chronologically under a strict token budget, preferring high fidelity for the most relevant turns while aiming to preserve a cheap trace of the dialogue. In a safety-oriented benchmark involving a user with a severe peanut allergy planning a trip to Thailand, AFM retains the allergy across both short and medium-length conversations, matches the safety performance of naive replay, and cuts average token usage by 66% relative to a replay baseline. We release a modular Python implementation of AFM designed for OpenAI-compatible APIs and offline operation, enabling practitioners to reduce inference cost without sacrificing safety or factual continuity in the evaluated scenario.

</details>


### [46] [On the Brittleness of LLMs: A Journey around Set Membership](https://arxiv.org/abs/2511.12728)
*Lea Hergert,Gábor Berend,Mario Szegedy,Gyorgy Turan,Márk Jelasity*

Main category: cs.CL

TL;DR: LLMs在复杂推理任务上表现超人类，但在简单集合成员查询任务中却频繁失败，显示出其可靠性和可解释性存在问题。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在简单推理任务中的失败模式，通过集合成员查询这种基础推理形式来揭示模型理解的局限性。

Method: 使用集合成员查询任务，系统评估不同提示措辞、语义结构、元素排序和模型选择对性能的影响。

Result: LLMs在这个基础任务上的表现持续脆弱且不可预测，表明模型对集合概念的理解是碎片化和复杂的。

Conclusion: 简单问题的大规模实验能够全面映射和分析失败模式，这种方法对LLM评估具有普遍价值。

Abstract: Large language models (LLMs) achieve superhuman performance on complex reasoning tasks, yet often fail on much simpler problems, raising concerns about their reliability and interpretability. We investigate this paradox through a focused study with two key design features: simplicity, to expose basic failure modes, and scale, to enable comprehensive controlled experiments. We focus on set membership queries -- among the most fundamental forms of reasoning -- using tasks like ``Is apple an element of the set \{pear, plum, apple, raspberry\}?''. We conduct a systematic empirical evaluation across prompt phrasing, semantic structure, element ordering, and model choice. Our large-scale analysis reveals that LLM performance on this elementary task is consistently brittle, and unpredictable across all dimensions, suggesting that the models' ``understanding'' of the set concept is fragmented and convoluted at best. Our work demonstrates that the large-scale experiments enabled by the simplicity of the problem allow us to map and analyze the failure modes comprehensively, making this approach a valuable methodology for LLM evaluation in general.

</details>


### [47] [Evidence of Phase Transitions in Small Transformer-Based Language Models](https://arxiv.org/abs/2511.12768)
*Noah Hong,Tao Hong*

Main category: cs.CL

TL;DR: 研究发现语言模型训练中存在相变现象，即使在小型模型中也能观察到，且可在线性训练空间中直接检测，这些相变在训练早期就出现。


<details>
  <summary>Details</summary>
Motivation: 探索相变现象是否仅限于大型语言模型，能否在线性训练空间中直接检测，以及这些相变是否在训练早期就出现。

Method: 训练小型GPT风格transformer模型，通过词汇使用演变分析（平均词长、正确/错误词数、词汇多样性变化），并应用泊松和亚泊松统计来量化词汇连接和重组。

Result: 在训练过程中发现明显的相变点，这些相变在标准损失或验证曲线中不明显，但通过词汇和统计探针变得可见。

Conclusion: 相变重组是语言模型训练的普遍特征，可在小型模型中观察到，在线性训练空间中可检测，并在训练早期出现，这为理解语言模型训练的非线性动力学提供了新视角。

Abstract: Phase transitions have been proposed as the origin of emergent abilities in large language models (LLMs), where new capabilities appear abruptly once models surpass critical thresholds of scale. Prior work, such as that of Wei et al., demonstrated these phenomena under model and data scaling, with transitions revealed after applying a log scale to training compute. In this work, we ask three complementary questions: (1) Are phase transitions unique to large models, or can they also be observed in small transformer-based language models? (2) Can such transitions be detected directly in linear training space, rather than only after log rescaling? and (3) Can these transitions emerge at early stages of training? To investigate, we train a small GPT-style transformer on a character-level corpus and analyze the evolution of vocabulary usage throughout training. We track the average word length, the number of correct versus incorrect words, and shifts in vocabulary diversity. Building on these measures, we apply Poisson and sub-Poisson statistics to quantify how words connect and reorganize. This combined analysis reveals a distinct transition point during training. Notably, these transitions are not apparent in standard loss or validation curves, but become visible through our vocabulary- and statistics-based probes. Our findings suggest that phase-transition reorganizations are a general feature of language model training, observable even in modest models, detectable directly in linear training space, and occurring surprisingly early as coherence emerges. This perspective provides new insight into the nonlinear dynamics of language model training and underscores the importance of tailored metrics for uncovering phase transition behaviors

</details>


### [48] [LLM Reinforcement in Context](https://arxiv.org/abs/2511.12782)
*Thomas Rivasseau*

Main category: cs.CL

TL;DR: 提出使用中断机制来增强大型语言模型的对齐性，通过定期在用户输入中插入控制语句来防止越狱行为。


<details>
  <summary>Details</summary>
Motivation: 当前LLM对齐研究主要关注通过训练和提示来提升模型对抗攻击的鲁棒性，但缺乏随用户输入长度扩展的对齐方法。研究发现LLM越狱概率随用户输入或对话长度增加而上升。

Method: 提出中断机制，即在用户输入中每隔x个token插入控制语句，这种方法可以推广到思维链过程以防止策略性行为。

Result: 论文提出了中断机制的概念框架，但未提供具体的实验结果数据。

Conclusion: 中断机制是增强LLM对齐性的潜在解决方案，能够随用户输入长度扩展，有助于防止模型越狱和策略性行为。

Abstract: Current Large Language Model alignment research mostly focuses on improving model robustness against adversarial attacks and misbehavior by training on examples and prompting. Research has shown that LLM jailbreak probability increases with the size of the user input or conversation length. There is a lack of appropriate research into means of strengthening alignment which also scale with user input length. We propose interruptions as a possible solution to this problem. Interruptions are control sentences added to the user input approximately every x tokens for some arbitrary x. We suggest that this can be generalized to the Chain-of-Thought process to prevent scheming.

</details>


### [49] [Evaluating Autoformalization Robustness via Semantically Similar Paraphrasing](https://arxiv.org/abs/2511.12784)
*Hayden Moore,Asfahan Shah*

Main category: cs.CL

TL;DR: 本文研究了LLMs在自动形式化任务中对语义相似但表述不同的自然语言输入的鲁棒性，发现在MiniF2F和ProofNet基准测试中，微小的语言变化会显著影响模型输出质量。


<details>
  <summary>Details</summary>
Motivation: LLMs在自动形式化方面表现出色，但可能对语义相似的自然语言输入敏感，导致形式化结果不稳定。本文旨在验证这种敏感性在自动形式化领域的存在。

Method: 使用MiniF2F和ProofNet基准，生成语义相似但表述不同的自然语言语句，在两个现代LLMs上进行交叉评估，测量语义有效性和编译有效性。

Result: 实验结果显示，在语义相似的输入变体上，LLMs的性能存在显著差异，表明微小的语言变化会严重影响模型输出。

Conclusion: LLMs在自动形式化任务中对自然语言输入的细微变化敏感，这揭示了当前模型在鲁棒性方面的局限性，需要进一步改进。

Abstract: Large Language Models (LLMs) have recently emerged as powerful tools for autoformalization. Despite their impressive performance, these models can still struggle to produce grounded and verifiable formalizations. Recent work in text-to-SQL, has revealed that LLMs can be sensitive to paraphrased natural language (NL) inputs, even when high degrees of semantic fidelity are preserved (Safarzadeh, Oroojlooyjadid, and Roth 2025). In this paper, we investigate this claim in the autoformalization domain. Specifically, we evaluate the robustness of LLMs generating formal proofs with semantically similar paraphrased NL statements by measuring semantic and compilation validity. Using the formal benchmarks MiniF2F (Zheng, Han, and Polu 2021) and Lean 4 version of ProofNet (Xin et al. 2024), and two modern LLMs, we generate paraphrased natural language statements and cross-evaluate these statements across both models. The results of this paper reveal performance variability across paraphrased inputs, demonstrating that minor shifts in NL statements can significantly impact model outputs.

</details>


### [50] [BioMedJImpact: A Comprehensive Dataset and LLM Pipeline for AI Engagement and Scientific Impact Analysis of Biomedical Journals](https://arxiv.org/abs/2511.12821)
*Ruiyu Wang,Yuzhang Xie,Xiao Hu,Carl Yang,Jiaying Lu*

Main category: cs.CL

TL;DR: BioMedJImpact是一个大规模生物医学期刊影响数据集，整合了文献计量指标、合作特征和LLM衍生的AI参与度指标，揭示了合作强度和AI参与度对期刊影响力的联合影响。


<details>
  <summary>Details</summary>
Motivation: 现有开放资源很少捕捉合作结构和AI研究如何共同塑造生物医学期刊声望，需要开发综合数据集来推进期刊层面的科学影响分析。

Method: 从PubMed Central的174万篇文章构建数据集，提出可复现的三阶段LLM流程提取AI参与度特征，分析合作强度和AI参与度对科学影响的联合作用。

Result: 发现两个一致趋势：合作强度更高的期刊（特别是作者团队更大更多样化）获得更高引用影响；AI参与度日益成为期刊声望的重要相关因素，尤其在四分位排名中。

Conclusion: BioMedJImpact既是捕捉生物医学与AI交叉的综合数据集，也是经过验证的方法框架，支持可扩展、内容感知的科学计量分析。

Abstract: Assessing journal impact is central to scholarly communication, yet existing open resources rarely capture how collaboration structures and artificial intelligence (AI) research jointly shape venue prestige in biomedicine. We present BioMedJImpact, a large-scale, biomedical-oriented dataset designed to advance journal-level analysis of scientific impact and AI engagement. Built from 1.74 million PubMed Central articles across 2,744 journals, BioMedJImpact integrates bibliometric indicators, collaboration features, and LLM-derived semantic indicators for AI engagement. Specifically, the AI engagement feature is extracted through a reproducible three-stage LLM pipeline that we propose. Using this dataset, we analyze how collaboration intensity and AI engagement jointly influence scientific impact across pre- and post-pandemic periods (2016-2019, 2020-2023). Two consistent trends emerge: journals with higher collaboration intensity, particularly those with larger and more diverse author teams, tend to achieve greater citation impact, and AI engagement has become an increasingly strong correlate of journal prestige, especially in quartile rankings. To further validate the three-stage LLM pipeline we proposed for deriving the AI engagement feature, we conduct human evaluation, confirming substantial agreement in AI relevance detection and consistent subfield classification. Together, these contributions demonstrate that BioMedJImpact serves as both a comprehensive dataset capturing the intersection of biomedicine and AI, and a validated methodological framework enabling scalable, content-aware scientometric analysis of scientific impact and innovation dynamics. Code is available at https://github.com/JonathanWry/BioMedJImpact.

</details>


### [51] [From Passive to Persuasive: Steering Emotional Nuance in Human-AI Negotiation](https://arxiv.org/abs/2511.12832)
*Niranjan Chebrolu,Gerard Christopher Yeo,Kokil Jaidka*

Main category: cs.CL

TL;DR: 通过定向激活工程引导LLaMA 3.1-8B展现更人性化的情感表达，使用归因修补识别关键干预位点，并通过对比文本对推导情感表达向量。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在对话流畅性上不断提升，但为其注入细腻、人性化的情感表达仍具挑战性。现有的对齐技术往往只处理表层输出或需要大量微调。

Method: 首先使用归因修补识别因果影响组件，通过诊断性对话任务观察激活模式找到关键干预位点。然后从对比文本对（目标情感的正面vs负面示例）的激活差异推导情感表达向量，并将这些向量应用于新的对话提示。

Result: 引导后的响应显示出增强的积极情感（如喜悦、信任）和更频繁的第一人称代词使用，表明更强的个人参与度。

Conclusion: 研究提供了一个精确且可解释的框架，为对话AI研究开辟了新方向。

Abstract: Large Language Models (LLMs) demonstrate increasing conversational fluency, yet instilling them with nuanced, human-like emotional expression remains a significant challenge. Current alignment techniques often address surface-level output or require extensive fine-tuning. This paper demonstrates that targeted activation engineering can steer LLaMA 3.1-8B to exhibit more human-like emotional nuances. We first employ attribution patching to identify causally influential components, to find a key intervention locus by observing activation patterns during diagnostic conversational tasks. We then derive emotional expression vectors from the difference in the activations generated by contrastive text pairs (positive vs. negative examples of target emotions). Applying these vectors to new conversational prompts significantly enhances emotional characteristics: steered responses show increased positive sentiment (e.g., joy, trust) and more frequent first-person pronoun usage, indicative of greater personal engagement. Our findings offer a precise and interpretable framework and new directions for the study of conversational AI.

</details>


### [52] [Quantifying consistency and accuracy of Latent Dirichlet Allocation](https://arxiv.org/abs/2511.12850)
*Saranzaya Magsarjav,Melissa Humphries,Jonathan Tuke,Lewis Mitchell*

Main category: cs.CL

TL;DR: 本文提出了一种新的稳定性度量方法，用于评估LDA主题模型的一致性，发现LDA能正确识别文档中的主题数量，但生成的主题与真实主题存在差异。


<details>
  <summary>Details</summary>
Motivation: 概率主题模型由于随机性会在多次运行时产生不一致的结果，这种不稳定性影响模型的可复现性、可靠性和解释性，引发对主题模型是否真正捕捉到有意义主题的质疑。

Method: 定义新的稳定性度量方法，结合准确性和一致性，利用LDA的生成特性生成具有真实标签的新语料库，并对这些语料库运行LDA 50次以评估输出变异性。

Result: LDA能够正确确定文档中的基础主题数量，多次重运行时返回相似主题，表现出较高的内部一致性，但这些主题并非真实主题。

Conclusion: 虽然LDA在识别主题数量方面表现良好且具有内部一致性，但其生成的主题与真实主题存在偏差，表明需要进一步改进主题模型的稳定性评估方法。

Abstract: Topic modelling in Natural Language Processing uncovers hidden topics in large, unlabelled text datasets. It is widely applied in fields such as information retrieval, content summarisation, and trend analysis across various disciplines. However, probabilistic topic models can produce different results when rerun due to their stochastic nature, leading to inconsistencies in latent topics. Factors like corpus shuffling, rare text removal, and document elimination contribute to these variations. This instability affects replicability, reliability, and interpretation, raising concerns about whether topic models capture meaningful topics or just noise. To address these problems, we defined a new stability measure that incorporates accuracy and consistency and uses the generative properties of LDA to generate a new corpus with ground truth. These generated corpora are run through LDA 50 times to determine the variability in the output. We show that LDA can correctly determine the underlying number of topics in the documents. We also find that LDA is more internally consistent, as the multiple reruns return similar topics; however, these topics are not the true topics.

</details>


### [53] [NeuroLex: A Lightweight Domain Language Model for EEG Report Understanding and Generation](https://arxiv.org/abs/2511.12851)
*Kang Yin,Hye-Bin Shin*

Main category: cs.CL

TL;DR: NeuroLex是一个专门针对脑电图报告的轻量级领域自适应语言模型，在EEG报告文本上训练，相比通用模型在EEG报告处理方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 通用语言模型无法捕捉临床脑电图报告中的领域特定语言约定，需要专门针对EEG报告语言特点的模型。

Method: 使用哈佛脑电图数据库的EEG报告文本，通过span-corruption预训练和指令式微调（报告润色、段落摘要、术语问答）来训练模型。

Result: 相比同规模通用模型，NeuroLex实现了更低的困惑度、更高的提取和摘要准确率、更好的标签效率，以及对否定和事实幻觉更强的鲁棒性。

Conclusion: NeuroLex为生物医学文本建模和脑机接口应用提供了可解释的、语言驱动的神经解码基础。

Abstract: Clinical electroencephalogram (EEG) reports encode domain-specific linguistic conventions that general-purpose language models (LMs) fail to capture. We introduce NeuroLex, a lightweight domain-adaptive language model trained purely on EEG report text from the Harvard Electroencephalography Database. Unlike existing biomedical LMs, NeuroLex is tailored to the linguistic and diagnostic characteristics of EEG reporting, enabling it to serve as both an independent textual model and a decoder backbone for multimodal EEG-language systems. Using span-corruption pretraining and instruction-style fine-tuning on report polishing, paragraph summarization, and terminology question answering, NeuroLex learns the syntax and reasoning patterns characteristic of EEG interpretation. Comprehensive evaluations show that it achieves lower perplexity, higher extraction and summarization accuracy, better label efficiency, and improved robustness to negation and factual hallucination compared with general models of the same scale. With an EEG-aware linguistic backbone, NeuroLex bridges biomedical text modeling and brain-computer interface applications, offering a foundation for interpretable and language-driven neural decoding.

</details>


### [54] [From Perception to Reasoning: Deep Thinking Empowers Multimodal Large Language Models](https://arxiv.org/abs/2511.12861)
*Wenxin Zhu,Andong Chen,Yuchen Song,Kehai Chen,Conghui Zhu,Ziyan Chen,Tiejun Zhao*

Main category: cs.CL

TL;DR: 本文对多模态思维链（MCoT）进行了系统性综述，分析了其背景动机、主流方法、评估基准和应用场景，并讨论了当前挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大语言模型在感知任务中取得显著成功，增强其复杂推理能力成为关键研究方向。现有模型存在推理路径不透明和泛化能力不足等问题，而思维链推理在语言模型中已证明能提高推理透明度和输出可解释性，有望在扩展至多模态领域后提升模型推理能力。

Method: 从三个维度介绍主流MCoT方法：思维链范式、后训练阶段和推理阶段，并分析其底层机制。

Result: 总结了现有的评估基准和指标，讨论了MCoT的应用场景。

Conclusion: 分析了MCoT当前面临的挑战，并对其未来研究方向进行了展望。

Abstract: With the remarkable success of Multimodal Large Language Models (MLLMs) in perception tasks, enhancing their complex reasoning capabilities has emerged as a critical research focus. Existing models still suffer from challenges such as opaque reasoning paths and insufficient generalization ability. Chain-of-Thought (CoT) reasoning, which has demonstrated significant efficacy in language models by enhancing reasoning transparency and output interpretability, holds promise for improving model reasoning capabilities when extended to the multimodal domain. This paper provides a systematic review centered on "Multimodal Chain-of-Thought" (MCoT). First, it analyzes the background and theoretical motivations for its inception from the perspectives of technical evolution and task demands. Then, it introduces mainstream MCoT methods from three aspects: CoT paradigms, the post-training stage, and the inference stage, while also analyzing their underlying mechanisms. Furthermore, the paper summarizes existing evaluation benchmarks and metrics, and discusses the application scenarios of MCoT. Finally, it analyzes the challenges currently facing MCoT and provides an outlook on its future research directions.

</details>


### [55] [Classification of Hope in Textual Data using Transformer-Based Models](https://arxiv.org/abs/2511.12874)
*Chukwuebuka Fortunate Ijezue,Tania-Amanda Fredrick Eneye,Maaz Amjad*

Main category: cs.CL

TL;DR: 本文比较了三种基于Transformer的架构（BERT、GPT-2、DeBERTa）在文本希望表达分类任务中的表现，发现BERT在准确性和计算效率方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 开发一个计算框架来分析文本中的希望表达，应用于心理健康和社交媒体分析领域。

Method: 使用BERT、GPT-2和DeBERTa三种Transformer架构进行二元分类（希望vs非希望）和多类别分类（五个希望相关类别）的对比实验。

Result: BERT在二元分类中达到84.49%准确率，多类别分类达到72.03%准确率，且训练时间最短（443秒）。GPT-2表现最差（79.34%二元，71.29%多类别），DeBERTa表现中等但计算成本最高（947秒多类别训练）。

Conclusion: 对于专业化的情感检测任务，架构的适用性可能比模型规模更重要，BERT在希望表达分类任务中表现出最佳平衡性。

Abstract: This paper presents a transformer-based approach for classifying hope expressions in text. We developed and compared three architectures (BERT, GPT-2, and DeBERTa) for both binary classification (Hope vs. Not Hope) and multiclass categorization (five hope-related categories). Our initial BERT implementation achieved 83.65% binary and 74.87% multiclass accuracy. In the extended comparison, BERT demonstrated superior performance (84.49% binary, 72.03% multiclass accuracy) while requiring significantly fewer computational resources (443s vs. 704s training time) than newer architectures. GPT-2 showed lowest overall accuracy (79.34% binary, 71.29% multiclass), while DeBERTa achieved moderate results (80.70% binary, 71.56% multiclass) but at substantially higher computational cost (947s for multiclass training). Error analysis revealed architecture-specific strengths in detecting nuanced hope expressions, with GPT-2 excelling at sarcasm detection (92.46% recall). This study provides a framework for computational analysis of hope, with applications in mental health and social media analysis, while demonstrating that architectural suitability may outweigh model size for specialized emotion detection tasks.

</details>


### [56] [Auditing Google's AI Overviews and Featured Snippets: A Case Study on Baby Care and Pregnancy](https://arxiv.org/abs/2511.12920)
*Desheng Hu,Joachim Baumann,Aleksandra Urman,Elsa Lichtenegger,Robin Forsberg,Aniko Hannak,Christo Wilson*

Main category: cs.CL

TL;DR: 对Google搜索中的AI生成内容（AI Overviews和Featured Snippets）进行质量审计，发现在婴儿护理和孕期相关查询中存在信息不一致、医疗安全保障缺失等问题。


<details>
  <summary>Details</summary>
Motivation: Google搜索越来越多地通过AI Overviews和Featured Snippets展示AI生成内容，用户依赖这些信息但无法控制其呈现方式，需要评估这些信息显示的质量和一致性。

Method: 通过系统算法审计1,508个真实的婴儿护理和孕期相关查询，使用稳健的评估框架评估答案一致性、相关性、医疗安全保障、来源类别和情感对齐等多个质量维度。

Result: AI Overviews和Featured Snippets在同一搜索结果页面上显示的信息有33%不一致；尽管相关性得分高，但医疗安全保障严重缺失（AIO仅11%，FS仅7%）；健康网站是主要来源，但FS也常链接商业来源。

Conclusion: 这些发现对公共卫生信息获取有重要影响，表明在AI介导的健康信息中需要更强的质量控制。该方法为高风险领域AI系统审计提供了可转移的框架。

Abstract: Google Search increasingly surfaces AI-generated content through features like AI Overviews (AIO) and Featured Snippets (FS), which users frequently rely on despite having no control over their presentation. Through a systematic algorithm audit of 1,508 real baby care and pregnancy-related queries, we evaluate the quality and consistency of these information displays. Our robust evaluation framework assesses multiple quality dimensions, including answer consistency, relevance, presence of medical safeguards, source categories, and sentiment alignment. Our results reveal concerning gaps in information consistency, with information in AIO and FS displayed on the same search result page being inconsistent with each other in 33% of cases. Despite high relevance scores, both features critically lack medical safeguards (present in just 11% of AIO and 7% of FS responses). While health and wellness websites dominate source categories for both, AIO and FS, FS also often link to commercial sources. These findings have important implications for public health information access and demonstrate the need for stronger quality controls in AI-mediated health information. Our methodology provides a transferable framework for auditing AI systems across high-stakes domains where information quality directly impacts user well-being.

</details>


### [57] [Visual Room 2.0: Seeing is Not Understanding for MLLMs](https://arxiv.org/abs/2511.12928)
*Haokun Li,Yazhou Zhang,Jizhi Ding,Qiuchi Li,Peng Zhang*

Main category: cs.CL

TL;DR: 提出了Visual Room 2.0基准，用于评估多模态大语言模型的感知-认知对齐，发现MLLMs的感知能力优于认知能力，且认知不依赖于感知推理。


<details>
  <summary>Details</summary>
Motivation: 质疑多模态大语言模型是否真正理解所见内容，基于塞尔中文屋思想扩展到多模态领域，认为'看见不等于理解'。

Method: 构建包含3个层次（低、中、高）17个代表性任务的层次化基准，涵盖从属性识别到场景理解的感知组件，以及从文本蕴含到因果和社会推理的认知组件，共350个多模态样本和2100个问题。

Result: 评估10个SOTA MLLMs发现：(1)感知能力比认知能力强8.0%；(2)认知不因果依赖于基于感知的推理；(3)认知随模型规模扩展，但感知不随模型变大而一致提升。

Conclusion: 将'看见≠理解'操作化为可测试假设，为MLLMs从感知处理到认知推理提供了新范式。

Abstract: Can multi-modal large language models (MLLMs) truly understand what they can see? Extending Searle's Chinese Room into the multi-modal domain, this paper proposes the Visual Room argument: MLLMs may describe every visual detail precisely yet fail to comprehend the underlying emotions and intentions, namely seeing is not understanding. Building on this, we introduce \textit{Visual Room} 2.0, a hierarchical benchmark for evaluating perception-cognition alignment of MLLMs. We model human perceptive and cognitive processes across three levels: low, middle, and high, covering 17 representative tasks. The perception component ranges from attribute recognition to scene understanding, while the cognition component extends from textual entailment to causal and social reasoning. The dataset contains 350 multi-modal samples, each with six progressive questions (2,100 in total) spanning perception to cognition. Evaluating 10 state-of-the-art (SoTA) MLLMs, we highlight three key findings: (1) MLLMs exhibit stronger perceptual competence than cognitive ability (8.0\%$\uparrow$); (2) cognition appears not causally dependent on perception-based reasoning; and (3) cognition scales with model size, but perception does not consistently improve with larger variants. This work operationalizes Seeing $\ne$ Understanding as a testable hypothesis, offering a new paradigm from perceptual processing to cognitive reasoning in MLLMs. Our dataset is available at https://huggingface.co/datasets/LHK2003/PCBench.

</details>


### [58] [Fine-Tuned LLMs Know They Don't Know: A Parameter-Efficient Approach to Recovering Honesty](https://arxiv.org/abs/2511.12991)
*Zeyu Shi,Ziming Wang,Tianyu Chen,Shiqi Gao,Haoyi Zhou,Qingyun Sun,Jianxin Li*

Main category: cs.CL

TL;DR: HCNR方法通过识别和恢复关键表达神经元来修复SFT后LLM的诚实性，相比基线方法速度提升2.23倍以上且数据需求减少10倍以上


<details>
  <summary>Details</summary>
Motivation: 监督微调严重损害了LLM的诚实性，但现有恢复方法假设SFT深度破坏了模型识别知识边界的能力，而实际上模型仍保留这种能力，只是表达这种意识的能力受损

Method: 提出Honesty-Critical Neurons Restoration (HCNR)方法，识别并恢复关键表达神经元到预训练状态，同时通过Hessian引导的补偿机制与任务导向神经元协调

Result: 在四个QA任务和五个LLM系列上的实验表明，HCNR有效恢复了33.25%的受损诚实性，相比基线方法速度提升至少2.23倍，数据需求减少10倍以上

Conclusion: HCNR为可信赖LLM部署提供了实用解决方案，能够在外科手术式修复诚实性的同时保持模型性能

Abstract: The honesty of Large Language Models (LLMs) is increasingly important for safe deployment in high-stakes domains. However, this crucial trait is severely undermined by supervised fine-tuning (SFT), a common technique for model specialization. Existing recovery methods rely on data-intensive global parameter adjustments, implicitly assuming that SFT deeply corrupts the models' ability to recognize their knowledge boundaries. However, we observe that fine-tuned LLMs still preserve this ability; what is damaged is their capacity to faithfully express that awareness. Building on this, we propose Honesty-Critical Neurons Restoration (HCNR) to surgically repair this suppressed capacity. HCNR identifies and restores key expression-governing neurons to their pre-trained state while harmonizing them with task-oriented neurons via Hessian-guided compensation. Experiments on four QA tasks and five LLM families demonstrate that HCNR effectively recovers 33.25% of the compromised honesty while achieving at least 2.23x speedup with over 10x less data compared to baseline methods, offering a practical solution for trustworthy LLM deployment.

</details>


### [59] [AA-Omniscience: Evaluating Cross-Domain Knowledge Reliability in Large Language Models](https://arxiv.org/abs/2511.13029)
*Declan Jackson,William Keating,George Cameron,Micah Hill-Smith*

Main category: cs.CL

TL;DR: AA-Omniscience基准测试评估语言模型的事实回忆能力和知识校准能力，涵盖42个经济相关主题的6000个问题。Claude 4.1 Opus获得最高分4.8，但大多数前沿模型在事实性和校准方面仍存在弱点。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型评估主要关注通用能力，但实际应用中需要确保事实准确性和识别知识空白的能力。

Method: 开发AA-Omniscience基准，包含来自权威学术和行业来源的6000个问题，覆盖6个领域的42个经济相关主题，使用全知指数(-100到100)评估模型。

Result: Claude 4.1 Opus获得最高分4.8，是仅有的三个得分超过0的模型之一。不同模型在不同领域表现各异，三个不同研究实验室的模型在六个领域分别领先。

Conclusion: 前沿模型在事实性和校准方面存在持续弱点，性能因领域而异，建议根据具体用例需求而非通用性能来选择模型。

Abstract: Existing language model evaluations primarily measure general capabilities, yet reliable use of these models across a range of domains demands factual accuracy and recognition of knowledge gaps. We introduce AA-Omniscience, a benchmark designed to measure both factual recall and knowledge calibration across 6,000 questions. Questions are derived from authoritative academic and industry sources, and cover 42 economically relevant topics within six different domains. The evaluation measures a model's Omniscience Index, a bounded metric (-100 to 100) measuring factual recall that jointly penalizes hallucinations and rewards abstention when uncertain, with 0 equating to a model that answers questions correctly as much as it does incorrectly. Among evaluated models, Claude 4.1 Opus attains the highest score (4.8), making it one of only three models to score above zero. These results reveal persistent factuality and calibration weaknesses across frontier models. Performance also varies by domain, with the models from three different research labs leading across the six domains. This performance variability suggests models should be chosen according to the demands of the use case rather than general performance for tasks where knowledge is important.

</details>


### [60] [How Good is BLI as an Alignment Measure: A Study in Word Embedding Paradigm](https://arxiv.org/abs/2511.13040)
*Kasun Wickramasinghe,Nisansa de Silva*

Main category: cs.CL

TL;DR: 本研究探讨了多语言嵌入模型与对齐单语言模型在双语词典归纳(BLI)任务上的表现差异，分析了BLI作为嵌入空间对齐度量的局限性，并提出了基于词干的新BLI方法和词汇剪枝技术来改进评估。


<details>
  <summary>Details</summary>
Motivation: 当前多语言嵌入已成为主流选择，但需要验证其是否在所有方面都优于对齐单语言模型，以及其更高的计算成本是否总是合理。本研究旨在探索两种方法的优缺点，特别是在高资源和低资源语言环境下的表现。

Method: 使用双语词典归纳(BLI)作为评估指标，比较传统嵌入对齐技术、新型多语言模型和组合对齐技术的性能。分析语言家族对BLI结果的影响，提出基于词干的BLI方法和词汇剪枝技术来改进评估。

Result: 发现BLI在某些情况下不能准确衡量嵌入空间的对齐程度。组合嵌入对齐技术通常表现更好，但在某些情况下（主要是低资源语言）多语言嵌入表现更优。

Conclusion: 多语言嵌入和对齐单语言模型各有优势，需要根据具体应用场景选择。提出的基于词干的BLI方法和词汇剪枝技术能更准确地评估嵌入空间对齐程度，特别是在处理多语言嵌入模型时。

Abstract: Sans a dwindling number of monolingual embedding studies originating predominantly from the low-resource domains, it is evident that multilingual embedding has become the de facto choice due to its adaptability to the usage of code-mixed languages, granting the ability to process multilingual documents in a language-agnostic manner, as well as removing the difficult task of aligning monolingual embeddings. But is this victory complete? Are the multilingual models better than aligned monolingual models in every aspect? Can the higher computational cost of multilingual models always be justified? Or is there a compromise between the two extremes? Bilingual Lexicon Induction is one of the most widely used metrics in terms of evaluating the degree of alignment between two embedding spaces. In this study, we explore the strengths and limitations of BLI as a measure to evaluate the degree of alignment of two embedding spaces. Further, we evaluate how well traditional embedding alignment techniques, novel multilingual models, and combined alignment techniques perform BLI tasks in the contexts of both high-resource and low-resource languages. In addition to that, we investigate the impact of the language families to which the pairs of languages belong. We identify that BLI does not measure the true degree of alignment in some cases and we propose solutions for them. We propose a novel stem-based BLI approach to evaluate two aligned embedding spaces that take into account the inflected nature of languages as opposed to the prevalent word-based BLI techniques. Further, we introduce a vocabulary pruning technique that is more informative in showing the degree of the alignment, especially performing BLI on multilingual embedding models. Often, combined embedding alignment techniques perform better while in certain cases multilingual embeddings perform better (mainly low-resource language cases).

</details>


### [61] [Spark-Prover-X1: Formal Theorem Proving Through Diverse Data Training](https://arxiv.org/abs/2511.13043)
*Xinyuan Zhou,Yi Lei,Xiaoyu Zhou,Jingyi Sun,Yu Zhu,Zhongyi Ye,Weitai Zhang,Quan Liu,Si Wei,Cong Liu*

Main category: cs.CL

TL;DR: Spark-Prover-X1是一个7B参数的定理证明模型，通过三阶段训练框架提升轻量级LLM的形式推理能力，在多个基准测试中达到同类开源模型的最优性能。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在自动定理证明中因缺乏多样化和高质量形式语言数据而受限的问题，探索如何释放中等规模LLM的推理潜力。

Method: 采用三阶段训练框架：1) 在广泛数学语料上进行持续预训练，引入"思维链增强状态预测"任务；2) 在专家迭代循环中进行监督微调；3) 使用组相对策略优化针对最具挑战性问题进行强化。

Result: Spark-Prover-X1-7B在类似规模开源模型中达到最先进性能，平均通过率37.0%(pass@32)，在PutnamBench上解决27个问题，在CombiBench上达到24.0%。

Conclusion: 多样化的训练数据和渐进细化的训练流程为增强轻量级LLM的形式推理能力提供了有效路径。

Abstract: Large Language Models (LLMs) have shown significant promise in automated theorem proving, yet progress is often constrained by the scarcity of diverse and high-quality formal language data. To address this issue, we introduce Spark-Prover-X1, a 7B parameter model trained via an three-stage framework designed to unlock the reasoning potential of more accessible and moderately-sized LLMs. The first stage infuses deep knowledge through continuous pre-training on a broad mathematical corpus, enhanced by a suite of novel data tasks. Key innovation is a "CoT-augmented state prediction" task to achieve fine-grained reasoning. The second stage employs Supervised Fine-tuning (SFT) within an expert iteration loop to specialize both the Spark-Prover-X1-7B and Spark-Formalizer-X1-7B models. Finally, a targeted round of Group Relative Policy Optimization (GRPO) is applied to sharpen the prover's capabilities on the most challenging problems. To facilitate robust evaluation, particularly on problems from real-world examinations, we also introduce ExamFormal-Bench, a new benchmark dataset of 402 formal problems. Experimental results demonstrate that Spark-Prover-X1-7B achieves state-of-the-art performance among similarly-sized open-source models, attaining a 37.0\% average pass rate (pass@32). It shows exceptional performance on difficult competition benchmarks, notably solving 27 problems on PutnamBench (pass@32) and achieving 24.0\% on CombiBench (pass@32). Our work validates that this diverse training data and progressively refined training pipeline provides an effective path for enhancing the formal reasoning capabilities of lightweight LLMs. Both Spark-Prover-X1-7B and Spark-Formalizer-X1-7B, along with the ExamFormal-Bench dataset, are made publicly available at:https://www.modelscope.cn/organization/iflytek, https://gitcode.com/ifly_opensource.

</details>


### [62] [BeDiscovER: The Benchmark of Discourse Understanding in the Era of Reasoning Language Models](https://arxiv.org/abs/2511.13095)
*Chuyuan Li,Giuseppe Carenini*

Main category: cs.CL

TL;DR: BeDiscovER是一个用于评估现代LLM话语理解能力的综合性基准套件，包含5个话语任务和52个数据集，涵盖词汇、句子和文档层面。评估发现前沿模型在时间推理方面表现良好，但在完整文档推理和某些语义话语现象上仍有困难。


<details>
  <summary>Details</summary>
Motivation: 随着推理语言模型的发展，需要构建一个全面的话语理解评估基准来测试现代LLM在话语层面的知识能力，特别是覆盖从词汇到文档的多层次话语任务。

Method: 整合5个公开可用的话语任务，包括话语词汇、(多)句子和文档层面，总共52个数据集。评估了开源LLM和前沿模型在这些任务上的表现。

Result: 最先进的模型在时间推理的算术方面表现出色，但在完整文档推理和某些细微语义话语现象（如修辞关系识别）上表现不佳。

Conclusion: BeDiscovER提供了一个全面的话语理解评估框架，揭示了现代LLM在话语理解方面的优势和局限，特别是在文档级推理和复杂语义现象处理上仍有改进空间。

Abstract: We introduce BeDiscovER (Benchmark of Discourse Understanding in the Era of Reasoning Language Models), an up-to-date, comprehensive suite for evaluating the discourse-level knowledge of modern LLMs. BeDiscovER compiles 5 publicly available discourse tasks across discourse lexicon, (multi-)sentential, and documental levels, with in total 52 individual datasets. It covers both extensively studied tasks such as discourse parsing and temporal relation extraction, as well as some novel challenges such as discourse particle disambiguation (e.g., ``just''), and also aggregates a shared task on Discourse Relation Parsing and Treebanking for multilingual and multi-framework discourse relation classification. We evaluate open-source LLMs: Qwen3 series, DeepSeek-R1, and frontier model such as GPT-5-mini on BeDiscovER, and find that state-of-the-art models exhibit strong performance in arithmetic aspect of temporal reasoning, but they struggle with full document reasoning and some subtle semantic and discourse phenomena, such as rhetorical relation recognition.

</details>


### [63] [Evaluating the Ability of Large Language Models to Identify Adherence to CONSORT Reporting Guidelines in Randomized Controlled Trials: A Methodological Evaluation Study](https://arxiv.org/abs/2511.13107)
*Zhichao He,Mouxiao Bian,Jianhong Zhu,Jiayuan Chen,Yunqiu Wang,Wenxia Zhao,Tianbin Li,Bing Han,Jie Xu,Junyan Wu*

Main category: cs.CL

TL;DR: 当代大型语言模型在零样本设置下评估随机对照试验对CONSORT 2010声明的依从性，表现中等，能够准确识别合规项目，但在检测不合规和不适用项目方面表现较差，目前尚无法替代人类专家进行试验质量评估。


<details>
  <summary>Details</summary>
Motivation: 手动验证CONSORT依从性是一个耗时费力的过程，成为同行评审和证据合成的重要瓶颈，需要评估LLMs在此任务中的准确性和可靠性。

Method: 构建包含150篇已发表RCT的金标准数据集，在零样本设置下评估当代LLMs识别CONSORT 2010声明依从性的能力，主要指标为三类分类任务的宏平均F1分数。

Result: 最佳模型Gemini-2.5-Flash和DeepSeek-R1的宏F1分数分别为0.634，Cohen's Kappa系数分别为0.280和0.282，仅与专家共识达成一般一致性。模型在识别合规项目时准确率高（F1>0.850），但在识别不合规和不适用项目时表现差（F1<0.400）。

Conclusion: LLMs作为CONSORT检查的初步筛选助手具有潜力，能够识别报告良好的项目，但目前无法可靠检测报告遗漏或方法学缺陷，不适合替代人类专家进行试验质量关键评估。

Abstract: The Consolidated Standards of Reporting Trials statement is the global benchmark for transparent and high-quality reporting of randomized controlled trials. Manual verification of CONSORT adherence is a laborious, time-intensive process that constitutes a significant bottleneck in peer review and evidence synthesis. This study aimed to systematically evaluate the accuracy and reliability of contemporary LLMs in identifying the adherence of published RCTs to the CONSORT 2010 statement under a zero-shot setting. We constructed a golden standard dataset of 150 published RCTs spanning diverse medical specialties. The primary outcome was the macro-averaged F1-score for the three-class classification task, supplemented by item-wise performance metrics and qualitative error analysis. Overall model performance was modest. The top-performing models, Gemini-2.5-Flash and DeepSeek-R1, achieved nearly identical macro F1 scores of 0.634 and Cohen's Kappa coefficients of 0.280 and 0.282, respectively, indicating only fair agreement with expert consensus. A striking performance disparity was observed across classes: while most models could identify compliant items with high accuracy (F1 score > 0.850), they struggled profoundly with identifying non-compliant and not applicable items, where F1 scores rarely exceeded 0.400. Notably, some high-profile models like GPT-4o underperformed, achieving a macro F1-score of only 0.521. LLMs show potential as preliminary screening assistants for CONSORT checks, capably identifying well-reported items. However, their current inability to reliably detect reporting omissions or methodological flaws makes them unsuitable for replacing human expertise in the critical appraisal of trial quality.

</details>


### [64] [Extracting Events Like Code: A Multi-Agent Programming Framework for Zero-Shot Event Extraction](https://arxiv.org/abs/2511.13118)
*Quanjiang Guo,Sijie Wang,Jinchuan Zhang,Ben Zhang,Zhao Kang,Ling Tian,Ke Yan*

Main category: cs.CL

TL;DR: Agent-Event-Coder (AEC) 是一个多智能体框架，将零样本事件抽取视为类似软件工程的代码生成过程，通过专门的智能体协作解决LLM在事件抽取中的结构无效输出问题。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在零样本事件抽取中的挑战，包括复杂推理需求、领域特定理解困难，以及直接提示导致的不完整或结构无效输出（如错误分类触发器、缺失参数和模式违规）。

Method: 提出AEC多智能体框架，将事件抽取分解为检索、规划、编码和验证四个专门子任务，每个任务由专用LLM智能体处理。事件模式表示为可执行类定义，通过验证智能体实现确定性验证和精确反馈。

Result: 在五个不同领域和六个LLM上的实验表明，AEC始终优于先前的零样本基线方法，证明了将事件抽取视为代码生成方法的有效性。

Conclusion: 通过将事件抽取视为结构化、迭代的代码生成过程，AEC能够使LLM在零样本设置下产生精确、完整且模式一致的事件抽取结果。

Abstract: Zero-shot event extraction (ZSEE) remains a significant challenge for large language models (LLMs) due to the need for complex reasoning and domain-specific understanding. Direct prompting often yields incomplete or structurally invalid outputs--such as misclassified triggers, missing arguments, and schema violations. To address these limitations, we present Agent-Event-Coder (AEC), a novel multi-agent framework that treats event extraction like software engineering: as a structured, iterative code-generation process. AEC decomposes ZSEE into specialized subtasks--retrieval, planning, coding, and verification--each handled by a dedicated LLM agent. Event schemas are represented as executable class definitions, enabling deterministic validation and precise feedback via a verification agent. This programming-inspired approach allows for systematic disambiguation and schema enforcement through iterative refinement. By leveraging collaborative agent workflows, AEC enables LLMs to produce precise, complete, and schema-consistent extractions in zero-shot settings. Experiments across five diverse domains and six LLMs demonstrate that AEC consistently outperforms prior zero-shot baselines, showcasing the power of treating event extraction like code generation. The code and data are released on https://github.com/UESTC-GQJ/Agent-Event-Coder.

</details>


### [65] [A Comparative Analysis of Recurrent and Attention Architectures for Isolated Sign Language Recognition](https://arxiv.org/abs/2511.13126)
*Nigar Alishzade,Gulchin Abdullayeva*

Main category: cs.CL

TL;DR: 比较了ConvLSTM和Vanilla Transformer在手语识别中的表现，发现Transformer在准确率上更优，而ConvLSTM在计算效率上更有优势。


<details>
  <summary>Details</summary>
Motivation: 系统比较循环神经网络和基于注意力的神经网络架构在孤立手语识别中的性能差异，为手语识别系统选择合适的架构提供指导。

Method: 在阿塞拜疆手语数据集(AzSLD)和美国手语词汇级数据集(WLASL)上实现并评估ConvLSTM和Vanilla Transformer两种代表性模型。

Result: 基于注意力的Vanilla Transformer在两个数据集的Top-1和Top-5准确率上均优于循环ConvLSTM，在AzSLD上达到76.8% Top-1准确率，在WLASL上达到88.3%。ConvLSTM虽然计算效率更高，但在识别准确率上落后，特别是在小数据集上。

Conclusion: 两种范式各有优势：Transformer在整体准确率和说话者独立性方面表现优异，而ConvLSTM在计算效率和时序建模方面具有优势。研究为根据应用需求和资源约束选择手语识别架构提供了指导。

Abstract: This study presents a systematic comparative analysis of recurrent and attention-based neural architectures for isolated sign language recognition. We implement and evaluate two representative models-ConvLSTM and Vanilla Transformer-on the Azerbaijani Sign Language Dataset (AzSLD) and the Word-Level American Sign Language (WLASL) dataset. Our results demonstrate that the attention-based Vanilla Transformer consistently outperforms the recurrent ConvLSTM in both Top-1 and Top-5 accuracy across datasets, achieving up to 76.8% Top-1 accuracy on AzSLD and 88.3% on WLASL. The ConvLSTM, while more computationally efficient, lags in recognition accuracy, particularly on smaller datasets. These findings highlight the complementary strengths of each paradigm: the Transformer excels in overall accuracy and signer independence, whereas the ConvLSTM offers advantages in computational efficiency and temporal modeling. The study provides a nuanced analysis of these trade-offs, offering guidance for architecture selection in sign language recognition systems depending on application requirements and resource constraints.

</details>


### [66] [Zero-Shot Grammar Competency Estimation Using Large Language Model Generated Pseudo Labels](https://arxiv.org/abs/2511.13152)
*Sourya Dipta Das,Shubham Kumar,Kuldeep Yadav*

Main category: cs.CL

TL;DR: 提出了一种零样本语法能力评估框架，利用未标记数据和大型语言模型生成伪标签，无需人工标注即可训练基于transformer的语法评分模型。


<details>
  <summary>Details</summary>
Motivation: 口语语法评估面临自发、非结构化和不流畅的挑战，且需要大量专家标注，大规模数据创建不切实际。

Method: 使用基于语法能力量表的提示词让LLM在未标记数据上生成预测作为伪标签，通过专门设计的训练框架处理标签噪声来训练transformer模型。

Result: 实验结果表明该方法能高精度估计语法能力分数，LLM选择对性能至关重要，训练中干净样本与噪声样本的比例影响稳定性和准确性。

Conclusion: 该方法为可扩展、低资源的语法评估系统铺平了道路，具有鲁棒性和可解释性。

Abstract: Grammar competency estimation is essential for assessing linguistic proficiency in both written and spoken language; however, the spoken modality presents additional challenges due to its spontaneous, unstructured, and disfluent nature. Developing accurate grammar scoring models further requires extensive expert annotation, making large-scale data creation impractical. To address these limitations, we propose a zero-shot grammar competency estimation framework that leverages unlabeled data and Large Language Models (LLMs) without relying on manual labels. During training, we employ LLM-generated predictions on unlabeled data by using grammar competency rubric-based prompts. These predictions, treated as pseudo labels, are utilized to train a transformer-based model through a novel training framework designed to handle label noise effectively. We show that the choice of LLM for pseudo-label generation critically affects model performance and that the ratio of clean-to-noisy samples during training strongly influences stability and accuracy. Finally, a qualitative analysis of error intensity and score prediction confirms the robustness and interpretability of our approach. Experimental results demonstrate the efficacy of our approach in estimating grammar competency scores with high accuracy, paving the way for scalable, low-resource grammar assessment systems.

</details>


### [67] [Distinguishing Repetition Disfluency from Morphological Reduplication in Bangla ASR Transcripts: A Novel Corpus and Benchmarking Analysis](https://arxiv.org/abs/2511.13159)
*Zaara Zabeen Arpa,Sadnam Sakib Apurbo,Nazia Karim Khan Oishee,Ajwad Abrar*

Main category: cs.CL

TL;DR: 开发了首个孟加拉语ASR转录本语料库，区分重复性不流利和形态学重叠现象，通过LLM和微调方法建立基准，BanglaBERT模型达到最佳性能。


<details>
  <summary>Details</summary>
Motivation: 解决孟加拉语ASR转录中词重复的歧义问题：区分无意的不流利重复和故意的语法结构重叠，避免标准不流利校正方法误删有效语言信息。

Method: 创建了20,000行手动标注的孟加拉语语料库，使用两种方法：1) 多语言LLM的少样本提示；2) 编码器模型的任务特定微调。

Result: LLM在少样本提示下达到82.68%准确率，但微调方法更优，BanglaBERT模型达到84.78%准确率和0.677 F1分数。

Conclusion: 建立了强大的语言感知基准，为开发复杂的语义保留文本规范化系统提供了必要数据。

Abstract: Automatic Speech Recognition (ASR) transcripts, especially in low-resource languages like Bangla, contain a critical ambiguity: word-word repetitions can be either Repetition Disfluency (unintentional ASR error/hesitation) or Morphological Reduplication (a deliberate grammatical construct). Standard disfluency correction fails by erroneously deleting valid linguistic information. To solve this, we introduce the first publicly available, 20,000-row Bangla corpus, manually annotated to explicitly distinguish between these two phenomena in noisy ASR transcripts. We benchmark this novel resource using two paradigms: state-of-the-art multilingual Large Language Models (LLMs) and task-specific fine-tuning of encoder models. LLMs achieve competitive performance (up to 82.68\% accuracy) with few-shot prompting. However, fine-tuning proves superior, with the language-specific BanglaBERT model achieving the highest accuracy of 84.78\% and an F1 score of 0.677. This establishes a strong, linguistically-informed baseline and provides essential data for developing sophisticated, semantic-preserving text normalization systems for Bangla.

</details>


### [68] [TCM-5CEval: Extended Deep Evaluation Benchmark for LLM's Comprehensive Clinical Research Competence in Traditional Chinese Medicine](https://arxiv.org/abs/2511.13169)
*Tianai Huang,Jiayuan Chen,Lu Lu,Pengcheng Chen,Tianbin Li,Bing Han,Wenchao Tang,Jie Xu,Ming Li*

Main category: cs.CL

TL;DR: TCM-5CEval是一个针对中医领域的更细粒度、更全面的LLM评估基准，涵盖五个关键维度：核心知识、经典文献素养、临床决策、中药学和临床非药物治疗。评估发现模型在基础知识回忆方面表现良好，但在经典文本解释和推理稳定性方面存在显著弱点。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在中医等高度专业化和文化丰富的领域应用时，需要更严谨和细致的评估。基于之前TCM-3CEval的工作，发现系统性的知识差距和文化语境对齐的重要性，需要更全面的评估工具。

Method: 开发了TCM-5CEval基准，包含五个评估维度：TCM-Exam（核心知识）、TCM-LitQA（经典文献素养）、TCM-MRCD（临床决策）、TCM-CMM（中药学）、TCM-ClinNPT（临床非药物治疗）。对15个主流LLM进行了全面评估，并采用基于排列的一致性测试来检验推理稳定性。

Result: 评估显示显著的性能差异，deepseek_r1和gemini_2_5_pro表现最佳。模型在基础知识回忆方面表现良好，但在经典文本解释方面存在困难。排列一致性测试显示所有模型都存在推理脆弱性，面对不同选项排序时性能大幅下降，表明普遍存在位置偏见和缺乏稳健理解。

Conclusion: TCM-5CEval不仅为中医领域的LLM能力提供了更详细的诊断工具，还暴露了其推理稳定性的根本弱点。该基准已上传至Medbench平台，以促进进一步研究和标准化比较。

Abstract: Large language models (LLMs) have demonstrated exceptional capabilities in general domains, yet their application in highly specialized and culturally-rich fields like Traditional Chinese Medicine (TCM) requires rigorous and nuanced evaluation. Building upon prior foundational work such as TCM-3CEval, which highlighted systemic knowledge gaps and the importance of cultural-contextual alignment, we introduce TCM-5CEval, a more granular and comprehensive benchmark. TCM-5CEval is designed to assess LLMs across five critical dimensions: (1) Core Knowledge (TCM-Exam), (2) Classical Literacy (TCM-LitQA), (3) Clinical Decision-making (TCM-MRCD), (4) Chinese Materia Medica (TCM-CMM), and (5) Clinical Non-pharmacological Therapy (TCM-ClinNPT). We conducted a thorough evaluation of fifteen prominent LLMs, revealing significant performance disparities and identifying top-performing models like deepseek\_r1 and gemini\_2\_5\_pro. Our findings show that while models exhibit proficiency in recalling foundational knowledge, they struggle with the interpretative complexities of classical texts. Critically, permutation-based consistency testing reveals widespread fragilities in model inference. All evaluated models, including the highest-scoring ones, displayed a substantial performance degradation when faced with varied question option ordering, indicating a pervasive sensitivity to positional bias and a lack of robust understanding. TCM-5CEval not only provides a more detailed diagnostic tool for LLM capabilities in TCM but aldso exposes fundamental weaknesses in their reasoning stability. To promote further research and standardized comparison, TCM-5CEval has been uploaded to the Medbench platform, joining its predecessor in the "In-depth Challenge for Comprehensive TCM Abilities" special track.

</details>


### [69] [Translation Entropy: A Statistical Framework for Evaluating Translation Systems](https://arxiv.org/abs/2511.13180)
*Ronit D. Gross,Yanir Harel,Ido Kanter*

Main category: cs.CL

TL;DR: 本文提出了一种量化评估翻译器性能的方法，通过分析翻译熵来客观衡量机器翻译系统的表现。


<details>
  <summary>Details</summary>
Motivation: 在信息时代，机器翻译变得越来越重要，但目前缺乏客观的量化方法来评估翻译器的性能，主要是因为单语言的熵值仍然未知。

Method: 通过选择基准句中的特定标记，替换为其他标记但保持翻译不变，统计这种现象的概率分布来计算翻译熵。该方法可扩展到替换多个标记的情况。

Result: 研究发现翻译退化度与标记退化度的乘积成正比，翻译熵沿解码器块增强，该方法能够对公开可用的翻译器进行定量排名并揭示互译熵的对称性。

Conclusion: 翻译熵是一个可测量的属性，为人工翻译器提供了客观的基准测试方法，基于MarianMT、T5-Base和NLLB-200翻译器的实验结果验证了该方法的有效性。

Abstract: The translation of written language has been known since the 3rd century BC; however, its necessity has become increasingly common in the information age. Today, many translators exist, based on encoder-decoder deep architectures, nevertheless, no quantitative objective methods are available to assess their performance, likely because the entropy of even a single language remains unknown. This study presents a quantitative method for estimating translation entropy, with the following key finding. Given a translator, several sentences that differ by only one selected token of a given pivot sentence yield identical translations. Analyzing the statistics of this phenomenon across an ensemble of such sentences, consisting each of a pivot selected token, yields the probabilities of replacing this specific token with others while preserving the translation. These probabilities constitute the entropy of the selected token, and the average across all selected pivot tokens provides an estimate of the translator's overall translation entropy, which is enhanced along the decoder blocks. This entropic measure allows for the quantitative ranking of several publicly available translators and reveals whether mutual translation entropy is symmetric. Extending the proposed method to include the replacement of two tokens in a given pivot sentence demonstrates a multiplicative effect, where translation degeneracy is proportional to the product of the degeneracies of the two tokens. These findings establish translation entropy as a measurable property and objective benchmarking of artificial translators. Results are based on MarianMT, T5-Base and NLLB-200 translators.

</details>


### [70] [Evaluating Large Language Models for Diacritic Restoration in Romanian Texts: A Comparative Study](https://arxiv.org/abs/2511.13182)
*Mihai Dan Nadas,Laura Diosan*

Main category: cs.CL

TL;DR: 评估多种大型语言模型在罗马尼亚语变音符号恢复任务中的表现，发现GPT-4o等模型表现优异，而Llama系列模型表现波动较大。


<details>
  <summary>Details</summary>
Motivation: 自动变音符号恢复对于处理像罗马尼亚语这样具有丰富变音符号的语言至关重要，需要评估当前大型语言模型在此任务上的性能。

Method: 使用综合语料库测试了包括GPT-3.5、GPT-4、GPT-4o、Gemini 1.0 Pro、Llama 2/3、Mixtral 8x7B等在内的多种LLM，采用从零样本到复杂多样本指令的多种提示模板。

Result: GPT-4o等模型实现了高精度的变音符号恢复，始终超过中性回显基线，而Meta的Llama系列模型表现出更大的变异性。

Conclusion: 模型架构、训练数据和提示设计对变音符号恢复性能有显著影响，为改进变音符号丰富语言的NLP工具指明了有前景的方向。

Abstract: Automatic diacritic restoration is crucial for text processing in languages with rich diacritical marks, such as Romanian. This study evaluates the performance of several large language models (LLMs) in restoring diacritics in Romanian texts. Using a comprehensive corpus, we tested models including OpenAI's GPT-3.5, GPT-4, GPT-4o, Google's Gemini 1.0 Pro, Meta's Llama 2 and Llama 3, MistralAI's Mixtral 8x7B Instruct, airoboros 70B, and OpenLLM-Ro's RoLlama 2 7B, under multiple prompt templates ranging from zero-shot to complex multi-shot instructions. Results show that models such as GPT-4o achieve high diacritic restoration accuracy, consistently surpassing a neutral echo baseline, while others, including Meta's Llama family, exhibit wider variability. These findings highlight the impact of model architecture, training data, and prompt design on diacritic restoration performance and outline promising directions for improving NLP tools for diacritic-rich languages.

</details>


### [71] [Seeing isn't Hearing: Benchmarking Vision Language Models at Interpreting Spectrograms](https://arxiv.org/abs/2511.13225)
*Tyler Loakman,Joseph James,Chenghua Lin*

Main category: cs.CL

TL;DR: 本文评估了视觉语言模型在语音识别任务中的表现，发现即使经过微调，模型在从语谱图和波形识别英语单词方面的表现也很少超过随机猜测。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型和视觉语言模型的发展，需要评估它们在融合视觉和语言模态任务中的能力，特别是作为语音学专家解释语音语谱图和波形的能力。

Method: 创建包含4000多个孤立英语单词的新数据集，通过多项选择任务测试模型从语谱图和波形预测正确音素或字母转录的能力，使用基于音素编辑距离的干扰项。

Result: 零样本和微调模型的表现很少超过随机猜测水平，表明需要特定的参数知识来解释这类图形，而不仅仅是配对样本。

Conclusion: 视觉语言模型需要专门的参数知识才能有效解释语音语谱图和波形，仅靠配对样本训练不足以获得这种能力。

Abstract: With the rise of Large Language Models (LLMs) and their vision-enabled counterparts (VLMs), numerous works have investigated their capabilities in tasks that fuse the modalities of vision and language. In this work, we benchmark the extent to which VLMs are able to act as highly-trained phoneticians, interpreting spectrograms and waveforms of speech. To do this, we synthesise a novel dataset containing 4k+ English words spoken in isolation alongside stylistically consistent spectrogram and waveform figures. We test the ability of VLMs to understand these representations of speech through a multiple-choice task whereby models must predict the correct phonemic or graphemic transcription of a spoken word when presented amongst 3 distractor transcriptions that have been selected based on their phonemic edit distance to the ground truth. We observe that both zero-shot and finetuned models rarely perform above chance, demonstrating the requirement for specific parametric knowledge of how to interpret such figures, rather than paired samples alone.

</details>


### [72] [Souper-Model: How Simple Arithmetic Unlocks State-of-the-Art LLM Performance](https://arxiv.org/abs/2511.13254)
*Shalini Maiti,Amar Budhiraja,Bhavul Gauri,Gaurav Chaurasia,Anton Protopopov,Alexis Audran-Reiss,Michael Slater,Despoina Magka,Tatiana Shavrina,Roberta Raileanu,Yoram Bachrach*

Main category: cs.CL

TL;DR: SoCE是一种基于类别专家的模型融合方法，通过识别不同类别的最优模型并使用非均匀加权平均来提升性能，在多项任务中达到SOTA结果。


<details>
  <summary>Details</summary>
Motivation: LLM训练资源密集且耗时，模型融合（model souping）作为一种无需重新训练就能提升性能的技术具有重要价值。传统均匀平均方法未充分利用不同类别任务中模型性能的低相关性。

Method: 提出SoCE方法：1）利用基准测试组成识别最优模型候选；2）为每个弱相关类别集群识别"专家"模型；3）使用优化的非均匀加权平均而非均匀权重进行模型融合。

Result: 该方法在多项领域（多语言能力、工具调用、数学等）提升了性能和鲁棒性，在伯克利函数调用排行榜上达到最先进结果。

Conclusion: SoCE提供了一种原则性的模型融合方法，通过利用类别间性能低相关性的观察和优化的加权策略，有效提升了模型性能而无需昂贵重新训练。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse domains, but their training remains resource- and time-intensive, requiring massive compute power and careful orchestration of training procedures. Model souping-the practice of averaging weights from multiple models of the same architecture-has emerged as a promising pre- and post-training technique that can enhance performance without expensive retraining. In this paper, we introduce Soup Of Category Experts (SoCE), a principled approach for model souping that utilizes benchmark composition to identify optimal model candidates and applies non-uniform weighted averaging to maximize performance. Contrary to previous uniform-averaging approaches, our method leverages the observation that benchmark categories often exhibit low inter-correlations in model performance. SoCE identifies "expert" models for each weakly-correlated category cluster and combines them using optimized weighted averaging rather than uniform weights. We demonstrate that the proposed method improves performance and robustness across multiple domains, including multilingual capabilities, tool calling, and math and achieves state-of-the-art results on the Berkeley Function Calling Leaderboard.

</details>


### [73] [RegionMarker: A Region-Triggered Semantic Watermarking Framework for Embedding-as-a-Service Copyright Protection](https://arxiv.org/abs/2511.13329)
*Shufan Yang,Zifeng Cheng,Zhiwei Jiang,Yafeng Yin,Cong Wang,Shiping Ge,Yuchen Fu,Qing Gu*

Main category: cs.CL

TL;DR: RegionMarker是一种区域触发语义水印框架，通过定义低维空间中的触发区域并向相关文本嵌入注入水印，为EaaS提供全面的版权保护。


<details>
  <summary>Details</summary>
Motivation: 现有的EaaS水印方法只能抵抗部分攻击，无法提供全面保护，存在被模型提取攻击导致经济损失的风险。

Method: 使用秘密降维矩阵投影到子空间，随机选择触发区域，在整个触发区域嵌入水印，并以文本嵌入作为水印本身。

Result: 在多个数据集上的实验表明，RegionMarker能有效抵抗不同攻击方法，保护EaaS版权。

Conclusion: RegionMarker框架通过区域触发的水印机制，为EaaS提供了全面的版权保护解决方案。

Abstract: Embedding-as-a-Service (EaaS) is an effective and convenient deployment solution for addressing various NLP tasks. Nevertheless, recent research has shown that EaaS is vulnerable to model extraction attacks, which could lead to significant economic losses for model providers. For copyright protection, existing methods inject watermark embeddings into text embeddings and use them to detect copyright infringement. However, current watermarking methods often resist only a subset of attacks and fail to provide \textit{comprehensive} protection. To this end, we present the region-triggered semantic watermarking framework called RegionMarker, which defines trigger regions within a low-dimensional space and injects watermarks into text embeddings associated with these regions. By utilizing a secret dimensionality reduction matrix to project onto this subspace and randomly selecting trigger regions, RegionMarker makes it difficult for watermark removal attacks to evade detection. Furthermore, by embedding watermarks across the entire trigger region and using the text embedding as the watermark, RegionMarker is resilient to both paraphrasing and dimension-perturbation attacks. Extensive experiments on various datasets show that RegionMarker is effective in resisting different attack methods, thereby protecting the copyright of EaaS.

</details>


### [74] [AHaSIS: Shared Task on Sentiment Analysis for Arabic Dialects](https://arxiv.org/abs/2511.13335)
*Maram Alharbi,Salmane Chafik,Saad Ezzini,Ruslan Mitkov,Tharindu Ranasinghe,Hansi Hettiarachchi*

Main category: cs.CL

TL;DR: 该论文介绍了阿拉伯语方言情感分析共享任务，使用从酒店评论翻译而来的沙特和摩洛哥方言数据集，支持方言感知NLP系统开发。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯世界酒店业越来越依赖客户反馈来改进服务，因此需要先进的阿拉伯语情感分析工具来处理方言差异。

Method: 创建多方言数据集，将现代标准阿拉伯语的酒店评论人工翻译为沙特和摩洛哥方言，并由母语者验证翻译准确性和情感保持。数据集包含538条情感平衡的评论。

Result: 超过40个团队注册参与，12个团队提交系统。最佳系统F1分数达到0.81，证明了跨阿拉伯方言情感分析的可行性。

Conclusion: 该资源支持开发方言感知NLP系统，用于客户体验分析的实际应用，同时显示了跨方言情感分析的持续挑战。

Abstract: The hospitality industry in the Arab world increasingly relies on customer feedback to shape services, driving the need for advanced Arabic sentiment analysis tools. To address this challenge, the Sentiment Analysis on Arabic Dialects in the Hospitality Domain shared task focuses on Sentiment Detection in Arabic Dialects. This task leverages a multi-dialect, manually curated dataset derived from hotel reviews originally written in Modern Standard Arabic (MSA) and translated into Saudi and Moroccan (Darija) dialects. The dataset consists of 538 sentiment-balanced reviews spanning positive, neutral, and negative categories. Translations were validated by native speakers to ensure dialectal accuracy and sentiment preservation. This resource supports the development of dialect-aware NLP systems for real-world applications in customer experience analysis. More than 40 teams have registered for the shared task, with 12 submitting systems during the evaluation phase. The top-performing system achieved an F1 score of 0.81, demonstrating the feasibility and ongoing challenges of sentiment analysis across Arabic dialects.

</details>


### [75] [Donors and Recipients: On Asymmetric Transfer Across Tasks and Languages with Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2511.13368)
*Kajetan Dymkiewicz,Ivan Vulic,Helen Yannakoudakis,Eilam Shapira,Roi Reichart,Anna Korhonen*

Main category: cs.CL

TL;DR: 该研究通过多语言多任务的PEFT/LoRA实验，揭示了两个关键模式：任务内跨语言迁移稳定为正，而跨任务迁移常导致性能下降；以及稳定的捐赠者-接收者结构。


<details>
  <summary>Details</summary>
Motivation: 理解LLMs在任务和语言维度上的改进如何相互影响，以及这种跨任务跨语言迁移的模式和规律。

Method: 采用PEFT/LoRA方法，在不同LLM家族和规模上进行控制实验，通过在单一任务-语言源上微调，测量在所有其他任务-语言目标对上的迁移效果。

Result: 发现任务内跨语言迁移可靠地为正，而跨任务迁移常导致性能下降；识别出稳定的捐赠者-接收者结构。

Conclusion: 这些发现对风险感知的微调和模型专业化具有重要意义，揭示了跨任务跨语言迁移的不对称性和结构性模式。

Abstract: Large language models (LLMs) perform strongly across tasks and languages, yet how improvements in one task or language affect other tasks and languages and their combinations remains poorly understood. We conduct a controlled PEFT/LoRA study across multiple open-weight LLM families and sizes, treating task and language as transfer axes while conditioning on model family and size; we fine-tune each model on a single task-language source and measure transfer as the percentage-point change versus its baseline score when evaluated on all other task-language target pairs. We decompose transfer into (i) Matched-Task (Cross-Language), (ii) Matched-Language (Cross-Task), and (iii) Cross-Task (Cross-Language) regimes. We uncover two consistent general patterns. First, a pronounced on-task vs. off-task asymmetry: Matched-Task (Cross-Language) transfer is reliably positive, whereas off-task transfer often incurs collateral degradation. Second, a stable donor-recipient structure across languages and tasks (hub donors vs. brittle recipients). We outline implications for risk-aware fine-tuning and model specialisation.

</details>


### [76] [Can Large Language Models Function as Qualified Pediatricians? A Systematic Evaluation in Real-World Clinical Contexts](https://arxiv.org/abs/2511.13381)
*Siyu Zhu,Mouxiao Bian,Yue Xie,Yongyu Tang,Zhikang Yu,Tianbin Li,Pengcheng Chen,Bing Han,Jie Xu,Xiaoyan Dong*

Main category: cs.CL

TL;DR: PEDIASBench评估框架显示，当前大语言模型在儿科医学中基础知识表现良好，但在复杂推理、动态诊疗和人文关怀方面仍有局限，无法独立承担儿科诊疗工作，但可作为决策支持和医学教育的辅助工具。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在医学领域的快速发展，需要评估它们是否能在真实临床环境中胜任儿科医生的工作，了解其当前能力和局限性。

Method: 开发PEDIASBench系统性评估框架，从基础知识应用、动态诊疗能力、儿科医疗安全与伦理三个维度，评估12个代表性模型在19个儿科亚专业和211种典型疾病上的表现。

Result: 最先进模型在基础知识上表现良好（Qwen3-235B-A22B在执业级问题上准确率超90%），但随着任务复杂度增加性能下降约15%；在动态诊疗中DeepSeek-R1得分最高（平均0.58），但多数模型难以适应实时患者变化；在医疗伦理安全任务上Qwen2.5-72B表现最佳（准确率92.05%），但人文敏感性有限。

Conclusion: 当前儿科大语言模型受限于有限的动态决策能力和未充分发展的人文关怀，未来应关注多模态整合和临床反馈-模型迭代循环，以提升安全性、可解释性和人机协作。虽然不能独立承担儿科诊疗，但在决策支持、医学教育和患者沟通方面具有潜力。

Abstract: With the rapid rise of large language models (LLMs) in medicine, a key question is whether they can function as competent pediatricians in real-world clinical settings. We developed PEDIASBench, a systematic evaluation framework centered on a knowledge-system framework and tailored to realistic clinical environments. PEDIASBench assesses LLMs across three dimensions: application of basic knowledge, dynamic diagnosis and treatment capability, and pediatric medical safety and medical ethics. We evaluated 12 representative models released over the past two years, including GPT-4o, Qwen3-235B-A22B, and DeepSeek-V3, covering 19 pediatric subspecialties and 211 prototypical diseases. State-of-the-art models performed well on foundational knowledge, with Qwen3-235B-A22B achieving over 90% accuracy on licensing-level questions, but performance declined ~15% as task complexity increased, revealing limitations in complex reasoning. Multiple-choice assessments highlighted weaknesses in integrative reasoning and knowledge recall. In dynamic diagnosis and treatment scenarios, DeepSeek-R1 scored highest in case reasoning (mean 0.58), yet most models struggled to adapt to real-time patient changes. On pediatric medical ethics and safety tasks, Qwen2.5-72B performed best (accuracy 92.05%), though humanistic sensitivity remained limited. These findings indicate that pediatric LLMs are constrained by limited dynamic decision-making and underdeveloped humanistic care. Future development should focus on multimodal integration and a clinical feedback-model iteration loop to enhance safety, interpretability, and human-AI collaboration. While current LLMs cannot independently perform pediatric care, they hold promise for decision support, medical education, and patient communication, laying the groundwork for a safe, trustworthy, and collaborative intelligent pediatric healthcare system.

</details>


### [77] [Mem-PAL: Towards Memory-based Personalized Dialogue Assistants for Long-term User-Agent Interaction](https://arxiv.org/abs/2511.13410)
*Zhaopei Huang,Qifeng Dai,Guozheng Wu,Xiaopeng Wu,Kehan Chen,Chuan Yu,Xubin Li,Tiezheng Ge,Wenxuan Wang,Qin Jin*

Main category: cs.CL

TL;DR: 提出了PAL-Bench基准和H²Memory框架，用于评估和改进面向服务的个性化对话助手在长期交互中的表现。


<details>
  <summary>Details</summary>
Motivation: 随着智能个人设备的普及，需要能够理解用户特定特征的个性化对话助手，但现有方法往往忽视长期交互的复杂性和用户主观特征。

Method: 开发了多步骤LLM合成流程创建PAL-Set中文数据集，并提出了分层异构内存框架H²Memory，结合检索增强生成来改进个性化响应生成。

Result: 在PAL-Bench和外部数据集上的综合实验证明了所提内存框架的有效性。

Conclusion: PAL-Bench为评估服务导向助手的个性化能力提供了新基准，H²Memory框架显著提升了长期用户-代理交互中的个性化表现。

Abstract: With the rise of smart personal devices, service-oriented human-agent interactions have become increasingly prevalent. This trend highlights the need for personalized dialogue assistants that can understand user-specific traits to accurately interpret requirements and tailor responses to individual preferences. However, existing approaches often overlook the complexities of long-term interactions and fail to capture users' subjective characteristics. To address these gaps, we present PAL-Bench, a new benchmark designed to evaluate the personalization capabilities of service-oriented assistants in long-term user-agent interactions. In the absence of available real-world data, we develop a multi-step LLM-based synthesis pipeline, which is further verified and refined by human annotators. This process yields PAL-Set, the first Chinese dataset comprising multi-session user logs and dialogue histories, which serves as the foundation for PAL-Bench. Furthermore, to improve personalized service-oriented interactions, we propose H$^2$Memory, a hierarchical and heterogeneous memory framework that incorporates retrieval-augmented generation to improve personalized response generation. Comprehensive experiments on both our PAL-Bench and an external dataset demonstrate the effectiveness of the proposed memory framework.

</details>


### [78] [Non-Linear Scoring Model for Translation Quality Evaluation](https://arxiv.org/abs/2511.13467)
*Serge Gladkoff,Lifeng Han,Katerina Gasova*

Main category: cs.CL

TL;DR: 提出了一种基于对数函数的非线性翻译质量评估模型，解决了传统线性评估方法在不同文本长度下的偏差问题。


<details>
  <summary>Details</summary>
Motivation: 传统的线性翻译质量评估方法在不同文本长度下会产生偏差：对短文本过度惩罚，对长文本惩罚不足，与专家直觉不一致。

Method: 构建了双参数对数模型 E(x) = a * ln(1 + b * x)，基于韦伯-费希纳定律和认知负荷理论，通过一维根查找步骤从两个容忍点进行校准。

Result: 实证数据显示可接受错误数量随样本大小呈对数增长而非线性增长，该模型提高了跨不同文本长度的评估公平性和评分者间信度。

Conclusion: 该非线性评分模型提供了更符合人类感知的翻译质量评估范式，为AI驱动的文档级评估提供了更强基础，可集成到现有评估工作流中。

Abstract: Analytic Translation Quality Evaluation (TQE), based on Multidimensional Quality Metrics (MQM), traditionally uses a linear error-to-penalty scale calibrated to a reference sample of 1000-2000 words. However, linear extrapolation biases judgment on samples of different sizes, over-penalizing short samples and under-penalizing long ones, producing misalignment with expert intuition.
  Building on the Multi-Range framework, this paper presents a calibrated, non-linear scoring model that better reflects how human content consumers perceive translation quality across samples of varying length. Empirical data from three large-scale enterprise environments shows that acceptable error counts grow logarithmically, not linearly, with sample size.
  Psychophysical and cognitive evidence, including the Weber-Fechner law and Cognitive Load Theory, supports this premise by explaining why the perceptual impact of additional errors diminishes while the cognitive burden grows with scale. We propose a two-parameter model
  E(x) = a * ln(1 + b * x), a, b > 0,
  anchored to a reference tolerance and calibrated from two tolerance points using a one-dimensional root-finding step. The model yields an explicit interval within which the linear approximation stays within +/-20 percent relative error and integrates into existing evaluation workflows with only a dynamic tolerance function added.
  The approach improves interpretability, fairness, and inter-rater reliability across both human and AI-generated translations. By operationalizing a perceptually valid scoring paradigm, it advances translation quality evaluation toward more accurate and scalable assessment. The model also provides a stronger basis for AI-based document-level evaluation aligned with human judgment. Implementation considerations for CAT/LQA systems and implications for human and AI-generated text evaluation are discussed.

</details>


### [79] [Aspect-Level Obfuscated Sentiment in Thai Financial Disclosures and Its Impact on Abnormal Returns](https://arxiv.org/abs/2511.13481)
*Attapol T. Rutherford,Sirisak Chueykamhang,Thachaparn Bunditlurdruk,Nanthicha Angsuwichitkul*

Main category: cs.CL

TL;DR: 使用基于方面的情感分析(ABSA)解码泰国财务年报中的模糊情感，开发标注指南并标注100多份报告，通过事件研究验证对股价的实际影响。


<details>
  <summary>Details</summary>
Motivation: 财务文件中常使用模糊语言来呈现积极或中性展望，即使实际状况不佳，理解这些情感对洞察市场行为至关重要。

Method: 开发模糊情感标注指南，标注100多份泰国财务年报，使用各种文本分类模型进行基准测试，并进行事件研究评估对股价的影响。

Result: 在情感分类任务中表现出色，市场反应受报告中特定方面选择性地影响。

Conclusion: 财务文本情感分析具有复杂性，解决模糊语言问题对准确评估市场情绪至关重要。

Abstract: Understanding sentiment in financial documents is crucial for gaining insights into market behavior. These reports often contain obfuscated language designed to present a positive or neutral outlook, even when underlying conditions may be less favorable. This paper presents a novel approach using Aspect-Based Sentiment Analysis (ABSA) to decode obfuscated sentiment in Thai financial annual reports. We develop specific guidelines for annotating obfuscated sentiment in these texts and annotate more than one hundred financial reports. We then benchmark various text classification models on this annotated dataset, demonstrating strong performance in sentiment classification. Additionally, we conduct an event study to evaluate the real-world implications of our sentiment analysis on stock prices. Our results suggest that market reactions are selectively influenced by specific aspects within the reports. Our findings underscore the complexity of sentiment analysis in financial texts and highlight the importance of addressing obfuscated language to accurately assess market sentiment.

</details>


### [80] [Applying Large Language Models to Characterize Public Narratives](https://arxiv.org/abs/2511.13505)
*Elinor Poole-Dayan,Daniel T Kessler,Hannah Chiou,Margaret Hughes,Emily S Lin,Marshall Ganz,Deb Roy*

Main category: cs.CL

TL;DR: 提出基于大语言模型的公共叙事自动标注框架，在8个叙事和14个代码上达到平均F1分数0.80，接近人类专家水平，并扩展到22个故事和政治演讲分析。


<details>
  <summary>Details</summary>
Motivation: 公共叙事是领导力发展和公民动员的关键工具，但系统分析面临主观解释和专家标注成本高的挑战。

Method: 开发了与领域专家共同制定的代码本，利用大语言模型自动化公共叙事的定性标注，并与专家标注进行性能比较。

Result: 大语言模型在8个叙事和14个代码上平均F1分数达到0.80，接近人类专家水平，并成功扩展到更大数据集和政治演讲分析。

Conclusion: 证明了LLM辅助标注在可扩展叙事分析中的潜力，为计算公民叙事研究指明了关键局限性和未来方向。

Abstract: Public Narratives (PNs) are key tools for leadership development and civic mobilization, yet their systematic analysis remains challenging due to their subjective interpretation and the high cost of expert annotation. In this work, we propose a novel computational framework that leverages large language models (LLMs) to automate the qualitative annotation of public narratives. Using a codebook we co-developed with subject-matter experts, we evaluate LLM performance against that of expert annotators. Our work reveals that LLMs can achieve near-human-expert performance, achieving an average F1 score of 0.80 across 8 narratives and 14 codes. We then extend our analysis to empirically explore how PN framework elements manifest across a larger dataset of 22 stories. Lastly, we extrapolate our analysis to a set of political speeches, establishing a novel lens in which to analyze political rhetoric in civic spaces. This study demonstrates the potential of LLM-assisted annotation for scalable narrative analysis and highlights key limitations and directions for future research in computational civic storytelling.

</details>


### [81] [Toward Conversational Hungarian Speech Recognition: Introducing the BEA-Large and BEA-Dialogue Datasets](https://arxiv.org/abs/2511.13529)
*Máté Gedeon,Piroska Zsófia Barta,Péter Mihajlik,Tekla Etelka Gráczi,Anna Kohári,Katalin Mády*

Main category: cs.CL

TL;DR: 为填补匈牙利语缺乏自发性和对话性语音语料库的空白，研究者构建了两个新数据集BEA-Large和BEA-Dialogue，并建立了可复现的ASR基线模型，展示了对话ASR的挑战性。


<details>
  <summary>Details</summary>
Motivation: 高资源语言的ASR发展得益于大量数据集，而匈牙利语等语言由于缺乏自发性和对话性语料库而代表性不足，需要填补这一空白。

Method: 从匈牙利语音语料库BEA的未处理部分构建了两个新数据集：BEA-Large（255小时自发语音）和BEA-Dialogue（85小时自然对话），并使用公开可用的ASR模型建立可复现基线。

Result: 微调的Fast Conformer模型在自发语音上词错误率为14.18%，在重复语音上为4.8%；说话人分离实验的错误率在13.05%到18.26%之间。

Conclusion: 结果突显了对话ASR的持续挑战性，特别是由于不流畅、重叠和非正式语音模式。通过发布这些数据集和基线，旨在推进匈牙利语音技术发展，并为其他语言开发自发性和对话性基准提供方法论框架。

Abstract: The advancement of automatic speech recognition (ASR) has been largely enhanced by extensive datasets in high-resource languages, while languages such as Hungarian remain underrepresented due to limited spontaneous and conversational corpora. To address this gap, we introduce two new datasets -- BEA-Large and BEA-Dialogue -- constructed from the previously unprocessed portions of the Hungarian speech corpus named BEA. BEA-Large extends BEA-Base with 255 hours of spontaneous speech from 433 speakers, enriched with detailed segment-level metadata. BEA-Dialogue, comprising 85 hours of spontaneous conversations, is a Hungarian speech corpus featuring natural dialogues partitioned into speaker-independent subsets, supporting research in conversational ASR and speaker diarization. We establish reproducible baselines on these datasets using publicly available ASR models, with the fine-tuned Fast Conformer model achieving word error rates as low as 14.18\% on spontaneous and 4.8\% on repeated speech. Diarization experiments yield diarization error rates between 13.05\% and 18.26\%, providing reference points for future improvements. The results highlight the persistent difficulty of conversational ASR, particularly due to disfluencies, overlaps, and informal speech patterns. By releasing these datasets and baselines, we aim to advance Hungarian speech technology and offer a methodological framework for developing spontaneous and conversational benchmarks in other languages.

</details>


### [82] [Beyond SELECT: A Comprehensive Taxonomy-Guided Benchmark for Real-World Text-to-SQL Translation](https://arxiv.org/abs/2511.13590)
*Hao Wang,Yuanfeng Song,Xiaoming Yin,Xing Chen*

Main category: cs.CL

TL;DR: 提出了基于核心意图、语句类型、语法结构和关键操作的文本到SQL分类法，并创建了SQL-Synth数据集，该数据集比现有基准具有更好的多样性和覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到SQL数据集覆盖范围有限，无法捕捉真实应用的多样性，需要更全面的分类法和数据集。

Method: 开发了文本到SQL分类法，并基于该分类法使用大语言模型构建了SQL-Synth数据集合成流水线。

Result: SQL-Synth数据集在多样性和覆盖率上优于现有基准，现有LLMs在该数据集上表现有限，但微调可以显著提升性能。

Conclusion: 提出的分类法具有重要影响，能够全面分析数据集和LLMs性能，并指导LLMs训练数据的构建。

Abstract: Text-to-SQL datasets are essential for training and evaluating text-to-SQL models, but existing datasets often suffer from limited coverage and fail to capture the diversity of real-world applications. To address this, we propose a novel taxonomy for text-to-SQL classification based on dimensions including core intents, statement types, syntax structures, and key actions. Using this taxonomy, we evaluate widely used public text-to-SQL datasets (e.g., Spider and Bird) and reveal limitations in their coverage and diversity. We then introduce a taxonomy-guided dataset synthesis pipeline, yielding a new dataset named SQL-Synth. This approach combines the taxonomy with Large Language Models (LLMs) to ensure the dataset reflects the breadth and complexity of real-world text-to-SQL applications. Extensive analysis and experimental results validate the effectiveness of our taxonomy, as SQL-Synth exhibits greater diversity and coverage compared to existing benchmarks. Moreover, we uncover that existing LLMs typically fall short in adequately capturing the full range of scenarios, resulting in limited performance on SQL-Synth. However, fine-tuning can substantially improve their performance in these scenarios. The proposed taxonomy has significant potential impact, as it not only enables comprehensive analysis of datasets and the performance of different LLMs, but also guides the construction of training data for LLMs.

</details>


### [83] [Omni Memory System for Personalized, Long Horizon, Self-Evolving Agents](https://arxiv.org/abs/2511.13593)
*Piaohong Wang,Motong Tian,Jiaxian Li,Yuan Liang,Yuqing Wang,Qianben Chen,Tiannan Wang,Zhicong Lu,Jiawei Ma,Yuchen Eleanor Jiang,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: 提出了O-Mem记忆框架，通过主动用户画像动态提取和更新用户特征，在个性化AI助手方面实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理在复杂环境中维持长期交互存在挑战，主要受限于上下文一致性和动态个性化能力不足。现有记忆系统依赖语义分组检索，可能忽略语义无关但关键的用户信息并引入检索噪声。

Method: 基于主动用户画像的记忆框架，动态提取和更新用户特征和事件记录，支持人物属性和主题相关上下文的分层检索。

Result: 在LoCoMo基准测试中达到51.76%，比之前最优方法LangMem提升近3%；在PERSONAMEM中达到62.99%，比A-Mem提升3.5%。同时提升了令牌和交互响应时间效率。

Conclusion: 为开发高效且类人的个性化AI助手开辟了有前景的方向。

Abstract: Recent advancements in LLM-powered agents have demonstrated significant potential in generating human-like responses; however, they continue to face challenges in maintaining long-term interactions within complex environments, primarily due to limitations in contextual consistency and dynamic personalization. Existing memory systems often depend on semantic grouping prior to retrieval, which can overlook semantically irrelevant yet critical user information and introduce retrieval noise. In this report, we propose the initial design of O-Mem, a novel memory framework based on active user profiling that dynamically extracts and updates user characteristics and event records from their proactive interactions with agents. O-Mem supports hierarchical retrieval of persona attributes and topic-related context, enabling more adaptive and coherent personalized responses. O-Mem achieves 51.76% on the public LoCoMo benchmark, a nearly 3% improvement upon LangMem,the previous state-of-the-art, and it achieves 62.99% on PERSONAMEM, a 3.5% improvement upon A-Mem,the previous state-of-the-art. O-Mem also boosts token and interaction response time efficiency compared to previous memory frameworks. Our work opens up promising directions for developing efficient and human-like personalized AI assistants in the future.

</details>


### [84] [Why is "Chicago" Predictive of Deceptive Reviews? Using LLMs to Discover Language Phenomena from Lexical Cues](https://arxiv.org/abs/2511.13658)
*Jiaming Qu,Mengtian Guo,Yue Wang*

Main category: cs.CL

TL;DR: 使用大型语言模型将机器学习检测到的欺骗性评论特征转化为人类可理解的语言现象，帮助人们在没有检测分类器的情况下评估在线评论的可信度。


<details>
  <summary>Details</summary>
Motivation: 欺骗性评论误导消费者、损害商家利益并破坏在线市场信任。虽然机器学习分类器能有效识别欺骗性评论，但其学到的特征对人类来说难以理解。

Method: 利用大型语言模型将机器学习学到的词汇线索转化为人类可理解的语言现象，这些现象基于实证数据，具有跨领域泛化能力。

Result: 通过这种方法获得的语言现象比LLM先验知识或上下文学习获得的现象更具预测性，且能泛化到相似领域。

Conclusion: 这些语言现象有助于人们在缺乏欺骗检测分类器的环境中批判性评估在线评论的可信度。

Abstract: Deceptive reviews mislead consumers, harm businesses, and undermine trust in online marketplaces. Machine learning classifiers can learn from large amounts of training examples to effectively distinguish deceptive reviews from genuine ones. However, the distinguishing features learned by these classifiers are often subtle, fragmented, and difficult for humans to interpret. In this work, we explore using large language models (LLMs) to translate machine-learned lexical cues into human-understandable language phenomena that can differentiate deceptive reviews from genuine ones. We show that language phenomena obtained in this manner are empirically grounded in data, generalizable across similar domains, and more predictive than phenomena either in LLMs' prior knowledge or obtained through in-context learning. These language phenomena have the potential to aid people in critically assessing the credibility of online reviews in environments where deception detection classifiers are unavailable.

</details>


### [85] [Crossing Borders: A Multimodal Challenge for Indian Poetry Translation and Image Generation](https://arxiv.org/abs/2511.13689)
*Sofia Jamil,Kotla Sai Charan,Sriparna Saha,Koustava Goswami,Joseph K J*

Main category: cs.CL

TL;DR: 提出了TAI框架，结合大语言模型和潜在扩散模型，通过翻译和图像生成来增强印度诗歌的全球可访问性，支持联合国可持续发展目标。


<details>
  <summary>Details</summary>
Motivation: 印度诗歌具有丰富的文化遗产，但由于语言复杂性和文化背景，非母语读者难以理解。现有研究忽视了印度语言诗歌，需要提升其可访问性。

Method: 使用TAI框架，包括：(1)基于几率比偏好对齐算法的翻译模块，准确翻译形态丰富的诗歌；(2)基于语义图的图像生成模块，捕捉隐喻和语义关系生成视觉表示。

Result: 综合实验评估显示TAI Diffusion在诗歌图像生成任务中优于强基线模型，并发布了包含1,570首21种低资源印度语言诗歌的MorphoVerse数据集。

Conclusion: 该工作填补了诗歌翻译和视觉理解的空白，旨在扩大印度诗歌的可访问性并丰富读者体验。

Abstract: Indian poetry, known for its linguistic complexity and deep cultural resonance, has a rich and varied heritage spanning thousands of years. However, its layered meanings, cultural allusions, and sophisticated grammatical constructions often pose challenges for comprehension, especially for non-native speakers or readers unfamiliar with its context and language. Despite its cultural significance, existing works on poetry have largely overlooked Indian language poems. In this paper, we propose the Translation and Image Generation (TAI) framework, leveraging Large Language Models (LLMs) and Latent Diffusion Models through appropriate prompt tuning. Our framework supports the United Nations Sustainable Development Goals of Quality Education (SDG 4) and Reduced Inequalities (SDG 10) by enhancing the accessibility of culturally rich Indian-language poetry to a global audience. It includes (1) a translation module that uses an Odds Ratio Preference Alignment Algorithm to accurately translate morphologically rich poetry into English, and (2) an image generation module that employs a semantic graph to capture tokens, dependencies, and semantic relationships between metaphors and their meanings, to create visually meaningful representations of Indian poems. Our comprehensive experimental evaluation, including both human and quantitative assessments, demonstrates the superiority of TAI Diffusion in poem image generation tasks, outperforming strong baselines. To further address the scarcity of resources for Indian-language poetry, we introduce the Morphologically Rich Indian Language Poems MorphoVerse Dataset, comprising 1,570 poems across 21 low-resource Indian languages. By addressing the gap in poetry translation and visual comprehension, this work aims to broaden accessibility and enrich the reader's experience.

</details>


### [86] [Generalist Foundation Models Are Not Clinical Enough for Hospital Operations](https://arxiv.org/abs/2511.13703)
*Lavender Y. Jiang,Angelica Chen,Xu Han,Xujin Chris Liu,Radhika Dua,Kevin Eaton,Frederick Wolff,Robert Steele,Jeff Zhang,Anton Alyakin,Qingkai Pan,Yanbing Chen,Karl L. Sangwon,Daniel A. Alber,Jaden Stryker,Jin Vivian Lee,Yindalon Aphinyanaphongs,Kyunghyun Cho,Eric Karl Oermann*

Main category: cs.CL

TL;DR: Lang1模型系列通过在医疗EHR数据和通用互联网文本上预训练，在医疗运营决策任务上表现优于通用大模型，特别是在微调后能在多个关键医疗任务上超越大70倍的通用模型。


<details>
  <summary>Details</summary>
Motivation: 通用基础模型在医疗知识和对话基准上表现良好，但缺乏医疗运营决策所需的专业知识，需要专门针对医疗领域训练的模型来支持医院运营决策。

Method: 开发Lang1模型系列(100M-7B参数)，在NYU Langone Health的EHR数据(80B token)和互联网文本(627B token)混合语料上预训练，并构建ReMedE基准评估五个关键医疗任务。

Result: 微调后的Lang1-1B在多个医疗任务上优于大70倍的通用模型，AUROC提升3.64%-6.75%；在零样本设置下，通用和专用模型在4/5任务上表现不佳(36.6%-71.7% AUROC)。

Conclusion: 医疗运营的预测能力需要明确的监督微调，而领域内预训练使微调更有效；有效的医疗AI系统需要领域内预训练、监督微调和真实世界评估的结合。

Abstract: Hospitals and healthcare systems rely on operational decisions that determine patient flow, cost, and quality of care. Despite strong performance on medical knowledge and conversational benchmarks, foundation models trained on general text may lack the specialized knowledge required for these operational decisions. We introduce Lang1, a family of models (100M-7B parameters) pretrained on a specialized corpus blending 80B clinical tokens from NYU Langone Health's EHRs and 627B tokens from the internet. To rigorously evaluate Lang1 in real-world settings, we developed the REalistic Medical Evaluation (ReMedE), a benchmark derived from 668,331 EHR notes that evaluates five critical tasks: 30-day readmission prediction, 30-day mortality prediction, length of stay, comorbidity coding, and predicting insurance claims denial. In zero-shot settings, both general-purpose and specialized models underperform on four of five tasks (36.6%-71.7% AUROC), with mortality prediction being an exception. After finetuning, Lang1-1B outperforms finetuned generalist models up to 70x larger and zero-shot models up to 671x larger, improving AUROC by 3.64%-6.75% and 1.66%-23.66% respectively. We also observed cross-task scaling with joint finetuning on multiple tasks leading to improvement on other tasks. Lang1-1B effectively transfers to out-of-distribution settings, including other clinical tasks and an external health system. Our findings suggest that predictive capabilities for hospital operations require explicit supervised finetuning, and that this finetuning process is made more efficient by in-domain pretraining on EHR. Our findings support the emerging view that specialized LLMs can compete with generalist models in specialized tasks, and show that effective healthcare systems AI requires the combination of in-domain pretraining, supervised finetuning, and real-world evaluation beyond proxy benchmarks.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [87] [Characterizing and Understanding Energy Footprint and Efficiency of Small Language Model on Edges](https://arxiv.org/abs/2511.11624)
*Md Romyull Islam,Bobin Deng,Nobel Dhar,Tu N. Nguyen,Selena He,Yong Shi,Kun Suo*

Main category: cs.DC

TL;DR: 评估了5个小型语言模型在边缘设备上的能效表现，发现Jetson Orin Nano GPU配置能效最高，Llama 3.2在准确性和能效间取得最佳平衡，TinyLlama适合低功耗环境但准确性较低。


<details>
  <summary>Details</summary>
Motivation: 边缘设备部署小型语言模型具有延迟低、不依赖网络连接等优势，但受限于计算资源和能源预算，需要评估其能效表现。

Method: 在Raspberry Pi 5、Jetson Nano和Jetson Orin Nano（CPU和GPU配置）上评估了Llama 3.2、Phi-3 Mini、TinyLlama和Gemma 2等5个代表性SLM的功率效率。

Result: Jetson Orin Nano GPU加速实现最高能效比，显著优于CPU配置；Llama 3.2提供最佳准确性与能效平衡；TinyLlama适合低功耗环境但准确性降低；Phi-3 Mini能耗最高。

Conclusion: GPU加速、内存带宽和模型架构是优化推理能效的关键因素，为AI、智能系统和移动平台在能源受限环境中平衡准确性、延迟和能效提供实践指导。

Abstract: Cloud-based large language models (LLMs) and their variants have significantly influenced real-world applications. Deploying smaller models (i.e., small language models (SLMs)) on edge devices offers additional advantages, such as reduced latency and independence from network connectivity. However, edge devices' limited computing resources and constrained energy budgets challenge efficient deployment. This study evaluates the power efficiency of five representative SLMs - Llama 3.2, Phi-3 Mini, TinyLlama, and Gemma 2 on Raspberry Pi 5, Jetson Nano, and Jetson Orin Nano (CPU and GPU configurations). Results show that Jetson Orin Nano with GPU acceleration achieves the highest energy-to-performance ratio, significantly outperforming CPU-based setups. Llama 3.2 provides the best balance of accuracy and power efficiency, while TinyLlama is well-suited for low-power environments at the cost of reduced accuracy. In contrast, Phi-3 Mini consumes the most energy despite its high accuracy. In addition, GPU acceleration, memory bandwidth, and model architecture are key in optimizing inference energy efficiency. Our empirical analysis offers practical insights for AI, smart systems, and mobile ad-hoc platforms to leverage tradeoffs from accuracy, inference latency, and power efficiency in energy-constrained environments.

</details>


### [88] [A Structure-Agnostic Co-Tuning Framework for LLMs and SLMs in Cloud-Edge Systems](https://arxiv.org/abs/2511.11678)
*Yuze Liu,Yunhan Wang,Tiehua Zhang,Zhishu Shen,Cheng Peng,Libing Wu,Feng Xia,Jiong Jin*

Main category: cs.DC

TL;DR: Co-PLMs是一个用于大语言模型和小语言模型协同训练的新框架，通过结构无关的相互学习实现异构模型间的知识交换，使用蒸馏代理模型作为桥梁，在保护设备特定领域知识的同时提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 智能应用激增使得带宽受限的云服务器难以实时处理大量LLM工作负载而不损害用户数据隐私，需要构建云边协同系统整合服务器LLM和移动边缘设备上的SLM。

Method: 提出Co-PLMs协同训练框架，采用结构无关的相互学习方法，使用蒸馏代理模型作为异构服务器LLM和边缘设备SLM之间的桥梁进行协同训练。

Result: 实验结果显示Co-PLMs优于现有最先进方法，在Rouge-L和EM指标上分别平均提升5.38%和4.88%。

Conclusion: Co-PLMs框架成功解决了异构语言模型协同训练中的结构异质性问题，实现了有效的知识交换和性能提升。

Abstract: The surge in intelligent applications driven by large language models (LLMs) has made it increasingly difficult for bandwidth-limited cloud servers to process extensive LLM workloads in real time without compromising user data privacy. To solve these problems, recent research has focused on constructing cloud-edge consortia that integrate server-based LLM with small language models (SLMs) on mobile edge devices. Furthermore, designing collaborative training mechanisms within such consortia to enhance inference performance has emerged as a promising research direction. However, the cross-domain deployment of SLMs, coupled with structural heterogeneity in SLMs architectures, poses significant challenges to enhancing model performance. To this end, we propose Co-PLMs, a novel co-tuning framework for collaborative training of large and small language models, which integrates the process of structure-agnostic mutual learning to realize knowledge exchange between the heterogeneous language models. This framework employs distilled proxy models (DPMs) as bridges to enable collaborative training between the heterogeneous server-based LLM and on-device SLMs, while preserving the domain-specific insights of each device. The experimental results show that Co-PLMs outperform state-of-the-art methods, achieving average increases of 5.38% in Rouge-L and 4.88% in EM.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [89] [LLM-Generated Negative News Headlines Dataset: Creation and Benchmarking Against Real Journalism](https://arxiv.org/abs/2511.11591)
*Olusola Babalola,Bolanle Ojokoh,Olutayo Boyinbode*

Main category: cs.AI

TL;DR: 本研究探讨了使用LLM生成的合成新闻标题作为真实世界数据替代品的可行性，特别是在负面情感文本分析领域，以解决数据获取和隐私问题。


<details>
  <summary>Details</summary>
Motivation: 克服NLP任务中真实数据获取的挑战和隐私担忧，探索LLM生成数据集在情感分析中的潜力。

Method: 使用定制提示创建负面新闻标题语料库，通过专家评审和嵌入空间分析验证合成数据，并与真实新闻标题进行多维度基准测试。

Result: 生成的标题在内容、语调、长度和风格上与真实标题高度匹配，仅在POS分析中的专有名词得分存在显著差异。

Conclusion: LLM生成的合成数据集可以有效替代真实数据用于NLP任务，特别是在负面情感分析领域。

Abstract: This research examines the potential of datasets generated by Large Language Models (LLMs) to support Natural Language Processing (NLP) tasks, aiming to overcome challenges related to data acquisition and privacy concerns associated with real-world data. Focusing on negative valence text, a critical component of sentiment analysis, we explore the use of LLM-generated synthetic news headlines as an alternative to real-world data. A specialized corpus of negative news headlines was created using tailored prompts to capture diverse negative sentiments across various societal domains. The synthetic headlines were validated by expert review and further analyzed in embedding space to assess their alignment with real-world negative news in terms of content, tone, length, and style. Key metrics such as correlation with real headlines, perplexity, coherence, and realism were evaluated. The synthetic dataset was benchmarked against two sets of real news headlines using evaluations including the Comparative Perplexity Test, Comparative Readability Test, Comparative POS Profiling, BERTScore, and Comparative Semantic Similarity. Results show the generated headlines match real headlines with the only marked divergence being in the proper noun score of the POS profile test.

</details>


### [90] [CLINB: A Climate Intelligence Benchmark for Foundational Models](https://arxiv.org/abs/2511.11597)
*Michelle Chen Huebscher,Katharine Mach,Aleksandar Stanić,Markus Leippold,Ben Gaiarin,Zeke Hausfather,Elisa Rawat,Erich Fischer,Massimiliano Ciaramita,Joeri Rogelj,Christian Buck,Lierni Sestorain Saralegui,Reto Knutti*

Main category: cs.AI

TL;DR: CLINB基准测试评估LLMs在气候变化领域的专业知识和证据支持能力，发现前沿模型具有博士级别的知识综合能力但存在严重的证据幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型处理复杂专业知识的能力，特别是在气候变化这一关键领域，需要建立可靠的基准来测试模型的知识质量和证据支持。

Method: 引入CLINB基准，基于真实用户问题和气候科学家制定的评估标准，采用基于模型的评估流程来测试多模态开放性问题回答。

Result: 前沿模型展现出卓越的知识综合能力，达到博士理解水平，甚至优于专家辅助的混合答案，但在证据基础方面存在严重问题，引用和图像幻觉率很高。

Conclusion: 弥合知识综合与可验证归因之间的差距对于AI在科学工作流中的部署至关重要，需要像CLINB这样的可靠基准来构建可信AI系统。

Abstract: Evaluating how Large Language Models (LLMs) handle complex, specialized knowledge remains a critical challenge. We address this through the lens of climate change by introducing CLINB, a benchmark that assesses models on open-ended, grounded, multimodal question answering tasks with clear requirements for knowledge quality and evidential support. CLINB relies on a dataset of real users' questions and evaluation rubrics curated by leading climate scientists. We implement and validate a model-based evaluation process and evaluate several frontier models. Our findings reveal a critical dichotomy. Frontier models demonstrate remarkable knowledge synthesis capabilities, often exhibiting PhD-level understanding and presentation quality. They outperform "hybrid" answers curated by domain experts assisted by weaker models. However, this performance is countered by failures in grounding. The quality of evidence varies, with substantial hallucination rates for references and images. We argue that bridging this gap between knowledge synthesis and verifiable attribution is essential for the deployment of AI in scientific workflows and that reliable, interpretable benchmarks like CLINB are needed to progress towards building trustworthy AI systems.

</details>


### [91] [SynBullying: A Multi LLM Synthetic Conversational Dataset for Cyberbullying Detectio](https://arxiv.org/abs/2511.11599)
*Arefeh Kazemi,Hamza Qadeer,Joachim Wagner,Hossein Hosseini,Sri Balaaji Natarajan Kalaivendan,Brian Davis*

Main category: cs.AI

TL;DR: SynBullying是一个用于研究和检测网络欺凌的合成多LLM对话数据集，通过大语言模型模拟真实的欺凌互动，提供可扩展且符合伦理的数据收集方案。


<details>
  <summary>Details</summary>
Motivation: 传统网络欺凌数据收集面临可扩展性和伦理问题，需要一种既能模拟真实对话结构又能安全获取标注数据的方法。

Method: 利用大语言模型生成多轮对话，提供对话结构、上下文感知标注和细粒度标签，涵盖多种网络欺凌类别。

Result: 数据集在对话结构、词汇模式、情感/毒性、角色动态、伤害强度和欺凌类型分布等五个维度进行评估，并测试了其作为训练数据和增强数据的性能。

Conclusion: SynBullying为网络欺凌研究提供了一个可扩展、伦理安全的合成数据集替代方案，支持详细的对话分析和分类任务。

Abstract: We introduce SynBullying, a synthetic multi-LLM conversational dataset for studying and detecting cyberbullying (CB). SynBullying provides a scalable and ethically safe alternative to human data collection by leveraging large language models (LLMs) to simulate realistic bullying interactions. The dataset offers (i) conversational structure, capturing multi-turn exchanges rather than isolated posts; (ii) context-aware annotations, where harmfulness is assessed within the conversational flow considering context, intent, and discourse dynamics; and (iii) fine-grained labeling, covering various CB categories for detailed linguistic and behavioral analysis. We evaluate SynBullying across five dimensions, including conversational structure, lexical patterns, sentiment/toxicity, role dynamics, harm intensity, and CB-type distribution. We further examine its utility by testing its performance as standalone training data and as an augmentation source for CB classification.

</details>


### [92] [Do LLMs Really Struggle at NL-FOL Translation? Revealing their Strengths via a Novel Benchmarking Strategy](https://arxiv.org/abs/2511.11816)
*Andrea Brunello,Luca Geatti,Michele Mignani,Angelo Montanari,Nicola Saccomanno*

Main category: cs.AI

TL;DR: 本文提出了一种新的评估协议来准确测试LLMs在自然语言到一阶逻辑翻译中的真实能力，发现对话导向的LLMs具有强大的NL-FOL翻译技能和真正的逻辑理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有评估NL-FOL翻译的数据集和协议存在局限性，可能导致对LLMs实际能力的误判，需要更准确的评估方法来区分真正的语义逻辑理解与表面模式识别。

Method: 提出新颖的评估协议，专门设计用于区分真正的语义级逻辑理解与表面模式识别、记忆和数据集污染，并应用该协议测试不同LLMs的性能。

Result: 使用新评估方法发现，最先进的对话导向LLMs展现出强大的NL-FOL翻译技能和真正的句子级逻辑理解能力，而嵌入中心模型表现明显较差。

Conclusion: 对话导向的LLMs在NL-FOL翻译方面具有显著能力，但需要专门的评估协议来准确衡量其真实的逻辑理解水平。

Abstract: Due to its expressiveness and unambiguous nature, First-Order Logic (FOL) is a powerful formalism for representing concepts expressed in natural language (NL). This is useful, e.g., for specifying and verifying desired system properties. While translating FOL into human-readable English is relatively straightforward, the inverse problem, converting NL to FOL (NL-FOL translation), has remained a longstanding challenge, for both humans and machines. Although the emergence of Large Language Models (LLMs) promised a breakthrough, recent literature provides contrasting results on their ability to perform NL-FOL translation. In this work, we provide a threefold contribution. First, we critically examine existing datasets and protocols for evaluating NL-FOL translation performance, revealing key limitations that may cause a misrepresentation of LLMs' actual capabilities. Second, to overcome these shortcomings, we propose a novel evaluation protocol explicitly designed to distinguish genuine semantic-level logical understanding from superficial pattern recognition, memorization, and dataset contamination. Third, using this new approach, we show that state-of-the-art, dialogue-oriented LLMs demonstrate strong NL-FOL translation skills and a genuine grasp of sentence-level logic, whereas embedding-centric models perform markedly worse.

</details>


### [93] [Forgetting-MarI: LLM Unlearning via Marginal Information Regularization](https://arxiv.org/abs/2511.11914)
*Shizhou Xu,Yuan Ni,Stefan Broecker,Thomas Strohmer*

Main category: cs.AI

TL;DR: Forgetting-MarI是一个LLM遗忘框架，通过惩罚边际信息来选择性移除要遗忘数据的参数知识，同时保留要保留数据的信息，提供可证明的不可检测性。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型在不断扩大数据集上训练，从训练模型中移除特定数据影响的能力对于隐私保护和法规遵从变得至关重要。遗忘方法可以在不从头重新训练的情况下选择性移除参数知识，这对于资源密集型模型如LLM尤为重要。

Method: 引入Forgetting-MarI框架，通过惩罚边际信息来仅移除要遗忘数据贡献的额外信息，同时保留要保留数据支持的信息。该方法提供了对未学习数据集在训练模型中残余影响的明确上界。

Result: 广泛实验证实该方法优于当前最先进的遗忘方法，在多样化基准测试中实现了可靠的遗忘和更好的通用模型性能保留。

Conclusion: 这一进展代表了使AI系统更可控、更符合隐私和版权法规且不损害其有效性的重要一步。

Abstract: As AI models are trained on ever-expanding datasets, the ability to remove the influence of specific data from trained models has become essential for privacy protection and regulatory compliance. Unlearning addresses this challenge by selectively removing parametric knowledge from the trained models without retraining from scratch, which is critical for resource-intensive models such as Large Language Models (LLMs). Existing unlearning methods often degrade model performance by removing more information than necessary when attempting to ''forget'' specific data. We introduce Forgetting-MarI, an LLM unlearning framework that provably removes only the additional (marginal) information contributed by the data to be unlearned, while preserving the information supported by the data to be retained. By penalizing marginal information, our method yields an explicit upper bound on the unlearn dataset's residual influence in the trained models, providing provable undetectability. Extensive experiments confirm that our approach outperforms current state-of-the-art unlearning methods, delivering reliable forgetting and better preserved general model performance across diverse benchmarks. This advancement represents an important step toward making AI systems more controllable and compliant with privacy and copyright regulations without compromising their effectiveness.

</details>


### [94] [WebCoach: Self-Evolving Web Agents with Cross-Session Memory Guidance](https://arxiv.org/abs/2511.12997)
*Genglin Liu,Shijie Geng,Sha Li,Hejie Cui,Sarah Zhang,Xin Liu,Tianyi Liu*

Main category: cs.AI

TL;DR: WebCoach是一个模型无关的自进化框架，通过为网页浏览代理添加跨会话持久记忆，实现长期规划、反思和持续学习，无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 当前多模态LLM代理在网页导航中存在重复错误，缺乏跨会话学习能力，限制了长期鲁棒性和样本效率。

Method: WebCoach包含三个核心组件：WebCondenser标准化原始导航日志为摘要，External Memory Store组织完整轨迹为情景记忆，Coach基于相似性和时效性检索相关经验并通过运行时钩子注入任务特定建议。

Result: 在WebVoyager基准测试中，WebCoach显著提升了三种不同LLM骨干的浏览器代理性能。使用38B模型时，任务成功率从47%提升到61%，同时减少或维持平均步骤数。较小基础模型配合WebCoach可达到与使用GPT-4o的相同网页代理相当的性能。

Conclusion: WebCoach通过赋予网页代理长期记忆能力，实现了自我进化，在复杂浏览任务中提高了鲁棒性，且无需重新训练即可随时间改进。

Abstract: Multimodal LLM-powered agents have recently demonstrated impressive capabilities in web navigation, enabling agents to complete complex browsing tasks across diverse domains. However, current agents struggle with repetitive errors and lack the ability to learn from past experiences across sessions, limiting their long-term robustness and sample efficiency. We introduce WebCoach, a model-agnostic self-evolving framework that equips web browsing agents with persistent cross-session memory, enabling improved long-term planning, reflection, and continual learning without retraining. WebCoach consists of three key components: (1) a WebCondenser, which standardizes raw navigation logs into concise summaries; (2) an External Memory Store, which organizes complete trajectories as episodic experiences; and (3) a Coach, which retrieves relevant experiences based on similarity and recency, and decides whether to inject task-specific advice into the agent via runtime hooks. This design empowers web agents to access long-term memory beyond their native context window, improving robustness in complex browsing tasks. Moreover, WebCoach achieves self-evolution by continuously curating episodic memory from new navigation trajectories, enabling agents to improve over time without retraining. Evaluations on the WebVoyager benchmark demonstrate that WebCoach consistently improves the performance of browser-use agents across three different LLM backbones. With a 38B model, it increases task success rates from 47% to 61% while reducing or maintaining the average number of steps. Notably, smaller base models with WebCoach achieve performance comparable to the same web agent using GPT-4o.

</details>


### [95] [PragWorld: A Benchmark Evaluating LLMs' Local World Model under Minimal Linguistic Alterations and Conversational Dynamics](https://arxiv.org/abs/2511.13021)
*Sachin Vashistha,Aryan Bibhuti,Atharva Naik,Martin Tutek,Somak Aditya*

Main category: cs.AI

TL;DR: 该研究评估语言模型在对话中构建和维护内部世界模型的能力，通过应用七种最小语言变化来测试模型的鲁棒性，并提出基于层正则化的微调策略来抑制有害层的影响。


<details>
  <summary>Details</summary>
Motivation: 现实对话包含丰富的语用元素，需要构建局部世界模型来编码这些元素并跟踪其状态变化。然而，语言模型是否能够构建和维护健壮的隐式对话表示尚不清楚。

Method: 对流行数据集中的对话应用七种最小语言变化，构建两个包含是非问题的基准测试，评估各种开源和闭源语言模型，并提出双视角可解释性框架识别有用和有害的transformer层。

Result: 语言模型在语言变化下难以保持鲁棒准确性，特别是在跟踪实体等关键细节方面表现不佳。有害层通常由于编码虚假信号或依赖捷径而影响模型性能。

Conclusion: 语言模型在对话世界模型构建方面存在局限性，但通过识别和抑制有害层的影响，可以改善其性能。

Abstract: Real-world conversations are rich with pragmatic elements, such as entity mentions, references, and implicatures. Understanding such nuances is a requirement for successful natural communication, and often requires building a local world model which encodes such elements and captures the dynamics of their evolving states. However, it is not well-understood whether language models (LMs) construct or maintain a robust implicit representation of conversations. In this work, we evaluate the ability of LMs to encode and update their internal world model in dyadic conversations and test their malleability under linguistic alterations. To facilitate this, we apply seven minimal linguistic alterations to conversations sourced from popular datasets and construct two benchmarks comprising yes-no questions. We evaluate a wide range of open and closed source LMs and observe that they struggle to maintain robust accuracy. Our analysis unveils that LMs struggle to memorize crucial details, such as tracking entities under linguistic alterations to conversations. We then propose a dual-perspective interpretability framework which identifies transformer layers that are useful or harmful and highlights linguistic alterations most influenced by harmful layers, typically due to encoding spurious signals or relying on shortcuts. Inspired by these insights, we propose two layer-regularization based fine-tuning strategies that suppress the effect of the harmful layers.

</details>


### [96] [STEP: Success-Rate-Aware Trajectory-Efficient Policy Optimization](https://arxiv.org/abs/2511.13091)
*Yuhan Chen,Yuxuan Liu,Long Zhang,Pengzhi Gao,Jian Luan,Wei Liu*

Main category: cs.AI

TL;DR: STEP框架通过基于任务成功率的动态采样分配和步骤级优化，解决了多轮交互强化学习中轨迹级优化的低效问题，显著提高了样本效率和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决在线强化学习中多轮交互的挑战，特别是轨迹级优化方法的低效性、误导性学习信号和高样本收集成本问题。

Method: 提出STEP框架：1）基于平滑成功率记录进行自适应轨迹重采样；2）计算成功率加权优势值；3）将轨迹分解为步骤级样本；4）应用步骤级GRPO增强来优化低成功率任务的更新。

Result: 在OSWorld和AndroidWorld上的实验表明，STEP相比轨迹级GRPO显著提高了样本效率和训练稳定性，在相同采样预算下收敛更快且泛化能力更好。

Conclusion: STEP通过动态采样分配和步骤级优化有效解决了多轮强化学习中的效率问题，为在线强化学习提供了更高效的训练框架。

Abstract: Multi-turn interaction remains challenging for online reinforcement learning. A common solution is trajectory-level optimization, which treats each trajectory as a single training sample. However, this approach can be inefficient and yield misleading learning signals: it applies uniform sampling across tasks regardless of difficulty, penalizes correct intermediate actions in failed trajectories, and incurs high sample-collection costs. To address these issues, we propose STEP (Success-rate-aware Trajectory-Efficient Policy optimization), a framework that dynamically allocates sampling based on per-task success rates and performs step-level optimization. STEP maintains a smoothed success-rate record to guide adaptive trajectory resampling, allocating more effort to harder tasks. It then computes success-rate-weighted advantages and decomposes trajectories into step-level samples. Finally, it applies a step-level GRPO augmentation to refine updates for low-success tasks. Experiments on OSWorld and AndroidWorld show that STEP substantially improves sample efficiency and training stability over trajectory-level GRPO, converging faster and generalizing better under the same sampling budget.

</details>


### [97] [Dropouts in Confidence: Moral Uncertainty in Human-LLM Alignment](https://arxiv.org/abs/2511.13290)
*Jea Kwon,Luiz Felipe Vecchietti,Sungwon Park,Meeyoung Cha*

Main category: cs.AI

TL;DR: 该研究探讨了AI模型在道德困境中的不确定性，通过分析32个开源模型在电车问题中的反应，发现模型架构和训练方法对道德不确定性的影响大于道德维度本身。通过引入推理时的随机性，可以增加模型的总熵并改善与人类道德判断的对齐。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统越来越多地参与道德决策场景，理解机器在道德推理中的不确定性对于构建可靠的AI系统至关重要。现有研究表明大型语言模型在道德困境中往往过于自信，需要系统性地研究其不确定性特征。

Method: 在经典电车问题中分析32个开源模型的反应，涵盖9个道德维度。通过测量二元熵（总熵、条件熵和互信息的线性组合）来量化不确定性，并在推理时通过'dropout'引入随机性。

Result: 模型间置信度差异大于道德维度内的差异；引入随机性后总熵增加，主要源于互信息上升，而条件熵基本不变；该机制显著改善了人类-LLM道德对齐，互信息与对齐分数变化呈正相关。

Conclusion: 通过有意调节不确定性并降低LLM在复杂道德场景中的置信度，可以更好地对齐模型生成决策与人类偏好，这为构建更可靠的道德AI系统提供了重要方向。

Abstract: Humans display significant uncertainty when confronted with moral dilemmas, yet the extent of such uncertainty in machines and AI agents remains underexplored. Recent studies have confirmed the overly confident tendencies of machine-generated responses, particularly in large language models (LLMs). As these systems are increasingly embedded in ethical decision-making scenarios, it is important to understand their moral reasoning and the inherent uncertainties in building reliable AI systems. This work examines how uncertainty influences moral decisions in the classical trolley problem, analyzing responses from 32 open-source models and 9 distinct moral dimensions. We first find that variance in model confidence is greater across models than within moral dimensions, suggesting that moral uncertainty is predominantly shaped by model architecture and training method. To quantify uncertainty, we measure binary entropy as a linear combination of total entropy, conditional entropy, and mutual information. To examine its effects, we introduce stochasticity into models via "dropout" at inference time. Our findings show that our mechanism increases total entropy, mainly through a rise in mutual information, while conditional entropy remains largely unchanged. Moreover, this mechanism significantly improves human-LLM moral alignment, with correlations in mutual information and alignment score shifts. Our results highlight the potential to better align model-generated decisions and human preferences by deliberately modulating uncertainty and reducing LLMs' confidence in morally complex scenarios.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [98] [EduAgentQG: A Multi-Agent Workflow Framework for Personalized Question Generation](https://arxiv.org/abs/2511.11635)
*Rui Jia,Min Zhang,Fengrui Liu,Bo Jiang,Kun Kuang,Zhongxiang Dai*

Main category: cs.CY

TL;DR: 提出了EduAgentQG多智能体协作框架，通过五个专业智能体的迭代反馈循环生成高质量、多样化的个性化问题，在数学问题数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 手动设计问题耗时且难以满足多样化学习需求，现有自动问题生成方法质量不稳定、多样性有限且与教育目标对齐不足。

Method: 五个专业智能体协作框架：规划器生成结构化设计计划和问题方向，写作者基于计划生成候选问题，求解器和教育者进行多维度二元评分，检查器进行最终验证。

Result: 在两个数学问题数据集上的实验表明，EduAgentQG在问题多样性、目标一致性和整体质量方面优于现有单智能体和多智能体方法。

Conclusion: 多智能体协作和迭代反馈循环能够生成高质量、多样化且与教育目标一致的问题，有效支持自适应学习和个性化评估。

Abstract: High-quality personalized question banks are crucial for supporting adaptive learning and individualized assessment. Manually designing questions is time-consuming and often fails to meet diverse learning needs, making automated question generation a crucial approach to reduce teachers' workload and improve the scalability of educational resources. However, most existing question generation methods rely on single-agent or rule-based pipelines, which still produce questions with unstable quality, limited diversity, and insufficient alignment with educational goals. To address these challenges, we propose EduAgentQG, a multi-agent collaborative framework for generating high-quality and diverse personalized questions. The framework consists of five specialized agents and operates through an iterative feedback loop: the Planner generates structured design plans and multiple question directions to enhance diversity; the Writer produces candidate questions based on the plan and optimizes their quality and diversity using feedback from the Solver and Educator; the Solver and Educator perform binary scoring across multiple evaluation dimensions and feed the evaluation results back to the Writer; the Checker conducts final verification, including answer correctness and clarity, ensuring alignment with educational goals. Through this multi-agent collaboration and iterative feedback loop, EduAgentQG generates questions that are both high-quality and diverse, while maintaining consistency with educational objectives. Experiments on two mathematics question datasets demonstrate that EduAgentQG outperforms existing single-agent and multi-agent methods in terms of question diversity, goal consistency, and overall quality.

</details>


### [99] [Automatic generation of DRI Statements](https://arxiv.org/abs/2511.11655)
*Maurice Flechtner*

Main category: cs.CY

TL;DR: 提出自动化DRI声明生成方法，利用NLP和LLM技术大幅减少人工工作量，为社会科学研究提供可复制的AI集成模板。


<details>
  <summary>Details</summary>
Motivation: 传统DRI声明生成过程复杂耗时，限制了群体审议质量评估的广泛应用，需要自动化解决方案来降低实施门槛。

Method: 采用先进的自然语言处理和大语言模型技术，开发系统化的自动化DRI声明生成框架。

Result: 成功实现了DRI声明的自动化生成，显著减少了调查准备所需的人工努力，降低了全面审议过程评估的实施障碍。

Conclusion: 该研究为将生成式人工智能整合到社会科学研究方法中提供了可复制的模板，推动了审议过程评估的普及化。

Abstract: Assessing the quality of group deliberation is essential for improving our understanding of deliberative processes. The Deliberative Reason Index (DRI) offers a sophisticated metric for evaluating group reasoning, but its implementation has been constrained by the complex and time-consuming process of statement generation. This thesis introduces an innovative, automated approach to DRI statement generation that leverages advanced natural language processing (NLP) and large language models (LLMs) to substantially reduce the human effort involved in survey preparation. Key contributions are a systematic framework for automated DRI statement generation and a methodological innovation that significantly lowers the barrier to conducting comprehensive deliberative process assessments. In addition, the findings provide a replicable template for integrating generative artificial intelligence into social science research methodologies.

</details>


### [100] [Generative AI as a Linguistic Equalizer in Global Science](https://arxiv.org/abs/2511.11687)
*Dragan Filimonovic,Christian Rutzer,Jeffrey Macher,Rolf Weder*

Main category: cs.CY

TL;DR: 生成式AI（特别是ChatGPT发布后）正在帮助非英语国家作者的科学论文在语言风格上更接近美国作者的写作风格，减少了全球科学交流中的语言障碍。


<details>
  <summary>Details</summary>
Motivation: 英语在全球科学中的主导地位长期以来为非母语者设置了障碍，生成式AI可能为解决这一不平等问题提供技术方案。

Method: 分析2021-2024年间的565万篇科学论文，使用SciBERT文本嵌入模型测量非英语国家作者的AI辅助与非AI辅助论文与美国作者写作风格的语言相似性。

Result: ChatGPT发布后，AI辅助论文显示出显著且不断增长的语言风格趋同效应，特别是在与英语语言距离较远的国家的国内合作团队中效果最明显。

Conclusion: 生成式AI正在通过减少语言障碍来重塑全球科学交流，为非英语国家作者提供了语言平等的机会。

Abstract: For decades, the dominance of English has created a substantial barrier in global science, disadvantaging non-native speakers. The recent rise of generative AI (GenAI) offers a potential technological response to this long-standing inequity. We provide the first large-scale evidence testing whether GenAI acts as a linguistic equalizer in global science. Drawing on 5.65 million scientific articles published from 2021 to 2024, we compare GenAI-assisted and non-assisted publications from authors in non-English-speaking countries. Using text embeddings derived from a pretrained large language model (SciBERT), we measure each publication's linguistic similarity to a benchmark of scientific writing from U.S.-based authors and track stylistic convergence over time. We find significant and growing convergence for GenAI-assisted publications after the release of ChatGPT in late 2022. The effect is strongest for domestic coauthor teams from countries linguistically distant from English. These findings provide large-scale evidence that GenAI is beginning to reshape global science communication by reducing language barriers in research.

</details>


### [101] [Leveraging Large Language Models for Career Mobility Analysis: A Study of Gender, Race, and Job Change Using U.S. Online Resume Profiles](https://arxiv.org/abs/2511.12010)
*Palakorn Achananuparp,Connie Xu,Yao Lu,Xavier Jayaraj Siddarth Ashok,Ee-Peng Lim*

Main category: cs.CY

TL;DR: 基于在线简历的大规模职业流动性分析显示，公司内部职位变动对向上流动性的促进作用最强，而女性和黑人大学毕业生从工作变动中获得的回报显著低于男性和白人同行。


<details>
  <summary>Details</summary>
Motivation: 研究美国大学毕业生职业流动性与性别、种族的关系，探讨工作变动如何影响向上职业流动性，以及不同性别和种族群体在职业流动性结果上的差异。

Method: 使用在线简历数据，开发基于大语言模型的FewSOC职业分类方法，处理缺失人口统计属性、工资数据和噪声职业标签等数据挑战，分析228,710条职业轨迹。

Result: 公司内部职位变动对向上流动性促进作用最强，其次是跨公司职位变动和跨公司平级调动。女性和黑人毕业生从工作变动中获得的回报显著低于男性和白人同行。多级敏感性分析证实这些差异具有稳健性。

Conclusion: 工作变动类型对职业流动性有显著影响，且存在基于性别和种族的系统性差异，女性和黑人大学毕业生在职业流动性方面面临结构性障碍。

Abstract: We present a large-scale analysis of career mobility of college-educated U.S. workers using online resume profiles to investigate how gender, race, and job change options are associated with upward mobility. This study addresses key research questions of how the job changes affect their upward career mobility, and how the outcomes of upward career mobility differ by gender and race. We address data challenges -- such as missing demographic attributes, missing wage data, and noisy occupation labels -- through various data processing and Artificial Intelligence (AI) methods. In particular, we develop a large language models (LLMs) based occupation classification method known as FewSOC that achieves accuracy significantly higher than the original occupation labels in the resume dataset. Analysis of 228,710 career trajectories reveals that intra-firm occupation change has been found to facilitate upward mobility most strongly, followed by inter-firm occupation change and inter-firm lateral move. Women and Black college graduates experience significantly lower returns from job changes than men and White peers. Multilevel sensitivity analyses confirm that these disparities are robust to cluster-level heterogeneity and reveal additional intersectional patterns.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [102] [Preference Learning from Physics-Based Feedback: Tuning Language Models to Design BCC/B2 Superalloys](https://arxiv.org/abs/2511.12036)
*Satanu Ghosh,Collin Holgate,Neal R. Brodnik,Doug Downey,Samantha Daly,Tresa M. Pollock,Samuel Carton*

Main category: cs.CE

TL;DR: 使用偏好学习优化语言模型进行BCC/B2超合金设计，通过热力学相计算提供物理基础奖励信号，实现多目标优化。


<details>
  <summary>Details</summary>
Motivation: 针对传统方法依赖启发式或人工反馈成本高的问题，开发基于物理基础反馈的语言模型优化方法，专门用于探索BCC/B2超合金这一具有极端环境应用潜力的材料家族。

Method: 使用三个开源模型（LLaMA-3.1、Gemma-2、OLMo-2），通过直接偏好优化（DPO）方法，利用热力学相计算产生的统一奖励信号对语言模型进行多目标优化。

Result: 成功展示了语言模型可以通过物理基础反馈进行偏好调优，为结构合金设计提供了新的优化框架。

Conclusion: 该框架具有通用性和可扩展性，为物理科学领域的智能设计空间探索提供了可行路径，是首个使用物理基础反馈进行语言模型偏好调优的结构合金设计研究。

Abstract: We apply preference learning to the task of language model-guided design of novel structural alloys. In contrast to prior work that focuses on generating stable inorganic crystals, our approach targets the synthesizeability of a specific structural class: BCC/B2 superalloys, an underexplored family of materials with potential applications in extreme environments. Using three open-weight models (LLaMA-3.1, Gemma-2, and OLMo-2), we demonstrate that language models can be optimized for multiple design objectives using a single, unified reward signal through Direct Preference Optimization (DPO). Unlike prior approaches that rely on heuristic or human-in-the-loop feedback (costly), our reward signal is derived from thermodynamic phase calculations, offering a scientifically grounded criterion for model tuning. To our knowledge, this is the first demonstration of preference-tuning a language model using physics-grounded feedback for structural alloy design. The resulting framework is general and extensible, providing a path forward for intelligent design-space exploration across a range of physical science domains.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [103] [How Far Do SSL Speech Models Listen for Tone? Temporal Focus of Tone Representation under Low-resource Transfer](https://arxiv.org/abs/2511.12285)
*Minu Kim,Ji Sub Um,Hoirin Kim*

Main category: eess.AS

TL;DR: 研究SSL语音模型在四种复杂声调语言（缅甸语、泰语、老挝语、越南语）中的声调感知能力，发现下游任务对声调迁移有显著影响。


<details>
  <summary>Details</summary>
Motivation: 声调在许多语言中至关重要，但在自监督学习语音模型中研究不足，特别是在普通话以外的语言中。

Method: 使用探针和梯度分析研究微调后的SSL模型，估计声调线索的时间跨度，比较不同下游任务的影响。

Result: 声调迁移因下游任务而异：语音识别微调使模型关注语言特定的声调线索，而韵律和语音相关任务则偏向过长时间跨度。

Conclusion: 声调迁移受下游任务影响，任务选择对声调建模中的时间关注有重要影响。

Abstract: Lexical tone is central to many languages but remains underexplored in self-supervised learning (SSL) speech models, especially beyond Mandarin. We study four languages with complex and diverse tone systems: Burmese, Thai, Lao, and Vietnamese, to examine how far such models listen for tone and how transfer operates in low-resource conditions. As a baseline reference, we estimate the temporal span of tone cues to be about 100 ms in Burmese and Thai, and about 180 ms in Lao and Vietnamese. Probes and gradient analyses on fine-tuned SSL models reveal that tone transfer varies by downstream task: automatic speech recognition fine-tuning aligns spans with language-specific tone cues, while prosody- and voice-related tasks bias the model toward overly long spans. These findings indicate that tone transfer is shaped by downstream task, highlighting task effects on temporal focus in tone modeling.

</details>


### [104] [VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing](https://arxiv.org/abs/2511.12347)
*Zhisheng Zheng,Puyuan Peng,Anuj Diwan,Cong Phuoc Huynh,Xiaohang Sun,Zhu Liu,Vimal Bhat,David Harwath*

Main category: eess.AS

TL;DR: VoiceCraft-X是一个统一的多语言语音编辑和零样本文本转语音合成模型，支持11种语言，使用Qwen3大语言模型和创新的令牌重排序机制。


<details>
  <summary>Details</summary>
Motivation: 解决多语言语音处理中的复杂挑战，将语音编辑和文本转语音合成统一到一个框架中，减少对每种语言大量数据的依赖。

Method: 使用自回归神经编解码语言模型，结合Qwen3大语言模型进行无音素跨语言文本处理，采用时间对齐的文本和语音令牌重排序机制。

Result: 模型能生成高质量、自然的多语言语音，在有限语言数据下仍表现稳健，成功实现语音编辑和文本转语音的统一处理。

Conclusion: 统一的自回归方法在复杂多语言语音应用中具有强大潜力，VoiceCraft-X展示了在多样化语言环境中处理语音任务的可行性。

Abstract: We introduce VoiceCraft-X, an autoregressive neural codec language model which unifies multilingual speech editing and zero-shot Text-to-Speech (TTS) synthesis across 11 languages: English, Mandarin, Korean, Japanese, Spanish, French, German, Dutch, Italian, Portuguese, and Polish. VoiceCraft-X utilizes the Qwen3 large language model for phoneme-free cross-lingual text processing and a novel token reordering mechanism with time-aligned text and speech tokens to handle both tasks as a single sequence generation problem. The model generates high-quality, natural-sounding speech, seamlessly creating new audio or editing existing recordings within one framework. VoiceCraft-X shows robust performance in diverse linguistic settings, even with limited per-language data, underscoring the power of unified autoregressive approaches for advancing complex, real-world multilingual speech applications. Audio samples are available at https://zhishengzheng.com/voicecraft-x/.

</details>


<div id='cs.GL'></div>

# cs.GL [[Back]](#toc)

### [105] [LLM Architecture, Scaling Laws, and Economics: A Quick Summary](https://arxiv.org/abs/2511.11572)
*William H. Press*

Main category: cs.GL

TL;DR: 本文总结了当前大型语言模型的标准QKV自注意力架构和Transformer结构，提供了计算和内存的扩展规律，以及2025年不同规模LLM参数的粗略成本估算，并讨论了DeepSeek是否应被视为特例。


<details>
  <summary>Details</summary>
Motivation: 虽然这些内容并非全新，但在总结形式中并不容易获得，作者希望提供一个系统性的概述。

Method: 采用总结和归纳的方法，分析标准LLM架构、扩展规律和成本估算。

Result: 提供了当前LLM架构的简明总结、计算和内存扩展规律，以及2025年不同规模LLM参数的粗略成本估算。

Conclusion: 这些材料虽然不新颖，但在总结形式中很有价值，特别是关于扩展规律和成本估算的部分。

Abstract: The current standard architecture of Large Language Models (LLMs) with QKV self-attention is briefly summarized, including the architecture of a typical Transformer. Scaling laws for compute (flops) and memory (parameters plus data) are given, along with their present (2025) rough cost estimates for the parameters of present LLMs of various scales, including discussion of whether DeepSeek should be viewed as a special case. Nothing here is new, but this material seems not otherwise readily available in summary form.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [106] [Evolving Prompts for Toxicity Search in Large Language Models](https://arxiv.org/abs/2511.12487)
*Onkar Shelar,Travis Desell*

Main category: cs.NE

TL;DR: ToxSearch是一个黑盒进化框架，通过同步稳态循环演化提示来测试模型安全性，发现词汇替换是最有效的攻击方法，且对抗性提示在不同模型间具有可转移性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型即使经过安全对齐后仍然容易受到对抗性提示的攻击，需要系统化的红队测试方法来评估模型安全性。

Method: 采用黑盒进化框架，使用多种操作符（词汇替换、否定、回译、改写和两种语义交叉操作符）在同步稳态循环中演化提示，通过审核oracle提供适应度指导。

Result: 词汇替换在产出-方差权衡方面表现最佳；语义相似性交叉作为精确的低吞吐量插入器；全局改写显示高方差但拒绝成本较高；对抗性提示在不同模型间具有可转移性，毒性大约减半。

Conclusion: 小的可控扰动是系统化红队测试的有效载体，防御措施应预见到对抗性提示的跨模型重用，而不仅仅是单一模型的强化。

Abstract: Large Language Models remain vulnerable to adversarial prompts that elicit toxic content even after safety alignment. We present ToxSearch, a black-box evolutionary framework that tests model safety by evolving prompts in a synchronous steady-state loop. The system employs a diverse set of operators, including lexical substitutions, negation, back-translation, paraphrasing, and two semantic crossover operators, while a moderation oracle provides fitness guidance. Operator-level analysis shows heterogeneous behavior: lexical substitutions offer the best yield-variance trade-off, semantic-similarity crossover acts as a precise low-throughput inserter, and global rewrites exhibit high variance with elevated refusal costs. Using elite prompts evolved on LLaMA 3.1 8B, we observe practically meaningful but attenuated cross-model transfer, with toxicity roughly halving on most targets, smaller LLaMA 3.2 variants showing the strongest resistance, and some cross-architecture models retaining higher toxicity. These results suggest that small, controllable perturbations are effective vehicles for systematic red-teaming and that defenses should anticipate cross-model reuse of adversarial prompts rather than focusing only on single-model hardening.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [107] [Attention Grounded Enhancement for Visual Document Retrieval](https://arxiv.org/abs/2511.13415)
*Wanqing Cui,Wei Huang,Yazhi Guo,Yibo Hu,Meiguang Jin,Junfeng Ma,Keping Bi*

Main category: cs.IR

TL;DR: 提出AGREE框架，利用多模态大语言模型的跨模态注意力作为局部监督信号，结合全局监督联合优化文档检索器，提升对非抽取式查询的处理能力


<details>
  <summary>Details</summary>
Motivation: 现有文档检索器仅使用粗粒度全局相关性标签训练，无法识别支持匹配的具体文档区域，导致依赖表面线索且难以处理语义关联

Method: AGREE框架通过多模态大语言模型的跨模态注意力获取局部监督信号，与全局监督联合优化检索器，学习文档匹配的驱动内容

Result: 在ViDoRe V2基准测试中显著优于仅使用全局监督的基线方法，促进查询词与文档区域的深度对齐

Conclusion: AGREE框架通过局部监督引导检索器学习相关性驱动内容，实现了更准确和可解释的检索，超越了表面级匹配

Abstract: Visual document retrieval requires understanding heterogeneous and multi-modal content to satisfy information needs. Recent advances use screenshot-based document encoding with fine-grained late interaction, significantly improving retrieval performance. However, retrievers are still trained with coarse global relevance labels, without revealing which regions support the match. As a result, retrievers tend to rely on surface-level cues and struggle to capture implicit semantic connections, hindering their ability to handle non-extractive queries. To alleviate this problem, we propose a \textbf{A}ttention-\textbf{G}rounded \textbf{RE}triever \textbf{E}nhancement (AGREE) framework. AGREE leverages cross-modal attention from multimodal large language models as proxy local supervision to guide the identification of relevant document regions. During training, AGREE combines local signals with the global signals to jointly optimize the retriever, enabling it to learn not only whether documents match, but also which content drives relevance. Experiments on the challenging ViDoRe V2 benchmark show that AGREE significantly outperforms the global-supervision-only baseline. Quantitative and qualitative analyses further demonstrate that AGREE promotes deeper alignment between query terms and document regions, moving beyond surface-level matching toward more accurate and interpretable retrieval. Our code is available at: https://anonymous.4open.science/r/AGREE-2025.

</details>


### [108] [Exploring Multi-Table Retrieval Through Iterative Search](https://arxiv.org/abs/2511.13418)
*Allaa Boutaleb,Bernd Amann,Rafael Angarita,Hubert Naacke*

Main category: cs.IR

TL;DR: 提出了一种迭代式多表检索框架，通过贪心连接感知算法在数据湖中平衡相关性、覆盖率和连接性，相比MIP方法快4-400倍且保持竞争性性能。


<details>
  <summary>Details</summary>
Motivation: 解决数据湖中多表问答的检索挑战，需要在语义相关性和结构连贯性（如连接性）之间取得平衡。精确优化方法计算复杂度过高，而简单贪心启发式方法无法找到连贯的可连接表集。

Method: 将多表检索构建为迭代搜索过程，提出贪心连接感知检索算法，整体优化相关性、覆盖率和连接性。

Result: 在5个NL2SQL基准测试中，迭代方法相比基于MIP的方法实现了竞争性的检索性能，同时速度提升4-400倍（取决于基准和搜索空间设置）。

Conclusion: 迭代启发式方法在实用、可扩展和组合感知检索方面具有巨大潜力。

Abstract: Open-domain question answering over datalakes requires retrieving and composing information from multiple tables, a challenging subtask that demands semantic relevance and structural coherence (e.g., joinability). While exact optimization methods like Mixed-Integer Programming (MIP) can ensure coherence, their computational complexity is often prohibitive. Conversely, simpler greedy heuristics that optimize for query coverage alone often fail to find these coherent, joinable sets. This paper frames multi-table retrieval as an iterative search process, arguing this approach offers advantages in scalability, interpretability, and flexibility. We propose a general framework and a concrete instantiation: a fast, effective Greedy Join-Aware Retrieval algorithm that holistically balances relevance, coverage, and joinability. Experiments across 5 NL2SQL benchmarks demonstrate that our iterative method achieves competitive retrieval performance compared to the MIP-based approach while being 4-400x faster depending on the benchmark and search space settings. This work highlights the potential of iterative heuristics for practical, scalable, and composition-aware retrieval.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [109] [A Content-Preserving Secure Linguistic Steganography](https://arxiv.org/abs/2511.12565)
*Lingyun Xiang,Chengfu Ou,Xu He,Zhongliang Yang,Yuling Liu*

Main category: cs.CR

TL;DR: 提出了一种内容保持的语言隐写方法CLstega，通过可控分布变换在不修改原始文本的情况下嵌入秘密信息，实现完美安全


<details>
  <summary>Details</summary>
Motivation: 现有语言隐写方法通过内容变换隐藏秘密信息，但会在正常文本和隐写文本之间产生细微差异，存在安全风险

Method: 使用增强掩码策略定位嵌入位置，设计动态分布隐写编码策略，通过微调原始掩码语言模型生成目标模型来提取秘密信息

Result: CLstega实现了100%的提取成功率，在安全性方面优于现有方法，有效平衡了嵌入容量和安全性

Conclusion: CLstega方法能够在不修改原始文本的情况下实现完美安全的隐蔽通信，同时保持原始文本的完整性

Abstract: Existing linguistic steganography methods primarily rely on content transformations to conceal secret messages. However, they often cause subtle yet looking-innocent deviations between normal and stego texts, posing potential security risks in real-world applications. To address this challenge, we propose a content-preserving linguistic steganography paradigm for perfectly secure covert communication without modifying the cover text. Based on this paradigm, we introduce CLstega (\textit{C}ontent-preserving \textit{L}inguistic \textit{stega}nography), a novel method that embeds secret messages through controllable distribution transformation. CLstega first applies an augmented masking strategy to locate and mask embedding positions, where MLM(masked language model)-predicted probability distributions are easily adjustable for transformation. Subsequently, a dynamic distribution steganographic coding strategy is designed to encode secret messages by deriving target distributions from the original probability distributions. To achieve this transformation, CLstega elaborately selects target words for embedding positions as labels to construct a masked sentence dataset, which is used to fine-tune the original MLM, producing a target MLM capable of directly extracting secret messages from the cover text. This approach ensures perfect security of secret messages while fully preserving the integrity of the original cover text. Experimental results show that CLstega can achieve a 100\% extraction success rate, and outperforms existing methods in security, effectively balancing embedding capacity and security.

</details>


### [110] [AutoMalDesc: Large-Scale Script Analysis for Cyber Threat Research](https://arxiv.org/abs/2511.13333)
*Alexandru-Mihai Apostu,Andrei Preda,Alexandra Daniela Damir,Diana Bolocan,Radu Tudor Ionescu,Ioana Croitoru,Mihaela Gaman*

Main category: cs.CR

TL;DR: AutoMalDesc是一个自动化静态分析摘要框架，通过自定步调学习流水线生成恶意软件检测的自然语言解释，无需大量人工标注。


<details>
  <summary>Details</summary>
Motivation: 尽管恶意软件检测系统已有显著进展，但生成全面的自然语言威胁检测解释仍是网络安全研究的开放问题。

Method: 利用迭代自定步调学习流水线，通过合成数据生成和验证循环逐步提升输出质量，基于少量专家标注示例进行初始训练后即可独立大规模运行。

Result: 在5种脚本语言的3600个多样化样本上评估显示，迭代间具有统计显著改进，摘要质量和分类准确性均获得一致提升。

Conclusion: 该框架生成的技术精确性和语言连贯性得到验证，并发布了包含10万+脚本样本的完整数据集以促进该领域研究。

Abstract: Generating thorough natural language explanations for threat detections remains an open problem in cybersecurity research, despite significant advances in automated malware detection systems. In this work, we present AutoMalDesc, an automated static analysis summarization framework that, following initial training on a small set of expert-curated examples, operates independently at scale. This approach leverages an iterative self-paced learning pipeline to progressively enhance output quality through synthetic data generation and validation cycles, eliminating the need for extensive manual data annotation. Evaluation across 3,600 diverse samples in five scripting languages demonstrates statistically significant improvements between iterations, showing consistent gains in both summary quality and classification accuracy. Our comprehensive validation approach combines quantitative metrics based on established malware labels with qualitative assessment from both human experts and LLM-based judges, confirming both technical precision and linguistic coherence of generated summaries. To facilitate reproducibility and advance research in this domain, we publish our complete dataset of more than 100K script samples, including annotated seed (0.9K) and test (3.6K) datasets, along with our methodology and evaluation framework.

</details>


### [111] [ForgeDAN: An Evolutionary Framework for Jailbreaking Aligned Large Language Models](https://arxiv.org/abs/2511.13548)
*Siyang Cheng,Gaotian Liu,Rui Mei,Yilin Wang,Kejia Zhang,Kaishuo Wei,Yuqi Yu,Weiping Wen,Xiaojie Wu,Junhua Liu*

Main category: cs.CR

TL;DR: ForgeDAN是一个新颖的进化框架，通过多策略文本扰动、可解释语义适应度评估和双维度越狱判断，有效生成对抗性提示来绕过LLM的安全对齐机制。


<details>
  <summary>Details</summary>
Motivation: 现有自动越狱生成方法存在突变多样性有限、适应度评估浅层和基于关键词检测脆弱等问题，需要更有效的解决方案。

Method: 采用字符、词、句子级别的多策略文本扰动增强攻击多样性；使用基于文本相似性模型的可解释语义适应度评估；集成基于LLM的分类器进行双维度越狱判断。

Result: 评估显示ForgeDAN实现了高越狱成功率，同时保持自然性和隐蔽性，优于现有最先进解决方案。

Conclusion: ForgeDAN框架在生成语义连贯且高效的对抗性提示方面表现出色，有效解决了现有方法的局限性。

Abstract: The rapid adoption of large language models (LLMs) has brought both transformative applications and new security risks, including jailbreak attacks that bypass alignment safeguards to elicit harmful outputs. Existing automated jailbreak generation approaches e.g. AutoDAN, suffer from limited mutation diversity, shallow fitness evaluation, and fragile keyword-based detection. To address these limitations, we propose ForgeDAN, a novel evolutionary framework for generating semantically coherent and highly effective adversarial prompts against aligned LLMs. First, ForgeDAN introduces multi-strategy textual perturbations across \textit{character, word, and sentence-level} operations to enhance attack diversity; then we employ interpretable semantic fitness evaluation based on a text similarity model to guide the evolutionary process toward semantically relevant and harmful outputs; finally, ForgeDAN integrates dual-dimensional jailbreak judgment, leveraging an LLM-based classifier to jointly assess model compliance and output harmfulness, thereby reducing false positives and improving detection effectiveness. Our evaluation demonstrates ForgeDAN achieves high jailbreaking success rates while maintaining naturalness and stealth, outperforming existing SOTA solutions.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [112] [Accepted with Minor Revisions: Value of AI-Assisted Scientific Writing](https://arxiv.org/abs/2511.12529)
*Sanchaita Hazra,Doeun Lee,Bodhisattwa Prasad Majumder,Sachin Kumar*

Main category: cs.HC

TL;DR: LLMs作为科学写作辅助工具的潜力研究，通过随机对照试验发现AI生成摘要经过少量修改可达到与人类写作相当的质量，但编辑行为主要受AI作者身份感知而非客观质量驱动，强调了来源披露在协作科学写作中的重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在各个领域应用广泛，但其作为科学写作辅助工具的有效性仍未被充分理解。科学写作需要精确性、多模态综合和领域专业知识，本研究旨在探索LLMs在支持领域专家进行科学写作（特别是摘要撰写）方面的潜力。

Method: 设计了激励性随机对照试验，采用假设会议设置，将具有相关专业知识的参与者分为作者和审稿人两组。采用2x2组间设计，考察摘要的隐含来源（人类写作vs AI生成）和来源披露情况。通过新颖的激励结构鼓励作者将提供的摘要编辑到可接受的质量水平。

Result: 作者对未标注来源的人类写作摘要编辑最多，而对AI生成摘要编辑较少，这通常是因为感知到AI生成摘要的可读性更高。当披露来源信息后，两种来源的编辑量趋于一致。审稿人的决定不受摘要来源影响，但与编辑次数显著相关。对AI生成摘要进行仔细的风格编辑（在知晓来源情况下）提高了接受机会。

Conclusion: AI生成的摘要经过最小程度的修改即可达到与人类写作相当的可接受水平，但观察到的编辑行为主要由对AI作者身份的感知而非客观质量驱动。研究结果强调了在协作科学写作中披露来源的重要性。

Abstract: Large Language Models have seen expanding application across domains, yet their effectiveness as assistive tools for scientific writing -- an endeavor requiring precision, multimodal synthesis, and domain expertise -- remains insufficiently understood. We examine the potential of LLMs to support domain experts in scientific writing, with a focus on abstract composition. We design an incentivized randomized controlled trial with a hypothetical conference setup where participants with relevant expertise are split into an author and reviewer pool. Inspired by methods in behavioral science, our novel incentive structure encourages authors to edit the provided abstracts to an acceptable quality for a peer-reviewed submission. Our 2x2 between-subject design expands into two dimensions: the implicit source of the provided abstract and the disclosure of it. We find authors make most edits when editing human-written abstracts compared to AI-generated abstracts without source attribution, often guided by higher perceived readability in AI generation. Upon disclosure of source information, the volume of edits converges in both source treatments. Reviewer decisions remain unaffected by the source of the abstract, but bear a significant correlation with the number of edits made. Careful stylistic edits, especially in the case of AI-generated abstracts, in the presence of source information, improve the chance of acceptance. We find that AI-generated abstracts hold potential to reach comparable levels of acceptability to human-written ones with minimal revision, and that perceptions of AI authorship, rather than objective quality, drive much of the observed editing behavior. Our findings reverberate the significance of source disclosure in collaborative scientific writing.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [113] [Decoupling Positional and Symbolic Attention Behavior in Transformers](https://arxiv.org/abs/2511.11579)
*Felipe Urrutia,Jorge Salas,Alexander Kozachinskiy,Cristian Buc Calderon,Hector Pasten,Cristobal Rojas*

Main category: cs.LG

TL;DR: 本文深入分析了Transformer中Rotary位置编码(RoPE)的位置信息与符号信息编码机制，提出了位置性头与符号性头的定义和量化指标，并证明通过控制注意力头可访问的频率可以因果性地控制模型性能。


<details>
  <summary>Details</summary>
Motivation: 理解RoPE成功的原因，特别是其如何分别使用不同频率编码位置信息和语义信息，以及这种编码机制如何影响注意力头的行为。

Method: 提出了位置性头和符号性头的理论定义与量化指标，分析了使用RoPE的Transformer LLMs，并设计了纯位置性和纯符号性的典型任务来验证理论。

Result: 发现所有注意力头的行为与频率使用之间存在强相关性，通过控制注意力头可访问的频率可以因果性地控制Transformer在特定任务上的性能。

Conclusion: RoPE的成功源于其能够有效分离位置信息和语义信息的编码，注意力头的行为与频率使用模式密切相关，这为理解和改进位置编码机制提供了理论基础。

Abstract: An important aspect subtending language understanding and production is the ability to independently encode positional and symbolic information of the words within a sentence. In Transformers, positional information is typically encoded using Positional Encodings (PEs). One such popular PE, namely Rotary PE (RoPE), has been widely used due to its empirical success. Recently, it has been argued that part of RoPE's success emerges from its ability to encode robust positional and semantic information using large and small frequencies, respectively. In this work, we perform a deeper dive into the positional versus symbolic dichotomy of attention heads behavior, both at the theoretical and empirical level. We provide general definitions of what it means for a head to behave positionally or symbolically, prove that these are two mutually exclusive behaviors and develop a metric to quantify them. We apply our framework to analyze Transformer-based LLMs using RoPE and find that all heads exhibit a strong correspondence between behavior and frequency use. Finally, we introduce canonical tasks designed to be either purely positional or symbolic, and demonstrate that the Transformer performance causally relates to the ability of attention heads to leverage the appropriate frequencies. In particular, we show that we can control the Transformer performance by controlling which frequencies the attention heads can access. Altogether, our work provides a detailed understanding of RoPE, and how its properties relate to model behavior.

</details>


### [114] [The Anatomy of a Triton Attention Kernel](https://arxiv.org/abs/2511.11581)
*Burkhard Ringlein,Jan van Lunteren,Radu Stoica,Thomas Parnell*

Main category: cs.LG

TL;DR: 开发了基于Triton的跨平台LLM推理系统，通过paged attention内核在NVIDIA和AMD GPU上实现最佳性能，将通用Triton注意力内核性能从19.7%提升到105.9%。


<details>
  <summary>Details</summary>
Motivation: 开发可移植、高效且无需底层手动调优的跨硬件架构LLM推理平台，解决产业界和学术界的长期需求。

Method: 使用Triton领域特定即时编译语言开发最先进的paged attention内核，结合高层方法、关键算法和系统级改进、参数自动调优，并集成到流行推理服务器中。

Result: 在NVIDIA和AMD GPU上实现最佳性能，将通用Triton注意力内核性能从19.7%提升到105.9%，超越现有最优方案。

Conclusion: 开源领域特定语言可以用于解锁模型在不同GPU厂商间的可移植性，证明了便携高效的跨平台LLM推理是可行的。

Abstract: A long-standing goal in both industry and academia is to develop an LLM inference platform that is portable across hardware architectures, eliminates the need for low-level hand-tuning, and still delivers best-in-class efficiency. In this work, we demonstrate that portable, efficient cross-platform LLM inference is indeed possible and share our experience. We develop a state-of-the-art paged attention kernel, the core performance-critical component of many LLM deployments, that builds exclusively on the domain-specific just-in-time compiled language Triton to achieve state-of-the-art performance on both NVIDIA and AMD GPUs. We describe our high-level approach, the key algorithmic and system-level improvements, the parameter auto-tuning required to unlock efficiency, and the integrations into a popular inference server that are necessary to bring the performance of a generic Triton attention kernel from 19.7% of the state-of-the-art to 105.9%. Our results highlight how open-source domain-specific languages can be leveraged to unlock model portability across different GPU vendors.

</details>


### [115] [Small Vocabularies, Big Gains: Pretraining and Tokenization in Time Series Models](https://arxiv.org/abs/2511.11622)
*Alexis Roger,Gwen Legate,Kashif Rasul,Yuriy Nevmyvaka,Irina Rish*

Main category: cs.LG

TL;DR: 本文系统研究了时间序列基础模型中分词器设计（缩放和量化策略）与迁移学习对模型性能的影响，发现分词器配置主要控制表示能力和稳定性，而迁移学习影响优化效率和对齐效果。


<details>
  <summary>Details</summary>
Motivation: 分词器和迁移学习是构建先进时间序列预测基础模型的两个关键组件，需要系统研究它们各自的作用和相互作用。

Method: 通过经验性训练实验和理论分析相结合的方法，研究不同分词器配置（特别是缩放和量化策略）与预训练/随机初始化对模型性能的影响。

Result: 预训练模型能更有效地利用设计良好的分词器，特别是在小词汇量时；而对齐不当的分词会削弱甚至逆转预训练的优势。小词汇量与预训练权重结合在多模态预测中特别有利。

Conclusion: 精心设计的分词器对时间序列建模至关重要，在离散表示学习中结合小词汇量和预训练权重是多模态预测设置中的优势策略。

Abstract: Tokenization and transfer learning are two critical components in building state of the art time series foundation models for forecasting. In this work, we systematically study the effect of tokenizer design, specifically scaling and quantization strategies, on model performance, alongside the impact of pretraining versus random initialization. We show that tokenizer configuration primarily governs the representational capacity and stability of the model, while transfer learning influences optimization efficiency and alignment. Using a combination of empirical training experiments and theoretical analyses, we demonstrate that pretrained models consistently leverage well-designed tokenizers more effectively, particularly at smaller vocabulary sizes. Conversely, misaligned tokenization can diminish or even invert the benefits of pretraining. These findings highlight the importance of careful tokenization in time series modeling and suggest that combining small, efficient vocabularies with pretrained weights is especially advantageous in multi-modal forecasting settings, where the overall vocabulary must be shared across modalities. Our results provide concrete guidance for designing tokenizers and leveraging transfer learning in discrete representation learning for continuous signals.

</details>


### [116] [H-Model: Dynamic Neural Architectures for Adaptive Processing](https://arxiv.org/abs/2511.11669)
*Dmytro Hospodarchuk*

Main category: cs.LG

TL;DR: 设计了一种能够根据输入数据动态调整内部结构的神经网络架构，通过路由机制实现迭代和自适应计算，这是一个概念性原型而非性能优化的模型。


<details>
  <summary>Details</summary>
Motivation: 探索可适应且可能更可解释的网络架构新方向，让系统不仅能学习表示，还能学习计算结构本身，而不是与现有语言模型竞争性能。

Method: 引入路由机制，允许每一层影响其输出在网络中的传播方式，实现基于输入数据和系统内部状态的条件化信息流。

Result: 由于计算资源和数据限制，这是一个初步研究，但初步观察显示出潜力，完整潜力需要在更有利的计算条件下评估。

Conclusion: 提出了一个概念性架构框架，为探索自适应网络开辟了新方向，但需要更多计算资源来充分评估其潜力。

Abstract: This article explores the design and experimentation of a neural network architecture capable of dynamically adjusting its internal structure based on the input data. The proposed model introduces a routing mechanism that allows each layer to influence how its outputs are propagated through the network, enabling iterative and adaptive computation. This concept is loosely inspired by the idea of thought processes and dynamic reasoning, where information flow is conditioned not only on the data itself, but also on the internal state of the system.
  It is important to note that this work does not aim to compete with state-of-the-art language models in terms of performance. Instead, it presents a conceptual prototype-an architectural framework that opens up a new direction for exploring adaptable and potentially more interpretable networks. The goal is not optimization of existing benchmarks but rather the proposal of a system that can learn not only representations, but also the structure of computation itself.
  Due to practical constraints in computing resources and data, this study remains a preliminary investigation. Nevertheless, initial observations show promise, and the architecture's full potential can only be evaluated in future experiments under more favorable computational conditions.

</details>


### [117] [Reasoning: From Reflection to Solution](https://arxiv.org/abs/2511.11712)
*Zixi Li*

Main category: cs.LG

TL;DR: 论文提出推理是状态空间中迭代算子应用收敛到固定点的过程，并通过OpenLM架构在OpenXOR问题上实现76%准确率，而现有LLMs为0%。


<details>
  <summary>Details</summary>
Motivation: 在LLMs在GSM8K等基准上取得超人表现的时代，需要厘清这些系统是学会了推理还是仅对推理轨迹进行模式匹配。

Method: 提出推理的数学定义（状态空间中迭代算子应用），开发OpenOperator理论和OpenLM架构实现该定义。

Result: 在OpenXOR问题上，OpenLM达到76%准确率，而现有最先进LLMs为0%，验证了理论定义的有效性。

Conclusion: 论文不仅批判现有系统，更关注理解推理的本质要求并构建提供真正推理能力的架构。

Abstract: What is reasoning? This question has driven centuries of philosophical inquiry, from Aristotle's syllogisms to modern computational complexity theory. In the age of large language models achieving superhuman performance on benchmarks like GSM8K (95\% accuracy) and HumanEval (90\% pass@1), we must ask: have these systems learned to \emph{reason}, or have they learned to \emph{pattern-match over reasoning traces}?
  This paper argues for a specific answer: \textbf{reasoning is iterative operator application in state spaces, converging to fixed points}. This definition is not merely philosophical -- it has concrete architectural implications that explain both the failures of current systems and the path to genuine reasoning capabilities.
  Our investigation begins with a puzzle (OpenXOR), progresses through theory (OpenOperator), and culminates in a working solution (OpenLM) that achieves 76\% accuracy where state-of-the-art LLMs achieve 0\%. This is not about criticizing existing systems, but about \emph{understanding what reasoning requires} and \emph{building architectures that provide it}.

</details>


### [118] [Better LLM Reasoning via Dual-Play](https://arxiv.org/abs/2511.11881)
*Zhengxin Zhang,Chengyu Huang,Aochong Oliver Li,Claire Cardie*

Main category: cs.LG

TL;DR: PasoDoble是一个新颖的LLM双角色对抗训练框架，通过让Proposer生成有挑战性的问题和Solver尝试解决这些问题，实现无监督的模型自我进化。


<details>
  <summary>Details</summary>
Motivation: 当前LLM训练严重依赖外部监督（如人工标注），而对抗学习特别是自博弈提供了减少外部依赖的替代方案。双角色对抗训练能促进持续竞争和相互进化，但将其适配到LLM仍面临奖励破解和训练不稳定的挑战。

Method: PasoDoble框架包含两个从同一基础模型初始化的模型：Proposer生成带真实答案的挑战性问题，Solver尝试解决这些问题。Proposer通过预训练数据集增强知识以确保问题质量和多样性。为避免奖励破解，Proposer仅因生成有效且能挑战Solver极限的问题而获奖励，Solver因正确解决问题而获奖励，两者联合更新。还引入了可选的离线范式来增强训练稳定性。

Result: 实验结果表明，PasoDoble能够提升LLM的推理性能。

Conclusion: PasoDoble成功实现了无监督的LLM双角色对抗训练，有效提升了模型推理能力，为解决LLM训练中的外部依赖问题提供了可行方案。

Abstract: Large Language Models (LLMs) have achieved remarkable progress through Reinforcement Learning with Verifiable Rewards (RLVR), yet still rely heavily on external supervision (e.g., curated labels). Adversarial learning, particularly through self-play, offers a promising alternative that enables models to iteratively learn from themselves - thus reducing reliance on external supervision. Dual-play extends adversarial learning by assigning specialized roles to two models and training them against each other, fostering sustained competition and mutual evolution. Despite its promise, adapting dual-play training to LLMs remains limited, largely due to their susceptibility to reward hacking and training instability. In this paper, we introduce PasoDoble, a novel LLM dual-play framework. PasoDoble adversarially trains two models initialized from the same base model: a Proposer, which generates challenging questions with ground-truth answers, and a Solver, which attempts to solve them. We enrich the Proposer with knowledge from a pre-training dataset to ensure the questions' quality and diversity. To avoid reward hacking, the Proposer is rewarded for producing only valid questions that push the Solver's limit, while the Solver is rewarded for solving them correctly, and both are updated jointly. To further enhance training stability, we introduce an optional offline paradigm that decouples Proposer and Solver updates, alternately updating each for several steps while holding the other fixed. Notably, PasoDoble operates without supervision during training. Experimental results show that PasoDoble can improve the reasoning performance of LLMs. Our project page is available at https://hcy123902.github.io/PasoDoble.

</details>


### [119] [Computational Measurement of Political Positions: A Review of Text-Based Ideal Point Estimation Algorithms](https://arxiv.org/abs/2511.13238)
*Patrick Parschan,Charlott Jakob*

Main category: cs.LG

TL;DR: 本文系统综述了无监督和半监督的基于文本的理想点估计算法(CT-IPE)，提出了概念框架区分四种方法家族，并为应用研究者提供实用指导。


<details>
  <summary>Details</summary>
Motivation: CT-IPE算法在政治学、计算社会科学等领域广泛用于从文本数据推断政治立场，但该领域缺乏系统比较和应用指导，呈现碎片化状态。

Method: 通过系统文献回顾识别25个CT-IPE算法，进行内容分析，建立概念框架区分文本方差生成、捕捉和聚合方式，识别四种方法家族。

Result: 识别了词频、主题建模、词嵌入和LLM四种方法家族，分析了各自假设、可解释性、可扩展性和局限性，提供了算法选择的权衡指导。

Conclusion: 不同算法的估计结果差异本身具有信息价值，强调需要系统基准测试，为应用研究提供结构化综合和实践指导。

Abstract: This article presents the first systematic review of unsupervised and semi-supervised computational text-based ideal point estimation (CT-IPE) algorithms, methods designed to infer latent political positions from textual data. These algorithms are widely used in political science, communication, computational social science, and computer science to estimate ideological preferences from parliamentary speeches, party manifestos, and social media. Over the past two decades, their development has closely followed broader NLP trends -- beginning with word-frequency models and most recently turning to large language models (LLMs). While this trajectory has greatly expanded the methodological toolkit, it has also produced a fragmented field that lacks systematic comparison and clear guidance for applied use. To address this gap, we identified 25 CT-IPE algorithms through a systematic literature review and conducted a manual content analysis of their modeling assumptions and development contexts. To compare them meaningfully, we introduce a conceptual framework that distinguishes how algorithms generate, capture, and aggregate textual variance. On this basis, we identify four methodological families -- word-frequency, topic modeling, word embedding, and LLM-based approaches -- and critically assess their assumptions, interpretability, scalability, and limitations. Our review offers three contributions. First, it provides a structured synthesis of two decades of algorithm development, clarifying how diverse methods relate to one another. Second, it translates these insights into practical guidance for applied researchers, highlighting trade-offs in transparency, technical requirements, and validation strategies that shape algorithm choice. Third, it emphasizes that differences in estimation outcomes across algorithms are themselves informative, underscoring the need for systematic benchmarking.

</details>


### [120] [P1: Mastering Physics Olympiads with Reinforcement Learning](https://arxiv.org/abs/2511.13612)
*Jiacheng Chen,Qianjia Cheng,Fangchen Yu,Haiyuan Wan,Yuchen Zhang,Shenghe Zheng,Junchi Yao,Qingyang Zhang,Haonan He,Yun Luo,Yufeng Zhao,Futing Wang,Li Sheng,Chengxing Xie,Yuxin Zuo,Yizhuo Li,Wenxauan Zeng,Yulun Wu,Rui Huang,Dongzhan Zhou,Kai Chen,Yu Qiao,Lei Bai,Yu Cheng,Ning Ding,Bowen Zhou,Peng Ye,Ganqu Cui*

Main category: cs.LG

TL;DR: P1系列是首个通过强化学习训练的开源物理推理模型，其中P1-235B-A22B在国际物理奥赛(IPhO 2025)获得金牌表现，并在13个国际/地区物理竞赛中赢得12枚金牌。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型从谜题解决转向科学级推理的需求，物理作为连接符号与现实的基础学科，是测试这种转变的最佳领域。

Method: 通过强化学习训练开源物理推理模型系列P1，并配备智能体框架PhysicsMinions。

Result: P1-235B-A22B在IPhO 2025获得金牌，13个物理竞赛中赢得12枚金牌；P1-235B-A22B+PhysicsMinions在IPhO 2025总体排名第一；P1模型在数学和编程等其他推理任务上也表现优异。

Conclusion: P1系列模型展示了卓越的物理推理能力和泛化性，为推进物理研究提供了强大的AI工具。

Abstract: Recent progress in large language models (LLMs) has moved the frontier from puzzle-solving to science-grade reasoning-the kind needed to tackle problems whose answers must stand against nature, not merely fit a rubric. Physics is the sharpest test of this shift, which binds symbols to reality in a fundamental way, serving as the cornerstone of most modern technologies. In this work, we manage to advance physics research by developing large language models with exceptional physics reasoning capabilities, especially excel at solving Olympiad-level physics problems. We introduce P1, a family of open-source physics reasoning models trained entirely through reinforcement learning (RL). Among them, P1-235B-A22B is the first open-source model with Gold-medal performance at the latest International Physics Olympiad (IPhO 2025), and wins 12 gold medals out of 13 international/regional physics competitions in 2024/2025. P1-30B-A3B also surpasses almost all other open-source models on IPhO 2025, getting a silver medal. Further equipped with an agentic framework PhysicsMinions, P1-235B-A22B+PhysicsMinions achieves overall No.1 on IPhO 2025, and obtains the highest average score over the 13 physics competitions. Besides physics, P1 models also present great performance on other reasoning tasks like math and coding, showing the great generalibility of P1 series.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [121] [Live-SWE-agent: Can Software Engineering Agents Self-Evolve on the Fly?](https://arxiv.org/abs/2511.13646)
*Chunqiu Steven Xia,Zhe Wang,Yan Yang,Yuxiang Wei,Lingming Zhang*

Main category: cs.SE

TL;DR: Live-SWE-agent是首个能够在运行时自主持续进化的软件代理，从基础bash工具开始，在解决真实软件问题时自主演化其脚手架实现，在SWE-bench Verified基准上达到75.4%的解决率，超越了所有现有开源软件代理。


<details>
  <summary>Details</summary>
Motivation: 现有LLM软件代理需要专门设计且可能不是最优的，而自改进代理需要昂贵的离线训练且泛化性差。因此需要能够在运行时自主进化的软件代理。

Method: 从仅具有bash工具的基础代理脚手架开始，在解决真实软件问题时自主演化其脚手架实现，实现实时自我改进。

Result: 在SWE-bench Verified基准上达到75.4%的解决率，超越所有开源代理；在SWE-Bench Pro基准上达到45.8%的最佳已知解决率。

Conclusion: Live-SWE-agent证明了软件代理能够在运行时自主进化的可行性，为构建更强大和自适应的软件工程代理提供了新方向。

Abstract: Large Language Models (LLMs) are reshaping almost all industries, including software engineering. In recent years, a number of LLM agents have been proposed to solve real-world software problems. Such software agents are typically equipped with a suite of coding tools and can autonomously decide the next actions to form complete trajectories to solve end-to-end software tasks. While promising, they typically require dedicated design and may still be suboptimal, since it can be extremely challenging and costly to exhaust the entire agent scaffold design space. Recognizing that software agents are inherently software themselves that can be further refined/modified, researchers have proposed a number of self-improving software agents recently, including the Darwin-Gödel Machine (DGM). Meanwhile, such self-improving agents require costly offline training on specific benchmarks and may not generalize well across different LLMs or benchmarks. In this paper, we propose Live-SWE-agent, the first live software agent that can autonomously and continuously evolve itself on-the-fly during runtime when solving real-world software problems. More specifically, Live-SWE-agent starts with the most basic agent scaffold with only access to bash tools (e.g., mini-SWE-agent), and autonomously evolves its own scaffold implementation while solving real-world software problems. Our evaluation on the widely studied SWE-bench Verified benchmark shows that Live-SWE-agent can achieve an impressive solve rate of 75.4% without test-time scaling, outperforming all existing open-source software agents and approaching the performance of the best proprietary solution. Moreover, Live-SWE-agent outperforms state-of-the-art manually crafted software agents on the recent SWE-Bench Pro benchmark, achieving the best-known solve rate of 45.8%.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [122] [D$^{3}$ToM: Decider-Guided Dynamic Token Merging for Accelerating Diffusion MLLMs](https://arxiv.org/abs/2511.12280)
*Shuochen Chang,Xiaofeng Zhang,Qingyang Liu,Li Niu*

Main category: cs.CV

TL;DR: D³ToM是一种用于加速扩散式多模态大语言模型推理的动态令牌合并方法，通过在每个去噪步骤中动态合并冗余视觉令牌来减少计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 扩散式多模态大语言模型虽然具有强大的非自回归生成能力，但推理速度显著慢于自回归模型，因为每个去噪步骤都需要对整个序列进行完整的双向自注意力计算，导致立方级的解码复杂度。

Method: 提出D³ToM方法：使用前一步生成的决策令牌构建视觉令牌重要性图，保留最显著的令牌，通过相似性聚合合并其余令牌。这是一个即插即用模块，集成在单个Transformer层中，物理上缩短视觉令牌序列而不改变模型参数。

Result: 大量实验表明，D³ToM在保持竞争力的同时显著加速了推理过程。

Conclusion: D³ToM通过动态令牌合并有效解决了扩散式多模态大语言模型的推理效率问题，在相同计算预算下实现了优越性能。

Abstract: Diffusion-based multimodal large language models (Diffusion MLLMs) have recently demonstrated impressive non-autoregressive generative capabilities across vision-and-language tasks. However, Diffusion MLLMs exhibit substantially slower inference than autoregressive models: Each denoising step employs full bidirectional self-attention over the entire sequence, resulting in cubic decoding complexity that becomes computationally impractical with thousands of visual tokens. To address this challenge, we propose D$^{3}$ToM, a Decider-guided dynamic token merging method that dynamically merges redundant visual tokens at different denoising steps to accelerate inference in Diffusion MLLMs. At each denoising step, D$^{3}$ToM uses decider tokens-the tokens generated in the previous denoising step-to build an importance map over all visual tokens. Then it maintains a proportion of the most salient tokens and merges the remainder through similarity-based aggregation. This plug-and-play module integrates into a single transformer layer, physically shortening the visual token sequence for all subsequent layers without altering model parameters. Moreover, D$^{3}$ToM employs a merge ratio that dynamically varies with each denoising step, aligns with the native decoding process of Diffusion MLLMs, achieving superior performance under equivalent computational budgets. Extensive experiments show that D$^{3}$ToM accelerates inference while preserving competitive performance. The code is released at https://github.com/bcmi/D3ToM-Diffusion-MLLM.

</details>


### [123] [DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions](https://arxiv.org/abs/2511.12452)
*Xiaoyu Lin,Aniket Ghorpade,Hansheng Zhu,Justin Qiu,Dea Rrozhani,Monica Lama,Mick Yang,Zixuan Bian,Ruohan Ren,Alan B. Hong,Jiatao Gu,Chris Callison-Burch*

Main category: cs.CV

TL;DR: DenseAnnotate是一个音频驱动的在线标注平台，通过语音注释创建密集、细粒度的图像和3D资产标注，解决了传统文本标注在表达性、速度和视觉特征捕捉方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型的训练数据主要依赖稀疏的互联网挖掘或手动输入注释，这些方法只能捕捉图像视觉内容的一小部分。密集注释更有价值但稀缺，传统文本标注管道在表达性、速度和专门领域（如多元文化图像和3D资产标注）方面存在不足。

Method: 开发了DenseAnnotate平台，允许标注者通过语音叙述观察内容，同时将口语短语与图像区域或3D场景部分同步链接。平台整合了语音转文本转录和注意力区域标记功能。

Result: 通过超过1000名标注者的案例研究，创建了包含3531张图像、898个3D场景和7460个3D对象的多模态数据集，包含20种语言的音频对齐密集注释。基于该数据集训练的模型在多语言能力上提升5%，文化对齐提升47%，3D空间能力提升54%。

Conclusion: DenseAnnotate平台为未来视觉语言研究提供了可行方法，可应用于各种任务和多样化数据类型，显著提升了标注效率和质量。

Abstract: With the rapid adoption of multimodal large language models (MLLMs) across diverse applications, there is a pressing need for task-centered, high-quality training data. A key limitation of current training datasets is their reliance on sparse annotations mined from the Internet or entered via manual typing that capture only a fraction of an image's visual content. Dense annotations are more valuable but remain scarce. Traditional text-based annotation pipelines are poorly suited for creating dense annotations: typing limits expressiveness, slows annotation speed, and underrepresents nuanced visual features, especially in specialized areas such as multicultural imagery and 3D asset annotation. In this paper, we present DenseAnnotate, an audio-driven online annotation platform that enables efficient creation of dense, fine-grained annotations for images and 3D assets. Annotators narrate observations aloud while synchronously linking spoken phrases to image regions or 3D scene parts. Our platform incorporates speech-to-text transcription and region-of-attention marking. To demonstrate the effectiveness of DenseAnnotate, we conducted case studies involving over 1,000 annotators across two domains: culturally diverse images and 3D scenes. We curate a human-annotated multi-modal dataset of 3,531 images, 898 3D scenes, and 7,460 3D objects, with audio-aligned dense annotations in 20 languages, including 8,746 image captions, 2,000 scene captions, and 19,000 object captions. Models trained on this dataset exhibit improvements of 5% in multilingual, 47% in cultural alignment, and 54% in 3D spatial capabilities. Our results show that our platform offers a feasible approach for future vision-language research and can be applied to various tasks and diverse types of data.

</details>


### [124] [Co-Layout: LLM-driven Co-optimization for Interior Layout](https://arxiv.org/abs/2511.12474)
*Chucheng Xiang,Ruchao Bao,Biyin Feng,Wenzheng Wu,Zhongyuan Liu,Yirui Guan,Ligang Liu*

Main category: cs.CV

TL;DR: 提出结合大语言模型和网格整数规划的自动室内设计框架，联合优化房间布局和家具摆放，采用粗到细优化策略提高效率。


<details>
  <summary>Details</summary>
Motivation: 现有两阶段设计流程在解决方案质量和计算效率方面存在不足，需要开发能够联合优化房间布局和家具摆放的自动化设计方法。

Method: 使用LLM从文本提示提取结构化设计约束，编码到基于网格的统一表示中，采用粗到细优化策略，先低分辨率简化问题再指导全分辨率求解。

Result: 实验结果表明，该方法在解决方案质量上显著优于现有两阶段设计流程，并通过粗到细策略实现了显著的计算效率提升。

Conclusion: 提出的联合优化框架在自动室内设计中表现出色，既提高了设计质量又优化了计算效率，为自动化设计提供了有效解决方案。

Abstract: We present a novel framework for automated interior design that combines large language models (LLMs) with grid-based integer programming to jointly optimize room layout and furniture placement. Given a textual prompt, the LLM-driven agent workflow extracts structured design constraints related to room configurations and furniture arrangements. These constraints are encoded into a unified grid-based representation inspired by ``Modulor". Our formulation accounts for key design requirements, including corridor connectivity, room accessibility, spatial exclusivity, and user-specified preferences. To improve computational efficiency, we adopt a coarse-to-fine optimization strategy that begins with a low-resolution grid to solve a simplified problem and guides the solution at the full resolution. Experimental results across diverse scenarios demonstrate that our joint optimization approach significantly outperforms existing two-stage design pipelines in solution quality, and achieves notable computational efficiency through the coarse-to-fine strategy.

</details>
