<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 31]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Activation-Space Personality Steering: Hybrid Layer Selection for Stable Trait Control in LLMs](https://arxiv.org/abs/2511.03738)
*Pranav Bhandari,Nicolas Fay,Sanjeevan Selvaganapathy,Amitava Datta,Usman Naseem,Mehwish Nasim*

Main category: cs.CL

TL;DR: 提出了一种基于大五人格特质提取隐藏状态激活的管道，通过低秩子空间发现方法识别特质特定最优层，实现LLM人格对齐和行为控制。


<details>
  <summary>Details</summary>
Motivation: LLM在生成中表现出隐含人格特质，但可靠控制或对齐这些特质以满足特定需求仍是一个开放挑战，需要有效的生成行为操纵机制。

Method: 从transformer层提取隐藏状态激活，应用低秩子空间发现方法，识别不同模型架构中特质特定最优层，通过动态层选择的灵活引导框架操作人格对齐方向。

Result: 发现人格特质占据低秩共享子空间，这些潜在结构可通过精心扰动转化为有效引导机制，不影响流畅性、方差和通用能力。

Conclusion: 该方法有助于弥合心理学理论与实际模型对齐之间的差距，为LLM人格特质控制提供了有效途径。

Abstract: Large Language Models exhibit implicit personalities in their generation, but
reliably controlling or aligning these traits to meet specific needs remains an
open challenge. The need for effective mechanisms for behavioural manipulation
of the model during generation is a critical gap in the literature that needs
to be fulfilled. Personality-aware LLMs hold a promising direction towards this
objective. However, the relationship between these psychological constructs and
their representations within LLMs remains underexplored and requires further
investigation. Moreover, it is intriguing to understand and study the use of
these representations to steer the models' behaviour. We propose a novel
pipeline that extracts hidden state activations from transformer layers using
the Big Five Personality Traits (Openness, Conscientiousness, Extraversion,
Agreeableness and Neuroticism), which is a comprehensive and empirically
validated framework to model human personality applies low-rank subspace
discovery methods, and identifies trait-specific optimal layers across
different model architectures for robust injection. The resulting
personality-aligned directions are then operationalised through a flexible
steering framework with dynamic layer selection, enabling precise control of
trait expression in LLM outputs. Our findings reveal that personality traits
occupy a low-rank shared subspace, and that these latent structures can be
transformed into actionable mechanisms for effective steering through careful
perturbations without impacting the fluency, variance and general capabilities,
helping to bridge the gap between psychological theory and practical model
alignment.

</details>


### [2] [TextualVerifier: Verify TextGrad Step-by-Step](https://arxiv.org/abs/2511.03739)
*Eugenius Mario Situmorang,Adila Alfa Krisnadhi,Ari Wibisono*

Main category: cs.CL

TL;DR: TextualVerifier是一个基于大语言模型的验证框架，通过思维链分解、变体生成、多数投票和共识聚合四个阶段，为TextGrad提供自我验证机制，显著提升文本推理的有效性和优化结果的可靠性。


<details>
  <summary>Details</summary>
Motivation: TextGrad作为文本自动微分方法，在文本优化中缺乏自我验证机制来确保推理有效性，这限制了其在复杂决策任务中的可靠性。

Method: 采用四阶段工作流：思维链分解、变体生成、多数投票和共识聚合，非侵入式集成到TextGrad的损失函数和优化结果验证阶段。

Result: 在PRM800K上推理步骤有效性提升29%；集成到TextGrad后在GPQA-Diamond、MMLU-ML和MMLU-CP基准上分别获得8.08、10.71和3.92个百分点的改进，损失函数集成使准确率从68.2%提升至70.4%。

Conclusion: TextualVerifier是首个基于LLM技术为TextGrad提供的自我验证框架，无需数值梯度即可实现更可靠的推理，为文本优化中的验证开辟了新方向。

Abstract: TextGrad is a novel approach to text-based automatic differentiation that
enables composite AI systems to perform optimization without explicit numerical
equations. However, it currently lacks self-verification mechanisms that ensure
reasoning validity in text-based decision making. This research introduces
TextualVerifier, a verification framework that leverages chain-of-thought
reasoning and majority voting with large language models to address this
verification gap. TextualVerifier implements a four-stage workflow:
chain-of-thought decomposition, variant generation, majority voting, and
consensus aggregation. It integrates non-invasively with TextGrad at both the
loss function and optimization result verification stages. Experimental
evaluation using the Gemini 1.5 Pro model is conducted in two phases: (1)
standalone evaluation on PRM800K, and (2) integrated evaluation with TextGrad
on GPQA-Diamond, MMLU-ML, and MMLU-CP benchmarks. Results show statistically
significant improvements (p < 0.001). In phase one, TextualVerifier improves
the validity of reasoning steps by 29 percent. In phase two, integration into
TextGrad loss function yields a 2.2 percentage point gain from 68.2 to 70.4
percent with a moderate overhead of 5.9 LLM calls on average. Further
evaluations of TextualVerifier versioning yield 8.08, 10.71, and 3.92
percentage point improvements on GPQA, MMLU-ML, and MMLU-CP respectively.
TextualVerifier thus presents the first self-verification framework for
TextGrad through LLM-based techniques without requiring numerical gradients,
enabling more reliable reasoning and opening new directions for verification in
text-based optimization.

</details>


### [3] [GRDD+: An Extended Greek Dialectal Dataset with Cross-Architecture Fine-tuning Evaluation](https://arxiv.org/abs/2511.03772)
*Stergios Chatzikyriakidis,Dimitris Papadakis,Sevasti-Ioanna Papaioannou,Erofili Psaltaki*

Main category: cs.CL

TL;DR: 扩展希腊方言数据集GRDD+，新增6种方言变体，总规模达637万词，是迄今规模最大、变体最多的希腊方言数据集。通过微调实验验证高质量方言数据对LLMs的影响。


<details>
  <summary>Details</summary>
Motivation: 补充现有希腊方言数据集的不足，增加更多方言变体和数据量，为希腊方言研究提供更全面的资源。

Method: 扩展GRDD数据集，新增Cretan、Cypriot、Pontic、Northern Greek等方言数据，并加入Greco-Corsican、Griko、Maniot、Heptanesian、Tsakonian、Katharevusa等6种新变体。对Llama-3-8B、Llama-3.1-8B、Krikri-8B进行微调实验，并与前沿模型对比。

Result: 构建了包含10种方言变体、总计6,374,939词的数据集，是迄今规模最大、变体最多的希腊方言数据集。

Conclusion: 成功创建了首个具有如此规模和变体多样性的希腊方言数据集，为希腊方言的LLM研究提供了重要资源。

Abstract: We present an extended Greek Dialectal Dataset (GRDD+) 1that complements the
existing GRDD dataset with more data from Cretan, Cypriot, Pontic and Northern
Greek, while we add six new varieties: Greco-Corsican, Griko (Southern Italian
Greek), Maniot, Heptanesian, Tsakonian, and Katharevusa Greek. The result is a
dataset with total size 6,374,939 words and 10 varieties. This is the first
dataset with such variation and size to date. We conduct a number of
fine-tuning experiments to see the effect of good quality dialectal data on a
number of LLMs. We fine-tune three model architectures (Llama-3-8B,
Llama-3.1-8B, Krikri-8B) and compare the results to frontier models
(Claude-3.7-Sonnet, Gemini-2.5, ChatGPT-5).

</details>


### [4] [PLLuM: A Family of Polish Large Language Models](https://arxiv.org/abs/2511.03823)
*Jan Kocoń,Maciej Piasecki,Arkadiusz Janz,Teddy Ferdinan,Łukasz Radliński,Bartłomiej Koptyra,Marcin Oleksy,Stanisław Woźniak,Paweł Walkowiak,Konrad Wojtasik,Julia Moska,Tomasz Naskręt,Bartosz Walkowiak,Mateusz Gniewkowski,Kamil Szyc,Dawid Motyka,Dawid Banach,Jonatan Dalasiński,Ewa Rudnicka,Bartłomiej Alberski,Tomasz Walkowiak,Aleksander Szczęsny,Maciej Markiewicz,Tomasz Bernaś,Hubert Mazur,Kamil Żyta,Mateusz Tykierko,Grzegorz Chodak,Tomasz Kajdanowicz,Przemysław Kazienko,Agnieszka Karlińska,Karolina Seweryn,Anna Kołos,Maciej Chrabąszcz,Katarzyna Lorenc,Aleksandra Krasnodębska,Artur Wilczek,Katarzyna Dziewulska,Paula Betscher,Zofia Cieślińska,Katarzyna Kowol,Daria Mikoś,Maciej Trzciński,Dawid Krutul,Marek Kozłowski,Sławomir Dadas,Rafał Poświata,Michał Perełkiewicz,Małgorzata Grębowiec,Maciej Kazuła,Marcin Białas,Roman Roszko,Danuta Roszko,Jurgita Vaičenonienė,Andrius Utka,Paweł Levchuk,Paweł Kowalski,Irena Prawdzic-Jankowska,Maciej Ogrodniczuk,Monika Borys,Anna Bulińska,Wiktoria Gumienna,Witold Kieraś,Dorota Komosińska,Katarzyna Krasnowska-Kieraś,Łukasz Kobyliński,Martyna Lewandowska,Marek Łaziński,Mikołaj Łątkowski,Dawid Mastalerz,Beata Milewicz,Agnieszka Anna Mykowiecka,Angelika Peljak-Łapińska,Sandra Penno,Zuzanna Przybysz,Michał Rudolf,Piotr Rybak,Karolina Saputa,Aleksandra Tomaszewska,Aleksander Wawer,Marcin Woliński,Joanna Wołoszyn,Alina Wróblewska,Bartosz Żuk,Filip Żarnecki,Konrad Kaczyński,Anna Cichosz,Zuzanna Deckert,Monika Garnys,Izabela Grabarczyk,Wojciech Janowski,Sylwia Karasińska,Aleksandra Kujawiak,Piotr Misztela,Maria Szymańska,Karolina Walkusz,Igor Siek,Jakub Kwiatkowski,Piotr Pęzik*

Main category: cs.CL

TL;DR: PLLuM是专门为波兰语开发的最大开源基础模型家族，旨在解决英语主导的AI领域中其他语言支持不足的问题。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型主要关注英语，导致对其他语言的支持有限。PLLuM旨在为波兰语提供高质量、透明且文化相关的语言模型，超越以英语为中心的商业格局。

Method: 开发过程包括构建1400亿token的波兰语文本语料库、77k自定义指令数据集和100k偏好优化数据集，采用负责任AI框架，包含严格的数据治理和混合模块进行输出校正和安全过滤。

Result: PLLuM展示了在公共管理下游任务中的实用性，通过公开发布这些模型，旨在促进开放研究并加强波兰的主权AI技术。

Conclusion: PLLuM成功开发了专门针对波兰语的大型语言模型家族，填补了非英语语言AI技术的空白，为波兰的AI主权技术发展做出了贡献。

Abstract: Large Language Models (LLMs) play a central role in modern artificial
intelligence, yet their development has been primarily focused on English,
resulting in limited support for other languages. We present PLLuM (Polish
Large Language Model), the largest open-source family of foundation models
tailored specifically for the Polish language. Developed by a consortium of
major Polish research institutions, PLLuM addresses the need for high-quality,
transparent, and culturally relevant language models beyond the English-centric
commercial landscape. We describe the development process, including the
construction of a new 140-billion-token Polish text corpus for pre-training, a
77k custom instructions dataset, and a 100k preference optimization dataset. A
key component is a Responsible AI framework that incorporates strict data
governance and a hybrid module for output correction and safety filtering. We
detail the models' architecture, training procedures, and alignment techniques
for both base and instruction-tuned variants, and demonstrate their utility in
a downstream task within public administration. By releasing these models
publicly, PLLuM aims to foster open research and strengthen sovereign AI
technologies in Poland.

</details>


### [5] [STARS: Segment-level Token Alignment with Rejection Sampling in Large Language Models](https://arxiv.org/abs/2511.03827)
*Mohammad Atif Quamar,Mohammad Areeb,Mikhail Kuznetsov,Muslum Ozgur Ozmen,Z. Berkay Celik*

Main category: cs.CL

TL;DR: STARS是一种解码时算法，通过迭代采样、评分和拒绝/接受固定大小的token片段来引导模型生成，显著提高计算效率和对齐质量。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型对齐方法（如微调）计算成本高且效果不理想，而推理时方法如Best-of-N采样需要不切实际的计算量才能达到最佳对齐效果。

Method: STARS采用分段级token对齐与拒绝采样，在解码时迭代采样、评分和拒绝/接受短固定大小的token片段，实现生成路径的早期修正。

Result: 在六个LLM上的实验表明，STARS在胜率上比监督微调高出14.9个百分点，比直接偏好优化高出4.3个百分点，与强Best-of-N基线保持高度竞争力。

Conclusion: 基于奖励引导的细粒度采样是传统微调和全序列排序方法的一种通用、鲁棒且高效的替代方案，用于对齐大语言模型。

Abstract: Aligning large language models with human values is crucial for their safe
deployment; however, existing methods, such as fine-tuning, are computationally
expensive and suboptimal. In contrast, inference-time approaches like Best-of-N
sampling require practically infeasible computation to achieve optimal
alignment. We propose STARS: Segment-level Token Alignment with Rejection
Sampling, a decoding-time algorithm that steers model generation by iteratively
sampling, scoring, and rejecting/accepting short, fixed-size token segments.
This allows for early correction of the generation path, significantly
improving computational efficiency and boosting alignment quality. Across a
suite of six LLMs, we show that STARS outperforms Supervised Fine-Tuning (SFT)
by up to 14.9 percentage points and Direct Preference Optimization (DPO) by up
to 4.3 percentage points on win-rates, while remaining highly competitive with
strong Best-of-N baselines. Our work establishes granular, reward-guided
sampling as a generalizable, robust, and efficient alternative to traditional
fine-tuning and full-sequence ranking methods for aligning LLMs.

</details>


### [6] [Divide, Cache, Conquer: Dichotomic Prompting for Efficient Multi-Label LLM-Based Classification](https://arxiv.org/abs/2511.03830)
*Mikołaj Langner,Jan Eliasz,Ewa Rudnicka,Jan Kocoń*

Main category: cs.CL

TL;DR: 提出一种基于二分决策序列的高效多标签文本分类方法，通过将分类任务分解为独立的yes/no查询，结合前缀缓存机制，在保持准确性的同时显著提升短文本推理效率。


<details>
  <summary>Details</summary>
Motivation: 解决传统多标签分类方法在大语言模型中效率低下的问题，特别是在短文本推理场景下，需要一种既能保持准确性又能提高效率的解决方案。

Method: 将多标签分类任务重新表述为二分决策序列，每个目标维度独立查询，结合前缀缓存机制；使用LLM-to-SLM蒸馏技术，通过强大的标注模型生成多标注数据来微调较小模型。

Result: 微调后的模型在零样本基线上显示出显著改进，特别是在训练过程中见过的维度上表现更好；该方法在情感文本分析的24个维度上得到验证。

Conclusion: 将多标签分类分解为二分查询，结合蒸馏和缓存感知推理，为基于LLM的分类提供了一个可扩展且有效的框架，该方法虽然以情感状态验证，但具有跨领域适用性。

Abstract: We introduce a method for efficient multi-label text classification with
large language models (LLMs), built on reformulating classification tasks as
sequences of dichotomic (yes/no) decisions. Instead of generating all labels in
a single structured response, each target dimension is queried independently,
which, combined with a prefix caching mechanism, yields substantial efficiency
gains for short-text inference without loss of accuracy. To demonstrate the
approach, we focus on affective text analysis, covering 24 dimensions including
emotions and sentiment. Using LLM-to-SLM distillation, a powerful annotator
model (DeepSeek-V3) provides multiple annotations per text, which are
aggregated to fine-tune smaller models (HerBERT-Large, CLARIN-1B, PLLuM-8B,
Gemma3-1B). The fine-tuned models show significant improvements over zero-shot
baselines, particularly on the dimensions seen during training. Our findings
suggest that decomposing multi-label classification into dichotomic queries,
combined with distillation and cache-aware inference, offers a scalable and
effective framework for LLM-based classification. While we validate the method
on affective states, the approach is general and applicable across domains.

</details>


### [7] [Evaluating Machine Translation Datasets for Low-Web Data Languages: A Gendered Lens](https://arxiv.org/abs/2511.03880)
*Hellina Hailu Nigatu,Bethelhem Yemane Mamo,Bontu Fufa Balcha,Debora Taye Tesfaye,Elbethel Daniel Zewdie,Ikram Behiru Nesiru,Jitu Ewnetu Hailu,Senait Mengesha Yayo*

Main category: cs.CL

TL;DR: 该研究分析了三种低资源语言（阿法尔奥罗莫语、阿姆哈拉语、提格里尼亚语）机器翻译数据集的质量，重点关注性别代表性，发现数据集中存在严重的性别偏见和有害内容。


<details>
  <summary>Details</summary>
Motivation: 随着低资源语言被纳入NLP研究，大规模数据收集往往重数量轻质量，可能导致技术表现不佳和产生有害内容，因此需要评估数据集质量。

Method: 研究调查了三种低资源语言的机器翻译数据集，分析其性别代表性，包括人名、动词的语法性别和刻板印象描绘。

Result: 发现训练数据主要包含政治和宗教领域文本，而基准数据集聚焦于新闻、健康和体育；数据集中存在严重的男性偏向，以及针对女性的有害和有毒描绘，数据量最大的语言问题最突出。

Conclusion: 数量不能保证质量，希望研究能促进对低资源语言数据集的进一步调查，并及早减轻有害内容。

Abstract: As low-resourced languages are increasingly incorporated into NLP research,
there is an emphasis on collecting large-scale datasets. But in prioritizing
quantity over quality, we risk 1) building language technologies that perform
poorly for these languages and 2) producing harmful content that perpetuates
societal biases. In this paper, we investigate the quality of Machine
Translation (MT) datasets for three low-resourced languages--Afan Oromo,
Amharic, and Tigrinya, with a focus on the gender representation in the
datasets. Our findings demonstrate that while training data has a large
representation of political and religious domain text, benchmark datasets are
focused on news, health, and sports. We also found a large skew towards the
male gender--in names of persons, the grammatical gender of verbs, and in
stereotypical depictions in the datasets. Further, we found harmful and toxic
depictions against women, which were more prominent for the language with the
largest amount of data, underscoring that quantity does not guarantee quality.
We hope that our work inspires further inquiry into the datasets collected for
low-resourced languages and prompts early mitigation of harmful content.
WARNING: This paper contains discussion of NSFW content that some may find
disturbing.

</details>


### [8] [GRAD: Graph-Retrieved Adaptive Decoding for Hallucination Mitigation](https://arxiv.org/abs/2511.03900)
*Manh Nguyen,Sunil Gupta,Dai Do,Hung Le*

Main category: cs.CL

TL;DR: GRAD是一种解码时方法，通过构建稀疏标记转移图来缓解LLM幻觉问题，无需重新训练模型，在多个问答基准测试中显著提升准确性和事实性。


<details>
  <summary>Details</summary>
Motivation: 解决LLM幻觉问题，现有方法依赖外部知识源但存在脆弱性和高成本问题，受知识图谱启发寻求更轻量级的解决方案。

Method: 构建稀疏标记转移图，在解码时将图检索的对数概率与模型对数概率自适应融合，偏向高证据延续同时保持流畅性。

Result: 在三个模型和多个问答基准测试中，GRAD持续超越基线方法，内在准确率提升9.7%，幻觉率降低8.6%，正确性提高6.9%。

Conclusion: GRAD提供了一种轻量级、即插即用的替代方案，证明语料级标记转移的统计证据能有效引导生成更真实可验证的输出。

Abstract: Hallucination mitigation remains a persistent challenge for large language
models (LLMs), even as model scales grow. Existing approaches often rely on
external knowledge sources, such as structured databases or knowledge graphs,
accessed through prompting or retrieval. However, prompt-based grounding is
fragile and domain-sensitive, while symbolic knowledge integration incurs heavy
retrieval and formatting costs. Motivated by knowledge graphs, we introduce
Graph-Retrieved Adaptive Decoding (GRAD), a decoding-time method that grounds
generation in corpus-derived evidence without retraining. GRAD constructs a
sparse token transition graph by accumulating next-token logits across a small
retrieved corpus in a single forward pass. During decoding, graph-retrieved
logits are max-normalized and adaptively fused with model logits to favor
high-evidence continuations while preserving fluency. Across three models and a
range of question-answering benchmarks spanning intrinsic, extrinsic
hallucination, and factuality tasks, GRAD consistently surpasses baselines,
achieving up to 9.7$\%$ higher intrinsic accuracy, 8.6$\%$ lower hallucination
rates, and 6.9$\%$ greater correctness compared to greedy decoding, while
attaining the highest truth--informativeness product score among all methods.
GRAD offers a lightweight, plug-and-play alternative to contrastive decoding
and knowledge graph augmentation, demonstrating that statistical evidence from
corpus-level token transitions can effectively steer generation toward more
truthful and verifiable outputs.

</details>


### [9] [Context informs pragmatic interpretation in vision-language models](https://arxiv.org/abs/2511.03908)
*Alvin Wei Ming Tan,Ben Prystawski,Veronica Boyce,Michael C. Frank*

Main category: cs.CL

TL;DR: 论文测试了人类和视觉语言模型在迭代参考游戏中的表现，发现模型在无相关上下文时表现差于人类，但在有相关上下文时性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 研究迭代参考游戏作为测试智能体在多轮语言环境中进行上下文敏感语用推理能力的测试案例。

Method: 在迭代参考游戏中测试人类和视觉语言模型，通过改变上下文的数量、顺序和相关性来评估性能。

Result: 无相关上下文时，模型表现高于随机但远差于人类；有相关上下文时，模型性能随试验次数显著提升。

Conclusion: 带有抽象指称物的少样本参考游戏对机器学习模型来说仍然是一个困难任务。

Abstract: Iterated reference games - in which players repeatedly pick out novel
referents using language - present a test case for agents' ability to perform
context-sensitive pragmatic reasoning in multi-turn linguistic environments. We
tested humans and vision-language models on trials from iterated reference
games, varying the given context in terms of amount, order, and relevance.
Without relevant context, models were above chance but substantially worse than
humans. However, with relevant context, model performance increased
dramatically over trials. Few-shot reference games with abstract referents
remain a difficult task for machine learning models.

</details>


### [10] [The Human Flourishing Geographic Index: A County-Level Dataset for the United States, 2013--2023](https://arxiv.org/abs/2511.03915)
*Stefano M. Iacus,Devika Jain,Andrea Nasuto,Giuseppe Porro,Marcello Carammia,Andrea Vezzulli*

Main category: cs.CL

TL;DR: 开发了人类繁荣地理指数(HFGI)，通过分析26亿条美国推文来量化多维的人类繁荣概念，提供县级和州级的高时空分辨率数据。


<details>
  <summary>Details</summary>
Motivation: 现有的人类繁荣测量方法缺乏精细的时空分辨率，需要超越经济指标来理解社会福祉。

Method: 使用微调的大型语言模型分析2013-2023年约26亿条地理定位的美国推文，根据哈佛全球繁荣研究框架分类48个指标的表达。

Result: 创建了包含县级和州级月度、年度指标的数据集，验证显示这些指标准确反映了基础构念，并与既有指标呈现预期相关性。

Conclusion: 该资源为福祉、不平等和社会变革的多学科分析提供了前所未有的分辨率，揭示了美国社交媒体话语中反映的人类繁荣动态。

Abstract: Quantifying human flourishing, a multidimensional construct including
happiness, health, purpose, virtue, relationships, and financial stability, is
critical for understanding societal well-being beyond economic indicators.
Existing measures often lack fine spatial and temporal resolution. Here we
introduce the Human Flourishing Geographic Index (HFGI), derived from analyzing
approximately 2.6 billion geolocated U.S. tweets (2013-2023) using fine-tuned
large language models to classify expressions across 48 indicators aligned with
Harvard's Global Flourishing Study framework plus attitudes towards migration
and perception of corruption. The dataset offers monthly and yearly county- and
state-level indicators of flourishing-related discourse, validated to confirm
that the measures accurately represent the underlying constructs and show
expected correlations with established indicators. This resource enables
multidisciplinary analyses of well-being, inequality, and social change at
unprecedented resolution, offering insights into the dynamics of human
flourishing as reflected in social media discourse across the United States
over the past decade.

</details>


### [11] [Direct Semantic Communication Between Large Language Models via Vector Translation](https://arxiv.org/abs/2511.03945)
*Fu-Chun Yang,Jason Eshraghian*

Main category: cs.CL

TL;DR: 提出了一种通过向量翻译实现跨模型语义交换的方法，避免了传统基于token的消息传递方式，减少了计算开销并提升了信息传输效率。


<details>
  <summary>Details</summary>
Motivation: 在多智能体设置中，LLMs通常通过token传递消息，这丢弃了大部分潜在语义，限制了信息传输并增加了不必要的计算开销。

Method: 使用学习的映射构建潜在桥梁，通过向量翻译实现表示空间之间的直接语义交换。训练了Llama-2-7B和Mistral-7B-Instruct之间的双编码器翻译器。

Result: 获得平均余弦对齐度0.538，以30%混合强度注入翻译向量可以引导目标模型生成而不破坏对数概率。双向评估显示2.01:1的传输不对称性。

Conclusion: 保守注入保持了计算稳定性，证明跨模型潜在通信是可行的，使协作AI系统能够共享意义而非token。

Abstract: In multi-agent settings, such as debate, reflection, or tool-calling, large
language models (LLMs) pass messages as plain tokens, discarding most latent
semantics. This constrains information transfer and adds unnecessary
computational overhead. We form a latent bridge via vector translations, which
use learned mappings that enable direct semantic exchange between
representation spaces. A dual-encoder translator trained between Llama-2-7B and
Mistral-7B-Instruct attains an average cosine alignment of 0.538. Injecting the
translated vectors at 30 percent blending strength steers the target model's
generation without destabilizing logits. Bidirectional evaluation shows a
2.01:1 transfer asymmetry, indicating that general-purpose models yield more
transferable representations than instruction-tuned variants. This conservative
injection preserves computational stability while demonstrating that
cross-model latent communication is feasible, enabling collaborative AI systems
that share meaning rather than tokens.

</details>


### [12] [Abductive Inference in Retrieval-Augmented Language Models: Generating and Validating Missing Premises](https://arxiv.org/abs/2511.04020)
*Shiyin Lin*

Main category: cs.CL

TL;DR: 提出了一个将溯因推理集成到检索增强LLMs中的框架，通过检测证据不足、生成候选缺失前提并进行验证，提高了RAG系统在知识密集型任务中的准确性和推理可信度。


<details>
  <summary>Details</summary>
Motivation: 现有的检索增强生成(RAG)管道在检索证据不完整时往往失败，导致推理过程中出现空白。溯因推理作为一种填补这些空白的原理性方法，可以增强RAG系统的鲁棒性和可解释性。

Method: 提出一个集成溯因推理的框架，包括检测证据不足、生成候选缺失前提，以及通过一致性和合理性检查来验证这些前提。

Result: 在溯因推理和多跳问答基准测试上的实验结果表明，该方法提高了答案准确性和推理可信度。

Conclusion: 溯因推理是增强RAG系统鲁棒性和可解释性的一个有前景的方向。

Abstract: Large Language Models (LLMs) enhanced with retrieval -- commonly referred to
as Retrieval-Augmented Generation (RAG) -- have demonstrated strong performance
in knowledge-intensive tasks. However, RAG pipelines often fail when retrieved
evidence is incomplete, leaving gaps in the reasoning process. In such cases,
\emph{abductive inference} -- the process of generating plausible missing
premises to explain observations -- offers a principled approach to bridge
these gaps. In this paper, we propose a framework that integrates abductive
inference into retrieval-augmented LLMs. Our method detects insufficient
evidence, generates candidate missing premises, and validates them through
consistency and plausibility checks. Experimental results on abductive
reasoning and multi-hop QA benchmarks show that our approach improves both
answer accuracy and reasoning faithfulness. This work highlights abductive
inference as a promising direction for enhancing the robustness and
explainability of RAG systems.

</details>


### [13] [WST: Weakly Supervised Transducer for Automatic Speech Recognition](https://arxiv.org/abs/2511.04035)
*Dongji Gao,Chenda Liao,Changliang Liu,Matthew Wiesner,Leibny Paola Garcia,Daniel Povey,Sanjeev Khudanpur,Jian Wu*

Main category: cs.CL

TL;DR: 提出了弱监督转换器（WST），能够在转录错误率高达70%的情况下保持性能，优于现有的基于CTC的弱监督方法。


<details>
  <summary>Details</summary>
Motivation: RNN-T在端到端语音识别中依赖大规模高质量标注数据，但这些数据成本高且难以获取，需要减少对此类数据的依赖。

Method: 设计了一个灵活的训练图，能够鲁棒地处理转录中的错误，无需额外的置信度估计或辅助预训练模型。

Result: 在合成和工业数据集上的实验表明，WST在高达70%的转录错误率下仍能有效保持性能，一致优于BTC和OTC等现有方法。

Conclusion: WST在实际ASR场景中具有实用性和鲁棒性，代码将公开提供。

Abstract: The Recurrent Neural Network-Transducer (RNN-T) is widely adopted in
end-to-end (E2E) automatic speech recognition (ASR) tasks but depends heavily
on large-scale, high-quality annotated data, which are often costly and
difficult to obtain. To mitigate this reliance, we propose a Weakly Supervised
Transducer (WST), which integrates a flexible training graph designed to
robustly handle errors in the transcripts without requiring additional
confidence estimation or auxiliary pre-trained models. Empirical evaluations on
synthetic and industrial datasets reveal that WST effectively maintains
performance even with transcription error rates of up to 70%, consistently
outperforming existing Connectionist Temporal Classification (CTC)-based weakly
supervised approaches, such as Bypass Temporal Classification (BTC) and
Omni-Temporal Classification (OTC). These results demonstrate the practical
utility and robustness of WST in realistic ASR settings. The implementation
will be publicly available.

</details>


### [14] [T-FIX: Text-Based Explanations with Features Interpretable to eXperts](https://arxiv.org/abs/2511.04070)
*Shreya Havaldar,Helen Jin,Chaehyeon Kim,Anton Xue,Weiqiu You,Marco Gatti,Bhuvnesh Jain,Helen Qu,Daniel A Hashimoto,Amin Madani,Rajat Deo,Sameed Ahmed M. Khatana,Gary E. Weissman,Lyle Ungar,Eric Wong*

Main category: cs.CL

TL;DR: 提出了T-FIX基准，用于评估LLM在知识密集型领域中的解释与专家直觉的对齐程度，开发了新的度量标准来衡量LLM解释与专家判断的一致性。


<details>
  <summary>Details</summary>
Motivation: 当前评估方案主要关注解释的合理性或内部忠实性，未能捕捉解释内容是否真正符合专家直觉，特别是在知识密集型领域（如手术、天文学、治疗）中，用户需要反映专家级推理的解释。

Method: 与领域专家合作，开发了新的度量标准来测量LLM解释与专家判断的对齐程度，构建了T-FIX基准，涵盖七个知识密集型领域。

Result: 提出了专家对齐作为评估解释的标准，并开发了相应的评估框架和度量方法。

Conclusion: 需要更关注LLM解释与专家直觉的对齐性，T-FIX基准为此提供了评估工具和方法。

Abstract: As LLMs are deployed in knowledge-intensive settings (e.g., surgery,
astronomy, therapy), users expect not just answers, but also meaningful
explanations for those answers. In these settings, users are often domain
experts (e.g., doctors, astrophysicists, psychologists) who require
explanations that reflect expert-level reasoning. However, current evaluation
schemes primarily emphasize plausibility or internal faithfulness of the
explanation, which fail to capture whether the content of the explanation truly
aligns with expert intuition. We formalize expert alignment as a criterion for
evaluating explanations with T-FIX, a benchmark spanning seven
knowledge-intensive domains. In collaboration with domain experts, we develop
novel metrics to measure the alignment of LLM explanations with expert
judgment.

</details>


### [15] [Plan of Knowledge: Retrieval-Augmented Large Language Models for Temporal Knowledge Graph Question Answering](https://arxiv.org/abs/2511.04072)
*Xinying Qian,Ying Zhang,Yu Zhao,Baohang Zhou,Xuhui Sui,Xiaojie Yuan*

Main category: cs.CL

TL;DR: 提出了PoK框架，通过知识规划和对比时序检索器来增强LLMs在时序知识图谱问答中的能力，显著提升了检索精度和推理准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能充分理解时间约束的复杂语义信息，而LLMs虽然具有强大的语义理解和推理泛化能力，但在时序推理方面存在幻觉和知识缺失的问题。

Method: 提出PoK框架，包含知识规划模块将复杂时序问题分解为子目标序列，以及时序知识存储库配合对比检索框架选择性检索语义和时间对齐的事实。

Result: 在四个基准TKGQA数据集上的实验表明，PoK显著提升了LLMs的检索精度和推理准确性，最高超越最先进方法56.0%。

Conclusion: 通过结合结构化规划和时序知识检索，PoK有效增强了时序推理的可解释性和事实一致性，为时序知识图谱问答提供了新的解决方案。

Abstract: Temporal Knowledge Graph Question Answering (TKGQA) aims to answer
time-sensitive questions by leveraging factual information from Temporal
Knowledge Graphs (TKGs). While previous studies have employed pre-trained TKG
embeddings or graph neural networks to inject temporal knowledge, they fail to
fully understand the complex semantic information of time constraints.
Recently, Large Language Models (LLMs) have shown remarkable progress,
benefiting from their strong semantic understanding and reasoning
generalization capabilities. However, their temporal reasoning ability remains
limited. LLMs frequently suffer from hallucination and a lack of knowledge. To
address these limitations, we propose the Plan of Knowledge framework with a
contrastive temporal retriever, which is named PoK. Specifically, the proposed
Plan of Knowledge module decomposes a complex temporal question into a sequence
of sub-objectives from the pre-defined tools, serving as intermediate guidance
for reasoning exploration. In parallel, we construct a Temporal Knowledge Store
(TKS) with a contrastive retrieval framework, enabling the model to selectively
retrieve semantically and temporally aligned facts from TKGs. By combining
structured planning with temporal knowledge retrieval, PoK effectively enhances
the interpretability and factual consistency of temporal reasoning. Extensive
experiments on four benchmark TKGQA datasets demonstrate that PoK significantly
improves the retrieval precision and reasoning accuracy of LLMs, surpassing the
performance of the state-of-the-art TKGQA methods by 56.0% at most.

</details>


### [16] [The truth is no diaper: Human and AI-generated associations to emotional words](https://arxiv.org/abs/2511.04077)
*Špela Vintar,Jan Jona Javoršek*

Main category: cs.CL

TL;DR: 比较人类与大型语言模型在情感词汇联想上的行为差异，发现LLMs的联想更可预测、缺乏创造性，且会放大刺激词的情感负荷


<details>
  <summary>Details</summary>
Motivation: 探索人类与大型语言模型在词汇联想行为上的相似性，特别是对情感词汇的联想机制，以了解LLMs是否能像人类一样形成创造性联想

Method: 通过比较人类参与者和大型语言模型对情感词汇的联想反应，分析两者在联想内容、情感强度和创造性方面的差异

Result: 人类与LLMs的联想重叠度中等，但LLMs的联想更可预测、缺乏创造性，且倾向于放大刺激词的情感负荷

Conclusion: 大型语言模型在词汇联想行为上与人类存在显著差异，其联想更模式化、情感放大化，缺乏人类联想的创造性和个体多样性

Abstract: Human word associations are a well-known method of gaining insight into the
internal mental lexicon, but the responses spontaneously offered by human
participants to word cues are not always predictable as they may be influenced
by personal experience, emotions or individual cognitive styles. The ability to
form associative links between seemingly unrelated concepts can be the driving
mechanisms of creativity. We perform a comparison of the associative behaviour
of humans compared to large language models. More specifically, we explore
associations to emotionally loaded words and try to determine whether large
language models generate associations in a similar way to humans. We find that
the overlap between humans and LLMs is moderate, but also that the associations
of LLMs tend to amplify the underlying emotional load of the stimulus, and that
they tend to be more predictable and less creative than human ones.

</details>


### [17] [Improving the Performance of Radiology Report De-identification with Large-Scale Training and Benchmarking Against Cloud Vendor Methods](https://arxiv.org/abs/2511.04079)
*Eva Prakash,Maayane Attias,Pierre Chambon,Justin Xu,Steven Truong,Jean-Benoit Delbrouck,Tessa Cook,Curtis Langlotz*

Main category: cs.CL

TL;DR: 基于Transformer的医疗报告去标识化模型，通过大规模多模态训练，在PHI检测方面超越了现有学术和商业系统，建立了临床文本安全处理的新基准。


<details>
  <summary>Details</summary>
Motivation: 提升放射学报告的自动去标识化能力，通过扩展训练数据集和与商业云供应商系统进行性能比较，以改进受保护健康信息(PHI)的检测效果。

Method: 在现有最先进的基于Transformer的PHI去标识化流程基础上，使用斯坦福大学两个大型标注放射学语料库进行微调，引入额外的PHI类别(AGE)，并采用"隐藏于众目睽睽之下"方法评估合成PHI生成的稳定性。

Result: 模型在Penn数据集上获得0.973的总体F1分数，在斯坦福数据集上获得0.996的F1分数，优于或保持了先前最先进模型的性能。在50个独立去标识化的Penn数据集上，合成PHI评估显示一致的检测能力(总体F1: 0.959)。模型在所有供应商系统上表现更优(总体F1: 0.960 vs. 0.632-0.754)。

Conclusion: 基于Transformer的去标识化模型在多样化的放射学数据集上训练，在PHI检测方面超越了先前的学术和商业系统，为安全的临床文本处理建立了新的基准。

Abstract: Objective: To enhance automated de-identification of radiology reports by
scaling transformer-based models through extensive training datasets and
benchmarking performance against commercial cloud vendor systems for protected
health information (PHI) detection. Materials and Methods: In this
retrospective study, we built upon a state-of-the-art, transformer-based, PHI
de-identification pipeline by fine-tuning on two large annotated radiology
corpora from Stanford University, encompassing chest X-ray, chest CT,
abdomen/pelvis CT, and brain MR reports and introducing an additional PHI
category (AGE) into the architecture. Model performance was evaluated on test
sets from Stanford and the University of Pennsylvania (Penn) for token-level
PHI detection. We further assessed (1) the stability of synthetic PHI
generation using a "hide-in-plain-sight" method and (2) performance against
commercial systems. Precision, recall, and F1 scores were computed across all
PHI categories. Results: Our model achieved overall F1 scores of 0.973 on the
Penn dataset and 0.996 on the Stanford dataset, outperforming or maintaining
the previous state-of-the-art model performance. Synthetic PHI evaluation
showed consistent detectability (overall F1: 0.959 [0.958-0.960]) across 50
independently de-identified Penn datasets. Our model outperformed all vendor
systems on synthetic Penn reports (overall F1: 0.960 vs. 0.632-0.754).
Discussion: Large-scale, multimodal training improved cross-institutional
generalization and robustness. Synthetic PHI generation preserved data utility
while ensuring privacy. Conclusion: A transformer-based de-identification model
trained on diverse radiology datasets outperforms prior academic and commercial
systems in PHI detection and establishes a new benchmark for secure clinical
text processing.

</details>


### [18] [A Characterization of List Language Identification in the Limit](https://arxiv.org/abs/2511.04103)
*Moses Charikar,Chirag Pabbaraju,Ambuj Tewari*

Main category: cs.CL

TL;DR: 本文研究了k列表语言识别问题，给出了k列表识别的精确特征描述，并建立了统计设置下的识别率界限


<details>
  <summary>Details</summary>
Motivation: 经典的语言识别在极限下是不可能的，但最近在语言生成方面的积极结果促使重新审视这个问题，通过给学习者提供每次输出k个猜测的能力来扩展识别能力

Method: 基于Angluin特征描述的递归版本，提出了k列表识别的精确特征描述，并建立了与统计学习理论的联系

Result: 给出了k列表识别的精确特征：语言集合可以被k列表识别当且仅当它可以分解为k个可识别的子集合；在统计设置下，可k列表识别的集合可以以指数速率识别，而不可识别的集合则无法以任何趋于零的速率识别

Conclusion: k列表识别为语言识别提供了新的可能性，建立了可识别性与识别速率之间的紧密联系，扩展了经典语言识别理论

Abstract: We study the problem of language identification in the limit, where given a
sequence of examples from a target language, the goal of the learner is to
output a sequence of guesses for the target language such that all the guesses
beyond some finite time are correct. Classical results of Gold showed that
language identification in the limit is impossible for essentially any
interesting collection of languages. Later, Angluin gave a precise
characterization of language collections for which this task is possible.
Motivated by recent positive results for the related problem of language
generation, we revisit the classic language identification problem in the
setting where the learner is given the additional power of producing a list of
$k$ guesses at each time step. The goal is to ensure that beyond some finite
time, one of the guesses is correct at each time step.
  We give an exact characterization of collections of languages that can be
$k$-list identified in the limit, based on a recursive version of Angluin's
characterization (for language identification with a list of size $1$). This
further leads to a conceptually appealing characterization: A language
collection can be $k$-list identified in the limit if and only if the
collection can be decomposed into $k$ collections of languages, each of which
can be identified in the limit (with a list of size $1$). We also use our
characterization to establish rates for list identification in the statistical
setting where the input is drawn as an i.i.d. stream from a distribution
supported on some language in the collection. Our results show that if a
collection is $k$-list identifiable in the limit, then the collection can be
$k$-list identified at an exponential rate, and this is best possible. On the
other hand, if a collection is not $k$-list identifiable in the limit, then it
cannot be $k$-list identified at any rate that goes to zero.

</details>


### [19] [Batch Prompting Suppresses Overthinking Reasoning Under Constraint: How Batch Prompting Suppresses Overthinking in Reasoning Models](https://arxiv.org/abs/2511.04108)
*Wenmo Qiu,Saurabh Srivastava*

Main category: cs.CL

TL;DR: 批处理提示不仅能降低推理成本，还能作为推理时正则化器，提高大型推理模型的准确性和效率，减少过度思考和使用犹豫语言，并出现集体效应。


<details>
  <summary>Details</summary>
Motivation: 探索批处理在大型语言模型中的额外好处，特别是作为多步推理的正则化机制。

Method: 在13个不同基准上进行全面研究，分析批处理对模型行为的影响，包括准确性、推理令牌使用、过度思考和集体效应。

Result: 批处理提高了准确性，同时大幅减少推理令牌使用（通常3-5倍），抑制过度思考，减少犹豫语言，并出现集体效应模式。

Conclusion: 批处理不仅是吞吐量优化工具，更是强大的推理时正则化器，能实现更高效可靠的LLM推理。

Abstract: Recent work has explored batch prompting as a strategy to amortize inference
cost in large language models (LLMs). In this paper, we show that batching
offers an additional, underappreciated benefit: it regularizes model behavior
during multi-step reasoning for Large Reasoning Models (LRMs). We conduct a
comprehensive study across 13 diverse benchmarks and observe that batching
improves accuracy while substantially reducing reasoning token usage, often by
3x-5x. Through detailed behavioral analysis, we find that batching suppresses
overthinking, reduces hedging language (e.g., repetitive self-corrections), and
encourages more decisive answers. Surprisingly, we also observe emergent
collective effects in batched inference: models often generalize patterns from
earlier examples to solve harder ones in the same batch. These findings
position batching not just as a throughput optimization, but as a powerful
inference-time regularizer for more efficient and reliable LLM reasoning.

</details>


### [20] [RIDE: Difficulty Evolving Perturbation with Item Response Theory for Mathematical Reasoning](https://arxiv.org/abs/2511.04120)
*Xinyuan Li,Murong Xu,Wenbiao Tao,Hanlun Zhu,Yike Zhao,Jipeng Zhang,Yunshi Lan*

Main category: cs.CL

TL;DR: RIDE是一个基于项目反应理论的对抗性问题重写框架，通过生成难度可控的数学问题变体来评估LLMs的真实数学推理能力，而非表面模式匹配。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在数学推理上的高表现可能源于训练数据泄露或表面模式匹配，而非真正的推理能力。需要对抗性评估方法来衡量真实的数学推理能力。

Method: 使用项目反应理论(IRT)测量问题难度，利用35个LLMs模拟学生构建难度排序器，在强化学习中使用该排序器作为奖励信号，指导问题重写模型生成不同难度的问题变体。

Result: 在竞赛级数学基准测试中，RIDE生成的扰动版本使先进LLMs性能平均下降21.73%（26个模型），暴露了数学推理的有限鲁棒性。

Conclusion: RIDE框架有效评估了LLMs的真实数学推理能力，证实了当前模型在数学推理鲁棒性方面的局限性。

Abstract: Large language models (LLMs) achieve high performance on mathematical
reasoning, but these results can be inflated by training data leakage or
superficial pattern matching rather than genuine reasoning. To this end, an
adversarial perturbation-based evaluation is needed to measure true
mathematical reasoning ability. Current rule-based perturbation methods often
generate ill-posed questions and impede the systematic evaluation of question
difficulty and the evolution of benchmarks. To bridge this gap, we propose
RIDE, a novel adversarial question-rewriting framework that leverages Item
Response Theory (IRT) to rigorously measure question difficulty and to generate
intrinsically more challenging, well-posed variations of mathematical problems.
We employ 35 LLMs to simulate students and build a difficulty ranker from their
responses. This ranker provides a reward signal during reinforcement learning
and guides a question-rewriting model to reformulate existing questions across
difficulty levels. Applying RIDE to competition-level mathematical benchmarks
yields perturbed versions that degrade advanced LLM performance, with
experiments showing an average 21.73% drop across 26 models, thereby exposing
limited robustness in mathematical reasoning and confirming the validity of our
evaluation approach.

</details>


### [21] [CantoASR: Prosody-Aware ASR-LALM Collaboration for Low-Resource Cantonese](https://arxiv.org/abs/2511.04139)
*Dazhong Chen,Yi-Cheng Lin,Yuchen Huang,Ziwei Gong,Di Jiang,Zeying Xie,Yi R.,Fung*

Main category: cs.CL

TL;DR: CantoASR是一个协作式ASR-LALM错误纠正框架，通过整合强制对齐、LoRA微调的Whisper和指令调优的Qwen-Audio，显著提升了低资源粤语语音识别的准确性。


<details>
  <summary>Details</summary>
Motivation: 粤语作为低资源语言，面临标注数据有限、六种声调、变调和口音变化等挑战，现有ASR模型如Whisper在粤语识别上词错误率较高。

Method: 结合强制对齐进行声学特征提取，使用LoRA微调的Whisper提升声调辨别能力，采用指令调优的Qwen-Audio进行韵律感知的错误纠正。

Result: 在自发粤语数据上的评估显示，相比Whisper-Large-V3在CER指标上取得了显著提升。

Conclusion: 将声学线索与LALM推理相结合，为低资源声调语言和方言的ASR提供了一种可扩展的策略。

Abstract: Automatic speech recognition (ASR) is critical for language accessibility,
yet low-resource Cantonese remains challenging due to limited annotated data,
six lexical tones, tone sandhi, and accent variation. Existing ASR models, such
as Whisper, often suffer from high word error rates. Large audio-language
models (LALMs), in contrast, can leverage broader contextual reasoning but
still require explicit tonal and prosodic acoustic cues. We introduce CantoASR,
a collaborative ASR-LALM error correction framework that integrates forced
alignment for acoustic feature extraction, a LoRA-finetuned Whisper for
improved tone discrimination, and an instruction-tuned Qwen-Audio for
prosody-aware correction. Evaluations on spontaneous Cantonese data show
substantial CER gains over Whisper-Large-V3. These findings suggest that
integrating acoustic cues with LALM reasoning provides a scalable strategy for
low-resource tonal and dialectal ASR.

</details>


### [22] [BAPPA: Benchmarking Agents, Plans, and Pipelines for Automated Text-to-SQL Generation](https://arxiv.org/abs/2511.04153)
*Fahim Ahmed,Md Mubtasim Ahasan,Jahir Sadik Monon,Muntasir Wahed,M Ashraful Amin,A K M Mahbubur Rahman,Amin Ahsan Ali*

Main category: cs.CL

TL;DR: 本文探索了三种多智能体LLM流水线来提升Text-to-SQL性能，发现多智能体讨论能显著提升小模型表现，其中LLM规划器-编码器流水线效果最佳，可将准确率从52.4%提升至56.4%。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在从自然语言生成SQL时面临模式规模大和复杂推理的挑战，先前工作主要关注复杂不实用的旗舰模型流水线，而忽略了更小更高效的模型。

Method: 提出了三种多智能体LLM流水线：(1)多智能体讨论流水线，智能体迭代批判和优化SQL查询，由法官合成最终答案；(2)规划器-编码器流水线，思考模型规划器生成逐步SQL生成计划，编码器合成查询；(3)编码器-聚合器流水线，多个编码器独立生成SQL查询，推理智能体选择最佳查询。

Result: 在Bird-Bench Mini-Dev数据集上的实验表明，多智能体讨论能提升小模型性能，Qwen2.5-7b-Instruct在三轮讨论后执行准确率提升10.6%。LLM规划器-编码器流水线效果最好，DeepSeek-R1-32B和QwQ-32B规划器将Gemma 3 27B IT准确率从52.4%提升至56.4%。

Conclusion: 多智能体方法能有效提升Text-to-SQL性能，特别是对小模型效果显著，LLM规划器-编码器流水线是最佳方案，为高效SQL生成提供了实用解决方案。

Abstract: Text-to-SQL systems provide a natural language interface that can enable even
laymen to access information stored in databases. However, existing Large
Language Models (LLM) struggle with SQL generation from natural instructions
due to large schema sizes and complex reasoning. Prior work often focuses on
complex, somewhat impractical pipelines using flagship models, while smaller,
efficient models remain overlooked. In this work, we explore three multi-agent
LLM pipelines, with systematic performance benchmarking across a range of small
to large open-source models: (1) Multi-agent discussion pipeline, where agents
iteratively critique and refine SQL queries, and a judge synthesizes the final
answer; (2) Planner-Coder pipeline, where a thinking model planner generates
stepwise SQL generation plans and a coder synthesizes queries; and (3)
Coder-Aggregator pipeline, where multiple coders independently generate SQL
queries, and a reasoning agent selects the best query. Experiments on the
Bird-Bench Mini-Dev set reveal that Multi-Agent discussion can improve small
model performance, with up to 10.6% increase in Execution Accuracy for
Qwen2.5-7b-Instruct seen after three rounds of discussion. Among the pipelines,
the LLM Reasoner-Coder pipeline yields the best results, with DeepSeek-R1-32B
and QwQ-32B planners boosting Gemma 3 27B IT accuracy from 52.4% to the highest
score of 56.4%. Codes are available at
https://github.com/treeDweller98/bappa-sql.

</details>


### [23] [Trustworthy LLM-Mediated Communication: Evaluating Information Fidelity in LLM as a Communicator (LAAC) Framework in Multiple Application Domains](https://arxiv.org/abs/2511.04184)
*Mohammed Musthafa Rafi,Adarsh Krishnamurthy,Aditya Balu*

Main category: cs.CL

TL;DR: LAAC提出将LLM作为智能通信中介的新范式，通过结构化对话捕获发送者意图，促进真实知识交换，避免AI生成内容的膨胀-压缩循环。


<details>
  <summary>Details</summary>
Motivation: 当前AI生成内容泛滥导致通信剧场化，发送者用LLM将简单想法膨胀为冗长内容，接收者用LLM压缩回摘要，双方都不接触真实内容。

Method: 使用多智能体架构，通过结构化对话捕获发送者意图，评估信息捕获保真度、可重现性和查询响应完整性三个信任维度。

Result: 初步发现存在可测量的信任差距，在高风险通信场景部署前需解决这些问题。

Conclusion: LAAC作为通信中介具有潜力，但需要解决信息保真度、一致性和可靠性等关键信任问题才能可靠部署。

Abstract: The proliferation of AI-generated content has created an absurd communication
theater where senders use LLMs to inflate simple ideas into verbose content,
recipients use LLMs to compress them back into summaries, and as a consequence
neither party engage with authentic content. LAAC (LLM as a Communicator)
proposes a paradigm shift - positioning LLMs as intelligent communication
intermediaries that capture the sender's intent through structured dialogue and
facilitate genuine knowledge exchange with recipients. Rather than perpetuating
cycles of AI-generated inflation and compression, LAAC enables authentic
communication across diverse contexts including academic papers, proposals,
professional emails, and cross-platform content generation. However, deploying
LLMs as trusted communication intermediaries raises critical questions about
information fidelity, consistency, and reliability. This position paper
systematically evaluates the trustworthiness requirements for LAAC's deployment
across multiple communication domains. We investigate three fundamental
dimensions: (1) Information Capture Fidelity - accuracy of intent extraction
during sender interviews across different communication types, (2)
Reproducibility - consistency of structured knowledge across multiple
interaction instances, and (3) Query Response Integrity - reliability of
recipient-facing responses without hallucination, source conflation, or
fabrication. Through controlled experiments spanning multiple LAAC use cases,
we assess these trust dimensions using LAAC's multi-agent architecture.
Preliminary findings reveal measurable trust gaps that must be addressed before
LAAC can be reliably deployed in high-stakes communication scenarios.

</details>


### [24] [Computational Turing Test Reveals Systematic Differences Between Human and AI Language](https://arxiv.org/abs/2511.04195)
*Nicolò Pagan,Petter Törnberg,Christopher A. Bail,Anikó Hannák,Christopher Barrie*

Main category: cs.CL

TL;DR: 本文提出了一个计算图灵测试框架来评估LLM生成文本的人类相似性，发现即使经过校准，LLM输出在情感表达等方面仍与人类文本有明显差异，且存在人类相似性与语义保真度之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 当前社会科学中广泛使用LLM模拟人类行为，但缺乏对LLM生成文本真实性的稳健评估工具，现有基于人类判断的验证方法存在不可靠的问题。

Method: 引入计算图灵测试框架，结合聚合指标（BERT检测性和语义相似度）与可解释语言特征（风格标记和主题模式）；系统比较9个开源LLM在5种校准策略下的表现，包括微调、风格提示和上下文检索。

Result: 即使经过校准，LLM输出在情感基调和情感表达方面仍明显可区分于人类文本；指令调优模型表现不如基础模型；模型规模扩大不会增强人类相似性；存在人类相似性与语义保真度之间的权衡。

Conclusion: 为LLM模拟提供了可扩展的验证和校准框架，同时警示当前LLM在捕捉人类交流方面存在局限性。

Abstract: Large language models (LLMs) are increasingly used in the social sciences to
simulate human behavior, based on the assumption that they can generate
realistic, human-like text. Yet this assumption remains largely untested.
Existing validation efforts rely heavily on human-judgment-based evaluations --
testing whether humans can distinguish AI from human output -- despite evidence
that such judgments are blunt and unreliable. As a result, the field lacks
robust tools for assessing the realism of LLM-generated text or for calibrating
models to real-world data. This paper makes two contributions. First, we
introduce a computational Turing test: a validation framework that integrates
aggregate metrics (BERT-based detectability and semantic similarity) with
interpretable linguistic features (stylistic markers and topical patterns) to
assess how closely LLMs approximate human language within a given dataset.
Second, we systematically compare nine open-weight LLMs across five calibration
strategies -- including fine-tuning, stylistic prompting, and context retrieval
-- benchmarking their ability to reproduce user interactions on X (formerly
Twitter), Bluesky, and Reddit. Our findings challenge core assumptions in the
literature. Even after calibration, LLM outputs remain clearly distinguishable
from human text, particularly in affective tone and emotional expression.
Instruction-tuned models underperform their base counterparts, and scaling up
model size does not enhance human-likeness. Crucially, we identify a trade-off:
optimizing for human-likeness often comes at the cost of semantic fidelity, and
vice versa. These results provide a much-needed scalable framework for
validation and calibration in LLM simulations -- and offer a cautionary note
about their current limitations in capturing human communication.

</details>


### [25] [LLM-as-a-Judge is Bad, Based on AI Attempting the Exam Qualifying for the Member of the Polish National Board of Appeal](https://arxiv.org/abs/2511.04205)
*Michał Karp,Anna Kubaszewska,Magdalena Król,Robert Król,Aleksander Smywiński-Pohl,Mateusz Szymański,Witold Wydmański*

Main category: cs.CL

TL;DR: 评估当前大语言模型能否通过波兰国家上诉法庭资格考试的实证研究，结果显示模型在知识测试中表现尚可，但在实践写作部分均未通过，且自动评估与官方评审存在差异。


<details>
  <summary>Details</summary>
Motivation: 研究当前大语言模型在法律专业资格考试中的表现，探讨其作为实际考生和自动评估者的可行性，以评估AI在法律领域的应用潜力。

Method: 构建混合信息恢复和提取管道，在闭卷和检索增强生成设置下测试多个LLM（包括GPT-4.1、Claude 4 Sonnet和Bielik-11B-v2.6），采用'LLM作为法官'方法自动评估模型生成的答案。

Result: 模型在知识测试中取得满意分数，但在实践写作部分均未达到及格线，且'LLM作为法官'的评估结果与官方评审委员会存在显著差异。

Conclusion: 尽管技术进步迅速，当前的大语言模型尚无法在波兰公共采购裁决中替代人类法官或独立考官，主要受限于幻觉问题、法律条款引用错误、逻辑论证弱点等关键限制。

Abstract: This study provides an empirical assessment of whether current large language
models (LLMs) can pass the official qualifying examination for membership in
Poland's National Appeal Chamber (Krajowa Izba Odwo{\l}awcza). The authors
examine two related ideas: using LLM as actual exam candidates and applying the
'LLM-as-a-judge' approach, in which model-generated answers are automatically
evaluated by other models. The paper describes the structure of the exam, which
includes a multiple-choice knowledge test on public procurement law and a
written judgment, and presents the hybrid information recovery and extraction
pipeline built to support the models. Several LLMs (including GPT-4.1, Claude 4
Sonnet and Bielik-11B-v2.6) were tested in closed-book and various
Retrieval-Augmented Generation settings. The results show that although the
models achieved satisfactory scores in the knowledge test, none met the passing
threshold in the practical written part, and the evaluations of the
'LLM-as-a-judge' often diverged from the judgments of the official examining
committee. The authors highlight key limitations: susceptibility to
hallucinations, incorrect citation of legal provisions, weaknesses in logical
argumentation, and the need for close collaboration between legal experts and
technical teams. The findings indicate that, despite rapid technological
progress, current LLMs cannot yet replace human judges or independent examiners
in Polish public procurement adjudication.

</details>


### [26] [REMIND: Input Loss Landscapes Reveal Residual Memorization in Post-Unlearning LLMs](https://arxiv.org/abs/2511.04228)
*Liran Cohen,Yaniv Nemcovesky,Avi Mendelson*

Main category: cs.CL

TL;DR: REMIND是一种新的机器学习遗忘评估方法，通过分析模型在输入微小变化时的损失模式来检测未学习数据的残余影响，比现有单点评估方法更敏感和可靠。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习遗忘评估方法主要在单个输入层面进行评估，可能忽略语义相似示例中的残余影响，这会危及隐私并导致间接信息泄露。

Method: REMIND分析模型在输入微小变化时的损失模式，发现未学习数据会产生更平坦、不陡峭的损失景观，而保留或无关数据则表现出更尖锐、更易变的模式。

Result: REMIND仅需基于查询的访问，在类似约束条件下优于现有方法，并在不同模型、数据集和转述输入中表现出鲁棒性，适合实际部署。

Conclusion: REMIND为语言模型中的遗忘评估提供了更敏感和可解释的衡量标准，为记忆和遗忘提供了新的视角。

Abstract: Machine unlearning aims to remove the influence of specific training data
from a model without requiring full retraining. This capability is crucial for
ensuring privacy, safety, and regulatory compliance. Therefore, verifying
whether a model has truly forgotten target data is essential for maintaining
reliability and trustworthiness. However, existing evaluation methods often
assess forgetting at the level of individual inputs. This approach may overlook
residual influence present in semantically similar examples. Such influence can
compromise privacy and lead to indirect information leakage. We propose REMIND
(Residual Memorization In Neighborhood Dynamics), a novel evaluation method
aiming to detect the subtle remaining influence of unlearned data and classify
whether the data has been effectively forgotten. REMIND analyzes the model's
loss over small input variations and reveals patterns unnoticed by single-point
evaluations. We show that unlearned data yield flatter, less steep loss
landscapes, while retained or unrelated data exhibit sharper, more volatile
patterns. REMIND requires only query-based access, outperforms existing methods
under similar constraints, and demonstrates robustness across different models,
datasets, and paraphrased inputs, making it practical for real-world
deployment. By providing a more sensitive and interpretable measure of
unlearning effectiveness, REMIND provides a reliable framework to assess
unlearning in language models. As a result, REMIND offers a novel perspective
on memorization and unlearning.

</details>


### [27] [Reusing Pre-Training Data at Test Time is a Compute Multiplier](https://arxiv.org/abs/2511.04234)
*Alex Fang,Thomas Voice,Ruoming Pang,Ludwig Schmidt,Tom Gunter*

Main category: cs.CL

TL;DR: 通过检索增强生成和测试时计算量化预训练过程中遗留的数据集价值，发现预训练方法未能充分利用现有数据集信息，检索可带来显著准确率提升


<details>
  <summary>Details</summary>
Motivation: 当前研究主要关注改进预训练数据集，但很少评估预训练机制从数据中提取知识和思想的效率

Method: 使用检索增强生成和测试时计算，在标准开源数据集上进行预训练后检索，评估MMLU、Math-500和SimpleQA等任务

Result: 检索带来显著准确率提升，在MMLU上检索相当于预训练计算量的5倍，通过额外测试时计算可进一步提升LLaMA 3.1 8B模型在MMLU上10个百分点

Conclusion: 当前预训练方法未能充分利用现有预训练数据集的信息，存在显著改进空间

Abstract: Large language models learn from their vast pre-training corpora, gaining the
ability to solve an ever increasing variety of tasks; yet although researchers
work to improve these datasets, there is little effort to understand how
efficient the pre-training apparatus is at extracting ideas and knowledge from
the data. In this work, we use retrieval augmented generation along with
test-time compute as a way to quantify how much dataset value was left behind
by the process of pre-training, and how this changes across scale. We
demonstrate that pre-training then retrieving from standard and largely
open-sourced datasets results in significant accuracy gains in MMLU, Math-500,
and SimpleQA, which persist through decontamination. For MMLU we observe that
retrieval acts as a ~5x compute multiplier versus pre-training alone. We show
that these results can be further improved by leveraging additional compute at
test time to parse the retrieved context, demonstrating a 10 percentage point
improvement on MMLU for the public LLaMA 3.1 8B model. Overall, our results
suggest that today's pre-training methods do not make full use of the
information in existing pre-training datasets, leaving significant room for
progress.

</details>


### [28] [Efficient Topic Extraction via Graph-Based Labeling: A Lightweight Alternative to Deep Models](https://arxiv.org/abs/2511.04248)
*Salma Mekaoui,Hiba Sofyan,Imane Amaaz,Imane Benchrif,Arsalane Zarghili,Ilham Chaker,Nikola S. Nikolov*

Main category: cs.CL

TL;DR: 提出一种基于图的方法进行主题标注，相比传统方法在计算效率上有优势，结果与ChatGPT-3.5相当。


<details>
  <summary>Details</summary>
Motivation: 现有主题建模方法产生的主题词分布缺乏清晰可解释性，且计算成本高，需要一种更高效的主题标注方法。

Method: 使用基于图的方法，通过丰富主题词的语义相关词并分析它们之间的关系来推导合适的标签。

Result: 在两个数据集上的比较研究表明，该方法在BERTScore和余弦相似度方面优于传统基准方法，结果与ChatGPT-3.5相当，同时保持计算效率。

Conclusion: 图基方法为高效主题标注提供了可行方案，未来可进一步研究提升可解释性和自动化水平。

Abstract: Extracting topics from text has become an essential task, especially with the
rapid growth of unstructured textual data. Most existing works rely on highly
computational methods to address this challenge. In this paper, we argue that
probabilistic and statistical approaches, such as topic modeling (TM), can
offer effective alternatives that require fewer computational resources. TM is
a statistical method that automatically discovers topics in large collections
of unlabeled text; however, it produces topics as distributions of
representative words, which often lack clear interpretability. Our objective is
to perform topic labeling by assigning meaningful labels to these sets of
words. To achieve this without relying on computationally expensive models, we
propose a graph-based approach that not only enriches topic words with
semantically related terms but also explores the relationships among them. By
analyzing these connections within the graph, we derive suitable labels that
accurately capture each topic's meaning. We present a comparative study between
our proposed method and several benchmarks, including ChatGPT-3.5, across two
different datasets. Our method achieved consistently better results than
traditional benchmarks in terms of BERTScore and cosine similarity and produced
results comparable to ChatGPT-3.5, while remaining computationally efficient.
Finally, we discuss future directions for topic labeling and highlight
potential research avenues for enhancing interpretability and automation.

</details>


### [29] [SSPO: Subsentence-level Policy Optimization](https://arxiv.org/abs/2511.04256)
*Kun Yang,Zikang chen,Yanmeng Wang,Zhigen Li*

Main category: cs.CL

TL;DR: SSPO提出句子级别的重要性比率，在GRPO和GSPO之间取得平衡，避免了训练崩溃和高方差问题，同时防止整个响应被剪裁机制丢弃。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR算法如GRPO存在不稳定策略更新问题，GSPO存在采样数据利用率低的问题，需要一种平衡的解决方案。

Method: SSPO应用句子级别的重要性比率，并在PPO-CLIP中使用句子熵来稳定调整剪裁边界，鼓励高熵标记探索并缩小低熵标记的剪裁范围。

Result: SSPO在五个数据集上平均得分46.57，超过GRPO(43.01)和GSPO(44.42)，在三个数据集上达到最先进性能。

Conclusion: SSPO通过采用GSPO的优点但避免其缺点，有效利用了生成数据，在强化学习验证奖励中表现出色。

Abstract: As a significant part of post-training of the Large Language Models (LLMs),
Reinforcement Learning from Verifiable Reward (RLVR) has greatly improved LLMs'
reasoning skills. However, some RLVR algorithms, such as GRPO (Group Relative
Policy Optimization) and GSPO (Group Sequence Policy Optimization), are
observed to suffer from unstable policy updates and low usage of sampling data,
respectively. The importance ratio of GRPO is calculated at the token level,
which focuses more on optimizing a single token. This will be easily affected
by outliers, leading to model training collapse. GSPO proposed the calculation
of the response level importance ratio, which solves the problem of high
variance and training noise accumulation in the calculation of the GRPO
importance ratio. However, since all the response tokens share a common
importance ratio, extreme values can easily raise or lower the overall mean,
leading to the entire response being mistakenly discarded, resulting in a
decrease in the utilization of sampled data. This paper introduces SSPO, which
applies sentence-level importance ratio, taking the balance between GRPO and
GSPO. SSPO not only avoids training collapse and high variance, but also
prevents the whole response tokens from being abandoned by the clipping
mechanism. Furthermore, we apply sentence entropy to PPO-CLIP to steadily
adjust the clipping bounds, encouraging high-entropy tokens to explore and
narrow the clipping range of low-entropy tokens. In particular, SSPO achieves
an average score of 46.57 across five datasets, surpassing GRPO (43.01) and
GSPO (44.42), and wins state-of-the-art performance on three datasets. These
results highlight SSPO's effectiveness in leveraging generated data by taking
the essence of GSPO but rejecting its shortcomings.

</details>


### [30] [Dynamic Jointly Batch Selection for Data Efficient Machine Translation Fine-Tuning](https://arxiv.org/abs/2511.04406)
*Mohammad Amin Ghanizadeh,Mohammad Javad Dousti*

Main category: cs.CL

TL;DR: 提出了一种基于学习性评分的数据选择方法，用于机器翻译模型的微调，通过学习者模型和预训练参考模型的协同作用来提高训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 数据质量及其有效选择是提高机器翻译模型性能的基础，对于构建稳健可靠的翻译系统至关重要。

Method: 定义学习性评分来系统评估数据点的训练效用，采用考虑数据点间相互依赖关系的批量选择策略，利用学习者模型和预训练参考模型的协同作用。

Result: 在英译波斯和其他语言对上的实验表明，相比iid基线，该方法可实现高达5倍的数据效率提升，使用缓存嵌入时计算效率提高24%，并带来更好的翻译性能。

Conclusion: 该方法通过智能数据选择显著提高了机器翻译微调的数据效率和计算效率，同时增强了模型的泛化能力。

Abstract: Data quality and its effective selection are fundamental to improving the
performance of machine translation models, serving as cornerstones for
achieving robust and reliable translation systems. This paper presents a data
selection methodology specifically designed for fine-tuning machine translation
systems, which leverages the synergy between a learner model and a pre-trained
reference model to enhance overall training effectiveness. By defining a
learnability score, our approach systematically evaluates the utility of data
points for training, ensuring that only the most relevant and impactful
examples contribute to the fine-tuning process. Furthermore, our method employs
a batch selection strategy which considers interdependencies among data points,
optimizing the efficiency of the training process while maintaining a focus on
data relevance. Experiments on English to Persian and several other language
pairs using an mBART model fine-tuned on the CCMatrix dataset demonstrate that
our method can achieve up to a fivefold improvement in data efficiency compared
to an iid baseline. Experimental results indicate that our approach improves
computational efficiency by 24 when utilizing cached embeddings, as it requires
fewer training data points. Additionally, it enhances generalization, resulting
in superior translation performance compared to random selection method.

</details>


### [31] [If I Could Turn Back Time: Temporal Reframing as a Historical Reasoning Task for LLMs](https://arxiv.org/abs/2511.04432)
*Lars Bungum,Charles Yijia Huang,Abeer Kashar*

Main category: cs.CL

TL;DR: 本研究测试了大型语言模型在时间推理方面的能力，使用1940年挪威书籍中的琐事问题，让模型以1940年的视角回答问题，并比较了英语和挪威语提示的效果。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在时间推理方面的能力，特别是测试它们能否在特定历史背景下回答问题，并比较不同语言提示对结果的影响。

Method: 使用1940年挪威书籍中的琐事问题，让LLMs以1940年的视角回答，分别在英语和挪威语环境下测试，采用LLM-as-judge方法进行评分，并有人工抽样验证。

Result: 英语提示始终比挪威语提示效果更好，这是一个意外结果；更大的LLM模型表现更好；测试了DeepSeek-R1、Gemma3、Qwen3、Llama3.1等模型家族以及专门为挪威语设计的最大LLM。

Conclusion: LLMs在时间推理方面具有一定能力，但语言选择对结果有显著影响，英语提示优于挪威语提示，模型规模对性能有积极影响。

Abstract: In this study, we experiment with the ability of LLMs to do temporal
reasoning. Using a Norwegian book from 1940 containing trivia questions, we
prompt the LLMs to answer the questions as if it were 1940. We also pose the
questions in both English and Norwegian. Correct answers are often presented as
sentences, and grading is done by means of LLM-as-judge, with sampled checks by
a native speaker. Prompting in English consistently gave better results than in
Norwegian, an unexpected result. In contrast, using larger LLMs improved
results. We tested the DeepSeek-R1, Gemma3, Qwen3, and Llama3.1 model families,
and also the largest available LLM especially crafted for Norwegian.

</details>
