<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 39]
- [cs.SE](#cs.SE) [Total: 2]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.AI](#cs.AI) [Total: 6]
- [cs.CV](#cs.CV) [Total: 3]
- [cs.LG](#cs.LG) [Total: 2]
- [cs.IR](#cs.IR) [Total: 2]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.CY](#cs.CY) [Total: 3]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Gender-Neutral Rewriting in Italian: Models, Approaches, and Trade-offs](https://arxiv.org/abs/2509.13480)
*Andrea Piergentili,Beatrice Savoldi,Matteo Negri,Luisa Bentivogli*

Main category: cs.CL

TL;DR: 本文首次系统评估了大型语言模型在意大利语性别中性改写任务中的表现，提出了衡量中立性和语义保真度的二维框架，发现开源模型优于现有专用模型，且微调后的小模型能达到最佳开源模型的性能。


<details>
  <summary>Details</summary>
Motivation: 性别中性改写旨在消除文本中不必要的性别指定同时保持原意，这在意大利语等语法性别语言中特别具有挑战性。目前缺乏对大型语言模型在该任务上的系统评估。

Method: 采用少样本提示比较多个LLM，对选定模型进行微调，并应用针对性清洗提升任务相关性。建立了衡量中立性和语义保真度的二维评估框架。

Result: 开源权重LLM在意大利语性别中性改写任务上优于现有专用模型，微调后的小模型能以更小的规模达到或超过最佳开源LLM的性能。

Conclusion: 研究展示了LLM在性别中性改写任务上的潜力，同时揭示了在优化训练数据时中立性和意义保持之间的权衡关系。

Abstract: Gender-neutral rewriting (GNR) aims to reformulate text to eliminate
unnecessary gender specifications while preserving meaning, a particularly
challenging task in grammatical-gender languages like Italian. In this work, we
conduct the first systematic evaluation of state-of-the-art large language
models (LLMs) for Italian GNR, introducing a two-dimensional framework that
measures both neutrality and semantic fidelity to the input. We compare
few-shot prompting across multiple LLMs, fine-tune selected models, and apply
targeted cleaning to boost task relevance. Our findings show that open-weight
LLMs outperform the only existing model dedicated to GNR in Italian, whereas
our fine-tuned models match or exceed the best open-weight LLM's performance at
a fraction of its size. Finally, we discuss the trade-off between optimizing
the training data for neutrality and meaning preservation.

</details>


### [2] [Op-Fed: Opinion, Stance, and Monetary Policy Annotations on FOMC Transcripts Using Active Learning](https://arxiv.org/abs/2509.13539)
*Alisa Kanganis,Katherine A. Keith*

Main category: cs.CL

TL;DR: Op-Fed数据集包含1044条人工标注的FOMC会议记录句子，用于货币政策立场分析，通过分层标注方案和主动学习解决类别不平衡和上下文依赖问题。


<details>
  <summary>Details</summary>
Motivation: 美联储FOMC的货币政策决策影响数百万人，但现有数据缺乏对货币政策立场的细粒度标注，且存在类别不平衡和上下文依赖的技术挑战。

Method: 开发五层分层标注方案分离观点、货币政策和立场维度；使用主动学习选择标注实例，将正例数量翻倍；评估LLM在零样本下的表现。

Result: 最佳闭源LLM在观点分类上达到0.80准确率，但在货币政策立场分类上仅0.61，低于人类基线0.89。

Conclusion: Op-Fed数据集可用于模型训练、置信度校准和作为未来标注工作的种子数据集，揭示了LLM在细粒度金融文本分析中的局限性。

Abstract: The U.S. Federal Open Market Committee (FOMC) regularly discusses and sets
monetary policy, affecting the borrowing and spending decisions of millions of
people. In this work, we release Op-Fed, a dataset of 1044 human-annotated
sentences and their contexts from FOMC transcripts. We faced two major
technical challenges in dataset creation: imbalanced classes -- we estimate
fewer than 8% of sentences express a non-neutral stance towards monetary policy
-- and inter-sentence dependence -- 65% of instances require context beyond the
sentence-level. To address these challenges, we developed a five-stage
hierarchical schema to isolate aspects of opinion, monetary policy, and stance
towards monetary policy as well as the level of context needed. Second, we
selected instances to annotate using active learning, roughly doubling the
number of positive instances across all schema aspects. Using Op-Fed, we found
a top-performing, closed-weight LLM achieves 0.80 zero-shot accuracy in opinion
classification but only 0.61 zero-shot accuracy classifying stance towards
monetary policy -- below our human baseline of 0.89. We expect Op-Fed to be
useful for future model training, confidence calibration, and as a seed dataset
for future annotation efforts.

</details>


### [3] [Overview of Dialog System Evaluation Track: Dimensionality, Language, Culture and Safety at DSTC 12](https://arxiv.org/abs/2509.13569)
*John Mendonça,Lining Zhang,Rahul Mallidi,Alon Lavie,Isabel Trancoso,Luis Fernando D'Haro,João Sedoc*

Main category: cs.CL

TL;DR: DSTC12 Track 1 针对对话系统评估的挑战，提出了两个子任务：多维度自动评估指标和多语言/多文化安全检测，结果显示现有方法在文化安全方面仍有较大改进空间


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的快速发展凸显了对话系统评估的重要性，但传统指标不足且安全考虑常存在文化偏见，需要更全面的评估框架

Method: 设置两个子任务：1) 对话级多维度自动评估指标（10个维度）；2) 多语言和多文化安全检测，提供基准模型并比较参与者提交结果

Result: 任务1中Llama-3-8B基线获得最高平均Spearman相关性(0.1681)；任务2中参与者在多语言安全子集上显著优于Llama-Guard-3-1B基线(最高ROC-AUC 0.9648)，但基线在文化子集上表现更好(0.5126 ROC-AUC)

Conclusion: 对话系统评估仍需重大改进，特别是在文化感知安全方面存在关键需求，多维度评估和多文化安全检测是未来重要研究方向

Abstract: The rapid advancement of Large Language Models (LLMs) has intensified the
need for robust dialogue system evaluation, yet comprehensive assessment
remains challenging. Traditional metrics often prove insufficient, and safety
considerations are frequently narrowly defined or culturally biased. The DSTC12
Track 1, "Dialog System Evaluation: Dimensionality, Language, Culture and
Safety," is part of the ongoing effort to address these critical gaps. The
track comprised two subtasks: (1) Dialogue-level, Multi-dimensional Automatic
Evaluation Metrics, and (2) Multilingual and Multicultural Safety Detection.
For Task 1, focused on 10 dialogue dimensions, a Llama-3-8B baseline achieved
the highest average Spearman's correlation (0.1681), indicating substantial
room for improvement. In Task 2, while participating teams significantly
outperformed a Llama-Guard-3-1B baseline on the multilingual safety subset (top
ROC-AUC 0.9648), the baseline proved superior on the cultural subset (0.5126
ROC-AUC), highlighting critical needs in culturally-aware safety. This paper
describes the datasets and baselines provided to participants, as well as
submission evaluation results for each of the two proposed subtasks.

</details>


### [4] [Latent Traits and Cross-Task Transfer: Deconstructing Dataset Interactions in LLM Fine-tuning](https://arxiv.org/abs/2509.13624)
*Shambhavi Krishna,Atharva Naik,Chaitali Agarwal,Sudharshan Govindan,Taesung Lee,Haw-Shiuan Chang*

Main category: cs.CL

TL;DR: 论文提出了一个分析框架来研究LLM跨任务迁移学习中的潜在能力和副作用，发现性能提升主要受隐藏统计因素和语言特征影响，而非表面数据集相似性。


<details>
  <summary>Details</summary>
Motivation: 由于LLM部署中经常遇到训练时未见的任务，且无法为所有任务获取高质量训练数据，需要依赖不同特性的数据集进行迁移学习，因此需要分析跨任务交互的复杂动态。

Method: 构建迁移学习矩阵和降维分析框架，训练10个模型来识别潜在能力（如推理、情感分类、自然语言理解、算术等），并分析迁移学习的副作用。

Result: 研究发现性能改进往往无法用表面数据集相似性或源数据质量来解释，而是受源数据集的隐藏统计因素（如类别分布和生成长度倾向）以及特定语言特征的影响更大。

Conclusion: 这项工作揭示了迁移学习的复杂动态，为更可预测和有效的LLM适应铺平了道路，强调了隐藏统计特征在跨任务迁移中的重要性。

Abstract: Large language models are increasingly deployed across diverse applications.
This often includes tasks LLMs have not encountered during training. This
implies that enumerating and obtaining the high-quality training data for all
tasks is infeasible. Thus, we often need to rely on transfer learning using
datasets with different characteristics, and anticipate out-of-distribution
requests. Motivated by this practical need, we propose an analysis framework,
building a transfer learning matrix and dimensionality reduction, to dissect
these cross-task interactions. We train and analyze 10 models to identify
latent abilities (e.g., Reasoning, Sentiment Classification, NLU, Arithmetic)
and discover the side effects of the transfer learning. Our findings reveal
that performance improvements often defy explanations based on surface-level
dataset similarity or source data quality. Instead, hidden statistical factors
of the source dataset, such as class distribution and generation length
proclivities, alongside specific linguistic features, are actually more
influential. This work offers insights into the complex dynamics of transfer
learning, paving the way for more predictable and effective LLM adaptation.

</details>


### [5] [Sparse Neurons Carry Strong Signals of Question Ambiguity in LLMs](https://arxiv.org/abs/2509.13664)
*Zhuoxuan Zhang,Jinhao Duan,Edward Kim,Kaidi Xu*

Main category: cs.CL

TL;DR: 研究发现LLMs在内部表征中线性编码问题歧义性，通过少量神经元即可检测和控制歧义性，实现从直接回答到弃权的行为控制


<details>
  <summary>Details</summary>
Motivation: 现实问题普遍存在歧义性，但LLMs往往自信回答而非寻求澄清，需要理解模型如何内部处理歧义信息

Method: 在模型预填充阶段识别歧义编码神经元(AENs)，训练探针进行歧义检测，通过神经元操作控制模型行为

Result: AENs探针在歧义检测上表现优异且具有跨数据集泛化能力，层析分析显示歧义信号在浅层编码，神经元操作可有效控制模型行为

Conclusion: LLMs形成紧凑的内部歧义表征，为实现可解释和可控的行为提供了新途径

Abstract: Ambiguity is pervasive in real-world questions, yet large language models
(LLMs) often respond with confident answers rather than seeking clarification.
In this work, we show that question ambiguity is linearly encoded in the
internal representations of LLMs and can be both detected and controlled at the
neuron level. During the model's pre-filling stage, we identify that a small
number of neurons, as few as one, encode question ambiguity information. Probes
trained on these Ambiguity-Encoding Neurons (AENs) achieve strong performance
on ambiguity detection and generalize across datasets, outperforming
prompting-based and representation-based baselines. Layerwise analysis reveals
that AENs emerge from shallow layers, suggesting early encoding of ambiguity
signals in the model's processing pipeline. Finally, we show that through
manipulating AENs, we can control LLM's behavior from direct answering to
abstention. Our findings reveal that LLMs form compact internal representations
of question ambiguity, enabling interpretable and controllable behavior.

</details>


### [6] [CL$^2$GEC: A Multi-Discipline Benchmark for Continual Learning in Chinese Literature Grammatical Error Correction](https://arxiv.org/abs/2509.13672)
*Shang Qin,Jingheng Ye,Yinghui Li,Hai-Tao Zheng,Qi Li,Jinxiao Shan,Zhixing Li,Hong-Gee Kim*

Main category: cs.CL

TL;DR: 提出了第一个中文文献语法纠错持续学习基准CL²GEC，包含10个学科的10,000条标注句子，评估大语言模型在跨学科语法纠错中的持续学习能力。


<details>
  <summary>Details</summary>
Motivation: 现有中文语法纠错研究缺乏多学科学术写作的专用基准，忽视了持续学习作为处理领域特定语言变异和防止灾难性遗忘的有效解决方案。

Method: 构建包含10个学科10,000句的标注数据集，在持续学习设置下评估大语言模型，包括顺序调优、参数高效适应和四种代表性持续学习算法。

Result: 实验结果显示基于正则化的方法比基于回放或简单顺序方法更能有效缓解遗忘问题。

Conclusion: 该基准为跨学科学术领域的自适应语法纠错研究提供了严谨的基础。

Abstract: The growing demand for automated writing assistance in diverse academic
domains highlights the need for robust Chinese Grammatical Error Correction
(CGEC) systems that can adapt across disciplines. However, existing CGEC
research largely lacks dedicated benchmarks for multi-disciplinary academic
writing, overlooking continual learning (CL) as a promising solution to handle
domain-specific linguistic variation and prevent catastrophic forgetting. To
fill this crucial gap, we introduce CL$^2$GEC, the first Continual Learning
benchmark for Chinese Literature Grammatical Error Correction, designed to
evaluate adaptive CGEC across multiple academic fields. Our benchmark includes
10,000 human-annotated sentences spanning 10 disciplines, each exhibiting
distinct linguistic styles and error patterns. CL$^2$GEC focuses on evaluating
grammatical error correction in a continual learning setting, simulating
sequential exposure to diverse academic disciplines to reflect real-world
editorial dynamics. We evaluate large language models under sequential tuning,
parameter-efficient adaptation, and four representative CL algorithms, using
both standard GEC metrics and continual learning metrics adapted to task-level
variation. Experimental results reveal that regularization-based methods
mitigate forgetting more effectively than replay-based or naive sequential
approaches. Our benchmark provides a rigorous foundation for future research in
adaptive grammatical error correction across diverse academic domains.

</details>


### [7] [AgentCTG: Harnessing Multi-Agent Collaboration for Fine-Grained Precise Control in Text Generation](https://arxiv.org/abs/2509.13677)
*Xinxu Zhou,Jiaqi Bai,Zhenqi Sun,Fanxiang Zeng,Yue Liu*

Main category: cs.CL

TL;DR: AgentCTG是一个新颖的可扩展框架，通过模拟多智能体工作流中的控制和调节机制，实现对文本生成的精确复杂控制，在多个公开数据集上达到最先进效果。


<details>
  <summary>Details</summary>
Motivation: 当前受控文本生成面临精细条件控制的挑战，实际应用中还需要考虑成本、可扩展性、领域知识学习和更精确控制等需求。

Method: 提出AgentCTG框架，模拟多智能体工作流的控制和调节机制，探索不同智能体间的协作方法，并引入自动提示模块来增强生成效果。

Result: 在多个公开数据集上达到最先进结果，在角色驱动重写任务中成功将原始文本转换为符合特定角色配置文件的新文本，同时保留领域知识。

Conclusion: 该框架显著提升了在线导航和角色扮演的内容交付体验，通过优化上下文相关文本生成，实现了更沉浸式的在线社区交互，促进了个性化和用户参与度。

Abstract: Although significant progress has been made in many tasks within the field of
Natural Language Processing (NLP), Controlled Text Generation (CTG) continues
to face numerous challenges, particularly in achieving fine-grained conditional
control over generation. Additionally, in real scenario and online
applications, cost considerations, scalability, domain knowledge learning and
more precise control are required, presenting more challenge for CTG. This
paper introduces a novel and scalable framework, AgentCTG, which aims to
enhance precise and complex control over the text generation by simulating the
control and regulation mechanisms in multi-agent workflows. We explore various
collaboration methods among different agents and introduce an auto-prompt
module to further enhance the generation effectiveness. AgentCTG achieves
state-of-the-art results on multiple public datasets. To validate its
effectiveness in practical applications, we propose a new challenging
Character-Driven Rewriting task, which aims to convert the original text into
new text that conform to specific character profiles and simultaneously
preserve the domain knowledge. When applied to online navigation with
role-playing, our approach significantly enhances the driving experience
through improved content delivery. By optimizing the generation of contextually
relevant text, we enable a more immersive interaction within online
communities, fostering greater personalization and user engagement.

</details>


### [8] [Improving Context Fidelity via Native Retrieval-Augmented Reasoning](https://arxiv.org/abs/2509.13683)
*Suyuchen Wang,Jinlin Wang,Xinyu Wang,Shiqi Li,Xiangru Tang,Sirui Hong,Xiao-Wen Chang,Chenglin Wu,Bang Liu*

Main category: cs.CL

TL;DR: CARE框架通过让LLMs在推理过程中显式整合上下文证据，显著提升了检索准确性和答案生成性能，无需昂贵监督微调


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在基于给定信息回答问题时存在上下文保真度不足、答案不一致的问题，现有方法要么依赖昂贵监督微调，要么无法有效利用给定上下文

Method: 提出CARE框架，教导LLMs在推理过程中显式整合上下文证据，利用模型自身的检索能力，仅需有限标记证据数据，通过策略性检索上下文token来增强推理链

Result: 在多个真实世界和反事实QA基准测试中，该方法显著优于监督微调、传统检索增强生成方法和外部检索解决方案

Conclusion: 这项工作是使LLMs在知识密集型任务中更加准确、可靠和高效的根本性进展

Abstract: Large language models (LLMs) often struggle with context fidelity, producing
inconsistent answers when responding to questions based on provided
information. Existing approaches either rely on expensive supervised
fine-tuning to generate evidence post-answer or train models to perform web
searches without necessarily improving utilization of the given context. We
propose CARE, a novel native retrieval-augmented reasoning framework that
teaches LLMs to explicitly integrate in-context evidence within their reasoning
process with the model's own retrieval capabilities. Our method requires
limited labeled evidence data while significantly enhancing both retrieval
accuracy and answer generation performance through strategically retrieved
in-context tokens in the reasoning chain. Extensive experiments on multiple
real-world and counterfactual QA benchmarks demonstrate that our approach
substantially outperforms supervised fine-tuning, traditional
retrieval-augmented generation methods, and external retrieval solutions. This
work represents a fundamental advancement in making LLMs more accurate,
reliable, and efficient for knowledge-intensive tasks.

</details>


### [9] [Can Large Language Models Robustly Perform Natural Language Inference for Japanese Comparatives?](https://arxiv.org/abs/2509.13695)
*Yosuke Mikami,Daiki Matsuoka,Hitomi Yanaka*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型在日语比较句自然语言推理任务中的表现，发现模型对提示格式敏感，且在处理日语特有语言现象时存在困难，但包含逻辑语义表示的提示能提升性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自然语言推理方面表现优异，但在处理涉及数值和逻辑表达式的推理时仍面临挑战，特别是在非主导语言（如日语）中处理比较句的鲁棒性尚未得到充分探索。

Method: 构建了一个专注于比较句的日语自然语言推理数据集，并在零样本和少样本设置下评估了各种大型语言模型，分析了提示格式和示例标签对模型性能的影响。

Result: 模型性能在零样本设置中对提示格式敏感，在少样本设置中受示例标签影响；模型在处理日语特有语言现象时表现不佳；包含逻辑语义表示的提示能帮助模型解决难以处理的推理问题。

Conclusion: 大型语言模型在处理日语比较句推理时存在局限性，但通过设计合适的提示（特别是包含逻辑语义表示）可以显著提升模型性能，这为改进跨语言推理能力提供了重要启示。

Abstract: Large Language Models (LLMs) perform remarkably well in Natural Language
Inference (NLI). However, NLI involving numerical and logical expressions
remains challenging. Comparatives are a key linguistic phenomenon related to
such inference, but the robustness of LLMs in handling them, especially in
languages that are not dominant in the models' training data, such as Japanese,
has not been sufficiently explored. To address this gap, we construct a
Japanese NLI dataset that focuses on comparatives and evaluate various LLMs in
zero-shot and few-shot settings. Our results show that the performance of the
models is sensitive to the prompt formats in the zero-shot setting and
influenced by the gold labels in the few-shot examples. The LLMs also struggle
to handle linguistic phenomena unique to Japanese. Furthermore, we observe that
prompts containing logical semantic representations help the models predict the
correct labels for inference problems that they struggle to solve even with
few-shot examples.

</details>


### [10] [Integrating Text and Time-Series into (Large) Language Models to Predict Medical Outcomes](https://arxiv.org/abs/2509.13696)
*Iyadh Ben Cheikh Larbi,Ajay Madhavan Ravichandran,Aljoscha Burchardt,Roland Roller*

Main category: cs.CL

TL;DR: 使用DSPy提示优化技术，将指令调优的大语言模型应用于临床分类任务，联合处理临床文本和结构化EHR数据，性能媲美专业多模态系统且更简单灵活


<details>
  <summary>Details</summary>
Motivation: 大语言模型在文本生成方面表现出色，但在处理涉及结构化数据（如时间序列）的临床分类任务方面仍有待探索

Method: 采用基于DSPy的提示优化技术，对指令调优的大语言模型进行适配，使其能够联合处理临床笔记和结构化电子健康记录输入

Result: 该方法在性能上与专业的多模态系统相当，同时需要更少的复杂性，并在不同任务间具有更好的适应性

Conclusion: 基于提示优化的LLM适配方法为临床分类任务提供了一种简单有效的解决方案，性能可与专业系统媲美且更具灵活性

Abstract: Large language models (LLMs) excel at text generation, but their ability to
handle clinical classification tasks involving structured data, such as time
series, remains underexplored. In this work, we adapt instruction-tuned LLMs
using DSPy-based prompt optimization to process clinical notes and structured
EHR inputs jointly. Our results show that this approach achieves performance on
par with specialized multimodal systems while requiring less complexity and
offering greater adaptability across tasks.

</details>


### [11] [DSCC-HS: A Dynamic Self-Reinforcing Framework for Hallucination Suppression in Large Language Models](https://arxiv.org/abs/2509.13702)
*Xiao Zheng*

Main category: cs.CL

TL;DR: DSCC-HS是一种新颖的主动式幻觉抑制框架，通过对抗训练的代理模型在自回归解码过程中实时引导大语言模型，无需修改目标模型即可显著提高事实准确性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型幻觉问题严重阻碍了其可靠部署，现有方法如RAG多为被动反应式，需要一种主动干预的解决方案。

Method: 基于双过程认知理论，使用紧凑代理模型分别作为事实对齐代理(FAP)和幻觉检测代理(HDP)，在推理时通过实时注入FAP和HDP对数概率差异的引导向量来动态引导目标模型。

Result: 在TruthfulQA上达到99.2%的事实一致性率，在BioGEN长文本基准上获得最高FActScore 46.50，实现最先进性能。

Conclusion: DSCC-HS为增强LLM事实性提供了一个原则性且高效的解决方案，验证了其作为即插即用幻觉抑制框架的有效性。

Abstract: Large Language Model (LLM) hallucination is a significant barrier to their
reliable deployment. Current methods like Retrieval-Augmented Generation (RAG)
are often reactive. We introduce **Dynamic Self-reinforcing Calibration for
Hallucination Suppression (DSCC-HS)**, a novel, proactive framework that
intervenes during autoregressive decoding. Inspired by dual-process cognitive
theory, DSCC-HS uses a compact proxy model, trained in adversarial roles as a
Factual Alignment Proxy (FAP) and a Hallucination Detection Proxy (HDP). During
inference, these proxies dynamically steer a large target model by injecting a
real-time steering vector, which is the difference between FAP and HDP logits,
at each decoding step. This plug-and-play approach requires no modification to
the target model. Our experiments on TruthfulQA and BioGEN show DSCC-HS
achieves state-of-the-art performance. On TruthfulQA, it reached a 99.2%
Factual Consistency Rate (FCR). On the long-form BioGEN benchmark, it attained
the highest FActScore of 46.50. These results validate DSCC-HS as a principled
and efficient solution for enhancing LLM factuality.

</details>


### [12] [Automated Triaging and Transfer Learning of Incident Learning Safety Reports Using Large Language Representational Models](https://arxiv.org/abs/2509.13706)
*Peter Beidler,Mark Nguyen,Kevin Lybarger,Ola Holmberg,Eric Ford,John Kang*

Main category: cs.CL

TL;DR: 开发了基于NLP的放射肿瘤学事件报告严重性筛查工具，使用SVM和BlueBERT模型，通过跨机构迁移学习显著提升泛化性能，达到与人类专家相近的检测水平。


<details>
  <summary>Details</summary>
Motivation: 医疗事件报告的手动审查耗时且需要专业知识，需要自动化工具来高效识别高严重性事件报告，提高医疗安全和质量改进效率。

Method: 使用7,094份机构报告和571份IAEA SAFRON报告，训练SVM和BlueBERT模型，采用跨机构迁移学习策略（BlueBERT_TRANSFER），并在人工编辑的清晰报告子集上评估性能。

Result: 机构内测试AUROC达0.82(SVM)/0.81(BlueBERT)；跨机构测试性能从0.42-0.56提升至0.78；在人工编辑数据集上模型性能(0.74-0.85)与人类专家(0.81)相当。

Conclusion: 成功开发了跨机构NLP模型，能够像人类专家一样有效检测放射肿瘤学高严重性事件报告，证明了NLP在医疗质量改进中的实用价值。

Abstract: PURPOSE: Incident reports are an important tool for safety and quality
improvement in healthcare, but manual review is time-consuming and requires
subject matter expertise. Here we present a natural language processing (NLP)
screening tool to detect high-severity incident reports in radiation oncology
across two institutions.
  METHODS AND MATERIALS: We used two text datasets to train and evaluate our
NLP models: 7,094 reports from our institution (Inst.), and 571 from IAEA
SAFRON (SF), all of which had severity scores labeled by clinical content
experts. We trained and evaluated two types of models: baseline support vector
machines (SVM) and BlueBERT which is a large language model pretrained on
PubMed abstracts and hospitalized patient data. We assessed for
generalizability of our model in two ways. First, we evaluated models trained
using Inst.-train on SF-test. Second, we trained a BlueBERT_TRANSFER model that
was first fine-tuned on Inst.-train then on SF-train before testing on SF-test
set. To further analyze model performance, we also examined a subset of 59
reports from our Inst. dataset, which were manually edited for clarity.
  RESULTS Classification performance on the Inst. test achieved AUROC 0.82
using SVM and 0.81 using BlueBERT. Without cross-institution transfer learning,
performance on the SF test was limited to an AUROC of 0.42 using SVM and 0.56
using BlueBERT. BlueBERT_TRANSFER, which was fine-tuned on both datasets,
improved the performance on SF test to AUROC 0.78. Performance of SVM, and
BlueBERT_TRANSFER models on the manually curated Inst. reports (AUROC 0.85 and
0.74) was similar to human performance (AUROC 0.81).
  CONCLUSION: In summary, we successfully developed cross-institution NLP
models on incident report text from radiation oncology centers. These models
were able to detect high-severity reports similarly to humans on a curated
dataset.

</details>


### [13] [DSPC: Dual-Stage Progressive Compression Framework for Efficient Long-Context Reasoning](https://arxiv.org/abs/2509.13723)
*Yaxin Gao,Yao Lu,Zongfei Zhang,Jiaqi Nie,Shanqing Yu,Qi Xuan*

Main category: cs.CL

TL;DR: 这篇论文提出了一种双阶段进阶压缩方法DSPC，通过语义相关句子筛选和关键词剪枝，在不需训练的情况下实现了高效的提示词压缩，在减少计算成本的同时保持了语义准确性。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型中提示词迅速增长导致的计算成本问题，现有方法多需要训练辅助模型而产生额外计算开销。

Method: 采用双阶段进阶压缩：粗粒度阶段使用TF-IDF进行语义相关句子筛选，细粒度阶段通过关注度贡献、跨模型损失差异和位置重要性来评估关键词重要性，并剪枝低效用到征。

Result: 在LLaMA-3.1-8B-Instruct和GPT-3.5-Turbo模型上验证，在限制词汇预算下实现了持续改进。在Longbench数据集的FewShot任务中，DSPC仅使用3倍更少的词汇实现了49.17的性能，超过最佳基线LongLLMLingua 7.76。

Conclusion: DSPC方法提供了一种高效的无训练提示压缩方案，能够在显著减少计算成本的同时保持语义准确性，为大语言模型的实际应用提供了实用的解决方案。

Abstract: Large language models (LLMs) have achieved remarkable success in many natural
language processing (NLP) tasks. To achieve more accurate output, the prompts
used to drive LLMs have become increasingly longer, which incurs higher
computational costs. To address this prompt inflation problem, prompt
compression has been proposed. However, most existing methods require training
a small auxiliary model for compression, incurring a significant amount of
additional computation. To avoid this, we propose a two-stage, training-free
approach, called Dual-Stage Progressive Compression (DSPC). In the
coarse-grained stage, semantic-related sentence filtering removes sentences
with low semantic value based on TF-IDF. In the fine-grained stage, token
importance is assessed using attention contribution, cross-model loss
difference, and positional importance, enabling the pruning of low-utility
tokens while preserving semantics. We validate DSPC on LLaMA-3.1-8B-Instruct
and GPT-3.5-Turbo under a constrained token budget and observe consistent
improvements. For instance, in the FewShot task of the Longbench dataset, DSPC
achieves a performance of 49.17 by using only 3x fewer tokens, outperforming
the best state-of-the-art baseline LongLLMLingua by 7.76.

</details>


### [14] [Implementing a Logical Inference System for Japanese Comparatives](https://arxiv.org/abs/2509.13734)
*Yosuke Mikami,Daiki Matsuoka,Hitomi Yanaka*

Main category: cs.CL

TL;DR: 本研究提出了一种基于组合语义的日语比较句逻辑推理系统ccg-jcomp，解决了现有系统直接应用于日语比较句的因难问题，并在日语NLI数据集上评估了其有效性。


<details>
  <summary>Details</summary>
Motivation: 日语和英语比较句在语法和语义上存在显著差异，使得现有的逻辑推理系统无法直接应用于日语比较句的自然语言推理任务。需要专门为日语比较句设计一种组合语义基础的逻辑推理系统。

Method: 提出ccg-jcomp系统，基于组合语义学的逻辑推理方法，专门处理日语比较句的语法和语义特征。通过对比较关系的形式化表达和逻辑推导来实现推理。

Result: 在含有比较表达的日语NLI数据集上进行了评估，并与现有的大语言模型(LLMs)进行了准确率对比，证明了该系统的有效性。

Conclusion: 该研究成功开发了一种专门处理日语比较句的逻辑推理系统，充分考虑了日语比较句的语法和语义特征，为日语自然语言推理领域提供了一种稳健的逻辑基础方法。

Abstract: Natural Language Inference (NLI) involving comparatives is challenging
because it requires understanding quantities and comparative relations
expressed by sentences. While some approaches leverage Large Language Models
(LLMs), we focus on logic-based approaches grounded in compositional semantics,
which are promising for robust handling of numerical and logical expressions.
Previous studies along these lines have proposed logical inference systems for
English comparatives. However, it has been pointed out that there are several
morphological and semantic differences between Japanese and English
comparatives. These differences make it difficult to apply such systems
directly to Japanese comparatives. To address this gap, this study proposes
ccg-jcomp, a logical inference system for Japanese comparatives based on
compositional semantics. We evaluate the proposed system on a Japanese NLI
dataset containing comparative expressions. We demonstrate the effectiveness of
our system by comparing its accuracy with that of existing LLMs.

</details>


### [15] [Exploring Data and Parameter Efficient Strategies for Arabic Dialect Identifications](https://arxiv.org/abs/2509.13775)
*Vani Kanjirangat,Ljiljana Dolamic,Fabio Rinaldi*

Main category: cs.CL

TL;DR: 本文探索了阿拉伯方言识别的数据效率和参数效率方法，包括软提示策略和LoRA重参数化，发现LoRA精调模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 研究阿拉伯方言识别任务中的数据效率和参数效率方法，以提高大语言模型在方言区分任务中的表现。

Method: 使用各种软提示策略（prefix-tuning、prompt-tuning、P-tuning、P-tuning V2）和LoRA重参数化方法，在阿拉伯特定编码器模型和多个数据集上进行实验。同时分析了零样本和少样本推理的硬提示策略。

Result: 大语言模型在少样本或零样本设置下区分方言细微差异时遇到困难；软提示编码器模型表现更好；LoRA基于精调的模型表现最佳，甚至超越了全量微调。

Conclusion: 对于阿拉伯方言识别任务，LoRA重参数化方法是最有效的参数效率学习策略，能够在保持高性能的同时减少训练资源需求。

Abstract: This paper discusses our exploration of different data-efficient and
parameter-efficient approaches to Arabic Dialect Identification (ADI). In
particular, we investigate various soft-prompting strategies, including
prefix-tuning, prompt-tuning, P-tuning, and P-tuning V2, as well as LoRA
reparameterizations. For the data-efficient strategy, we analyze hard prompting
with zero-shot and few-shot inferences to analyze the dialect identification
capabilities of Large Language Models (LLMs). For the parameter-efficient PEFT
approaches, we conducted our experiments using Arabic-specific encoder models
on several major datasets. We also analyzed the n-shot inferences on
open-source decoder-only models, a general multilingual model (Phi-3.5), and an
Arabic-specific one(SILMA). We observed that the LLMs generally struggle to
differentiate the dialectal nuances in the few-shot or zero-shot setups. The
soft-prompted encoder variants perform better, while the LoRA-based fine-tuned
models perform best, even surpassing full fine-tuning.

</details>


### [16] [Teaching According to Talents! Instruction Tuning LLMs with Competence-Aware Curriculum Learning](https://arxiv.org/abs/2509.13790)
*Yangning Li,Tingwei Lu,Yinghui Li,Yankai Chen,Wei-Chieh Huang,Wenhao Jiang,Hui Wang,Hai-Tao Zheng,Philip S. Yu*

Main category: cs.CL

TL;DR: CAMPUS是一个动态多视角课程学习框架，通过能力感知的课程调整和动态子课程选择，解决了传统课程学习中课程刚性问题，显著提升了指令调优效果。


<details>
  <summary>Details</summary>
Motivation: 当前课程学习方法依赖静态启发式难度指标，存在课程刚性问题，无法适应模型在训练过程中的能力演变，导致学习轨迹固定且可能次优。

Method: 提出CAMPUS框架，包含三个核心优势：动态子课程选择、能力感知的课程进度调整、基于多难度的调度策略。

Result: 大量实验证明CAMPUS在高效指令调优方面优于其他最先进的基线方法。

Conclusion: CAMPUS通过动态适应模型能力演变的课程学习策略，有效解决了课程刚性问题，为指令调优提供了更优的学习轨迹。

Abstract: Efficient instruction tuning aims to enhance the ultimate performance of
large language models (LLMs) trained on a given instruction dataset. Curriculum
learning as a typical data organization strategy has shown preliminary
effectiveness in instruction tuning. However, current curriculum tuning methods
suffer from the curriculum rigidity, since they rely solely on static heuristic
difficulty metrics. These methods fail to adapt to the evolving capabilities of
models during training, resulting in a fixed and potentially sub-optimal
learning trajectory. To address the issue, Competence-Aware Multi-Perspective
cUrriculum inStruction tuning framework termed CAMPUS is proposed. CAMPUS
offers several advantages: (1) Dynamic selection for sub-curriculum. (2)
Competency-aware adjustment to the curriculum schedule. (3) Multiple
difficulty-based scheduling. Extensive experiments prove the superior
performance of CAMPUS, compared to other state-of-the-art baselines for
efficient instruction tuning.

</details>


### [17] [Measuring Gender Bias in Job Title Matching for Grammatical Gender Languages](https://arxiv.org/abs/2509.13803)
*Laura García-Sardiña,Hermenegildo Fabregat,Daniel Deniz,Rabih Zbib*

Main category: cs.CL

TL;DR: 这篇论文研究语法性别在职业名称中的显式分配如何影响自动职业排名系统的结果，提出了使用RBO指标来评估性别偏见，并为四种语言构建了包含阳性和阴性形式职业的测试集。


<details>
  <summary>Details</summary>
Motivation: 研究语法性别在职业名称中的显式分配如何影响自动职业排名系统的结果，评估职业排名系统中的性别偏见问题。

Method: 提出使用RBO（Rank-Biased Overlap）指标来评估性别偏见，为四种具有语法性别的语言构建了测试集，包含阳性和阴性形式的职业名称，并进行了性别和匹配相关性标注。

Result: 使用新的测试集和方法评估了多个外部多语言模型，显示所有模型都存在不同程度的性别偏见。

Conclusion: 该研究为评估职业排名系统中的性别偏见提供了基础工具和方法，并证实了现有多语言模型都存在性别偏见问题。

Abstract: This work sets the ground for studying how explicit grammatical gender
assignment in job titles can affect the results of automatic job ranking
systems. We propose the usage of metrics for ranking comparison controlling for
gender to evaluate gender bias in job title ranking systems, in particular RBO
(Rank-Biased Overlap). We generate and share test sets for a job title matching
task in four grammatical gender languages, including occupations in masculine
and feminine form and annotated by gender and matching relevance. We use the
new test sets and the proposed methodology to evaluate the gender bias of
several out-of-the-box multilingual models to set as baselines, showing that
all of them exhibit varying degrees of gender bias.

</details>


### [18] [Geometric Uncertainty for Detecting and Correcting Hallucinations in LLMs](https://arxiv.org/abs/2509.13813)
*Edward Phillips,Sean Wu,Soheila Molaei,Danielle Belgrave,Anshul Thakur,David Clifton*

Main category: cs.CL

TL;DR: 通过几何架构框架，提出了可以同时估计全局和局部不确定性的黑盒方法，用于检测大语言模型的幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在幻觉问题，但现有的黑盒方法只能提供全局不确定性估计，而能提供局部不确定性的方法需要白盒访问权限。需要一种新方法来解决这个问题。

Method: 基于archetypal analysis的几何框架，通过响应嵌入的凸包立体积（Geometric Volume）来测量全局不确定性，通过Geometric Suspicion来排序响应可靠性并选择更可靠的响应。

Result: 在短文本问答数据集上表现与现有方法相当或更好，在医疗数据集上表现更优，特别是在幻觉风险高的场景下。

Conclusion: 该几何框架提供了一种无需白盒访问的方法，可以同时进行全局和局部不确定性估计，有效检测幻觉现象。

Abstract: Large language models demonstrate impressive results across diverse tasks but
are still known to hallucinate, generating linguistically plausible but
incorrect answers to questions. Uncertainty quantification has been proposed as
a strategy for hallucination detection, but no existing black-box approach
provides estimates for both global and local uncertainty. The former attributes
uncertainty to a batch of responses, while the latter attributes uncertainty to
individual responses. Current local methods typically rely on white-box access
to internal model states, whilst black-box methods only provide global
uncertainty estimates. We introduce a geometric framework to address this,
based on archetypal analysis of batches of responses sampled with only
black-box model access. At the global level, we propose Geometric Volume, which
measures the convex hull volume of archetypes derived from response embeddings.
At the local level, we propose Geometric Suspicion, which ranks responses by
reliability and enables hallucination reduction through preferential response
selection. Unlike prior dispersion methods which yield only a single global
score, our approach provides semantic boundary points which have utility for
attributing reliability to individual responses. Experiments show that our
framework performs comparably to or better than prior methods on short form
question-answering datasets, and achieves superior results on medical datasets
where hallucinations carry particularly critical risks. We also provide
theoretical justification by proving a link between convex hull volume and
entropy.

</details>


### [19] [Findings of the Third Automatic Minuting (AutoMin) Challenge](https://arxiv.org/abs/2509.13814)
*Kartik Shinde,Laurent Besacier,Ondrej Bojar,Thibaut Thonet,Tirthankar Ghosal*

Main category: cs.CL

TL;DR: AutoMin 2025共享任务聚焦于会议纪要自动生成和问答两个任务，涵盖英语和捷克语的项目会议和欧洲议会场景，参与团队较少但包含多个基线系统评估。


<details>
  <summary>Details</summary>
Motivation: 推动自动会议纪要生成技术的发展，特别是在多语言和结构化纪要方面的应用，同时探索基于会议转录的问答能力。

Method: 设置两个主要任务：结构化会议纪要生成（涵盖英捷双语的项目会议和欧洲议会）和问答任务（包括单语英语问答和跨语言英捷问答）。使用多个基线LLM系统进行综合评估。

Result: 2025年参与度较低，只有1个团队参与纪要任务，2个团队参与问答任务。组织者提供了多个基线系统来评估当前LLM在相关任务上的表现。

Conclusion: 尽管参与团队有限，但通过基线系统的全面评估，为自动会议纪要生成和会议转录问答任务提供了有价值的基准，展示了当前LLM技术在这两个任务上的能力水平。

Abstract: This paper presents the third edition of AutoMin, a shared task on automatic
meeting summarization into minutes. In 2025, AutoMin featured the main task of
minuting, the creation of structured meeting minutes, as well as a new task:
question answering (QA) based on meeting transcripts.
  The minuting task covered two languages, English and Czech, and two domains:
project meetings and European Parliament sessions. The QA task focused solely
on project meetings and was available in two settings: monolingual QA in
English, and cross-lingual QA, where questions were asked and answered in Czech
based on English meetings.
  Participation in 2025 was more limited compared to previous years, with only
one team joining the minuting task and two teams participating in QA. However,
as organizers, we included multiple baseline systems to enable a comprehensive
evaluation of current (2025) large language models (LLMs) on both tasks.

</details>


### [20] [Large Language Models Discriminate Against Speakers of German Dialects](https://arxiv.org/abs/2509.13835)
*Minh Duc Bui,Carolin Holtermann,Valentin Hofmann,Anne Lauscher,Katharina von der Wense*

Main category: cs.CL

TL;DR: 研究发现大语言模型对德语方言话者存在显著偏见，包括负面形容词联想和偏见性决策，而明确标注语言人口统计特征反而会增强偏见。


<details>
  <summary>Details</summary>
Motivation: 方言作为人类文化的重要组成部分，话者却面临负面社会定型。研究要分析大语言模型是否也存在类似的方言偏见。

Method: 采用社会语言学方法，通过关联任务和决策任务评估模型的方言命名偏见和方言使用偏见，并构建包含七种德语方言的新评估语料库。

Result: 所有模型都显示出显著的方言偏见：(1)在关联任务中产生负面形容词；(2)在决策中复现这些偏见；(3)明确标注语言人口特征比隐含线索更增强偏见。

Conclusion: 大语言模型存在系统性的方言偏见，这与社会中对方言话者的负面定型相一致，需要重视模型训练过程中的语言偏见问题。

Abstract: Dialects represent a significant component of human culture and are found
across all regions of the world. In Germany, more than 40% of the population
speaks a regional dialect (Adler and Hansen, 2022). However, despite cultural
importance, individuals speaking dialects often face negative societal
stereotypes. We examine whether such stereotypes are mirrored by large language
models (LLMs). We draw on the sociolinguistic literature on dialect perception
to analyze traits commonly associated with dialect speakers. Based on these
traits, we assess the dialect naming bias and dialect usage bias expressed by
LLMs in two tasks: an association task and a decision task. To assess a model's
dialect usage bias, we construct a novel evaluation corpus that pairs sentences
from seven regional German dialects (e.g., Alemannic and Bavarian) with their
standard German counterparts. We find that: (1) in the association task, all
evaluated LLMs exhibit significant dialect naming and dialect usage bias
against German dialect speakers, reflected in negative adjective associations;
(2) all models reproduce these dialect naming and dialect usage biases in their
decision making; and (3) contrary to prior work showing minimal bias with
explicit demographic mentions, we find that explicitly labeling linguistic
demographics--German dialect speakers--amplifies bias more than implicit cues
like dialect usage.

</details>


### [21] [Do LLMs Align Human Values Regarding Social Biases? Judging and Explaining Social Biases with LLMs](https://arxiv.org/abs/2509.13869)
*Yang Liu,Chenhui Chu*

Main category: cs.CL

TL;DR: 研究发现大型语言模型在社交偏见方面与人类价值观存在错位，模型规模大小不一定影响错位率，不同模型家族对特定场景类型有偏好，小模型经过微调后能生成更易读的解释但模型认同度较低。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型与人类价值观在社交偏见方面的对齐情况，特别是探究不同类型偏见场景下的差异，以及模型对偏见解释的理解能力。

Method: 通过分析12个来自4个模型家族的LLM和4个数据集，评估模型在不同类型偏见场景中的对齐表现，并研究模型对偏见解释的理解能力，同时通过微调小模型赋予其解释偏见的能力。

Result: 大参数规模的LLM不一定有更低的错位率和攻击成功率；LLM对特定类型场景有对齐偏好；同一模型家族的模型判断一致性更高；LLM对偏见解释的理解能力无显著差异；微调后的小模型能生成更易读但模型认同度较低的解释。

Conclusion: LLM在社交偏见价值观对齐方面存在场景类型偏好和模型家族一致性特征，模型规模不是决定对齐质量的关键因素，小模型经过适当训练可以具备偏见解释能力但需要进一步提升模型认同度。

Abstract: Large language models (LLMs) can lead to undesired consequences when
misaligned with human values, especially in scenarios involving complex and
sensitive social biases. Previous studies have revealed the misalignment of
LLMs with human values using expert-designed or agent-based emulated bias
scenarios. However, it remains unclear whether the alignment of LLMs with human
values differs across different types of scenarios (e.g., scenarios containing
negative vs. non-negative questions). In this study, we investigate the
alignment of LLMs with human values regarding social biases (HVSB) in different
types of bias scenarios. Through extensive analysis of 12 LLMs from four model
families and four datasets, we demonstrate that LLMs with large model parameter
scales do not necessarily have lower misalignment rate and attack success rate.
Moreover, LLMs show a certain degree of alignment preference for specific types
of scenarios and the LLMs from the same model family tend to have higher
judgment consistency. In addition, we study the understanding capacity of LLMs
with their explanations of HVSB. We find no significant differences in the
understanding of HVSB across LLMs. We also find LLMs prefer their own generated
explanations. Additionally, we endow smaller language models (LMs) with the
ability to explain HVSB. The generation results show that the explanations
generated by the fine-tuned smaller LMs are more readable, but have a
relatively lower model agreeability.

</details>


### [22] [Combining Evidence and Reasoning for Biomedical Fact-Checking](https://arxiv.org/abs/2509.13879)
*Mariano Barone,Antonio Romano,Giuseppe Riccio,Marco Postiglione,Vincenzo Moscato*

Main category: cs.CL

TL;DR: CER框架通过结合科学证据检索、大语言模型推理和监督验证预测，在生物医学事实核查中实现了最先进的性能，有效减少幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 医疗领域的错误信息（如疫苗犹豫和未经证实的治疗方法）对公共卫生和医疗系统信任构成威胁，而生物医学事实核查由于复杂术语、领域专业知识和科学证据基础需求而具有独特挑战。

Method: 提出CER框架，整合科学证据检索、大语言模型推理和监督验证预测，利用大语言模型的文本生成能力和高质量生物医学科学证据检索技术。

Result: 在专家标注数据集（HealthFC、BioASQ-7b、SciFact）上展示了最先进的性能和良好的跨数据集泛化能力。

Conclusion: CER框架通过证据检索和推理的有效结合，为生物医学事实核查提供了可靠解决方案，代码和数据已开源以确保透明度和可重复性。

Abstract: Misinformation in healthcare, from vaccine hesitancy to unproven treatments,
poses risks to public health and trust in medical systems. While machine
learning and natural language processing have advanced automated fact-checking,
validating biomedical claims remains uniquely challenging due to complex
terminology, the need for domain expertise, and the critical importance of
grounding in scientific evidence. We introduce CER (Combining Evidence and
Reasoning), a novel framework for biomedical fact-checking that integrates
scientific evidence retrieval, reasoning via large language models, and
supervised veracity prediction. By integrating the text-generation capabilities
of large language models with advanced retrieval techniques for high-quality
biomedical scientific evidence, CER effectively mitigates the risk of
hallucinations, ensuring that generated outputs are grounded in verifiable,
evidence-based sources. Evaluations on expert-annotated datasets (HealthFC,
BioASQ-7b, SciFact) demonstrate state-of-the-art performance and promising
cross-dataset generalization. Code and data are released for transparency and
reproducibility: https: //github.com/PRAISELab-PicusLab/CER.

</details>


### [23] [Combating Biomedical Misinformation through Multi-modal Claim Detection and Evidence-based Verification](https://arxiv.org/abs/2509.13888)
*Mariano Barone,Antonio Romano,Giuseppe Riccio,Marco Postiglione,Vincenzo Moscato*

Main category: cs.CL

TL;DR: CER是一个用于生物医学事实核查的新框架，结合科学证据检索、大语言模型推理和监督真实性预测，在多个专业标注数据集上达到最先进性能


<details>
  <summary>Details</summary>
Motivation: 医疗领域中的错误信息（如疫苗犹豫和未经证实的治疗方法）对公共卫生和医疗系统信任构成风险，而生物医学声明验证因复杂术语、领域专业知识需求和科学证据基础的重要性而具有独特挑战

Method: CER框架整合科学证据检索、大语言模型推理和监督真实性预测，通过将大语言模型的文本生成能力与高质量生物医学科学证据的先进检索技术相结合，有效减少幻觉风险

Result: 在专家标注数据集（HealthFC、BioASQ-7b、SciFact）上的评估展示了最先进的性能和良好的跨数据集泛化能力

Conclusion: CER框架为生物医学事实核查提供了有效解决方案，通过结合证据检索和语言模型推理确保输出基于可验证的证据来源，代码和数据已开源以确保透明度和可重复性

Abstract: Misinformation in healthcare, from vaccine hesitancy to unproven treatments,
poses risks to public health and trust in medical systems. While machine
learning and natural language processing have advanced automated fact-checking,
validating biomedical claims remains uniquely challenging due to complex
terminology, the need for domain expertise, and the critical importance of
grounding in scientific evidence. We introduce CER (Combining Evidence and
Reasoning), a novel framework for biomedical fact-checking that integrates
scientific evidence retrieval, reasoning via large language models, and
supervised veracity prediction. By integrating the text-generation capabilities
of large language models with advanced retrieval techniques for high-quality
biomedical scientific evidence, CER effectively mitigates the risk of
hallucinations, ensuring that generated outputs are grounded in verifiable,
evidence-based sources. Evaluations on expert-annotated datasets (HealthFC,
BioASQ-7b, SciFact) demonstrate state-of-the-art performance and promising
cross-dataset generalization. Code and data are released for transparency and
reproducibility: https://github.com/PRAISELab-PicusLab/CER

</details>


### [24] [Do Large Language Models Understand Word Senses?](https://arxiv.org/abs/2509.13905)
*Domenico Meconi,Simone Stirpe,Federico Martelli,Leonardo Lavalle,Roberto Navigli*

Main category: cs.CL

TL;DR: 该论文评估了大语言模型在词义消歧任务中的表现，发现GPT-4o和DeepSeek-V3等领先模型在WSD任务上达到与专门系统相当的性能，并在生成任务中表现出高达98%的准确率。


<details>
  <summary>Details</summary>
Motivation: 尽管已有大量评估工作，但大语言模型是否真正理解词义仍未被充分探索，论文旨在填补这一空白。

Method: 通过评估指令调优LLMs的词义消歧能力，并与专门设计的先进系统进行比较；同时评估两个顶级开源和闭源LLMs在三种生成设置下的词义理解能力：定义生成、自由形式解释和示例生成。

Result: 在WSD任务中，领先模型达到与专门系统相当的性能，且在不同领域和难度级别上表现出更强的鲁棒性；在生成任务中，LLMs能够以高达98%的准确率解释上下文中的词义，其中自由形式解释任务表现最佳。

Conclusion: 大语言模型在词义理解和消歧方面表现出色，不仅能够与专门系统竞争，还在生成任务中展现出强大的词义解释能力，特别是在自由形式的解释任务中表现最优。

Abstract: Understanding the meaning of words in context is a fundamental capability for
Large Language Models (LLMs). Despite extensive evaluation efforts, the extent
to which LLMs show evidence that they truly grasp word senses remains
underexplored. In this paper, we address this gap by evaluating both i) the
Word Sense Disambiguation (WSD) capabilities of instruction-tuned LLMs,
comparing their performance to state-of-the-art systems specifically designed
for the task, and ii) the ability of two top-performing open- and closed-source
LLMs to understand word senses in three generative settings: definition
generation, free-form explanation, and example generation. Notably, we find
that, in the WSD task, leading models such as GPT-4o and DeepSeek-V3 achieve
performance on par with specialized WSD systems, while also demonstrating
greater robustness across domains and levels of difficulty. In the generation
tasks, results reveal that LLMs can explain the meaning of words in context up
to 98\% accuracy, with the highest performance observed in the free-form
explanation task, which best aligns with their generative capabilities.

</details>


### [25] [Linguistic Nepotism: Trading-off Quality for Language Preference in Multilingual RAG](https://arxiv.org/abs/2509.13930)
*Dayeon Ki,Marine Carpuat,Paul McNamee,Daniel Khashabi,Eugene Yang,Dawn Lawrie,Kevin Duh*

Main category: cs.CL

TL;DR: 研究发现多语言检索增强生成系统存在英语偏好偏见，模型会优先引用英文文档而非最相关的文档，这种偏见在资源较少语言和中间位置文档中更加明显。


<details>
  <summary>Details</summary>
Motivation: 研究多语言检索增强生成系统中不同文档语言的混合是否会对生成和引用产生意外影响，特别是语言偏好是否会影响引用选择。

Method: 使用模型内部机制的控制方法，在保持文档相关性等因素恒定的情况下，测量语言偏好，覆盖8种语言和6个开源模型。

Result: 模型在英语查询时优先引用英文来源，这种偏见在低资源语言和中间位置文档中被放大。模型有时会牺牲文档相关性来满足语言偏好。

Conclusion: 引用选择并非总是由信息量驱动，语言偏好显著影响多语言上下文中的引用行为，这对多语言RAG系统的设计和评估具有重要意义。

Abstract: Multilingual Retrieval-Augmented Generation (mRAG) systems enable language
models to answer knowledge-intensive queries with citation-supported responses
across languages. While such systems have been proposed, an open questions is
whether the mixture of different document languages impacts generation and
citation in unintended ways. To investigate, we introduce a controlled
methodology using model internals to measure language preference while holding
other factors such as document relevance constant. Across eight languages and
six open-weight models, we find that models preferentially cite English sources
when queries are in English, with this bias amplified for lower-resource
languages and for documents positioned mid-context. Crucially, we find that
models sometimes trade-off document relevance for language preference,
indicating that citation choices are not always driven by informativeness
alone. Our findings shed light on how language models leverage multilingual
context and influence citation behavior.

</details>


### [26] [Long-context Reference-based MT Quality Estimation](https://arxiv.org/abs/2509.13980)
*Sami Ul Haq,Chinonso Cynthia Osuji,Sheila Castilho,Brian Davis*

Main category: cs.CL

TL;DR: 基于COMET框架构建的翻译质量评估系统，通过长上下文数据增强和多种人工标注数据集整合，在WMT25评测中展现出优于短片段模型的性能


<details>
  <summary>Details</summary>
Motivation: 现有的翻译质量评估模型主要基于短片段训练，缺乏对长上下文信息的利用，限制了与人工评估的相关性

Method: 使用COMET框架，通过拼接领域内人工标注句子构建长上下文训练数据，整合MQM、SQM、DA等多种人工标注数据集，训练多语言回归模型预测质量分数

Result: 实验结果表明，引入长上下文信息相比仅使用短片段训练的模型，能够显著提高与人工评估的相关性

Conclusion: 长上下文信息的有效利用是提升翻译质量评估性能的关键因素，多数据集整合和长上下文训练策略具有重要价值

Abstract: In this paper, we present our submission to the Tenth Conference on Machine
Translation (WMT25) Shared Task on Automated Translation Quality Evaluation.
  Our systems are built upon the COMET framework and trained to predict
segment-level Error Span Annotation (ESA) scores using augmented long-context
data.
  To construct long-context training data, we concatenate in-domain,
human-annotated sentences and compute a weighted average of their scores.
  We integrate multiple human judgment datasets (MQM, SQM, and DA) by
normalising their scales and train multilingual regression models to predict
quality scores from the source, hypothesis, and reference translations.
  Experimental results show that incorporating long-context information
improves correlations with human judgments compared to models trained only on
short segments.

</details>


### [27] [Slim-SC: Thought Pruning for Efficient Scaling with Self-Consistency](https://arxiv.org/abs/2509.13990)
*Colin Hong,Xu Guo,Anand Chaanan Singh,Esha Choukse,Dmitrii Ustiugov*

Main category: cs.CL

TL;DR: Slim-SC是一种通过逐步剪枝冗余推理链来加速自洽性测试时缩放的方法，在保持或提升准确性的同时显著降低计算开销


<details>
  <summary>Details</summary>
Motivation: 自洽性(SC)方法虽然能提升LLM推理性能，但其数量级的计算开销限制了实际部署。现有加速方法主要依赖模型置信度或缺乏理论支持的启发式方法

Method: 提出Slim-SC方法，通过理论分析发现改进机会，在思维层面使用链间相似性识别和移除冗余推理链的逐步剪枝策略

Result: 在三个STEM推理数据集和两种LLM架构上，Slim-SC将推理延迟降低45%，KVC使用降低26%，同时保持或提升准确性

Conclusion: Slim-SC为自洽性提供了一种简单高效的测试时缩放替代方案，有效解决了计算开销问题

Abstract: Recently, Test-Time Scaling (TTS) has gained increasing attention for
improving LLM reasoning performance at test time without retraining the model.
A notable TTS technique is Self-Consistency (SC), which generates multiple
reasoning chains in parallel and selects the final answer via majority voting.
While effective, the order-of-magnitude computational overhead limits its broad
deployment. Prior attempts to accelerate SC mainly rely on model-based
confidence scores or heuristics with limited empirical support. For the first
time, we theoretically and empirically analyze the inefficiencies of SC and
reveal actionable opportunities for improvement. Building on these insights, we
propose Slim-SC, a step-wise pruning strategy that identifies and removes
redundant chains using inter-chain similarity at the thought level. Experiments
on three STEM reasoning datasets and two recent LLM architectures show that
Slim-SC reduces inference latency and KVC usage by up to 45% and 26%,
respectively, with R1-Distill, while maintaining or improving accuracy, thus
offering a simple yet efficient TTS alternative for SC.

</details>


### [28] [Early Stopping Chain-of-thoughts in Large Language Models](https://arxiv.org/abs/2509.14004)
*Minjia Mao,Bowen Yin,Yu Zhu,Xiao Fang*

Main category: cs.CL

TL;DR: ES-CoT是一种推理时方法，通过检测答案收敛性来提前停止思维链生成，减少约41%的推理token，同时保持与标准CoT相当的准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在解决复杂问题时需要生成长思维链，但这会产生高昂的推理成本。研究旨在通过检测答案收敛性来缩短思维链生成，减少推理开销。

Method: 在每个推理步骤结束时提示LLM输出当前最终答案（步骤答案），跟踪连续相同步骤答案的运行长度作为答案收敛的度量。当运行长度出现急剧增加并超过最小阈值时终止生成。

Result: 在五个推理数据集和三个LLM上的实验表明，ES-CoT平均减少约41%的推理token，同时保持与标准CoT相当的准确性。该方法还能无缝集成自一致性提示，并在超参数选择上保持鲁棒性。

Conclusion: ES-CoT是一种实用有效的推理效率提升方法，通过答案收敛检测实现早期停止，显著降低推理成本而不损失性能。

Abstract: Reasoning large language models (LLMs) have demonstrated superior capacities
in solving complicated problems by generating long chain-of-thoughts (CoT), but
such a lengthy CoT incurs high inference costs. In this study, we introduce
ES-CoT, an inference-time method that shortens CoT generation by detecting
answer convergence and stopping early with minimal performance loss. At the end
of each reasoning step, we prompt the LLM to output its current final answer,
denoted as a step answer. We then track the run length of consecutive identical
step answers as a measure of answer convergence. Once the run length exhibits a
sharp increase and exceeds a minimum threshold, the generation is terminated.
We provide both empirical and theoretical support for this heuristic: step
answers steadily converge to the final answer, and large run-length jumps
reliably mark this convergence. Experiments on five reasoning datasets across
three LLMs show that ES-CoT reduces the number of inference tokens by about
41\% on average while maintaining accuracy comparable to standard CoT. Further,
ES-CoT integrates seamlessly with self-consistency prompting and remains robust
across hyperparameter choices, highlighting it as a practical and effective
approach for efficient reasoning.

</details>


### [29] [Hala Technical Report: Building Arabic-Centric Instruction & Translation Models at Scale](https://arxiv.org/abs/2509.14008)
*Hasan Abed Al Kader Hammoud,Mohammad Zbeeb,Bernard Ghanem*

Main category: cs.CL

TL;DR: Hala是一个阿拉伯语为中心的指令和翻译模型家族，通过translate-and-tune流程构建，在阿拉伯语基准测试中取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 解决阿拉伯语NLP领域高质量指令数据和专用模型的缺乏问题，推动阿拉伯语NLP研究发展。

Method: 使用FP8压缩的AR-EN教师模型生成高质量双语监督数据，然后微调轻量级语言模型来翻译英语指令集，最后通过slerp合并技术平衡阿拉伯语专业化和基础模型优势。

Result: 在阿拉伯语基准测试中，Hala模型在"nano"(≤2B)和"small"(7-9B)类别中都取得了最先进的结果，超越了其基础模型。

Conclusion: Hala系列模型为阿拉伯语NLP提供了有效的解决方案，通过释放模型、数据、评估和配方来加速阿拉伯语NLP研究。

Abstract: We present Hala, a family of Arabic-centric instruction and translation
models built with our translate-and-tune pipeline. We first compress a strong
AR$\leftrightarrow$EN teacher to FP8 (yielding $\sim$2$\times$ higher
throughput with no quality loss) and use it to create high-fidelity bilingual
supervision. A lightweight language model LFM2-1.2B is then fine-tuned on this
data and used to translate high-quality English instruction sets into Arabic,
producing a million-scale corpus tailored to instruction following. We train
Hala models at 350M, 700M, 1.2B, and 9B parameters, and apply slerp merging to
balance Arabic specialization with base-model strengths. On Arabic-centric
benchmarks, Hala achieves state-of-the-art results within both the "nano"
($\leq$2B) and "small" (7-9B) categories, outperforming their bases. We release
models, data, evaluation, and recipes to accelerate research in Arabic NLP.

</details>


### [30] [Audio-Based Crowd-Sourced Evaluation of Machine Translation Quality](https://arxiv.org/abs/2509.14023)
*Sami Ul Haq,Sheila Castilho,Yvette Graham*

Main category: cs.CL

TL;DR: 这篇论文比较了文本评估和音频评估在机器翻译质量评估中的效果，发现音频评估能够提供更丰富自然的评估结果，建议将语音评估纳入未来的MT评估框架


<details>
  <summary>Details</summary>
Motivation: 虽然机器翻译取得了显著进步，但质量评估仍以文本为中心，而实际应用中许多翻译是通过语音进行的，需要更自然的语音评估方式

Method: 使用Amazon Mechanical Turk收集群众分析对10个WMT翻译系统进行文本和音频评估，进行统计显著性测试和自我复制实验验证音频评估的可靠性

Result: 音频评估得出的排名与文本评估基本一致，但在某些情况下能够识别翻译系统间的显著差异，这归因于语音模态更丰富自然

Conclusion: 音频评估方式能够提供更自然的评估结果，建议将语音基于评估纳入未来的机器翻译评估框架

Abstract: Machine Translation (MT) has achieved remarkable performance, with growing
interest in speech translation and multimodal approaches. However, despite
these advancements, MT quality assessment remains largely text centric,
typically relying on human experts who read and compare texts. Since many
real-world MT applications (e.g Google Translate Voice Mode, iFLYTEK
Translator) involve translation being spoken rather printed or read, a more
natural way to assess translation quality would be through speech as opposed
text-only evaluations. This study compares text-only and audio-based
evaluations of 10 MT systems from the WMT General MT Shared Task, using
crowd-sourced judgments collected via Amazon Mechanical Turk. We additionally,
performed statistical significance testing and self-replication experiments to
test reliability and consistency of audio-based approach. Crowd-sourced
assessments based on audio yield rankings largely consistent with text only
evaluations but, in some cases, identify significant differences between
translation systems. We attribute this to speech richer, more natural modality
and propose incorporating speech-based assessments into future MT evaluation
frameworks.

</details>


### [31] [You Are What You Train: Effects of Data Composition on Training Context-aware Machine Translation Models](https://arxiv.org/abs/2509.14031)
*Paweł Mąka,Yusuf Can Semerci,Jan Scholtes,Gerasimos Spanakis*

Main category: cs.CL

TL;DR: 训练数据中上下文丰富样本的稀疏性是阻碍机器翻译模型有效利用上下文的关键瓶颈，通过构建控制比例的训练数据集验证了该假设，并提出了两种改进策略


<details>
  <summary>Details</summary>
Motivation: 标准训练数据中上下文丰富样本的稀疏性被认为是机器翻译难以有效利用上下文的主要原因，需要系统验证这一假设并寻找解决方案

Method: 构建具有受控比例上下文相关样本的训练数据集，在单语和多语设置下验证数据稀疏性与模型性能的关系，并提出两种训练策略来更好地利用可用数据

Result: 证实了训练数据稀疏性与模型性能之间的强关联，发现不同上下文现象的改进不具有通用性，提出的训练策略在单语和多语设置下分别带来6%和8%的准确率提升

Conclusion: 数据稀疏性是上下文利用的关键瓶颈，需要针对性的训练策略来改善上下文利用能力，跨语言迁移效果有限

Abstract: Achieving human-level translations requires leveraging context to ensure
coherence and handle complex phenomena like pronoun disambiguation. Sparsity of
contextually rich examples in the standard training data has been hypothesized
as the reason for the difficulty of context utilization. In this work, we
systematically validate this claim in both single- and multilingual settings by
constructing training datasets with a controlled proportions of contextually
relevant examples. We demonstrate a strong association between training data
sparsity and model performance confirming sparsity as a key bottleneck.
Importantly, we reveal that improvements in one contextual phenomenon do no
generalize to others. While we observe some cross-lingual transfer, it is not
significantly higher between languages within the same sub-family. Finally, we
propose and empirically evaluate two training strategies designed to leverage
the available data. These strategies improve context utilization, resulting in
accuracy gains of up to 6 and 8 percentage points on the ctxPro evaluation in
single- and multilingual settings respectively.

</details>


### [32] [Enhancing Multi-Agent Debate System Performance via Confidence Expression](https://arxiv.org/abs/2509.14034)
*Zijie Lin,Bryan Hooi*

Main category: cs.CL

TL;DR: 提出ConfMAD框架，在多智能体辩论系统中引入置信度表达机制，解决LLM在辩论中难以有效沟通知识优势和避免过早收敛的问题，提升辩论效果和系统性能。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体辩论系统中，虽然某些LLM在特定任务上具有更优的知识或推理能力，但由于缺乏置信度表达，难以在辩论中清晰传达这种优势。不恰当的置信度表达会导致智能体固执坚持错误信念或过早收敛于次优答案，降低辩论效果。

Method: 提出ConfMAD框架，在多智能体辩论过程中集成置信度表达机制，让LLM能够明确传达其置信度水平。

Result: 实验结果表明该方法有效，并进一步分析了置信度如何影响辩论动态，为设计置信度感知的多智能体辩论系统提供了见解。

Conclusion: 在多智能体辩论系统中引入置信度表达机制能够显著提升辩论效果和整体系统性能，为构建更有效的协作AI系统提供了重要思路。

Abstract: Generative Large Language Models (LLMs) have demonstrated remarkable
performance across a wide range of tasks. Recent research has introduced
Multi-Agent Debate (MAD) systems, which leverage multiple LLMs to simulate
human debate and thereby improve task performance. However, while some LLMs may
possess superior knowledge or reasoning capabilities for specific tasks, they
often struggle to clearly communicate this advantage during debates, in part
due to a lack of confidence expression. Moreover, inappropriate confidence
expression can cause agents in MAD systems to either stubbornly maintain
incorrect beliefs or converge prematurely on suboptimal answers, ultimately
reducing debate effectiveness and overall system performance. To address these
challenges, we propose incorporating confidence expression into MAD systems to
allow LLMs to explicitly communicate their confidence levels. To validate this
approach, we develop ConfMAD, a MAD framework that integrates confidence
expression throughout the debate process. Experimental results demonstrate the
effectiveness of our method, and we further analyze how confidence influences
debate dynamics, offering insights into the design of confidence-aware MAD
systems.

</details>


### [33] [SSL-SSAW: Self-Supervised Learning with Sigmoid Self-Attention Weighting for Question-Based Sign Language Translation](https://arxiv.org/abs/2509.14036)
*Zekang Liu,Wei Feng,Fanhua Shang,Lianyu Hu,Jichao Feng,Liqing Gao*

Main category: cs.CL

TL;DR: 提出基于问题的手语翻译任务(QB-SLT)，通过跨模态自监督学习和Sigmoid自注意力加权融合方法，利用对话上下文提升手语翻译质量，在新建数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 对话在手语翻译中提供重要上下文线索，但传统依赖gloss标注成本高。问题对话自然发生且更易标注，探索如何有效整合对话信息来提升翻译效果。

Method: 提出SSL-SSAW融合方法：使用对比学习对齐多模态特征，引入Sigmoid自注意力加权模块自适应提取问题和手语序列特征，通过自监督学习利用问题文本增强表示能力。

Result: 在CSL-Daily-QA和PHOENIX-2014T-QA数据集上达到SOTA性能，问题辅助可以达到甚至超越gloss辅助的效果，可视化结果证明对话整合能有效提升翻译质量。

Conclusion: QB-SLT任务和SSL-SSAW方法有效证明了利用对话上下文可以显著提升手语翻译性能，且问题标注比gloss标注更易获取，具有实际应用价值。

Abstract: Sign Language Translation (SLT) bridges the communication gap between deaf
people and hearing people, where dialogue provides crucial contextual cues to
aid in translation. Building on this foundational concept, this paper proposes
Question-based Sign Language Translation (QB-SLT), a novel task that explores
the efficient integration of dialogue. Unlike gloss (sign language
transcription) annotations, dialogue naturally occurs in communication and is
easier to annotate. The key challenge lies in aligning multimodality features
while leveraging the context of the question to improve translation. To address
this issue, we propose a cross-modality Self-supervised Learning with Sigmoid
Self-attention Weighting (SSL-SSAW) fusion method for sign language
translation. Specifically, we employ contrastive learning to align
multimodality features in QB-SLT, then introduce a Sigmoid Self-attention
Weighting (SSAW) module for adaptive feature extraction from question and sign
language sequences. Additionally, we leverage available question text through
self-supervised learning to enhance representation and translation
capabilities. We evaluated our approach on newly constructed CSL-Daily-QA and
PHOENIX-2014T-QA datasets, where SSL-SSAW achieved SOTA performance. Notably,
easily accessible question assistance can achieve or even surpass the
performance of gloss assistance. Furthermore, visualization results demonstrate
the effectiveness of incorporating dialogue in improving translation quality.

</details>


### [34] [Canary-1B-v2 & Parakeet-TDT-0.6B-v3: Efficient and High-Performance Models for Multilingual ASR and AST](https://arxiv.org/abs/2509.14128)
*Monica Sekoyan,Nithin Rao Koluguri,Nune Tadevosyan,Piotr Zelasko,Travis Bartley,Nick Karpov,Jagadeesh Balam,Boris Ginsburg*

Main category: cs.CL

TL;DR: Canary-1B-v2是一个快速、鲁棒的多语言语音识别和语音翻译模型，支持25种欧洲语言，在保持高性能的同时比Whisper-large-v3快10倍。


<details>
  <summary>Details</summary>
Motivation: 开发一个高效的多语言自动语音识别和语音到文本翻译模型，解决现有模型速度慢、参数过多的问题，同时减少幻觉现象。

Method: 采用FastConformer编码器和Transformer解码器架构，使用两阶段预训练和微调过程，包含动态数据平衡技术，并利用NeMo强制对齐器提供时间戳。

Result: 模型在英语ASR上超越Whisper-large-v3且速度快10倍，在多语言ASR和AST任务上与Seamless-M4T-v2-large等大型模型竞争表现优异。

Conclusion: Canary-1B-v2证明了在相对较小的参数规模下可以实现高性能的多语言语音处理，同时发布了更轻量级的Parakeet-TDT-0.6B-v3模型。

Abstract: This report introduces Canary-1B-v2, a fast, robust multilingual model for
Automatic Speech Recognition (ASR) and Speech-to-Text Translation (AST). Built
with a FastConformer encoder and Transformer decoder, it supports 25 languages
primarily European. The model was trained on 1.7M hours of total data samples,
including Granary and NeMo ASR Set 3.0, with non-speech audio added to reduce
hallucinations for ASR and AST. We describe its two-stage pre-training and
fine-tuning process with dynamic data balancing, as well as experiments with an
nGPT encoder. Results show nGPT scales well with massive data, while
FastConformer excels after fine-tuning. For timestamps, Canary-1B-v2 uses the
NeMo Forced Aligner (NFA) with an auxiliary CTC model, providing reliable
segment-level timestamps for ASR and AST. Evaluations show Canary-1B-v2
outperforms Whisper-large-v3 on English ASR while being 10x faster, and
delivers competitive multilingual ASR and AST performance against larger models
like Seamless-M4T-v2-large and LLM-based systems. We also release
Parakeet-TDT-0.6B-v3, a successor to v2, offering multilingual ASR across the
same 25 languages with just 600M parameters.

</details>


### [35] [CS-FLEURS: A Massively Multilingual and Code-Switched Speech Dataset](https://arxiv.org/abs/2509.14161)
*Brian Yan,Injy Hamed,Shuichiro Shimizu,Vasista Lodagala,William Chen,Olga Iakovenko,Bashar Talafha,Amir Hussein,Alexander Polok,Kalvin Chang,Dominik Klement,Sara Althubaiti,Puyuan Peng,Matthew Wiesner,Thamar Solorio,Ahmed Ali,Sanjeev Khudanpur,Shinji Watanabe,Chih-Chen Chen,Zhen Wu,Karim Benharrak,Anuj Diwan,Samuele Cornell,Eunjung Yeo,Kwanghee Choi,Carlos Carvalho,Karen Rosero*

Main category: cs.CL

TL;DR: CS-FLEURS是一个新的代码切换语音识别和翻译数据集，包含4个测试集，覆盖113种语言对的52种语言，以及128小时的训练数据，旨在扩展代码切换语音研究的范围。


<details>
  <summary>Details</summary>
Motivation: 为了解决高资源语言之外的代码切换语音识别和翻译系统开发和评估的需求，提供更广泛的语言覆盖和多样化的数据来源。

Method: 构建包含4个测试集的数据集：1）14种X-英语语言对的真实语音阅读合成代码切换句子；2）16种X-英语语言对的生成式文本转语音；3）60种{阿拉伯语、普通话、印地语、西班牙语}-X语言对的生成式文本转语音；4）45种X-英语低资源语言对的拼接式文本转语音。同时提供128小时的生成式文本转语音训练数据。

Result: 成功创建了CS-FLEURS数据集，包含总计113种独特代码切换语言对，覆盖52种语言，为代码切换语音研究提供了全面的基准测试资源。

Conclusion: CS-FLEURS数据集有助于拓宽未来代码切换语音研究的范围，为开发更强大的多语言代码切换语音系统提供了重要资源。

Abstract: We present CS-FLEURS, a new dataset for developing and evaluating
code-switched speech recognition and translation systems beyond high-resourced
languages. CS-FLEURS consists of 4 test sets which cover in total 113 unique
code-switched language pairs across 52 languages: 1) a 14 X-English language
pair set with real voices reading synthetically generated code-switched
sentences, 2) a 16 X-English language pair set with generative text-to-speech
3) a 60 {Arabic, Mandarin, Hindi, Spanish}-X language pair set with the
generative text-to-speech, and 4) a 45 X-English lower-resourced language pair
test set with concatenative text-to-speech. Besides the four test sets,
CS-FLEURS also provides a training set with 128 hours of generative
text-to-speech data across 16 X-English language pairs. Our hope is that
CS-FLEURS helps to broaden the scope of future code-switched speech research.
Dataset link: https://huggingface.co/datasets/byan/cs-fleurs.

</details>


### [36] [AssoCiAm: A Benchmark for Evaluating Association Thinking while Circumventing Ambiguity](https://arxiv.org/abs/2509.14171)
*Yifan Liu,Wenkuan Zhao,Shanshan Zhong,Jinghui Qin,Mingfu Liang,Zhongzhan Huang,Wushao Wen*

Main category: cs.CL

TL;DR: 提出了AssoCiAm基准，通过混合计算方法解决关联任务中的模糊性问题，评估MLLMs的联想能力，发现认知与联想之间存在强正相关关系。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型(MLLMs)评估框架往往忽视了关联任务中固有的模糊性，这种模糊性源于联想的发散性，会降低评估的可靠性。

Method: 将模糊性分解为内部模糊性和外部模糊性，引入AssoCiAm基准，采用混合计算方法来规避模糊性，对MLLMs进行广泛实验。

Result: 发现认知与联想之间存在强正相关关系，评估过程中的模糊性会使MLLMs的行为变得更加随机化，验证了该方法能确保更准确可靠的评估。

Conclusion: AssoCiAm基准能够有效解决关联评估中的模糊性问题，为MLLMs的联想能力提供更可靠的评估框架，支持向AGI发展的研究。

Abstract: Recent advancements in multimodal large language models (MLLMs) have garnered
significant attention, offering a promising pathway toward artificial general
intelligence (AGI). Among the essential capabilities required for AGI,
creativity has emerged as a critical trait for MLLMs, with association serving
as its foundation. Association reflects a model' s ability to think creatively,
making it vital to evaluate and understand. While several frameworks have been
proposed to assess associative ability, they often overlook the inherent
ambiguity in association tasks, which arises from the divergent nature of
associations and undermines the reliability of evaluations. To address this
issue, we decompose ambiguity into two types-internal ambiguity and external
ambiguity-and introduce AssoCiAm, a benchmark designed to evaluate associative
ability while circumventing the ambiguity through a hybrid computational
method. We then conduct extensive experiments on MLLMs, revealing a strong
positive correlation between cognition and association. Additionally, we
observe that the presence of ambiguity in the evaluation process causes MLLMs'
behavior to become more random-like. Finally, we validate the effectiveness of
our method in ensuring more accurate and reliable evaluations. See Project Page
for the data and codes.

</details>


### [37] [Synthesizing Behaviorally-Grounded Reasoning Chains: A Data-Generation Framework for Personal Finance LLMs](https://arxiv.org/abs/2509.14180)
*Akhil Theerthala*

Main category: cs.CL

TL;DR: 提出了一个端到端的个性化财务顾问框架，通过整合金融背景和行为金融研究构建监督数据，训练8B参数模型在准确性、流畅性和个性化方面达到14-32B大模型的性能，同时成本降低80%。


<details>
  <summary>Details</summary>
Motivation: 现有财务顾问系统维护成本高且收益不佳，需要开发更高效、个性化的端到端财务顾问解决方案。

Method: 构建包含金融背景和行为金融研究的19k样本推理数据集，对Qwen-3-8B模型进行全面微调。

Result: 8B模型在事实准确性、流畅性和个性化指标上与14-32B大模型性能相当，成本降低80%。

Conclusion: 通过精心数据筛选和行为整合，小模型可以实现与大模型相当的财务顾问性能，显著降低成本。

Abstract: Personalized financial advice requires consideration of user goals,
constraints, risk tolerance, and jurisdiction. Prior LLM work has focused on
support systems for investors and financial planners. Simultaneously, numerous
recent studies examine broader personal finance tasks, including budgeting,
debt management, retirement, and estate planning, through agentic pipelines
that incur high maintenance costs, yielding less than 25% of their expected
financial returns. In this study, we introduce a novel and reproducible
framework that integrates relevant financial context with behavioral finance
studies to construct supervision data for end-to-end advisors. Using this
framework, we create a 19k sample reasoning dataset and conduct a comprehensive
fine-tuning of the Qwen-3-8B model on the dataset. Through a held-out test
split and a blind LLM-jury study, we demonstrate that through careful data
curation and behavioral integration, our 8B model achieves performance
comparable to significantly larger baselines (14-32B parameters) across factual
accuracy, fluency, and personalization metrics while incurring 80% lower costs
than the larger counterparts.

</details>


### [38] [Framing Migration: A Computational Analysis of UK Parliamentary Discourse](https://arxiv.org/abs/2509.14197)
*Vahid Ghafouri,Robert McNeil,Teodor Yankov,Madeleine Sumption,Luc Rocher,Scott A. Hale,Adam Mahdi*

Main category: cs.CL

TL;DR: 使用大语言模型分析英美议会75年移民辩论，发现美国政治极化加剧而英国党派立场相对一致，但保守党与工党意识形态差距在2025年达到最负面水平。英国移民话语转向安全化叙事，长期整合框架减少。


<details>
  <summary>Details</summary>
Motivation: 通过大规模计算分析比较英美议会移民话语的长期演变，探索大语言模型在政治历史语境中进行细粒度话语分析的可行性。

Method: 使用开源大语言模型对英国议会75年辩论和美国国会辩论进行立场标注，建立半自动化框架提取细粒度叙事框架，追踪跨时间和政党的移民态度变化。

Result: 美国移民话语日益极化，英国各党派立场相对一致但保守党与工党意识形态差距在2025年达到最负面；英国话语转向边境管控等安全化叙事，社会整合等长期框架减少；移民讨论从国内法转向国际法和人权。

Conclusion: 大语言模型能够支持政治历史语境中可扩展的细粒度话语分析，揭示了移民话语的长期演变趋势和国别差异。

Abstract: We present a large-scale computational analysis of migration-related
discourse in UK parliamentary debates spanning over 75 years and compare it
with US congressional discourse. Using open-weight LLMs, we annotate each
statement with high-level stances toward migrants and track the net tone toward
migrants across time and political parties. For the UK, we extend this with a
semi-automated framework for extracting fine-grained narrative frames to
capture nuances of migration discourse. Our findings show that, while US
discourse has grown increasingly polarised, UK parliamentary attitudes remain
relatively aligned across parties, with a persistent ideological gap between
Labour and the Conservatives, reaching its most negative level in 2025. The
analysis of narrative frames in the UK parliamentary statements reveals a shift
toward securitised narratives such as border control and illegal immigration,
while longer-term integration-oriented frames such as social integration have
declined. Moreover, discussions of national law about immigration have been
replaced over time by international law and human rights, revealing nuances in
discourse trends. Taken together broadly, our findings demonstrate how LLMs can
support scalable, fine-grained discourse analysis in political and historical
contexts.

</details>


### [39] [Apertus: Democratizing Open and Compliant LLMs for Global Language Environments](https://arxiv.org/abs/2509.14233)
*Alejandro Hernández-Cano,Alexander Hägele,Allen Hao Huang,Angelika Romanou,Antoni-Joan Solergibert,Barna Pasztor,Bettina Messmer,Dhia Garbaya,Eduard Frank Ďurech,Ido Hakimi,Juan García Giraldo,Mete Ismayilzada,Negar Foroutan,Skander Moalla,Tiancheng Chen,Vinko Sabolčec,Yixuan Xu,Michael Aerni,Badr AlKhamissi,Ines Altemir Marinas,Mohammad Hossein Amani,Matin Ansaripour,Ilia Badanin,Harold Benoit,Emanuela Boros,Nicholas Browning,Fabian Bösch,Maximilian Böther,Niklas Canova,Camille Challier,Clement Charmillot,Jonathan Coles,Jan Deriu,Arnout Devos,Lukas Drescher,Daniil Dzenhaliou,Maud Ehrmann,Dongyang Fan,Simin Fan,Silin Gao,Miguel Gila,María Grandury,Diba Hashemi,Alexander Hoyle,Jiaming Jiang,Mark Klein,Andrei Kucharavy,Anastasiia Kucherenko,Frederike Lübeck,Roman Machacek,Theofilos Manitaras,Andreas Marfurt,Kyle Matoba,Simon Matrenok,Henrique Mendoncça,Fawzi Roberto Mohamed,Syrielle Montariol,Luca Mouchel,Sven Najem-Meyer,Jingwei Ni,Gennaro Oliva,Matteo Pagliardini,Elia Palme,Andrei Panferov,Léo Paoletti,Marco Passerini,Ivan Pavlov,Auguste Poiroux,Kaustubh Ponkshe,Nathan Ranchin,Javi Rando,Mathieu Sauser,Jakhongir Saydaliev,Muhammad Ali Sayfiddinov,Marian Schneider,Stefano Schuppli,Marco Scialanga,Andrei Semenov,Kumar Shridhar,Raghav Singhal,Anna Sotnikova,Alexander Sternfeld,Ayush Kumar Tarun,Paul Teiletche,Jannis Vamvas,Xiaozhe Yao,Hao Zhao Alexander Ilic,Ana Klimovic,Andreas Krause,Caglar Gulcehre,David Rosenthal,Elliott Ash,Florian Tramèr,Joost VandeVondele,Livio Veraldi,Martin Rajman,Thomas Schulthess,Torsten Hoefler,Antoine Bosselut,Martin Jaggi,Imanol Schlag*

Main category: cs.CL

TL;DR: Apertus是一个完全开源的大型语言模型套件，专注于数据合规性和多语言表示，使用开放可用数据训练，避免版权问题，支持1800多种语言，在8B和70B规模上达到先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决当前开源模型生态系统中数据合规性和多语言代表性的系统性缺陷，许多现有模型发布权重时缺乏可复现的数据管道且不尊重内容所有者权利。

Method: 使用完全开放可用数据进行预训练，尊重robots.txt排除规则，过滤非许可、有毒和个人身份信息内容；采用Goldfish目标函数抑制数据记忆同时保持下游任务性能；在1800多种语言的15T tokens上训练，40%数据为非英语内容。

Result: Apertus模型在多语言基准测试中接近完全开源模型的最先进结果，与开源权重对应模型相当或超越；发布了8B和70B两种规模的模型。

Conclusion: Apertus提供了一个完全透明、合规的开源LLM解决方案，不仅发布模型权重，还包括数据准备脚本、检查点、评估套件和训练代码等所有科学成果，支持透明审计和扩展。

Abstract: We present Apertus, a fully open suite of large language models (LLMs)
designed to address two systemic shortcomings in today's open model ecosystem:
data compliance and multilingual representation. Unlike many prior models that
release weights without reproducible data pipelines or regard for content-owner
rights, Apertus models are pretrained exclusively on openly available data,
retroactively respecting robots.txt exclusions and filtering for
non-permissive, toxic, and personally identifiable content. To mitigate risks
of memorization, we adopt the Goldfish objective during pretraining, strongly
suppressing verbatim recall of data while retaining downstream task
performance. The Apertus models also expand multilingual coverage, training on
15T tokens from over 1800 languages, with ~40% of pretraining data allocated to
non-English content. Released at 8B and 70B scales, Apertus approaches
state-of-the-art results among fully open models on multilingual benchmarks,
rivalling or surpassing open-weight counterparts. Beyond model weights, we
release all scientific artifacts from our development cycle with a permissive
license, including data preparation scripts, checkpoints, evaluation suites,
and training code, enabling transparent audit and extension.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [40] [An Empirical Study on Failures in Automated Issue Solving](https://arxiv.org/abs/2509.13941)
*Simiao Liu,Fang Liu,Liehao Li,Xin Tan,Yinghao Zhu,Xiaoli Lian,Li Zhang*

Main category: cs.SE

TL;DR: 本文分析了SWE-Bench基准测试中自动化问题解决的失败模式，提出了包含3个阶段、9个主要类别和25个子类别的失败分类法，并设计了专家-执行者协作框架来解决推理缺陷和认知僵局问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLM驱动的自动化问题解决工具在SWE-Bench基准测试中仍有大量失败案例，但现有评估主要关注聚合成功率，缺乏对失败根本原因的系统分析，难以诊断模型弱点或指导针对性改进。

Method: 首先分析三种最先进工具在SWE-Bench-Verified任务中的性能和效率，然后对150个失败案例进行系统手动分析，建立失败模式分类法，最后提出专家-执行者协作框架，其中专家代理提供战略监督和纠偏。

Result: 研究发现两种架构范式具有不同的失败特征，大多数代理失败源于有缺陷的推理和认知僵局。提出的专家-执行者框架成功解决了领先单代理22.2%之前无法解决的问题。

Conclusion: 通过诊断性评估和协作设计，可以构建更强大的自动化问题解决代理，失败模式分析为改进模型性能提供了明确方向。

Abstract: Automated issue solving seeks to autonomously identify and repair defective
code snippets across an entire codebase. SWE-Bench has emerged as the most
widely adopted benchmark for evaluating progress in this area. While LLM-based
agentic tools show great promise, they still fail on a substantial portion of
tasks. Moreover, current evaluations primarily report aggregate issue-solving
rates, which obscure the underlying causes of success and failure, making it
challenging to diagnose model weaknesses or guide targeted improvements. To
bridge this gap, we first analyze the performance and efficiency of three SOTA
tools, spanning both pipeline-based and agentic architectures, in automated
issue solving tasks of SWE-Bench-Verified under varying task characteristics.
Furthermore, to move from high-level performance metrics to underlying cause
analysis, we conducted a systematic manual analysis of 150 failed instances.
From this analysis, we developed a comprehensive taxonomy of failure modes
comprising 3 primary phases, 9 main categories, and 25 fine-grained
subcategories. Then we systematically analyze the distribution of the
identified failure modes, the results reveal distinct failure fingerprints
between the two architectural paradigms, with the majority of agentic failures
stemming from flawed reasoning and cognitive deadlocks. Motivated by these
insights, we propose a collaborative Expert-Executor framework. It introduces a
supervisory Expert agent tasked with providing strategic oversight and
course-correction for a primary Executor agent. This architecture is designed
to correct flawed reasoning and break the cognitive deadlocks that frequently
lead to failure. Experiments show that our framework solves 22.2% of previously
intractable issues for a leading single agent. These findings pave the way for
building more robust agents through diagnostic evaluation and collaborative
design.

</details>


### [41] [Reasoning Efficiently Through Adaptive Chain-of-Thought Compression: A Self-Optimizing Framework](https://arxiv.org/abs/2509.14093)
*Kerui Huang,Shuhan Liu,Xing Hu,Tongtong Xu,Lingfeng Bao,Xin Xia*

Main category: cs.SE

TL;DR: Chain-of-Thought推理虽然能提升大语言模型性能，但会带来高计算成本。研究发现过长的推理反而有害，因此提出了SEER自适应框架来压缩推理过程，在保持准确性的同时显著减少计算开销。


<details>
  <summary>Details</summary>
Motivation: Chain-of-Thought推理虽然能提高LLM在算术、逻辑和常识任务中的准确性和鲁棒性，但会导致计算成本激增（延迟增加、内存使用增加、KV缓存需求增加），特别是在需要简洁确定性输出的软件工程任务中。研究发现过长的推理反而会导致截断、准确率下降和高达5倍的延迟。

Method: 提出了SEER（Self-Enhancing Efficient Reasoning）自适应框架，结合Best-of-N采样和任务感知自适应过滤，通过预推理输出动态调整阈值来压缩Chain-of-Thought推理，减少冗余和计算开销。

Result: SEER在三个软件工程任务和一个数学任务上的评估显示：平均缩短Chain-of-Thought推理42.1%，通过减少截断提高了准确性，并消除了大多数无限循环。

Conclusion: SEER是一个实用方法，可以使Chain-of-Thought增强的LLM在资源受限的情况下更加高效和鲁棒，挑战了"推理越长越好"的假设，并强调了自适应控制的重要性。

Abstract: Chain-of-Thought (CoT) reasoning enhances Large Language Models (LLMs) by
prompting intermediate steps, improving accuracy and robustness in arithmetic,
logic, and commonsense tasks. However, this benefit comes with high
computational costs: longer outputs increase latency, memory usage, and
KV-cache demands. These issues are especially critical in software engineering
tasks where concise and deterministic outputs are required. To investigate
these trade-offs, we conduct an empirical study based on code generation
benchmarks. The results reveal that longer CoT does not always help. Excessive
reasoning often causes truncation, accuracy drops, and latency up to five times
higher, with failed outputs consistently longer than successful ones. These
findings challenge the assumption that longer reasoning is inherently better
and highlight the need for adaptive CoT control. Motivated by this, we propose
SEER (Self-Enhancing Efficient Reasoning), an adaptive framework that
compresses CoT while preserving accuracy. SEER combines Best-of-N sampling with
task-aware adaptive filtering, dynamically adjusting thresholds based on
pre-inference outputs to reduce verbosity and computational overhead. We then
evaluate SEER on three software engineering tasks and one math task. On
average, SEER shortens CoT by 42.1%, improves accuracy by reducing truncation,
and eliminates most infinite loops. These results demonstrate SEER as a
practical method to make CoT-enhanced LLMs more efficient and robust, even
under resource constraints.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [42] [When Avatars Have Personality: Effects on Engagement and Communication in Immersive Medical Training](https://arxiv.org/abs/2509.14132)
*Julia S. Dollis,Iago A. Brito,Fernanda B. Färber,Pedro S. F. B. Ribeiro,Rafael T. Sousa,Arlindo R. Galvão Filho*

Main category: cs.HC

TL;DR: 将大型语言模型集成到VR中创建具有医学一致性和个性特征的虚拟患者，用于医疗沟通培训，医生评估显示该方法可行且有效


<details>
  <summary>Details</summary>
Motivation: VR在模拟物理环境方面表现出色，但在训练复杂人际技能方面效果有限，因为缺乏心理上合理的虚拟人类。这在医疗教育等高风险领域是一个关键缺口，因为沟通是核心能力

Method: 提出一个框架，将大型语言模型(LLMs)集成到沉浸式VR中，创建医学上一致的虚拟患者，具有独特且一致的人格特征。采用模块化架构，将人格与临床数据分离。通过混合方法的受试者内研究进行评估，让执业医师参与模拟咨询

Result: 该方法不仅可行，而且被医师认为是一种高度有益和有效的培训增强手段。分析揭示了关键设计原则，包括"真实性-冗长悖论"（沟通较少的智能体可能显得更人工）以及挑战需要被感知为真实才能具有指导性

Conclusion: 这项工作为开发下一代社会智能VR培训环境提供了一个经过验证的框架和关键见解

Abstract: While virtual reality (VR) excels at simulating physical environments, its
effectiveness for training complex interpersonal skills is limited by a lack of
psychologically plausible virtual humans. This is a critical gap in high-stakes
domains like medical education, where communication is a core competency. This
paper introduces a framework that integrates large language models (LLMs) into
immersive VR to create medically coherent virtual patients with distinct,
consistent personalities, built on a modular architecture that decouples
personality from clinical data. We evaluated our system in a mixed-method,
within-subjects study with licensed physicians who engaged in simulated
consultations. Results demonstrate that the approach is not only feasible but
is also perceived by physicians as a highly rewarding and effective training
enhancement. Furthermore, our analysis uncovers critical design principles,
including a ``realism-verbosity paradox" where less communicative agents can
seem more artificial, and the need for challenges to be perceived as authentic
to be instructive. This work provides a validated framework and key insights
for developing the next generation of socially intelligent VR training
environments.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [43] [Explicit Reasoning Makes Better Judges: A Systematic Study on Accuracy, Efficiency, and Robustness](https://arxiv.org/abs/2509.13332)
*Pratik Jayarao,Himanshu Gupta,Neeraj Varshney,Chaitanya Dwivedi*

Main category: cs.AI

TL;DR: 这篇论文系统性比较了"思考型"和"非思考型"LLM在LLM作为评判的情况下的表现，发现明确的推理过程能够显著提高准确性、效率和稳健性


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型越来越多地被采用为自动化评判器，确保其可靠性、效率和稳健性变得至关重要

Method: 使用开源Qwen 3模型（0.6B、1.7B和4B参数），在RewardBench任务上评估准确性和计算效率，并检验多种增强策略如上下文学习、指南引导评判、参考基准评估和n最佳聚合

Result: 思考型模型准确性高约10%，计算开销仅为2倍以下，而少量学习等增强方法效果谨慎但成本高过8倍以下；思考型模型在各种偏见条件下保持更高一致性（6%平均更高），多语言实验也证实了明确推理的优势

Conclusion: 明确的推理过程在LLM作为评判的范式中具有明显优势，不仅在准确性和效率方面，还在稳健性方面都有显著改善

Abstract: As Large Language Models (LLMs) are increasingly adopted as automated judges
in benchmarking and reward modeling, ensuring their reliability, efficiency,
and robustness has become critical. In this work, we present a systematic
comparison of "thinking" and "non-thinking" LLMs in the LLM-as-a-judge paradigm
using open-source Qwen 3 models of relatively small sizes (0.6B, 1.7B, and 4B
parameters). We evaluate both accuracy and computational efficiency (FLOPs) on
RewardBench tasks, and further examine augmentation strategies for non-thinking
models, including in-context learning, rubric-guided judging, reference-based
evaluation, and n-best aggregation. Our results show that despite these
enhancements, non-thinking models generally fall short of their thinking
counterparts. Our results show that thinking models achieve approximately 10%
points higher accuracy with little overhead (under 2x), in contrast to
augmentation strategies like few-shot learning, which deliver modest gains at a
higher cost (>8x). Bias and robustness analyses further demonstrate that
thinking models maintain significantly greater consistency under a variety of
bias conditions such as positional, bandwagon, identity, diversity, and random
biases (6% higher on average). We further extend our experiments to the
multilingual setting and our results confirm that explicit reasoning extends
its benefits beyond English. Overall, our work results in several important
findings that provide systematic evidence that explicit reasoning offers clear
advantages in the LLM-as-a-judge paradigm not only in accuracy and efficiency
but also in robustness.

</details>


### [44] [Teaching LLMs to Plan: Logical Chain-of-Thought Instruction Tuning for Symbolic Planning](https://arxiv.org/abs/2509.13351)
*Pulkit Verma,Ngoc La,Anthony Favier,Swaroop Mishra,Julie A. Shah*

Main category: cs.AI

TL;DR: 提出了PDDL-Instruct指令调优框架，通过逻辑思维链推理增强大语言模型的符号规划能力，在标准基准测试中达到94%的规划准确率，相比基线模型提升66%


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在各种任务中表现出色，但在需要形式化表示（如PDDL）的结构化符号规划方面能力有限，需要弥合通用推理能力与自动规划所需逻辑精度之间的差距

Method: 开发指令提示引导模型通过精确的逻辑推理步骤，严格推理动作适用性、状态转换和计划有效性，将规划过程分解为关于前提条件满足、效果应用和不变性保持的显式推理链

Result: 在多个规划领域的实验结果显示，基于思维链推理的指令调优模型规划能力显著提升，在标准基准测试中达到94%的规划准确率

Conclusion: 该工作为大语言模型与自动规划之间的鸿沟搭建了桥梁，为开发更好的AI规划系统提供了有前景的方向

Abstract: Large language models (LLMs) have demonstrated impressive capabilities across
diverse tasks, yet their ability to perform structured symbolic planning
remains limited, particularly in domains requiring formal representations like
the Planning Domain Definition Language (PDDL). In this paper, we present a
novel instruction tuning framework, PDDL-Instruct, designed to enhance LLMs'
symbolic planning capabilities through logical chain-of-thought reasoning. Our
approach focuses on teaching models to rigorously reason about action
applicability, state transitions, and plan validity using explicit logical
inference steps. By developing instruction prompts that guide models through
the precise logical reasoning required to determine when actions can be applied
in a given state, we enable LLMs to self-correct their planning processes
through structured reflection. The framework systematically builds verification
skills by decomposing the planning process into explicit reasoning chains about
precondition satisfaction, effect application, and invariant preservation.
Experimental results on multiple planning domains show that our
chain-of-thought reasoning based instruction-tuned models are significantly
better at planning, achieving planning accuracy of up to 94% on standard
benchmarks, representing a 66% absolute improvement over baseline models. This
work bridges the gap between the general reasoning capabilities of LLMs and the
logical precision required for automated planning, offering a promising
direction for developing better AI planning systems.

</details>


### [45] [SteeringControl: Holistic Evaluation of Alignment Steering in LLMs](https://arxiv.org/abs/2509.13450)
*Vincent Siu,Nicholas Crispino,David Park,Nathan W. Henry,Zhun Wang,Yang Liu,Dawn Song,Chenguang Wang*

Main category: cs.AI

TL;DR: SteeringControl是一个评估表示引导方法的基准测试，重点关注偏见、有害生成和幻觉等核心对齐目标，以及这些方法对次要行为（如奉承和常识道德）的影响。


<details>
  <summary>Details</summary>
Motivation: 现有的对齐工作主要关注真实性或推理能力来展示表示引导的副作用，但许多权衡关系尚未被系统性地理解。

Method: 构建了一个模块化的引导框架，基于独特组件作为现有方法的构建块，收集了安全相关的主要和次要行为数据集，评估了五种流行引导方法的有效性。

Result: 在Qwen-2.5-7B和Llama-3.1-8B上的结果显示，强引导性能取决于引导方法、模型和目标行为的特定组合，不良组合会导致严重的概念纠缠。

Conclusion: 表示引导的效果高度依赖于方法、模型和目标的组合选择，需要系统性的评估框架来理解这些复杂的权衡关系。

Abstract: We introduce SteeringControl, a benchmark for evaluating representation
steering methods across core alignment objectives--bias, harmful generation,
and hallucination--and their effects on secondary behaviors such as sycophancy
and commonsense morality. While prior alignment work often highlights
truthfulness or reasoning ability to demonstrate the side effects of
representation steering, we find there are many unexplored tradeoffs not yet
understood in a systematic way. We collect a dataset of safety-relevant primary
and secondary behaviors to evaluate steering effectiveness and behavioral
entanglement centered around five popular steering methods. To enable this, we
craft a modular steering framework based on unique components that serve as the
building blocks of many existing methods. Our results on Qwen-2.5-7B and
Llama-3.1-8B find that strong steering performance is dependent on the specific
combination of steering method, model, and targeted behavior, and that severe
concept entanglement can result from poor combinations of these three as well.
We release our code here:
https://github.com/wang-research-lab/SteeringControl.git.

</details>


### [46] [See, Think, Act: Teaching Multimodal Agents to Effectively Interact with GUI by Identifying Toggles](https://arxiv.org/abs/2509.13615)
*Zongru Wu,Rui Mao,Zhiyuan Tian,Pengzhou Cheng,Tianjie Ju,Zheng Wu,Lingzhong Dong,Haiyue Sheng,Zhuosheng Zhang,Gongshen Liu*

Main category: cs.AI

TL;DR: 提出了State-aware Reasoning (StaR)训练方法，解决多模态代理在GUI切换控制指令执行不可靠的问题，特别是在当前状态与期望状态匹配时的错误执行。


<details>
  <summary>Details</summary>
Motivation: 现有多模态代理在图形用户界面(GUI)切换控制指令执行方面存在不可靠性，特别是在当前切换状态已经符合期望状态时无法正确处理，这成为GUI控制的关键瓶颈。

Method: 构建了基于公开数据集的二进制切换指令状态控制基准，提出了StaR训练方法，教导代理感知当前切换状态、分析指令中的期望状态，并相应采取行动。

Result: 在三个多模态代理上的实验表明，StaR可以将切换指令执行准确率提高30%以上。在三个公开基准测试上的进一步评估显示，StaR还能提升一般任务性能。动态环境评估突显了StaR在现实应用中的潜力。

Conclusion: StaR方法有效解决了多模态代理在GUI切换控制中的可靠性问题，显著提升了指令执行准确率，并展现出在真实世界应用中的良好前景。

Abstract: The advent of multimodal agents facilitates effective interaction within
graphical user interface (GUI), especially in ubiquitous GUI control. However,
their inability to reliably execute toggle control instructions remains a key
bottleneck. To investigate this, we construct a state control benchmark with
binary toggle instructions from public datasets. Evaluations of existing agents
demonstrate their unreliability, particularly when the current toggle state
already matches the desired state. To address the challenge, we propose
State-aware Reasoning (StaR), a training method that teaches agents to perceive
the current toggle state, analyze the desired state from the instruction, and
act accordingly. Experiments on three multimodal agents demonstrate that StaR
can improve toggle instruction execution accuracy by over 30\%. Further
evaluations on three public benchmarks show that StaR also enhances general
task performance. Finally, evaluations on a dynamic environment highlight the
potential of StaR for real-world applications. Code, benchmark, and
StaR-enhanced agents are available at https://github.com/ZrW00/StaR.

</details>


### [47] [THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical Reasoning](https://arxiv.org/abs/2509.13761)
*Qikai Chang,Zhenrong Zhang,Pengfei Hu,Jiefeng Ma,Yicheng Pan,Jianshu Zhang,Jun Du,Quan Liu,Jianqing Gao*

Main category: cs.AI

TL;DR: THOR是一个通过强化学习进行工具集成层次优化的方法，用于提升LLM在数学推理和代码生成任务中的性能，通过多智能体数据生成、分层优化和自校正机制实现最先进效果


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在数学推理方面取得显著进展，但在高精度任务如数值计算和符号操作方面仍存在困难。现有工具集成方法面临三个关键挑战：构建工具集成推理数据、进行细粒度优化和增强推理能力

Method: 提出THOR方法：1) TIRGen多智能体actor-critic流水线构建高质量工具集成推理数据集；2) 分层RL策略联合优化轨迹级问题解决和步骤级代码生成；3) 自校正机制利用工具反馈动态修正推理路径

Result: THOR在不同模型上展现出强泛化能力，在推理和非推理模型中都表现有效。在多个数学基准测试中达到同类规模模型的最先进性能，同时在代码基准测试上也获得一致改进

Conclusion: THOR通过工具集成层次优化有效解决了LLM在高精度数学任务中的局限性，其多智能体数据生成、分层RL优化和动态自校正机制为工具增强的LLM推理提供了有效解决方案

Abstract: Large Language Models (LLMs) have made remarkable progress in mathematical
reasoning, but still continue to struggle with high-precision tasks like
numerical computation and formal symbolic manipulation. Integrating external
tools has emerged as a promising approach to bridge this gap. Despite recent
advances, existing methods struggle with three key challenges: constructing
tool-integrated reasoning data, performing fine-grained optimization, and
enhancing inference. To overcome these limitations, we propose THOR
(Tool-Integrated Hierarchical Optimization via RL). First, we introduce TIRGen,
a multi-agent actor-critic-based pipeline for constructing high-quality
datasets of tool-integrated reasoning paths, aligning with the policy and
generalizing well across diverse models. Second, to perform fine-grained
hierarchical optimization, we introduce an RL strategy that jointly optimizes
for both trajectory-level problem solving and step-level code generation. This
is motivated by our key insight that the success of an intermediate tool call
is a strong predictor of the final answer's correctness. Finally, THOR
incorporates a self-correction mechanism that leverages immediate tool feedback
to dynamically revise erroneous reasoning paths during inference. Our approach
demonstrates strong generalization across diverse models, performing
effectively in both reasoning and non-reasoning models. It further achieves
state-of-the-art performance for models of a similar scale on multiple
mathematical benchmarks, while also delivering consistent improvements on code
benchmarks. Our code will be publicly available at
https://github.com/JingMog/THOR.

</details>


### [48] [Exploring Major Transitions in the Evolution of Biological Cognition With Artificial Neural Networks](https://arxiv.org/abs/2509.13968)
*Konstantinos Voudouris,Andrew Barron,Marta Halina,Colin Klein,Matishalin Patel*

Main category: cs.AI

TL;DR: 该研究使用人工神经网络模型探索信息流结构变化如何导致认知性能的跃迁式变化，发现循环网络相比前馈网络在处理复杂语法时表现出质的性能提升，并观察到训练难度形成的过渡屏障效应。


<details>
  <summary>Details</summary>
Motivation: 探索认知进化是否通过一系列主要转变来实现，这些转变通过操纵生物神经网络结构来根本改变信息流，从而产生认知性能的跃迁式变化。

Method: 使用理想化信息流模型和人工神经网络，比较前馈、循环和分层拓扑结构，在控制网络大小和资源的情况下测试它们学习不同复杂度人工语法的性能。

Result: 循环网络相比前馈网络在处理输入类型方面有质的扩展，在学习最复杂语法时表现出质的性能提升；循环网络的训练难度形成了过渡屏障和偶然不可逆性；分层网络在语法学习任务中并未优于非分层网络。

Conclusion: 某些信息流结构的变化确实能够产生认知性能的跃迁式转变，这支持了认知进化可能通过主要转变实现的假说，同时训练难度等特征也符合进化转变的关键特性。

Abstract: Transitional accounts of evolution emphasise a few changes that shape what is
evolvable, with dramatic consequences for derived lineages. More recently it
has been proposed that cognition might also have evolved via a series of major
transitions that manipulate the structure of biological neural networks,
fundamentally changing the flow of information. We used idealised models of
information flow, artificial neural networks (ANNs), to evaluate whether
changes in information flow in a network can yield a transitional change in
cognitive performance. We compared networks with feed-forward, recurrent and
laminated topologies, and tested their performance learning artificial grammars
that differed in complexity, controlling for network size and resources. We
documented a qualitative expansion in the types of input that recurrent
networks can process compared to feed-forward networks, and a related
qualitative increase in performance for learning the most complex grammars. We
also noted how the difficulty in training recurrent networks poses a form of
transition barrier and contingent irreversibility -- other key features of
evolutionary transitions. Not all changes in network topology confer a
performance advantage in this task set. Laminated networks did not outperform
non-laminated networks in grammar learning. Overall, our findings show how some
changes in information flow can yield transitions in cognitive performance.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [49] [Annotating Satellite Images of Forests with Keywords from a Specialized Corpus in the Context of Change Detection](https://arxiv.org/abs/2509.13586)
*Nathalie Neptune,Josiane Mothe*

Main category: cs.CV

TL;DR: 基于深度学习的亚马逊雨林砍伐检测方法，通过卫星图像对比和视觉语义模型自动标注变化区域


<details>
  <summary>Details</summary>
Motivation: 亚马逊雨林砍伐对全球碳排放和生物多样性有重大影响，需要有效的监测工具来检测和研究砍伐影响

Method: 利用地球观测卫星图像对，采用深度学习技术比较不同日期同一区域的图像，识别森林覆盖变化，并提出视觉语义模型自动用相关关键词标注检测到的变化

Result: 在亚马逊图像对数据集上评估了该方法，证明了其在检测砍伐和生成相关标注方面的有效性

Conclusion: 该方法为监测和研究亚马逊砍伐影响提供了有用工具，虽然专注于环境应用，但具有通用性可应用于其他领域

Abstract: The Amazon rain forest is a vital ecosystem that plays a crucial role in
regulating the Earth's climate and providing habitat for countless species.
Deforestation in the Amazon is a major concern as it has a significant impact
on global carbon emissions and biodiversity. In this paper, we present a method
for detecting deforestation in the Amazon using image pairs from Earth
observation satellites. Our method leverages deep learning techniques to
compare the images of the same area at different dates and identify changes in
the forest cover. We also propose a visual semantic model that automatically
annotates the detected changes with relevant keywords. The candidate annotation
for images are extracted from scientific documents related to the Amazon
region. We evaluate our approach on a dataset of Amazon image pairs and
demonstrate its effectiveness in detecting deforestation and generating
relevant annotations. Our method provides a useful tool for monitoring and
studying the impact of deforestation in the Amazon. While we focus on
environment applications of our work by using images of deforestation in the
Amazon rain forest to demonstrate the effectiveness of our proposed approach,
it is generic enough to be applied to other domains.

</details>


### [50] [Diving into Mitigating Hallucinations from a Vision Perspective for Large Vision-Language Models](https://arxiv.org/abs/2509.13836)
*Weihang Wang,Xinhao Li,Ziyue Wang,Yan Pang,Jielei Zhang,Peiyi Li,Qiang Zhang,Longwen Gao*

Main category: cs.CV

TL;DR: 提出了VHBench-10基准测试和VisionWeaver方法，通过细粒度幻觉分类和多专家特征聚合来减少大型视觉语言模型的物体幻觉问题


<details>
  <summary>Details</summary>
Motivation: 不同视觉编码器的训练范式差异导致其具有不同的归纳偏置，从而产生不同的幻觉表现，现有基准测试无法捕捉这种细粒度差异

Method: 构建VHBench-10基准测试（约10,000样本，10个细粒度幻觉类别），提出VisionWeaver - 基于上下文感知路由网络，使用全局视觉特征生成路由信号，动态聚合多个专家视觉特征

Result: 评估确认不同编码器具有独特的幻觉特征，VisionWeaver能显著减少幻觉并提升整体模型性能

Conclusion: 视觉编码器的选择对LVLMs的幻觉问题至关重要，提出的VisionWeaver方法通过动态特征聚合有效缓解了物体幻觉问题

Abstract: Object hallucination in Large Vision-Language Models (LVLMs) significantly
impedes their real-world applicability. As the primary component for accurately
interpreting visual information, the choice of visual encoder is pivotal. We
hypothesize that the diverse training paradigms employed by different visual
encoders instill them with distinct inductive biases, which leads to their
diverse hallucination performances. Existing benchmarks typically focus on
coarse-grained hallucination detection and fail to capture the diverse
hallucinations elaborated in our hypothesis. To systematically analyze these
effects, we introduce VHBench-10, a comprehensive benchmark with approximately
10,000 samples for evaluating LVLMs across ten fine-grained hallucination
categories. Our evaluations confirm encoders exhibit unique hallucination
characteristics. Building on these insights and the suboptimality of simple
feature fusion, we propose VisionWeaver, a novel Context-Aware Routing Network.
It employs global visual features to generate routing signals, dynamically
aggregating visual features from multiple specialized experts. Comprehensive
experiments confirm the effectiveness of VisionWeaver in significantly reducing
hallucinations and improving overall model performance.

</details>


### [51] [Dense Video Understanding with Gated Residual Tokenization](https://arxiv.org/abs/2509.14199)
*Haichao Zhang,Wenhao Chai,Shwai He,Ang Li,Yun Fu*

Main category: cs.CV

TL;DR: 该论文提出了一种高帧率视频理解方案DVU和相应的DIVE测试标准，通过GRT框架减少令牌化成本和计算量，实现了高效的密集时间分辨率视频理解。


<details>
  <summary>Details</summary>
Motivation: 当前VLLM模型和测试标准主要使用低帧率采样，弃置了密集的时间信息，无法满足需要精确时间对齐的任务如讲座理解。

Method: 提出GRT两阶段框架：(1)运动补偿间间隔令牌化通过像素级运动估计跳过静态区域，实现次线性令牌增长；(2)语义场景内令牌合并融合同一场景内的静态区域令牌，保持动态语义同时减少冗余。

Result: 在DIVE标准上，GRT表现超过更大的VLLM基线模型，并且体现出与帧率的正向缩放性能。

Conclusion: 密集时间信息对视频理解至关重要，GRT框架能够高效地支持高帧率视频理解，为密集时间推理任务提供了可行解决方案。

Abstract: High temporal resolution is essential for capturing fine-grained details in
video understanding. However, current video large language models (VLLMs) and
benchmarks mostly rely on low-frame-rate sampling, such as uniform sampling or
keyframe selection, discarding dense temporal information. This compromise
avoids the high cost of tokenizing every frame, which otherwise leads to
redundant computation and linear token growth as video length increases. While
this trade-off works for slowly changing content, it fails for tasks like
lecture comprehension, where information appears in nearly every frame and
requires precise temporal alignment. To address this gap, we introduce Dense
Video Understanding (DVU), which enables high-FPS video comprehension by
reducing both tokenization time and token overhead. Existing benchmarks are
also limited, as their QA pairs focus on coarse content changes. We therefore
propose DIVE (Dense Information Video Evaluation), the first benchmark designed
for dense temporal reasoning. To make DVU practical, we present Gated Residual
Tokenization (GRT), a two-stage framework: (1) Motion-Compensated Inter-Gated
Tokenization uses pixel-level motion estimation to skip static regions during
tokenization, achieving sub-linear growth in token count and compute. (2)
Semantic-Scene Intra-Tokenization Merging fuses tokens across static regions
within a scene, further reducing redundancy while preserving dynamic semantics.
Experiments on DIVE show that GRT outperforms larger VLLM baselines and scales
positively with FPS. These results highlight the importance of dense temporal
information and demonstrate that GRT enables efficient, scalable high-FPS video
understanding.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [52] [Privacy-Aware In-Context Learning for Large Language Models](https://arxiv.org/abs/2509.13625)
*Bishnu Bhusal,Manoj Acharya,Ramneet Kaur,Colin Samplawski,Anirban Roy,Adam D. Cobb,Rohit Chadha,Susmit Jha*

Main category: cs.LG

TL;DR: 这篇论文提出了一种基于差分隐私的私有预测框架，用于生成高质量合成文本同时提供强隐私保证。该方法无需微调模型，通过聚合分布和混合操作来提高效用性和隐私性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在自然语言处理方面取得重大进展，但存在敏感信息泄露的隐私风险。对手可能从提示中提取敏感信息，因此需要一种能够生成高质量文本同时确保强隐私保证的方法。

Method: 采用差分隐私(DP)框架，在不需要微调基础模型的前提下，对私有记录进行推理并聚合交叉网分布。还提出了简单的混合操作，将私有和公共推理结果结合起来以进一步提升效用性。

Result: 经验评估显示，该方法在上下文学习(ICL)任务上超过了之前的最先进方法，能够生成更长且连贯的合成文本，同时保持隐私保证。

Conclusion: 该研究提供了一种有前景的方向，能够在保持高效用性的同时实现隐私保护的文本生成，为解决大语言模型的隐私风险问题提供了有效的技术方案。

Abstract: Large language models (LLMs) have significantly transformed natural language
understanding and generation, but they raise privacy concerns due to potential
exposure of sensitive information. Studies have highlighted the risk of
information leakage, where adversaries can extract sensitive information
embedded in the prompts. In this work, we introduce a novel private prediction
framework for generating high-quality synthetic text with strong privacy
guarantees. Our approach leverages the Differential Privacy (DP) framework to
ensure worst-case theoretical bounds on information leakage without requiring
any fine-tuning of the underlying models.The proposed method performs inference
on private records and aggregates the resulting per-token output distributions.
This enables the generation of longer and coherent synthetic text while
maintaining privacy guarantees. Additionally, we propose a simple blending
operation that combines private and public inference to further enhance
utility. Empirical evaluations demonstrate that our approach outperforms
previous state-of-the-art methods on in-context-learning (ICL) tasks, making it
a promising direction for privacy-preserving text generation while maintaining
high utility.

</details>


### [53] [Language models' activations linearly encode training-order recency](https://arxiv.org/abs/2509.14223)
*Dmitrii Krasheninnikov,Richard E. Turner,David Krueger*

Main category: cs.LG

TL;DR: 语言模型的激活值线性编码了信息在训练过程中被学习的时间顺序，模型能够区分不同时间学习的信息


<details>
  <summary>Details</summary>
Motivation: 研究语言模型是否以及如何编码信息的学习时间顺序，这对于理解模型如何处理冲突数据和知识修改具有重要意义

Method: 通过顺序微调Llama-3.2-1B模型在六个不相交但相似的命名实体数据集上，分析激活值的线性编码特性，使用线性探测和微调方法验证时间信号的准确性

Result: 发现激活值中心点在2D子空间中按训练顺序直线排列，线性探测能准确区分早期和晚期实体（约90%准确率），微调后模型能报告未见实体的训练阶段（约80%准确率）

Conclusion: 语言模型确实能够通过激活值线性编码信息的学习时间，这一发现对模型处理冲突数据和知识更新具有重要意义

Abstract: We show that language models' activations linearly encode when information
was learned during training. Our setup involves creating a model with a known
training order by sequentially fine-tuning Llama-3.2-1B on six disjoint but
otherwise similar datasets about named entities. We find that the average
activations of test samples for the six training datasets encode the training
order: when projected into a 2D subspace, these centroids are arranged exactly
in the order of training and lie on a straight line. Further, we show that
linear probes can accurately (~90%) distinguish "early" vs. "late" entities,
generalizing to entities unseen during the probes' own training. The model can
also be fine-tuned to explicitly report an unseen entity's training stage (~80%
accuracy). Interestingly, this temporal signal does not seem attributable to
simple differences in activation magnitudes, losses, or model confidence. Our
paper demonstrates that models are capable of differentiating information by
its acquisition time, and carries significant implications for how they might
manage conflicting data and respond to knowledge modifications.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [54] [Enhancing Time Awareness in Generative Recommendation](https://arxiv.org/abs/2509.13957)
*Sunkyung Lee,Seongmin Park,Jonghyo Kim,Mincheol Yoon,Jongwuk Lee*

Main category: cs.IR

TL;DR: GRUT模型通过时间感知提示和趋势感知推理，有效捕捉用户偏好中的时间动态信息，在生成式推荐任务中显著提升性能


<details>
  <summary>Details</summary>
Motivation: 现有生成式推荐方法主要关注物品序列顺序，但忽视了物品间的时间动态特性，这些时间信息能够反映用户偏好的演变过程

Method: 提出GRUT模型，包含时间感知提示（用户级时间上下文和物品级转移上下文）和训练免费的趋势感知推理方法，结合生成概率和趋势信息

Result: 在四个基准数据集上，Recall@5和NDCG@5指标分别提升15.4%和14.3%，显著优于现有最先进模型

Conclusion: 时间动态信息对捕捉用户偏好演变至关重要，GRUT模型通过有效利用各种时间信号，在生成式推荐任务中取得了显著性能提升

Abstract: Generative recommendation has emerged as a promising paradigm that formulates
the recommendations into a text-to-text generation task, harnessing the vast
knowledge of large language models. However, existing studies focus on
considering the sequential order of items and neglect to handle the temporal
dynamics across items, which can imply evolving user preferences. To address
this limitation, we propose a novel model, Generative Recommender Using Time
awareness (GRUT), effectively capturing hidden user preferences via various
temporal signals. We first introduce Time-aware Prompting, consisting of two
key contexts. The user-level temporal context models personalized temporal
patterns across timestamps and time intervals, while the item-level transition
context provides transition patterns across users. We also devise Trend-aware
Inference, a training-free method that enhances rankings by incorporating trend
information about items with generation likelihood. Extensive experiments
demonstrate that GRUT outperforms state-of-the-art models, with gains of up to
15.4% and 14.3% in Recall@5 and NDCG@5 across four benchmark datasets. The
source code is available at https://github.com/skleee/GRUT.

</details>


### [55] [GEM-Bench: A Benchmark for Ad-Injected Response Generation within Generative Engine Marketing](https://arxiv.org/abs/2509.14221)
*Silan Hu,Shiqi Zhang,Yimin Shi,Xiaokui Xiao*

Main category: cs.IR

TL;DR: GEM-Bench是首个针对生成引擎营销(GEM)中广告注入响应生成的综合基准，包含三个数据集、多维度评估指标和可扩展的多智能体框架基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试并非专门为GEM中的广告注入响应生成和评估而设计，这限制了该领域未来的研究发展。

Method: 提出了GEM-Bench基准，包括：1）涵盖聊天机器人和搜索场景的三个精选数据集；2）捕捉用户满意度和参与度多个维度的度量本体；3）在可扩展多智能体框架中实现的多个基线解决方案。

Result: 初步结果表明：基于提示的简单方法能实现合理的参与度（如点击率），但往往会降低用户满意度；而基于预先生成无广告响应再插入广告的方法有助于缓解此问题，但会引入额外开销。

Conclusion: 研究结果强调了未来需要设计更有效和高效的解决方案来生成GEM中的广告注入响应。

Abstract: Generative Engine Marketing (GEM) is an emerging ecosystem for monetizing
generative engines, such as LLM-based chatbots, by seamlessly integrating
relevant advertisements into their responses. At the core of GEM lies the
generation and evaluation of ad-injected responses. However, existing
benchmarks are not specifically designed for this purpose, which limits future
research. To address this gap, we propose GEM-Bench, the first comprehensive
benchmark for ad-injected response generation in GEM. GEM-Bench includes three
curated datasets covering both chatbot and search scenarios, a metric ontology
that captures multiple dimensions of user satisfaction and engagement, and
several baseline solutions implemented within an extensible multi-agent
framework. Our preliminary results indicate that, while simple prompt-based
methods achieve reasonable engagement such as click-through rate, they often
reduce user satisfaction. In contrast, approaches that insert ads based on
pre-generated ad-free responses help mitigate this issue but introduce
additional overhead. These findings highlight the need for future research on
designing more effective and efficient solutions for generating ad-injected
responses in GEM.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [56] [TICL: Text-Embedding KNN For Speech In-Context Learning Unlocks Speech Recognition Abilities of Large Multimodal Models](https://arxiv.org/abs/2509.13395)
*Haolong Zheng,Yekaterina Yegorova,Mark Hasegawa-Johnson*

Main category: eess.AS

TL;DR: 提出了TICL方法，通过文本嵌入KNN选择语义相关的上下文示例，显著提升多模态模型的语音识别性能，无需微调即可在多种挑战性任务上实现高达84.7%的相对WER降低。


<details>
  <summary>Details</summary>
Motivation: 语音基础模型已展现出上下文学习能力，但有效的上下文示例选择方法尚未充分探索，需要一种简单有效的方法来提升现有多模态模型的语音识别性能。

Method: 提出TICL方法，使用文本嵌入KNN基于语义上下文选择示例，构建简单管道来增强现有多模态模型的语音识别能力，无需进行模型微调。

Result: 在重口音英语、多语言语音和儿童语音等挑战性ASR任务上，该方法使模型超越了零样本性能，实现了高达84.7%的相对WER降低，消融研究证明了方法的鲁棒性和效率。

Conclusion: TICL方法通过语义上下文选择有效提升了语音上下文学习性能，为现有多模态模型的语音识别能力增强提供了一种简单有效的解决方案。

Abstract: Speech foundation models have recently demonstrated the ability to perform
Speech In-Context Learning (SICL). Selecting effective in-context examples is
crucial for SICL performance, yet selection methodologies remain underexplored.
In this work, we propose Text-Embedding KNN for SICL (TICL), a simple pipeline
that uses semantic context to enhance off-the-shelf large multimodal models'
speech recognition ability without fine-tuning. Across challenging automatic
speech recognition tasks, including accented English, multilingual speech, and
children's speech, our method enables models to surpass zero-shot performance
with up to 84.7% relative WER reduction. We conduct ablation studies to show
the robustness and efficiency of our method.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [57] [A TRRIP Down Memory Lane: Temperature-Based Re-Reference Interval Prediction For Instruction Caching](https://arxiv.org/abs/2509.14041)
*Henry Kao,Nikhil Sreekumar,Prabhdeep Singh Soni,Ali Sedaghati,Fang Su,Bryan Chan,Maziar Goudarzi,Reza Azimi*

Main category: cs.AR

TL;DR: TRRIP是一种软硬件协同设计方法，通过编译器分析代码温度（热/冷）并利用操作系统接口向硬件提供温度信息，优化指令缓存替换策略，减少热代码的驱逐率，在移动CPU上实现性能提升。


<details>
  <summary>Details</summary>
Motivation: 现代移动CPU软件由于复杂的运行时行为导致指令重用距离大，传统指令缓存替换策略不足。移动代码常出现CPU前端大量停顿，导致其他CPU资源饥饿。应用复杂度和代码占用增长速度快于片上内存，传统硬件中心方法已不适用。

Method: 提出TRRIP软硬件协同设计：编译器分析、分类和转换代码温度（热/冷），通过定义良好的OS接口使用代码页属性向硬件提供温度信息摘要。轻量级硬件扩展利用代码温度属性优化指令缓存替换策略。

Result: TRRIP可将指令L2 MPKI降低26.5%，在已使用PGO优化的移动代码上，基于RRIP缓存替换实现3.9%的几何平均加速比。

Conclusion: TRRIP设计实用且可在真实移动系统中采用，通过软硬件协同优化有效解决了移动CPU指令缓存管理问题，显著提升了性能。

Abstract: Modern mobile CPU software pose challenges for conventional instruction cache
replacement policies due to their complex runtime behavior causing high reuse
distance between executions of the same instruction. Mobile code commonly
suffers from large amounts of stalls in the CPU frontend and thus starvation of
the rest of the CPU resources. Complexity of these applications and their code
footprint are projected to grow at a rate faster than available on-chip memory
due to power and area constraints, making conventional hardware-centric methods
for managing instruction caches to be inadequate. We present a novel
software-hardware co-design approach called TRRIP (Temperature-based
Re-Reference Interval Prediction) that enables the compiler to analyze,
classify, and transform code based on "temperature" (hot/cold), and to provide
the hardware with a summary of code temperature information through a
well-defined OS interface based on using code page attributes. TRRIP's
lightweight hardware extension employs code temperature attributes to optimize
the instruction cache replacement policy resulting in the eviction rate
reduction of hot code. TRRIP is designed to be practical and adoptable in real
mobile systems that have strict feature requirements on both the software and
hardware components. TRRIP can reduce the L2 MPKI for instructions by 26.5%
resulting in geomean speedup of 3.9%, on top of RRIP cache replacement running
mobile code already optimized using PGO.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [58] [Noise Supervised Contrastive Learning and Feature-Perturbed for Anomalous Sound Detection](https://arxiv.org/abs/2509.13853)
*Shun Huang,Zhihua Fang,Liang He*

Main category: cs.SD

TL;DR: 提出了一种名为OS-SCL的单阶段监督对比学习方法，通过扰动嵌入空间特征和使用带噪声的监督对比学习，显著减少了不同机器同类型样本的误报问题，在DCASE 2020挑战赛上取得了优异性能。


<details>
  <summary>Details</summary>
Motivation: 解决无监督异常声音检测中，处理来自不同机器的同类型样本时频繁出现误报的问题，现有自监督方法尚未有效解决这一挑战。

Method: 提出OS-SCL训练技术：1）在嵌入空间扰动特征；2）采用单阶段带噪声监督对比学习方法；3）提出新的时频特征TFgram从原始音频中提取关键信息。

Result: 仅使用Log-Mel特征时达到94.64% AUC、88.42% pAUC和89.24% mAUC；使用TFgram特征时进一步提升至95.71% AUC、90.23% pAUC和91.23% mAUC。

Conclusion: OS-SCL方法有效解决了不同机器同类型样本的误报问题，提出的TFgram特征能更好地捕捉异常声音检测所需的关键信息，在DCASE 2020任务2上取得了state-of-the-art性能。

Abstract: Unsupervised anomalous sound detection aims to detect unknown anomalous
sounds by training a model using only normal audio data. Despite advancements
in self-supervised methods, the issue of frequent false alarms when handling
samples of the same type from different machines remains unresolved. This paper
introduces a novel training technique called one-stage supervised contrastive
learning (OS-SCL), which significantly addresses this problem by perturbing
features in the embedding space and employing a one-stage noisy supervised
contrastive learning approach. On the DCASE 2020 Challenge Task 2, it achieved
94.64\% AUC, 88.42\% pAUC, and 89.24\% mAUC using only Log-Mel features.
Additionally, a time-frequency feature named TFgram is proposed, which is
extracted from raw audio. This feature effectively captures critical
information for anomalous sound detection, ultimately achieving 95.71\% AUC,
90.23\% pAUC, and 91.23\% mAUC. The source code is available at:
\underline{www.github.com/huangswt/OS-SCL}.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [59] [An AI-Powered Framework for Analyzing Collective Idea Evolution in Deliberative Assemblies](https://arxiv.org/abs/2509.12577)
*Elinor Poole-Dayan,Deb Roy,Jad Kabbara*

Main category: cs.CY

TL;DR: 本文开发了基于LLM的方法来分析审议大会的转录文本，追踪观点演化和投票动态，为理解审议过程提供新的实证工具


<details>
  <summary>Details</summary>
Motivation: 在政治极化和社会分裂加剧的背景下，代表性审议大会作为解决复杂政策问题的民主论坛日益重要，但缺乏系统追踪观点演化的实证研究

Method: 开发基于大语言模型(LLM)的方法论，分析技术增强的线下审议大会转录文本，识别和可视化表达建议的空间，重构代表观点的演变过程

Result: 该方法能够揭示传统大会输出中不可见的高分辨率动态，为审议过程提供新颖的实证洞见

Conclusion: LLM方法能够有效追踪审议过程中观点的演化和优先排序，为理解民主审议机制提供了有力的分析工具

Abstract: In an era of increasing societal fragmentation, political polarization, and
erosion of public trust in institutions, representative deliberative assemblies
are emerging as a promising democratic forum for developing effective policy
outcomes on complex global issues. Despite theoretical attention, there remains
limited empirical work that systematically traces how specific ideas evolve,
are prioritized, or are discarded during deliberation to form policy
recommendations. Addressing these gaps, this work poses two central questions:
(1) How might we trace the evolution and distillation of ideas into concrete
recommendations within deliberative assemblies? (2) How does the deliberative
process shape delegate perspectives and influence voting dynamics over the
course of the assembly? To address these questions, we develop LLM-based
methodologies for empirically analyzing transcripts from a tech-enhanced
in-person deliberative assembly. The framework identifies and visualizes the
space of expressed suggestions. We also empirically reconstruct each delegate's
evolving perspective throughout the assembly. Our methods contribute novel
empirical insights into deliberative processes and demonstrate how LLMs can
surface high-resolution dynamics otherwise invisible in traditional assembly
outputs.

</details>


### [60] [Accuracy Paradox in Large Language Models: Regulating Hallucination Risks in Generative AI](https://arxiv.org/abs/2509.13345)
*Zihao Li,Weiwei Yi,Jiahong Chen*

Main category: cs.CY

TL;DR: 这篇论文提出"准确性谜困"，让为准确性作为主要指标来应对LLM幽灵问题可能会产生反效果，并呼吁更多元化、语境敏感的AI可信管理方案


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在日常决策中的普及，幽灵问题带来的认知风险和社会风险需要紧急关注。虽然准确性被视为减轻这些危害的主要标准，但过度依赖准确性可能会误诊问题并产生负面效果

Method: 基于跨学科文献，建立了幽灵类型分类法，并从三个相互交织的维度分析准确性谜困：输出结果、个体和社会。还检视了欧盟AI法案、GDPR和DSA等相关法规

Result: 准确性仅作为可靠性的表面代理指标，激励优化语言流畅性而忽视认知可靠性；无法检测非事实错误但仍导致误导的危害；法规对准确性的过度强调模糊了幽灵问题的更广泛社会影响

Conclusion: 当前法规在结构上无法有效处理这些认知、关系和系统性危害，需要向更多元化、语境敏感和抵御操纵的AI可信管理方案转变

Abstract: As Large Language Models (LLMs) permeate everyday decision-making, their
epistemic and societal risks demand urgent scrutiny. Hallucinations, the
generation of fabricated, misleading, oversimplified or untrustworthy outputs,
has emerged as imperative challenges. While regulatory, academic, and technical
discourse position accuracy as the principal benchmark for mitigating such
harms, this article contends that overreliance on accuracy misdiagnoses the
problem and has counterproductive effect: the accuracy paradox. Drawing on
interdisciplinary literatures, this article develops a taxonomy of
hallucination types and shows the paradox along three intertwining dimensions:
outputs, individuals and society. First, accuracy functions as a superficial
proxy for reliability, incentivising the optimisation of rhetorical fluency and
surface-level correctness over epistemic trustworthiness. This encourages
passive user trust in outputs that appear accurate but epistemically untenable.
Second, accuracy as a singular metric fails to detect harms that are not
factually false but are nonetheless misleading, value-laden, or socially
distorting, including consensus illusions, sycophantic alignment, and subtle
manipulation. Third, regulatory overemphasis on accuracy obscures the wider
societal consequences of hallucination, including social sorting, privacy
violations, equity harms, epistemic convergence that marginalises dissent,
reduces pluralism, and causes social deskilling. By examining the EU AI Act,
GDPR, and DSA, the article argues that current regulations are not yet
structurally equipped to address these epistemic, relational, and systemic
harms and exacerbated by the overreliance on accuracy. By exposing such
conceptual and practical challenges, this article calls for a fundamental shift
towards pluralistic, context-aware, and manipulation-resilient approaches to AI
trustworthy governance.

</details>


### [61] [CogniAlign: Survivability-Grounded Multi-Agent Moral Reasoning for Safe and Transparent AI](https://arxiv.org/abs/2509.13356)
*Hasin Jawad Ali,Ilhamul Azam,Ajwad Abrar,Md. Kamrul Hasan,Hasan Mahmud*

Main category: cs.CY

TL;DR: CogniAlign是一个基于自然主义道德现实主义的AI对齐框架，通过多学科专家代理的审议机制进行道德推理，在60多个道德问题上显著优于GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 解决AI与人类价值观对齐的挑战，现有方法存在抽象性、道德原则冲突性和不透明性问题。

Method: 基于自然主义道德现实主义，以生存能力为基础定义道德推理，通过神经科学、心理学、社会学和进化生物学等多学科专家代理进行结构化审议，由仲裁者综合判断。

Result: 在60多个道德问题上，CogniAlign平均在分析质量上提升16.2分，广度上提升14.3分，解释深度上提升28.4分。在海因茨困境中得分89.2 vs GPT-4o的69.2。

Conclusion: CogniAlign通过减少黑盒推理和避免欺骗性对齐，展示了跨学科审议作为可扩展AI对齐路径的潜力，实现安全透明的AI对齐。

Abstract: The challenge of aligning artificial intelligence (AI) with human values
persists due to the abstract and often conflicting nature of moral principles
and the opacity of existing approaches. This paper introduces CogniAlign, a
multi-agent deliberation framework based on naturalistic moral realism, that
grounds moral reasoning in survivability, defined across individual and
collective dimensions, and operationalizes it through structured deliberations
among discipline-specific scientist agents. Each agent, representing
neuroscience, psychology, sociology, and evolutionary biology, provides
arguments and rebuttals that are synthesized by an arbiter into transparent and
empirically anchored judgments. We evaluate CogniAlign on classic and novel
moral questions and compare its outputs against GPT-4o using a five-part
ethical audit framework. Results show that CogniAlign consistently outperforms
the baseline across more than sixty moral questions, with average performance
gains of 16.2 points in analytic quality, 14.3 points in breadth, and 28.4
points in depth of explanation. In the Heinz dilemma, for example, CogniAlign
achieved an overall score of 89.2 compared to GPT-4o's 69.2, demonstrating a
decisive advantage in handling moral reasoning. By reducing black-box reasoning
and avoiding deceptive alignment, CogniAlign highlights the potential of
interdisciplinary deliberation as a scalable pathway for safe and transparent
AI alignment.

</details>
