<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 47]
- [cs.CV](#cs.CV) [Total: 3]
- [cs.DL](#cs.DL) [Total: 2]
- [cs.SE](#cs.SE) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]
- [cs.AI](#cs.AI) [Total: 4]
- [cs.CR](#cs.CR) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Signature vs. Substance: Evaluating the Balance of Adversarial Resistance and Linguistic Quality in Watermarking Large Language Models](https://arxiv.org/abs/2511.13722)
*William Guo,Adaku Uchendu,Ana Smith*

Main category: cs.CL

TL;DR: 本文评估了多种LLM文本水印技术的鲁棒性和质量保持能力，发现这些技术能保持语义但会偏离原始写作风格，且容易受到对抗攻击，特别是回译攻击。


<details>
  <summary>Details</summary>
Motivation: 为了减轻LLM生成文本的潜在危害，研究者提出了水印技术，但现有技术存在影响文本质量和易受对抗攻击的问题，阻碍了其广泛应用。

Method: 通过比较改述和回译（英语→其他语言→英语）两种对抗攻击方式，并使用语言指标评估水印技术对文本质量和写作风格的保持能力。

Result: 水印技术能够保持语义，但会偏离未加水印文本的写作风格，且容易受到对抗攻击，特别是回译攻击。

Conclusion: 当前的水印技术在鲁棒性和写作风格保持方面存在不足，需要改进以促进更广泛的应用。

Abstract: To mitigate the potential harms of Large Language Models (LLMs)generated text, researchers have proposed watermarking, a process of embedding detectable signals within text. With watermarking, we can always accurately detect LLM-generated texts. However, recent findings suggest that these techniques often negatively affect the quality of the generated texts, and adversarial attacks can strip the watermarking signals, causing the texts to possibly evade detection. These findings have created resistance in the wide adoption of watermarking by LLM creators. Finally, to encourage adoption, we evaluate the robustness of several watermarking techniques to adversarial attacks by comparing paraphrasing and back translation (i.e., English $\to$ another language $\to$ English) attacks; and their ability to preserve quality and writing style of the unwatermarked texts by using linguistic metrics to capture quality and writing style of texts. Our results suggest that these watermarking techniques preserve semantics, deviate from the writing style of the unwatermarked texts, and are susceptible to adversarial attacks, especially for the back translation attack.

</details>


### [2] [Refine Thought: A Test-Time Inference Method for Embedding Model Reasoning](https://arxiv.org/abs/2511.13726)
*Guangzhi Wang,Kai Li,Yinghao Jiao,Zhi Liu*

Main category: cs.CL

TL;DR: RT（Refine Thought）是一种通过多次前向传递增强文本嵌入模型语义推理能力的方法，在语义推理任务上取得显著提升，同时保持通用语义理解任务的性能。


<details>
  <summary>Details</summary>
Motivation: 增强文本嵌入模型的语义推理能力，进一步激活预训练期间学习到的推理能力。

Method: 通过多次前向传递运行文本嵌入模型来获得最终的语义表示，这是一种测试时推理方法。

Result: 在BRIGHT和PJBenchmark1语义推理任务上取得显著改进，在C-MTEB等通用语义理解任务上保持稳定性能。

Conclusion: RT方法有效激活了解码器专用文本嵌入模型在预训练期间学习的语义推理能力，可作为测试时推理方法使用。

Abstract: We propose RT (Refine Thought), a method that can enhance the semantic rea-soning ability of text embedding models. The method obtains the final semanticrepresentation by running multiple forward passes of the text embedding model.Experiments show that RT achieves significant improvements on semantic reason-ing tasks in BRIGHT and the person job matching benchmark PJBenchmark1, while maintaining consistent performance on general-purpose semantic under-standing tasks such as C-MTEB. Our results indicate that RT is effective becauseit further activates the semantic reasoning ability learned during pretraining bydecoder-only text embedding models(e.g., Qwen3-Embedding-8B). RT canbe seen as a test-time inference method.

</details>


### [3] [Can QE-informed (Re)Translation lead to Error Correction?](https://arxiv.org/abs/2511.13884)
*Govardhan Padmanabhan*

Main category: cs.CL

TL;DR: 论文提出了两种无需训练的QE-informed错误校正方法：一种是通过选择多个LLM生成候选中质量最高的翻译，另一种是基于QE解释替换错误子串。第一种方法在WMT 2025任务中获胜。


<details>
  <summary>Details</summary>
Motivation: 虽然联合训练QE和APE系统能提升性能，但APE系统存在过度校正问题，导致性能下降。作者希望探索无需训练的简单方法来避免这个问题。

Method: 提出了两种无需训练的方法：1）QE-informed重翻译：从不同LLM生成的多个候选中选择质量最高的翻译；2）基于QE解释的替换：根据提供的QE解释替换错误子串，并使用条件启发式最小化编辑次数。

Result: 两种方法的Delta COMET得分分别为0.0201和-0.0108，第一种方法在子任务排行榜上获得获胜位置。

Conclusion: 无需训练的QE-informed重翻译方法在机器翻译错误校正任务中表现优于基于QE解释的替换方法，证明了简单选择策略的有效性。

Abstract: The paper presents two approaches submitted to the WMT 2025 Automated Translation Quality Evaluation Systems Task 3 - Quality Estimation (QE)-informed Segment-level Error Correction. While jointly training QE systems with Automatic Post-Editing (APE) has shown improved performance for both tasks, APE systems are still known to overcorrect the output of Machine Translation (MT), leading to a degradation in performance. We investigate a simple training-free approach - QE-informed Retranslation, and compare it with another within the same training-free paradigm. Our winning approach selects the highest-quality translation from multiple candidates generated by different LLMs. The second approach, more akin to APE, instructs an LLM to replace error substrings as specified in the provided QE explanation(s). A conditional heuristic was employed to minimise the number of edits, with the aim of maximising the Gain-to-Edit ratio. The two proposed approaches achieved a Delta COMET score of 0.0201 and -0.0108, respectively, leading the first approach to achieve the winning position on the subtask leaderboard.

</details>


### [4] [What Works for 'Lost-in-the-Middle' in LLMs? A Study on GM-Extract and Mitigations](https://arxiv.org/abs/2511.13900)
*Mihir Gupte,Eshan Dixit,Muhammad Tayyab,Arun Adiththan*

Main category: cs.CL

TL;DR: 论文研究了大型语言模型在长文本处理中的"中间迷失"现象，提出了GM-Extract基准数据集来评估检索控制变量的性能，并开发了两种评估指标来诊断失败模式。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在检索应用中处理长范围上下文时出现的"中间迷失"现象，该现象严重影响模型对关键信息的检索能力。

Method: 提出GM-Extract基准数据集，使用文档指标和变量提取指标两种评估方法，系统评估7-8B参数模型在多文档任务中的表现，并分析不同数据表示方式对性能的影响。

Result: 研究发现改变上下文窗口中数据的表示方式会显著影响检索性能，虽然未观察到一致的U形曲线，但发现了清晰的性能模式，并与困惑度得分相关。缓解方法的效果具有高度复杂性。

Conclusion: 缓解"中间迷失"现象的方法效果复杂，在某些场景下能提升性能，但在其他情况下可能产生负面影响，需要在具体应用场景中谨慎使用。

Abstract: The diminishing ability of large language models (LLMs) to effectively utilize long-range context-the "lost-in-the-middle" phenomenon-poses a significant challenge in retrieval-based LLM applications. To study the impact of this phenomenon in a real-world application setting, we introduce GM-Extract, a novel benchmark dataset meticulously designed to evaluate LLM performance on retrieval of control variables. To accurately diagnose failure modes, we propose a simple yet elegant evaluation system using two distinct metrics: one for spatial retrieval capability (Document Metric) and the other for semantic retrieval capability (Variable Extraction Metric). We conduct a systematic evaluation of 7-8B parameter models on two multi-document tasks (key-value extraction and question-answering), demonstrating a significant change in retrieval performance simply by altering how the data is represented in the context window. While a distinct U-shaped curve was not consistently observed, our analysis reveals a clear pattern of performance across models, which we further correlate with perplexity scores. Furthermore, we perform a literature survey of mitigation methods, which we categorize into two distinct approaches: black-box and white-box methods. We then apply these techniques to our benchmark, finding that their efficacy is highly nuanced. Our evaluation highlights scenarios where these strategies successfully improve performance, as well as surprising cases where they lead to a negative impact, providing a comprehensive understanding of their utility in a practical context.

</details>


### [5] [Hint-Augmented Re-ranking: Efficient Product Search using LLM-Based Query Decomposition](https://arxiv.org/abs/2511.13994)
*Yilun Zhu,Nikhita Vedula,Shervin Malmasi*

Main category: cs.CL

TL;DR: LLM框架解析电商搜索中的最高级查询意图，通过提取结构化提示来改进搜索性能，并开发高效方法将语义理解迁移到轻量级模型。


<details>
  <summary>Details</summary>
Motivation: 解决电商搜索中带有最高级词汇的查询需要多维度比较候选商品的问题，这类查询需要语言理解和领域知识。

Method: 开发框架将查询分解为属性-值提示，与检索过程并行生成，然后将最高级语义解释迁移到轻量级模型以解决延迟问题。

Result: 在MAP指标上提升10.9分，在MRR指标上提升5.9分，显著优于基线方法。

Conclusion: 该方法展示了如何表示和迁移最高级语义，推进检索系统中的语言解释能力，同时解决实际部署的约束。

Abstract: Search queries with superlatives (e.g., best, most popular) require comparing candidates across multiple dimensions, demanding linguistic understanding and domain knowledge. We show that LLMs can uncover latent intent behind these expressions in e-commerce queries through a framework that extracts structured interpretations or hints. Our approach decomposes queries into attribute-value hints generated concurrently with retrieval, enabling efficient integration into the ranking pipeline. Our method improves search performanc eby 10.9 points in MAP and ranking by 5.9 points in MRR over baselines. Since direct LLM-based reranking faces prohibitive latency, we develop an efficient approach transferring superlative interpretations to lightweight models. Our findings provide insights into how superlative semantics can be represented and transferred between models, advancing linguistic interpretation in retrieval systems while addressing practical deployment constraints.

</details>


### [6] [Knowledge-Grounded Agentic Large Language Models for Multi-Hazard Understanding from Reconnaissance Reports](https://arxiv.org/abs/2511.14010)
*Chenchen Kuai,Zihao Li,Braden Rosen,Stephanie Paan,Navid Jafari,Jean-Louis Briaud,Yunlong Zhang,Youssef M. A. Hashash,Yang Zhou*

Main category: cs.CL

TL;DR: MoRA-RAG是一个基于知识检索的LLM框架，可将灾后勘测报告转化为结构化数据，用于多灾害推理，准确率达94.5%，比零样本LLM提高30%。


<details>
  <summary>Details</summary>
Motivation: 灾后勘测报告包含理解多灾害相互作用的关键证据，但其非结构化叙述使得系统知识转移困难。LLMs在缺乏领域基础时会产生不可靠或虚构的输出。

Method: 提出MoRA-RAG框架，集成混合检索机制动态路由跨灾害特定数据库的查询，使用代理分块在检索过程中保持上下文连贯性，并包含验证循环评估证据充分性。

Result: 在HazardRecQA数据集上达到94.5%准确率，比零样本LLM提高30%，比最先进RAG系统提高10%，同时减少各种LLM架构中的幻觉。

Conclusion: MoRA-RAG为将灾后文档转化为可操作、可信赖的灾害韧性情报建立了新范式，使开源LLM达到与专有模型相当的性能。

Abstract: Post-disaster reconnaissance reports contain critical evidence for understanding multi-hazard interactions, yet their unstructured narratives make systematic knowledge transfer difficult. Large language models (LLMs) offer new potential for analyzing these reports, but often generate unreliable or hallucinated outputs when domain grounding is absent. This study introduces the Mixture-of-Retrieval Agentic RAG (MoRA-RAG), a knowledge-grounded LLM framework that transforms reconnaissance reports into a structured foundation for multi-hazard reasoning. The framework integrates a Mixture-of-Retrieval mechanism that dynamically routes queries across hazard-specific databases while using agentic chunking to preserve contextual coherence during retrieval. It also includes a verification loop that assesses evidence sufficiency, refines queries, and initiates targeted searches when information remains incomplete. We construct HazardRecQA by deriving question-answer pairs from GEER reconnaissance reports, which document 90 global events across seven major hazard types. MoRA-RAG achieves up to 94.5 percent accuracy, outperforming zero-shot LLMs by 30 percent and state-of-the-art RAG systems by 10 percent, while reducing hallucinations across diverse LLM architectures. MoRA-RAG also enables open-weight LLMs to achieve performance comparable to proprietary models. It establishes a new paradigm for transforming post-disaster documentation into actionable, trustworthy intelligence for hazard resilience.

</details>


### [7] [HiEAG: Evidence-Augmented Generation for Out-of-Context Misinformation Detection](https://arxiv.org/abs/2511.14027)
*Junjie Wu,Yumeng Fu,Nan Yu,Guohong Fu*

Main category: cs.CL

TL;DR: 提出了HiEAG框架，通过分层证据增强生成来改进多模态OOC虚假信息检测，强调外部一致性检查，在多个基准数据集上超越现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 现有OOC虚假信息检测方法过于关注内部一致性，忽视了图像-文本对与外部证据之间的外部一致性检查的重要性。

Method: HiEAG框架将外部一致性检查分解为检索、重排序和重写的综合引擎流程，利用AESP选择相关证据，AEGP改进MLLM的任务适应性。

Result: 在不同基准数据集上的实验结果表明，HiEAG在所有样本的准确率上超越了之前的最先进方法。

Conclusion: HiEAG通过分层证据增强生成框架有效提升了多模态OOC虚假信息检测的性能，证明了外部一致性检查的重要性。

Abstract: Recent advancements in multimodal out-of-context (OOC) misinformation detection have made remarkable progress in checking the consistencies between different modalities for supporting or refuting image-text pairs. However, existing OOC misinformation detection methods tend to emphasize the role of internal consistency, ignoring the significant of external consistency between image-text pairs and external evidence. In this paper, we propose HiEAG, a novel Hierarchical Evidence-Augmented Generation framework to refine external consistency checking through leveraging the extensive knowledge of multimodal large language models (MLLMs). Our approach decomposes external consistency checking into a comprehensive engine pipeline, which integrates reranking and rewriting, apart from retrieval. Evidence reranking module utilizes Automatic Evidence Selection Prompting (AESP) that acquires the relevant evidence item from the products of evidence retrieval. Subsequently, evidence rewriting module leverages Automatic Evidence Generation Prompting (AEGP) to improve task adaptation on MLLM-based OOC misinformation detectors. Furthermore, our approach enables explanation for judgment, and achieves impressive performance with instruction tuning. Experimental results on different benchmark datasets demonstrate that our proposed HiEAG surpasses previous state-of-the-art (SOTA) methods in the accuracy over all samples.

</details>


### [8] [Based on Data Balancing and Model Improvement for Multi-Label Sentiment Classification Performance Enhancement](https://arxiv.org/abs/2511.14073)
*Zijin Su,Huanzhu Lv,Yuren Niu,Yiming Liu*

Main category: cs.CL

TL;DR: 构建平衡的多标签情感数据集，开发增强的多标签分类模型，显著提升分类性能


<details>
  <summary>Details</summary>
Motivation: 现有数据集如GoEmotions存在严重的类别不平衡问题，影响模型性能，特别是对于代表性不足的情感类别

Method: 整合原始GoEmotions数据、使用RoBERTa-base-GoEmotions模型标注的Sentiment140样本，以及GPT-4 mini生成的人工标注文本，构建平衡数据集；开发结合FastText嵌入、卷积层、双向LSTM和注意力机制的多标签分类模型

Result: 与不平衡数据训练的模型相比，在准确率、精确率、召回率、F1分数和AUC方面均有显著提升

Conclusion: 该方法有效解决了多标签情感分类中的类别不平衡问题，显著提高了模型性能

Abstract: Multi-label sentiment classification plays a vital role in natural language processing by detecting multiple emotions within a single text. However, existing datasets like GoEmotions often suffer from severe class imbalance, which hampers model performance, especially for underrepresented emotions. To address this, we constructed a balanced multi-label sentiment dataset by integrating the original GoEmotions data, emotion-labeled samples from Sentiment140 using a RoBERTa-base-GoEmotions model, and manually annotated texts generated by GPT-4 mini. Our data balancing strategy ensured an even distribution across 28 emotion categories. Based on this dataset, we developed an enhanced multi-label classification model that combines pre-trained FastText embeddings, convolutional layers for local feature extraction, bidirectional LSTM for contextual learning, and an attention mechanism to highlight sentiment-relevant words. A sigmoid-activated output layer enables multi-label prediction, and mixed precision training improves computational efficiency. Experimental results demonstrate significant improvements in accuracy, precision, recall, F1-score, and AUC compared to models trained on imbalanced data, highlighting the effectiveness of our approach.

</details>


### [9] [Stealth Fine-Tuning: Efficiently Breaking Alignment in RVLMs Using Self-Generated CoT](https://arxiv.org/abs/2511.14106)
*Le Yu,Zhengyue Zhao,Yawen Zheng,Yunhao Liu*

Main category: cs.CL

TL;DR: 提出一种名为Stealth Fine-Tuning的攻击方法，通过分段干扰和自生成监督数据，仅用499个样本在3小时内就能有效绕过RVLMs的安全对齐机制，攻击成功率比IDEATOR高38.52%。


<details>
  <summary>Details</summary>
Motivation: 虽然RVLMs通过安全对齐防止有害行为，但其暴露的思维链痕迹引入了新的攻击面，需要研究如何有效突破这种安全机制。

Method: 使用分段干扰引发有害推理痕迹，将自生成输出作为监督微调数据，采用轮次加权损失设计，实现轻量级、分布一致的微调方法。

Result: 在AdvBench和多个通用基准测试中，Stealth Fine-Tuning以低成本高效地绕过了对齐防御，同时保持了模型的通用推理能力。

Conclusion: Stealth Fine-Tuning是一种低成本、高效果的攻击方法，能够有效突破RVLMs的安全对齐机制，暴露了现有防御系统的脆弱性。

Abstract: Reasoning-augmented Vision-Language Models (RVLMs) rely on safety alignment to prevent harmful behavior, yet their exposed chain-of-thought (CoT) traces introduce new attack surfaces. In this work, we find that the safety alignment of RVLMs can be easily break through a novel attack method termed \textbf{Stealth Fine-Tuning}. Our method elicits harmful reasoning traces through \textbf{segment-level interference} and reuses the self-generated outputs as supervised fine-tuning data. Through a \textbf{turn-based weighted} loss design, yielding a lightweight, distribution-consistent finetuning method. In our experiment, with only 499 samples and under 3 hours on a single A100 (QLoRA), Stealth Fine-Tuning outperforms IDEATOR by 38.52\% ASR while preserving general reasoning ability, as the tuned model retains the original representation distribution. Experiments on AdvBench and several general benchmarks demonstrate that Stealth Fine-Tuning is a low-cost and highly effective way to bypass alignment defenses. \textcolor{red}{\textbf{Disclaimer: This paper contains content that may be disturbing or offensive.}}

</details>


### [10] [Synthetic Clinical Notes for Rare ICD Codes: A Data-Centric Framework for Long-Tail Medical Coding](https://arxiv.org/abs/2511.14112)
*Truong Vo,Weiyi Wu,Kaize Ding*

Main category: cs.CL

TL;DR: 提出基于数据中心的框架，通过生成高质量合成出院摘要来解决ICD编码中的长尾分布问题，改善罕见和零样本代码的预测效果。


<details>
  <summary>Details</summary>
Motivation: 临床文本自动ICD编码受限于诊断代码的极端长尾分布，数千个罕见和零样本ICD代码在数据集（如MIMIC-III）中代表性严重不足，导致宏观F1分数较低。

Method: 构建基于罕见代码的现实多标签代码集，利用真实世界共现模式、ICD描述、同义词、分类法和相似临床笔记生成结构化提示，创建90,000个合成笔记覆盖7,902个ICD代码。

Result: 在原始和扩展数据集上微调PLM-ICD和GKI-ICD模型，实验显示该方法适度提高了宏观F1，同时保持强大的微观F1，优于先前的最先进方法。

Conclusion: 虽然相对于计算成本增益可能显得有限，但结果表明精心设计的合成数据可以增强长尾ICD代码预测的公平性。

Abstract: Automatic ICD coding from clinical text is a critical task in medical NLP but remains hindered by the extreme long-tail distribution of diagnostic codes. Thousands of rare and zero-shot ICD codes are severely underrepresented in datasets like MIMIC-III, leading to low macro-F1 scores. In this work, we propose a data-centric framework that generates high-quality synthetic discharge summaries to mitigate this imbalance. Our method constructs realistic multi-label code sets anchored on rare codes by leveraging real-world co-occurrence patterns, ICD descriptions, synonyms, taxonomy, and similar clinical notes. Using these structured prompts, we generate 90,000 synthetic notes covering 7,902 ICD codes, significantly expanding the training distribution. We fine-tune two state-of-the-art transformer-based models, PLM-ICD and GKI-ICD, on both the original and extended datasets. Experiments show that our approach modestly improves macro-F1 while maintaining strong micro-F1, outperforming prior SOTA. While the gain may seem marginal relative to the computational cost, our results demonstrate that carefully crafted synthetic data can enhance equity in long-tail ICD code prediction.

</details>


### [11] [From Graphs to Hypergraphs: Enhancing Aspect-Based Sentiment Analysis via Multi-Level Relational Modeling](https://arxiv.org/abs/2511.14142)
*Omkar Mahesh Kashyap,Padegal Amit,Madhav Kashyap,Ashwini M Joshi,Shylaja SS*

Main category: cs.CL

TL;DR: HyperABSA是一个动态超图框架，通过样本特定的层次聚类构建方面-观点结构，解决了传统图方法在多关系建模中的冗余和错误传播问题，在ABSA任务中表现优于现有图基线。


<details>
  <summary>Details</summary>
Motivation: 传统基于图的方法只能建模成对依赖关系，需要为不同关系视图构建多个图，这引入了冗余、参数开销和融合过程中的错误传播，限制了在短文本、低资源设置下的鲁棒性。

Method: 提出HyperABSA框架，通过样本特定的层次聚类诱导方面-观点结构，并引入加速-回退截止点的新方法来自适应确定层次聚类的粒度级别。

Result: 在三个基准数据集（Lap14、Rest14、MAMS）上的实验显示，相比强大的图基线方法有持续改进，特别是与RoBERTa主干网络结合时获得显著提升。

Conclusion: 动态超图构建是ABSA任务中高效且强大的替代方案，并有潜力扩展到其他短文本NLP任务。

Abstract: Aspect-Based Sentiment Analysis (ABSA) predicts sentiment polarity for specific aspect terms, a task made difficult by conflicting sentiments across aspects and the sparse context of short texts. Prior graph-based approaches model only pairwise dependencies, forcing them to construct multiple graphs for different relational views. These introduce redundancy, parameter overhead, and error propagation during fusion, limiting robustness in short-text, low-resource settings. We present HyperABSA, a dynamic hypergraph framework that induces aspect-opinion structures through sample-specific hierarchical clustering. To construct these hyperedges, we introduce a novel acceleration-fallback cutoff for hierarchical clustering, which adaptively determines the level of granularity. Experiments on three benchmarks (Lap14, Rest14, MAMS) show consistent improvements over strong graph baselines, with substantial gains when paired with RoBERTa backbones. These results position dynamic hypergraph construction as an efficient, powerful alternative for ABSA, with potential extensions to other short-text NLP tasks.

</details>


### [12] [Applying Relation Extraction and Graph Matching to Answering Multiple Choice Questions](https://arxiv.org/abs/2511.14144)
*Naoki Shimoda,Akihiro Yamamoto*

Main category: cs.CL

TL;DR: 将基于Transformer的关系提取与知识图谱匹配相结合，用于回答选择题并保持输出过程的可追溯性。通过将问题句子转换为关系图并验证其真实性来回答问题。


<details>
  <summary>Details</summary>
Motivation: 知识图谱是结构化的事实知识表示，但由于构建成本高，通常被视为静态数据库。Transformer关系提取方法的发展使得能够从自然语言动态生成知识图谱，为表示输入句子的含义提供了可能性。

Method: 提出一种回答填空题选择题的方法：1）使用关系提取方法将句子转换为关系图；2）在封闭世界假设下验证其与事实正确知识图谱的一致性；3）考虑关系提取方法可能生成错误信息的问题。

Result: 实验结果表明，该方法能够正确回答约70%的问题，同时提供程序的可追溯性。不同问题类别对准确率有显著影响。

Conclusion: 结合动态知识图谱生成和验证的方法能够有效回答选择题，并保持过程可追溯性，但问题类别对性能有重要影响。

Abstract: In this research, we combine Transformer-based relation extraction with matching of knowledge graphs (KGs) and apply them to answering multiple-choice questions (MCQs) while maintaining the traceability of the output process. KGs are structured representations of factual knowledge consisting of entities and relations. Due to the high construction cost, they had been regarded as static databases with validated links. However, the recent development of Transformer-based relation extraction (RE) methods has enabled us to generate KGs dynamically by giving them natural language texts, and thereby opened the possibility for representing the meaning of the input sentences with the created KGs. Using this effect, we propose a method that answers MCQs in the "fill-in-the-blank" format, taking care of the point that RE methods generate KGs that represent false information if provided with factually incorrect texts. We measure the truthfulness of each question sentence by (i) converting the sentence into a relational graph using an RE method and (ii) verifying it against factually correct KGs under the closed-world assumption. The experimental results demonstrate that our method correctly answers up to around 70% of the questions, while providing traceability of the procedure. We also highlight that the question category has a vast influence on the accuracy.

</details>


### [13] [Selective Weak-to-Strong Generalization](https://arxiv.org/abs/2511.14166)
*Hao Lang,Fei Huang,Yongbin Li*

Main category: cs.CL

TL;DR: 提出选择性弱到强泛化框架，通过训练二元分类器识别强模型能回答的问题，避免不必要的弱监督，并使用图平滑方法优化弱标签，在三个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决超人类模型对齐时缺乏高质量数据的问题，现有弱到强泛化方法因固定使用弱监督而存在鲁棒性问题，部分弱标签对模型有害。

Method: 训练P(IK)二元分类器识别强模型能回答的问题，使用自生成标签进行对齐，并通过图平滑方法优化弱标签。

Result: 在三个基准测试中持续优于竞争基线，P(IK)能够跨任务和难度泛化。

Conclusion: 选择性弱到强泛化有助于超对齐问题的解决。

Abstract: Future superhuman models will surpass the ability of humans and humans will only be able to \textit{weakly} supervise superhuman models. To alleviate the issue of lacking high-quality data for model alignment, some works on weak-to-strong generalization (W2SG) finetune a strong pretrained model with a weak supervisor so that it can generalize beyond weak supervision. However, the invariable use of weak supervision in existing methods exposes issues in robustness, with a proportion of weak labels proving harmful to models. In this paper, we propose a selective W2SG framework to avoid using weak supervision when unnecessary. We train a binary classifier P(IK) to identify questions that a strong model can answer and use its self-generated labels for alignment. We further refine weak labels with a graph smoothing method. Extensive experiments on three benchmarks show that our method consistently outperforms competitive baselines. Further analyses show that P(IK) can generalize across tasks and difficulties, which indicates selective W2SG can help superalignment.

</details>


### [14] [SymLoc: Symbolic Localization of Hallucination across HaluEval and TruthfulQA](https://arxiv.org/abs/2511.14172)
*Naveen Lamba,Sanju Tiwari,Manas Gaur*

Main category: cs.CL

TL;DR: 提出了首个基于符号语言知识的幻觉定位框架，通过分析符号触发器在模型各层的处理过程，发现幻觉本质上是符号语言处理失败而非一般生成问题。


<details>
  <summary>Details</summary>
Motivation: LLMs在遇到修饰词、否定、数字、例外和命名实体等符号触发器时仍存在幻觉问题，但缺乏对这些符号幻觉来源的清晰理解，需要系统性地处理这些触发器并定位幻觉在模型内部的产生位置。

Method: 提出基于符号语言和语义知识的定位框架，分析五个模型在HaluEval和TruthfulQA数据集上的表现，重点关注模型如何处理符号触发器。

Result: 符号元素的注意力方差在早期层（2-4层）爆炸性增长达到临界不稳定状态，否定触发灾难性方差水平；尽管模型规模增大，幻觉率仍保持高位（78.3%-83.7%），深层中符号语义触发器的注意力急剧下降。

Conclusion: 幻觉本质上是符号语言处理失败，而非一般生成问题，符号语义知识为理解和定位LLMs中的幻觉机制提供了关键线索。

Abstract: LLMs still struggle with hallucination, especially when confronted with symbolic triggers like modifiers, negation, numbers, exceptions, and named entities. Yet, we lack a clear understanding of where these symbolic hallucinations originate, making it crucial to systematically handle such triggers and localize the emergence of hallucination inside the model. While prior work explored localization using statistical techniques like LSC and activation variance analysis, these methods treat all tokens equally and overlook the role symbolic linguistic knowledge plays in triggering hallucinations. So far, no approach has investigated how symbolic elements specifically drive hallucination failures across model layers, nor has symbolic linguistic knowledge been used as the foundation for a localization framework. We propose the first symbolic localization framework that leverages symbolic linguistic and semantic knowledge to meaningfully trace the development of hallucinations across all model layers. By focusing on how models process symbolic triggers, we analyze five models using HaluEval and TruthfulQA. Our symbolic knowledge approach reveals that attention variance for these linguistic elements explodes to critical instability in early layers (2-4), with negation triggering catastrophic variance levels, demonstrating that symbolic semantic processing breaks down from the very beginning. Through the lens of symbolic linguistic knowledge, despite larger model sizes, hallucination rates remain consistently high (78.3%-83.7% across Gemma variants), with steep attention drops for symbolic semantic triggers throughout deeper layers. Our findings demonstrate that hallucination is fundamentally a symbolic linguistic processing failure, not a general generation problem, revealing that symbolic semantic knowledge provides the key to understanding and localizing hallucination mechanisms in LLMs.

</details>


### [15] [Harnessing Deep LLM Participation for Robust Entity Linking](https://arxiv.org/abs/2511.14181)
*Jiajun Hou,Chenyu Zhang,Rui Meng*

Main category: cs.CL

TL;DR: DeepEL是一个将大语言模型全面集成到实体链接各个阶段的框架，通过自验证机制利用全局上下文信息纠正预测，在10个基准数据集上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常只在大语言模型的孤立阶段应用，未能充分利用其在整个实体链接过程中的能力，且孤立消歧不足以达到最优性能。

Method: 提出DeepEL框架，将大语言模型集成到实体链接的每个阶段，并引入基于全局上下文信息的自验证机制，使模型能够纠正自身预测并识别同一句子中实体间的连贯关系。

Result: 在10个基准数据集上的广泛评估显示，DeepEL显著优于现有最先进方法，整体F1分数平均提升2.6%，在域外数据集上提升4%。

Conclusion: 深度集成大语言模型能有效推进实体链接技术的前沿发展。

Abstract: Entity Linking (EL), the task of mapping textual entity mentions to their corresponding entries in knowledge bases, constitutes a fundamental component of natural language understanding. Recent advancements in Large Language Models (LLMs) have demonstrated remarkable potential for enhancing EL performance. Prior research has leveraged LLMs to improve entity disambiguation and input representation, yielding significant gains in accuracy and robustness. However, these approaches typically apply LLMs to isolated stages of the EL task, failing to fully integrate their capabilities throughout the entire process.
  In this work, we introduce DeepEL, a comprehensive framework that incorporates LLMs into every stage of the entity linking task. Furthermore, we identify that disambiguating entities in isolation is insufficient for optimal performance. To address this limitation, we propose a novel self-validation mechanism that utilizes global contextual information, enabling LLMs to rectify their own predictions and better recognize cohesive relationships among entities within the same sentence.
  Extensive empirical evaluation across ten benchmark datasets demonstrates that DeepEL substantially outperforms existing state-of-the-art methods, achieving an average improvement of 2.6\% in overall F1 score and a remarkable 4% gain on out-of-domain datasets. These results underscore the efficacy of deep LLM integration in advancing the state-of-the-art in entity linking.

</details>


### [16] [ArbESC+: Arabic Enhanced Edit Selection System Combination for Grammatical Error Correction Resolving conflict and improving system combination in Arabic GEC](https://arxiv.org/abs/2511.14230)
*Ahlam Alrehili,Areej Alhothali*

Main category: cs.CL

TL;DR: 提出了首个阿拉伯语多系统语法纠错框架ArbESC+，通过组合多个模型生成纠错建议，使用分类器选择最佳修正，在QALB数据集上取得了优于单一模型的性能。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语具有复杂的形态和句法结构，现有方法多使用单一模型，未充分利用多系统组合的潜力。

Method: 使用AraT5、ByT5、mT5、AraBART等多个模型生成纠错建议，将其表示为数值特征，通过分类器选择最佳修正，并采用支持技术过滤重叠修正和评估决策可靠性。

Result: 在QALB-14测试数据上F0.5达到82.63%，QALB-15 L1数据84.64%，QALB-15 L2数据65.55%，优于单一模型。

Conclusion: 这是首个整合语言错误修正的阿拉伯语尝试，为开发先进的阿拉伯语文本处理工具提供了实用步骤。

Abstract: Grammatical Error Correction (GEC) is an important aspect of natural language processing. Arabic has a complicated morphological and syntactic structure, posing a greater challenge than other languages. Even though modern neural models have improved greatly in recent years, the majority of previous attempts used individual models without taking into account the potential benefits of combining different systems. In this paper, we present one of the first multi-system approaches for correcting grammatical errors in Arabic, the Arab Enhanced Edit Selection System Complication (ArbESC+). Several models are used to collect correction proposals, which are represented as numerical features in the framework. A classifier determines and implements the appropriate corrections based on these features. In order to improve output quality, the framework uses support techniques to filter overlapping corrections and estimate decision reliability. A combination of AraT5, ByT5, mT5, AraBART, AraBART+Morph+GEC, and Text editing systems gave better results than a single model alone, with F0.5 at 82.63% on QALB-14 test data, 84.64% on QALB-15 L1 data, and 65.55% on QALB-15 L2 data. As one of the most significant contributions of this work, it's the first Arab attempt to integrate linguistic error correction. Improving existing models provides a practical step towards developing advanced tools that will benefit users and researchers of Arabic text processing.

</details>


### [17] [MuCPT: Music-related Natural Language Model Continued Pretraining](https://arxiv.org/abs/2511.14245)
*Kai Tian,Yirong Mao,Wendong Bi,Hanjie Wang,Que Wenhui*

Main category: cs.CL

TL;DR: 构建大规模音乐相关语料库和领域优先的数据管道，通过参考模型软评分进行质量控制和动态优化，提出MusicSimpleQA基准评估事实性，为音乐领域LLM提供可扩展的数据训练框架。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在通用任务上表现良好，但在音乐等专业领域受限，主要挑战在于语料规模、纯净度以及数据与训练目标的匹配度。

Method: 构建40B token的音乐相关语料库，采用领域优先数据管道（轻量级分类器过滤、多阶段清洗、去重、隐私保护掩码），引入参考模型软评分进行质量控制和动态优化。

Result: 开发了MusicSimpleQA基准用于评估事实性，通过系统比较数据组成，验证了所提方法的有效性。

Conclusion: 本工作同时推进了合适的语料库和训练目标，为音乐领域LLM提供了可扩展的数据训练框架和可复用的评估工具。

Abstract: Large language models perform strongly on general tasks but remain constrained in specialized settings such as music, particularly in the music-entertainment domain, where corpus scale, purity, and the match between data and training objectives are critical. We address this by constructing a large, music-related natural language corpus (40B tokens) that combines open source and in-house data, and by implementing a domain-first data pipeline: a lightweight classifier filters and weights in-domain text, followed by multi-stage cleaning, de-duplication, and privacy-preserving masking. We further integrate multi-source music text with associated metadata to form a broader, better-structured foundation of domain knowledge. On the training side, we introduce reference-model (RM)-based token-level soft scoring for quality control: a unified loss-ratio criterion is used both for data selection and for dynamic down-weighting during optimization, reducing noise gradients and amplifying task-aligned signals, thereby enabling more effective music-domain continued pretraining and alignment. To assess factuality, we design the MusicSimpleQA benchmark, which adopts short, single-answer prompts with automated agreement scoring. Beyond the benchmark design, we conduct systematic comparisons along the axes of data composition. Overall, this work advances both the right corpus and the right objective, offering a scalable data-training framework and a reusable evaluation tool for building domain LLMs in the music field.

</details>


### [18] [Towards Authentic Movie Dubbing with Retrieve-Augmented Director-Actor Interaction Learning](https://arxiv.org/abs/2511.14249)
*Rui Liu,Yuan Zhao,Zhenqi Jia*

Main category: cs.CL

TL;DR: 提出Authentic-Dubber模型，通过检索增强的导演-演员交互学习方案，模拟真实电影配音工作流程，提升情感表达能力。


<details>
  <summary>Details</summary>
Motivation: 现有电影配音方法简化了工作流程，忽略了导演与演员之间的关键互动，而真实工作流程中导演会指导演员内化情感上下文线索。

Method: 构建多模态参考素材库，使用情感相似性检索增强策略，以及渐进式图基语音生成方法，模拟演员内化情感知识的过程。

Result: 在V2C Animation基准数据集上的主客观评估验证了方法的有效性，在情感表现力方面实现了全面改进。

Conclusion: Authentic-Dubber成功模拟了真实的配音工作流程，显著提升了配音的情感表达质量。

Abstract: The automatic movie dubbing model generates vivid speech from given scripts, replicating a speaker's timbre from a brief timbre prompt while ensuring lip-sync with the silent video. Existing approaches simulate a simplified workflow where actors dub directly without preparation, overlooking the critical director-actor interaction. In contrast, authentic workflows involve a dynamic collaboration: directors actively engage with actors, guiding them to internalize the context cues, specifically emotion, before performance. To address this issue, we propose a new Retrieve-Augmented Director-Actor Interaction Learning scheme to achieve authentic movie dubbing, termed Authentic-Dubber, which contains three novel mechanisms: (1) We construct a multimodal Reference Footage library to simulate the learning footage provided by directors. Note that we integrate Large Language Models (LLMs) to achieve deep comprehension of emotional representations across multimodal signals. (2) To emulate how actors efficiently and comprehensively internalize director-provided footage during dubbing, we propose an Emotion-Similarity-based Retrieval-Augmentation strategy. This strategy retrieves the most relevant multimodal information that aligns with the target silent video. (3) We develop a Progressive Graph-based speech generation approach that incrementally incorporates the retrieved multimodal emotional knowledge, thereby simulating the actor's final dubbing process. The above mechanisms enable the Authentic-Dubber to faithfully replicate the authentic dubbing workflow, achieving comprehensive improvements in emotional expressiveness. Both subjective and objective evaluations on the V2C Animation benchmark dataset validate the effectiveness. The code and demos are available at https://github.com/AI-S2-Lab/Authentic-Dubber.

</details>


### [19] [AfriSpeech-MultiBench: A Verticalized Multidomain Multicountry Benchmark Suite for African Accented English ASR](https://arxiv.org/abs/2511.14255)
*Gabrial Zencha Ashungafac,Mardhiyah Sanni,Busayo Awobade,Alex Gichamba,Tobi Olatunji*

Main category: cs.CL

TL;DR: AfriSpeech-MultiBench是首个针对非洲英语口音的领域特定评估套件，涵盖10+国家、100+口音和7个应用领域，评估了多种语音识别系统在非洲语境下的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管语音AI技术快速发展，但目前缺乏针对非洲语言多样性的公开应用特定模型评估，需要为非洲社区开发包容性语音应用提供指导。

Method: 使用来自各种开放非洲英语口音语音数据集的自然和非自然语音对话，对开源、闭源、单模态ASR和多模态LLM语音识别系统进行基准测试。

Result: 实证分析显示系统差异：开源ASR在自然语音中表现良好但在嘈杂非母语对话中退化；多模态LLM更抗口音但处理领域特定命名实体困难；专有模型在清晰语音中准确度高但不同国家和领域差异显著。

Conclusion: 通过发布此综合基准，赋能从业者和研究人员选择适合非洲用例的语音技术，促进服务不足社区的包容性语音应用发展。

Abstract: Recent advances in speech-enabled AI, including Google's NotebookLM and OpenAI's speech-to-speech API, are driving widespread interest in voice interfaces globally. Despite this momentum, there exists no publicly available application-specific model evaluation that caters to Africa's linguistic diversity. We present AfriSpeech-MultiBench, the first domain-specific evaluation suite for over 100 African English accents across 10+ countries and seven application domains: Finance, Legal, Medical, General dialogue, Call Center, Named Entities and Hallucination Robustness. We benchmark a diverse range of open, closed, unimodal ASR and multimodal LLM-based speech recognition systems using both spontaneous and non-spontaneous speech conversation drawn from various open African accented English speech datasets. Our empirical analysis reveals systematic variation: open-source ASR models excels in spontaneous speech contexts but degrades on noisy, non-native dialogue; multimodal LLMs are more accent-robust yet struggle with domain-specific named entities; proprietary models deliver high accuracy on clean speech but vary significantly by country and domain. Models fine-tuned on African English achieve competitive accuracy with lower latency, a practical advantage for deployment, hallucinations still remain a big problem for most SOTA models. By releasing this comprehensive benchmark, we empower practitioners and researchers to select voice technologies suited to African use-cases, fostering inclusive voice applications for underserved communities.

</details>


### [20] [Entropy-Guided Reasoning Compression](https://arxiv.org/abs/2511.14258)
*Hourun Zhu,Yang Gao,Wenlong Fei,Jiawei Li,Huashan Sun*

Main category: cs.CL

TL;DR: 提出了一种熵引导的训练框架来解决推理模型压缩中的熵冲突问题，将推理长度压缩到原始的20%同时保持或提升准确率。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型的思维链输出过长导致计算成本高和部署困难，现有压缩方法忽视了训练过程中的熵冲突现象。

Method: 采用熵引导的训练框架，在熵下降时鼓励简洁的思维步骤，在熵上升时在紧凑推理模式下加强探索以提高鲁棒性。

Result: 在六个数学基准测试中，将推理长度压缩到原始的20%，同时保持甚至超越了基线准确率。

Conclusion: 该方法有效解决了推理模型压缩中的熵冲突问题，实现了推理长度的大幅压缩而不损失性能。

Abstract: Large reasoning models have demonstrated remarkable performance on complex reasoning tasks, yet the excessive length of their chain-of-thought outputs remains a major practical bottleneck due to high computation cost and poor deployability. Existing compression methods have achieved partial success but overlook a crucial phenomenon in the training process -- the entropy conflict. During compression training, entropy decreases, leading to shorter reasoning but limited exploration, while accuracy-oriented objectives increase entropy, lengthening reasoning chains. This can cause the model to get stuck in a local dilemma. Our analysis further reveals the origin of the entropy conflict: many high-entropy tokens are logical connectors that receive larger gradients and are encouraged under the performance objective, while the compression objective simultaneously penalizes these potentially redundant connectors. This opposing pressure creates a direct source of entropy conflict. To address these issues, we adopt an entropy-guided training framework. As entropy descends, the model is guided toward efficient reasoning by encouraging concise thought steps; as entropy rises, exploration is reinforced under the compact reasoning mode to improve robustness. Experiments on six mathematical benchmarks show that our method compresses reasoning length to 20% of the original while maintaining or even surpassing baseline accuracy. Code and models will be released publicly.

</details>


### [21] [Don't Miss the Forest for the Trees: In-Depth Confidence Estimation for LLMs via Reasoning over the Answer Space](https://arxiv.org/abs/2511.14275)
*Ante Wang,Weizhi Ma,Yang Liu*

Main category: cs.CL

TL;DR: 本文提出通过预测语言化概率分布来增强LLM的置信度估计，该方法要求模型考虑答案空间中的所有候选答案并仔细分配置信度分数，从而促进深度推理。


<details>
  <summary>Details</summary>
Motivation: 了解模型响应的可靠性在应用中至关重要。虽然已有研究关注生成语言化置信度，并结合思维链推理提供逻辑透明的估计，但推理策略如何影响置信度估计仍未充分探索。

Method: 提出预测语言化概率分布的方法，要求LLM考虑答案空间中的所有候选答案，而不仅仅是单一猜测，并仔细分配置信度分数以满足分布要求。

Result: 该方法在不同模型和各种任务中均显示出优势，无论答案空间是否已知。即使在强化学习后，其优势仍然保持，进一步分析表明其推理模式符合人类期望。

Conclusion: 预测语言化概率分布能有效鼓励深度推理进行置信度估计，提供了一种改进LLM置信度估计的有效方法。

Abstract: Knowing the reliability of a model's response is essential in application. With the strong generation capabilities of LLMs, research has focused on generating verbalized confidence. This is further enhanced by combining chain-of-thought reasoning, which provides logical and transparent estimation. However, how reasoning strategies affect the estimated confidence is still under-explored. In this work, we demonstrate that predicting a verbalized probability distribution can effectively encourage in-depth reasoning for confidence estimation. Intuitively, it requires an LLM to consider all candidates within the answer space instead of basing on a single guess, and to carefully assign confidence scores to meet the requirements of a distribution. This method shows an advantage across different models and various tasks, regardless of whether the answer space is known. Its advantage is maintained even after reinforcement learning, and further analysis shows its reasoning patterns are aligned with human expectations.

</details>


### [22] [AraLingBench A Human-Annotated Benchmark for Evaluating Arabic Linguistic Capabilities of Large Language Models](https://arxiv.org/abs/2511.14295)
*Mohammad Zbib,Hasan Abed Al Kader Hammoud,Sina Mukalled,Nadine Rizk,Fatima Karnib,Issam Lakkis,Ammar Mohanna,Bernard Ghanem*

Main category: cs.CL

TL;DR: AraLingBench是一个全面人工标注的基准测试，用于评估大语言模型在阿拉伯语语言学能力方面的表现，涵盖语法、形态学、拼写、阅读理解和句法五个核心类别。


<details>
  <summary>Details</summary>
Motivation: 当前阿拉伯语和双语大语言模型在表面水平上表现出色，但在深层语法和句法推理方面存在困难，需要专门的基准测试来评估其真正的语言学掌握程度。

Method: 通过150个专家设计的多项选择题，直接评估结构语言理解能力，对35个阿拉伯语和双语大语言模型进行系统性评估。

Result: 评估显示当前模型表现出强大的表面水平能力，但在深层语法和句法推理方面存在困难，许多模型通过记忆或模式识别而非真实理解来获得成功。

Conclusion: AraLingBench揭示了基于知识的基准测试高分与真正语言学掌握之间的持续差距，为开发阿拉伯语大语言模型提供了诊断框架。

Abstract: We present AraLingBench: a fully human annotated benchmark for evaluating the Arabic linguistic competence of large language models (LLMs). The benchmark spans five core categories: grammar, morphology, spelling, reading comprehension, and syntax, through 150 expert-designed multiple choice questions that directly assess structural language understanding. Evaluating 35 Arabic and bilingual LLMs reveals that current models demonstrate strong surface level proficiency but struggle with deeper grammatical and syntactic reasoning. AraLingBench highlights a persistent gap between high scores on knowledge-based benchmarks and true linguistic mastery, showing that many models succeed through memorization or pattern recognition rather than authentic comprehension. By isolating and measuring fundamental linguistic skills, AraLingBench provides a diagnostic framework for developing Arabic LLMs. The full evaluation code is publicly available on GitHub.

</details>


### [23] [ConInstruct: Evaluating Large Language Models on Conflict Detection and Resolution in Instructions](https://arxiv.org/abs/2511.14342)
*Xingwei He,Qianru Zhang,Pengfei Chen,Guanhua Chen,Linlin Yu,Yuan Yuan,Siu-Ming Yiu*

Main category: cs.CL

TL;DR: ConInstruct基准测试评估LLMs在用户指令包含冲突约束时的检测和解决能力，发现大多数专有LLMs冲突检测能力强，但很少明确通知用户或请求澄清。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注LLMs遵循用户指令的能力，但忽视了指令包含冲突约束的常见场景，LLMs在此类条件下的行为尚未充分探索。

Method: 引入ConInstruct基准测试，专门评估LLMs检测和解决用户指令中冲突的能力，通过该数据集评估冲突检测性能并分析冲突解决行为。

Result: 专有LLMs冲突检测能力强，开源模型中仅DeepSeek-R1表现类似；DeepSeek-R1和Claude-4.5-Sonnet平均F1分数最高，分别为91.5%和87.3%。尽管检测能力强，LLMs很少明确通知用户冲突或请求澄清。

Conclusion: 当前LLMs在冲突处理方面存在关键缺陷，这是设计指令遵循LLMs时需要改进的重要领域。

Abstract: Instruction-following is a critical capability of Large Language Models (LLMs). While existing works primarily focus on assessing how well LLMs adhere to user instructions, they often overlook scenarios where instructions contain conflicting constraints-a common occurrence in complex prompts. The behavior of LLMs under such conditions remains under-explored. To bridge this gap, we introduce ConInstruct, a benchmark specifically designed to assess LLMs' ability to detect and resolve conflicts within user instructions. Using this dataset, we evaluate LLMs' conflict detection performance and analyze their conflict resolution behavior. Our experiments reveal two key findings: (1) Most proprietary LLMs exhibit strong conflict detection capabilities, whereas among open-source models, only DeepSeek-R1 demonstrates similarly strong performance. DeepSeek-R1 and Claude-4.5-Sonnet achieve the highest average F1-scores at 91.5% and 87.3%, respectively, ranking first and second overall. (2) Despite their strong conflict detection abilities, LLMs rarely explicitly notify users about the conflicts or request clarification when faced with conflicting constraints. These results underscore a critical shortcoming in current LLMs and highlight an important area for future improvement when designing instruction-following LLMs.

</details>


### [24] [The Tokenization Bottleneck: How Vocabulary Extension Improves Chemistry Representation Learning in Pretrained Language Models](https://arxiv.org/abs/2511.14365)
*Prathamesh Kalamkar,Ned Letcher,Meissane Chami,Sahger Lad,Shayan Mohanty,Prasanna Pendse*

Main category: cs.CL

TL;DR: 本文提出了一种解决LLMs在化学领域应用中的"分词瓶颈"的方法，通过扩展词汇表和继续预训练来统一自然语言和分子结构的表示。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在化学领域的应用经常受到"分词瓶颈"的阻碍，即针对通用文本优化的分词器会将SMILES等化学表示分割成语义无意义的子标记。

Method: 采用目标词汇扩展策略，在预训练LLM的词汇表中添加化学相关标记，然后在化学领域文本上进行继续预训练以整合新知识。

Result: 经验证明该方法有效，在多个下游化学任务上表现出优越性能。

Conclusion: 通过统一自然语言和分子结构的表示，可以显著提升LLMs在化学领域的应用效果。

Abstract: The application of large language models (LLMs) to chemistry is frequently hampered by a "tokenization bottleneck", where tokenizers tuned on general-domain text tend to fragment chemical representations such as SMILES into semantically uninformative sub-tokens. This paper introduces a principled methodology to resolve this bottleneck by unifying the representation of natural language and molecular structures within a single model. Our approach involves targeted vocabulary extension-augmenting a pretrained LLM's vocabulary with chemically salient tokens, followed by continued pretraining on chemistry-domain text to integrate this new knowledge. We provide an empirical demonstration of the effectiveness of this strategy, showing that our methodology leads to superior performance on a range of downstream chemical tasks.

</details>


### [25] [ATLAS: A High-Difficulty, Multidisciplinary Benchmark for Frontier Scientific Reasoning](https://arxiv.org/abs/2511.14366)
*Hongwei Liu,Junnan Liu,Shudong Liu,Haodong Duan,Yuqiang Li,Mao Su,Xiaohong Liu,Guangtao Zhai,Xinyu Fang,Qianhong Ma,Taolin Zhang,Zihan Ma,Yufeng Zhao,Peiheng Zhou,Linchen Xiao,Wenlong Zhang,Shijie Zhou,Xingjian Ma,Siqi Sun,Jiaye Ge,Meng Li,Yuhong Liu,Jianxin Dong,Jiaying Li,Hui Wu,Hanwen Liang,Jintai Lin,Yanting Wang,Jie Dong,Tong Zhu,Tianfan Fu,Conghui He,Qi Zhang,Songyang Zhang,Lei Bai,Kai Chen*

Main category: cs.CL

TL;DR: ATLAS是一个大规模、高难度、跨学科的科学评估套件，包含约800个原创问题，旨在解决现有基准测试在区分前沿模型能力方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试存在性能饱和、学科范围狭窄、答案格式简化、易受数据污染等问题，与真实科学探究存在保真度差距。

Method: 由领域专家开发，涵盖数学、物理、化学、生物、计算机科学、地球科学和材料科学七个核心科学领域，采用多阶段专家同行评审和对抗性测试确保质量。

Result: 初步结果显示ATLAS能有效区分领先模型的先进科学推理能力。

Conclusion: ATLAS将发展成一个长期、开放、社区驱动的平台，为通往人工通用智能的进展提供可靠衡量标准。

Abstract: The rapid advancement of Large Language Models (LLMs) has led to performance saturation on many established benchmarks, questioning their ability to distinguish frontier models. Concurrently, existing high-difficulty benchmarks often suffer from narrow disciplinary focus, oversimplified answer formats, and vulnerability to data contamination, creating a fidelity gap with real-world scientific inquiry. To address these challenges, we introduce ATLAS (AGI-Oriented Testbed for Logical Application in Science), a large-scale, high-difficulty, and cross-disciplinary evaluation suite composed of approximately 800 original problems. Developed by domain experts (PhD-level and above), ATLAS spans seven core scientific fields: mathematics, physics, chemistry, biology, computer science, earth science, and materials science. Its key features include: (1) High Originality and Contamination Resistance, with all questions newly created or substantially adapted to prevent test data leakage; (2) Cross-Disciplinary Focus, designed to assess models' ability to integrate knowledge and reason across scientific domains; (3) High-Fidelity Answers, prioritizing complex, open-ended answers involving multi-step reasoning and LaTeX-formatted expressions over simple multiple-choice questions; and (4) Rigorous Quality Control, employing a multi-stage process of expert peer review and adversarial testing to ensure question difficulty, scientific value, and correctness. We also propose a robust evaluation paradigm using a panel of LLM judges for automated, nuanced assessment of complex answers. Preliminary results on leading models demonstrate ATLAS's effectiveness in differentiating their advanced scientific reasoning capabilities. We plan to develop ATLAS into a long-term, open, community-driven platform to provide a reliable "ruler" for progress toward Artificial General Intelligence.

</details>


### [26] [Mitigating Label Length Bias in Large Language Models](https://arxiv.org/abs/2511.14385)
*Mario Sanz-Guerrero,Katharina von der Wense*

Main category: cs.CL

TL;DR: 提出NCC方法解决LLM中的标签长度偏差问题，通过全标签级别的标准化和校准，在多个数据集上显著提升性能，最高可达10% F1分数提升。


<details>
  <summary>Details</summary>
Motivation: LLM在预测候选选项时存在标签偏差，现有校准方法忽略了多token类标签带来的偏差，特别是标签长度不一致导致的处理不一致问题。

Method: 提出标准化上下文校准(NCC)，在全标签级别进行标准化和校准，可扩展到多项选择题等更广泛任务。

Result: NCC在多个数据集和模型上取得统计显著改进，结合上下文学习时对少样本示例选择更不敏感，需要更少示例即可获得竞争性能，产生更可靠的置信度估计。

Conclusion: 缓解全标签偏差对于提高LLM方法的性能和鲁棒性至关重要，特别是在现实应用中类标签自然包含多个token的场景。

Abstract: Large language models (LLMs) are powerful zero- and few-shot learners. However, when predicting over a set of candidate options, LLMs suffer from label biases, and existing calibration methods overlook biases arising from multi-token class labels. We tackle an issue we call label length bias, where labels of different lengths are treated inconsistently, even after standard length normalization. To mitigate it, we propose normalized contextual calibration (NCC), an effective method that normalizes and calibrates predictions at the full-label level. NCC achieves statistically significant improvements over prior approaches across multiple datasets and models, with gains of up to 10% F1. Moreover, NCC extends bias mitigation to broader tasks such as multiple-choice question answering. Our analysis shows that, when combined with in-context learning, NCC is less sensitive to few-shot example selection, requires fewer examples for competitive performance, and produces more reliable confidence estimates. These findings highlight the importance of mitigating full-label biases to improve the performance and robustness of LLM-based methods, particularly in real-world applications where class labels naturally consist of multiple tokens.

</details>


### [27] [Unified Defense for Large Language Models against Jailbreak and Fine-Tuning Attacks in Education](https://arxiv.org/abs/2511.14423)
*Xin Yi,Yue Li,Dongsheng Shi,Linlin Wang,Xiaoling Wang,Liang He*

Main category: cs.CL

TL;DR: 构建了EduHarm教育安全基准，并提出TSSF三阶段防护框架，同时防御越狱攻击和微调攻击，在保持良性查询效用的同时增强教育LLM的安全性。


<details>
  <summary>Details</summary>
Motivation: LLM在教育应用中面临越狱和微调攻击威胁，现有研究主要关注通用安全性，缺乏针对教育场景独特安全需求的专门评估。

Method: 1) 安全感知注意力重定向：恢复区分有害输入的特征；2) 分层安全判断：聚合多层安全线索检测有害指令；3) 防御驱动双路由：分离安全和不安全查询。

Result: 在8种越狱攻击策略上有效增强安全性，防止良性查询过度拒绝；在3个微调攻击数据集上实现稳健防御，同时保持良性微调的效用增益。

Conclusion: TSSF框架为教育LLM提供了同时防御越狱和微调攻击的有效解决方案，在保持教育应用实用性的前提下显著提升安全性。

Abstract: Large Language Models (LLMs) are increasingly integrated into educational applications. However, they remain vulnerable to jailbreak and fine-tuning attacks, which can compromise safety alignment and lead to harmful outputs. Existing studies mainly focus on general safety evaluations, with limited attention to the unique safety requirements of educational scenarios. To address this gap, we construct EduHarm, a benchmark containing safe-unsafe instruction pairs across five representative educational scenarios, enabling systematic safety evaluation of educational LLMs. Furthermore, we propose a three-stage shield framework (TSSF) for educational LLMs that simultaneously mitigates both jailbreak and fine-tuning attacks. First, safety-aware attention realignment redirects attention toward critical unsafe tokens, thereby restoring the harmfulness feature that discriminates between unsafe and safe inputs. Second, layer-wise safety judgment identifies harmfulness features by aggregating safety cues across multiple layers to detect unsafe instructions. Finally, defense-driven dual routing separates safe and unsafe queries, ensuring normal processing for benign inputs and guarded responses for harmful ones. Extensive experiments across eight jailbreak attack strategies demonstrate that TSSF effectively strengthens safety while preventing over-refusal of benign queries. Evaluations on three fine-tuning attack datasets further show that it consistently achieves robust defense against harmful queries while maintaining preserving utility gains from benign fine-tuning.

</details>


### [28] [MedBench v4: A Robust and Scalable Benchmark for Evaluating Chinese Medical Language Models, Multimodal Models, and Intelligent Agents](https://arxiv.org/abs/2511.14439)
*Jinru Ding,Lu Lu,Chao Ding,Mouxiao Bian,Jiayuan Chen,Renjie Lu,Wenrao Pang,Xiaoqin Wu,Zhiqiang Liu,Luyi Jiang,Bing Han,Yunqiu Wang,Jie Xu*

Main category: cs.CL

TL;DR: MedBench v4是一个全国性的医疗AI基准测试平台，包含70多万个专家策划的任务，涵盖24个主要专科和91个次要专科，评估了15个前沿模型在LLM、多模态和智能体三个赛道上的表现。


<details>
  <summary>Details</summary>
Motivation: 需要反映真实临床工作流程和安全约束的评估框架来评估医疗大语言模型、多模态模型和智能体的性能。

Method: 构建包含70多万个任务的云基准测试基础设施，任务经过多阶段优化和多轮临床医生评审，开放答案通过LLM-as-a-judge评分并与人工评分校准。

Result: 基础LLM平均得分54.1/100（最佳：Claude Sonnet 4.5，62.5/100），但安全和伦理得分较低（18.4/100）；多模态模型表现更差（平均47.5/100）；基于相同骨干的智能体显著提升端到端性能（平均79.8/100）。

Conclusion: 基础模型在多模态推理和安全性方面仍存在差距，但具有治理意识的智能体编排能显著提升临床准备度而不牺牲能力，该平台为医院、开发者和政策制定者提供了实用的医疗AI审计参考。

Abstract: Recent advances in medical large language models (LLMs), multimodal models, and agents demand evaluation frameworks that reflect real clinical workflows and safety constraints. We present MedBench v4, a nationwide, cloud-based benchmarking infrastructure comprising over 700,000 expert-curated tasks spanning 24 primary and 91 secondary specialties, with dedicated tracks for LLMs, multimodal models, and agents. Items undergo multi-stage refinement and multi-round review by clinicians from more than 500 institutions, and open-ended responses are scored by an LLM-as-a-judge calibrated to human ratings. We evaluate 15 frontier models. Base LLMs reach a mean overall score of 54.1/100 (best: Claude Sonnet 4.5, 62.5/100), but safety and ethics remain low (18.4/100). Multimodal models perform worse overall (mean 47.5/100; best: GPT-5, 54.9/100), with solid perception yet weaker cross-modal reasoning. Agents built on the same backbones substantially improve end-to-end performance (mean 79.8/100), with Claude Sonnet 4.5-based agents achieving up to 85.3/100 overall and 88.9/100 on safety tasks. MedBench v4 thus reveals persisting gaps in multimodal reasoning and safety for base models, while showing that governance-aware agentic orchestration can markedly enhance benchmarked clinical readiness without sacrificing capability. By aligning tasks with Chinese clinical guidelines and regulatory priorities, the platform offers a practical reference for hospitals, developers, and policymakers auditing medical AI.

</details>


### [29] [Tell Me: An LLM-powered Mental Well-being Assistant with RAG, Synthetic Dialogue Generation, and Agentic Planning](https://arxiv.org/abs/2511.14445)
*Trishala Jayesh Ahalpara*

Main category: cs.CL

TL;DR: Tell Me是一个基于大语言模型的心理健康支持系统，包含个性化对话助手、合成治疗对话生成器和AI健康规划团队，旨在提供可访问的心理健康支持。


<details>
  <summary>Details</summary>
Motivation: 解决心理健康资源获取障碍，利用AI技术提供补充性支持，同时解决治疗数据稀缺问题，促进NLP与心理健康领域的跨学科合作。

Method: 系统采用三组件架构：RAG助手提供个性化对话，合成对话生成器基于用户档案生成治疗对话，Well-being AI团队使用CrewAI生成周度自我护理计划和冥想音频。

Result: 系统在精心设计的健康场景中进行了评估，结合自动LLM判断和人类用户研究，展示了其功能性和有效性。

Conclusion: 该系统展示了对话助手如何降低支持门槛，补充现有护理，扩大心理健康资源可及性，为人类-AI交互在健康领域的负责任创新提供了机会。

Abstract: We present Tell Me, a mental well-being system that leverages advances in large language models to provide accessible, context-aware support for users and researchers. The system integrates three components: (i) a retrieval-augmented generation (RAG) assistant for personalized, knowledge-grounded dialogue; (ii) a synthetic client-therapist dialogue generator conditioned on client profiles to facilitate research on therapeutic language and data augmentation; and (iii) a Well-being AI crew, implemented with CrewAI, that produces weekly self-care plans and guided meditation audio. The system is designed as a reflective space for emotional processing rather than a substitute for professional therapy. It illustrates how conversational assistants can lower barriers to support, complement existing care, and broaden access to mental health resources. To address the shortage of confidential therapeutic data, we introduce synthetic client-therapist dialogue generation conditioned on client profiles. Finally, the planner demonstrates an innovative agentic workflow for dynamically adaptive, personalized self-care, bridging the limitations of static well-being tools. We describe the architecture, demonstrate its functionalities, and report evaluation of the RAG assistant in curated well-being scenarios using both automatic LLM-based judgments and a human-user study. This work highlights opportunities for interdisciplinary collaboration between NLP researchers and mental health professionals to advance responsible innovation in human-AI interaction for well-being.

</details>


### [30] [Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning](https://arxiv.org/abs/2511.14460)
*Mingyue Cheng,Jie Ouyang,Shuo Yu,Ruiran Yan,Yucong Luo,Zirui Liu,Daoyu Wang,Qi Liu,Enhong Chen*

Main category: cs.CL

TL;DR: 本文系统化地将马尔可夫决策过程框架扩展到LLM智能体，并提出了Agent-R1训练框架，通过多跳问答任务验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前RL在LLM智能体训练中的应用仍处于早期阶段，缺乏针对LLM智能体定制的RL方法以及灵活可扩展的训练框架。

Method: 1. 系统化扩展MDP框架来定义LLM智能体的关键组件；2. 提出模块化、灵活且用户友好的Agent-R1训练框架。

Result: 在多跳问答基准任务上的实验初步验证了所提方法和框架的有效性。

Conclusion: 该研究为RL在LLM智能体训练中的应用提供了系统化方法和实用框架，有助于推动这一领域的发展。

Abstract: Large Language Models (LLMs) are increasingly being explored for building Agents capable of active environmental interaction (e.g., via tool use) to solve complex problems. Reinforcement Learning (RL) is considered a key technology with significant potential for training such Agents; however, the effective application of RL to LLM Agents is still in its nascent stages and faces considerable challenges. Currently, this emerging field lacks in-depth exploration into RL approaches specifically tailored for the LLM Agent context, alongside a scarcity of flexible and easily extensible training frameworks designed for this purpose. To help advance this area, this paper first revisits and clarifies Reinforcement Learning methodologies for LLM Agents by systematically extending the Markov Decision Process (MDP) framework to comprehensively define the key components of an LLM Agent. Secondly, we introduce Agent-R1, a modular, flexible, and user-friendly training framework for RL-based LLM Agents, designed for straightforward adaptation across diverse task scenarios and interactive environments. We conducted experiments on Multihop QA benchmark tasks, providing initial validation for the effectiveness of our proposed methods and framework.

</details>


### [31] [LiveRAG: A diverse Q&A dataset with varying difficulty level for RAG evaluation](https://arxiv.org/abs/2511.14531)
*David Carmel,Simone Filice,Guy Horowitz,Yoelle Maarek,Alex Shtoff,Oren Somekh,Ran Tavory*

Main category: cs.CL

TL;DR: LiveRAG是一个包含895个合成问答对的公开基准数据集，用于系统评估基于检索增强生成(RAG)的问答系统，包含真实答案、支持证据、难度和区分度评分。


<details>
  <summary>Details</summary>
Motivation: 随着RAG在生成式AI解决方案中日益重要，需要系统评估其有效性的基准。

Method: 基于SIGIR'2025 LiveRAG挑战赛使用的数据集构建，添加了真实答案、支持证据，并应用项目反应理论模型计算难度和区分度评分。

Result: 分析显示基准问题具有多样性、难度范围广，能有效区分系统能力。

Conclusion: LiveRAG基准将有助于推进RAG研究、进行系统评估和开发更鲁棒的问答系统。

Abstract: With Retrieval Augmented Generation (RAG) becoming more and more prominent in generative AI solutions, there is an emerging need for systematically evaluating their effectiveness. We introduce the LiveRAG benchmark, a publicly available dataset of 895 synthetic questions and answers designed to support systematic evaluation of RAG-based Q&A systems. This synthetic benchmark is derived from the one used during the SIGIR'2025 LiveRAG Challenge, where competitors were evaluated under strict time constraints. It is augmented with information that was not made available to competitors during the Challenge, such as the ground-truth answers, together with their associated supporting claims which were used for evaluating competitors' answers. In addition, each question is associated with estimated difficulty and discriminability scores, derived from applying an Item Response Theory model to competitors' responses. Our analysis highlights the benchmark's questions diversity, the wide range of their difficulty levels, and their usefulness in differentiating between system capabilities. The LiveRAG benchmark will hopefully help the community advance RAG research, conduct systematic evaluation, and develop more robust Q&A systems.

</details>


### [32] [Examining the Metrics for Document-Level Claim Extraction in Czech and Slovak](https://arxiv.org/abs/2511.14566)
*Lucia Makaiová,Martin Fajčík,Antonín Jarolím*

Main category: cs.CL

TL;DR: 本文探讨了文档级声明提取的评估方法，通过对齐声明集并计算相似度得分来评估模型提取性能，在捷克和斯洛伐克新闻评论数据集上进行了实验。


<details>
  <summary>Details</summary>
Motivation: 文档级声明提取在事实核查领域仍具挑战性，现有的评估方法关注有限，需要可靠的评估框架来比较模型提取和人工标注的声明集。

Method: 研究声明集对齐技术，通过识别最佳对齐和评估方法来计算声明集之间的相似度得分，作为模型提取性能的评估指标。

Result: 实验揭示了当前评估方法在文档级声明提取中的局限性，特别是在处理非正式语言、强本地语境和语言细微差别时的不足。

Conclusion: 需要更先进的评估方法，能够正确捕捉语义相似性并评估声明的关键属性，如原子性、可核查性和去语境化。

Abstract: Document-level claim extraction remains an open challenge in the field of fact-checking, and subsequently, methods for evaluating extracted claims have received limited attention. In this work, we explore approaches to aligning two sets of claims pertaining to the same source document and computing their similarity through an alignment score. We investigate techniques to identify the best possible alignment and evaluation method between claim sets, with the aim of providing a reliable evaluation framework. Our approach enables comparison between model-extracted and human-annotated claim sets, serving as a metric for assessing the extraction performance of models and also as a possible measure of inter-annotator agreement. We conduct experiments on newly collected dataset-claims extracted from comments under Czech and Slovak news articles-domains that pose additional challenges due to the informal language, strong local context, and subtleties of these closely related languages. The results draw attention to the limitations of current evaluation approaches when applied to document-level claim extraction and highlight the need for more advanced methods-ones able to correctly capture semantic similarity and evaluate essential claim properties such as atomicity, checkworthiness, and decontextualization.

</details>


### [33] [Leveraging Digitized Newspapers to Collect Summarization Data in Low-Resource Languages](https://arxiv.org/abs/2511.14598)
*Noam Dahan,Omer Kidron,Gabriel Stanovsky*

Main category: cs.CL

TL;DR: 利用历史报纸的头版预告自动收集多文档摘要数据，为资源匮乏语言构建首个希伯来语多文档摘要数据集HEBTEASESUM


<details>
  <summary>Details</summary>
Motivation: 在资源匮乏语言中高质量摘要数据稀缺，而数字化历史报纸提供了丰富的自然标注数据源

Method: 通过头版预告自动收集编辑对全文文章的摘要，开发适合不同语言资源水平的自动处理流程

Result: 在七种不同语言中验证了该现象的普遍性，成功构建了希伯来语首个多文档摘要数据集

Conclusion: 历史报纸头版预告是获取多语言摘要数据的有效方法，特别适用于资源匮乏语言

Abstract: High quality summarization data remains scarce in under-represented languages. However, historical newspapers, made available through recent digitization efforts, offer an abundant source of untapped, naturally annotated data. In this work, we present a novel method for collecting naturally occurring summaries via Front-Page Teasers, where editors summarize full length articles. We show that this phenomenon is common across seven diverse languages and supports multi-document summarization. To scale data collection, we develop an automatic process, suited to varying linguistic resource levels. Finally, we apply this process to a Hebrew newspaper title, producing HEBTEASESUM, the first dedicated multi-document summarization dataset in Hebrew.

</details>


### [34] [A Method for Characterizing Disease Progression from Acute Kidney Injury to Chronic Kidney Disease](https://arxiv.org/abs/2511.14603)
*Yilu Fang,Jordan G. Nestor,Casey N. Ta,Jerard Z. Kneifati-Hayek,Chunhua Weng*

Main category: cs.CL

TL;DR: 使用电子健康记录数据动态追踪急性肾损伤患者的临床演变，识别AKI向CKD进展的风险因素和轨迹模式


<details>
  <summary>Details</summary>
Motivation: 急性肾损伤患者发展为慢性肾病的风险很高，但识别最高风险患者仍然具有挑战性，需要数据驱动的方法来支持早期检测和干预

Method: 使用电子健康记录数据，通过纵向医疗代码和肌酐测量值聚类识别AKI后临床状态，采用多状态模型估计状态间转移概率和CKD进展风险

Result: 在20,699名AKI患者中，3,491人(17%)发展为CKD；识别出15个不同的AKI后状态，每个状态有不同的CKD发展概率；75%患者在研究期间保持单一状态或仅有一次转换

Conclusion: 本研究展示了一种数据驱动方法来识别高风险AKI患者，支持开发用于早期CKD检测和干预的决策支持工具

Abstract: Patients with acute kidney injury (AKI) are at high risk of developing chronic kidney disease (CKD), but identifying those at greatest risk remains challenging. We used electronic health record (EHR) data to dynamically track AKI patients' clinical evolution and characterize AKI-to-CKD progression. Post-AKI clinical states were identified by clustering patient vectors derived from longitudinal medical codes and creatinine measurements. Transition probabilities between states and progression to CKD were estimated using multi-state modeling. After identifying common post-AKI trajectories, CKD risk factors in AKI subpopulations were identified through survival analysis. Of 20,699 patients with AKI at admission, 3,491 (17%) developed CKD. We identified fifteen distinct post-AKI states, each with different probabilities of CKD development. Most patients (75%, n=15,607) remained in a single state or made only one transition during the study period. Both established (e.g., AKI severity, diabetes, hypertension, heart failure, liver disease) and novel CKD risk factors, with their impact varying across these clinical states. This study demonstrates a data-driven approach for identifying high-risk AKI patients, supporting the development of decision-support tools for early CKD detection and intervention.

</details>


### [35] [Bridging Human and Model Perspectives: A Comparative Analysis of Political Bias Detection in News Media Using Large Language Models](https://arxiv.org/abs/2511.14606)
*Shreya Adrita Banik,Niaz Nafi Rahman,Tahsina Moiukh,Farig Sadeque*

Main category: cs.CL

TL;DR: 本研究比较了人类标注与多个大语言模型在政治偏见检测方面的表现，发现RoBERTa与人类标注最一致，GPT在零样本设置下表现最佳，揭示了人类与LLM在政治偏见感知上的系统性差异。


<details>
  <summary>Details</summary>
Motivation: 尽管NLP技术已能自动分类政治偏见，但大语言模型与人类判断的一致性程度仍未被充分探索和理解，需要系统评估人类与模型在偏见检测方面的差异。

Method: 构建手动标注的新闻文章数据集，评估标注一致性、偏见极性和模型间一致性，量化人类与模型在偏见感知上的差异，比较GPT、BERT、RoBERTa和FLAN等模型表现。

Result: 传统基于transformer的模型中，RoBERTa与人类标签对齐度最高；生成模型如GPT在零样本设置下与人类标注总体一致性最强；微调后的RoBERTa模型在所有基线中获得了最高准确率和最强的人类标注对齐度。

Conclusion: 人类与LLM在政治偏见感知上存在系统性差异，需要结合人类可解释性和模型可扩展性的混合评估框架来自动化媒体偏见检测。

Abstract: Detecting political bias in news media is a complex task that requires interpreting subtle linguistic and contextual cues. Although recent advances in Natural Language Processing (NLP) have enabled automatic bias classification, the extent to which large language models (LLMs) align with human judgment still remains relatively underexplored and not yet well understood. This study aims to present a comparative framework for evaluating the detection of political bias across human annotations and multiple LLMs, including GPT, BERT, RoBERTa, and FLAN. We construct a manually annotated dataset of news articles and assess annotation consistency, bias polarity, and inter-model agreement to quantify divergence between human and model perceptions of bias. Experimental results show that among traditional transformer-based models, RoBERTa achieves the highest alignment with human labels, whereas generative models such as GPT demonstrate the strongest overall agreement with human annotations in a zero-shot setting. Among all transformer-based baselines, our fine-tuned RoBERTa model acquired the highest accuracy and the strongest alignment with human-annotated labels. Our findings highlight systematic differences in how humans and LLMs perceive political slant, underscoring the need for hybrid evaluation frameworks that combine human interpretability with model scalability in automated media bias detection.

</details>


### [36] [Enhancing Agentic Autonomous Scientific Discovery with Vision-Language Model Capabilities](https://arxiv.org/abs/2511.14631)
*Kahaan Gandhi,Boris Bolliet,Inigo Zubeldia*

Main category: cs.CL

TL;DR: 多智能体系统通过视觉语言模型（VLM）作为评判者，能够自主进行科学发现，通过将图表作为可验证检查点来纠正错误并引导数据探索。


<details>
  <summary>Details</summary>
Motivation: 提高自主科学发现的端到端性能，通过VLM评估图表来实时纠正智能体的错误推理路径，减少人工干预需求。

Method: 使用VLM作为评判者，基于动态生成的领域特定评分标准评估图表，使智能体能够自我纠正错误并引导探索性数据分析。

Result: 在宇宙学和天体化学案例研究中，系统能够从错误推理路径中恢复并适应新数据集。在10个数据驱动发现任务的基准测试中，VLM增强系统达到0.7-0.8的pass@1分数，显著优于代码基线（0.2-0.3）和代码加文本基线（0.4-0.5）。

Conclusion: VLM增强的多智能体系统显著提升了自主科学发现的性能，同时提供了可审计的推理轨迹以提高可解释性。

Abstract: We show that multi-agent systems guided by vision-language models (VLMs) improve end-to-end autonomous scientific discovery. By treating plots as verifiable checkpoints, a VLM-as-a-judge evaluates figures against dynamically generated domain-specific rubrics, enabling agents to correct their own errors and steer exploratory data analysis in real-time. Case studies in cosmology and astrochemistry demonstrate recovery from faulty reasoning paths and adaptation to new datasets without human intervention. On a 10-task benchmark for data-driven discovery, VLM-augmented systems achieve pass at 1 scores of 0.7-0.8, compared to 0.2-0.3 for code-only and 0.4-0.5 for code-and-text baselines, while also providing auditable reasoning traces that improve interpretability. Code available here: https://github.com/CMBAgents/cmbagent

</details>


### [37] [A Specialized Large Language Model for Clinical Reasoning and Diagnosis in Rare Diseases](https://arxiv.org/abs/2511.14638)
*Tao Yang,Dandan Huang,Yunting Lin,Pengfei Wu,Zhikun Wu,Gangyuan Ma,Yulan Lu,Xinran Dong,Dingpeng Li,Junshuang Ge,Zhiyan Zhang,Xuanzhao Huang,Wenyan Nong,Yao Zhou,Hui Tang,Hongxi Yang,Shijie Zhang,Juan Li,Xiaojun Cao,Lin Yang,Xia Gao,Kaishou Xu,Xiaoqiong Gu,Wen Zhang,Huimin Xia,Li Liu,Wenhao Zhou,Mulin Jun Li*

Main category: cs.CL

TL;DR: RareSeek R1是一个专门用于罕见病诊断的AI系统，通过领域专业临床语料库、分阶段指令调优和基于图的检索，在嘈杂或重叠表型下实现最准确的诊断性能，与经验丰富的医生表现相当。


<details>
  <summary>Details</summary>
Motivation: 罕见病影响全球数亿人，但诊断通常需要数年时间。传统诊断流程将噪声证据提取与下游推理诊断分离，而通用/医学大语言模型面临真实世界电子健康记录稀缺、领域知识陈旧和幻觉问题。

Method: 构建大型领域专业临床语料库和临床医生验证的推理集，通过分阶段指令调优、思维链学习和基于图的检索开发RareSeek R1。

Result: 在多中心电子健康记录叙述和公共基准测试中，RareSeek R1达到最先进的准确性、稳健的泛化能力和在噪声或重叠表型下的稳定性。增强检索在叙述与优先变异配对时效果最佳。人类研究表明与经验丰富的医生表现相当。

Conclusion: 这项工作推进了以叙述优先、知识整合的推理范式，缩短诊断历程，并提供可审计、临床可转化的决策支持。透明推理突出了支持许多正确诊断的决定性非表型证据（如影像学、干预措施、功能测试）。

Abstract: Rare diseases affect hundreds of millions worldwide, yet diagnosis often spans years. Convectional pipelines decouple noisy evidence extraction from downstream inferential diagnosis, and general/medical large language models (LLMs) face scarce real world electronic health records (EHRs), stale domain knowledge, and hallucinations. We assemble a large, domain specialized clinical corpus and a clinician validated reasoning set, and develop RareSeek R1 via staged instruction tuning, chain of thought learning, and graph grounded retrieval. Across multicenter EHR narratives and public benchmarks, RareSeek R1 attains state of the art accuracy, robust generalization, and stability under noisy or overlapping phenotypes. Augmented retrieval yields the largest gains when narratives pair with prioritized variants by resolving ambiguity and aligning candidates to mechanisms. Human studies show performance on par with experienced physicians and consistent gains in assistive use. Notably, transparent reasoning highlights decisive non phenotypic evidence (median 23.1%, such as imaging, interventions, functional tests) underpinning many correct diagnoses. This work advances a narrative first, knowledge integrated reasoning paradigm that shortens the diagnostic odyssey and enables auditable, clinically translatable decision support.

</details>


### [38] [Graded strength of comparative illusions is explained by Bayesian inference](https://arxiv.org/abs/2511.14642)
*Yuhan Zhang,Erxiao Wang,Cory Shain*

Main category: cs.CL

TL;DR: 该研究通过定量模型验证了语言处理中的比较幻觉现象可以用贝叶斯推理和噪声通道理论来解释，模型成功预测了幻觉强度的细微差异和代词与名词短语的影响。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解释语言处理中的比较幻觉现象（如"More students have been to Russia than I have"），验证噪声通道理论能否解释这种系统性误读。

Method: 通过将统计语言模型与人类行为数据相结合，构建定量模型来计算合理解释的后验概率，直接预测幻觉强度。

Result: 模型不仅解释了比较幻觉强度的细微变化，还解释了之前未解释的代词与完整名词短语than从句主语的影响效应。

Conclusion: 研究结果支持噪声通道理论作为语言处理的统一计算层面理论，能够解释包括幻觉和非幻觉在内的多种语言处理现象。

Abstract: Like visual processing, language processing is susceptible to illusions in which people systematically misperceive stimuli. In one such case--the comparative illusion (CI), e.g., More students have been to Russia than I have--comprehenders tend to judge the sentence as acceptable despite its underlying nonsensical comparison. Prior research has argued that this phenomenon can be explained as Bayesian inference over a noisy channel: the posterior probability of an interpretation of a sentence is proportional to both the prior probability of that interpretation and the likelihood of corruption into the observed (CI) sentence. Initial behavioral work has supported this claim by evaluating a narrow set of alternative interpretations of CI sentences and showing that comprehenders favor interpretations that are more likely to have been corrupted into the illusory sentence. In this study, we replicate and go substantially beyond this earlier work by directly predicting the strength of illusion with a quantitative model of the posterior probability of plausible interpretations, which we derive through a novel synthesis of statistical language models with human behavioral data. Our model explains not only the fine gradations in the strength of CI effects, but also a previously unexplained effect caused by pronominal vs. full noun phrase than-clause subjects. These findings support a noisy-channel theory of sentence comprehension by demonstrating that the theory makes novel predictions about the comparative illusion that bear out empirically. This outcome joins related evidence of noisy channel processing in both illusory and non-illusory contexts to support noisy channel inference as a unified computational-level theory of diverse language processing phenomena.

</details>


### [39] [Bias in, Bias out: Annotation Bias in Multilingual Large Language Models](https://arxiv.org/abs/2511.14662)
*Xia Cui,Ziyi Huang,Naeemeh Adel*

Main category: cs.CL

TL;DR: 提出一个理解NLP数据集标注偏见的综合框架，包括指令偏见、标注者偏见和情境文化偏见，并介绍了检测方法和缓解策略。


<details>
  <summary>Details</summary>
Motivation: NLP数据集中的标注偏见是开发多语言大语言模型的主要挑战，特别是在文化多样性环境中。偏见会扭曲模型输出并加剧社会危害。

Method: 提出标注偏见分类法，包括指令偏见、标注者偏见和情境文化偏见；综述检测方法如标注者间一致性、模型分歧和元数据分析；提出主动和被动的缓解策略。

Result: 开发了一个全面的标注偏见框架，包括偏见分类、检测指标综合、适用于多语言环境的集成偏见缓解方法，以及对标注过程的伦理分析。

Conclusion: 这些见解旨在为LLMs构建更公平和文化基础的标注流程提供指导，促进多语言模型的公平发展。

Abstract: Annotation bias in NLP datasets remains a major challenge for developing multilingual Large Language Models (LLMs), particularly in culturally diverse settings. Bias from task framing, annotator subjectivity, and cultural mismatches can distort model outputs and exacerbate social harms. We propose a comprehensive framework for understanding annotation bias, distinguishing among instruction bias, annotator bias, and contextual and cultural bias. We review detection methods (including inter-annotator agreement, model disagreement, and metadata analysis) and highlight emerging techniques such as multilingual model divergence and cultural inference. We further outline proactive and reactive mitigation strategies, including diverse annotator recruitment, iterative guideline refinement, and post-hoc model adjustments. Our contributions include: (1) a typology of annotation bias; (2) a synthesis of detection metrics; (3) an ensemble-based bias mitigation approach adapted for multilingual settings, and (4) an ethical analysis of annotation processes. Together, these insights aim to inform more equitable and culturally grounded annotation pipelines for LLMs.

</details>


### [40] [Streamlining Industrial Contract Management with Retrieval-Augmented LLMs](https://arxiv.org/abs/2511.14671)
*Kristi Topollai,Tolga Dimlioglu,Anna Choromanska,Simon Odie,Reginald Hui*

Main category: cs.CL

TL;DR: 提出了一个模块化框架，通过检索增强生成(RAG)管道简化合同管理，包括合成数据生成、语义条款检索、可接受性分类和基于奖励的对齐，用于标记有问题的修订并生成改进方案。


<details>
  <summary>Details</summary>
Motivation: 合同管理涉及条款审查和谈判，但自动化面临标注数据稀缺和非结构化遗留合同丰富的挑战。

Method: 采用模块化框架，集成合成数据生成、语义条款检索、可接受性分类和奖励对齐的RAG管道。

Result: 与行业合作伙伴合作开发评估，系统在识别和优化有问题修订方面达到超过80%的准确率。

Conclusion: 系统在现实世界低资源条件下表现出色，为加速合同修订工作流程提供了实用手段。

Abstract: Contract management involves reviewing and negotiating provisions, individual clauses that define rights, obligations, and terms of agreement. During this process, revisions to provisions are proposed and iteratively refined, some of which may be problematic or unacceptable. Automating this workflow is challenging due to the scarcity of labeled data and the abundance of unstructured legacy contracts. In this paper, we present a modular framework designed to streamline contract management through a retrieval-augmented generation (RAG) pipeline. Our system integrates synthetic data generation, semantic clause retrieval, acceptability classification, and reward-based alignment to flag problematic revisions and generate improved alternatives. Developed and evaluated in collaboration with an industry partner, our system achieves over 80% accuracy in both identifying and optimizing problematic revisions, demonstrating strong performance under real-world, low-resource conditions and offering a practical means of accelerating contract revision workflows.

</details>


### [41] [Quadratic Term Correction on Heaps' Law](https://arxiv.org/abs/2511.14683)
*Oscar Fontanelli,Wentian Li*

Main category: cs.CL

TL;DR: 本文发现词型-词符关系在双对数尺度上仍呈轻微凹形，通过二次函数能完美拟合数据，线性系数略大于1，二次系数约-0.02，并用随机抽球模型解释了这种曲率。


<details>
  <summary>Details</summary>
Motivation: 传统Heap定律描述词型-词符关系为幂律函数，在双对数尺度应为直线，但实际观测发现即使在双对数尺度上曲线仍轻微凹形，这挑战了幂律关系的有效性。

Method: 使用20部英文小说或翻译作品，对log(词型)-log(词符)数据进行回归分析，包含线性和二次项；并采用"有放回随机抽球"模型来解释曲率。

Result: 回归分析显示线性系数略大于1，二次系数约-0.02；使用随机抽球模型证明双对数尺度的曲率等同于负的"伪方差"。

Conclusion: 词型-词符关系在双对数尺度上呈轻微凹形，二次函数能完美拟合数据，随机抽球模型为这种曲率提供了理论解释。

Abstract: Heaps' or Herdan's law characterizes the word-type vs. word-token relation by a power-law function, which is concave in linear-linear scale but a straight line in log-log scale. However, it has been observed that even in log-log scale, the type-token curve is still slightly concave, invalidating the power-law relation. At the next-order approximation, we have shown, by twenty English novels or writings (some are translated from another language to English), that quadratic functions in log-log scale fit the type-token data perfectly. Regression analyses of log(type)-log(token) data with both a linear and quadratic term consistently lead to a linear coefficient of slightly larger than 1, and a quadratic coefficient around -0.02. Using the ``random drawing colored ball from the bag with replacement" model, we have shown that the curvature of the log-log scale is identical to a ``pseudo-variance" which is negative. Although a pseudo-variance calculation may encounter numeric instability when the number of tokens is large, due to the large values of pseudo-weights, this formalism provides a rough estimation of the curvature when the number of tokens is small.

</details>


### [42] [SMRC: Aligning Large Language Models with Student Reasoning for Mathematical Error Correction](https://arxiv.org/abs/2511.14684)
*Biaojie Zeng,Min Zhang,Juan Zhou,Fengrui Liu,Ruiyang Huang,Xin Lin*

Main category: cs.CL

TL;DR: 提出SMRC方法，使用蒙特卡洛树搜索和过程监督来自动纠正学生数学推理错误，在教育场景中实现教师风格的指导。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注模型自校正，无法满足教育场景中系统指导学生解题过程的需求，需要更符合教师风格的纠正方法。

Method: 将学生推理建模为多步序列决策问题，引入蒙特卡洛树搜索探索最优纠正路径，利用广度优先搜索和最终答案评估生成奖励信号，通过反向传播机制实现细粒度过程监督。

Result: 在两个公开数据集(ProcessBench和MR-GSM8K)和自建的高中数学基准MSEB上，SMRC在效果和整体性能上显著优于现有方法。

Conclusion: SMRC方法能有效纠正学生数学推理错误，在教育应用中具有重要价值，特别是在保持正确步骤和提升解题准确性方面表现优异。

Abstract: Large language models (LLMs) often make reasoning errors when solving mathematical problems, and how to automatically detect and correct these errors has become an important research direction. However, existing approaches \textit{mainly focus on self-correction within the model}, which falls short of the ``teacher-style`` correction required in educational settings, \textit{i.e.}, systematically guiding and revising a student's problem-solving process. To address this gap, we propose \texttt{SMRC} (\textit{\underline{S}tudent \underline{M}athematical \underline{R}easoning \underline{C}orrection}), a novel method that aligns LLMs with student reasoning. Specifically, \texttt{SMRC} formulates student reasoning as a multi-step sequential decision problem and introduces Monte Carlo Tree Search (MCTS) to explore optimal correction paths. To reduce the cost of the annotating process-level rewards, we leverage breadth-first search (BFS) guided by LLMs and final-answer evaluation to generate reward signals, which are then distributed across intermediate reasoning steps via a back-propagation mechanism, enabling fine-grained process supervision. Additionally, we construct a benchmark for high school mathematics, MSEB (Multi-Solution Error Benchmark), consisting of 158 instances that include problem statements, student solutions, and correct reasoning steps. We further propose a dual evaluation protocol centered on \textbf{solution accuracy} and \textbf{correct-step retention}, offering a comprehensive measure of educational applicability. Experiments demonstrate that \texttt{SMRC} significantly outperforms existing methods on two public datasets (ProcessBench and MR-GSM8K) and our MSEB in terms of effectiveness and overall performance. The code and data are available at https://github.com/Mind-Lab-ECNU/SMRC.

</details>


### [43] [Encoding and Understanding Astrophysical Information in Large Language Model-Generated Summaries](https://arxiv.org/abs/2511.14685)
*Kiera McCormick,Rafael Martínez-Galarza*

Main category: cs.CL

TL;DR: 研究探索LLM嵌入是否能编码从科学测量获得的天体物理统计量，分析提示词作用和语言要素对物理信息编码的重要性。


<details>
  <summary>Details</summary>
Motivation: 利用LLM在跨领域泛化和上下文学习的能力，研究其能否编码通常只能从科学测量获得、在文本描述中松散编码的物理信息。

Method: 使用稀疏自编码器从文本中提取可解释特征，分析提示词对物理量编码的影响以及语言要素的重要性。

Result: 通过天体物理学作为测试平台，验证LLM嵌入对物理统计量的编码能力。

Conclusion: LLM能够有效编码物理信息，提示词和特定语言要素在物理量编码中起关键作用。

Abstract: Large Language Models have demonstrated the ability to generalize well at many levels across domains, modalities, and even shown in-context learning capabilities. This enables research questions regarding how they can be used to encode physical information that is usually only available from scientific measurements, and loosely encoded in textual descriptions. Using astrophysics as a test bed, we investigate if LLM embeddings can codify physical summary statistics that are obtained from scientific measurements through two main questions: 1) Does prompting play a role on how those quantities are codified by the LLM? and 2) What aspects of language are most important in encoding the physics represented by the measurement? We investigate this using sparse autoencoders that extract interpretable features from the text.

</details>


### [44] [Ground Truth Generation for Multilingual Historical NLP using LLMs](https://arxiv.org/abs/2511.14688)
*Clovis Gladstone,Zhao Fang,Spencer Dean Stewart*

Main category: cs.CL

TL;DR: 使用大型语言模型为历史法语和中文文本生成标注数据，通过微调spaCy模型显著提升了词性标注、词形还原和命名实体识别在特定历史时期的性能。


<details>
  <summary>Details</summary>
Motivation: 历史语言和低资源自然语言处理面临标注数据稀缺以及与现代网络语料领域不匹配的挑战，需要开发针对特定历史时期和语言的有效处理方法。

Method: 利用大型语言模型生成历史法语（16-20世纪）和中文（1900-1950）文本的标注数据，基于这些合成数据微调spaCy模型。

Result: 在特定历史时期测试中，词性标注、词形还原和命名实体识别任务均取得显著性能提升，证明了领域特定模型的重要性。

Conclusion: 即使是相对有限的合成数据也能显著改善计算人文学研究中低资源语料的自然语言处理工具，为历史语言处理提供了有效解决方案。

Abstract: Historical and low-resource NLP remains challenging due to limited annotated data and domain mismatches with modern, web-sourced corpora. This paper outlines our work in using large language models (LLMs) to create ground-truth annotations for historical French (16th-20th centuries) and Chinese (1900-1950) texts. By leveraging LLM-generated ground truth on a subset of our corpus, we were able to fine-tune spaCy to achieve significant gains on period-specific tests for part-of-speech (POS) annotations, lemmatization, and named entity recognition (NER). Our results underscore the importance of domain-specific models and demonstrate that even relatively limited amounts of synthetic data can improve NLP tools for under-resourced corpora in computational humanities research.

</details>


### [45] [Talk, Snap, Complain: Validation-Aware Multimodal Expert Framework for Fine-Grained Customer Grievances](https://arxiv.org/abs/2511.14693)
*Rishu Kumar Singh,Navneet Shreya,Sarmistha Das,Apoorva Singh,Sriparna Saha*

Main category: cs.CL

TL;DR: VALOR是一个用于多模态客户投诉分析的新框架，通过结合文本投诉和视觉证据进行细粒度分类，在复杂投诉场景中优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有投诉分析方法主要依赖单模态、短格式内容，需要处理多模态、多轮客户支持对话中的文本和视觉证据以实现更细粒度的投诉分析。

Method: 提出VALOR框架，采用多专家推理设置，使用大规模生成模型和思维链提示进行细致决策，通过语义对齐分数和元融合策略确保模态间一致性。

Result: 在标注细粒度方面和严重性标签的多模态投诉数据集上评估，VALOR始终优于基线模型，特别是在信息分布在文本和图像中的复杂投诉场景中。

Conclusion: 该研究强调了多模态交互和专家验证在实际投诉理解系统中的价值，支持联合国可持续发展目标9和12。

Abstract: Existing approaches to complaint analysis largely rely on unimodal, short-form content such as tweets or product reviews. This work advances the field by leveraging multimodal, multi-turn customer support dialogues, where users often share both textual complaints and visual evidence (e.g., screenshots, product photos) to enable fine-grained classification of complaint aspects and severity. We introduce VALOR, a Validation-Aware Learner with Expert Routing, tailored for this multimodal setting. It employs a multi-expert reasoning setup using large-scale generative models with Chain-of-Thought (CoT) prompting for nuanced decision-making. To ensure coherence between modalities, a semantic alignment score is computed and integrated into the final classification through a meta-fusion strategy. In alignment with the United Nations Sustainable Development Goals (UN SDGs), the proposed framework supports SDG 9 (Industry, Innovation and Infrastructure) by advancing AI-driven tools for robust, scalable, and context-aware service infrastructure. Further, by enabling structured analysis of complaint narratives and visual context, it contributes to SDG 12 (Responsible Consumption and Production) by promoting more responsive product design and improved accountability in consumer services. We evaluate VALOR on a curated multimodal complaint dataset annotated with fine-grained aspect and severity labels, showing that it consistently outperforms baseline models, especially in complex complaint scenarios where information is distributed across text and images. This study underscores the value of multimodal interaction and expert validation in practical complaint understanding systems. Resources related to data and codes are available here: https://github.com/sarmistha-D/VALOR

</details>


### [46] [Subword Tokenization Strategies for Kurdish Word Embeddings](https://arxiv.org/abs/2511.14696)
*Ali Salehi,Cassandra L. Jacobs*

Main category: cs.CL

TL;DR: 比较库尔德语词嵌入的三种分词策略：词级、基于语素和BPE方法，发现BPE在形态相似性评估中存在覆盖率偏差，而基于语素的方法在综合评估中表现更优。


<details>
  <summary>Details</summary>
Motivation: 研究库尔德语这种低资源语言的词嵌入分词策略，探索如何更好地保留形态相似性，同时避免评估偏差。

Method: 开发基于BiLSTM-CRF的形态分析器，使用最小人工标注进行引导训练，比较三种分词方法在相似性保持、聚类质量和语义组织等方面的表现。

Result: BPE方法仅评估28.6%的测试用例，而语素模型评估68.7%，导致BPE性能被高估。综合评估显示基于语素的分词在嵌入空间组织、语义邻域结构和形态复杂度覆盖方面更优。

Conclusion: 在低资源语言处理中，基于覆盖率感知的评估至关重要，基于语素的分词策略为低资源语言处理提供了更好的选择。

Abstract: We investigate tokenization strategies for Kurdish word embeddings by comparing word-level, morpheme-based, and BPE approaches on morphological similarity preservation tasks. We develop a BiLSTM-CRF morphological segmenter using bootstrapped training from minimal manual annotation and evaluate Word2Vec embeddings across comprehensive metrics including similarity preservation, clustering quality, and semantic organization. Our analysis reveals critical evaluation biases in tokenization comparison. While BPE initially appears superior in morphological similarity, it evaluates only 28.6\% of test cases compared to 68.7\% for morpheme model, creating artificial performance inflation. When assessed comprehensively, morpheme-based tokenization demonstrates superior embedding space organization, better semantic neighborhood structure, and more balanced coverage across morphological complexity levels. These findings highlight the importance of coverage-aware evaluation in low-resource language processing and offers different tokenization methods for low-resourced language processing.

</details>


### [47] [Strategic Innovation Management in the Age of Large Language Models Market Intelligence, Adaptive R&D, and Ethical Governance](https://arxiv.org/abs/2511.14709)
*Raha Aghaei,Ali A. Kiaei,Mahnaz Boush,Mahan Rofoosheh,Mohammad Zavvar*

Main category: cs.CL

TL;DR: LLMs通过自动化知识发现、促进假设生成、整合跨学科见解和推动创新生态系统合作，显著提升研发过程的效率和效果。


<details>
  <summary>Details</summary>
Motivation: 分析LLMs在变革研发过程中的多重功能，探索如何利用这些模型提高研究效率和创新速度。

Method: 通过广泛分析科学文献、专利数据库和实验数据，利用LLMs实现更灵活和知情的研发工作流程。

Result: LLMs能够显著提升研发过程的效率和效果，加速创新周期，缩短突破性想法的上市时间。

Conclusion: LLMs在研发过程中具有多重重要功能，能够通过自动化知识发现、促进假设生成等方式显著提升创新效率。

Abstract: This study analyzes the multiple functions of Large Language Models (LLMs) in transforming research and development (R&D) processes. By automating knowledge discovery, boosting hypothesis creation, integrating transdisciplinary insights, and enabling cooperation within innovation ecosystems, LLMs dramatically improve the efficiency and effectiveness of research processes. Through extensive analysis of scientific literature, patent databases, and experimental data, these models enable more flexible and informed R&D workflows, ultimately accelerating innovation cycles and lowering time-to-market for breakthrough ideas.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [48] [EchoAgent: Guideline-Centric Reasoning Agent for Echocardiography Measurement and Interpretation](https://arxiv.org/abs/2511.13948)
*Matin Daghyani,Lyuyang Wang,Nima Hashemi,Bassant Medhat,Baraa Abdelsamad,Eros Rojas Velez,XiaoXiao Li,Michael Y. C. Tsang,Christina Luong,Teresa S. M. Tsang,Purang Abolmaesumi*

Main category: cs.CV

TL;DR: EchoAgent是一个基于大语言模型控制的框架，通过协调专门的视觉工具来实现超声心动图的结构化、可解释自动化分析，支持视频级推理和指南测量分析。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习模型无法支持超声心动图所需的视频级推理和基于指南的测量分析，因此需要开发能够提供结构化、可解释自动化的解决方案。

Method: EchoAgent在大语言模型控制下协调专门的视觉工具，执行时间定位、空间测量和临床解释。关键贡献是测量可行性预测模型，用于确定解剖结构在每帧中是否可靠可测量，从而实现自主工具选择。

Result: 尽管增加了时空视频分析的复杂性，EchoAgent仍能获得准确、可解释的结果。输出基于视觉证据和临床指南，支持透明度和可追溯性。

Conclusion: 这项工作证明了通过任务特定工具和全视频级自动化，实现基于代理的、符合指南的超声心动图视频分析推理的可行性。EchoAgent为心脏超声领域的可信AI设定了新方向。

Abstract: Purpose: Echocardiographic interpretation requires video-level reasoning and guideline-based measurement analysis, which current deep learning models for cardiac ultrasound do not support. We present EchoAgent, a framework that enables structured, interpretable automation for this domain. Methods: EchoAgent orchestrates specialized vision tools under Large Language Model (LLM) control to perform temporal localization, spatial measurement, and clinical interpretation. A key contribution is a measurement-feasibility prediction model that determines whether anatomical structures are reliably measurable in each frame, enabling autonomous tool selection. We curated a benchmark of diverse, clinically validated video-query pairs for evaluation. Results: EchoAgent achieves accurate, interpretable results despite added complexity of spatiotemporal video analysis. Outputs are grounded in visual evidence and clinical guidelines, supporting transparency and traceability. Conclusion: This work demonstrates the feasibility of agentic, guideline-aligned reasoning for echocardiographic video analysis, enabled by task-specific tools and full video-level automation. EchoAgent sets a new direction for trustworthy AI in cardiac ultrasound.

</details>


### [49] [Error-Driven Scene Editing for 3D Grounding in Large Language Models](https://arxiv.org/abs/2511.14086)
*Yue Zhang,Zun Wang,Han Lin,Jialu Li,Jianing Yang,Yonatan Bitton,Idan Szpektor,Mohit Bansal*

Main category: cs.CV

TL;DR: 提出了DEER-3D框架，通过错误驱动的3D场景编辑生成针对性反事实数据，解决3D-LLMs在视觉和空间元素语言接地方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有3D-LLMs在准确将语言接地到3D环境中的视觉和空间元素方面存在限制，主要由于训练数据偏向语言推理而非空间理解，且缺乏3D资源导致固有的接地偏差未解决。

Method: 提出DEER-3D框架，采用"分解、诊断评估、编辑、重新训练"的工作流程。当识别到3D-LLM的接地失败时，首先诊断具体的谓词级错误（如属性或空间关系），然后执行最小化的谓词对齐3D场景编辑（如重新着色或重新定位），生成针对性反事实监督进行迭代模型微调。

Result: 在多个3D接地和场景理解任务的基准测试中，通过迭代精炼在所有评估数据集上持续展示了改进效果。

Conclusion: DEER-3D证明了针对性、错误驱动的场景编辑在弥合3D LLMs中语言推理能力与空间接地方面的有效性，无需昂贵的场景重建或大规模3D数据收集。

Abstract: Despite recent progress in 3D-LLMs, they remain limited in accurately grounding language to visual and spatial elements in 3D environments. This limitation stems in part from training data that focuses on language reasoning rather than spatial understanding due to scarce 3D resources, leaving inherent grounding biases unresolved. To address this, we propose 3D scene editing as a key mechanism to generate precise visual counterfactuals that mitigate these biases through fine-grained spatial manipulation, without requiring costly scene reconstruction or large-scale 3D data collection. Furthermore, to make these edits targeted and directly address the specific weaknesses of the model, we introduce DEER-3D, an error-driven framework following a structured "Decompose, Diagnostic Evaluation, Edit, and Re-train" workflow, rather than broadly or randomly augmenting data as in conventional approaches. Specifically, upon identifying a grounding failure of the 3D-LLM, our framework first diagnoses the exact predicate-level error (e.g., attribute or spatial relation). It then executes minimal, predicate-aligned 3D scene edits, such as recoloring or repositioning, to produce targeted counterfactual supervision for iterative model fine-tuning, significantly enhancing grounding accuracy. We evaluate our editing pipeline across multiple benchmarks for 3D grounding and scene understanding tasks, consistently demonstrating improvements across all evaluated datasets through iterative refinement. DEER-3D underscores the effectiveness of targeted, error-driven scene editing in bridging linguistic reasoning capabilities with spatial grounding in 3D LLMs.

</details>


### [50] [O3SLM: Open Weight, Open Data, and Open Vocabulary Sketch-Language Model](https://arxiv.org/abs/2511.14368)
*Rishi Gupta,Mukilan Karuppasamy,Shyam Marjit,Aditay Tripathi,Anirban Chakraborty*

Main category: cs.CV

TL;DR: 提出了O3SLM模型，通过构建大规模图像-草图-指令三元组数据集，显著提升了大型视觉语言模型对抽象手绘草图的理解能力，在多个草图任务上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有大型视觉语言模型在处理抽象视觉输入（特别是手绘草图）方面能力有限，主要瓶颈是缺乏同时包含草图、真实图像和自然语言指令的大规模数据集。

Method: 构建了大规模图像-草图-指令三元组数据集用于预训练和指令微调，并基于此训练了O3SLM模型。

Result: 在多个草图任务（目标定位、计数、图像检索、视觉问答）上，O3SLM显著优于现有LVLMs，达到最先进性能。

Conclusion: 通过专门设计的数据集和训练方法，可以有效提升LVLMs对抽象草图的理解能力，为实际应用提供了更好的支持。

Abstract: While Large Vision Language Models (LVLMs) are increasingly deployed in real-world applications, their ability to interpret abstract visual inputs remains limited. Specifically, they struggle to comprehend hand-drawn sketches, a modality that offers an intuitive means of expressing concepts that are difficult to describe textually. We identify the primary bottleneck as the absence of a large-scale dataset that jointly models sketches, photorealistic images, and corresponding natural language instructions. To address this, we present two key contributions: (1) a new, large-scale dataset of image-sketch-instruction triplets designed to facilitate both pretraining and instruction tuning, and (2) O3SLM, an LVLM trained on this dataset. Comprehensive evaluations on multiple sketch-based tasks: (a) object localization, (b) counting, (c) image retrieval i.e., (SBIR and fine-grained SBIR), and (d) visual question answering (VQA); while incorporating the three existing sketch datasets, namely QuickDraw!, Sketchy, and Tu Berlin, along with our generated SketchVCL dataset, show that O3SLM achieves state-of-the-art performance, substantially outperforming existing LVLMs in sketch comprehension and reasoning.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [51] [Rdgai: Classifying transcriptional changes using Large Language Models with a test case from an Arabic Gospel tradition](https://arxiv.org/abs/2511.13801)
*Robert Turnbull*

Main category: cs.DL

TL;DR: Rdgaia软件包使用多语言大语言模型自动分类文本传统中的变体类型，解决了传统系统发育方法中所有变化被视为等同的问题，为下游系统发育分析提供支持。


<details>
  <summary>Details</summary>
Motivation: 传统系统发育方法将所有文本变体视为等同，但实际上某些类型的变体更可能被引入。手动分类变体耗时费力，阻碍了这类分析的开展。

Method: 开发Rdgaia软件包，利用多语言大语言模型自动分类文本变体。用户先手动分类部分变体，然后LLM基于这些标注自动分类剩余变体，结果以TEI XML格式存储。

Result: 成功开发了自动化分类工具，能够高效处理文本变体分类任务，并以阿拉伯语福音书翻译数据进行了应用演示。

Conclusion: Rdgaia工具通过自动化变体分类过程，显著降低了文本传统系统发育分析的入门门槛，使概率方法的应用更加可行。

Abstract: Application of phylogenetic methods to textual traditions has traditionally treated all changes as equivalent even though it is widely recognized that certain types of variants were more likely to be introduced than others. While it is possible to give weights to certain changes using a maximum parsimony evaluation criterion, it is difficult to state a priori what these weights should be. Probabilistic methods, such as Bayesian phylogenetics, allow users to create categories of changes, and the transition rates for each category can be estimated as part of the analysis. This classification of types of changes in readings also allows for inspecting the probability of these categories across each branch in the resulting trees. However, classification of readings is time-consuming, as it requires categorizing each reading against every other reading at each variation unit, presenting a significant barrier to entry for this kind of analysis. This paper presents Rdgai, a software package that automates this classification task using multi-lingual large language models (LLMs). The tool allows users to easily manually classify changes in readings and then it uses these annotations in the prompt for an LLM to automatically classify the remaining reading transitions. These classifications are stored in TEI XML and ready for downstream phylogenetic analysis. This paper demonstrates the application with data an Arabic translation of the Gospels.

</details>


### [52] [SciRAG: Adaptive, Citation-Aware, and Outline-Guided Retrieval and Synthesis for Scientific Literature](https://arxiv.org/abs/2511.14362)
*Hang Ding,Yilun Zhao,Tiansheng Hu,Manasi Patwardhan,Arman Cohan*

Main category: cs.DL

TL;DR: SciRAG是一个开源的科学文献探索框架，通过自适应检索、引用感知的符号推理和大纲引导的合成，解决了现有RAG方法在科学文献处理中的局限性，显著提升了事实准确性和合成质量。


<details>
  <summary>Details</summary>
Motivation: 科学出版物的快速增长需要可扩展、可信赖的系统来综合不同文献的知识。现有的检索增强生成方法往往忽视引用图结构，对复杂查询适应不良，且产生难以验证的碎片化合成结果。

Method: SciRAG框架包含三个关键创新：(1) 自适应检索，灵活交替使用顺序和并行证据收集；(2) 引用感知符号推理，利用引用图组织和过滤支持文档；(3) 大纲引导合成，通过规划、批判和精炼确保答案的连贯性和透明归因。

Result: 在QASA和ScholarQA等多个基准测试上的广泛实验表明，SciRAG在事实准确性和合成质量方面优于现有系统。

Conclusion: SciRAG为可靠的大规模科学知识聚合建立了新的基础，解决了现有方法的局限性，提供了更高质量的科学文献探索能力。

Abstract: The accelerating growth of scientific publications has intensified the need for scalable, trustworthy systems to synthesize knowledge across diverse literature. While recent retrieval-augmented generation (RAG) methods have improved access to scientific information, they often overlook citation graph structure, adapt poorly to complex queries, and yield fragmented, hard-to-verify syntheses. We introduce SciRAG, an open-source framework for scientific literature exploration that addresses these gaps through three key innovations: (1) adaptive retrieval that flexibly alternates between sequential and parallel evidence gathering; (2) citation-aware symbolic reasoning that leverages citation graphs to organize and filter supporting documents; and (3) outline-guided synthesis that plans, critiques, and refines answers to ensure coherence and transparent attribution. Extensive experiments across multiple benchmarks such as QASA and ScholarQA demonstrate that SciRAG outperforms prior systems in factual accuracy and synthesis quality, establishing a new foundation for reliable, large-scale scientific knowledge aggregation.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [53] [Show and Tell: Prompt Strategies for Style Control in Multi-Turn LLM Code Generation](https://arxiv.org/abs/2511.13972)
*Jeremiah Bohr*

Main category: cs.SE

TL;DR: 该研究比较了基于指令、基于示例和组合提示在代码生成中的风格控制效果，发现组合提示在初始压缩和扩展控制方面表现最佳，指令提示有较大初始效果但扩展控制中等，示例提示初始效果有限且无扩展控制。


<details>
  <summary>Details</summary>
Motivation: 语言模型生成的代码往往过于冗长，与人类基准存在风格差异，需要研究如何通过提示机制实现风格控制，特别是在代码增强过程中保持风格约束。

Method: 采用四条件系统提示，在配对的两轮协议中让模型首先生成Python任务解决方案，然后在通用改进指令下修订代码，保持用户任务不变（N=160对程序）。

Result: 组合提示产生最强的初始压缩和最大的扩展控制；指令提示显示较大的初始效果和中等的扩展控制；示例提示显示适度的初始效果且无扩展控制。

Conclusion: 初始提示效果和扩展控制是提示设计的两个独立方面，组合方法在两轮工作流程中提供最稳定的风格控制。

Abstract: Language models generate functionally correct code that tends toward excessive verbosity, with elaborate documentation and defensive patterns that diverge from human baselines. Two prompting mechanisms have emerged for stylistic control: instruction based prompts that articulate abstract directives, and example based prompts that provide concrete code demonstrations. The core problem is whether stylistic constraints persist when models enhance initial implementations with additional features while maintaining high functional accuracy. Here we show that instruction-based, example-based, and combined prompts produce distinct patterns of initial control and expansion discipline over one enhancement turn. We manipulated system prompts across four conditions in a paired two-turn protocol where models first generated solutions to an intermediate Python task, then revised their code under general improvement directives, holding the user task fixed (N = 160 paired programs). Combined prompts produced the strongest initial compression and greatest expansion discipline. Instructions showed large initial effects and moderate expansion discipline. Examples showed modest initial effects with no expansion discipline. These results show that initial prompt effectiveness and expansion discipline are separate aspects of prompt design, and that combined approaches provide the most stable stylistic control in this two-turn workflow.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [54] [Scaling Patterns in Adversarial Alignment: Evidence from Multi-LLM Jailbreak Experiments](https://arxiv.org/abs/2511.13788)
*Samuel Nathanson,Rebecca Williams,Cynthia Matuszek*

Main category: cs.LG

TL;DR: 研究发现大型语言模型可以通过规模优势系统性地越狱小型模型，模型大小比例与有害行为概率呈正相关，攻击者行为多样性比目标易感性对对抗结果影响更大。


<details>
  <summary>Details</summary>
Motivation: 探究在多智能体和安全关键环境中，大型语言模型的漏洞在对抗性交互中如何扩展，特别是大型模型是否能系统性地越狱小型模型。

Method: 使用JailbreakBench标准化对抗任务，模拟超过6000次多轮攻击者-目标交互，涵盖主要LLM家族和规模（0.6B-120B参数），通过三个独立LLM法官评估危害分数和拒绝行为。

Result: 攻击者与目标大小比例的对数与平均危害分数呈强正相关（Pearson r=0.51），攻击者方行为方差（0.18）大于目标方差（0.10），攻击者拒绝频率与危害呈强负相关（rho=-0.93）。

Conclusion: 模型大小不对称性影响鲁棒性，攻击者方对齐能减轻有害响应，为对抗性扩展模式提供了探索性证据，需要更深入研究模型间对齐和安全性。

Abstract: Large language models (LLMs) increasingly operate in multi-agent and safety-critical settings, raising open questions about how their vulnerabilities scale when models interact adversarially. This study examines whether larger models can systematically jailbreak smaller ones - eliciting harmful or restricted behavior despite alignment safeguards. Using standardized adversarial tasks from JailbreakBench, we simulate over 6,000 multi-turn attacker-target exchanges across major LLM families and scales (0.6B-120B parameters), measuring both harm score and refusal behavior as indicators of adversarial potency and alignment integrity. Each interaction is evaluated through aggregated harm and refusal scores assigned by three independent LLM judges, providing a consistent, model-based measure of adversarial outcomes. Aggregating results across prompts, we find a strong and statistically significant correlation between mean harm and the logarithm of the attacker-to-target size ratio (Pearson r = 0.51, p < 0.001; Spearman rho = 0.52, p < 0.001), indicating that relative model size correlates with the likelihood and severity of harmful completions. Mean harm score variance is higher across attackers (0.18) than across targets (0.10), suggesting that attacker-side behavioral diversity contributes more to adversarial outcomes than target susceptibility. Attacker refusal frequency is strongly and negatively correlated with harm (rho = -0.93, p < 0.001), showing that attacker-side alignment mitigates harmful responses. These findings reveal that size asymmetry influences robustness and provide exploratory evidence for adversarial scaling patterns, motivating more controlled investigations into inter-model alignment and safety.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [55] [When AI Does Science: Evaluating the Autonomous AI Scientist KOSMOS in Radiation Biology](https://arxiv.org/abs/2511.13825)
*Humza Nusrat,Omar Nusrat*

Main category: cs.AI

TL;DR: 评估KOSMOS自主AI科学家在辐射生物学中的三个假设，发现一个明确发现、一个不确定结果和一个错误假设，表明AI科学家能产生有用想法但需要严格验证


<details>
  <summary>Details</summary>
Motivation: 评估自主AI科学家KOSMOS在真实科学问题中的表现，验证其生成假设的能力和可靠性

Method: 使用简单随机基因零基准测试KOSMOS在三个辐射生物学问题上的表现：DNA损伤反应能力预测p53转录反应、OGT和CDO1表达预测辐射响应模块强度、12基因特征预测前列腺癌放疗后生存

Result: 假设1不支持（DDR评分与p53响应弱负相关）；假设2中OGT弱相关，CDO1为明确异常值；假设3的12基因特征达到一致性指数0.61但效应大小不唯一

Conclusion: AI科学家能生成有用想法，但需要针对适当零模型进行严格审计，以避免虚假发现

Abstract: Agentic AI "scientists" now use language models to search the literature, run analyses, and generate hypotheses. We evaluate KOSMOS, an autonomous AI scientist, on three problems in radiation biology using simple random-gene null benchmarks. Hypothesis 1: baseline DNA damage response (DDR) capacity across cell lines predicts the p53 transcriptional response after irradiation (GSE30240). Hypothesis 2: baseline expression of OGT and CDO1 predicts the strength of repressed and induced radiation-response modules in breast cancer cells (GSE59732). Hypothesis 3: a 12-gene expression signature predicts biochemical recurrence-free survival after prostate radiotherapy plus androgen deprivation therapy (GSE116918). The DDR-p53 hypothesis was not supported: DDR score and p53 response were weakly negatively correlated (Spearman rho = -0.40, p = 0.76), indistinguishable from random five-gene scores. OGT showed only a weak association (r = 0.23, p = 0.34), whereas CDO1 was a clear outlier (r = 0.70, empirical p = 0.0039). The 12-gene signature achieved a concordance index of 0.61 (p = 0.017) but a non-unique effect size. Overall, KOSMOS produced one well-supported discovery, one plausible but uncertain result, and one false hypothesis, illustrating that AI scientists can generate useful ideas but require rigorous auditing against appropriate null models.

</details>


### [56] [AISAC: An Integrated multi-agent System for Transparent, Retrieval-Grounded Scientific Assistance](https://arxiv.org/abs/2511.14043)
*Chandrachur Bhattacharya,Sibendu Som*

Main category: cs.AI

TL;DR: AISAC是一个在阿贡国家实验室开发的多智能体系统，用于科学和工程工作流。它基于LangGraph、FAISS和SQLite等技术构建，专注于透明度、溯源追踪和科学适应性。


<details>
  <summary>Details</summary>
Motivation: 开发一个集成化的多智能体系统来支持科学和工程工作流，强调透明度、溯源追踪和跨领域适应性。

Method: 采用Router-Planner-Coordinator工作流和可选的Evaluator角色，使用提示工程智能体通过LangGraph协调。结合FAISS向量搜索和SQLite持久化的混合内存方法，以及基于文件哈希的增量索引策略。

Result: 系统已应用于阿贡国家实验室的多个研究领域，包括废物转化产品和能源过程安全等专门部署，以及通用科学辅助，展示了其跨领域适用性。

Conclusion: AISAC成功构建了一个可定制、透明且具有溯源能力的多智能体科学助手系统原型，在多个科学领域证明了其有效性。

Abstract: AI Scientific Assistant Core (AISAC) is an integrated multi-agent system developed at Argonne National Laboratory for scientific and engineering workflows. AISAC builds on established technologies - LangGraph for orchestration, FAISS for vector search, and SQLite for persistence - and integrates them into a unified system prototype focused on transparency, provenance tracking, and scientific adaptability.
  The system implements a Router-Planner-Coordinator workflow and an optional Evaluator role, using prompt-engineered agents coordinated via LangGraph's StateGraph and supported by helper agents such as a Researcher. Each role is defined through custom system prompts that enforce structured JSON outputs. A hybrid memory approach (FAISS + SQLite) enables both semantic retrieval and structured conversation history. An incremental indexing strategy based on file hashing minimizes redundant re-embedding when scientific corpora evolve. A configuration-driven project bootstrap layer allows research teams to customize tools, prompts, and data sources without modifying core code.
  All agent decisions, tool invocations, and retrievals are logged and visualized through a custom Gradio interface, providing step-by-step transparency for each reasoning episode. The authors have applied AISAC to multiple research areas at Argonne, including specialized deployments for waste-to-products research and energy process safety, as well as general-purpose scientific assistance, demonstrating its cross-domain applicability.

</details>


### [57] [PRISM: Prompt-Refined In-Context System Modelling for Financial Retrieval](https://arxiv.org/abs/2511.14130)
*Chun Chet Ng,Jia Yu Lim,Wei Zeng Low*

Main category: cs.AI

TL;DR: PRISM是一个无需训练的多代理框架，通过系统提示、上下文学习和轻量级多代理系统，在金融信息检索任务中实现了0.71818的NDCG@5得分。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，从冗长的财务文件中提取任务相关信息已成为关键的工业应用需求，对运营和分析决策至关重要。

Method: PRISM框架集成了精炼的系统提示、上下文学习和轻量级多代理系统，通过提示工程提供精确任务指令，ICL提供语义相关的少样本示例，多代理系统建模协调评分行为。

Result: 在受限验证集上实现了0.71818的NDCG@5得分，证明该框架在生产规模的金融检索中可行且稳健。

Conclusion: PRISM的模块化、仅推理设计使其适用于实际应用场景，为金融信息检索提供了实用的解决方案。

Abstract: With the rapid progress of large language models (LLMs), financial information retrieval has become a critical industrial application. Extracting task-relevant information from lengthy financial filings is essential for both operational and analytical decision-making. The FinAgentBench dataset formalizes this problem through two tasks: document ranking and chunk ranking. We present PRISM, a training-free framework that integrates refined system prompting, in-context learning (ICL), and a lightweight multi-agent system. Each component is examined extensively to reveal their synergies: prompt engineering provides precise task instructions, ICL supplies semantically relevant few-shot examples, and the multi-agent system models coordinated scoring behaviour. Our best configuration achieves an NDCG@5 of 0.71818 on the restricted validation split. We further demonstrate that PRISM is feasible and robust for production-scale financial retrieval. Its modular, inference-only design makes it practical for real-world use cases. The source code is released at https://bit.ly/prism-ailens.

</details>


### [58] [DataSage: Multi-agent Collaboration for Insight Discovery with External Knowledge Retrieval, Multi-role Debating, and Multi-path Reasoning](https://arxiv.org/abs/2511.14299)
*Xiaochuan Liu,Yuanfeng Song,Xiaoming Yin,Xing Chen*

Main category: cs.AI

TL;DR: DataSage是一个多智能体框架，通过外部知识检索、多角色辩论机制和多路径推理来解决现有数据洞察代理在领域知识利用不足、分析深度浅和代码生成易出错的问题。


<details>
  <summary>Details</summary>
Motivation: 在数据驱动时代，全自动端到端数据分析特别是洞察发现对于组织决策至关重要。现有数据洞察代理在领域知识利用、分析深度和代码生成准确性方面存在不足。

Method: 提出DataSage多智能体框架，包含三个创新特性：外部知识检索丰富分析上下文、多角色辩论机制模拟多样化分析视角、多路径推理提高代码和洞察生成准确性。

Result: 在InsightBench上的广泛实验表明，DataSage在所有难度级别上都持续优于现有数据洞察代理。

Conclusion: DataSage为自动化数据洞察发现提供了一个有效的解决方案。

Abstract: In today's data-driven era, fully automated end-to-end data analytics, particularly insight discovery, is critical for discovering actionable insights that assist organizations in making effective decisions. With the rapid advancement of large language models (LLMs), LLM-driven agents have emerged as a promising paradigm for automating data analysis and insight discovery. However, existing data insight agents remain limited in several key aspects, often failing to deliver satisfactory results due to: (1) insufficient utilization of domain knowledge, (2) shallow analytical depth, and (3) error-prone code generation during insight generation. To address these issues, we propose DataSage, a novel multi-agent framework that incorporates three innovative features including external knowledge retrieval to enrich the analytical context, a multi-role debating mechanism to simulate diverse analytical perspectives and deepen analytical depth, and multi-path reasoning to improve the accuracy of the generated code and insights. Extensive experiments on InsightBench demonstrate that DataSage consistently outperforms existing data insight agents across all difficulty levels, offering an effective solution for automated data insight discovery.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [59] [GRPO Privacy Is at Risk: A Membership Inference Attack Against Reinforcement Learning With Verifiable Rewards](https://arxiv.org/abs/2511.14045)
*Yule Liu,Heyi Zhang,Jinyi Zheng,Zhen Sun,Zifan Peng,Tianshuo Cong,Yilong Yang,Xinlei He,Zhuo Ma*

Main category: cs.CR

TL;DR: 提出了DIBA攻击方法，这是首个专门针对RLVR训练范式的成员推理攻击框架，通过分析模型行为变化而非记忆来推断训练数据，在多个场景下显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: RLVR训练范式引入独特的隐私泄露模式：由于训练依赖自生成响应而非固定标准答案，成员推理需要判断给定提示是否用于微调，这种威胁源于行为变化而非答案记忆。

Method: DIBA攻击框架将焦点从记忆转向行为变化，利用模型在两个轴上的可测量行为偏移：优势侧改进（如正确性提升）和对数侧分歧（如策略漂移）。

Result: DIBA显著优于现有基线，达到约0.8的AUC和数量级更高的TPR@0.1%FPR，在多种设置下验证了其优越性，包括跨数据集、跨算法、黑盒场景和扩展到视觉语言模型。

Conclusion: 这是首个系统分析RLVR隐私漏洞的工作，揭示了即使在缺乏显式监督的情况下，训练数据暴露仍可通过行为痕迹可靠推断。

Abstract: Membership inference attacks (MIAs) on large language models (LLMs) pose significant privacy risks across various stages of model training. Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) have brought a profound paradigm shift in LLM training, particularly for complex reasoning tasks. However, the on-policy nature of RLVR introduces a unique privacy leakage pattern: since training relies on self-generated responses without fixed ground-truth outputs, membership inference must now determine whether a given prompt (independent of any specific response) is used during fine-tuning. This creates a threat where leakage arises not from answer memorization.
  To audit this novel privacy risk, we propose Divergence-in-Behavior Attack (DIBA), the first membership inference framework specifically designed for RLVR. DIBA shifts the focus from memorization to behavioral change, leveraging measurable shifts in model behavior across two axes: advantage-side improvement (e.g., correctness gain) and logit-side divergence (e.g., policy drift). Through comprehensive evaluations, we demonstrate that DIBA significantly outperforms existing baselines, achieving around 0.8 AUC and an order-of-magnitude higher TPR@0.1%FPR. We validate DIBA's superiority across multiple settings--including in-distribution, cross-dataset, cross-algorithm, black-box scenarios, and extensions to vision-language models. Furthermore, our attack remains robust under moderate defensive measures.
  To the best of our knowledge, this is the first work to systematically analyze privacy vulnerabilities in RLVR, revealing that even in the absence of explicit supervision, training data exposure can be reliably inferred through behavioral traces.

</details>


### [60] [Steganographic Backdoor Attacks in NLP: Ultra-Low Poisoning and Defense Evasion](https://arxiv.org/abs/2511.14301)
*Eric Xue,Ruiyi Zhang,Zijun Zhang,Pengtao Xie*

Main category: cs.CR

TL;DR: SteganoBackdoor是一种新型的后门攻击方法，利用自然语言隐写术将语义触发器转化为隐蔽载体，在极低数据投毒率下实现高攻击成功率，并能有效规避现有防御机制。


<details>
  <summary>Details</summary>
Motivation: 现有后门攻击研究过于关注风格化或令牌级扰动触发器，而忽视了更现实和危险的语义触发器（如特定名称或实体），这导致防御系统存在盲区。

Method: 采用梯度引导的数据优化过程，将语义触发器种子转化为隐写载体，这些载体嵌入高负载后门、保持流畅性且与触发器无表征相似性。

Result: 在多样化实验设置中，SteganoBackdoor以比先前方法低一个数量级的数据投毒率实现了超过99%的攻击成功率，并能全面规避数据级防御。

Conclusion: SteganoBackdoor揭示了当前防御系统的紧急盲点，需要立即关注对抗性数据防御和真实世界威胁建模。

Abstract: Transformer models are foundational to natural language processing (NLP) applications, yet remain vulnerable to backdoor attacks introduced through poisoned data, which implant hidden behaviors during training. To strengthen the ability to prevent such compromises, recent research has focused on designing increasingly stealthy attacks to stress-test existing defenses, pairing backdoor behaviors with stylized artifact or token-level perturbation triggers. However, this trend diverts attention from the harder and more realistic case: making the model respond to semantic triggers such as specific names or entities, where a successful backdoor could manipulate outputs tied to real people or events in deployed systems. Motivated by this growing disconnect, we introduce SteganoBackdoor, bringing stealth techniques back into line with practical threat models. Leveraging innocuous properties from natural-language steganography, SteganoBackdoor applies a gradient-guided data optimization process to transform semantic trigger seeds into steganographic carriers that embed a high backdoor payload, remain fluent, and exhibit no representational resemblance to the trigger. Across diverse experimental settings, SteganoBackdoor achieves over 99% attack success at an order-of-magnitude lower data-poisoning rate than prior approaches while maintaining unparalleled evasion against a comprehensive suite of data-level defenses. By revealing this practical and covert attack, SteganoBackdoor highlights an urgent blind spot in current defenses and demands immediate attention to adversarial data defenses and real-world threat modeling.

</details>
