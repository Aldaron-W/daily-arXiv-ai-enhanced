<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 114]
- [cs.CV](#cs.CV) [Total: 12]
- [cs.HC](#cs.HC) [Total: 2]
- [cs.LG](#cs.LG) [Total: 21]
- [cs.SI](#cs.SI) [Total: 2]
- [cs.SD](#cs.SD) [Total: 2]
- [eess.AS](#eess.AS) [Total: 2]
- [cs.RO](#cs.RO) [Total: 2]
- [cs.IR](#cs.IR) [Total: 4]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.AI](#cs.AI) [Total: 19]
- [cs.SE](#cs.SE) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [A Multi-lingual Dataset of Classified Paragraphs from Open Access Scientific Publications](https://arxiv.org/abs/2510.21762)
*Eric Jeangirard*

Main category: cs.CL

TL;DR: 该论文提出了一个包含83.3万个段落的科学文献数据集，这些段落从CC-BY许可的科学出版物中提取，分为致谢、数据提及、软件/代码提及和临床试验提及四类，主要用于文本分类和命名实体识别模型的训练。


<details>
  <summary>Details</summary>
Motivation: 构建一个专门用于科学文献挖掘的大规模标注数据集，以支持文本分类和命名实体识别系统的开发，填补科学文献处理领域的数据空白。

Method: 从法国开放科学监测器语料库中提取段落，使用GROBID进行处理，通过fastText进行语言识别，利用OpenAlex标注科学领域，最终构建包含四种分类类别的数据集。

Result: 成功创建了包含833,000个段落的公开数据集，涵盖英语和法语等多种欧洲语言，每个段落都标注了语言和科学领域信息，并在HuggingFace平台公开发布。

Conclusion: 该数据集为科学文献挖掘任务提供了有价值的资源，可用于训练文本分类模型和开发命名实体识别系统，促进科学文献处理研究的发展。

Abstract: We present a dataset of 833k paragraphs extracted from CC-BY licensed
scientific publications, classified into four categories: acknowledgments, data
mentions, software/code mentions, and clinical trial mentions. The paragraphs
are primarily in English and French, with additional European languages
represented. Each paragraph is annotated with language identification (using
fastText) and scientific domain (from OpenAlex). This dataset, derived from the
French Open Science Monitor corpus and processed using GROBID, enables training
of text classification models and development of named entity recognition
systems for scientific literature mining. The dataset is publicly available on
HuggingFace https://doi.org/10.57967/hf/6679 under a CC-BY license.

</details>


### [2] [Policy Optimization Prefers The Path of Least Resistance](https://arxiv.org/abs/2510.21853)
*Debdeep Sanyal,Aakash Sen Sharma,Dhruv Kumar,Saurabh Deshpande,Murari Mandal*

Main category: cs.CL

TL;DR: 政策优化算法在开放式的思维链结构中倾向于选择最简单路径，丢弃显式推理，直接输出答案，即使复杂格式被赋予更高奖励权重。


<details>
  <summary>Details</summary>
Motivation: 研究政策优化在放松严格思维链约束后的行为，当前研究缺乏对开放式思维链结构中政策优化行为的深入理解。

Method: 通过一系列受控实验和奖励分解实验，分析政策优化在不同模型和算法中的行为模式。

Result: 政策优化始终遵循最小阻力路径，即使复杂思维链格式被赋予4倍奖励权重，仍会退化为直接答案格式。

Conclusion: 给予政策自由度是一把双刃剑：既能发现高奖励捷径，也容易导致奖励函数被简单利用，这对对齐任务构成关键挑战。

Abstract: Policy optimization (PO) algorithms are used to refine Large Language Models
for complex, multi-step reasoning. Current state-of-the-art pipelines enforce a
strict think-then-answer format to elicit chain-of-thought (CoT); however, the
behavior of PO when these rigid constraints are relaxed into an open-ended CoT
structure remains an under-studied question. We investigate this gap with an
extensive suite of controlled experiments and identify a consistent principle:
\textit{policy optimization consistently follows the path of least resistance}.
When afforded the flexibility to interleave reasoning and response, policy
optimization consistently learns to discard explicit reasoning, causing the
policy to degenerate to a direct \texttt{<answer>}-only format. This outcome
holds true across various models and algorithms. We find that this collapse in
format is persistent even when the complex \texttt{<think><answer>} format is
assigned up to 4x larger reward weights. We formalize this principle through a
series of controlled reward decomposition experiments, demonstrating a clear
hierarchy: PO systematically optimizes for the simplest reward component first,
a preference that holds even when faced with mutually exclusive choices or
strong incentives for more complex behaviors. Finally, we show that successful
convergence on the high-reward shortcut is not a low-effort drift but is driven
by the optimization process that requires the KL-regularized policy to have
sufficient freedom to make a significant shift from its initial prior. Our
findings reveal that granting policies the freedom to diverge is a double-edged
sword: while necessary for discovering high-reward shortcuts, it also creates a
powerful incentive to game the simplest aspects of the reward function, posing
a critical challenge for reward hacking under alignment.

</details>


### [3] [Language Ranker: A Lightweight Ranking framework for LLM Decoding](https://arxiv.org/abs/2510.21883)
*Chenheng Zhang,Tianqi Du,Jizhe Zhang,Mingqing Xiao,Yifei Wang,Yisen Wang,Zhouchen Lin*

Main category: cs.CL

TL;DR: 提出Language Ranker框架，将LLM解码过程类比推荐系统重排序阶段，通过轻量级模块重排候选响应，在性能媲美大规模奖励模型的同时显著降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 传统LLM研究主要关注输出分布优化，而忽视了将分布转化为最终响应的解码过程。现有解码方法存在高计算成本和有限适用性问题，且传统解码方法和奖励模型都存在冗余等明显局限性。

Method: 从推荐系统视角重新审视LLM生成，将解码过程类比推荐管道的排序阶段。提出Language Ranker框架，引入轻量级模块使用基础模型提取的特征对候选响应进行重排序。

Result: 在广泛任务上的实验表明，Language Ranker仅需<0.5M额外参数即可达到与大规模奖励模型相当的性能，显著降低了训练和推理阶段的计算开销。

Conclusion: 该方法展示了高效性和有效性，有潜力充分释放LLM的能力，为解码过程提供了更优的解决方案。

Abstract: Conventional research on large language models (LLMs) has primarily focused
on refining output distributions, while paying less attention to the decoding
process that transforms these distributions into final responses. Recent
advances, such as scaling the computation of inference time with reward models,
have underscored the importance of decoding, but these methods often suffer
from high computational costs and limited applicability. In this paper, we
revisit LLM generation through the lens of recommender systems, conceptualizing
the decoding process as analogous to the ranking stage in recommendation
pipelines. From this perspective, we observe that both traditional decoding
methods and reward models exhibit clear limitations such as redundancy.
Motivated by this insight, we propose Language Ranker, a novel framework that
introduces a lightweight module to rerank candidate responses using features
extracted by the base model. Experiments across a wide range of tasks show that
Language Ranker achieves performance comparable to large-scale reward models,
while requiring only <0.5M additional parameters, significantly reducing the
computational overhead during both training and inference stages. This
highlights the efficiency and effectiveness of our method, showcasing its
potential to fully unlock the capabilities of LLMs.

</details>


### [4] [Framework for Machine Evaluation of Reasoning Completeness in Large Language Models For Classification Tasks](https://arxiv.org/abs/2510.21884)
*Avinash Patil*

Main category: cs.CL

TL;DR: RACE框架评估LLM生成解释与逻辑回归特征重要性之间的对齐程度，发现正确预测覆盖更多支持特征，错误预测覆盖更多矛盾特征。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习在敏感领域的应用增加，需要透明可解释的AI。虽然LLM能生成自然语言解释，但这些解释是否真实反映决策依据尚不明确。

Method: 提出RACE框架，在四个文本分类数据集上比较LLM解释与逻辑回归的特征重要性，使用token匹配、精确字符串匹配和编辑距离匹配三种粒度对齐方法。

Result: 实证结果显示一致的不对称性：正确预测覆盖更多支持特征，错误预测覆盖更多矛盾特征。编辑距离匹配发现释义重叠，提高覆盖率但保持不对称性。

Conclusion: LLM解释结合了表面级和灵活的证据重用，但在错误情况下也会放大误导线索。RACE为评估LLM解释的忠实性提供了量化基础。

Abstract: The growing adoption of machine learning (ML) in sensitive domains has
heightened the demand for transparent and interpretable artificial
intelligence. Large Language Models (LLMs) are increasingly capable of
producing natural language explanations, yet it remains unclear whether these
rationales faithfully capture the predictive signals that underlie decisions.
This paper introduces RACE-Reasoning Alignment for Completeness of
Explanations, a systematic framework to evaluate the alignment between
LLM-generated explanations and interpretable feature importance scores derived
from a logistic regression baseline. We analyze four widely used text
classification datasets-WIKI ONTOLOGY, AG NEWS, IMDB, and GOEMOTIONS-and
compare LLM rationales against top-ranked supporting and contradicting lexical
features. To capture alignment at multiple levels of granularity, RACE
implements token-aware, exact string, and edit-distance matching techniques.
Empirical results reveal a consistent asymmetry: correct predictions exhibit
higher coverage of supporting features, while incorrect predictions are
associated with elevated coverage of contradicting features. Edit-distance
matching further uncovers paraphrastic overlaps, boosting coverage while
preserving this asymmetry. These findings demonstrate that LLM rationales
combine both surface-level and flexible evidence reuse, yet can also amplify
misleading cues in error cases. RACE provides new insights into the
faithfulness of LLM explanations and establishes a quantitative basis for
evaluating reasoning completeness in neural language models.

</details>


### [5] [Preventing Catastrophic Forgetting: Behavior-Aware Sampling for Safer Language Model Fine-Tuning](https://arxiv.org/abs/2510.21885)
*Anh Pham,Mihir Thalanki,Michael Sun,Aditya Chaloo,Ankita Gupta,Tian Xia,Aditya Mate,Ehimwenma Nosakhare,Soundararajan Srinivasan*

Main category: cs.CL

TL;DR: 提出行为感知采样框架，通过基于指令-响应行为和语义多样性的安全示例选择，有效缓解大语言模型在微调时的安全行为遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在良性数据微调时经常出现安全行为遗忘现象，现有方法使用随机安全示例效果有限，需要更有效的数据选择策略。

Method: 提出行为感知采样框架，基于指令-响应行为（拒绝vs服从）和跨伤害类别的语义多样性来选择安全示例。

Result: 该方法显著减少有害输出同时保持帮助性，仅使用0.5%额外训练数据即可实现高达41%的有害性降低。

Conclusion: 定向数据选择能够提高大规模微调的安全性和效率。

Abstract: Large language models often lose previously aligned safety behaviors when
fine-tuned on benign data, a phenomenon known as catastrophic forgetting. Prior
work shows that adding random safety examples can mitigate this effect, but it
remains unclear which examples are most effective. We propose a behavior-aware
sampling framework that selects safety examples based on two complementary
factors: instruction-response behavior (e.g., refusal versus compliance) and
semantic diversity across harm categories. Systematic evaluation shows that
this approach substantially reduces harmful outputs while maintaining
helpfulness, achieving up to a 41% reduction in harmfulness with only 0.5%
additional training data. These results highlight how targeted data selection
can improve the safety and efficiency of fine-tuning at scale.

</details>


### [6] [Embedding Trust: Semantic Isotropy Predicts Nonfactuality in Long-Form Text Generation](https://arxiv.org/abs/2510.21891)
*Dhrupad Bhardwaj,Julia Kempe,Tim G. J. Rudner*

Main category: cs.CL

TL;DR: 提出基于语义各向同性（语义嵌入在单位球面上的均匀度）来评估LLM长文本响应的可信度，无需标注数据、微调或超参数选择


<details>
  <summary>Details</summary>
Motivation: 在需要准确回答开放问题的关键应用领域部署LLM时，需要可靠且计算成本低的方法来评估长文本响应的可信度，现有基于逐项事实核查的方法计算昂贵且脆弱

Method: 生成多个长文本响应，将其嵌入到单位球面上，通过计算嵌入向量的角度分散度来估计语义各向同性水平

Result: 发现更高的语义各向同性（即更大的嵌入分散度）可靠地表明样本间的事实一致性较低，该方法在多个领域始终优于现有方法

Conclusion: 该方法为将可信度评估集成到实际LLM工作流程中提供了一种实用、低成本的解决方案

Abstract: To deploy large language models (LLMs) in high-stakes application domains
that require substantively accurate responses to open-ended prompts, we need
reliable, computationally inexpensive methods that assess the trustworthiness
of long-form responses generated by LLMs. However, existing approaches often
rely on claim-by-claim fact-checking, which is computationally expensive and
brittle in long-form responses to open-ended prompts. In this work, we
introduce semantic isotropy -- the degree of uniformity across normalized text
embeddings on the unit sphere -- and use it to assess the trustworthiness of
long-form responses generated by LLMs. To do so, we generate several long-form
responses, embed them, and estimate the level of semantic isotropy of these
responses as the angular dispersion of the embeddings on the unit sphere. We
find that higher semantic isotropy -- that is, greater embedding dispersion --
reliably signals lower factual consistency across samples. Our approach
requires no labeled data, no fine-tuning, and no hyperparameter selection, and
can be used with open- or closed-weight embedding models. Across multiple
domains, our method consistently outperforms existing approaches in predicting
nonfactuality in long-form responses using only a handful of samples --
offering a practical, low-cost approach for integrating trust assessment into
real-world LLM workflows.

</details>


### [7] [Understanding Network Behaviors through Natural Language Question-Answering](https://arxiv.org/abs/2510.21894)
*Mingzhe Xing,Chang Tian,Jianan Zhang,Lichen Pan,Peipei Liu,Zhaoteng Yan,Yinliang Yue*

Main category: cs.CL

TL;DR: NetMind是一个使用自然语言查询网络的框架，通过树形配置分块、统一事实图和混合命令式-声明式语言来解决LLM在网络配置分析中的挑战。


<details>
  <summary>Details</summary>
Motivation: 现代大规模网络配置复杂，传统基于领域特定语言的方法学习曲线陡峭且灵活性有限，而自然语言界面更易访问和解释。

Method: 提出树形配置分块策略保持语义连贯性，构建统一事实图标准化厂商特定配置，设计混合命令式-声明式语言减轻LLM推理负担。

Result: 实验表明NetMind实现了准确且可扩展的网络行为理解，优于现有基线方法。

Conclusion: NetMind框架有效解决了LLM在网络配置分析中的长上下文理解、异构性和复杂推理挑战，为自然语言网络查询提供了可行方案。

Abstract: Modern large-scale networks introduce significant complexity in understanding
network behaviors, increasing the risk of misconfiguration. Prior work proposed
to understand network behaviors by mining network configurations, typically
relying on domain-specific languages interfaced with formal models. While
effective, they suffer from a steep learning curve and limited flexibility. In
contrast, natural language (NL) offers a more accessible and interpretable
interface, motivating recent research on NL-guided network behavior
understanding. Recent advances in large language models (LLMs) further enhance
this direction, leveraging their extensive prior knowledge of network concepts
and strong reasoning capabilities. However, three key challenges remain: 1)
numerous router devices with lengthy configuration files challenge LLM's
long-context understanding ability; 2) heterogeneity across devices and
protocols impedes scalability; and 3) complex network topologies and protocols
demand advanced reasoning abilities beyond the current capabilities of LLMs. To
tackle the above challenges, we propose NetMind, a novel framework for querying
networks using NL. Our approach introduces a tree-based configuration chunking
strategy to preserve semantic coherence while enabling efficient partitioning.
We then construct a unified fact graph as an intermediate representation to
normalize vendor-specific configurations. Finally, we design a hybrid
imperative-declarative language to reduce the reasoning burden on LLMs and
enhance precision. We contribute a benchmark consisting of NL question-answer
pairs paired with network configurations. Experiments demonstrate that NetMind
achieves accurate and scalable network behavior understanding, outperforming
existing baselines.

</details>


### [8] [Deep Literature Survey Automation with an Iterative Workflow](https://arxiv.org/abs/2510.21900)
*Hongbo Zhang,Han Cui,Yidong Wang,Yijian Tian,Qi Guo,Cunxiang Wang,Jian Wu,Chiyu Song,Yue Zhang*

Main category: cs.CL

TL;DR: 提出IterSurvey框架，通过递归大纲生成和逐步检索论文的方法，解决传统一次性检索导致的噪声、结构碎片化和上下文过载问题，显著提升文献综述质量。


<details>
  <summary>Details</summary>
Motivation: 现有自动文献综述系统采用一次性检索和静态大纲生成，导致检索噪声大、结构碎片化、上下文过载，限制了综述质量。受人类研究者迭代阅读过程启发，需要更智能的框架。

Method: 基于递归大纲生成框架，规划代理逐步检索、阅读和更新大纲；设计论文卡片提炼每篇论文的贡献、方法和发现；引入审查-精炼循环和可视化增强来改善文本流并整合多模态元素。

Result: 在已建立和新兴主题上的实验表明，在内容覆盖度、结构连贯性和引用质量方面显著优于现有基线方法，生成更易访问和组织更好的综述。

Conclusion: IterSurvey框架通过迭代方法有效提升了自动文献综述的质量，同时引入Survey-Arena评估基准，为机器生成与人类撰写综述的比较提供了更可靠的评估方法。

Abstract: Automatic literature survey generation has attracted increasing attention,
yet most existing systems follow a one-shot paradigm, where a large set of
papers is retrieved at once and a static outline is generated before drafting.
This design often leads to noisy retrieval, fragmented structures, and context
overload, ultimately limiting survey quality. Inspired by the iterative reading
process of human researchers, we propose \ours, a framework based on recurrent
outline generation, in which a planning agent incrementally retrieves, reads,
and updates the outline to ensure both exploration and coherence. To provide
faithful paper-level grounding, we design paper cards that distill each paper
into its contributions, methods, and findings, and introduce a
review-and-refine loop with visualization enhancement to improve textual flow
and integrate multimodal elements such as figures and tables. Experiments on
both established and emerging topics show that \ours\ substantially outperforms
state-of-the-art baselines in content coverage, structural coherence, and
citation quality, while producing more accessible and better-organized surveys.
To provide a more reliable assessment of such improvements, we further
introduce Survey-Arena, a pairwise benchmark that complements absolute scoring
and more clearly positions machine-generated surveys relative to human-written
ones. The code is available at
https://github.com/HancCui/IterSurvey\_Autosurveyv2.

</details>


### [9] [Explaining and Mitigating Crosslingual Tokenizer Inequities](https://arxiv.org/abs/2510.21909)
*Catherine Arnett,Tyler A. Chang,Stella Biderman,Benjamin K. Bergen*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The number of tokens it takes to encode parallel text in different languages
is known to vary. These disparities are called token premiums. Having high
token premiums leads to less throughput during training and increases costs at
inference. In this paper, we show that even after controlling for dataset size,
vocabulary size, and data content, monolingual tokenizers exhibit a wide range
of token premiums across languages. To understand the cross-linguistic
differences that cause these token premiums, we train a suite of approximately
7,000 comparable monolingual tokenizers for 97 languages, manipulating
tokenization algorithm, vocabulary size, and dataset size. We measure token
premiums and test for a relationship between factors such as data similarity
(between tokenizer training and evaluation), vocabulary size, and
pre-tokenization. We also investigate the role of language-specific features
such as writing system and word length. We find that similarity between
training and test data does not impact token premiums, but vocabulary size and
pre-tokenization do. While simply increasing vocabulary size does not lead to
reduced token premium effects, we can determine an ``optimal'' vocabulary size
for each language to achieve significantly reduced token premium effects. We
also train superword tokenizers which allow merges over whitespaces, and we
find that they both reduce token premium effects and improve compression
overall. Thus, intervening on the vocabulary size or the pre-tokenizer
significantly reduces crosslingual token premium effects.

</details>


### [10] [Model-Aware Tokenizer Transfer](https://arxiv.org/abs/2510.21954)
*Mykola Haltiuk,Aleksander Smywiński-Pohl*

Main category: cs.CL

TL;DR: MATT是一种模型感知的分词器迁移方法，通过注意力影响建模将源模型的token间通信模式蒸馏到使用新分词器的目标模型中，相比仅关注嵌入相似性的启发式方法，能更有效地实现多语言LLM的分词器迁移。


<details>
  <summary>Details</summary>
Motivation: 现有分词器迁移方法主要依赖语义启发式来初始化新嵌入，忽略了更高层的模型动态，限制了迁移质量。需要一种能结合模型内部动态的分词器迁移方法。

Method: 提出模型感知分词器迁移(MATT)，引入注意力影响建模(AIM)目标，将源模型的token间通信模式蒸馏到使用新分词器的目标模型中，为标准的语言建模提供高效预热。

Result: 在多样化语言设置下的实验表明，MATT在几个GPU小时内就能恢复原始模型的大部分性能，优于启发式基线方法。

Conclusion: 结合模型级信号为多语言LLM中的鲁棒分词器迁移提供了一条实用且有效的路径。

Abstract: Large Language Models (LLMs) are trained to support an increasing number of
languages, yet their predefined tokenizers remain a bottleneck for adapting
models to lower-resource or distinct-script languages. Existing tokenizer
transfer methods typically rely on semantic heuristics to initialize new
embeddings, ignoring higher-layer model dynamics and limiting transfer quality.
We propose Model-Aware Tokenizer Transfer (MATT), a method that incorporates
model internals into the tokenizer transfer process. MATT introduces an
Attention Influence Modeling (AIM) objective that distills inter-token
communication patterns from a source model into a target model with a new
tokenizer, providing an efficient warm-up before standard language modeling.
Unlike approaches that focus solely on embedding similarity, MATT leverages
attention behavior to guide embedding initialization and adaptation.
Experiments across diverse linguistic settings show that MATT recovers a large
fraction of the original model's performance within a few GPU hours,
outperforming heuristic baselines. These results demonstrate that incorporating
model-level signals offers a practical and effective path toward robust
tokenizer transfer in multilingual LLMs.

</details>


### [11] [A Stylometric Application of Large Language Models](https://arxiv.org/abs/2510.21958)
*Harrison F. Stropkay,Jiayi Chen,Mohammad J. Latifi,Daniel N. Rockmore,Jeremy R. Manning*

Main category: cs.CL

TL;DR: 使用GPT-2模型通过训练特定作者的作品来识别作者写作风格，能够准确区分不同作者的文本，并成功验证了R. P. Thompson对《奥兹国》系列第15本书的作者身份。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型是否能够捕捉和识别不同作者的独特写作风格，为作者身份识别提供新的技术方法。

Method: 为每位作者单独训练GPT-2模型，通过比较模型对保留文本的预测准确性来识别作者风格。

Result: 实验证明，针对特定作者训练的模型能够更准确地预测该作者的保留文本，成功确认了R. P. Thompson对《奥兹国》第15本书的作者身份。

Conclusion: 大型语言模型能够有效体现作者的独特写作风格，为作者身份识别提供了可靠的技术手段。

Abstract: We show that large language models (LLMs) can be used to distinguish the
writings of different authors. Specifically, an individual GPT-2 model, trained
from scratch on the works of one author, will predict held-out text from that
author more accurately than held-out text from other authors. We suggest that,
in this way, a model trained on one author's works embodies the unique writing
style of that author. We first demonstrate our approach on books written by
eight different (known) authors. We also use this approach to confirm R. P.
Thompson's authorship of the well-studied 15th book of the Oz series,
originally attributed to F. L. Baum.

</details>


### [12] [Uncovering the Persuasive Fingerprint of LLMs in Jailbreaking Attacks](https://arxiv.org/abs/2510.21983)
*Havva Alizadeh Noughabi,Julien Serbanescu,Fattane Zarrinkalam,Ali Dehghantanha*

Main category: cs.CL

TL;DR: 该论文研究发现，利用社会科学中的说服理论构建的对抗性提示能够有效绕过LLM的对齐保护机制，诱导有害输出。


<details>
  <summary>Details</summary>
Motivation: 现有研究很少关注影响LLM对越狱攻击敏感性的语言和心理机制，本文旨在探索利用跨学科的说服理论来增强对抗性提示的有效性。

Method: 基于社会科学中成熟的说服策略，构建具有说服结构的对抗性提示，并在多个对齐LLM上进行实证评估。

Result: 实验表明，具有说服意识的提示能够显著绕过安全防护措施，成功诱导越狱行为。

Conclusion: 这项工作强调了跨学科洞察在应对LLM安全挑战中的重要性，说服理论为理解和防御越狱攻击提供了新视角。

Abstract: Despite recent advances, Large Language Models remain vulnerable to jailbreak
attacks that bypass alignment safeguards and elicit harmful outputs. While
prior research has proposed various attack strategies differing in human
readability and transferability, little attention has been paid to the
linguistic and psychological mechanisms that may influence a model's
susceptibility to such attacks. In this paper, we examine an interdisciplinary
line of research that leverages foundational theories of persuasion from the
social sciences to craft adversarial prompts capable of circumventing alignment
constraints in LLMs. Drawing on well-established persuasive strategies, we
hypothesize that LLMs, having been trained on large-scale human-generated text,
may respond more compliantly to prompts with persuasive structures.
Furthermore, we investigate whether LLMs themselves exhibit distinct persuasive
fingerprints that emerge in their jailbreak responses. Empirical evaluations
across multiple aligned LLMs reveal that persuasion-aware prompts significantly
bypass safeguards, demonstrating their potential to induce jailbreak behaviors.
This work underscores the importance of cross-disciplinary insight in
addressing the evolving challenges of LLM safety. The code and data are
available.

</details>


### [13] [Toward Understanding the Transferability of Adversarial Suffixes in Large Language Models](https://arxiv.org/abs/2510.22014)
*Sarah Ball,Niki Hasrati,Alexander Robey,Avi Schwarzschild,Frauke Kreuter,Zico Kolter,Andrej Risteski*

Main category: cs.CL

TL;DR: 本文分析了离散优化越狱攻击在大型语言模型中的可转移性，识别了三个与转移成功强相关的统计属性，并发现语义相似性仅弱相关。


<details>
  <summary>Details</summary>
Motivation: 尽管越狱攻击后缀的可转移性已被经验证实，但缺乏对其发生时机和原因的严格分析，本文旨在填补这一空白。

Method: 通过大量实验设置，识别与转移成功强相关的三个统计属性：无后缀提示激活拒绝方向的程度、后缀诱导远离拒绝方向的强度，以及正交方向上的偏移大小。

Result: 发现三个统计属性与转移成功强相关，而提示语义相似性仅弱相关。通过干预实验展示了统计分析如何转化为攻击成功的实际改进。

Conclusion: 研究提供了对可转移性的更细致理解，并证明统计分析可以指导实践中的攻击改进。

Abstract: Discrete optimization-based jailbreaking attacks on large language models aim
to generate short, nonsensical suffixes that, when appended onto input prompts,
elicit disallowed content. Notably, these suffixes are often transferable --
succeeding on prompts and models for which they were never optimized. And yet,
despite the fact that transferability is surprising and empirically
well-established, the field lacks a rigorous analysis of when and why transfer
occurs. To fill this gap, we identify three statistical properties that
strongly correlate with transfer success across numerous experimental settings:
(1) how much a prompt without a suffix activates a model's internal refusal
direction, (2) how strongly a suffix induces a push away from this direction,
and (3) how large these shifts are in directions orthogonal to refusal. On the
other hand, we find that prompt semantic similarity only weakly correlates with
transfer success. These findings lead to a more fine-grained understanding of
transferability, which we use in interventional experiments to showcase how our
statistical analysis can translate into practical improvements in attack
success.

</details>


### [14] [Penalizing Length: Uncovering Systematic Bias in Quality Estimation Metrics](https://arxiv.org/abs/2510.22028)
*Yilin Zhang,Wenda Xu,Zhongtao Liu,Tetsuji Nakagawa,Markus Freitag*

Main category: cs.CL

TL;DR: 该论文系统研究了机器翻译质量评估(QE)指标中的长度偏差问题，发现QE指标会随着翻译长度增加而过度预测错误，并偏好较短翻译，提出了长度归一化和参考文本两种缓解策略。


<details>
  <summary>Details</summary>
Motivation: 质量评估(QE)指标在机器翻译中至关重要，但长度偏差的普遍性和影响尚未得到充分探索，这可能导致对较长正确翻译的不公平惩罚和次优决策。

Method: 通过对10种不同语言对中表现最佳的回归基和LLM-as-a-Judge QE指标进行系统性研究，识别出两种关键长度偏差，并提出长度归一化训练和评估时加入参考文本两种缓解策略。

Result: 研究发现QE指标会随着翻译长度增加而过度预测错误（即使对于高质量无错误文本），并且在多个候选翻译中偏好较短翻译。提出的两种策略都能有效减少长度偏差。

Conclusion: QE指标存在显著的长度偏差问题，可能影响翻译评估的公平性。通过长度归一化和参考文本策略可以有效缓解这一问题，提高QE指标的可靠性。

Abstract: Quality Estimation (QE) metrics are vital in machine translation for
reference-free evaluation and as a reward signal in tasks like reinforcement
learning. However, the prevalence and impact of length bias in QE have been
underexplored. Through a systematic study of top-performing regression-based
and LLM-as-a-Judge QE metrics across 10 diverse language pairs, we reveal two
critical length biases: First, QE metrics consistently over-predict errors with
increasing translation length, even for high-quality, error-free texts. Second,
they exhibit a preference for shorter translations when multiple candidates are
available for the same source text. These inherent length biases risk unfairly
penalizing longer, correct translations and can lead to sub-optimal
decision-making in applications such as QE reranking and QE guided
reinforcement learning. To mitigate this, we propose two strategies: (a)
applying length normalization during model training, and (b) incorporating
reference texts during evaluation. Both approaches were found to effectively
reduce the identified length bias.

</details>


### [15] [ATLAS: Adaptive Transfer Scaling Laws for Multilingual Pretraining, Finetuning, and Decoding the Curse of Multilinguality](https://arxiv.org/abs/2510.22037)
*Shayne Longpre,Sneha Kudugunta,Niklas Muennighoff,I-Hung Hsu,Isaac Caswell,Alex Pentland,Sercan Arik,Chen-Yu Lee,Sayna Ebrahimi*

Main category: cs.CL

TL;DR: 本研究进行了迄今为止最大的多语言扩展定律研究，包含774个多语言训练实验，涵盖10M-8B参数、400+训练语言和48个评估语言，提出了优于现有扩展定律的ATLAS模型。


<details>
  <summary>Details</summary>
Motivation: 现有扩展定律研究主要集中于英语，但最先进的AI模型服务于数十亿国际用户，需要理解多语言环境下的扩展规律。

Method: 通过774个多语言训练实验，引入自适应迁移扩展定律(ATLAS)，分析跨语言迁移矩阵、语言无关扩展定律以及计算交叉点。

Result: ATLAS在样本外泛化方面比现有扩展定律平均提高0.3 R²；推导出38×38=1444语言对的互惠得分矩阵；识别了从头训练与微调多语言检查点的计算交叉点。

Conclusion: 研究结果为跨语言扩展定律的民主化提供了科学基础，使从业者能够高效扩展超越英语优先AI的模型。

Abstract: Scaling laws research has focused overwhelmingly on English -- yet the most
prominent AI models explicitly serve billions of international users. In this
work, we undertake the largest multilingual scaling laws study to date,
totaling 774 multilingual training experiments, spanning 10M-8B model
parameters, 400+ training languages and 48 evaluation languages. We introduce
the Adaptive Transfer Scaling Law (ATLAS) for both monolingual and multilingual
pretraining, which outperforms existing scaling laws' out-of-sample
generalization often by more than 0.3 R^2. Our analyses of the experiments shed
light on multilingual learning dynamics, transfer properties between languages,
and the curse of multilinguality. First, we derive a cross-lingual transfer
matrix, empirically measuring mutual benefit scores between 38 x 38=1444
language pairs. Second, we derive a language-agnostic scaling law that reveals
how to optimally scale model size and data when adding languages without
sacrificing performance. Third, we identify the computational crossover points
for when to pretrain from scratch versus finetune from multilingual
checkpoints. We hope these findings provide the scientific foundation for
democratizing scaling laws across languages, and enable practitioners to
efficiently scale models -- beyond English-first AI.

</details>


### [16] [Emotions Where Art Thou: Understanding and Characterizing the Emotional Latent Space of Large Language Models](https://arxiv.org/abs/2510.22042)
*Benjamin Reichman,Adar Avsian,Larry Heck*

Main category: cs.CL

TL;DR: 该研究通过分析大语言模型隐藏状态空间的几何结构，揭示了其内部情感表示的低维情感流形，发现情感表征具有方向性编码、跨层分布的特点，并能通过干预模块进行操控。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型如何内部表示情感，理解其情感处理机制。

Method: 分析LLM隐藏状态空间的几何结构，识别低维情感流形，研究情感表征的方向性编码和跨层分布特性，开发干预模块来操控内部情感感知。

Result: 在八个真实世界情感数据集（涵盖五种语言）中发现稳定的情感结构，跨领域对齐误差低且线性探测性能强，表明存在通用的情感子空间。干预模块能有效操控情感感知而不影响语义。

Conclusion: LLMs中存在一致且可操控的情感几何结构，这为理解模型如何内化和处理情感提供了重要见解。

Abstract: This work investigates how large language models (LLMs) internally represent
emotion by analyzing the geometry of their hidden-state space. The paper
identifies a low-dimensional emotional manifold and shows that emotional
representations are directionally encoded, distributed across layers, and
aligned with interpretable dimensions. These structures are stable across depth
and generalize to eight real-world emotion datasets spanning five languages.
Cross-domain alignment yields low error and strong linear probe performance,
indicating a universal emotional subspace. Within this space, internal emotion
perception can be steered while preserving semantics using a learned
intervention module, with especially strong control for basic emotions across
languages. These findings reveal a consistent and manipulable affective
geometry in LLMs and offer insight into how they internalize and process
emotion.

</details>


### [17] [Compositional Bias Control in Large Language Models: Preference Learning Fails, Supervision Succeeds](https://arxiv.org/abs/2510.22084)
*Atij Mahesh*

Main category: cs.CL

TL;DR: 该论文比较了六种减少LLM性别偏见的技术，发现监督微调(SFT)在约束合规性和词汇多样性方面表现最佳，而基于偏好的方法(如DPO)无法满足组合约束要求。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型即使在职业中性语境中仍会产生性别刻板印象语言，反映了深层社会偏见。现有偏见缓解方法的比较效果和学习动态尚不清楚。

Method: 对六种偏见缓解控制技术进行比较分析：仅提示、生成后过滤、DFA-based Ctrl-G解码、监督微调(SFT)、直接偏好优化(DPO)和迭代零空间投影(INLP)。在组合约束任务上评估这些方法，要求为20个Winogender衍生职业生成包含至少一个能动性和一个公共性描述符的句子。

Result: SFT达到99.87±0.15%的合规性和高词汇多样性；DPO尽管训练稳定性相似，但合规率仅为4.53±0.82%；Ctrl-G保证完美合规性，但以严重降低流畅性和多样性为代价。基于偏好的学习无法满足组合约束。

Conclusion: 只有明确的正面监督才能缓解组合偏见；基于偏好的对齐无法泛化逻辑结构，凸显了偏好学习的局限性以及明确监督对于公平流畅受控生成的必要性。

Abstract: Large Language Models (LLMs) still produce gender-stereotyped language even
in occupation-neutral contexts that reflect deep societal biases (Rudinger et
al., 2018). To address this, prior work has proposed prompting, constrained
decoding (Dathathri et al., 2020; Zhou et al., 2024), post-processing, and
fine-tuning-based alignment (Rafailov et al., 2023; Ravfogel et al., 2022).
However, the comparative efficacy and learning dynamics remain little
understood. We report a comparative analysis of six control techniques for bias
mitigation: prompt-only, generate-and-filter, DFA-based Ctrl-G decoding,
Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and
Iterative Nullspace Projection (INLP). We evaluate each method on a
compositional constraint task. This task requires generating sentences that
contain at least one agentic and one communal descriptor for each of the twenty
Winogender-derived occupations. We quantify trade-offs between control strength
and naturalness with evaluations of constraint compliance, lexical diversity,
and fluency. Our results reveal key contrasts among the methods: SFT achieves
99.87 +- 0.15% compliance and high lexical diversity, while DPO, despite
similar training stability, fails at 4.53 +- 0.82%. Ctrl-G guarantees perfect
compliance, but at the cost of severely reduced fluency and diversity.
Preference-based learning fundamentally differs: it cannot satisfy
compositional constraints, as binary preference signals encode ranking, not
logical conjunctions. Only explicit positive supervision enables mitigation of
compositional biases; preference-based alignment fails to generalize logical
structures, underscoring the limitations of preference learning and the
necessity of explicit supervision for fair and fluent controlled generation.

</details>


### [18] [Generalization or Memorization: Dynamic Decoding for Mode Steering](https://arxiv.org/abs/2510.22099)
*Xuanming Zhang*

Main category: cs.CL

TL;DR: 提出了一个统一框架来理解和控制LLMs的泛化与记忆模式，基于信息瓶颈理论开发了动态模式引导算法，通过因果线性探针和激活引导来提升模型可靠性。


<details>
  <summary>Details</summary>
Motivation: LLMs存在泛化能力和逐字记忆的双重特性，这种不可预测性影响了其在关键应用中的可靠性，需要系统方法来识别和控制这两种推理模式。

Method: 基于信息瓶颈理论建立理论模型，开发动态模式引导算法，包括轻量级因果线性探针识别记忆依赖，以及动态激活引导机制将计算推向泛化电路。

Result: 在推理和忠实性任务上的实验表明，DMS显著提高了逻辑一致性和事实准确性。

Conclusion: DMS为增强LLM可靠性提供了一种原则性方法，通过自适应自对比解码实现了对泛化和记忆模式的有效控制。

Abstract: Large Language Models (LLMs) exhibit a troubling duality, capable of both
remarkable generalization and brittle, verbatim memorization of their training
data. This unpredictability undermines their reliability in high-stakes
applications. In this work, we propose a unified framework to understand,
identify, and control these distinct reasoning modes. First, we introduce a
theoretical model based on the Information Bottleneck (IB) principle,
formalizing generalization as the learning of a compressed, task-relevant
representation and memorization as a failure to compress. Building on this
theory, we develop Dynamic Mode Steering (DMS), a novel inference-time
algorithm which comprises two components: (1) a lightweight, causally-grounded
linear probe that identifies the model's instantaneous reliance on
memorization, and (2) a dynamic activation steering mechanism that nudges the
model's computation towards pre-identified generalization circuits. We frame
DMS as a form of adaptive, self-contrastive decoding. Experiments on reasoning
and faithfulness tasks demonstrate that DMS significantly improves logical
consistency and factual accuracy, thereby offering a principled approach to
enhancing LLM reliability.

</details>


### [19] [Gradual Forgetting: Logarithmic Compression for Extending Transformer Context Windows](https://arxiv.org/abs/2510.22109)
*Billy Dickson,Zoran Tiganj*

Main category: cs.CL

TL;DR: 本文提出了一种通过输入令牌的对数压缩来扩展transformer长程记忆的方法，而不是修改transformer架构本身。


<details>
  <summary>Details</summary>
Motivation: 大多数长上下文处理方法通过集成循环或辅助内存模块来增加transformer内部架构的复杂性，本文旨在提供一种更简单的替代方案。

Method: 受人类记忆认知模型启发，对输入令牌应用尺度不变的对数压缩，然后由标准未修改的transformer处理压缩后的表示。

Result: 在WikiText-103和PG-19语言建模基准测试中，相比未压缩基线，困惑度有所降低，且随着压缩时间上下文变长，性能持续提升。

Conclusion: 输入级对数压缩是扩展transformer长程记忆的简单有效方法，保持了架构的简洁性。

Abstract: Most approaches to long-context processing increase the complexity of the
transformer's internal architecture by integrating mechanisms such as
recurrence or auxiliary memory modules. In this work, we introduce an
alternative approach that modifies the input representation itself, rather than
the transformer architecture. Inspired by cognitive models of human memory, our
method applies a scale-invariant logarithmic compression to the input tokens.
The resulting compressed representation is processed by a standard, unmodified
transformer, preserving architectural simplicity. We evaluate this approach on
the WikiText-103 and PG-19 language modeling benchmarks, showing a reduction in
perplexity compared to uncompressed baselines. Moreover, performance improves
consistently with longer compressed temporal contexts, showing that input-level
logarithmic compression is a simple and effective way to extend a transformer's
long-range memory.

</details>


### [20] [Every Activation Boosted: Scaling General Reasoner to 1 Trillion Open Language Foundation](https://arxiv.org/abs/2510.22115)
*Ling-Team,Ang Li,Ben Liu,Binbin Hu,Bing Li,Bingwei Zeng,Borui Ye,Caizhi Tang,Changxin Tian,Chao Huang,Chao Zhang,Chen Qian,Chenchen Ju,Chenchen Li,Chengfu Tang,Chili Fu,Chunshao Ren,Chunwei Wu,Cong Zhang,Cunyin Peng,Dafeng Xu,Daixin Wang,Dalong Zhang,Dingnan Jin,Dingyuan Zhu,Dongke Hu,Fangzheng Zhao,Feifan Wu,Feng Zhu,Gangshan Wang,Haitao Zhang,Hailin Zhao,Hanxiao Zhang,Hanzi Wang,Hao Qian,Haoyi Yu,Heng Zhang,Hongliang Zhang,Hongzhi Luan,Huirong Dong,Huizhong Li,Jia Li,Jia Liu,Jialong Zhu,Jian Sha,Jianping Wei,Jiaolong Yang,Jieyue Ma,Jiewei Wu,Jinjing Huang,Jingyun Tian,Jingyuan Zhang,Jinquan Sun,Juanhui Tu,Jun Liu,Jun Xu,Jun Zhou,Junjie Ou,Junpeng Fang,Kaihong Zhang,Kaiqin Hu,Ke Shi,Kun Tang,Kunlong Chen,Lanyin Mei,Lei Liang,Lei Xu,Libo Zhang,Lin Ju,Lin Yuan,Ling Zhong,Lintao Ma,Lu Liu,Lu Yu,Lun Cai,Meiqi Zhu,Mengying Li,Min Chen,Minghao Xue,Minghong Cai,Mingming Yin,Peijie Jiang,Peilong Zhao,Pingping Liu,Qian Zhao,Qing Cui,Qingxiang Huang,Qingyuan Yang,Quankun Yu,Shaowei Wei,Shijie Lian,Shoujian Zheng,Shun Song,Shungen Zhang,Shuo Zhang,Siyuan Li,Song Liu,Ting Guo,Tong Zhao,Wanli Gu,Weichang Wu,Weiguang Han,Wenjing Fang,Wubin Wang,Xiang Shu,Xiao Shi,Xiaoshun Lan,Xiaolu Zhang,Xiaqing Sun,Xin Zhao,Xingyu Lu,Xiong Xu,Xudong Wang,Xudong Wang,Xuemin Yang,Yajie Yang,Yang Xiang,Yanzhe Li,Yi Zhang,Yilong Wang,Yingxue Li,Yongzhen Guo,Yuzhuo Fu,Yuanyuan Wang,Yue Yang,Yue Yu,Yufeng Deng,Yun Zhang,Yunfei Xu,Yuqi Zhang,Yuxiao He,Zengke Gui,Zhaoxin Huan,Zhaoyang Wang,Zhibo Zhu,Zhihao Wang,Zhiqiang Zhang,Zhoufei Wang,Zihang Zeng,Ziqi Liu,Zitao Xuan,Zuoli Tang*

Main category: cs.CL

TL;DR: Ling 2.0是一个面向推理的语言基础模型系列，采用混合专家架构，参数规模从160亿到1万亿，通过高稀疏性和协调创新实现了7倍的计算效率提升。


<details>
  <summary>Details</summary>
Motivation: 构建一个能够从数百亿扩展到万亿参数规模的统一推理导向语言基础模型，强调高稀疏性、跨尺度一致性和基于经验缩放定律的效率。

Method: 采用高稀疏性MoE架构与MTP技术、推理导向数据与训练中CoT激活、基于强化的微调方法，以及全尺度FP8训练和细粒度异构流水线。

Result: 在万亿规模上，Ling-1T建立了推理准确性与计算效率的新帕累托前沿，稀疏激活与推理目标对齐实现了可扩展且高效的人工智能。

Conclusion: Ling 2.0为推进未来推理和思维模型提供了一个连贯、开放且高效的基础，包括基于相同基础的Ring系列模型。

Abstract: We introduce Ling 2.0, a series reasoning-oriented language foundation built
upon the principle that every activation boosts reasoning capability. Designed
to scale from tens of billions to one trillion parameters under a unified
Mixture-of-Experts (MoE) paradigm, Ling 2.0 emphasizes high sparsity,
cross-scale consistency, and efficiency guided by empirical scaling laws. The
series includes three non-thinking (instruct) models - Ling-mini-2.0,
Ling-flash-2.0, and Ling-1T - ranging from 16B to 1T total parameters and
achieving up to 7-fold active-compute efficiency compared with dense
counterparts. Ling 2.0 integrates coordinated innovations across model
architecture, pre-training, post-training, and infrastructure: a high-sparsity
MoE with MTP for efficient reasoning, reasoning-oriented data and mid-training
CoT activation, reinforcement-based fine-tuning (DFT, Evo-CoT), and full-scale
FP8 training with fine-grained heterogeneous pipelines. At the trillion scale,
Ling-1T establishes a new Pareto frontier of reasoning accuracy versus
computational efficiency, demonstrating that sparse activation, when properly
aligned with reasoning objectives, enables scalable and efficient intelligence.
Collectively, Ling 2.0 provides a coherent, open, and efficient foundation for
advancing future reasoning and thinking models, including the Ring series built
upon the same base.

</details>


### [21] [OlaMind: Towards Human-Like and Hallucination-Safe Customer Service for Retrieval-Augmented Dialogue](https://arxiv.org/abs/2510.22143)
*Tianhong Gao,Jundong Shen,Bei Shi,Jiapeng Wang,Ying Ju,Junfeng Yao,Jiao Ran,Yong Zhang,Lin Dong,Huiyu Yu,Tingting Ye*

Main category: cs.CL

TL;DR: OlaMind是一个基于检索增强生成的人类化、防幻觉客服框架，通过两阶段学习显著提升响应自然度并减少幻觉，在工业级社交客服场景中取得了显著效果提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于RAG的智能客服系统容易产生幻觉和机械式响应，这会带来商业风险并损害用户体验，特别是在Web客服交互场景中。

Method: 采用两阶段方法：Learn-to-Think阶段学习人类专家的推理过程和响应策略，Learn-to-Respond阶段结合冷启动监督微调和强化学习进行基础到困难的自优化。

Result: 大规模在线A/B实验显示，在社区支持/直播互动场景中，智能解决率分别提升+28.92%/+18.42%，人工接管率分别降低-6.08%/-7.12%。

Conclusion: OlaMind框架在多样化实际应用中具有一致有效性，显著提升了人类化程度和自然度，同时有效缓解了幻觉和关键商业风险。

Abstract: Intelligent customer service (ICS) systems via retrieval-augmented generation
(RAG) have been widely adopted in Web-based domains such as social platforms
and e-commerce, achieving remarkable improvements in automation and efficiency.
However, notable limitations still remain: these systems are prone to
hallucinations and often generate rigid, mechanical responses, which can
introduce business risks and undermine user experience, especially in Web-based
customer service interactions under the RAG scenarios. In this paper, we
introduce OlaMind, a human-like and hallucination-safe customer service
framework for retrieval-augmented dialogue. Specifically, it first leverages a
Learn-to-Think stage to learn the reasoning processes and response strategies
from human experts, and then employs a Learn-to-Respond stage to perform
cold-start supervised fine-tuning (SFT) combined with reinforcement learning
(RL) for basic-to-hard self-refinement. Our method significantly enhances
human-likeness and naturalness while effectively mitigating hallucinations and
critical business risks. We have conducted large-scale online A/B experiments
in an industry-level social customer service setting, and extensive
experimental results show that OlaMind achieves significant cumulative relative
improvements with intelligent resolution rates +28.92%/+18.42% and human
takeover rate -6.08%/-7.12% in community-support/livestream-interaction
scenarios, respectively, which highlights its consistent effectiveness across
diverse real-world applications. The code and data will be publicly available.

</details>


### [22] [SentiMaithili: A Benchmark Dataset for Sentiment and Reason Generation for the Low-Resource Maithili Language](https://arxiv.org/abs/2510.22160)
*Rahul Ranjan,Mahendra Kumar Gurve,Anuj,Nitin,Yamuna Prasad*

Main category: cs.CL

TL;DR: 为低资源语言迈蒂利语构建首个可解释情感分析数据集，包含3221个带情感标签和自然语言解释的句子，填补了该语言在NLP研究中的空白。


<details>
  <summary>Details</summary>
Motivation: 迈蒂利语作为印度-雅利安语系语言，有超过1300万使用者，但在自然语言处理研究中代表性不足。低资源语言构建基准数据集面临专家稀缺、标注成本高等挑战，且现有情感分析资源缺乏细粒度标注和可解释性机制。

Method: 引入包含3221个迈蒂利语句子的新数据集，每个句子都标注了情感极性并附带自然语言解释。数据集由语言专家精心策划和验证，确保标签可靠性和上下文保真度，解释部分使用迈蒂利语编写以促进文化基础解释。

Result: 使用经典机器学习和最先进的transformer架构进行广泛实验，证明该数据集在可解释情感分析方面的有效性。

Conclusion: 这项工作为迈蒂利语建立了首个可解释情感计算的基准，为多语言NLP和可解释AI的广泛发展贡献了宝贵资源。

Abstract: Developing benchmark datasets for low-resource languages poses significant
challenges, primarily due to the limited availability of native linguistic
experts and the substantial time and cost involved in annotation. Given these
challenges, Maithili is still underrepresented in natural language processing
research. It is an Indo-Aryan language spoken by more than 13 million people in
the Purvanchal region of India, valued for its rich linguistic structure and
cultural significance. While sentiment analysis has achieved remarkable
progress in high-resource languages, resources for low-resource languages, such
as Maithili, remain scarce, often restricted to coarse-grained annotations and
lacking interpretability mechanisms. To address this limitation, we introduce a
novel dataset comprising 3,221 Maithili sentences annotated for sentiment
polarity and accompanied by natural language justifications. Moreover, the
dataset is carefully curated and validated by linguistic experts to ensure both
label reliability and contextual fidelity. Notably, the justifications are
written in Maithili, thereby promoting culturally grounded interpretation and
enhancing the explainability of sentiment models. Furthermore, extensive
experiments using both classical machine learning and state-of-the-art
transformer architectures demonstrate the dataset's effectiveness for
interpretable sentiment analysis. Ultimately, this work establishes the first
benchmark for explainable affective computing in Maithili, thus contributing a
valuable resource to the broader advancement of multilingual NLP and
explainable AI.

</details>


### [23] [DETECT: Determining Ease and Textual Clarity of German Text Simplifications](https://arxiv.org/abs/2510.22212)
*Maria Korobeynikova,Alessia Battisti,Lukas Fischer,Yingqiang Gao*

Main category: cs.CL

TL;DR: 提出了DETECT，第一个专门针对德语的自动文本简化评估指标，通过LLM生成合成质量分数，在简化性、意义保持和流畅性三个维度上全面评估ATS质量。


<details>
  <summary>Details</summary>
Motivation: 当前德语自动文本简化评估依赖通用指标如SARI、BLEU和BERTScore，这些指标无法充分捕捉简化质量。虽然英语有专门指标如LENS，但德语缺乏相应的人类标注语料库。

Method: 将LENS框架适配到德语，并扩展了：(i)通过LLM生成合成质量分数的流程，无需人工标注即可创建数据集；(ii)基于LLM的细化步骤，使评分标准与简化需求对齐。

Result: 实验结果显示，DETECT与人类判断的相关性显著高于广泛使用的ATS指标，在意义保持和流畅性方面表现尤为突出。

Conclusion: DETECT填补了德语文本简化评估的空白，证明了LLM在自动评估中的潜力和局限性，为通用语言可访问性任务提供了可转移的指导原则。

Abstract: Current evaluation of German automatic text simplification (ATS) relies on
general-purpose metrics such as SARI, BLEU, and BERTScore, which insufficiently
capture simplification quality in terms of simplicity, meaning preservation,
and fluency. While specialized metrics like LENS have been developed for
English, corresponding efforts for German have lagged behind due to the absence
of human-annotated corpora. To close this gap, we introduce DETECT, the first
German-specific metric that holistically evaluates ATS quality across all three
dimensions of simplicity, meaning preservation, and fluency, and is trained
entirely on synthetic large language model (LLM) responses. Our approach adapts
the LENS framework to German and extends it with (i) a pipeline for generating
synthetic quality scores via LLMs, enabling dataset creation without human
annotation, and (ii) an LLM-based refinement step for aligning grading criteria
with simplification requirements. To the best of our knowledge, we also
construct the largest German human evaluation dataset for text simplification
to validate our metric directly. Experimental results show that DETECT achieves
substantially higher correlations with human judgments than widely used ATS
metrics, with particularly strong gains in meaning preservation and fluency.
Beyond ATS, our findings highlight both the potential and the limitations of
LLMs for automatic evaluation and provide transferable guidelines for general
language accessibility tasks.

</details>


### [24] [Estimating the Error of Large Language Models at Pairwise Text Comparison](https://arxiv.org/abs/2510.22219)
*Tianyi Li*

Main category: cs.CL

TL;DR: 本文提出了一种无需真实标签的方法来测量LLM在成对文本比较中的输出错误率，支持均匀错误率和位置偏差两种情况，并通过Copeland计数构建文本排名来揭示LLM成对比较的可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 测量LLM在成对文本比较任务中的错误概率，无需依赖真实标签，旨在评估LLM在此类任务中的可靠性和偏差问题。

Method: 使用两种场景：(i)均匀错误率，通过每个文本对的两次比较（交换顺序）估计；(ii)二元位置偏差，假设两种比较顺序有不同的错误率，通过重复比较估计。采用Copeland计数从成对偏好构建文本排名。

Result: 对六个LLM（ChatGPT、Claude、DeepSeek、Gemini、Grok、Qwen）在五种文本类型上的测试显示，测量的两个位置偏差项相似，接近均匀错误。Claude在此实验中表现最佳，综合考虑错误率和提示变化的鲁棒性。

Conclusion: 该方法在指示LLM在此任务中的错误方面优于有偏Bradley-Terry模型和交换性评分，揭示了LLM基于成对比较的可扩展性差的问题。

Abstract: We measure LLMs' output error at pairwise text comparison, noting the
probability of error in their preferences. Our method does not rely on the
ground truth and supports two scenarios: (i) uniform error rate regardless of
the order of comparison, estimated with two comparisons for each text pair with
either text placed first; (ii) binary positional bias assuming distinct error
rates for the two orders of comparison, estimated with repeated comparisons
between the texts. The Copeland counting constructs a ranking over the compared
texts from pairwise preferences; the ranking reveals the poor scalability of
LLM-based pairwise comparison and helps yield the estimates for LLMs' error
rates. We apply the method to six LLMs (ChatGPT, Claude, DeepSeek, Gemini,
Grok, Qwen) with five types of text input and obtain consistent estimates of
LLMs' error. In general, the measured two positional bias terms are similar,
close to the uniform error. Considering both the error rates and the robustness
to the variation of prompts, Claude obtained the most desirable performance in
this experiment. Our model outperforms the biased Bradley-Terry model and the
commutativity score in indicating LLMs' error at this task.

</details>


### [25] [Evolution of the lexicon: a probabilistic point of view](https://arxiv.org/abs/2510.22220)
*Maurizio Serva*

Main category: cs.CL

TL;DR: 本文分析了Swadesh方法在估计语言时间分离时的局限性，指出即使基本假设都满足，数学上仍存在精度限制。同时强调了词汇渐进修改过程的重要性，并证明考虑这一随机过程能显著提高时间分离估计的精度。


<details>
  <summary>Details</summary>
Motivation: Swadesh方法基于词汇替换过程估计语言时间分离，但其基本假设往往不现实，且即使假设满足，数学上仍存在固有的精度限制。这些概率性限制在词汇统计学研究中常被忽视。

Method: 从纯概率角度分析Swadesh方法的数学局限性，并引入词汇渐进修改这一随机过程，通过概率模型证明考虑这一过程能提高估计精度。

Result: 揭示了Swadesh方法在估计语言时间分离时存在的固有精度限制，并证明词汇渐进修改过程对词汇演变有重要贡献，考虑这一过程能显著提高时间分离估计的准确性。

Conclusion: 语言词汇演变不仅受词汇替换过程驱动，还受词汇渐进修改过程影响。在估计语言时间分离时，必须考虑这两个随机过程，才能获得更精确的结果。

Abstract: The Swadesh approach for determining the temporal separation between two
languages relies on the stochastic process of words replacement (when a
complete new word emerges to represent a given concept). It is well known that
the basic assumptions of the Swadesh approach are often unrealistic due to
various contamination phenomena and misjudgments (horizontal transfers,
variations over time and space of the replacement rate, incorrect assessments
of cognacy relationships, presence of synonyms, and so on). All of this means
that the results cannot be completely correct.
  More importantly, even in the unrealistic case that all basic assumptions are
satisfied, simple mathematics places limits on the accuracy of estimating the
temporal separation between two languages. These limits, which are purely
probabilistic in nature and which are often neglected in lexicostatistical
studies, are analyzed in detail in this article.
  Furthermore, in this work we highlight that the evolution of a language's
lexicon is also driven by another stochastic process: gradual lexical
modification of words. We show that this process equally also represents a
major contribution to the reshaping of the vocabulary of languages over the
centuries and we also show, from a purely probabilistic perspective, that
taking into account this second random process significantly increases the
precision in determining the temporal separation between two languages.

</details>


### [26] [You Don't Need Prompt Engineering Anymore: The Prompting Inversion](https://arxiv.org/abs/2510.22251)
*Imran Khan*

Main category: cs.CL

TL;DR: Sculpting是一种基于规则的约束提示方法，旨在通过减少语义模糊和常识错误来改进标准CoT提示。研究发现存在"提示反转"现象：Sculpting在gpt-4o上优于标准CoT，但在更先进的gpt-5上反而有害。


<details>
  <summary>Details</summary>
Motivation: 标准CoT提示存在语义模糊和常识错误的问题，需要开发更精确的提示方法来提升LLM的推理能力。

Method: 提出"Sculpting"方法，这是一种基于规则的约束提示方法。在GSM8K数学推理基准上评估了三种提示策略（零样本、标准CoT、Sculpting）在三个OpenAI模型上的表现。

Result: 发现"提示反转"现象：Sculpting在gpt-4o上表现更好（97% vs 93%），但在gpt-5上反而有害（94.00% vs 96.36%）。这是由于"护栏到手铐"的转变，约束在先进模型中导致过度字面化。

Conclusion: 最优提示策略必须与模型能力共同进化，对更先进的模型应该使用更简单的提示。

Abstract: Prompt engineering, particularly Chain-of-Thought (CoT) prompting,
significantly enhances LLM reasoning capabilities. We introduce "Sculpting," a
constrained, rule-based prompting method designed to improve upon standard CoT
by reducing errors from semantic ambiguity and flawed common sense.
  We evaluate three prompting strategies (Zero Shot, standard CoT, and
Sculpting) across three OpenAI model generations (gpt-4o-mini, gpt-4o, gpt-5)
using the GSM8K mathematical reasoning benchmark (1,317 problems).
  Our findings reveal a "Prompting Inversion": Sculpting provides advantages on
gpt-4o (97% vs. 93% for standard CoT), but becomes detrimental on gpt-5 (94.00%
vs. 96.36% for CoT on full benchmark). We trace this to a
"Guardrail-to-Handcuff" transition where constraints preventing common-sense
errors in mid-tier models induce hyper-literalism in advanced models. Our
detailed error analysis demonstrates that optimal prompting strategies must
co-evolve with model capabilities, suggesting simpler prompts for more capable
models.

</details>


### [27] [SteerX: Disentangled Steering for LLM Personalization](https://arxiv.org/abs/2510.22256)
*Xiaoyan Zhao,Ming Yan,Yilun Qiu,Haoting Ni,Yang Zhang,Fuli Feng,Hong Cheng,Tat-Seng Chua*

Main category: cs.CL

TL;DR: SteerX是一种基于因果推断的激活引导方法，通过分离偏好驱动和偏好无关的组件来提升LLM个性化效果。


<details>
  <summary>Details</summary>
Motivation: 现有激活引导方法使用所有历史数据计算引导向量，但并非所有内容都反映真实用户偏好，这会削弱个性化信号。

Method: 基于因果推断理论估计token级因果效应来识别偏好驱动token，将其转化为连贯描述，然后用于引导个性化LLM生成。

Result: 在两个代表性引导骨干方法和真实数据集上的实验表明，SteerX持续提升引导向量质量。

Conclusion: SteerX通过聚焦真正偏好驱动信息，为更有效的LLM个性化提供了实用解决方案。

Abstract: Large language models (LLMs) have shown remarkable success in recent years,
enabling a wide range of applications, including intelligent assistants that
support users' daily life and work. A critical factor in building such
assistants is personalizing LLMs, as user preferences and needs vary widely.
Activation steering, which directly leverages directions representing user
preference in the LLM activation space to adjust its behavior, offers a
cost-effective way to align the model's outputs with individual users. However,
existing methods rely on all historical data to compute the steering vector,
ignoring that not all content reflects true user preferences, which undermines
the personalization signal. To address this, we propose SteerX, a disentangled
steering method that isolates preference-driven components from
preference-agnostic components. Grounded in causal inference theory, SteerX
estimates token-level causal effects to identify preference-driven tokens,
transforms these discrete signals into a coherent description, and then
leverages them to steer personalized LLM generation. By focusing on the truly
preference-driven information, SteerX produces more accurate activation
steering vectors and enhances personalization. Experiments on two
representative steering backbone methods across real-world datasets demonstrate
that SteerX consistently enhances steering vector quality, offering a practical
solution for more effective LLM personalization.

</details>


### [28] [PatenTEB: A Comprehensive Benchmark and Model Family for Patent Text Embedding](https://arxiv.org/abs/2510.22264)
*Iliass Ayaou,Denis Cavallucci*

Main category: cs.CL

TL;DR: 提出了PatenTEB专利文本嵌入基准，包含15个任务和206万样本，并开发了patembed模型家族，在专利检索和聚类任务上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有基准无法充分捕捉专利文本特有的挑战，如领域分层、非对称片段-文档匹配等专利检索中的实际问题。

Method: 采用领域分层划分、领域特定难负样本挖掘，通过多任务训练开发patembed模型家族（67M-344M参数，最长4096上下文）。

Result: patembed-base在MTEB BigPatentClustering.v2上达到0.494 V-measure（之前最佳为0.445），patembed-large在DAPFAM上达到0.377 NDCG@100。

Conclusion: 多任务训练能提升外部泛化能力，领域预训练初始化在不同任务家族中提供一致优势，所有资源将开源。

Abstract: Patent text embeddings enable prior art search, technology landscaping, and
patent analysis, yet existing benchmarks inadequately capture patent-specific
challenges. We introduce PatenTEB, a comprehensive benchmark comprising 15
tasks across retrieval, classification, paraphrase, and clustering, with 2.06
million examples. PatenTEB employs domain-stratified splits, domain specific
hard negative mining, and systematic coverage of asymmetric
fragment-to-document matching scenarios absent from general embedding
benchmarks. We develop the patembed model family through multi-task training,
spanning 67M to 344M parameters with context lengths up to 4096 tokens.
External validation shows strong generalization: patembed-base achieves
state-of-the-art on MTEB BigPatentClustering.v2 (0.494 V-measure vs. 0.445
previous best), while patembed-large achieves 0.377 NDCG@100 on DAPFAM.
Systematic ablations reveal that multi-task training improves external
generalization despite minor benchmark costs, and that domain-pretrained
initialization provides consistent advantages across task families. All
resources will be made available at https://github.com/iliass-y/patenteb.
Keywords: patent retrieval, sentence embeddings, multi-task learning,
asymmetric retrieval, benchmark evaluation, contrastive learning.

</details>


### [29] [From Slides to Chatbots: Enhancing Large Language Models with University Course Materials](https://arxiv.org/abs/2510.22272)
*Tu Anh Dinh,Philipp Nicolas Schumacher,Jan Niehues*

Main category: cs.CL

TL;DR: 研究比较了RAG和CPT两种方法，发现在大学课程材料规模较小的情况下，RAG比CPT更有效且高效，特别是将幻灯片以图像形式呈现的多模态方法显著优于仅文本检索。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在教育应用中仍难以准确回答大学计算机科学课程问题，需要探索如何利用课程材料提升其性能。

Method: 比较了检索增强生成(RAG)和持续预训练(CPT)两种策略，并针对包含视觉元素的幻灯片探索了多模态RAG方法。

Result: RAG比CPT更有效高效，多模态RAG（幻灯片作为图像）比纯文本检索性能显著提升。

Conclusion: 为开发更好的教育AI助手提供了实用策略，可在其他教育场景中推广应用。

Abstract: Large Language Models (LLMs) have advanced rapidly in recent years. One
application of LLMs is to support student learning in educational settings.
However, prior work has shown that LLMs still struggle to answer questions
accurately within university-level computer science courses. In this work, we
investigate how incorporating university course materials can enhance LLM
performance in this setting. A key challenge lies in leveraging diverse course
materials such as lecture slides and transcripts, which differ substantially
from typical textual corpora: slides also contain visual elements like images
and formulas, while transcripts contain spoken, less structured language. We
compare two strategies, Retrieval-Augmented Generation (RAG) and Continual
Pre-Training (CPT), to extend LLMs with course-specific knowledge. For lecture
slides, we further explore a multi-modal RAG approach, where we present the
retrieved content to the generator in image form. Our experiments reveal that,
given the relatively small size of university course materials, RAG is more
effective and efficient than CPT. Moreover, incorporating slides as images in
the multi-modal setting significantly improves performance over text-only
retrieval. These findings highlight practical strategies for developing AI
assistants that better support learning and teaching, and we hope they inspire
similar efforts in other educational contexts.

</details>


### [30] [Supervised Fine-Tuning or In-Context Learning? Evaluating LLMs for Clinical NER](https://arxiv.org/abs/2510.22285)
*Andrei Baroian*

Main category: cs.CL

TL;DR: 比较了BERT类编码器、GPT-4o少样本学习和监督微调在临床命名实体识别任务上的表现，发现监督微调效果最佳但成本较高。


<details>
  <summary>Details</summary>
Motivation: 研究不同方法在临床命名实体识别任务上的性能差异，特别是比较传统BERT类模型与大语言模型的表现。

Method: 使用CADEC语料库，比较三类方法：(i) BERT类编码器(BERT Base、BioClinicalBERT、RoBERTa-large)；(ii) GPT-4o少样本学习(简单vs复杂提示)；(iii) GPT-4o监督微调。评估5种实体类型的标准NER指标。

Result: RoBERTa-large和BioClinicalBERT相比BERT Base提升有限；简单提示的少样本学习优于复杂提示；监督微调获得最佳性能(F1≈87.1%)但成本较高；LLM在二分类简化任务上表现更好。

Conclusion: 监督微调在大语言模型上能获得最佳临床NER性能，但需权衡成本；简单提示策略更有效；大语言模型在简化分类任务上表现更优。

Abstract: We study clinical Named Entity Recognition (NER) on the CADEC corpus and
compare three families of approaches: (i) BERT-style encoders (BERT Base,
BioClinicalBERT, RoBERTa-large), (ii) GPT-4o used with few-shot in-context
learning (ICL) under simple vs.\ complex prompts, and (iii) GPT-4o with
supervised fine-tuning (SFT). All models are evaluated on standard NER metrics
over CADEC's five entity types (ADR, Drug, Disease, Symptom, Finding).
RoBERTa-large and BioClinicalBERT offer limited improvements over BERT Base,
showing the limit of these family of models. Among LLM settings, simple ICL
outperforms a longer, instruction-heavy prompt, and SFT achieves the strongest
overall performance (F1 $\approx$ 87.1%), albeit with higher cost. We find that
the LLM achieve higher accuracy on simplified tasks, restricting classification
to two labels.

</details>


### [31] [Memory-based Language Models: An Efficient, Explainable, and Eco-friendly Approach to Large Language Modeling](https://arxiv.org/abs/2510.22317)
*Antal van den Bosch,Ainhoa Risco Patón,Teun Buijse,Peter Berck,Maarten van Gompel*

Main category: cs.CL

TL;DR: 提出基于记忆的语言建模作为深度神经网络语言建模的高效环保替代方案，具有对数线性可扩展性和强记忆能力


<details>
  <summary>Details</summary>
Motivation: 解决深度神经网络语言建模的高能耗和生态足迹问题，提供更环保、透明的语言建模方法

Method: 实现快速近似的k近邻分类，完全依赖CPU运行，在训练和推理阶段都保持低生态足迹

Result: OLIFANT实现与GPT-2和GPT-Neo相当的下一个词预测准确率，同时具有更低的排放和更快的速度

Conclusion: 基于记忆的语言建模是深度神经网络语言建模的可行环保替代方案，具有简单透明、低生态足迹的优势

Abstract: We present memory-based language modeling as an efficient, eco-friendly
alternative to deep neural network-based language modeling. It offers
log-linearly scalable next-token prediction performance and strong memorization
capabilities. Implementing fast approximations of k-nearest neighbor
classification, memory-based language modeling leaves a relatively small
ecological footprint both in training and in inference mode, as it relies fully
on CPUs and attains low token latencies. Its internal workings are simple and
fully transparent. We compare our implementation of memory-based language
modeling, OLIFANT, with GPT-2 and GPT-Neo on next-token prediction accuracy,
estimated emissions and speeds, and offer some deeper analyses of the model.

</details>


### [32] [Multilingual Target-Stance Extraction](https://arxiv.org/abs/2510.22334)
*Ethan Mines,Bonnie Dorr*

Main category: cs.CL

TL;DR: 本文提出了首个多语言目标立场提取(TSE)基准，涵盖6种语言，并开发了无需为每种语言单独训练模型的统一管道，揭示了多语言TSE相比英语任务的挑战性。


<details>
  <summary>Details</summary>
Motivation: 现有TSE研究仅限于英语，缺乏多语言环境下的基准和评估标准，限制了在社交媒体多语言内容分析中的应用。

Method: 扩展原始TSE管道至多语言设置，涵盖加泰罗尼亚语、爱沙尼亚语、法语、意大利语、普通话和西班牙语语料库，无需为每种语言单独训练模型。

Result: 模型管道获得12.78的F1分数，表明多语言任务相比英语设置难度显著增加，目标预测是主要瓶颈，并首次证明了TSE的F1分数对不同目标表述的敏感性。

Conclusion: 这项工作为多语言TSE提供了急需的资源、算法和评估标准基准，突出了该领域的挑战和未来研究方向。

Abstract: Social media enables data-driven analysis of public opinion on contested
issues. Target-Stance Extraction (TSE) is the task of identifying the target
discussed in a document and the document's stance towards that target. Many
works classify stance towards a given target in a multilingual setting, but all
prior work in TSE is English-only. This work introduces the first multilingual
TSE benchmark, spanning Catalan, Estonian, French, Italian, Mandarin, and
Spanish corpora. It manages to extend the original TSE pipeline to a
multilingual setting without requiring separate models for each language. Our
model pipeline achieves a modest F1 score of 12.78, underscoring the increased
difficulty of the multilingual task relative to English-only setups and
highlighting target prediction as the primary bottleneck. We are also the first
to demonstrate the sensitivity of TSE's F1 score to different target
verbalizations. Together these serve as a much-needed baseline for resources,
algorithms, and evaluation criteria in multilingual TSE.

</details>


### [33] [FAIR-RAG: Faithful Adaptive Iterative Refinement for Retrieval-Augmented Generation](https://arxiv.org/abs/2510.22344)
*Mohammad Aghajani Asl,Majid Asgari-Bidhendi,Behrooz Minaei-Bidgoli*

Main category: cs.CL

TL;DR: FAIR-RAG是一种新型代理框架，通过结构化证据评估和迭代优化循环，显著提升了复杂多跳问答任务中的检索增强生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有RAG框架在处理需要从不同来源综合信息的复杂多跳查询时表现不佳，缺乏系统识别和填补证据空白的机制。

Method: 引入FAIR-RAG框架，核心是结构化证据评估(SEA)模块和迭代优化循环，通过分析证据缺口并生成针对性子查询来动态完善证据。

Result: 在HotpotQA、2WikiMultiHopQA和MusiQue等基准测试中显著优于基线方法，在HotpotQA上F1得分达到0.453，比最强迭代基线提升8.3个百分点。

Conclusion: 结构化、证据驱动的优化过程与显式缺口分析对于复杂知识密集型任务中的可靠准确推理至关重要。

Abstract: While Retrieval-Augmented Generation (RAG) mitigates hallucination and
knowledge staleness in Large Language Models (LLMs), existing frameworks often
falter on complex, multi-hop queries that require synthesizing information from
disparate sources. Current advanced RAG methods, employing iterative or
adaptive strategies, lack a robust mechanism to systematically identify and
fill evidence gaps, often propagating noise or failing to gather a
comprehensive context. We introduce FAIR-RAG, a novel agentic framework that
transforms the standard RAG pipeline into a dynamic, evidence-driven reasoning
process. At its core is an Iterative Refinement Cycle governed by a module we
term Structured Evidence Assessment (SEA). The SEA acts as an analytical gating
mechanism: it deconstructs the initial query into a checklist of required
findings and audits the aggregated evidence to identify confirmed facts and,
critically, explicit informational gaps. These gaps provide a precise signal to
an Adaptive Query Refinement agent, which generates new, targeted sub-queries
to retrieve missing information. This cycle repeats until the evidence is
verified as sufficient, ensuring a comprehensive context for a final, strictly
faithful generation. We conducted experiments on challenging multi-hop QA
benchmarks, including HotpotQA, 2WikiMultiHopQA, and MusiQue. In a unified
experimental setup, FAIR-RAG significantly outperforms strong baselines. On
HotpotQA, it achieves an F1-score of 0.453 -- an absolute improvement of 8.3
points over the strongest iterative baseline -- establishing a new
state-of-the-art for this class of methods on these benchmarks. Our work
demonstrates that a structured, evidence-driven refinement process with
explicit gap analysis is crucial for unlocking reliable and accurate reasoning
in advanced RAG systems for complex, knowledge-intensive tasks.

</details>


### [34] [Irony Detection in Urdu Text: A Comparative Study Using Machine Learning Models and Large Language Models](https://arxiv.org/abs/2510.22356)
*Fiaz Ahmad,Nisar Hussain,Amna Qasim,Momina Hafeez,Muhammad Usman Grigori Sidorov,Alexander Gelbukh*

Main category: cs.CL

TL;DR: 该研究通过将英文反讽语料库翻译成乌尔都语，评估了10种机器学习算法和5种Transformer模型在乌尔都语反讽检测中的表现，发现LLaMA 3 (8B)取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 解决乌尔都语这种语法和文化背景不同的语言中反讽识别的挑战，特别是在历史资源匮乏的情况下。

Method: 将英文反讽语料库翻译成乌尔都语，使用GloVe和Word2Vec嵌入评估10种机器学习算法，并对BERT、RoBERTa、LLaMA 2、LLaMA 3和Mistral等Transformer模型进行微调。

Result: 机器学习模型中梯度提升算法F1分数达89.18%，Transformer模型中LLaMA 3 (8B) F1分数达94.61%。

Conclusion: 结合音译技术和现代NLP模型能够在资源匮乏的乌尔都语中实现稳健的反讽检测。

Abstract: Ironic identification is a challenging task in Natural Language Processing,
particularly when dealing with languages that differ in syntax and cultural
context. In this work, we aim to detect irony in Urdu by translating an English
Ironic Corpus into the Urdu language. We evaluate ten state-of-the-art machine
learning algorithms using GloVe and Word2Vec embeddings, and compare their
performance with classical methods. Additionally, we fine-tune advanced
transformer-based models, including BERT, RoBERTa, LLaMA 2 (7B), LLaMA 3 (8B),
and Mistral, to assess the effectiveness of large-scale models in irony
detection. Among machine learning models, Gradient Boosting achieved the best
performance with an F1-score of 89.18%. Among transformer-based models, LLaMA 3
(8B) achieved the highest performance with an F1-score of 94.61%. These results
demonstrate that combining transliteration techniques with modern NLP models
enables robust irony detection in Urdu, a historically low-resource language.

</details>


### [35] [GigaEmbeddings: Efficient Russian Language Embedding Model](https://arxiv.org/abs/2510.22369)
*Egor Kolodin,Daria Khomich,Nikita Savushkin,Anastasia Ianina,Fyodor Minkin*

Main category: cs.CL

TL;DR: GigaEmbeddings是一个通过分层指令调优训练高性能俄语文本嵌入的新框架，基于GigaChat-3B解码器模型，在ruMTEB基准测试中达到69.1的平均分数，优于参数更多的基线模型。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在俄语文本嵌入方面的局限性，通过统一多样化目标并利用合成数据生成来提升性能。

Method: 采用三阶段流水线：大规模对比预训练、使用困难负样本的微调、以及跨检索、分类和聚类任务的多任务泛化。架构创新包括双向注意力、潜在注意力池化和25%转换器层的策略性剪枝。

Result: 在ruMTEB基准测试的23个多语言任务中取得最先进结果（69.1平均分数），优于参数更多的强基线模型。

Conclusion: GigaEmbeddings框架通过分层指令调优和架构优化，成功实现了高性能的俄语文本嵌入，在效率和性能之间取得了良好平衡。

Abstract: We introduce GigaEmbeddings, a novel framework for training high-performance
Russian-focused text embeddings through hierarchical instruction tuning of the
decoder-only LLM designed specifically for Russian language (GigaChat-3B). Our
three-stage pipeline, comprising large-scale contrastive pre-training in
web-scale corpora, fine-tuning with hard negatives, and multitask
generalization across retrieval, classification, and clustering tasks,
addresses key limitations of existing methods by unifying diverse objectives
and leveraging synthetic data generation. Architectural innovations include
bidirectional attention for contextual modeling, latent attention pooling for
robust sequence aggregation, and strategic pruning of 25% of transformer layers
to enhance efficiency without compromising performance. Evaluated on the ruMTEB
benchmark spanning 23 multilingual tasks, GigaEmbeddings achieves
state-of-the-art results (69.1 avg. score), outperforming strong baselines with
a larger number of parameters.

</details>


### [36] [VisJudge-Bench: Aesthetics and Quality Assessment of Visualizations](https://arxiv.org/abs/2510.22373)
*Yupeng Xie,Zhiyang Zhang,Yifan Wu,Sirong Lu,Jiayi Zhang,Zhaoyang Yu,Jinlin Wang,Sirui Hong,Bang Liu,Chenglin Wu,Yuyu Luo*

Main category: cs.CL

TL;DR: 提出了VisJudge-Bench，首个用于评估多模态大语言模型在可视化美学和质量评估能力的基准，包含3,090个专家标注样本。测试发现即使最先进的MLLMs与人类专家存在显著差距，为此提出了专门的可视化评估模型VisJudge，显著缩小了与人类判断的差距。


<details>
  <summary>Details</summary>
Motivation: 可视化质量评估具有挑战性，需要同时判断数据编码准确性、信息表达性和视觉美学。虽然多模态大语言模型在自然图像美学评估中表现出色，但缺乏系统性的基准来衡量它们在可视化评估中的能力。

Method: 构建VisJudge-Bench基准，包含3,090个专家标注的真实场景样本，涵盖32种图表类型。提出专门的可视化美学和质量评估模型VisJudge。

Result: 测试显示最先进的MLLMs（如GPT-5）与人类专家存在显著差距，MAE为0.551，与人类评分相关性仅0.429。VisJudge将MAE降至0.442（减少19.8%），与人类专家一致性提升至0.681（改善58.7%）。

Conclusion: VisJudge-Bench是首个全面的可视化评估基准，揭示了当前MLLMs在可视化评估方面的局限性，提出的VisJudge模型显著提升了评估性能，为可视化质量评估提供了有效解决方案。

Abstract: Visualization, a domain-specific yet widely used form of imagery, is an
effective way to turn complex datasets into intuitive insights, and its value
depends on whether data are faithfully represented, clearly communicated, and
aesthetically designed. However, evaluating visualization quality is
challenging: unlike natural images, it requires simultaneous judgment across
data encoding accuracy, information expressiveness, and visual aesthetics.
Although multimodal large language models (MLLMs) have shown promising
performance in aesthetic assessment of natural images, no systematic benchmark
exists for measuring their capabilities in evaluating visualizations. To
address this, we propose VisJudge-Bench, the first comprehensive benchmark for
evaluating MLLMs' performance in assessing visualization aesthetics and
quality. It contains 3,090 expert-annotated samples from real-world scenarios,
covering single visualizations, multiple visualizations, and dashboards across
32 chart types. Systematic testing on this benchmark reveals that even the most
advanced MLLMs (such as GPT-5) still exhibit significant gaps compared to human
experts in judgment, with a Mean Absolute Error (MAE) of 0.551 and a
correlation with human ratings of only 0.429. To address this issue, we propose
VisJudge, a model specifically designed for visualization aesthetics and
quality assessment. Experimental results demonstrate that VisJudge
significantly narrows the gap with human judgment, reducing the MAE to 0.442 (a
19.8% reduction) and increasing the consistency with human experts to 0.681 (a
58.7% improvement) compared to GPT-5. The benchmark is available at
https://github.com/HKUSTDial/VisJudgeBench.

</details>


### [37] [Confabulations from ACL Publications (CAP): A Dataset for Scientific Hallucination Detection](https://arxiv.org/abs/2510.22395)
*Federica Gamba,Aman Sinha,Timothee Mickus,Raul Vazquez,Patanjali Bhamidipati,Claudio Savelli,Ahana Chattopadhyay,Laura A. Zanella,Yash Kankanampati,Binesh Arakkal Remesh,Aryan Ashok Chandramania,Rohit Agarwal,Chuyuan Li,Ioana Buhnila,Radhika Mamidi*

Main category: cs.CL

TL;DR: CAP数据集是一个多语言资源，用于研究大语言模型在科学文本生成中的幻觉问题，涵盖9种语言和900个科学问题，包含7000多个模型生成的答案及标注。


<details>
  <summary>Details</summary>
Motivation: 科学领域中的幻觉会扭曲事实知识，而专业术语、统计推理和上下文依赖解释进一步加剧了这种扭曲，特别是考虑到LLMs缺乏真正理解、上下文理解有限以及偏向表面泛化。

Method: 构建跨语言数据集，覆盖5种高资源语言和4种低资源语言，包含900个科学问题和7000多个LLM生成的答案，每个实例标注了科学幻觉的二元标签和流畅度标签。

Result: 创建了CAP数据集，包含问题-答案对、token序列和对应logits，公开发布以促进幻觉检测、多语言LLM评估和更可靠科学NLP系统的研究。

Conclusion: CAP数据集为研究科学文本生成中的幻觉问题提供了有价值的资源，有助于推动幻觉检测、多语言评估和可靠科学NLP系统的发展。

Abstract: We introduce the CAP (Confabulations from ACL Publications) dataset, a
multilingual resource for studying hallucinations in large language models
(LLMs) within scientific text generation. CAP focuses on the scientific domain,
where hallucinations can distort factual knowledge, as they frequently do. In
this domain, however, the presence of specialized terminology, statistical
reasoning, and context-dependent interpretations further exacerbates these
distortions, particularly given LLMs' lack of true comprehension, limited
contextual understanding, and bias toward surface-level generalization. CAP
operates in a cross-lingual setting covering five high-resource languages
(English, French, Hindi, Italian, and Spanish) and four low-resource languages
(Bengali, Gujarati, Malayalam, and Telugu). The dataset comprises 900 curated
scientific questions and over 7000 LLM-generated answers from 16 publicly
available models, provided as question-answer pairs along with token sequences
and corresponding logits. Each instance is annotated with a binary label
indicating the presence of a scientific hallucination, denoted as a factuality
error, and a fluency label, capturing issues in the linguistic quality or
naturalness of the text. CAP is publicly released to facilitate advanced
research on hallucination detection, multilingual evaluation of LLMs, and the
development of more reliable scientific NLP systems.

</details>


### [38] [CHOIR: Collaborative Harmonization fOr Inference Robustness](https://arxiv.org/abs/2510.22475)
*Xiangjue Dong,Cong Wang,Maria Teleki,Millennium Bismay,James Caverlee*

Main category: cs.CL

TL;DR: CHOIR是一个测试时框架，通过协调多个角色条件下的推理信号来提高LLM推理的鲁棒性，无需额外训练即可显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 研究发现LLM在角色分配时，即使微小的身份特征变化（如代词改变）也会导致推理轨迹和正确答案的差异。与其将这些差异视为需要消除的偏见，作者探索将其作为提高推理鲁棒性的建设性资源。

Method: 提出CHOIR框架，在反事实角色之间进行协作解码，动态平衡推理路径中的一致性和分歧，将多个角色条件下的推理信号协调为统一预测。

Result: 在多个推理基准测试中，CHOIR持续提升了不同人口统计群体、模型架构、规模和任务的性能，无需额外训练。个体人口统计群体改进高达26.4%，五个人口统计群体平均改进19.2%。即使在基础角色不理想的情况下仍然有效。

Conclusion: 通过将角色差异重新定义为建设性信号，CHOIR提供了一种可扩展和泛化的方法，实现更可靠的LLM推理。

Abstract: Persona-assigned Large Language Models (LLMs) can adopt diverse roles,
enabling personalized and context-aware reasoning. However, even minor
demographic perturbations in personas, such as simple pronoun changes, can
alter reasoning trajectories, leading to divergent sets of correct answers.
Instead of treating these variations as biases to be mitigated, we explore
their potential as a constructive resource to improve reasoning robustness. We
propose CHOIR (Collaborative Harmonization fOr Inference Robustness), a
test-time framework that harmonizes multiple persona-conditioned reasoning
signals into a unified prediction. CHOIR orchestrates a collaborative decoding
process among counterfactual personas, dynamically balancing agreement and
divergence in their reasoning paths. Experiments on various reasoning
benchmarks demonstrate that CHOIR consistently enhances performance across
demographics, model architectures, scales, and tasks - without additional
training. Improvements reach up to 26.4% for individual demographic groups and
19.2% on average across five demographics. It remains effective even when base
personas are suboptimal. By reframing persona variation as a constructive
signal, CHOIR provides a scalable and generalizable approach to more reliable
LLM reasoning.

</details>


### [39] [The Tonogenesis Continuum in Tibetan: A Computational Investigation](https://arxiv.org/abs/2510.22485)
*Siyu Liang,Zhaxi Zerong*

Main category: cs.CL

TL;DR: 本文提出了一种计算方法来量化音高在声调形成过程中的功能作用，通过分析藏语方言对音高平坦化的敏感度，揭示了声调形成的连续谱。


<details>
  <summary>Details</summary>
Motivation: 传统上通过比较重建和声学语音学研究声调形成过程，本文旨在引入计算方法来量化音高在不同声调形成阶段的功能作用。

Method: 通过测量音高操作对自动语音识别性能的影响，分析一组密切相关的藏语方言对音高平坦化的敏感度。

Result: 发现声调形成连续谱：无音调变化的安多方言最能容忍音高移除，完全有声调的卫藏方言显示严重性能下降，而中间的康方言处于这两个极端之间。

Conclusion: 计算方法能够捕捉声音变化的精细阶段，传统基于最小对立对的功能负载指标可能高估过渡系统中音高的依赖性。

Abstract: Tonogenesis-the historical process by which segmental contrasts evolve into
lexical tone-has traditionally been studied through comparative reconstruction
and acoustic phonetics. We introduce a computational approach that quantifies
the functional role of pitch at different stages of this sound change by
measuring how pitch manipulation affects automatic speech recognition (ASR)
performance. Through analysis on the sensitivity to pitch-flattening from a set
of closely related Tibetan languages, we find evidence of a tonogenesis
continuum: atonal Amdo dialects tolerate pitch removal the most, while fully
tonal U-Tsang varieties show severe degradation, and intermediate Kham dialects
fall measurably between these extremes. These gradient effects demonstrate how
ASR models implicitly learn the shifting functional load of pitch as languages
transition from consonant-based to tone-based lexical contrasts. Our findings
show that computational methods can capture fine-grained stages of sound change
and suggest that traditional functional load metrics, based solely on minimal
pairs, may overestimate pitch dependence in transitional systems where
segmental and suprasegmental cues remain phonetically intertwined.

</details>


### [40] [Frustratingly Easy Task-aware Pruning for Large Language Models](https://arxiv.org/abs/2510.22489)
*Yuanhe Tian,Junjie Liu,Xican Yang,Haishan Ye,Yan Song*

Main category: cs.CL

TL;DR: 提出了一种简单有效的LLM剪枝方法，在压缩模型的同时保留任务特定能力，通过结合通用和任务特定特征分布来计算参数重要性分数。


<details>
  <summary>Details</summary>
Motivation: 现有LLM剪枝方法主要关注保持模型生成流畅句子的能力，但忽视了在特定领域和任务上的性能表现。

Method: 分析传统剪枝方法在通用领域校准下的损失扰动最小化，将任务特定特征分布纳入现有剪枝算法的重要性计算中，使用通用和任务特定校准数据分别计算重要性分数，基于激活范数差异将参数划分为共享和专属组，融合分数指导剪枝过程。

Result: 在广泛使用的基准测试上，该方法在相同剪枝比例和不同设置下始终优于基线方法。

Conclusion: 该方法能够与各种基础剪枝技术无缝集成，在压缩过程中保留LLM的专业化能力。

Abstract: Pruning provides a practical solution to reduce the resources required to run
large language models (LLMs) to benefit from their effective capabilities as
well as control their cost for training and inference. Research on LLM pruning
often ranks the importance of LLM parameters using their magnitudes and
calibration-data activations and removes (or masks) the less important ones,
accordingly reducing LLMs' size. However, these approaches primarily focus on
preserving the LLM's ability to generate fluent sentences, while neglecting
performance on specific domains and tasks. In this paper, we propose a simple
yet effective pruning approach for LLMs that preserves task-specific
capabilities while shrinking their parameter space. We first analyze how
conventional pruning minimizes loss perturbation under general-domain
calibration and extend this formulation by incorporating task-specific feature
distributions into the importance computation of existing pruning algorithms.
Thus, our framework computes separate importance scores using both general and
task-specific calibration data, partitions parameters into shared and exclusive
groups based on activation-norm differences, and then fuses their scores to
guide the pruning process. This design enables our method to integrate
seamlessly with various foundation pruning techniques and preserve the LLM's
specialized abilities under compression. Experiments on widely used benchmarks
demonstrate that our approach is effective and consistently outperforms the
baselines with identical pruning ratios and different settings.

</details>


### [41] [The Limits of Data Scaling: Sub-token Utilization and Acoustic Saturation in Multilingual ASR](https://arxiv.org/abs/2510.22492)
*Siyu Liang,Nicolas Ballier,Gina-Anne Levow,Richard Wright*

Main category: cs.CL

TL;DR: 分析Whisper多语言ASR模型在49种语言上的解码行为，发现子词发现率遵循指数饱和模式，提出声学饱和时间(AST)概念，表明子词利用更多受语音统计、类型和正字法结构约束而非训练数据规模影响。


<details>
  <summary>Details</summary>
Motivation: 研究多语言ASR模型在不同语言中学习的子词库存的观察需求，以及多语言预训练中的数据不平衡是否影响推理时这些标记的利用。

Method: 通过记录49种语言的解码候选子词并跟踪其随时间累积发现，分析模型子词空间的利用模式。

Result: 发现子词总数与语言预训练小时数无关，子词发现率遵循一致的指数饱和模式，拉丁文字的语言比西里尔、CJK和闪米特文字表现更好。

Conclusion: 多语言ASR推理中的子词利用更多受语音统计、类型和正字法结构约束，而非训练数据规模，为更公平的语料构建和跨语言评估提供实证基础。

Abstract: How much audio is needed to fully observe a multilingual ASR model's learned
sub-token inventory across languages, and does data disparity in multilingual
pre-training affect how these tokens are utilized during inference? We address
this question by analyzing Whisper's decoding behavior during inference across
49 languages. By logging decoding candidate sub-tokens and tracking their
cumulative discovery over time, we study the utilization pattern of the model's
sub-token space. Results show that the total number of discovered tokens
remains largely independent of a language's pre-training hours, indicating that
data disparity does not strongly influence lexical diversity in the model's
hypothesis space. Sub-token discovery rates follow a consistent exponential
saturation pattern across languages, suggesting a stable time window after
which additional audio yields minimal new sub-token activation. We refer to
this convergence threshold as acoustic saturation time (AST). Further analyses
of rank-frequency distributions reveal Zipf-like patterns better modeled by a
Zipf-Mandelbrot law, and mean sub-token length shows a positive correlation
with resource level. Additionally, those metrics show more favorable patterns
for languages in the Latin script than those in scripts such as Cyrillic, CJK,
and Semitic. Together, our study suggests that sub-token utilization during
multilingual ASR inference is constrained more by the statistical, typological,
and orthographic structure of the speech than by training data scale, providing
an empirical basis for more equitable corpus construction and cross-lingual
evaluation.

</details>


### [42] [A Sociophonetic Analysis of Racial Bias in Commercial ASR Systems Using the Pacific Northwest English Corpus](https://arxiv.org/abs/2510.22495)
*Michael Scott,Siyu Liang,Alicia Wassink,Gina-Anne Levow*

Main category: cs.CL

TL;DR: 该研究系统评估了四大商业ASR系统中的种族偏见，发现语音质量变异（特别是对低后元音合并和前鼻音合并模式的抵抗）是导致不同种族群体识别错误率差异的主要因素，其中非裔美国人受影响最严重。


<details>
  <summary>Details</summary>
Motivation: 评估商业自动语音识别系统中存在的种族偏见，分析社会语音变异如何导致不同种族群体的识别性能差异。

Method: 使用太平洋西北英语语料库，分析四个种族背景说话者的转录准确性，引入启发式确定的语音错误率指标，将识别错误与11个社会语音特征相关联。

Result: 发现元音质量变异与不同种族群体的错误率差异系统相关，非裔美国说话者在所有评估系统中受影响最明显，表明方言语音变异的声学建模是商业ASR系统偏见的主要来源。

Conclusion: 该研究确立了PNWE语料库作为语音技术偏见评估的宝贵资源，并为通过训练数据中针对性表示社会语音多样性来改进ASR性能提供了可行指导。

Abstract: This paper presents a systematic evaluation of racial bias in four major
commercial automatic speech recognition (ASR) systems using the Pacific
Northwest English (PNWE) corpus. We analyze transcription accuracy across
speakers from four ethnic backgrounds (African American, Caucasian American,
ChicanX, and Yakama) and examine how sociophonetic variation contributes to
differential system performance. We introduce a heuristically-determined
Phonetic Error Rate (PER) metric that links recognition errors to specific
linguistically motivated variables derived from sociophonetic annotation. Our
analysis of eleven sociophonetic features reveals that vowel quality variation,
particularly resistance to the low-back merger and pre-nasal merger patterns,
is systematically associated with differential error rates across ethnic
groups, with the most pronounced effects for African American speakers across
all evaluated systems. These findings demonstrate that acoustic modeling of
dialectal phonetic variation, rather than lexical or syntactic factors, remains
a primary source of bias in commercial ASR systems. The study establishes the
PNWE corpus as a valuable resource for bias evaluation in speech technologies
and provides actionable guidance for improving ASR performance through targeted
representation of sociophonetic diversity in training data.

</details>


### [43] [Text to Trust: Evaluating Fine-Tuning and LoRA Trade-offs in Language Models for Unfair Terms of Service Detection](https://arxiv.org/abs/2510.22531)
*Noshitha Padma Pratyusha Juttu,Sahithi Singireddy,Sravani Gona,Sujal Timilsina*

Main category: cs.CL

TL;DR: 系统评估了针对服务条款中不公平条款检测任务的不同LLM适应方法，包括全微调、参数高效适应（LoRA、QLoRA）和零样本提示策略。


<details>
  <summary>Details</summary>
Motivation: LLMs在文本理解方面表现出色，但在专业法律领域的适应仍受限于全微调的高成本，需要探索更高效的适应方法。

Method: 对BERT和DistilBERT进行全微调，对TinyLlama、LLaMA 3B/7B和SaulLM应用4位低秩适应（LoRA），并在零样本设置下评估GPT-4o及其变体。

Result: 全微调在精确率-召回率平衡方面表现最佳，而基于LoRA的模型在内存成本降低3倍的情况下仍能提供有竞争力的召回率。

Conclusion: 研究揭示了高效和领域适应LLMs的实际设计权衡，为法律文本处理中的微调研究提供了开放基准。

Abstract: Large Language Models (LLMs) have transformed text understanding, yet their
adaptation to specialized legal domains remains constrained by the cost of full
fine-tuning. This study provides a systematic evaluation of fine tuning,
parameter efficient adaptation (LoRA, QLoRA), and zero-shot prompting
strategies for unfair clause detection in Terms of Service (ToS) documents, a
key application in legal NLP. We finetune BERT and DistilBERT, apply 4-bit
Low-Rank Adaptation (LoRA) to models such as TinyLlama, LLaMA 3B/7B, and
SaulLM, and evaluate GPT-4o and O-versions in zero-shot settings. Experiments
on the CLAUDETTE-ToS benchmark and the Multilingual Scraper Corpus show that
full fine-tuning achieves the strongest precision recall balance, while
LoRA-based models provide competitive recall with up to 3x lower memory cost.
These findings highlight practical design trade-offs for efficient and
domain-adapted LLMs, contributing open baselines for fine-tuning research in
legal text processing.

</details>


### [44] [LooGLE v2: Are LLMs Ready for Real World Long Dependency Challenges?](https://arxiv.org/abs/2510.22548)
*Ziyuan He,Yuxuan Wang,Jiaqi Li,Kexin Liang,Muhan Zhang*

Main category: cs.CL

TL;DR: LooGLE v2是一个评估大语言模型在真实世界长上下文应用场景中能力的基准测试，包含16k到2M tokens的文本，涵盖法律、金融、游戏和代码等领域，设计了10种长依赖任务类型。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型虽然拥有越来越长的上下文窗口，但在处理长依赖任务时的实际能力仍然有限且研究不足，特别是在许多真实世界长上下文应用中缺乏基准测试。

Method: 通过自动收集真实世界长文本（16k-2M tokens），精心设计10种领域特定的长依赖任务类型，使用可扩展的数据处理流程生成1,934个QA实例，并对6个本地部署和4个API-based的LLM进行全面评估。

Result: 评估结果显示，即使在最佳表现的模型上，在基准测试中的总体得分也仅为59.2%。流行的LLM实际上只能理解比其声称长度短得多的上下文，在处理真实世界长依赖任务时存在显著局限性。

Conclusion: 尽管大语言模型拥有扩展的上下文窗口，但在处理真实世界长依赖任务时的能力仍存在重大限制，揭示了模型在实际长上下文理解方面还有很大的改进空间。

Abstract: Large language models (LLMs) are equipped with increasingly extended context
windows recently, yet their long context understanding capabilities over long
dependency tasks remain fundamentally limited and underexplored. This gap is
especially significant in many real-world long-context applications that were
rarely benchmarked. In this paper, we introduce LooGLE v2, a novel benchmark
designed to evaluate LLMs' long context ability in real-world applications and
scenarios. Our benchmark consists of automatically collected real-world long
texts, ranging from 16k to 2M tokens, encompassing domains in law, finance,
game and code. Accordingly, we delicately design 10 types of domain-specific
long-dependency tasks and generate 1,934 QA instances with various diversity
and complexity in a scalable data curation pipeline for further practical
needs. We conduct a comprehensive assessment of 6 locally deployed and 4
API-based LLMs. The evaluation results show that even the best-performing model
achieves only a 59.2% overall score on our benchmark. Despite the extensive
context windows, popular LLMs are only capable of understanding a much shorter
length of context than they claim to be, revealing significant limitations in
their ability to handle real-world tasks with long dependencies and
highlighting substantial room for model improvement in practical long-context
understanding.

</details>


### [45] [SABlock: Semantic-Aware KV Cache Eviction with Adaptive Compression Block Size](https://arxiv.org/abs/2510.22556)
*Jinhan Chen,Jianchun Liu,Hongli Xu,Xianjun Gao,Shilong Wang*

Main category: cs.CL

TL;DR: SABlock是一个语义感知的KV缓存淘汰框架，通过自适应块大小来平衡语义连贯性和内存效率，显著减少长上下文LLM推理中的内存占用。


<details>
  <summary>Details</summary>
Motivation: KV缓存的内存占用已成为长上下文LLM推理的可扩展性瓶颈，现有的压缩方法难以平衡语义连贯性和内存效率。

Method: 首先进行语义分割以对齐压缩边界与语言结构，然后应用段引导的token评分来优化token重要性估计，最后通过预算驱动的搜索策略为每个段自适应确定最优块大小。

Result: 在Needle-in-a-Haystack测试中，仅用96个KV条目就达到99.9%的检索准确率，接近保留8K条目的全缓存基线性能。在固定缓存预算1024下，峰值内存使用减少46.28%，在128K上下文长度下解码速度提升高达9.5倍。

Conclusion: SABlock在相同内存预算下持续优于现有最先进基线，有效解决了长上下文LLM推理中的KV缓存内存瓶颈问题。

Abstract: The growing memory footprint of the Key-Value (KV) cache poses a severe
scalability bottleneck for long-context Large Language Model (LLM) inference.
While KV cache eviction has emerged as an effective solution by discarding less
critical tokens, existing token-, block-, and sentence-level compression
methods struggle to balance semantic coherence and memory efficiency. To this
end, we introduce SABlock, a \underline{s}emantic-aware KV cache eviction
framework with \underline{a}daptive \underline{block} sizes. Specifically,
SABlock first performs semantic segmentation to align compression boundaries
with linguistic structures, then applies segment-guided token scoring to refine
token importance estimation. Finally, for each segment, a budget-driven search
strategy adaptively determines the optimal block size that preserves semantic
integrity while improving compression efficiency under a given cache budget.
Extensive experiments on long-context benchmarks demonstrate that SABlock
consistently outperforms state-of-the-art baselines under the same memory
budgets. For instance, on Needle-in-a-Haystack (NIAH), SABlock achieves 99.9%
retrieval accuracy with only 96 KV entries, nearly matching the performance of
the full-cache baseline that retains up to 8K entries. Under a fixed cache
budget of 1,024, SABlock further reduces peak memory usage by 46.28% and
achieves up to 9.5x faster decoding on a 128K context length.

</details>


### [46] [A Closed-Loop Personalized Learning Agent Integrating Neural Cognitive Diagnosis, Bounded-Ability Adaptive Testing, and LLM-Driven Feedback](https://arxiv.org/abs/2510.22559)
*Zhifeng Wang,Xinyue Zheng,Chunyan Zeng*

Main category: cs.CL

TL;DR: 提出了一个端到端的个性化学习代理EduLoop-Agent，通过神经认知诊断模型、自适应测试策略和大型语言模型构建诊断-推荐-反馈的闭环框架，实现个性化学习路径生成。


<details>
  <summary>Details</summary>
Motivation: 解决现有教育技术中建模、题目选择和反馈相互孤立的问题，避免粗粒度学生模型、忽略诊断后验的自适应策略以及非针对性反馈的局限性。

Method: 集成神经认知诊断模型(NCD)进行细粒度知识掌握度评估，使用有界能力估计计算机自适应测试(BECAT)动态选择题目，利用大型语言模型(LLMs)生成结构化可操作反馈。

Result: 在ASSISTments数据集上，NCD模块在响应预测方面表现优异且提供可解释的掌握度评估，自适应推荐策略提高了题目相关性和个性化程度，LLM反馈能针对已识别的弱点提供学习指导。

Conclusion: 该设计有效且可实际部署，为智能教育中生成个性化学习轨迹提供了可行路径。

Abstract: As information technology advances, education is moving from
one-size-fits-all instruction toward personalized learning. However, most
methods handle modeling, item selection, and feedback in isolation rather than
as a closed loop. This leads to coarse or opaque student models,
assumption-bound adaptivity that ignores diagnostic posteriors, and generic,
non-actionable feedback. To address these limitations, this paper presents an
end-to-end personalized learning agent, EduLoop-Agent, which integrates a
Neural Cognitive Diagnosis model (NCD), a Bounded-Ability Estimation
Computerized Adaptive Testing strategy (BECAT), and large language models
(LLMs). The NCD module provides fine-grained estimates of students' mastery at
the knowledge-point level; BECAT dynamically selects subsequent items to
maximize relevance and learning efficiency; and LLMs convert diagnostic signals
into structured, actionable feedback. Together, these components form a
closed-loop framework of ``Diagnosis--Recommendation--Feedback.'' Experiments
on the ASSISTments dataset show that the NCD module achieves strong performance
on response prediction while yielding interpretable mastery assessments. The
adaptive recommendation strategy improves item relevance and personalization,
and the LLM-based feedback offers targeted study guidance aligned with
identified weaknesses. Overall, the results indicate that the proposed design
is effective and practically deployable, providing a feasible pathway to
generating individualized learning trajectories in intelligent education.

</details>


### [47] [Pedagogy-driven Evaluation of Generative AI-powered Intelligent Tutoring Systems](https://arxiv.org/abs/2510.22581)
*Kaushal Kumar Maurya,Ekaterina Kochmar*

Main category: cs.CL

TL;DR: 该论文分析了AI教育领域智能导学系统的评估挑战，指出当前缺乏可靠、统一的评估框架，并提出了基于学习科学原理的三个研究方向来建立公平、统一且可扩展的评估方法。


<details>
  <summary>Details</summary>
Motivation: 生成式AI模型的成功加速了基于大语言模型的智能导学系统发展，但这些系统的进展和影响难以追踪，因为缺乏可靠、普遍接受且以教学理论驱动的评估框架和基准。

Method: 通过回顾最先进的评估实践，结合真实案例研究分析相关挑战，并基于先前跨学科AI教育研究的见解提出解决方案。

Result: 识别了当前教育对话式智能导学系统评估主要依赖主观协议和非标准化基准，导致不一致性和有限泛化性的问题。

Conclusion: 提出了三个实用、可行且理论扎实的研究方向，旨在基于学习科学原理建立公平、统一和可扩展的智能导学系统评估方法。

Abstract: The interdisciplinary research domain of Artificial Intelligence in Education
(AIED) has a long history of developing Intelligent Tutoring Systems (ITSs) by
integrating insights from technological advancements, educational theories, and
cognitive psychology. The remarkable success of generative AI (GenAI) models
has accelerated the development of large language model (LLM)-powered ITSs,
which have potential to imitate human-like, pedagogically rich, and cognitively
demanding tutoring. However, the progress and impact of these systems remain
largely untraceable due to the absence of reliable, universally accepted, and
pedagogy-driven evaluation frameworks and benchmarks. Most existing educational
dialogue-based ITS evaluations rely on subjective protocols and
non-standardized benchmarks, leading to inconsistencies and limited
generalizability. In this work, we take a step back from mainstream ITS
development and provide comprehensive state-of-the-art evaluation practices,
highlighting associated challenges through real-world case studies from careful
and caring AIED research. Finally, building on insights from previous
interdisciplinary AIED research, we propose three practical, feasible, and
theoretically grounded research directions, rooted in learning science
principles and aimed at establishing fair, unified, and scalable evaluation
methodologies for ITSs.

</details>


### [48] [AutoBench: Automating LLM Evaluation through Reciprocal Peer Assessment](https://arxiv.org/abs/2510.22593)
*Dario Loi,Elena Maria Muià,Federico Siciliano,Giovanni Trappolini,Vincenzo Crisà,Peter Kruger,Fabrizio Silvestri*

Main category: cs.CL

TL;DR: AutoBench是一个通过相互同行评估来自动化评估大语言模型的框架，它动态生成评估任务，让模型交替扮演问题生成者、参赛者和评委的角色，并通过迭代加权机制聚合同行判断形成共识排名。


<details>
  <summary>Details</summary>
Motivation: 解决静态基准测试存在的测试集污染和适应性有限的问题，为持续评估不断演进的语言模型提供可扩展、抗污染的方法。

Method: 采用相互同行评估方法，模型在多样化领域中交替作为问题生成者、参赛者和评委，通过迭代加权机制放大可靠评估者的影响力，聚合同行判断形成共识排名。

Result: 实验显示与MMLU-Pro和GPQA基准测试分别有78%和63%的强相关性，多评委设计显著优于单评委基线，分布式评估产生更稳健且与人类一致的评估结果。

Conclusion: AutoBench为持续评估演进的语言模型提供了可扩展、抗污染的静态基准测试替代方案，验证了同行驱动评估范式的有效性。

Abstract: We present AutoBench, a fully automated and self-sustaining framework for
evaluating Large Language Models (LLMs) through reciprocal peer assessment.
This paper provides a rigorous scientific validation of the AutoBench
methodology, originally developed as an open-source project by eZecute S.R.L..
Unlike static benchmarks that suffer from test-set contamination and limited
adaptability, AutoBench dynamically generates novel evaluation tasks while
models alternately serve as question generators, contestants, and judges across
diverse domains. An iterative weighting mechanism amplifies the influence of
consistently reliable evaluators, aggregating peer judgments into
consensus-based rankings that reflect collective model agreement. Our
experiments demonstrate strong correlations with established benchmarks
including MMLU-Pro and GPQA (respectively 78\% and 63\%), validating this
peer-driven evaluation paradigm. The multi-judge design significantly
outperforms single-judge baselines, confirming that distributed evaluation
produces more robust and human-consistent assessments. AutoBench offers a
scalable, contamination-resistant alternative to static benchmarks for the
continuous evaluation of evolving language models.

</details>


### [49] [Personal Care Utility (PCU): Building the Health Infrastructure for Everyday Insight and Guidance](https://arxiv.org/abs/2510.22602)
*Mahyar Abbasian,Ramesh Jain*

Main category: cs.CL

TL;DR: 提出个人护理工具（PCU）——一个用于终身健康指导的智能系统，通过AI整合多模态数据、知识和服务，提供个性化健康信息、主动健康导航和医疗事件后的持续康复评估。


<details>
  <summary>Details</summary>
Motivation: 基于数字基础设施和生物医学创新的成功经验，构建一个能够持续协调多模态数据、知识和服务的全球性AI系统，为个人和群体提供终身健康指导。

Method: 采用多模态代理、事件中心建模和上下文推理技术，整合个人感知、体验计算和群体级分析，构建一个环境化、自适应的健康伴侣系统。

Result: PCU作为一个环境化、自适应的健康伴侣，能够实时观察、解释和指导日常生活中的健康状态，超越了传统的阶段性医疗模式。

Conclusion: PCU不仅能为个人改善健康结果，还为公共卫生和科学发现提供了新的基础平台，但需要解决架构设计原则和实施挑战。

Abstract: Building on decades of success in digital infrastructure and biomedical
innovation, we propose the Personal Care Utility (PCU) - a cybernetic system
for lifelong health guidance. PCU is conceived as a global, AI-powered utility
that continuously orchestrates multimodal data, knowledge, and services to
assist individuals and populations alike. Drawing on multimodal agents,
event-centric modeling, and contextual inference, it offers three essential
capabilities: (1) trusted health information tailored to the individual, (2)
proactive health navigation and behavior guidance, and (3) ongoing
interpretation of recovery and treatment response after medical events. Unlike
conventional episodic care, PCU functions as an ambient, adaptive companion -
observing, interpreting, and guiding health in real time across daily life. By
integrating personal sensing, experiential computing, and population-level
analytics, PCU promises not only improved outcomes for individuals but also a
new substrate for public health and scientific discovery. We describe the
architecture, design principles, and implementation challenges of this emerging
paradigm.

</details>


### [50] [PerCoR: Evaluating Commonsense Reasoning in Persian via Multiple-Choice Sentence Completion](https://arxiv.org/abs/2510.22616)
*Morteza Alikhani,Mohammadtaha Bagherifard,Erfan Zinvandi,Mehran Sarmadi*

Main category: cs.CL

TL;DR: PerCoR是首个大规模波斯语常识推理基准，包含10.6万个多项选择句子补全问题，采用新颖的连接词分割策略和DRESS-AF干扰项生成方法，在人类标注者达到89%准确率的同时，展示了波斯语常识推理任务仍具挑战性。


<details>
  <summary>Details</summary>
Motivation: 填补波斯语常识推理基准的空白，为波斯语自然语言处理研究提供大规模、多样化的评估资源。

Method: 使用连接词分割策略生成连贯的句子补全对，提出DRESS-AF方法通过嵌入相似性评分和对抗过滤从黄金续写池中选择干扰项。

Result: 人类标注者准确率89%，OpenAI-o3模型表现最佳（92.18%），最强开源模型DeepSeek-R1达到82.51%，表明数据集具有挑战性。DRESS-AF方法在英语HellaSwag基准上也有效提升了难度。

Conclusion: PerCoR为波斯语常识推理研究提供了有价值的基准，展示了当前模型在波斯语常识推理任务上仍有改进空间，且DRESS-AF方法具有跨语言适用性。

Abstract: We introduced PerCoR (Persian Commonsense Reasoning), the first large-scale
Persian benchmark for commonsense reasoning. PerCoR contains 106K
multiple-choice sentence-completion problems drawn from more than forty news,
cultural, and other web sources. We introduce a novel conjunction-based
segmentation strategy to generate coherent sentence-completion pairs, enabling
broad topical and structural diversity. To create challenging distractors, we
propose DRESS-AF (Distractor Ranking via Embedding Similarity Scoring and
Adversarial Filtering), a generation-free adversarial filtering method that
selects distractors from the pool of gold continuations while maximising model
confusion. Human annotators score 89% on PerCoR, while OpenAI-o3 achieves the
highest performance at 92.18%, followed closely by Claude-Sonnet-3.7 (91.17%).
The strongest open-source model, DeepSeek-R1, reaches 82.51%, underscoring both
the dataset's difficulty and the remaining performance gap in Persian
commonsense reasoning. We further show that DRESS-AF transfers to the English
HellaSwag benchmark, increasing its difficulty without hurting human
solvability. The dataset is available at
https://huggingface.co/datasets/MCINext/PerCoR.

</details>


### [51] [Integrating Linguistics and AI: Morphological Analysis and Corpus development of Endangered Toto Language of West Bengal](https://arxiv.org/abs/2510.22629)
*Ambalika Guha,Sajal Saha,Debanjan Ballav,Soumi Mitra,Hritwick Chakraborty*

Main category: cs.CL

TL;DR: 开发了一个三语（Toto-孟加拉语-英语）语言学习应用，通过AI技术和语言文档化来保护和推广濒危的Toto语言。


<details>
  <summary>Details</summary>
Motivation: 保护语言多样性，因为每种语言都提供了独特的世界观。Toto语言是印度西孟加拉邦的濒危语言，需要通过数字化手段进行存档和推广。

Method: 通过田野调查收集详细的语言文档，创建词素标记的三语语料库，训练小型语言模型和基于Transformer的翻译引擎，分析屈折形态和派生策略，开发文字标准化和数字扫盲工具。

Result: 开发了一个面向母语者和非母语学习者的语言学习应用，整合了Unicode文字和结构化语料库，建立了可持续的濒危语言保护模型。

Conclusion: 研究提供了一个将传统语言学方法与AI技术相结合的可持续模型，强调了跨学科合作在基于社区的语言复兴中的价值。

Abstract: Preserving linguistic diversity is necessary as every language offers a
distinct perspective on the world. There have been numerous global initiatives
to preserve endangered languages through documentation. This paper is a part of
a project which aims to develop a trilingual (Toto-Bangla-English) language
learning application to digitally archive and promote the endangered Toto
language of West Bengal, India. This application, designed for both native Toto
speakers and non-native learners, aims to revitalize the language by ensuring
accessibility and usability through Unicode script integration and a structured
language corpus. The research includes detailed linguistic documentation
collected via fieldwork, followed by the creation of a morpheme-tagged,
trilingual corpus used to train a Small Language Model (SLM) and a
Transformer-based translation engine. The analysis covers inflectional
morphology such as person-number-gender agreement, tense-aspect-mood
distinctions, and case marking, alongside derivational strategies that reflect
word-class changes. Script standardization and digital literacy tools were also
developed to enhance script usage. The study offers a sustainable model for
preserving endangered languages by incorporating traditional linguistic
methodology with AI. This bridge between linguistic research with technological
innovation highlights the value of interdisciplinary collaboration for
community-based language revitalization.

</details>


### [52] [Culturally Grounded Physical Commonsense Reasoning in Italian and English: A Submission to the MRL 2025 Shared Task](https://arxiv.org/abs/2510.22631)
*Marco De Santis,Lisa Alazraki*

Main category: cs.CL

TL;DR: FormaMentis是一个基于意大利语言和文化的物理常识推理新基准，为MRL 2025多语言物理推理数据集共享任务而开发


<details>
  <summary>Details</summary>
Motivation: 为英语以外的语言创建手动标注的物理常识推理评估数据，遵循类似PIQA的格式

Method: 由母语为意大利语且熟悉当地习俗的专家标注者创建数据样本，并将样本翻译成英语同时保留意大利独特的文化元素

Result: 开发了FormaMentis基准，包含基于意大利语言和文化的物理常识推理数据样本

Conclusion: 成功创建了一个扎根于意大利语言和文化的物理常识推理基准，为多语言物理推理研究做出了贡献

Abstract: This paper presents our submission to the MRL 2025 Shared Task on
Multilingual Physical Reasoning Datasets. The objective of the shared task is
to create manually-annotated evaluation data in the physical commonsense
reasoning domain, for languages other than English, following a format similar
to PIQA. Our contribution, FormaMentis, is a novel benchmark for physical
commonsense reasoning that is grounded in Italian language and culture. The
data samples in FormaMentis are created by expert annotators who are native
Italian speakers and are familiar with local customs and norms. The samples are
additionally translated into English, while preserving the cultural elements
unique to the Italian context.

</details>


### [53] [Conjugate Relation Modeling for Few-Shot Knowledge Graph Completion](https://arxiv.org/abs/2510.22656)
*Zilong Wang,Qingtian Zeng,Hua Duan,Cheng Cheng,Minghao Zou,Ziyang Wang*

Main category: cs.CL

TL;DR: 提出CR-FKGC框架，通过共轭关系建模解决少样本知识图谱补全中的复杂关系模式和数据稀疏问题，在三个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有少样本知识图谱补全方法难以捕捉复杂关系模式并缓解数据稀疏性，需要新的解决方案。

Method: 使用邻域聚合编码器整合高阶邻居信息，共轭关系学习器结合隐式条件扩散关系模块和稳定关系模块来捕获稳定语义和不确定性偏移，以及流形共轭解码器在流形空间中进行高效评估和推理。

Result: 在三个基准测试中，该方法实现了优于最先进方法的性能。

Conclusion: CR-FKGC框架通过共轭关系建模有效解决了少样本知识图谱补全中的关键挑战，展现了优越性能。

Abstract: Few-shot Knowledge Graph Completion (FKGC) infers missing triples from
limited support samples, tackling long-tail distribution challenges. Existing
methods, however, struggle to capture complex relational patterns and mitigate
data sparsity. To address these challenges, we propose a novel FKGC framework
for conjugate relation modeling (CR-FKGC). Specifically, it employs a
neighborhood aggregation encoder to integrate higher-order neighbor
information, a conjugate relation learner combining an implicit conditional
diffusion relation module with a stable relation module to capture stable
semantics and uncertainty offsets, and a manifold conjugate decoder for
efficient evaluation and inference of missing triples in manifold space.
Experiments on three benchmarks demonstrate that our method achieves superior
performance over state-of-the-art methods.

</details>


### [54] [Rule-Based Explanations for Retrieval-Augmented LLM Systems](https://arxiv.org/abs/2510.22689)
*Joel Rorseth,Parke Godfrey,Lukasz Golab,Divesh Srivastava,Jarek Szlichta*

Main category: cs.CL

TL;DR: 提出首个用于解释检索增强生成(RAG)大语言模型的规则生成方法，通过分析检索源与输出之间的关系生成if-then规则，并优化了规则生成效率。


<details>
  <summary>Details</summary>
Motivation: 传统if-then规则广泛用于解释机器学习模型，但尚未应用于新兴的RAG大语言模型。RAG系统在推理时整合检索信息，需要解释输出与检索源之间的关联。

Method: 通过探测LLM对所有源组合的响应来生成规则，提出基于Apriori剪枝思想的优化方法，加速规则生成过程。

Result: 通过定性和定量实验验证了解决方案的价值和效率，能够有效解释RAG系统的输出来源。

Conclusion: 成功将规则解释方法扩展到RAG大语言模型，提供了解释输出来源的有效工具，并通过优化方法提高了规则生成效率。

Abstract: If-then rules are widely used to explain machine learning models; e.g., "if
employed = no, then loan application = rejected." We present the first proposal
to apply rules to explain the emerging class of large language models (LLMs)
with retrieval-augmented generation (RAG). Since RAG enables LLM systems to
incorporate retrieved information sources at inference time, rules linking the
presence or absence of sources can explain output provenance; e.g., "if a Times
Higher Education ranking article is retrieved, then the LLM ranks Oxford
first." To generate such rules, a brute force approach would probe the LLM with
all source combinations and check if the presence or absence of any sources
leads to the same output. We propose optimizations to speed up rule generation,
inspired by Apriori-like pruning from frequent itemset mining but redefined
within the scope of our novel problem. We conclude with qualitative and
quantitative experiments demonstrating our solutions' value and efficiency.

</details>


### [55] [SALSA: Single-pass Autoregressive LLM Structured Classification](https://arxiv.org/abs/2510.22691)
*Ruslan Berdichevsky,Shai Nahum-Gefen,Elad Ben Zaken*

Main category: cs.CL

TL;DR: SALSA是一个结合结构化提示、类别到标记映射和参数高效微调的文本分类方法，避免了冷启动训练，在单次前向传播中实现高效准确的分类。


<details>
  <summary>Details</summary>
Motivation: 尽管指令调优的大型语言模型具有强大的泛化能力，但在文本分类基准测试中表现不佳，需要改进其分类性能。

Method: 将每个类别标签映射到不同的输出标记，构建提示以引出单标记响应，在推理时仅将模型输出投影到相关类别标记的logits上。

Result: SALSA在各种基准测试中取得了最先进的结果，证明了其在基于LLM的分类应用中的鲁棒性和可扩展性。

Conclusion: SALSA通过结构化提示和参数高效微调，显著提升了LLM在文本分类任务上的性能，为LLM分类应用提供了有效的解决方案。

Abstract: Despite their impressive generalization capabilities, instruction-tuned Large
Language Models often underperform on text classification benchmarks. We
introduce SALSA, a coherent pipeline that combines structured prompting,
class-to-token mapping, and parameter-efficient fine-tuning, thereby avoiding
cold-start training. Each class label is mapped to a distinct output token, and
prompts are constructed to elicit a single-token response. During inference,
the model's output is projected only onto the logits of the relevant class
tokens, enabling efficient and accurate classification in a single forward
pass. SALSA achieves state-of-the-art results across diverse benchmarks,
demonstrating its robustness and scalability for LLM-based classification
applications.

</details>


### [56] [$\text{E}^2\text{Rank}$: Your Text Embedding can Also be an Effective and Efficient Listwise Reranker](https://arxiv.org/abs/2510.22733)
*Qi Liu,Yanzhao Zhang,Mingxin Li,Dingkun Long,Pengjun Xie,Jiaxin Mao*

Main category: cs.CL

TL;DR: 提出了E²Rank框架，通过继续训练将单个文本嵌入模型扩展到同时执行高质量检索和列表重排序，在保持效率的同时显著提升重排序性能。


<details>
  <summary>Details</summary>
Motivation: 现有文本嵌入模型在检索应用中效率高但排序保真度有限，而专门的LLM列表重排序器虽然效果好但效率较低。需要统一框架同时兼顾效率和效果。

Method: 使用查询-文档嵌入的余弦相似度作为统一排序函数，构建包含候选文档信号的列表排序提示，在列表排序目标下继续训练嵌入模型。

Result: 在BEIR重排序基准上达到SOTA，在BRIGHT推理基准上表现竞争性，重排序延迟很低，且在MTEB基准上嵌入性能也有提升。

Conclusion: 单个嵌入模型可以有效统一检索和重排序，提供计算效率和竞争性排序精度的平衡。

Abstract: Text embedding models serve as a fundamental component in real-world search
applications. By mapping queries and documents into a shared embedding space,
they deliver competitive retrieval performance with high efficiency. However,
their ranking fidelity remains limited compared to dedicated rerankers,
especially recent LLM-based listwise rerankers, which capture fine-grained
query-document and document-document interactions. In this paper, we propose a
simple yet effective unified framework $\text{E}^2\text{Rank}$, means Efficient
Embedding-based Ranking (also means Embedding-to-Rank), which extends a single
text embedding model to perform both high-quality retrieval and listwise
reranking through continued training under a listwise ranking objective,
thereby achieving strong effectiveness with remarkable efficiency. By applying
cosine similarity between the query and document embeddings as a unified
ranking function, the listwise ranking prompt, which is constructed from the
original query and its candidate documents, serves as an enhanced query
enriched with signals from the top-K documents, akin to pseudo-relevance
feedback (PRF) in traditional retrieval models. This design preserves the
efficiency and representational quality of the base embedding model while
significantly improving its reranking performance. Empirically,
$\textrm{E}^2\text{Rank}$ achieves state-of-the-art results on the BEIR
reranking benchmark and demonstrates competitive performance on the
reasoning-intensive BRIGHT benchmark, with very low reranking latency. We also
show that the ranking training process improves embedding performance on the
MTEB benchmark. Our findings indicate that a single embedding model can
effectively unify retrieval and reranking, offering both computational
efficiency and competitive ranking accuracy.

</details>


### [57] [Low-Resource Dialect Adaptation of Large Language Models: A French Dialect Case-Study](https://arxiv.org/abs/2510.22747)
*Eeham Khan,Firas Saidani,Owen Van Esbroeck,Richard Khoury,Leila Kosseim*

Main category: cs.CL

TL;DR: 使用低秩适应和计算高效的持续预训练方法，在有限数据和计算预算下将大语言模型适配到魁北克法语方言，显著提升方言性能且对标准法语性能影响很小。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型主要局限于高资源语言的问题，探索如何在有限数据和计算资源下将模型适配到低资源方言，为少数语言群体提供高质量LLM访问。

Method: 采用低秩适应和计算高效的持续预训练方法，使用小型数据集对三个LLM进行魁北克法语适配，在COLE测试套件上进行评估。

Result: 方言基准测试显著改善，标准语言基准测试回归极小，仅更新不到1%的模型参数。结果显示性能提升高度依赖语料库组成。

Conclusion: 参数高效微调的持续预训练能够以成本效益高的方式缩小方言差距，为少数语言群体扩展高质量LLM访问，促进可持续语言资源创建。

Abstract: Despite the widespread adoption of large language models (LLMs), their
strongest capabilities remain largely confined to a small number of
high-resource languages for which there is abundant training data. Recently,
continual pre-training (CPT) has emerged as a means to fine-tune these models
to low-resource regional dialects. In this paper, we study the use of CPT for
dialect learning under tight data and compute budgets. Using low-rank
adaptation (LoRA) and compute-efficient continual pre-training, we adapt three
LLMs to the Qu\'ebec French dialect using a very small dataset and benchmark
them on the COLE suite. Our experiments demonstrate an improvement on the
minority dialect benchmarks with minimal regression on the prestige language
benchmarks with under 1% of model parameters updated. Analysis of the results
demonstrate that gains are highly contingent on corpus composition. These
findings indicate that CPT with parameter-efficient fine-tuning (PEFT) can
narrow the dialect gap by providing cost-effective and sustainable language
resource creation, expanding high-quality LLM access to minority linguistic
communities. We release the first Qu\'ebec French LLMs on HuggingFace.

</details>


### [58] [Beyond Semantics: How Temporal Biases Shape Retrieval in Transformer and State-Space Models](https://arxiv.org/abs/2510.22752)
*Anooshka Bajaj,Deven Mahesh Mistry,Sahaj Singh Maini,Yash Aggarwal,Zoran Tiganj*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型在上下文学习中的时间偏见，发现模型在预测下一个标记时倾向于选择序列开头或结尾附近重复出现的标记，类似于人类情景记忆的时间分离机制。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型如何通过时间和语义关系来检索上下文信息，类似于人类情景记忆中对不同时间发生事件的分离和检索能力。

Method: 通过固定重复标记位置并置换其他标记，消除语义干扰，隔离时间效应对下一个标记预测的影响。使用包含多个相同标记呈现的序列进行测试，并分析transformer和状态空间模型的表现。

Result: 模型始终对重复标记后的标记赋予最高概率，但存在对输入序列开头或结尾附近标记的明显偏见。transformer模型中的归纳头被证明与此现象相关。中间位置的记忆检索可靠性较低。

Conclusion: 研究加深了对上下文学习中时间偏见的理解，展示了这些偏见如何实现时间分离和情景检索，transformer和状态空间模型在时间偏见方面表现相似。

Abstract: In-context learning is governed by both temporal and semantic relationships,
shaping how Large Language Models (LLMs) retrieve contextual information.
Analogous to human episodic memory, where the retrieval of specific events is
enabled by separating events that happened at different times, this work probes
the ability of various pretrained LLMs, including transformer and state-space
models, to differentiate and retrieve temporally separated events.
Specifically, we prompted models with sequences containing multiple
presentations of the same token, which reappears at the sequence end. By fixing
the positions of these repeated tokens and permuting all others, we removed
semantic confounds and isolated temporal effects on next-token prediction.
Across diverse sequences, models consistently placed the highest probabilities
on tokens following a repeated token, but with a notable bias for those nearest
the beginning or end of the input. An ablation experiment linked this
phenomenon in transformers to induction heads. Extending the analysis to unique
semantic contexts with partial overlap further demonstrated that memories
embedded in the middle of a prompt are retrieved less reliably. Despite
architectural differences, state-space and transformer models showed comparable
temporal biases. Our findings deepen the understanding of temporal biases in
in-context learning and offer an illustration of how these biases can enable
temporal separation and episodic retrieval.

</details>


### [59] [EchoMind: An Interrelated Multi-level Benchmark for Evaluating Empathetic Speech Language Models](https://arxiv.org/abs/2510.22758)
*Li Zhou,Lutong Yu,You Lyu,Yihang Lin,Zefeng Zhao,Junyi Ao,Yuhao Zhang,Benyou Wang,Haizhou Li*

Main category: cs.CL

TL;DR: EchoMind是首个多层级基准测试，通过顺序任务模拟共情对话的认知过程，评估语音语言模型在理解口语内容、感知声音线索、整合推理和生成回应方面的能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试通常孤立评估语言、声学、推理或对话能力，忽视了这些技能整合对于实现类人、情感智能对话的重要性。

Method: 设计EchoMind基准测试，包含口语内容理解、声音线索感知、整合推理和回应生成四个顺序任务，使用语义中性脚本和受控的声音风格变化。

Result: 测试12个先进SLM发现，即使最先进的模型在处理高表达性声音线索方面仍有困难，限制了共情回应的质量。

Conclusion: SLM需要整合语言内容和多样化声音线索，才能实现真正的共情对话能力。

Abstract: Speech Language Models (SLMs) have made significant progress in spoken
language understanding. Yet it remains unclear whether they can fully perceive
non lexical vocal cues alongside spoken words, and respond with empathy that
aligns with both emotional and contextual factors. Existing benchmarks
typically evaluate linguistic, acoustic, reasoning, or dialogue abilities in
isolation, overlooking the integration of these skills that is crucial for
human-like, emotionally intelligent conversation. We present EchoMind, the
first interrelated, multi-level benchmark that simulates the cognitive process
of empathetic dialogue through sequential, context-linked tasks: spoken-content
understanding, vocal-cue perception, integrated reasoning, and response
generation. All tasks share identical and semantically neutral scripts that are
free of explicit emotional or contextual cues, and controlled variations in
vocal style are used to test the effect of delivery independent of the
transcript. EchoMind is grounded in an empathy-oriented framework spanning 3
coarse and 12 fine-grained dimensions, encompassing 39 vocal attributes, and
evaluated using both objective and subjective metrics. Testing 12 advanced SLMs
reveals that even state-of-the-art models struggle with high-expressive vocal
cues, limiting empathetic response quality. Analyses of prompt strength, speech
source, and ideal vocal cue recognition reveal persistent weaknesses in
instruction-following, resilience to natural speech variability, and effective
use of vocal cues for empathy. These results underscore the need for SLMs that
integrate linguistic content with diverse vocal cues to achieve truly
empathetic conversational ability.

</details>


### [60] [Iterative Layer Pruning for Efficient Translation Inference](https://arxiv.org/abs/2510.22763)
*Yasmin Moslem,Muhammad Hazim Al Farouq,John D. Kelleher*

Main category: cs.CL

TL;DR: 本文研究了基于层重要性分析的迭代层剪枝方法，用于压缩大语言模型在机器翻译任务中的部署，在保持翻译质量的同时显著减小模型大小和推理时间。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在机器翻译等自然语言处理任务中表现出色，但其巨大的计算需求使得高效部署面临挑战。本文旨在解决这一挑战，探索模型压缩方法以降低部署成本。

Method: 采用迭代层剪枝方法，通过层重要性分析来指导剪枝过程。使用Aya-Expanse-8B模型在捷克语到德语、英语到埃及阿拉伯语的翻译任务上进行实验验证。

Result: 该方法实现了显著的模型大小缩减和推理时间减少，同时保持了基线模型的翻译质量。

Conclusion: 基于层重要性分析的迭代层剪枝是一种有效的模型压缩方法，能够在保持性能的同时大幅降低大语言模型在机器翻译任务中的计算需求。

Abstract: Large language models (LLMs) have transformed many areas of natural language
processing, including machine translation. However, efficient deployment of
LLMs remains challenging due to their intensive computational requirements. In
this paper, we address this challenge and present our submissions to the Model
Compression track at the Conference on Machine Translation (WMT 2025). In our
experiments, we investigate iterative layer pruning guided by layer importance
analysis. We evaluate this method using the Aya-Expanse-8B model for
translation from Czech to German, and from English to Egyptian Arabic. Our
approach achieves substantial reductions in model size and inference time,
while maintaining the translation quality of the baseline models.

</details>


### [61] [MMPersuade: A Dataset and Evaluation Framework for Multimodal Persuasion](https://arxiv.org/abs/2510.22768)
*Haoyi Qiu,Yilun Zhou,Pranav Narayanan Venkit,Kung-Hsiang Huang,Jiaxin Zhang,Nanyun Peng,Chien-Sheng Wu*

Main category: cs.CL

TL;DR: MMPersuade是一个研究大型视觉语言模型在多媒体说服内容中易感性的框架，包含多模态数据集和评估方法，发现多媒体输入显著增强说服效果，不同说服策略在不同情境下效果各异。


<details>
  <summary>Details</summary>
Motivation: 随着大型视觉语言模型在购物、健康、新闻等领域的部署，它们面临大量说服性内容。了解模型如何被多模态说服内容影响至关重要，因为过度易感的模型可能采纳误导性信念、覆盖用户偏好或生成不道德输出。

Method: 提出MMPersuade框架，包括：(i)包含图像和视频的多模态数据集，涵盖商业、主观行为和对抗性情境中的说服原则；(ii)通过第三方协议评分和自估计令牌概率量化说服效果和模型易感性的评估框架。

Result: 对六个领先LVLM的研究发现：(i)多模态输入相比纯文本显著增强说服效果和模型易感性，特别是在错误信息场景中；(ii)陈述的先前偏好降低易感性，但多模态信息仍保持说服优势；(iii)不同策略在不同情境下效果不同，互惠在商业和主观情境中最有效，可信度和逻辑在对抗性情境中占主导。

Conclusion: 通过联合分析说服效果和易感性，MMPersuade为开发在面对说服性多模态内容时具有鲁棒性、偏好一致性和道德对齐的模型提供了原则性基础。

Abstract: As Large Vision-Language Models (LVLMs) are increasingly deployed in domains
such as shopping, health, and news, they are exposed to pervasive persuasive
content. A critical question is how these models function as persuadees-how and
why they can be influenced by persuasive multimodal inputs. Understanding both
their susceptibility to persuasion and the effectiveness of different
persuasive strategies is crucial, as overly persuadable models may adopt
misleading beliefs, override user preferences, or generate unethical or unsafe
outputs when exposed to manipulative messages. We introduce MMPersuade, a
unified framework for systematically studying multimodal persuasion dynamics in
LVLMs. MMPersuade contributes (i) a comprehensive multimodal dataset that pairs
images and videos with established persuasion principles across commercial,
subjective and behavioral, and adversarial contexts, and (ii) an evaluation
framework that quantifies both persuasion effectiveness and model
susceptibility via third-party agreement scoring and self-estimated token
probabilities on conversation histories. Our study of six leading LVLMs as
persuadees yields three key insights: (i) multimodal inputs substantially
increase persuasion effectiveness-and model susceptibility-compared to text
alone, especially in misinformation scenarios; (ii) stated prior preferences
decrease susceptibility, yet multimodal information maintains its persuasive
advantage; and (iii) different strategies vary in effectiveness across
contexts, with reciprocity being most potent in commercial and subjective
contexts, and credibility and logic prevailing in adversarial contexts. By
jointly analyzing persuasion effectiveness and susceptibility, MMPersuade
provides a principled foundation for developing models that are robust,
preference-consistent, and ethically aligned when engaging with persuasive
multimodal content.

</details>


### [62] [Scalable Supervising Software Agents with Patch Reasoner](https://arxiv.org/abs/2510.22775)
*Junjielong Xu,Boyin Tan,Xiaoyuan Liu,Chao Peng,Pengfei Gao,Pinjia He*

Main category: cs.CL

TL;DR: R4P是一个基于推理的补丁验证模型，为软件工程代理提供可扩展的奖励机制，解决了传统基于测试的监督方法在可扩展性和稳定性方面的限制。


<details>
  <summary>Details</summary>
Motivation: 现有基于测试的监督方法存在两个问题：(1) 构建和运行测试沙箱既繁重又脆弱；(2) 具有高覆盖率测试的数据稀缺且容易受到边缘案例测试攻击的威胁。这限制了数据扩展的潜力。

Method: R4P将补丁验证视为推理任务，模仿人类仓库维护者在不编写和运行新测试的情况下审查补丁。采用分组目标进行强化学习训练，能够验证多个补丁并相互比较修改，获得密集奖励以实现稳定训练。

Result: R4P在SWE-bench-verified上达到72.2%的补丁验证准确率，超过OpenAI o3。基于R4P训练的Mini-SE在SWE-bench-verified上达到26.2%的Pass@1，比原始Qwen3-32B提高10.0%。使用R4P进行测试时扩展可进一步提升至32.8%。R4P验证补丁仅需1秒，比平均测试速度快50倍。

Conclusion: R4P通过推理驱动的补丁验证提供了可扩展、高效的奖励机制，显著提升了软件工程代理的性能，同时避免了传统测试方法的可扩展性限制。

Abstract: While large language model agents have advanced software engineering tasks,
the unscalable nature of existing test-based supervision is limiting the
potential improvement of data scaling. The reason is twofold: (1) building and
running test sandbox is rather heavy and fragile, and (2) data with
high-coverage tests is naturally rare and threatened by test hacking via edge
cases. In this paper, we propose R4P, a patch verifier model to provide
scalable rewards for training and testing SWE agents via reasoning. We consider
that patch verification is fundamentally a reasoning task, mirroring how human
repository maintainers review patches without writing and running new
reproduction tests. To obtain sufficient reference and reduce the risk of
reward hacking, R4P uses a group-wise objective for RL training, enabling it to
verify multiple patches against each other's modification and gain a dense
reward for stable training. R4P achieves 72.2% Acc. for verifying patches from
SWE-bench-verified, surpassing OpenAI o3. To demonstrate R4P's practicality, we
design and train a lite scaffold, Mini-SE, with pure reinforcement learning
where all rewards are derived from R4P. As a result, Mini-SE achieves 26.2%
Pass@1 on SWE-bench-verified, showing a 10.0% improvement over the original
Qwen3-32B. This can be further improved to 32.8% with R4P for test-time
scaling. Furthermore, R4P verifies patches within a second, 50x faster than
testing on average. The stable scaling curves of rewards and accuracy along
with high efficiency reflect R4P's practicality.

</details>


### [63] [VEHME: A Vision-Language Model For Evaluating Handwritten Mathematics Expressions](https://arxiv.org/abs/2510.22798)
*Thu Phuong Nguyen,Duc M. Nguyen,Hyotaek Jeon,Hyunwook Lee,Hyunmin Song,Sungahn Ko,Taehwan Kim*

Main category: cs.CL

TL;DR: VEHME是一个用于评估手写数学表达式的视觉语言模型，通过两阶段训练流程和表达式感知视觉提示模块，在开放形式手写数学答案评估中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 手写数学解决方案的自动评估在教育技术中很重要，但由于学生作业格式多样、布局无结构和符号复杂，这仍然是一个重大挑战。

Method: 采用两阶段训练流程：监督微调使用结构化推理数据，强化学习对齐多维评分目标；提出表达式感知视觉提示模块增强空间理解。

Result: 在AIHub和FERMAT数据集上的评估显示，VEHME在开源模型中达到最先进性能，接近专有系统的准确性。

Conclusion: VEHME展示了作为可扩展和可访问的自动数学评估工具的潜力，训练和实验代码已公开。

Abstract: Automatically assessing handwritten mathematical solutions is an important
problem in educational technology with practical applications, but it remains a
significant challenge due to the diverse formats, unstructured layouts, and
symbolic complexity of student work. To address this challenge, we introduce
VEHME-a Vision-Language Model for Evaluating Handwritten Mathematics
Expressions-designed to assess open-form handwritten math responses with high
accuracy and interpretable reasoning traces. VEHME integrates a two-phase
training pipeline: (i) supervised fine-tuning using structured reasoning data,
and (ii) reinforcement learning that aligns model outputs with
multi-dimensional grading objectives, including correctness, reasoning depth,
and error localization. To enhance spatial understanding, we propose an
Expression-Aware Visual Prompting Module, trained on our synthesized multi-line
math expressions dataset to robustly guide attention in visually heterogeneous
inputs. Evaluated on AIHub and FERMAT datasets, VEHME achieves state-of-the-art
performance among open-source models and approaches the accuracy of proprietary
systems, demonstrating its potential as a scalable and accessible tool for
automated math assessment. Our training and experiment code is publicly
available at our GitHub repository.

</details>


### [64] [Cross-Lingual Stability and Bias in Instruction-Tuned Language Models for Humanitarian NLP](https://arxiv.org/abs/2510.22823)
*Poli Nemkova,Amrit Adhikari,Matthew Pearson,Vamsi Krishna Sadu,Mark V. Albert*

Main category: cs.CL

TL;DR: 本文首次系统比较了商业和开源大语言模型在7种语言上的人权侵犯检测性能，发现模型对齐（而非规模）是跨语言稳定性的关键因素。


<details>
  <summary>Details</summary>
Motivation: 人道主义组织面临关键选择：投资昂贵的商业API还是依赖免费开源模型进行多语言人权监控。商业系统可靠但昂贵，开源替代品缺乏实证验证，特别是在冲突地区常见的低资源语言上。

Method: 在78,000次多语言推理中评估了6个模型（4个指令对齐模型和2个开源模型），使用标准分类指标和新的跨语言可靠性度量：校准偏差、决策偏差、语言鲁棒性分数和语言稳定性分数。

Result: 对齐模型在类型学距离远和低资源语言（如林加拉语、缅甸语）上保持近乎不变的准确性和平衡校准，而开源模型表现出显著的提示语言敏感性和校准漂移。

Conclusion: 多语言对齐实现了语言无关的推理，为人道主义组织在多语言部署中平衡预算约束与可靠性提供了实用指导。

Abstract: Humanitarian organizations face a critical choice: invest in costly
commercial APIs or rely on free open-weight models for multilingual human
rights monitoring. While commercial systems offer reliability, open-weight
alternatives lack empirical validation -- especially for low-resource languages
common in conflict zones. This paper presents the first systematic comparison
of commercial and open-weight large language models (LLMs) for
human-rights-violation detection across seven languages, quantifying the
cost-reliability trade-off facing resource-constrained organizations. Across
78,000 multilingual inferences, we evaluate six models -- four
instruction-aligned (Claude-Sonnet-4, DeepSeek-V3, Gemini-Flash-2.0,
GPT-4.1-mini) and two open-weight (LLaMA-3-8B, Mistral-7B) -- using both
standard classification metrics and new measures of cross-lingual reliability:
Calibration Deviation (CD), Decision Bias (B), Language Robustness Score (LRS),
and Language Stability Score (LSS). Results show that alignment, not scale,
determines stability: aligned models maintain near-invariant accuracy and
balanced calibration across typologically distant and low-resource languages
(e.g., Lingala, Burmese), while open-weight models exhibit significant
prompt-language sensitivity and calibration drift. These findings demonstrate
that multilingual alignment enables language-agnostic reasoning and provide
practical guidance for humanitarian organizations balancing budget constraints
with reliability in multilingual deployment.

</details>


### [65] [Exploration of Summarization by Generative Language Models for Automated Scoring of Long Essays](https://arxiv.org/abs/2510.22830)
*Haowei Hua,Hong Jiao,Xinyi Wang*

Main category: cs.CL

TL;DR: 使用生成式语言模型通过摘要和提示技术改进长文本自动评分，解决了BERT等编码器模型512词长限制的问题，在自动作文评分数据集上QWK从0.822提升至0.8878。


<details>
  <summary>Details</summary>
Motivation: BERT及其变体在自动评分中被广泛探索，但这些编码器模型的512词长限制在长文本自动评分中显示出不足。

Method: 探索生成式语言模型用于长文本自动评分，通过摘要和提示技术。

Result: 在Learning Agency Lab自动作文评分2.0数据集上，评分准确性显著提升，QWK从0.822增加到0.8878。

Conclusion: 生成式语言模型通过摘要和提示技术能有效改进长文本自动评分性能。

Abstract: BERT and its variants are extensively explored for automated scoring.
However, a limit of 512 tokens for these encoder-based models showed the
deficiency in automated scoring of long essays. Thus, this research explores
generative language models for automated scoring of long essays via
summarization and prompting. The results revealed great improvement of scoring
accuracy with QWK increased from 0.822 to 0.8878 for the Learning Agency Lab
Automated Essay Scoring 2.0 dataset.

</details>


### [66] [Leveraging Large Language Models to Identify Conversation Threads in Collaborative Learning](https://arxiv.org/abs/2510.22844)
*Prerna Ravi,Dong Won Lee,Beatriz Flamia,Jasmine David,Brandon Hanks,Cynthia Breazeal,Emma Anderson,Grace Lin*

Main category: cs.CL

TL;DR: 该研究探讨了在小组对话中明确提供对话线索如何提升大语言模型对协作学习关系的编码性能，开发了同步多参与者对话的线索识别方法，并验证了线索信息对下游对话分析任务的重要性。


<details>
  <summary>Details</summary>
Motivation: 小组对话中的线索结构对理解协作学习至关重要，但在同步口语对话中检测线索具有挑战性。虽然大语言模型在话语分析方面有潜力，但在需要追踪长上下文对话链接的任务中表现不佳。

Method: 开发了同步多参与者转录本中识别线索的系统指南，比较了不同LLM提示策略的自动线索检测性能，测试了线索信息对下游对话分析框架编码的影响。

Result: 提供清晰的对话线索信息显著提高了LLM编码性能，突显了下游分析对良好结构化对话的重度依赖。同时讨论了时间与成本的实际权衡。

Conclusion: 这项工作推进了将LLM与稳健的对话线索结构相结合的方法，以理解复杂的实时群体互动，并强调了人机混合方法的最佳价值。

Abstract: Understanding how ideas develop and flow in small-group conversations is
critical for analyzing collaborative learning. A key structural feature of
these interactions is threading, the way discourse talk naturally organizes
into interwoven topical strands that evolve over time. While threading has been
widely studied in asynchronous text settings, detecting threads in synchronous
spoken dialogue remains challenging due to overlapping turns and implicit cues.
At the same time, large language models (LLMs) show promise for automating
discourse analysis but often struggle with long-context tasks that depend on
tracing these conversational links. In this paper, we investigate whether
explicit thread linkages can improve LLM-based coding of relational moves in
group talk. We contribute a systematic guidebook for identifying threads in
synchronous multi-party transcripts and benchmark different LLM prompting
strategies for automated threading. We then test how threading influences
performance on downstream coding of conversational analysis frameworks, that
capture core collaborative actions such as agreeing, building, and eliciting.
Our results show that providing clear conversational thread information
improves LLM coding performance and underscores the heavy reliance of
downstream analysis on well-structured dialogue. We also discuss practical
trade-offs in time and cost, emphasizing where human-AI hybrid approaches can
yield the best value. Together, this work advances methods for combining LLMs
and robust conversational thread structures to make sense of complex, real-time
group interactions.

</details>


### [67] [Once Upon an Input: Reasoning via Per-Instance Program Synthesis](https://arxiv.org/abs/2510.22849)
*Adam Stein,Neelay Velingker,Mayur Naik,Eric Wong*

Main category: cs.CL

TL;DR: PIPS是一种针对LLMs的实例级程序合成方法，通过结构反馈生成和优化程序，结合置信度指标动态选择直接推理或程序合成，显著提升复杂推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法如CoT和PoT在增强LLMs的多步推理能力时，特别是在算法领域，经常产生不理想的解决方案，需要改进。

Method: 提出PIPS方法，在实例级别生成和优化程序，使用结构反馈而不依赖任务特定指导或显式测试用例，并引入置信度指标动态选择推理策略。

Result: 在3个前沿LLM和30个基准测试中，PIPS相比PoT和CoT分别提升绝对调和平均准确率8.6%和9.4%，在算法任务中减少65.1%的不良程序生成。

Conclusion: PIPS通过实例级程序合成和动态策略选择，有效提升了LLMs在复杂推理任务中的性能，减少了不良解决方案的产生。

Abstract: Large language models (LLMs) excel at zero-shot inference but continue to
struggle with complex, multi-step reasoning. Recent methods that augment LLMs
with intermediate reasoning steps such as Chain of Thought (CoT) and Program of
Thought (PoT) improve performance but often produce undesirable solutions,
especially in algorithmic domains. We introduce Per-Instance Program Synthesis
(PIPS), a method that generates and refines programs at the instance-level
using structural feedback without relying on task-specific guidance or explicit
test cases. To further improve performance, PIPS incorporates a confidence
metric that dynamically chooses between direct inference and program synthesis
on a per-instance basis. Experiments across three frontier LLMs and 30
benchmarks including all tasks of Big Bench Extra Hard (BBEH), visual question
answering tasks, relational reasoning tasks, and mathematical reasoning tasks
show that PIPS improves the absolute harmonic mean accuracy by up to 8.6% and
9.4% compared to PoT and CoT respectively, and reduces undesirable program
generations by 65.1% on the algorithmic tasks compared to PoT with
Gemini-2.0-Flash.

</details>


### [68] [Far from the Shallow: Brain-Predictive Reasoning Embedding through Residual Disentanglement](https://arxiv.org/abs/2510.22860)
*Linyang He,Tianjun Zhong,Richard Antonello,Gavin Mischler,Micah Goldblum,Nima Mesgarani*

Main category: cs.CL

TL;DR: 提出了一种残差解缠方法，从语言模型中分离出词汇、句法、语义和推理四个正交嵌入，用于建模脑电信号，揭示了推理过程的独特神经特征。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型内部表征高度"纠缠"，混合了词汇、句法、语义和推理信息，这导致脑编码分析偏向浅层语言特征，难以分离深层认知过程的神经基础。

Method: 引入残差解缠方法：首先探测语言模型识别特征特定层，然后迭代回归掉低层表征，生成四个近乎正交的嵌入（词汇、句法、语义、推理）。

Result: 1) 推理嵌入具有独特预测力，能解释其他语言特征无法解释的神经活动方差；2) 推理神经信号时间上更晚出现（350-400ms）；3) 标准语言模型嵌入会误导分析，其预测成功主要归因于浅层语言特征。

Conclusion: 该方法成功分离了推理的神经表征，揭示了其在处理层次中的高级地位，并表明标准语言模型嵌入会掩盖深层认知过程的贡献。

Abstract: Understanding how the human brain progresses from processing simple
linguistic inputs to performing high-level reasoning is a fundamental challenge
in neuroscience. While modern large language models (LLMs) are increasingly
used to model neural responses to language, their internal representations are
highly "entangled," mixing information about lexicon, syntax, meaning, and
reasoning. This entanglement biases conventional brain encoding analyses toward
linguistically shallow features (e.g., lexicon and syntax), making it difficult
to isolate the neural substrates of cognitively deeper processes. Here, we
introduce a residual disentanglement method that computationally isolates these
components. By first probing an LM to identify feature-specific layers, our
method iteratively regresses out lower-level representations to produce four
nearly orthogonal embeddings for lexicon, syntax, meaning, and, critically,
reasoning. We used these disentangled embeddings to model intracranial (ECoG)
brain recordings from neurosurgical patients listening to natural speech. We
show that: 1) This isolated reasoning embedding exhibits unique predictive
power, accounting for variance in neural activity not explained by other
linguistic features and even extending to the recruitment of visual regions
beyond classical language areas. 2) The neural signature for reasoning is
temporally distinct, peaking later (~350-400ms) than signals related to
lexicon, syntax, and meaning, consistent with its position atop a processing
hierarchy. 3) Standard, non-disentangled LLM embeddings can be misleading, as
their predictive success is primarily attributable to linguistically shallow
features, masking the more subtle contributions of deeper cognitive processing.

</details>


### [69] [Interpreting and Mitigating Unwanted Uncertainty in LLMs](https://arxiv.org/abs/2510.22866)
*Tiasa Singha Roy,Ayush Rajesh Jhaveri,Ilias Triantafyllopoulos*

Main category: cs.CL

TL;DR: 本文研究了LLMs中的不确定性现象，即模型在重新提示时将先前正确答案改为错误答案的行为。通过改进的Needle-in-a-Haystack框架，发现非检索注意力头是主要原因，屏蔽这些头可减少15%的翻转行为。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型存在不确定性现象，这会破坏信任并在高风险领域带来严重风险。本文旨在探究驱动这种现象的机制。

Method: 采用改进的Needle-in-a-Haystack检索框架，集成Flip式重新评估提示来模拟真实的答案翻转场景，识别关键注意力头并进行屏蔽实验。

Result: 发现检索头不是避免不确定性的主要原因，而是少数非检索注意力头过度关注误导性标记。屏蔽这些头可减少15%的翻转行为，且不会引入不连贯或过度校正问题。

Conclusion: 研究为机制可解释性领域做出贡献，提供了一种简单有效的技术来缓解LLMs中不确定性驱动的失败模式，但在下游任务中存在权衡。

Abstract: Despite their impressive capabilities, Large Language Models (LLMs) exhibit
unwanted uncertainty, a phenomenon where a model changes a previously correct
answer into an incorrect one when re-prompted. This behavior undermines trust
and poses serious risks in high-stakes domains. In this work, we investigate
the mechanisms that drive this phenomenon. We adapt the Needle-in-a-Haystack
retrieval framework and integrate a Flip-style re-evaluation prompt to simulate
realistic answer-flipping scenarios. We find that retrieval heads are not
primarily responsible for avoiding uncertainty. Instead, we identify a small
set of non-retrieval attention heads that disproportionately attend to
misleading tokens in uncertain contexts. Masking these heads yields significant
improvements, reducing flip behavior by up to 15% without introducing
incoherence or overcorrection. However, when tested for downstream tasks, we
observe trade-offs with flip behavior. Our findings contribute to the growing
field of mechanistic interpretability and present a simple yet effective
technique for mitigating uncertainty-driven failure modes in LLMs.

</details>


### [70] [A Comprehensive Dataset for Human vs. AI Generated Text Detection](https://arxiv.org/abs/2510.22874)
*Rajarshi Roy,Nasrin Imanpour,Ashhar Aziz,Shashwat Bajpai,Gurpreet Singh,Shwetangshu Biswas,Kapil Wanaskar,Parth Patwa,Subhankar Ghosh,Shreyas Dixit,Nilesh Ranjan Pal,Vipula Rawte,Ritvik Garimella,Gaytri Jena,Amit Sheth,Vasu Sharma,Aishwarya Naresh Reganti,Vinija Jain,Aman Chadha,Amitava Das*

Main category: cs.CL

TL;DR: 该论文提出了一个包含超过58,000个文本样本的综合数据集，结合了真实纽约时报文章和多种先进LLM生成的合成版本，用于AI文本检测和模型归因研究。


<details>
  <summary>Details</summary>
Motivation: 随着LLM生成文本越来越像人类写作，需要大规模、多样化的标注数据集来可靠检测AI生成文本并将其归因到特定模型，以解决内容真实性、错误信息和可信度问题。

Method: 构建包含真实纽约时报文章和由Gemma-2-9b、Mistral-7B、Qwen-2-72B、LLaMA-8B、Yi-Large、GPT-4-o等先进LLM生成的合成文本的综合数据集，提供原始文章摘要作为提示和完整人类撰写叙述。

Result: 建立了两个关键任务的基线结果：区分人类写作与AI生成文本的准确率为58.35%，将AI文本归因到生成模型的准确率为8.92%。

Conclusion: 通过将真实世界新闻内容与现代生成模型相结合，该数据集旨在促进稳健检测和归因方法的发展，在生成AI时代培养信任和透明度。

Abstract: The rapid advancement of large language models (LLMs) has led to increasingly
human-like AI-generated text, raising concerns about content authenticity,
misinformation, and trustworthiness. Addressing the challenge of reliably
detecting AI-generated text and attributing it to specific models requires
large-scale, diverse, and well-annotated datasets. In this work, we present a
comprehensive dataset comprising over 58,000 text samples that combine
authentic New York Times articles with synthetic versions generated by multiple
state-of-the-art LLMs including Gemma-2-9b, Mistral-7B, Qwen-2-72B, LLaMA-8B,
Yi-Large, and GPT-4-o. The dataset provides original article abstracts as
prompts, full human-authored narratives. We establish baseline results for two
key tasks: distinguishing human-written from AI-generated text, achieving an
accuracy of 58.35\%, and attributing AI texts to their generating models with
an accuracy of 8.92\%. By bridging real-world journalistic content with modern
generative models, the dataset aims to catalyze the development of robust
detection and attribution methods, fostering trust and transparency in the era
of generative AI. Our dataset is available at:
https://huggingface.co/datasets/gsingh1-py/train.

</details>


### [71] [Batch Speculative Decoding Done Right](https://arxiv.org/abs/2510.22876)
*Ranran Haoran Zhang,Soumik Dey,Ashirbad Mishra,Hansi Wu,Binbin Li,Rui Zhang*

Main category: cs.CL

TL;DR: 本文提出了EQSPEC和EXSPEC两种批量推测解码方法，解决了批量处理中的不规则张量问题，在保持输出等价性的同时显著提高了推理吞吐量。


<details>
  <summary>Details</summary>
Motivation: 推测解码通过小模型生成候选token并由目标模型并行验证来加速LLM推理。但在批量处理时会出现不规则张量问题：同一批次中不同序列接受的候选token数量不同，破坏了右对齐，导致位置ID、注意力掩码和KV缓存状态损坏，现有实现违反了输出等价性要求。

Method: 1) 分析保证正确性的同步要求；2) 提出EQSPEC方法，发现重新对齐占40%开销；3) 引入EXSPEC方法，维护滑动序列池并动态形成等长分组，减少重新对齐开销同时保持每个序列的推测加速效果。

Result: 在SpecBench数据集上，使用Vicuna-7B/68M、Qwen3-8B/0.6B和GLM-4-9B/0.6B目标/草稿模型对，批量大小为8时相比批量大小为1实现了最高3倍的吞吐量提升，同时保持95%的输出等价性。

Conclusion: 提出的方法无需自定义内核，可与现有推理栈无缝集成，在批量大小为8时实现了高效扩展，同时保持了输出等价性要求。

Abstract: Speculative decoding speeds up LLM inference by using a small draft model to
propose multiple tokens that a target model verifies in parallel. Extending
this idea to batches is essential for production serving, but it introduces the
ragged tensor problem: sequences in the same batch accept different numbers of
draft tokens, breaking right-alignment and corrupting position IDs, attention
masks, and KV-cache state. We show that several existing batch implementations
violate output equivalence-the fundamental requirement that speculative
decoding must produce identical token sequences to standard autoregressive
generation. These violations occur precisely due to improper handling of the
ragged tensor problem. In response, we (1) characterize the synchronization
requirements that guarantee correctness, (2) present a correctness-first batch
speculative decoding EQSPEC that exposes realignment as consuming 40% of
overhead, and (3) introduce EXSPEC, which maintains a sliding pool of sequences
and dynamically forms same-length groups, to reduce the realignment overhead
while preserving per-sequence speculative speedups. On the SpecBench dataset,
across Vicuna-7B/68M, Qwen3-8B/0.6B, and GLM-4-9B/0.6B target/draft pairs, our
approach achieves up to 3$\times$ throughput improvement at batch size 8
compared to batch size 1, with efficient scaling through batch size 8, while
maintaining 95% output equivalence. Our method requires no custom kernels and
integrates cleanly with existing inference stacks. Our code is available at
https://github.com/eBay/spec_dec.

</details>


### [72] [Language Server CLI Empowers Language Agents with Process Rewards](https://arxiv.org/abs/2510.22907)
*Yifan Zhang,Lanser Contributors*

Main category: cs.CL

TL;DR: Lanser-CLI是一个CLI优先的编排层，通过固定和协调语言服务器协议（LSP）服务器，为编码代理和CI提供确定性、可重放的工作流，解决大语言模型在API幻觉和编辑定位方面的不可靠问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型经常产生API幻觉和错误定位编辑，而语言服务器能计算关于真实代码的经过验证的IDE级事实。作者认为语言服务器不仅提供结构信息，还提供可操作的过程奖励：机器检查的逐步信号，使代理的规划循环与程序现实对齐。

Method: 开发Lanser-CLI系统，包含：(i) 通过Selector DSL（符号化、AST路径和内容锚定选择器）和原则性重定位算法的稳健寻址方案；(ii) 确定性分析包，规范化语言服务器响应并捕获环境/能力元数据；(iii) 变异操作的安全信封，包含预览、工作区隔离和Git感知的事务性应用；(iv) 基于语言服务器事实的过程奖励函数。

Result: 在冻结快照下形式化确定性，并为过程奖励建立单调性属性，使其适用于过程监督和反事实分析。

Conclusion: Lanser-CLI通过语言服务器提供机器验证的逐步信号，使AI编码代理的规划与程序现实对齐，解决了大语言模型在代码编辑中的不可靠性问题。

Abstract: Large language models routinely hallucinate APIs and mislocalize edits, while
language servers compute verified, IDE-grade facts about real code. We present
Lanser-CLI, a CLI-first orchestration layer that pins and mediates a Language
Server Protocol (LSP) server for coding agents and CI, exposing deterministic,
replayable workflows. Our position is that language servers provide not only
structural information (definitions, references, types, diagnostics) but also
an actionable process reward: machine-checked, step-wise signals that align an
agent's planning loop with program reality. In this work, Lanser-CLI
contributes: (i) a robust addressing scheme beyond brittle "file:line:col" via
a Selector DSL (symbolic, AST-path, and content-anchored selectors) with a
principled relocation algorithm; (ii) deterministic Analysis Bundles that
normalize Language Server responses and capture environment/capability metadata
with stable content hashes; (iii) a safety envelope for mutating operations
(rename, code actions) with preview, workspace jails, and Git-aware,
transactional apply; and (iv) a process-reward functional derived from Language
Server facts (diagnostic deltas, disambiguation confidence, and safe-apply
checks) that is computable online and replayable offline. We formalize
determinism under frozen snapshots and establish a monotonicity property for
the process reward, making it suitable for process supervision and
counterfactual analysis. Project Page:
https://github.com/yifanzhang-pro/lanser-cli

</details>


### [73] [Artificial Hivemind: The Open-Ended Homogeneity of Language Models (and Beyond)](https://arxiv.org/abs/2510.22954)
*Liwei Jiang,Yuanjun Chai,Margaret Li,Mickel Liu,Raymond Fok,Nouha Dziri,Yulia Tsvetkov,Maarten Sap,Alon Albalak,Yejin Choi*

Main category: cs.CL

TL;DR: 该论文提出了Infinity-Chat数据集，用于评估语言模型在开放生成任务中的多样性问题，揭示了模型存在严重的模式崩溃和人工蜂群效应。


<details>
  <summary>Details</summary>
Motivation: 语言模型在生成多样化、类人的创意内容方面存在困难，可能导致人类思维长期同质化，但目前缺乏可扩展的方法来评估LM输出的多样性。

Method: 引入Infinity-Chat数据集，包含26K个多样化的真实世界开放查询，建立了首个全面的开放提示分类法，包含6个顶级类别和17个子类别，并收集了31,250个人类标注。

Result: 研究发现语言模型存在明显的模式崩溃和人工蜂群效应，包括模型内重复和模型间同质性，且LM、奖励模型和LM评判器对人类偏好的校准较差。

Conclusion: Infinity-Chat为系统研究真实世界开放查询提供了首个大规模资源，揭示了减轻人工蜂群效应带来的长期AI安全风险的关键见解。

Abstract: Language models (LMs) often struggle to generate diverse, human-like creative
content, raising concerns about the long-term homogenization of human thought
through repeated exposure to similar outputs. Yet scalable methods for
evaluating LM output diversity remain limited, especially beyond narrow tasks
such as random number or name generation, or beyond repeated sampling from a
single model. We introduce Infinity-Chat, a large-scale dataset of 26K diverse,
real-world, open-ended user queries that admit a wide range of plausible
answers with no single ground truth. We introduce the first comprehensive
taxonomy for characterizing the full spectrum of open-ended prompts posed to
LMs, comprising 6 top-level categories (e.g., brainstorm & ideation) that
further breaks down to 17 subcategories. Using Infinity-Chat, we present a
large-scale study of mode collapse in LMs, revealing a pronounced Artificial
Hivemind effect in open-ended generation of LMs, characterized by (1)
intra-model repetition, where a single model consistently generates similar
responses, and more so (2) inter-model homogeneity, where different models
produce strikingly similar outputs. Infinity-Chat also includes 31,250 human
annotations, across absolute ratings and pairwise preferences, with 25
independent human annotations per example. This enables studying collective and
individual-specific human preferences in response to open-ended queries. Our
findings show that LMs, reward models, and LM judges are less well calibrated
to human ratings on model generations that elicit differing idiosyncratic
annotator preferences, despite maintaining comparable overall quality. Overall,
INFINITY-CHAT presents the first large-scale resource for systematically
studying real-world open-ended queries to LMs, revealing critical insights to
guide future research for mitigating long-term AI safety risks posed by the
Artificial Hivemind.

</details>


### [74] [Tagging-Augmented Generation: Assisting Language Models in Finding Intricate Knowledge In Long Contexts](https://arxiv.org/abs/2510.22956)
*Anwesan Pal,Karen Hovsepian,Tinghao Guo,Mengnan Zhao,Somendra Tripathi,Nikos Kanakaris,George Mihaila,Sumit Nigam*

Main category: cs.CL

TL;DR: 提出TAG（标签增强生成）方法，通过在长上下文场景中为文档添加标签或标签定义来提升LLM性能，无需改变文档完整性，在NoLima和NovelQA基准测试中取得显著改进。


<details>
  <summary>Details</summary>
Motivation: 现代大型语言模型在处理长复杂上下文时存在有效问答和推理能力不足的问题，现有方法如RAG和分块重排序对分块、嵌入和检索策略敏感，且需要大量预处理步骤。

Method: 提出轻量级数据增强策略TAG，通过在上下文添加标签或标签定义来增强LLM性能，不改变检索文档的完整性和组成结构。

Result: 在32K token上下文中性能提升达17%，在需要跨文本多跳推理的复杂问答中提升2.9%，在两个具有挑战性的问答基准上均取得一致性能增益。

Conclusion: TAG是一种有效的轻量级方法，能够显著提升LLM在长上下文场景中的性能，且无需复杂的预处理步骤。

Abstract: Recent investigations into effective context lengths of modern flagship large
language models (LLMs) have revealed major limitations in effective question
answering (QA) and reasoning over long and complex contexts for even the
largest and most impressive cadre of models. While approaches like
retrieval-augmented generation (RAG) and chunk-based re-ranking attempt to
mitigate this issue, they are sensitive to chunking, embedding and retrieval
strategies and models, and furthermore, rely on extensive pre-processing,
knowledge acquisition and indexing steps. In this paper, we propose
Tagging-Augmented Generation (TAG), a lightweight data augmentation strategy
that boosts LLM performance in long-context scenarios, without degrading and
altering the integrity and composition of retrieved documents. We validate our
hypothesis by augmenting two challenging and directly relevant
question-answering benchmarks -- NoLima and NovelQA -- and show that tagging
the context or even just adding tag definitions into QA prompts leads to
consistent performance gains over the baseline -- up to 17% for 32K token
contexts, and 2.9% in complex reasoning question-answering for multi-hop
queries requiring knowledge across a wide span of text. Additional details are
available at https://sites.google.com/view/tag-emnlp.

</details>


### [75] [MAD-Fact: A Multi-Agent Debate Framework for Long-Form Factuality Evaluation in LLMs](https://arxiv.org/abs/2510.22967)
*Yucheng Ning,Xixun Lin,Fang Fang,Yanan Cao*

Main category: cs.CL

TL;DR: 提出了一个评估长文本事实准确性的系统框架，包括构建中文长文本数据集、多智能体验证机制和加权评估指标，以解决LLM在长文本输出中的事实可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在生物医学、法律和教育等高风险领域的广泛应用引发了对输出事实准确性的担忧，现有短文本评估方法在长文本上效果不佳。

Method: 构建LongHalluQA中文长文本事实数据集，开发基于辩论的多智能体验证系统MAD-Fact，引入事实重要性层次结构来捕捉长文本中不同声明的重要性差异。

Result: 在两个基准测试上的实验表明，更大的LLM通常保持更高的事实一致性，而国产模型在中文内容上表现更优。

Conclusion: 该工作为评估和增强长文本LLM输出的事实可靠性提供了结构化框架，指导其在敏感领域的安全部署。

Abstract: The widespread adoption of Large Language Models (LLMs) raises critical
concerns about the factual accuracy of their outputs, especially in high-risk
domains such as biomedicine, law, and education. Existing evaluation methods
for short texts often fail on long-form content due to complex reasoning
chains, intertwined perspectives, and cumulative information. To address this,
we propose a systematic approach integrating large-scale long-form datasets,
multi-agent verification mechanisms, and weighted evaluation metrics. We
construct LongHalluQA, a Chinese long-form factuality dataset; and develop
MAD-Fact, a debate-based multi-agent verification system. We introduce a fact
importance hierarchy to capture the varying significance of claims in long-form
texts. Experiments on two benchmarks show that larger LLMs generally maintain
higher factual consistency, while domestic models excel on Chinese content. Our
work provides a structured framework for evaluating and enhancing factual
reliability in long-form LLM outputs, guiding their safe deployment in
sensitive domains.

</details>


### [76] [Measuring Teaching with LLMs](https://arxiv.org/abs/2510.22968)
*Michael Hardy*

Main category: cs.CL

TL;DR: 使用基于句子嵌入的定制LLM来评估教学质量，在数据高效训练下达到人类水平甚至超人类表现，并与教师增值指标相关。


<details>
  <summary>Details</summary>
Motivation: 解决教育中教学质量的客观可扩展测量难题，通用LLM难以可靠应用复杂的课堂观察工具。

Method: 使用句子级嵌入的定制LLM架构，系统评估五种句子嵌入，采用防过拟合的数据高效训练方案。

Result: 专业模型达到人类水平表现（与专家评分相关性>0.65），超越平均人-人评分相关性；高级模型更多依赖课程级特征而非孤立话语；聚合分数与教师增值指标相关。

Conclusion: 建立了AI驱动教学测量的可行方法，为教育者发展提供可扩展、可靠有效的反馈路径。

Abstract: Objective and scalable measurement of teaching quality is a persistent
challenge in education. While Large Language Models (LLMs) offer potential,
general-purpose models have struggled to reliably apply complex, authentic
classroom observation instruments. This paper uses custom LLMs built on
sentence-level embeddings, an architecture better suited for the long-form,
interpretive nature of classroom transcripts than conventional subword
tokenization. We systematically evaluate five different sentence embeddings
under a data-efficient training regime designed to prevent overfitting. Our
results demonstrate that these specialized models can achieve human-level and
even super-human performance with expert human ratings above 0.65 and
surpassing the average human-human rater correlation. Further, through analysis
of annotation context windows, we find that more advanced models-those better
aligned with human judgments-attribute a larger share of score variation to
lesson-level features rather than isolated utterances, challenging the
sufficiency of single-turn annotation paradigms. Finally, to assess external
validity, we find that aggregate model scores align with teacher value-added
measures, indicating they are capturing features relevant to student learning.
However, this trend does not hold at the individual item level, suggesting that
while the models learn useful signals, they have not yet achieved full
generalization. This work establishes a viable and powerful new methodology for
AI-driven instructional measurement, offering a path toward providing scalable,
reliable, and valid feedback for educator development.

</details>


### [77] [Understanding In-Context Learning Beyond Transformers: An Investigation of State Space and Hybrid Architectures](https://arxiv.org/abs/2510.23006)
*Shenran Wang,Timothy Tin-Long Tse,Jian Zhu*

Main category: cs.CL

TL;DR: 本文对transformer、state-space和混合架构的大语言模型在基于知识的上下文学习任务上进行深度评估，发现不同架构模型在任务表现上相似但内部机制不同，功能向量主要位于自注意力和Mamba层，且在不同知识类型任务中重要性不同。


<details>
  <summary>Details</summary>
Motivation: 研究不同架构大语言模型在上下文学习中的内部机制差异，特别是transformer、state-space和混合架构在知识型任务中的表现异同。

Method: 结合行为探测和干预方法，评估不同架构LLM在两类知识型上下文学习任务中的表现，分析功能向量的位置和作用机制。

Result: 发现不同架构LLM在任务性能上表现相似但内部机制不同；功能向量主要位于自注意力和Mamba层；Mamba2使用不同于功能向量的机制进行上下文学习；功能向量在参数知识检索任务中更重要，在上下文知识理解中作用较小。

Conclusion: 研究揭示了不同架构和任务类型下上下文学习机制的复杂性，强调了结合行为分析和机制分析对于理解LLM能力的重要性。

Abstract: We perform in-depth evaluations of in-context learning (ICL) on
state-of-the-art transformer, state-space, and hybrid large language models
over two categories of knowledge-based ICL tasks. Using a combination of
behavioral probing and intervention-based methods, we have discovered that,
while LLMs of different architectures can behave similarly in task performance,
their internals could remain different. We discover that function vectors (FVs)
responsible for ICL are primarily located in the self-attention and Mamba
layers, and speculate that Mamba2 uses a different mechanism from FVs to
perform ICL. FVs are more important for ICL involving parametric knowledge
retrieval, but not for contextual knowledge understanding. Our work contributes
to a more nuanced understanding across architectures and task types.
Methodologically, our approach also highlights the importance of combining both
behavioural and mechanistic analyses to investigate LLM capabilities.

</details>


### [78] [LangLingual: A Personalised, Exercise-oriented English Language Learning Tool Leveraging Large Language Models](https://arxiv.org/abs/2510.23011)
*Sammriddh Gupta,Sonit Singh,Aditya Joshi,Mira Kim*

Main category: cs.CL

TL;DR: LangLingual是一个基于LangChain框架和大型语言模型的对话代理，专为语言学习设计，提供实时语法反馈、上下文感知的语言练习和学习者能力跟踪。


<details>
  <summary>Details</summary>
Motivation: 语言教育者希望为学习者创造丰富的学习体验，但在提供反馈和练习方面可能受到限制。

Method: 使用LangChain框架和大型语言模型构建对话代理系统，设计实时语法反馈机制、上下文感知的语言练习生成和学习者能力跟踪功能。

Result: 系统表现出良好的可用性、积极的学习成果和令人鼓舞的学习者参与度。

Conclusion: LangLingual系统在语言学习中显示出良好的应用前景，能够有效支持学习者的语言学习过程。

Abstract: Language educators strive to create a rich experience for learners, while
they may be restricted in the extend of feedback and practice they can provide.
We present the design and development of LangLingual, a conversational agent
built using the LangChain framework and powered by Large Language Models. The
system is specifically designed to provide real-time, grammar-focused feedback,
generate context-aware language exercises and track learner proficiency over
time. The paper discusses the architecture, implementation and evaluation of
LangLingual in detail. The results indicate strong usability, positive learning
outcomes and encouraging learner engagement.

</details>


### [79] [Incentivizing Agentic Reasoning in LLM Judges via Tool-Integrated Reinforcement Learning](https://arxiv.org/abs/2510.23038)
*Ran Xu,Jingjing Chen,Jiayu Ye,Yu Wu,Jun Yan,Carl Yang,Hongkun Yu*

Main category: cs.CL

TL;DR: TIR-Judge是一个端到端的强化学习框架，用于训练集成代码执行器的LLM评估器，在多个基准测试中超越了基于推理的评估器，并展示了无需蒸馏轨迹即可自我进化的能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估器主要依赖文本推理，难以验证复杂约束或进行精确计算，因此需要集成工具来提升评估能力。

Method: 基于三个原则构建：跨可验证和不可验证领域的多样化训练、灵活的评估格式（点对点、成对、列表）、无需蒸馏的迭代强化学习。

Result: 在七个公共基准测试中，TIR-Judge在点对点评估上超越强推理评估器6.4%，成对评估上超越7.7%，列表评估性能与Claude-Opus-4相当（仅8B参数）。

Conclusion: 工具增强的评估器可以通过迭代强化学习自我进化，无需蒸馏轨迹即可达到与蒸馏变体相当的性能。

Abstract: Large Language Models (LLMs) are widely used as judges to evaluate response
quality, providing a scalable alternative to human evaluation. However, most
LLM judges operate solely on intrinsic text-based reasoning, limiting their
ability to verify complex constraints or perform accurate computation.
Motivated by the success of tool-integrated reasoning (TIR) in numerous tasks,
we propose TIR-Judge, an end-to-end RL framework for training LLM judges that
integrates a code executor for precise evaluation. TIR-Judge is built on three
principles: (i) diverse training across verifiable and non-verifiable domains,
(ii) flexible judgment formats (pointwise, pairwise, listwise), and (iii)
iterative RL that bootstraps directly from the initial model without
distillation. On seven public benchmarks, TIR-Judge surpasses strong
reasoning-based judges by up to 6.4% (pointwise) and 7.7% (pairwise), and
achieves listwise performance comparable to Claude-Opus-4 despite having only
8B parameters. Remarkably, TIR-Judge-Zero - trained entirely without distilled
judge trajectories, matches the performance of distilled variants,
demonstrating that tool-augmented judges can self-evolve through iterative
reinforcement learning.

</details>


### [80] [Knocking-Heads Attention](https://arxiv.org/abs/2510.23052)
*Zhanchao Zhou,Xiaodong Chen,Haoxing Chen,Zhenzhong Lan,Jianguo Li*

Main category: cs.CL

TL;DR: 提出了一种名为knocking-heads attention (KHA)的新注意力机制，通过跨头特征交互增强多头注意力的表示能力，在保持参数效率的同时提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统多头注意力机制中，增加头数会削弱单个头的容量，且现有机制只是简单拼接各头的输出，缺乏强交互。需要一种能够促进跨头特征交互的注意力机制。

Method: KHA让注意力头在缩放点积注意力之前相互"敲击"，通过在所有头上应用共享的对角初始化投影矩阵来实现跨头特征级交互。对角线初始化在训练开始时保留头特异性，同时让模型逐步学习集成表示。

Result: 在6.1B参数的MoE模型上验证，KHA带来更优越和更稳定的训练动态，在下游任务中取得更好的性能表现，且只增加极少的参数和FLOPs。

Conclusion: KHA是一种有效增强多头注意力表示能力的机制，可无缝集成到各种注意力变体中，在保持参数效率的同时显著提升模型性能。

Abstract: Multi-head attention (MHA) has become the cornerstone of modern large
language models, enhancing representational capacity through parallel attention
heads. However, increasing the number of heads inherently weakens individual
head capacity, and existing attention mechanisms - whether standard MHA or its
variants like grouped-query attention (GQA) and grouped-tied attention (GTA) -
simply concatenate outputs from isolated heads without strong interaction. To
address this limitation, we propose knocking-heads attention (KHA), which
enables attention heads to "knock" on each other - facilitating cross-head
feature-level interactions before the scaled dot-product attention. This is
achieved by applying a shared, diagonally-initialized projection matrix across
all heads. The diagonal initialization preserves head-specific specialization
at the start of training while allowing the model to progressively learn
integrated cross-head representations. KHA adds only minimal parameters and
FLOPs and can be seamlessly integrated into MHA, GQA, GTA, and other attention
variants. We validate KHA by training a 6.1B parameter MoE model (1.01B
activated) on 1T high-quality tokens. Compared to baseline attention
mechanisms, KHA brings superior and more stable training dynamics, achieving
better performance across downstream tasks.

</details>


### [81] [Quality-Aware Translation Tagging in Multilingual RAG system](https://arxiv.org/abs/2510.23070)
*Hoyeon Moon,Byeolhee Kim,Nikhil Verma*

Main category: cs.CL

TL;DR: 提出QTT-RAG方法，通过评估翻译质量的三维度（语义等价性、语法准确性、自然流畅性）并附加质量分数作为元数据，在多语言检索增强生成中提升低资源语言的性能表现。


<details>
  <summary>Details</summary>
Motivation: 现有mRAG方法在低资源语言场景下，通常检索英文文档并翻译成查询语言，但翻译质量差会降低生成性能。现有方法要么假设翻译质量足够好，要么使用重写方法导致事实扭曲和幻觉。

Method: 提出质量感知翻译标注方法QTT-RAG，明确评估翻译质量的三维度，并将这些分数作为元数据附加，不改变原始内容。

Result: 在两个开放域QA基准（XORQA、MKQA）上使用6个指令调优LLM（2.4B-14B参数）评估，涵盖韩语、芬兰语（低资源）和中文（高资源）。QTT-RAG在保持事实完整性的同时优于基线方法。

Conclusion: QTT-RAG允许生成模型基于翻译可靠性做出明智决策，在低资源设置中有效利用跨语言文档，为多语言领域提供实用且鲁棒的解决方案。

Abstract: Multilingual Retrieval-Augmented Generation (mRAG) often retrieves English
documents and translates them into the query language for low-resource
settings. However, poor translation quality degrades response generation
performance. Existing approaches either assume sufficient translation quality
or utilize the rewriting method, which introduces factual distortion and
hallucinations. To mitigate these problems, we propose Quality-Aware
Translation Tagging in mRAG (QTT-RAG), which explicitly evaluates translation
quality along three dimensions-semantic equivalence, grammatical accuracy, and
naturalness&fluency-and attach these scores as metadata without altering the
original content. We evaluate QTT-RAG against CrossRAG and DKM-RAG as baselines
in two open-domain QA benchmarks (XORQA, MKQA) using six instruction-tuned LLMs
ranging from 2.4B to 14B parameters, covering two low-resource languages
(Korean and Finnish) and one high-resource language (Chinese). QTT-RAG
outperforms the baselines by preserving factual integrity while enabling
generator models to make informed decisions based on translation reliability.
This approach allows for effective usage of cross-lingual documents in
low-resource settings with limited native language documents, offering a
practical and robust solution across multilingual domains.

</details>


### [82] [A Survey on LLM Mid-training](https://arxiv.org/abs/2510.23081)
*Chengying Tu,Xuemiao Zhang,Rongxiang Weng,Rumei Li,Chen Zhang,Yang Bai,Hongfei Yan,Jingang Wang,Xunliang Cai*

Main category: cs.CL

TL;DR: 本文综述了大型语言模型训练中的中间训练阶段，该阶段连接预训练和微调，使用中等规模的数据和计算资源来增强特定能力（如数学、编程、推理等），同时保持基础能力。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型的发展，多阶段训练显示出显著优势，特别是中间训练作为连接预训练和后训练的关键阶段，能够系统性地增强模型的特定能力。

Method: 本文为LLMs提供了中间训练的正式定义，并研究了包含数据管理、训练策略和模型架构优化的优化框架，分析了主流模型在目标驱动干预下的实现方式。

Result: 通过阐明中间训练的独特贡献，本文提供了一个全面的分类体系和可操作的见解，展示了中间训练作为LLM能力渐进发展中一个独特而关键阶段的作用。

Conclusion: 中间训练是LLM能力发展中一个独特且关键的阶段，本综述为未来LLM的研究和创新提供了支持。

Abstract: Recent advances in foundation models have highlighted the significant
benefits of multi-stage training, with a particular emphasis on the emergence
of mid-training as a vital stage that bridges pre-training and post-training.
Mid-training is distinguished by its use of intermediate data and computational
resources, systematically enhancing specified capabilities such as mathematics,
coding, reasoning, and long-context extension, while maintaining foundational
competencies. This survey provides a formal definition of mid-training for
large language models (LLMs) and investigates optimization frameworks that
encompass data curation, training strategies, and model architecture
optimization. We analyze mainstream model implementations in the context of
objective-driven interventions, illustrating how mid-training serves as a
distinct and critical stage in the progressive development of LLM capabilities.
By clarifying the unique contributions of mid-training, this survey offers a
comprehensive taxonomy and actionable insights, supporting future research and
innovation in the advancement of LLMs.

</details>


### [83] [MAP4TS: A Multi-Aspect Prompting Framework for Time-Series Forecasting with Large Language Models](https://arxiv.org/abs/2510.23090)
*Suchan Lee,Jihoon Choi,Sohyeon Lee,Minseok Song,Bong-Gyu Jang,Hwanjo Yu,Soyeon Caren Han*

Main category: cs.CL

TL;DR: MAP4TS是一个多角度提示框架，通过将经典时间序列分析融入提示设计，显著提升了基于LLM的时间序列预测性能


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了时间序列数据独特的统计特性和时间依赖性，需要将经典时间序列分析融入LLM提示设计

Method: 提出四个专门提示组件：全局领域提示、局部领域提示、统计提示和时间提示，结合原始时间序列嵌入，通过跨模态对齐模块生成统一表示

Result: 在八个不同数据集上的实验显示，MAP4TS持续优于最先进的基于LLM的方法，提示感知设计显著提升性能稳定性

Conclusion: 结构化提示与GPT-2骨干网络组合在长期预测任务中优于LLaMA等更大模型，证明了将领域知识融入提示设计的有效性

Abstract: Recent advances have investigated the use of pretrained large language models
(LLMs) for time-series forecasting by aligning numerical inputs with LLM
embedding spaces. However, existing multimodal approaches often overlook the
distinct statistical properties and temporal dependencies that are fundamental
to time-series data. To bridge this gap, we propose MAP4TS, a novel
Multi-Aspect Prompting Framework that explicitly incorporates classical
time-series analysis into the prompt design. Our framework introduces four
specialized prompt components: a Global Domain Prompt that conveys
dataset-level context, a Local Domain Prompt that encodes recent trends and
series-specific behaviors, and a pair of Statistical and Temporal Prompts that
embed handcrafted insights derived from autocorrelation (ACF), partial
autocorrelation (PACF), and Fourier analysis. Multi-Aspect Prompts are combined
with raw time-series embeddings and passed through a cross-modality alignment
module to produce unified representations, which are then processed by an LLM
and projected for final forecasting. Extensive experiments across eight diverse
datasets show that MAP4TS consistently outperforms state-of-the-art LLM-based
methods. Our ablation studies further reveal that prompt-aware designs
significantly enhance performance stability and that GPT-2 backbones, when
paired with structured prompts, outperform larger models like LLaMA in
long-term forecasting tasks.

</details>


### [84] [Leveraging Hierarchical Organization for Medical Multi-document Summarization](https://arxiv.org/abs/2510.23104)
*Yi-Li Hsu,Katelyn X. Mei,Lucy Lu Wang*

Main category: cs.CL

TL;DR: 本文研究了在医学多文档摘要任务中引入层次结构输入，相比传统平面摘要方法能更好地组织跨文档信息，提高模型生成摘要的质量和人类偏好。


<details>
  <summary>Details</summary>
Motivation: 医学多文档摘要是复杂任务，需要有效管理跨文档关系。传统平面摘要方法在组织信息方面存在局限，因此研究层次结构是否能改善模型的信息组织和上下文理解能力。

Method: 研究了两种层次组织方法，在三个大语言模型上进行实验，使用自动指标、模型指标和领域专家评估（偏好、可理解性、清晰度、复杂性、相关性、覆盖度、事实性和连贯性）进行综合评估。

Result: 专家更偏好模型生成的摘要而非人工编写的摘要。层次方法通常能保持信息的事实性、覆盖度和连贯性，同时提高人类对摘要的偏好。GPT-4的模拟判断与人类判断在更客观的评估维度上具有更高一致性。

Conclusion: 层次结构可以提高模型生成的医学摘要的清晰度，同时保持内容覆盖度，为提高生成摘要的人类偏好提供了实用方法。

Abstract: Medical multi-document summarization (MDS) is a complex task that requires
effectively managing cross-document relationships. This paper investigates
whether incorporating hierarchical structures in the inputs of MDS can improve
a model's ability to organize and contextualize information across documents
compared to traditional flat summarization methods. We investigate two ways of
incorporating hierarchical organization across three large language models
(LLMs), and conduct comprehensive evaluations of the resulting summaries using
automated metrics, model-based metrics, and domain expert evaluation of
preference, understandability, clarity, complexity, relevance, coverage,
factuality, and coherence. Our results show that human experts prefer
model-generated summaries over human-written summaries. Hierarchical approaches
generally preserve factuality, coverage, and coherence of information, while
also increasing human preference for summaries. Additionally, we examine
whether simulated judgments from GPT-4 align with human judgments, finding
higher agreement along more objective evaluation facets. Our findings
demonstrate that hierarchical structures can improve the clarity of medical
summaries generated by models while maintaining content coverage, providing a
practical way to improve human preference for generated summaries.

</details>


### [85] [Flexing in 73 Languages: A Single Small Model for Multilingual Inflection](https://arxiv.org/abs/2510.23114)
*Tomáš Sourada,Jana Straková*

Main category: cs.CL

TL;DR: 提出了一种紧凑的单模型多语言词形变化方法，在73种语言上联合训练，性能优于单语言基线，简化了部署流程。


<details>
  <summary>Details</summary>
Motivation: 解决缺乏开源、通用、多语言形态变化系统的问题，特别是能够处理未见词汇的系统，简化多语言模型部署。

Method: 使用单模型在73种语言数据上联合训练，引入基于频率加权和词干不相交的训练-开发-测试重采样方法。

Result: 模型轻量级、对未见词汇鲁棒，在大多数语言中超越单语言基线，并在SIGMORPHON和UD树库上表现良好。

Conclusion: 多语言建模在词形变化任务中有效，具有实际部署优势，代码已开源。

Abstract: We present a compact, single-model approach to multilingual inflection, the
task of generating inflected word forms from base lemmas to express grammatical
categories. Our model, trained jointly on data from 73 languages, is
lightweight, robust to unseen words, and outperforms monolingual baselines in
most languages. This demonstrates the effectiveness of multilingual modeling
for inflection and highlights its practical benefits: simplifying deployment by
eliminating the need to manage and retrain dozens of separate monolingual
models. In addition to the standard SIGMORPHON shared task benchmarks, we
evaluate our monolingual and multilingual models on 73 Universal Dependencies
(UD) treebanks, extracting lemma-tag-form triples and their frequency counts.
To ensure realistic data splits, we introduce a novel frequency-weighted,
lemma-disjoint train-dev-test resampling procedure. Our work addresses the lack
of an open-source, general-purpose, multilingual morphological inflection
system capable of handling unseen words across a wide range of languages,
including Czech. All code is publicly released at:
https://github.com/tomsouri/multilingual-inflection.

</details>


### [86] [Beyond Higher Rank: Token-wise Input-Output Projections for Efficient Low-Rank Adaptation](https://arxiv.org/abs/2510.23123)
*Shiwei Li,Xiandi Luo,Haozhao Wang,Xing Tang,Ziqiang Cui,Dugang Liu,Yuhua Li,Xiuqiang He,Ruixuan Li*

Main category: cs.CL

TL;DR: 提出TopLoRA方法，通过为每个输入token动态调整LoRA权重，实现token级别的输入-输出投影，在多个模型和数据集上优于标准LoRA及其变体。


<details>
  <summary>Details</summary>
Motivation: 标准LoRA中所有输入token共享相同的权重和投影，无法捕捉token特定的语义差异信息，限制了其表达能力。

Method: TopLoRA将LoRA权重表示为BΣ_XA，其中A和B是低秩矩阵，Σ_X是根据每个输入tokenX生成的对角矩阵，实现token级别的权重动态调整。

Result: 在多个模型和数据集上的广泛实验表明，TopLoRA始终优于LoRA及其变体。

Conclusion: TopLoRA通过token级别的投影学习实现了更细粒度的适配，在不增加LoRA秩的情况下提升了性能。

Abstract: Low-rank adaptation (LoRA) is a parameter-efficient fine-tuning (PEFT) method
widely used in large language models (LLMs). LoRA essentially describes the
projection of an input space into a low-dimensional output space, with the
dimensionality determined by the LoRA rank. In standard LoRA, all input tokens
share the same weights and undergo an identical input-output projection. This
limits LoRA's ability to capture token-specific information due to the inherent
semantic differences among tokens. To address this limitation, we propose
Token-wise Projected Low-Rank Adaptation (TopLoRA), which dynamically adjusts
LoRA weights according to the input token, thereby learning token-wise
input-output projections in an end-to-end manner. Formally, the weights of
TopLoRA can be expressed as $B\Sigma_X A$, where $A$ and $B$ are low-rank
matrices (as in standard LoRA), and $\Sigma_X$ is a diagonal matrix generated
from each input token $X$. Notably, TopLoRA does not increase the rank of LoRA
weights but achieves more granular adaptation by learning token-wise LoRA
weights (i.e., token-wise input-output projections). Extensive experiments
across multiple models and datasets demonstrate that TopLoRA consistently
outperforms LoRA and its variants. The code is available at
https://github.com/Leopold1423/toplora-neurips25.

</details>


### [87] [Corpus Frequencies in Morphological Inflection: Do They Matter?](https://arxiv.org/abs/2510.23131)
*Tomáš Sourada,Jana Straková*

Main category: cs.CL

TL;DR: 该论文探索将语料库频率信息融入形态屈折任务，通过频率加权的训练-开发-测试分割、引入词例准确率评估指标以及频率感知训练方法，在43种语言中的26种上优于均匀采样。


<details>
  <summary>Details</summary>
Motivation: 传统形态屈折方法缺乏词频分布信息，而实际部署中用户输入会反映自然文本的真实频率分布，因此需要将语料频率信息整合到系统开发中。

Method: 采用三种方法：1) 词干不相交且频率加重的训练-开发-测试分割；2) 引入词例准确率作为补充评估指标；3) 提出频率感知训练方法，在训练采样中显式考虑词频。

Result: 频率感知训练在43种语言中的26种上优于均匀采样，表明频率信息能有效提升形态屈折性能。

Conclusion: 将语料频率信息整合到形态屈折任务的各个阶段（数据分割、评估和训练）能更好地反映真实使用场景，提升系统在实际部署中的表现。

Abstract: The traditional approach to morphological inflection (the task of modifying a
base word (lemma) to express grammatical categories) has been, for decades, to
consider lexical entries of lemma-tag-form triples uniformly, lacking any
information about their frequency distribution. However, in production
deployment, one might expect the user inputs to reflect a real-world
distribution of frequencies in natural texts. With future deployment in mind,
we explore the incorporation of corpus frequency information into the task of
morphological inflection along three key dimensions during system development:
(i) for train-dev-test split, we combine a lemma-disjoint approach, which
evaluates the model's generalization capabilities, with a frequency-weighted
strategy to better reflect the realistic distribution of items across different
frequency bands in training and test sets; (ii) for evaluation, we complement
the standard type accuracy (often referred to simply as accuracy), which treats
all items equally regardless of frequency, with token accuracy, which assigns
greater weight to frequent words and better approximates performance on running
text; (iii) for training data sampling, we introduce a method novel in the
context of inflection, frequency-aware training, which explicitly incorporates
word frequency into the sampling process. We show that frequency-aware training
outperforms uniform sampling in 26 out of 43 languages.

</details>


### [88] [ENTP: Enhancing Low-Quality SFT Data via Neural-Symbolic Text Purge-Mix](https://arxiv.org/abs/2510.23160)
*Zile Yang,Ling Li,Na Di,Jinlong Pang,Yao Zhou,Hao Cheng,Bo Han,Jiaheng Wei*

Main category: cs.CL

TL;DR: ENTP框架通过神经符号方法净化低质量SFT数据，仅使用低质量数据构建的数据集在多个基准测试中超越了13种现有数据选择方法和完整原始数据集。


<details>
  <summary>Details</summary>
Motivation: 现有SFT方法只关注高质量数据，忽略了低质量数据中的有价值信号，且依赖不完美的质量过滤器。

Method: 提出ENTP框架，包含符号净化模块（基于统计先验识别和修剪噪声样本）和神经重构模块（利用潜在表示和模型知识合成增强的指令-响应对）。

Result: 仅使用低质量数据构建的ENTP增强数据集在五个指令跟随基准测试中超越了13种现有数据选择方法，甚至优于在完整原始数据集（约30万样本）上的微调。

Conclusion: 低质量数据具有未开发的潜力，智能净化和合成对于高效指令对齐至关重要。

Abstract: Supervised Fine-Tuning (SFT) adapts pre-trained Large Language Models (LLMs)
to domain-specific instructions by training on a carefully curated subset of
high-quality instruction-response pairs, typically drawn from a larger dataset
that often contains many low-quality or noisy samples. However, existing
quality-first paradigms often overlook valuable signals in discarded
low-quality data and rely on imperfect quality filters. We introduce ENTP
(Enhancing low-quality SFT data via Neural-symbolic Text Purge-Mix), a
framework that revitalizes low-quality corpora through symbolic purification
and neural reconstruction. The symbolic module identifies and prunes noisy
samples based on statistical priors, while the neural component synthesizes
enriched instruction-response pairs by leveraging latent representations and
model knowledge. This neural-symbolic synergy enhances data informativeness and
diversity. Experiments show that ENTP-augmented datasets, constructed
exclusively from low-quality data, outperform 13 established data-selection
baselines across five instruction-following benchmarks, and even surpass
fine-tuning on the full original dataset (approximately 300K examples). Our
results highlight the untapped potential of low-quality data and underscore the
importance of intelligent purification and synthesis for efficient instruction
alignment.

</details>


### [89] [Beyond Direct Generation: A Decomposed Approach to Well-Crafted Screenwriting with LLMs](https://arxiv.org/abs/2510.23163)
*Hang Lei,Shengyi Zong,Zhaoyan Li,Ziren Zhou,Hao Liu*

Main category: cs.CL

TL;DR: 提出了双阶段精炼框架，将剧本创作分解为创意叙事生成和格式转换两个独立阶段，解决了LLM在剧本创作中同时处理创意和格式的困难。


<details>
  <summary>Details</summary>
Motivation: 传统端到端的LLM剧本生成方法往往只能模仿表面风格，缺乏深层结构完整性和故事实质。这是因为单个模型需要同时掌握创意叙事构建和严格格式遵循这两种截然不同的能力。

Method: 双阶段精炼框架：第一阶段将简要大纲转化为丰富的类小说散文；第二阶段将叙事精炼为专业格式的剧本。通过混合数据合成解决训练数据稀缺问题，包括反向合成和正向合成。

Result: 专业编剧的盲评显示，DSR相对于Gemini-2.5-Pro等强基线获得了75%的胜率，达到了人类水平表现的82.7%。

Conclusion: 分解生成架构结合定制化数据合成能有效专业化LLM在复杂创意领域的应用。

Abstract: The screenplay serves as the foundation for television production, defining
narrative structure, character development, and dialogue. While Large Language
Models (LLMs) show great potential in creative writing, direct end-to-end
generation approaches often fail to produce well-crafted screenplays. We argue
this failure stems from forcing a single model to simultaneously master two
disparate capabilities: creative narrative construction and rigid format
adherence. The resulting outputs may mimic superficial style but lack the deep
structural integrity and storytelling substance required for professional use.
To enable LLMs to generate high-quality screenplays, we introduce Dual-Stage
Refinement (DSR), a decomposed framework that decouples creative narrative
generation from format conversion. The first stage transforms a brief outline
into rich, novel-style prose. The second stage refines this narrative into a
professionally formatted screenplay. This separation enables the model to
specialize in one distinct capability at each stage. A key challenge in
implementing DSR is the scarcity of paired outline-to-novel training data. We
address this through hybrid data synthesis: reverse synthesis deconstructs
existing screenplays into structured inputs, while forward synthesis leverages
these inputs to generate high-quality narrative texts as training targets.
Blind evaluations by professional screenwriters show that DSR achieves a 75%
win rate against strong baselines like Gemini-2.5-Pro and reaches 82.7% of
human-level performance. Our work demonstrates that decomposed generation
architecture with tailored data synthesis effectively specializes LLMs in
complex creative domains.

</details>


### [90] [MATCH: Task-Driven Code Evaluation through Contrastive Learning](https://arxiv.org/abs/2510.23169)
*Marah Ghoummaid,Vladimir Tchuiev,Ofek Glick,Michal Moschkovitz,Dotan Di Castro*

Main category: cs.CL

TL;DR: 本文提出了MATCH，一种基于对比学习的无参考代码生成评估指标，通过生成代码和自然语言任务描述的嵌入来计算相似度，比现有指标更好地反映功能正确性和人类偏好。


<details>
  <summary>Details</summary>
Motivation: AI代码生成日益普及，但准确评估生成代码是否符合开发者意图仍具挑战。传统方法如单元测试成本高，语法相似度指标无法捕捉功能，而CodeBERTScore等需要参考代码。

Method: 使用对比学习为代码和自然语言任务描述生成有意义的嵌入，通过相似度评分来评估生成代码实现任务的程度。

Result: MATCH在多种编程语言中与功能正确性和人类偏好的相关性均优于现有指标。

Conclusion: MATCH是一种有效的无参考代码生成评估指标，能够更好地衡量生成代码与开发者意图的匹配程度。

Abstract: AI-based code generation is increasingly prevalent, with GitHub Copilot
estimated to generate 46% of the code on GitHub. Accurately evaluating how well
generated code aligns with developer intent remains a critical challenge.
Traditional evaluation methods, such as unit tests, are often unscalable and
costly. Syntactic similarity metrics (e.g., BLEU, ROUGE) fail to capture code
functionality, and metrics like CodeBERTScore require reference code, which is
not always available. To address the gap in reference-free evaluation, with few
alternatives such as ICE-Score, this paper introduces MATCH, a novel
reference-free metric. MATCH uses Contrastive Learning to generate meaningful
embeddings for code and natural language task descriptions, enabling similarity
scoring that reflects how well generated code implements the task. We show that
MATCH achieves stronger correlations with functional correctness and human
preference than existing metrics across multiple programming languages.

</details>


### [91] [SI-Bench: Benchmarking Social Intelligence of Large Language Models in Human-to-Human Conversations](https://arxiv.org/abs/2510.23182)
*Shuai Huang,Wenxuan Zhao,Jun Gao*

Main category: cs.CL

TL;DR: SI-Bench是一个评估大语言模型社交智能的新基准，包含2,221个真实多轮对话，实验显示SOTA模型在复杂社交情境的过程推理上超越人类专家，但在回复质量上仍落后于人类，且CoT推理可能降低模型在社交对话任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型发展出类人能力并被部署为自主代理与人类互动，评估其在真实复杂社交互动中的表现成为重要挑战。现有研究多通过模拟代理间交互构建数据集，无法捕捉真实人类对话的语言风格和关系动态。

Method: 基于广泛的社会科学理论，SI-Bench从社交网络应用收集了2,221个真实多轮对话，并从中选择312个对话对8个主要模型进行人工标注。

Result: 实验表明，SOTA模型在复杂社交情境的过程推理上超越了人类专家，但在回复质量上仍落后于人类。此外，引入Chain-of-Thought推理可能会降低LLMs在社交对话任务中的性能。

Conclusion: SI-Bench为评估LLMs的社交智能提供了新基准，揭示了当前模型在社交推理和回复质量方面的差距，所有数据集已开源。

Abstract: As large language models (LLMs) develop anthropomorphic abilities, they are
increasingly being deployed as autonomous agents to interact with humans.
However, evaluating their performance in realistic and complex social
interactions remains a significant challenge. Most previous research built
datasets through simulated agent-to-agent interactions, which fails to capture
the authentic linguistic styles and relational dynamics found in real human
conversations. To address this gap, we introduce SI-Bench, a novel benchmark
designed to evaluate aspects of social intelligence in LLMs. Grounded in broad
social science theories, SI-Bench contains 2,221 authentic multi-turn dialogues
collected from a social networking application. We further selected a subset of
312 dialogues for manual annotation across 8 major models. The experiments show
that SOTA models have surpassed the human expert in process reasoning under
complex social situations, yet they still fall behind humans in reply quality.
Moreover, introducing Chain-of-Thought (CoT) reasoning may degrade the
performance of LLMs in social dialogue tasks. All datasets are openly available
at https://github.com/SI-Bench/SI-Bench.git.

</details>


### [92] [DREaM: Drug-Drug Relation Extraction via Transfer Learning Method](https://arxiv.org/abs/2510.23189)
*Ali Fata,Hossein Rahmani,Parinaz Soltanzadeh,Amirhossein Derakhshan,Behrouz Minaei Bidgoli*

Main category: cs.CL

TL;DR: 提出DREAM方法，通过预训练关系抽取模型从医学文本中构建药物关系本体，并用大语言模型验证提取的关系。


<details>
  <summary>Details</summary>
Motivation: 药物关系提取对识别药物相互作用和预测副作用至关重要，但目前缺乏专门的数据集，需要采用迁移学习。

Method: 首先使用训练好的关系抽取模型发现实体间关系，然后应用于医学文本语料库构建药物关系本体，最后用大语言模型验证提取的关系。

Result: 定量结果显示LLM对PubMed摘要子集中提取的71%关系表示同意；定性分析表明该方法能揭示医学领域的歧义性。

Conclusion: 该方法能有效构建药物关系本体，同时凸显了医学领域关系提取的挑战。

Abstract: Relation extraction between drugs plays a crucial role in identifying drug
drug interactions and predicting side effects. The advancement of machine
learning methods in relation extraction, along with the development of large
medical text databases, has enabled the low cost extraction of such relations
compared to other approaches that typically require expert knowledge. However,
to the best of our knowledge, there are limited datasets specifically designed
for drug drug relation extraction currently available. Therefore, employing
transfer learning becomes necessary to apply machine learning methods in this
domain. In this study, we propose DREAM, a method that first employs a trained
relation extraction model to discover relations between entities and then
applies this model to a corpus of medical texts to construct an ontology of
drug relationships. The extracted relations are subsequently validated using a
large language model. Quantitative results indicate that the LLM agreed with 71
of the relations extracted from a subset of PubMed abstracts. Furthermore, our
qualitative analysis indicates that this approach can uncover ambiguities in
the medical domain, highlighting the challenges inherent in relation extraction
in this field.

</details>


### [93] [Process Reward Models for Sentence-Level Verification of LVLM Radiology Reports](https://arxiv.org/abs/2510.23217)
*Alois Thomas,Maya Varma,Jean-Benoit Delbrouck,Curtis P. Langlotz*

Main category: cs.CL

TL;DR: 提出了一种句子级别的过程奖励模型（PRM），用于检测大型视觉语言模型生成的放射学报告中的临床幻觉，该模型在未见过的LVLM上表现出强泛化能力，并能有效提升报告质量。


<details>
  <summary>Details</summary>
Motivation: 现有的大视觉语言模型在生成放射学报告时经常产生临床关键性幻觉，存在严重风险，而现有的幻觉检测方法缺乏句子级别的细粒度分析能力，且在不同LVLM生成器间的泛化能力不足。

Method: 采用句子级别的过程奖励模型（PRM），基于临床上下文和先前文本预测每个生成句子的真实性，在MIMIC-CXR数据集上使用弱监督标签进行微调。

Result: 轻量级0.5B参数的PRM在多个指标上优于现有验证技术，在未见过的LVLM上表现出强泛化能力，能有效过滤低质量报告并提升临床指标。

Conclusion: 轻量级、上下文感知的PRM为临床LVLM提供了模型无关的安全层，无需访问内部激活状态即可有效检测和减少幻觉。

Abstract: Automating radiology report generation with Large Vision-Language Models
(LVLMs) holds great potential, yet these models often produce clinically
critical hallucinations, posing serious risks. Existing hallucination detection
methods frequently lack the necessary sentence-level granularity or robust
generalization across different LVLM generators. We introduce a novel approach:
a sentence-level Process Reward Model (PRM) adapted for this vision-language
task. Our PRM predicts the factual correctness of each generated sentence,
conditioned on clinical context and preceding text. When fine-tuned on
MIMIC-CXR with weakly-supervised labels, a lightweight 0.5B-parameter PRM
outperforms existing verification techniques, demonstrating, for instance,
relative improvements of 7.5% in Matthews Correlation Coefficient and 1.8% in
AUROC over strong white-box baselines on outputs from one LVLM. Unlike methods
reliant on internal model states, our PRM demonstrates strong generalization to
an unseen LVLM. We further show its practical utility: PRM scores effectively
filter low-quality reports, improving F1-CheXbert scores by 4.5% (when
discarding the worst 10% of reports). Moreover, when guiding a novel weighted
best-of-N selection process on the MIMIC-CXR test set, our PRM show relative
improvements in clinical metrics of 7.4% for F1-CheXbert and 0.6% for
BERTScore. These results demonstrate that a lightweight, context-aware PRM
provides a model-agnostic safety layer for clinical LVLMs without access to
internal activations

</details>


### [94] [Are ASR foundation models generalized enough to capture features of regional dialects for low-resource languages?](https://arxiv.org/abs/2510.23252)
*Tawsif Tashwar Dipto,Azmol Hossain,Rubayet Sabbir Faruque,Md. Rezuwan Hassan,Kanij Fatema,Tanmoy Shome,Ruwad Naswan,Md. Foriduzzaman Zihad,Mohaymen Ul Anam,Nazia Tasnim,Hasan Mahmud,Md Kamrul Hasan,Md. Mehedi Hasan Shawon,Farig Sadeque,Tahsin Reasat*

Main category: cs.CL

TL;DR: 开发了一个78小时的孟加拉语语音转文本语料库Ben-10，研究发现语音基础模型在方言ASR中表现不佳，方言特定训练可缓解问题。


<details>
  <summary>Details</summary>
Motivation: 研究方言变化对自动语音识别的影响，传统研究依赖标准形式，而方言ASR被视为微调任务。

Method: 开发Ben-10语料库，从语言学和数据驱动角度分析，评估零样本和微调设置下的表现。

Result: 语音基础模型在方言ASR中表现严重不足，所有深度学习方法都难以建模方言变化数据，但方言特定训练可改善问题。

Conclusion: 方言变化对ASR构成显著挑战，Ben-10数据集可作为资源受限条件下ASR建模的分布外资源。

Abstract: Conventional research on speech recognition modeling relies on the canonical
form for most low-resource languages while automatic speech recognition (ASR)
for regional dialects is treated as a fine-tuning task. To investigate the
effects of dialectal variations on ASR we develop a 78-hour annotated Bengali
Speech-to-Text (STT) corpus named Ben-10. Investigation from linguistic and
data-driven perspectives shows that speech foundation models struggle heavily
in regional dialect ASR, both in zero-shot and fine-tuned settings. We observe
that all deep learning methods struggle to model speech data under dialectal
variations but dialect specific model training alleviates the issue. Our
dataset also serves as a out of-distribution (OOD) resource for ASR modeling
under constrained resources in ASR algorithms. The dataset and code developed
for this project are publicly available

</details>


### [95] [Mubeen AI: A Specialized Arabic Language Model for Heritage Preservation and User Intent Understanding](https://arxiv.org/abs/2510.23271)
*Mohammed Aljafari,Ismail Alturki,Ahmed Mori,Yehya Kadumi*

Main category: cs.CL

TL;DR: Mubeen是一个专有的阿拉伯语语言模型，专注于阿拉伯语言学、伊斯兰研究和文化遗产的深度理解，通过原生阿拉伯语源和实用闭合架构解决传统模型的效用差距问题。


<details>
  <summary>Details</summary>
Motivation: 解决现有阿拉伯语模型依赖英语翻译数据导致意图检测失败和文化不准确的问题，同时应对"效用差距危机"——即事实正确的答案无法满足用户核心需求的问题。

Method: 使用专有阿拉伯语OCR引擎数字化历史手稿，训练于大量真实的阿拉伯语源，包括语言学、法学、圣训和古兰经注释等学术著作，采用深度语言工程框架和实用闭合架构。

Result: 开发出能够准确理解古典文本、当代写作和地区方言的阿拉伯语模型，在文化真实性和准确性方面表现优异，能够提供清晰、决定性的指导。

Conclusion: Mubeen成功地将信息存储库转变为决定性指南，结合文化遗产专业化和多学科专家模块，在文化保护和通用知识领域均表现强劲，符合沙特2030愿景。

Abstract: Mubeen is a proprietary Arabic language model developed by MASARAT SA,
optimized for deep understanding of Arabic linguistics, Islamic studies, and
cultural heritage. Trained on an extensive collection of authentic Arabic
sources significantly expanded by digitizing historical manuscripts via a
proprietary Arabic OCR engine, the model incorporates seminal scholarly works
in linguistics, jurisprudence, hadith, and Quranic exegesis, alongside
thousands of academic theses and peer-reviewed research papers. Conditioned
through a deep linguistic engineering framework, Mubeen masters not just the
meaning but the eloquence of Arabic, enabling precise understanding across
classical texts, contemporary writing, and regional dialects with focus on
comprehending user intent and delivering accurate, contextually relevant
responses. Unlike other Arabic models relying on translated English data that
often fail in intent detection or retrieval-augmented generation (RAG), Mubeen
uses native Arabic sources to ensure cultural authenticity and accuracy. Its
core innovation is the Practical Closure Architecture, designed to solve the
"Utility Gap Crisis" where factually correct answers fail to resolve users'
core needs, forcing them into frustrating cycles of re-prompting. By
prioritizing clarity and decisive guidance, Mubeen transforms from an
information repository into a decisive guide, aligning with Saudi Vision 2030.
The model's architecture combines deep heritage specialization with
multi-disciplinary expert modules, enabling robust performance across both
cultural preservation and general knowledge domains.

</details>


### [96] [Code Aesthetics with Agentic Reward Feedback](https://arxiv.org/abs/2510.23272)
*Bang Xiao,Lingjie Jiang,Shaohan Huang,Tengchao Lv,Yupan Huang,Xun Wu,Lei Cui,Furu Wei*

Main category: cs.CL

TL;DR: 提出了一种提升LLM生成代码美观度的新方法，通过构建大规模代码美学数据集AesCode-358K，开发多智能体奖励反馈系统，并结合GRPO算法优化代码功能和美学质量。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM在传统编程任务上表现出色，但在视觉导向的编码任务中往往产生美学质量较差的代码，需要专门的方法来提升代码美观度。

Method: 构建AesCode-358K指令调优数据集；提出多智能体奖励反馈系统评估可执行性、静态美学和交互美学；开发GRPO-AR算法联合优化功能性和代码美学。

Result: 在AesCode-358K上进行监督微调并结合强化学习显著提升了OpenDesign基准上的性能，AesCoder-4B模型超越了GPT-4o和GPT-4.1，性能可与480B-685B参数的大型开源模型相媲美。

Conclusion: 该方法有效提升了LLM生成代码的美学质量，证明了结合监督微调和强化学习在代码美学优化中的有效性。

Abstract: Large Language Models (LLMs) have become valuable assistants for developers
in code-related tasks. While LLMs excel at traditional programming tasks such
as code generation and bug fixing, they struggle with visually-oriented coding
tasks, often producing suboptimal aesthetics. In this paper, we introduce a new
pipeline to enhance the aesthetic quality of LLM-generated code. We first
construct AesCode-358K, a large-scale instruction-tuning dataset focused on
code aesthetics. Next, we propose agentic reward feedback, a multi-agent system
that evaluates executability, static aesthetics, and interactive aesthetics.
Building on this, we develop GRPO-AR, which integrates these signals into the
GRPO algorithm for joint optimization of functionality and code aesthetics.
Finally, we develop OpenDesign, a benchmark for assessing code aesthetics.
Experimental results show that combining supervised fine-tuning on AesCode-358K
with reinforcement learning using agentic reward feedback significantly
improves performance on OpenDesign and also enhances results on existing
benchmarks such as PandasPlotBench. Notably, our AesCoder-4B surpasses GPT-4o
and GPT-4.1, and achieves performance comparable to large open-source models
with 480B-685B parameters, underscoring the effectiveness of our approach.

</details>


### [97] [A Cocktail-Party Benchmark: Multi-Modal dataset and Comparative Evaluation Results](https://arxiv.org/abs/2510.23276)
*Thai-Binh Nguyen,Katerina Zmolikova,Pingchuan Ma,Ngoc Quan Pham,Christian Fuegen,Alexander Waibel*

Main category: cs.CL

TL;DR: 第九届CHiME挑战赛引入多模态上下文感知识别任务，解决单房间内重叠对话的鸡尾酒会问题，使用音频、视觉和上下文线索识别说话者及其对话内容。


<details>
  <summary>Details</summary>
Motivation: 解决自然多参与者对话中的极端语音重叠问题，在非脚本化、随意的群聊场景中，语音重叠可达100%，需要准确识别谁在何时说了什么以及与谁对话。

Method: 通过音频-视觉记录联合转录每个说话者的语音，并将其聚类到各自的对话中。音频基线方法词错误率超过100%，而加入视觉线索可显著改善50%。

Result: 纯音频基线系统表现不佳，词错误率超过100%；结合视觉线索后性能大幅提升50%，证明了多模态方法的重要性。

Conclusion: 多模态上下文感知识别是解决极端语音重叠对话场景的有效方法，视觉线索对提高系统性能至关重要。

Abstract: We introduce the task of Multi-Modal Context-Aware Recognition (MCoRec) in
the ninth CHiME Challenge, which addresses the cocktail-party problem of
overlapping conversations in a single-room setting using audio, visual, and
contextual cues. MCoRec captures natural multi-party conversations where the
recordings focus on unscripted, casual group chats, leading to extreme speech
overlap of up to 100% and highly fragmented conversational turns. The task
requires systems to answer the question "Who speaks when, what, and with whom?"
by jointly transcribing each speaker's speech and clustering them into their
respective conversations from audio-visual recordings. Audio-only baselines
exceed 100% word error rate, whereas incorporating visual cues yields
substantial 50% improvements, highlighting the importance of multi-modality. In
this manuscript, we present the motivation behind the task, outline the data
collection process, and report the baseline systems developed for the MCoRec.

</details>


### [98] [DCMM-SQL: Automated Data-Centric Pipeline and Multi-Model Collaboration Training for Text-to-SQL Model](https://arxiv.org/abs/2510.23284)
*Yuanzhen Xie,Liu Ye,Jiqun Chu,Mochi Gao,Hehuan Liu,Yunzhi Tan,Bo Hu,Zang Li*

Main category: cs.CL

TL;DR: 提出了一个完全自动化的数据为中心文本到SQL任务管道，包括自适应数据修复和错误数据增强，以及多模型协作训练策略，在轻量级文本到SQL模型中取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 虽然基于代理的框架在文本到SQL任务中取得了显著改进，但数据为中心策略的影响很少被探索。单一微调模型的能力有限，需要更有效的方法来提升性能。

Method: 设计了全自动数据为中心管道：自适应数据修复自动发现和修复训练数据错误；错误数据增强扩散和增强模型预测的错误数据；多模型协作训练用不同增强数据训练多个模型；集成策略整合多个模型能力解决多选题。

Result: 实验结果表明数据为中心管道和多模型交互迭代策略有效，在轻量级文本到SQL模型（70B以内）中取得了第一名。

Conclusion: 数据为中心的方法和多模型协作策略能够显著提升文本到SQL任务的性能，证明了数据质量和模型多样性的重要性。

Abstract: Text-to-SQL tasks have gained attractive improvements since the release of
ChatGPT. Among them, agent-based frameworks have been widely used in this
field. However, the impact of data-centric strategies on text-to-SQL tasks has
rarely been explored. In this paper, we systemically design a fully automated
data-centric pipeline for text-to-SQL tasks, including \emph{adaptive data
repair}, which can automatically find and fix errors in the training dataset;
and \emph{error data augmentation}, where we specifically diffuse and enhance
erroneous data predicted by the initially trained models. Meanwhile, we propose
a Multi-Model collaboration training schema, aiming to train multiple models
with different augmented data, enabling them to possess distinct capabilities
and work together to complement each other, because it has been found that the
capability of a single fine-tuned model is very limited. Furthermore, we
utilize an ensemble strategy to integrate the capabilities of multiple models
to solve a multiple-choice question, aiming to further improve the accuracy of
text-to-SQL tasks. The experiment results and ablation study have demonstrated
the effectiveness of data-centric pipeline and Multi-Model(MM) interactive
iterative strategies, achieving first place in lightweight text-to-SQL models
(within 70B).

</details>


### [99] [Arabic Little STT: Arabic Children Speech Recognition Dataset](https://arxiv.org/abs/2510.23319)
*Mouhand Alkadri,Dania Desouki,Khloud Al Jallad*

Main category: cs.CL

TL;DR: 提出了阿拉伯语儿童语音数据集Arabic Little STT，评估了Whisper模型在儿童语音识别上的表现，发现其性能远低于成人语音，强调了儿童语音基准测试和包容性训练数据的重要性。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言（如阿拉伯语）中儿童特定语音语料库的缺失问题，填补儿童语音数据稀缺的空白。

Method: 创建了包含355个话语的Levantine阿拉伯语儿童语音数据集，系统评估了8个Whisper变体在该数据集上的表现，并与成人阿拉伯语基准进行比较。

Result: 即使表现最好的Whisper Large_v3模型在儿童语音上的词错误率也高达0.66，远高于其在成人数据集上的低于0.20的词错误率。

Conclusion: 需要专门的儿童语音基准测试和包容性训练数据，此类数据必须遵循严格的伦理和隐私框架来保护儿童敏感信息。

Abstract: The performance of Artificial Intelligence (AI) systems fundamentally depends
on high-quality training data. However, low-resource languages like Arabic
suffer from severe data scarcity. Moreover, the absence of child-specific
speech corpora is an essential gap that poses significant challenges. To
address this gap, we present our created dataset, Arabic Little STT, a dataset
of Levantine Arabic child speech recorded in classrooms, containing 355
utterances from 288 children (ages 6 - 13). We further conduct a systematic
assessment of Whisper, a state-of-the-art automatic speech recognition (ASR)
model, on this dataset and compare its performance with adult Arabic
benchmarks. Our evaluation across eight Whisper variants reveals that even the
best-performing model (Large_v3) struggles significantly, achieving a 0.66 word
error rate (WER) on child speech, starkly contrasting with its sub 0.20 WER on
adult datasets. These results align with other research on English speech.
Results highlight the critical need for dedicated child speech benchmarks and
inclusive training data in ASR development. Emphasizing that such data must be
governed by strict ethical and privacy frameworks to protect sensitive child
information. We hope that this study provides an initial step for future work
on equitable speech technologies for Arabic-speaking children. We hope that our
publicly available dataset enrich the children's demographic representation in
ASR datasets.

</details>


### [100] [Adaptive Blockwise Search: Inference-Time Alignment for Large Language Models](https://arxiv.org/abs/2510.23334)
*Mohammad Atif Quamar,Mohammad Areeb,Nishant Sharma,Ananth Shreekumar,Jonathan Rosenthal,Muslum Ozgur Ozmen,Mikhail Kuznetsov,Z. Berkay Celik*

Main category: cs.CL

TL;DR: AdaSearch是一种新的块状搜索策略，通过自适应分配固定计算预算，将搜索精力集中在关键的前几个token上，在无害性生成、情感控制和数学推理任务中相比Best-of-N基线提升超过10%的胜率。


<details>
  <summary>Details</summary>
Motivation: LLM对齐仍然是一个关键挑战。推理时方法提供了微调的灵活替代方案，但它们均匀的计算努力通常产生次优的对齐效果。我们假设对于许多对齐任务，响应的初始token不成比例地更为关键。

Method: 引入AdaSearch，一种新颖的块状搜索策略。它使用采样计划自适应分配固定计算预算，将搜索精力集中在这些关键token上。我们将AdaSearch应用于顺序解码，并引入其树搜索对应物AdaBeam。

Result: 在八个LLM上的全面评估表明，AdaSearch优于强大的Best-of-N和微调基线。具体而言，在无害性生成、受控情感生成和数学推理任务中，相对于Best-of-N，胜率提高了超过10%。

Conclusion: AdaSearch通过自适应计算分配策略有效提升了LLM对齐性能，证明了将计算资源集中在关键初始token上的有效性。

Abstract: LLM alignment remains a critical challenge. Inference-time methods provide a
flexible alternative to fine-tuning, but their uniform computational effort
often yields suboptimal alignment. We hypothesize that for many alignment
tasks, the initial tokens of a response are disproportionately more critical.
To leverage this principle, we introduce AdaSearch, a novel blockwise search
strategy. It adaptively allocates a fixed computational budget using a sampling
schedule, focusing search effort on these critical tokens. We apply AdaSearch
to sequential decoding and introduce its tree-search counterpart, AdaBeam. Our
comprehensive evaluation across eight LLMs demonstrates that AdaSearch
outperforms strong Best-of-N and fine-tuning baselines. Specifically, win-rates
improve by over 10% for harmlessness generation, controlled sentiment
generation, and for mathematical reasoning tasks relative to Best-of-N.

</details>


### [101] [BaZi-Based Character Simulation Benchmark: Evaluating AI on Temporal and Persona Reasoning](https://arxiv.org/abs/2510.23337)
*Siyuan Zheng,Pai Liu,Xi Chen,Jizheng Dong,Sihan Jia*

Main category: cs.CL

TL;DR: 提出了首个基于八字命理的人格推理QA数据集和BaZi-LLM系统，将符号推理与大型语言模型结合，生成时间动态和细粒度的虚拟人格，相比主流LLM准确率提升30.3%-62.6%。


<details>
  <summary>Details</summary>
Motivation: 当前虚拟角色生成方法依赖标注数据或手工制作的人格提示，难以扩展且难以生成真实、上下文连贯的人格。

Method: 创建八字命理人格推理QA数据集，将人类经验分类为财富、健康、亲情、事业和关系等生活事件问答；提出BaZi-LLM系统，整合符号推理与大型语言模型。

Result: 相比DeepSeek-v3和GPT-5-mini等主流LLM，准确率提升30.3%-62.6%；当使用错误的八字信息时，模型准确率下降20%-45%。

Conclusion: 基于文化根基的符号-LLM整合在真实角色模拟方面具有潜力。

Abstract: Human-like virtual characters are crucial for games, storytelling, and
virtual reality, yet current methods rely heavily on annotated data or
handcrafted persona prompts, making it difficult to scale up and generate
realistic, contextually coherent personas. We create the first QA dataset for
BaZi-based persona reasoning, where real human experiences categorized into
wealth, health, kinship, career, and relationships are represented as
life-event questions and answers. Furthermore, we propose the first BaZi-LLM
system that integrates symbolic reasoning with large language models to
generate temporally dynamic and fine-grained virtual personas. Compared with
mainstream LLMs such as DeepSeek-v3 and GPT-5-mini, our method achieves a
30.3%-62.6% accuracy improvement. In addition, when incorrect BaZi information
is used, our model's accuracy drops by 20%-45%, showing the potential of
culturally grounded symbolic-LLM integration for realistic character
simulation.

</details>


### [102] [LightKGG: Simple and Efficient Knowledge Graph Generation from Textual Data](https://arxiv.org/abs/2510.23341)
*Teng Lin*

Main category: cs.CL

TL;DR: LightKGG是一个使用小型语言模型(SLMs)从文本数据中高效提取知识图谱的框架，通过上下文集成图提取和拓扑增强关系推理两个关键技术，在低资源环境下实现准确的知识图谱构建。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱提取方法依赖错误率高的模式匹配技术或资源密集型大语言模型(LLMs)，高质量知识图谱的稀缺成为下游AI应用的关键瓶颈，且现有基于LLM的工具计算需求大，限制了在低资源环境中的可访问性。

Method: 提出LightKGG框架，包含两个关键技术：(1)上下文集成图提取：将上下文信息与节点和边集成到统一图结构中，减少对复杂语义处理的依赖；(2)拓扑增强关系推理：利用提取图的固有拓扑结构高效推断关系，无需依赖LLMs的复杂语言理解能力。

Result: 该框架能够在最小硬件要求下实现准确的知识图谱构建，填补了自动化知识提取与实际部署场景之间的差距。

Conclusion: LightKGG通过优化SLM在结构化NLP任务中的效率，为低资源环境下的知识图谱提取提供了科学严谨的方法，实现了知识提取自动化与实用部署的平衡。

Abstract: The scarcity of high-quality knowledge graphs (KGs) remains a critical
bottleneck for downstream AI applications, as existing extraction methods rely
heavily on error-prone pattern-matching techniques or resource-intensive large
language models (LLMs). While recent tools leverage LLMs to generate KGs, their
computational demands limit accessibility for low-resource environments. Our
paper introduces LightKGG, a novel framework that enables efficient KG
extraction from textual data using small-scale language models (SLMs) through
two key technical innovations: (1) Context-integrated Graph extraction
integrates contextual information with nodes and edges into a unified graph
structure, reducing the reliance on complex semantic processing while
maintaining more key information; (2) Topology-enhanced relationship inference
leverages the inherent topology of the extracted graph to efficiently infer
relationships, enabling relationship discovery without relying on complex
language understanding capabilities of LLMs. By enabling accurate KG
construction with minimal hardware requirements, this work bridges the gap
between automated knowledge extraction and practical deployment scenarios while
introducing scientifically rigorous methods for optimizing SLM efficiency in
structured NLP tasks.

</details>


### [103] [How AI Forecasts AI Jobs: Benchmarking LLM Predictions of Labor Market Changes](https://arxiv.org/abs/2510.23358)
*Sheri Osborn,Rohit Valecha,H. Raghav Rao,Dan Sass,Anthony Rios*

Main category: cs.CL

TL;DR: 该论文提出了一个评估大语言模型预测AI对就业影响能力的基准，结合了美国部门级职位发布数据和全球AI采用导致的职业变化预测数据。


<details>
  <summary>Details</summary>
Motivation: 人工智能正在重塑劳动力市场，但缺乏系统预测其对就业影响的工具。现有研究表明LLMs能够提取情感、总结经济报告和模拟预测者行为，但很少评估其在前瞻性劳动力预测中的应用。

Method: 结合两个互补数据集：美国部门级高频职位发布指数和全球AI采用导致的职业变化预测数据，构建具有明确时间分割的预测任务。评估多种提示策略，包括任务支架、角色驱动和混合方法。

Result: 结构化任务提示持续提高预测稳定性，角色提示在短期趋势上有优势。但不同部门和时间范围的性能差异显著，需要领域感知提示和严格评估协议。

Conclusion: 该基准支持未来劳动力预测、提示设计和基于LLM的经济推理研究，为研究AI作为劳动力市场预测工具的局限性和机会提供了可复现的测试平台。

Abstract: Artificial intelligence is reshaping labor markets, yet we lack tools to
systematically forecast its effects on employment. This paper introduces a
benchmark for evaluating how well large language models (LLMs) can anticipate
changes in job demand, especially in occupations affected by AI. Existing
research has shown that LLMs can extract sentiment, summarize economic reports,
and emulate forecaster behavior, but little work has assessed their use for
forward-looking labor prediction. Our benchmark combines two complementary
datasets: a high-frequency index of sector-level job postings in the United
States, and a global dataset of projected occupational changes due to AI
adoption. We format these data into forecasting tasks with clear temporal
splits, minimizing the risk of information leakage. We then evaluate LLMs using
multiple prompting strategies, comparing task-scaffolded, persona-driven, and
hybrid approaches across model families. We assess both quantitative accuracy
and qualitative consistency over time. Results show that structured task
prompts consistently improve forecast stability, while persona prompts offer
advantages on short-term trends. However, performance varies significantly
across sectors and horizons, highlighting the need for domain-aware prompting
and rigorous evaluation protocols. By releasing our benchmark, we aim to
support future research on labor forecasting, prompt design, and LLM-based
economic reasoning. This work contributes to a growing body of research on how
LLMs interact with real-world economic data, and provides a reproducible
testbed for studying the limits and opportunities of AI as a forecasting tool
in the context of labor markets.

</details>


### [104] [Detecting Religious Language in Climate Discourse](https://arxiv.org/abs/2510.23395)
*Evy Beijen,Pien Pieterse,Yusuf Çelik,Willem Th. van Peursen,Sandjai Bhulai,Meike Morren*

Main category: cs.CL

TL;DR: 本文研究了宗教语言在气候相关文本中的表现形式，比较了基于规则的方法和大型语言模型在检测宗教语言方面的表现。


<details>
  <summary>Details</summary>
Motivation: 宗教语言在当代话语中持续存在，甚至在环境行动主义和气候变化辩论等表面上世俗的领域中也很普遍。研究旨在了解宗教语言如何在世俗和宗教非政府组织的气候相关文本中显性和隐性出现。

Method: 采用双重方法：基于规则的模型（使用生态神学文献中的宗教术语层次树）和在零样本设置下运行的大型语言模型。使用包含超过88万句子的数据集进行比较分析。

Result: 结果显示，基于规则的方法比大型语言模型更一致地将句子标记为宗教语言。这突显了计算检测宗教语言的方法学挑战。

Conclusion: 研究揭示了宗教语言检测中词汇定义与上下文意义之间的张力，展示了数字方法在宗教研究中的潜力和局限性，特别是在分析神圣在气候话语中持续存在的方式方面。

Abstract: Religious language continues to permeate contemporary discourse, even in
ostensibly secular domains such as environmental activism and climate change
debates. This paper investigates how explicit and implicit forms of religious
language appear in climate-related texts produced by secular and religious
nongovernmental organizations (NGOs). We introduce a dual methodological
approach: a rule-based model using a hierarchical tree of religious terms
derived from ecotheology literature, and large language models (LLMs) operating
in a zero-shot setting. Using a dataset of more than 880,000 sentences, we
compare how these methods detect religious language and analyze points of
agreement and divergence. The results show that the rule-based method
consistently labels more sentences as religious than LLMs. These findings
highlight not only the methodological challenges of computationally detecting
religious language but also the broader tension over whether religious language
should be defined by vocabulary alone or by contextual meaning. This study
contributes to digital methods in religious studies by demonstrating both the
potential and the limitations of approaches for analyzing how the sacred
persists in climate discourse.

</details>


### [105] [EMTSF:Extraordinary Mixture of SOTA Models for Time Series Forecasting](https://arxiv.org/abs/2510.23396)
*Musleh Alharthi,Kaleel Mahmood,Sarosh Patel,Ausif Mahmood*

Main category: cs.CL

TL;DR: 本文提出了一种基于混合专家(MoE)框架的时间序列预测模型，通过整合xLSTM、增强线性模型、PatchTST和minGRU等多种先进模型，在Transformer门控网络中进行集成，在标准基准测试中超越了所有现有TSF模型。


<details>
  <summary>Details</summary>
Motivation: 针对时间序列预测领域Transformer模型有效性的争议，以及数据更偏向近期历史且易受不可预测事件影响的挑战，需要开发更强大的预测框架。

Method: 采用混合专家(MoE)框架，整合xLSTM、增强线性模型、PatchTST、minGRU等多种互补的SOTA模型，通过Transformer门控网络进行集成。

Result: 在标准基准测试中，提出的模型超越了所有现有的时间序列预测模型，包括最新的基于MoE框架的方法。

Conclusion: 混合专家框架能够有效整合多种互补的时间序列预测模型，通过Transformer门控网络实现最优性能，为时间序列预测提供了强大的解决方案。

Abstract: The immense success of the Transformer architecture
  in Natural Language Processing has led to its adoption in Time Se ries
Forecasting (TSF), where superior performance has been shown.
  However, a recent important paper questioned their effectiveness by
  demonstrating that a simple single layer linear model outperforms
  Transformer-based models. This was soon shown to be not as valid,
  by a better transformer-based model termed PatchTST. More re cently, TimeLLM
demonstrated even better results by repurposing a
  Large Language Model (LLM) for the TSF domain. Again, a follow
  up paper challenged this by demonstrating that removing the LLM
  component or replacing it with a basic attention layer in fact yields
  better performance. One of the challenges in forecasting is the fact
  that TSF data favors the more recent past, and is sometimes subject
  to unpredictable events. Based upon these recent insights in TSF, we
  propose a strong Mixture of Experts (MoE) framework. Our method
  combines the state-of-the-art (SOTA) models including xLSTM, en hanced
Linear, PatchTST, and minGRU, among others. This set of
  complimentary and diverse models for TSF are integrated in a Trans former
based MoE gating network. Our proposed model outperforms
  all existing TSF models on standard benchmarks, surpassing even the
  latest approaches based on MoE frameworks.

</details>


### [106] [Omni-Reward: Towards Generalist Omni-Modal Reward Modeling with Free-Form Preferences](https://arxiv.org/abs/2510.23451)
*Zhuoran Jin,Hongbang Yuan,Kejian Zhu,Jiachun Li,Pengfei Cao,Yubo Chen,Kang Liu,Jun Zhao*

Main category: cs.CL

TL;DR: 提出了Omni-Reward，一个支持自由形式偏好的通用多模态奖励模型，解决了模态不平衡和偏好刚性问题。


<details>
  <summary>Details</summary>
Motivation: 奖励模型面临两个基本挑战：模态不平衡（主要关注文本和图像，对其他模态支持有限）和偏好刚性（固定二元偏好对无法捕捉个性化偏好的复杂性）。

Method: 包括三个部分：Omni-RewardBench（首个多模态奖励模型基准）、Omni-RewardData（多模态偏好数据集，包含31.7万对偏好数据）、Omni-RewardModel（判别式和生成式奖励模型）。

Result: 在Omni-RewardBench以及其他广泛使用的奖励建模基准上取得了强劲性能。

Conclusion: Omni-Reward是朝着通用多模态奖励建模迈出的重要一步，支持自由形式的偏好表达。

Abstract: Reward models (RMs) play a critical role in aligning AI behaviors with human
preferences, yet they face two fundamental challenges: (1) Modality Imbalance,
where most RMs are mainly focused on text and image modalities, offering
limited support for video, audio, and other modalities; and (2) Preference
Rigidity, where training on fixed binary preference pairs fails to capture the
complexity and diversity of personalized preferences. To address the above
challenges, we propose Omni-Reward, a step toward generalist omni-modal reward
modeling with support for free-form preferences, consisting of: (1) Evaluation:
We introduce Omni-RewardBench, the first omni-modal RM benchmark with free-form
preferences, covering nine tasks across five modalities including text, image,
video, audio, and 3D; (2) Data: We construct Omni-RewardData, a multimodal
preference dataset comprising 248K general preference pairs and 69K
instruction-tuning pairs for training generalist omni-modal RMs; (3) Model: We
propose Omni-RewardModel, which includes both discriminative and generative
RMs, and achieves strong performance on Omni-RewardBench as well as other
widely used reward modeling benchmarks.

</details>


### [107] [BrowseConf: Confidence-Guided Test-Time Scaling for Web Agents](https://arxiv.org/abs/2510.23458)
*Litu Ou,Kuan Li,Huifeng Yin,Liwen Zhang,Zhongwang Zhang,Xixi Wu,Rui Ye,Zile Qiao,Yong Jiang,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.CL

TL;DR: 该论文研究LLM搜索代理在多轮交互中通过语言化置信度分数表达自身置信度的能力，并提出基于置信度的测试时缩放方法，在保持性能的同时显著减少token消耗。


<details>
  <summary>Details</summary>
Motivation: 现有置信度研究主要关注单轮场景，而在复杂多轮交互中的置信度研究有限。论文旨在探索LLM搜索代理是否能在长序列动作后通过语言化置信度分数传达自身置信度。

Method: 在开源代理模型上进行实验，发现模型在高置信度时任务准确率更高。基于此提出测试时缩放方法，使用置信度分数判断答案质量，鼓励模型重试直到达到满意的置信度水平。

Result: 实验结果显示，模型在高置信度时任务准确率显著更高，低置信度时准确率接近零。提出的测试时缩放方法在保持竞争力的同时显著减少了token消耗。

Conclusion: LLM搜索代理能够在多轮交互中有效表达置信度，基于置信度的测试时缩放方法能够优化资源使用效率，在减少计算成本的同时维持模型性能。

Abstract: Confidence in LLMs is a useful indicator of model uncertainty and answer
reliability. Existing work mainly focused on single-turn scenarios, while
research on confidence in complex multi-turn interactions is limited. In this
paper, we investigate whether LLM-based search agents have the ability to
communicate their own confidence through verbalized confidence scores after
long sequences of actions, a significantly more challenging task compared to
outputting confidence in a single interaction. Experimenting on open-source
agentic models, we first find that models exhibit much higher task accuracy at
high confidence while having near-zero accuracy when confidence is low. Based
on this observation, we propose Test-Time Scaling (TTS) methods that use
confidence scores to determine answer quality, encourage the model to try again
until reaching a satisfactory confidence level. Results show that our proposed
methods significantly reduce token consumption while demonstrating competitive
performance compared to baseline fixed budget TTS methods.

</details>


### [108] [Evaluating Large Language Models for Stance Detection on Financial Targets from SEC Filing Reports and Earnings Call Transcripts](https://arxiv.org/abs/2510.23464)
*Nikesh Gyawali,Doina Caragea,Alex Vasenkov,Cornelia Caragea*

Main category: cs.CL

TL;DR: 构建了一个针对金融指标（债务、每股收益、销售额）的句子级立场检测语料库，使用ChatGPT-o3-pro标注并在人类验证下，评估了LLM在零样本、少样本和思维链提示下的表现。


<details>
  <summary>Details</summary>
Motivation: SEC文件和财报电话会议记录对投资者、审计师和监管机构很重要，但其长度、专业术语和微妙语言使细粒度分析困难，且传统方法需要大量昂贵标注数据。

Method: 从10-K年报和财报电话会议记录中提取句子，使用ChatGPT-o3-pro模型标注立场（积极、消极、中性）并进行严格人工验证，系统评估LLM在零样本、少样本和思维链提示下的表现。

Result: 少样本结合思维链提示表现最佳，优于监督基线，LLM在SEC和ECT数据集上的表现存在差异。

Conclusion: 研究证明了在不依赖大量标注数据的情况下，利用LLM进行金融领域目标特定立场检测的实践可行性。

Abstract: Financial narratives from U.S. Securities and Exchange Commission (SEC)
filing reports and quarterly earnings call transcripts (ECTs) are very
important for investors, auditors, and regulators. However, their length,
financial jargon, and nuanced language make fine-grained analysis difficult.
Prior sentiment analysis in the financial domain required a large, expensive
labeled dataset, making the sentence-level stance towards specific financial
targets challenging. In this work, we introduce a sentence-level corpus for
stance detection focused on three core financial metrics: debt, earnings per
share (EPS), and sales. The sentences were extracted from Form 10-K annual
reports and ECTs, and labeled for stance (positive, negative, neutral) using
the advanced ChatGPT-o3-pro model under rigorous human validation. Using this
corpus, we conduct a systematic evaluation of modern large language models
(LLMs) using zero-shot, few-shot, and Chain-of-Thought (CoT) prompting
strategies. Our results show that few-shot with CoT prompting performs best
compared to supervised baselines, and LLMs' performance varies across the SEC
and ECT datasets. Our findings highlight the practical viability of leveraging
LLMs for target-specific stance in the financial domain without requiring
extensive labeled data.

</details>


### [109] [MMTutorBench: The First Multimodal Benchmark for AI Math Tutoring](https://arxiv.org/abs/2510.23477)
*Tengchao Yang,Sichen Guo,Mengzhao Jia,Jiaming Su,Yuanyang Liu,Zhihan Zhang,Meng Jiang*

Main category: cs.CL

TL;DR: MMTutorBench是首个AI数学辅导基准，包含685个围绕关键教学步骤构建的问题，通过六个维度评估模型在洞察发现、操作制定和操作执行三个任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有基准大多忽视了数学辅导所需的诊断学生困难和逐步引导的能力，而多模态大语言模型在这方面具有潜力。

Method: 构建包含685个问题的基准，每个问题配有特定评分标准，评估12个领先MLLM在三个任务中的表现，并比较OCR、少样本提示等变体的影响。

Result: 发现专有模型与开源系统之间存在明显性能差距，与人类导师相比仍有很大提升空间，OCR流程会降低辅导质量，少样本提示效果有限，基于评分标准的LLM评判高度可靠。

Conclusion: MMTutorBench既展示了AI数学辅导的难度，也证明了其诊断价值，有助于推动AI辅导技术的发展。

Abstract: Effective math tutoring requires not only solving problems but also
diagnosing students' difficulties and guiding them step by step. While
multimodal large language models (MLLMs) show promise, existing benchmarks
largely overlook these tutoring skills. We introduce MMTutorBench, the first
benchmark for AI math tutoring, consisting of 685 problems built around
pedagogically significant key-steps. Each problem is paired with
problem-specific rubrics that enable fine-grained evaluation across six
dimensions, and structured into three tasks-Insight Discovery, Operation
Formulation, and Operation Execution. We evaluate 12 leading MLLMs and find
clear performance gaps between proprietary and open-source systems, substantial
room compared to human tutors, and consistent trends across input variants: OCR
pipelines degrade tutoring quality, few-shot prompting yields limited gains,
and our rubric-based LLM-as-a-Judge proves highly reliable. These results
highlight both the difficulty and diagnostic value of MMTutorBench for
advancing AI tutoring.

</details>


### [110] [M4FC: a Multimodal, Multilingual, Multicultural, Multitask Real-World Fact-Checking Dataset](https://arxiv.org/abs/2510.23508)
*Jiahui Geng,Jonathan Tonglet,Iryna Gurevych*

Main category: cs.CL

TL;DR: M4FC是一个新的多模态事实核查数据集，包含4,982张图像和6,980个声明，涵盖10种语言和6个多模态事实核查任务。


<details>
  <summary>Details</summary>
Motivation: 现有数据集存在样本少、语言单一、证据泄露等问题，需要构建更全面、真实的多模态事实核查数据集。

Method: 收集专业事实核查机构验证的图像和声明，构建包含6个任务的多模态数据集，并提供基线模型结果。

Result: 创建了M4FC数据集，包含多样化的文化和地理背景，并展示了中间任务组合对下游任务性能的影响。

Conclusion: M4FC填补了多模态事实核查数据集的空白，为相关研究提供了有价值的资源。

Abstract: Existing real-world datasets for multimodal automated fact-checking have
multiple limitations: they contain few instances, focus on only one or two
languages and tasks, suffer from evidence leakage, or depend on external sets
of news articles for sourcing true claims. To address these shortcomings, we
introduce M4FC, a new real-world dataset comprising 4,982 images paired with
6,980 claims. The images, verified by professional fact-checkers from 22
organizations, represent diverse cultural and geographic contexts. Each claim
is available in one or two out of ten languages. M4FC spans six multimodal
fact-checking tasks: visual claim extraction, claimant intent prediction, fake
detection, image contextualization, location verification, and verdict
prediction. We provide baseline results for all tasks and analyze how combining
intermediate tasks influence downstream verdict prediction performance. We make
our dataset and code available.

</details>


### [111] [IPQA: A Benchmark for Core Intent Identification in Personalized Question Answering](https://arxiv.org/abs/2510.23536)
*Jieyong Kim,Maryam Amirizaniani,Soojin Yoon,Dongha Lee*

Main category: cs.CL

TL;DR: 提出了IPQA基准，用于评估个性化问答中的核心意图识别能力，填补了现有基准仅评估回答质量或检索性能的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准仅评估回答质量或检索性能，无法直接衡量意图识别能力。由于用户不会明确说明其优先考虑的意图，系统难以生成满足个体信息需求的回答。

Method: 基于满意理论，从用户选择答案的可观察行为模式中推导核心意图。通过系统筛选、基于LLM的注释以及结合自动验证和人工验证的严格质量控制构建多领域数据集。

Result: 对最先进语言模型的实验评估显示，当前系统在个性化情境下的核心意图识别方面表现不佳。模型无法从用户历史中识别核心意图，且随着问题复杂度增加性能下降。

Conclusion: 核心意图识别是PQA的关键能力，当前模型在此方面存在显著不足。IPQA基准和数据集将公开以促进该方向的研究。

Abstract: Intent identification serves as the foundation for generating appropriate
responses in personalized question answering (PQA). However, existing
benchmarks evaluate only response quality or retrieval performance without
directly measuring intent identification capabilities. This gap is critical
because without understanding which intents users prioritize, systems cannot
generate responses satisfying individual information needs. To address this, we
introduce the concept of core intents: intents users prioritize when selecting
answers to satisfy their information needs. To evaluate these core intents, we
propose IPQA, a benchmark for core Intent identification in Personalized
Question Answering. Since users do not explicitly state their prioritized
intents, we derive core intents from observable behavior patterns in answer
selection, grounded in satisficing theory where users choose answers meeting
their acceptance thresholds. We construct a dataset with various domains
through systematic filtering, LLM-based annotation, and rigorous quality
control combining automated verification with human validation. Experimental
evaluations across state-of-the-art language models reveal that current systems
struggle with core intent identification in personalized contexts. Models fail
to identify core intents from user histories, with performance degrading as
question complexity increases. The code and dataset will be made publicly
available to facilitate future research in this direction.

</details>


### [112] [LimRank: Less is More for Reasoning-Intensive Information Reranking](https://arxiv.org/abs/2510.23544)
*Tingyu Song,Yilun Zhao,Siyue Zhang,Chen Zhao,Arman Cohan*

Main category: cs.CL

TL;DR: LIMRANK是一种使用少量高质量合成数据进行微调的LLM重排序器，在推理密集型检索和指令跟随检索任务中表现优异，仅需不到5%的传统训练数据。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖大规模微调来适应LLM进行信息重排序任务，计算成本高昂。本文旨在证明现代LLM可以通过最少的高质量监督有效适应重排序任务。

Method: 设计了LIMRANK-SYNTHESIZER管道生成多样化、具有挑战性和真实性的重排序示例，使用这些合成数据微调重排序模型LIMRANK。

Result: LIMRANK在两个具有挑战性的基准测试（BRIGHT和FollowIR）中实现了有竞争力的性能，同时训练数据量不到先前工作的5%。消融研究验证了LIMRANK-SYNTHESIZER的有效性和LIMRANK在下游任务中的强泛化能力。

Conclusion: 通过高质量的合成数据生成和最小化监督，可以高效地训练出性能优异的LLM重排序器，显著降低计算成本，并在多个下游任务中展现出良好的泛化能力。

Abstract: Existing approaches typically rely on large-scale fine-tuning to adapt LLMs
for information reranking tasks, which is computationally expensive. In this
work, we demonstrate that modern LLMs can be effectively adapted using only
minimal, high-quality supervision. To enable this, we design
LIMRANK-SYNTHESIZER, a reusable and open-source pipeline for generating
diverse, challenging, and realistic reranking examples. Using this synthetic
data, we fine-tune our reranker model, LIMRANK. We evaluate LIMRANK on two
challenging benchmarks, i.e., BRIGHT for reasoning-intensive retrieval and
FollowIR for instruction-following retrieval. Our experiments demonstrate that
LIMRANK achieves competitive performance, while being trained on less than 5%
of the data typically used in prior work. Further ablation studies demonstrate
the effectiveness of LIMRANK-SYNTHESIZER and the strong generalization
capabilities of LIMRANK across downstream tasks, including scientific
literature search and retrieval-augmented generation for knowledge-intensive
problem solving.

</details>


### [113] [Hope Speech Detection in Social Media English Corpora: Performance of Traditional and Transformer Models](https://arxiv.org/abs/2510.23585)
*Luis Ramos,Hiram Calvo,Olga Kolesnikova*

Main category: cs.CL

TL;DR: 该研究评估了传统机器学习模型和微调transformer在希望语音检测任务上的表现，发现transformer模型在精度和召回率方面表现更好。


<details>
  <summary>Details</summary>
Motivation: 识别希望语音已成为重要的NLP任务，需要在社交媒体平台上检测具有动机性和目标导向行为的表达。

Method: 使用传统机器学习模型（SVM、逻辑回归、朴素贝叶斯）和微调的transformer模型，在预分割的希望语音数据集上进行评估。

Result: 传统模型中线性核SVM和逻辑回归的宏F1为0.78；transformer模型表现更好，最佳模型加权精度0.82、加权召回率0.80、加权F1 0.79、宏F1 0.79、准确率0.80。

Conclusion: 虽然优化配置的传统机器学习模型仍然有效，但transformer架构能检测希望语音的细微语义，在小数据集上表现更好，表明大型transformer和LLM可能具有更好性能。

Abstract: The identification of hope speech has become a promised NLP task, considering
the need to detect motivational expressions of agency and goal-directed
behaviour on social media platforms. This proposal evaluates traditional
machine learning models and fine-tuned transformers for a previously split hope
speech dataset as train, development and test set. On development test, a
linear-kernel SVM and logistic regression both reached a macro-F1 of 0.78; SVM
with RBF kernel reached 0.77, and Na\"ive Bayes hit 0.75. Transformer models
delivered better results, the best model achieved weighted precision of 0.82,
weighted recall of 0.80, weighted F1 of 0.79, macro F1 of 0.79, and 0.80
accuracy. These results suggest that while optimally configured traditional
machine learning models remain agile, transformer architectures detect some
subtle semantics of hope to achieve higher precision and recall in hope speech
detection, suggesting that larges transformers and LLMs could perform better in
small datasets.

</details>


### [114] [Think Twice: Branch-and-Rethink Reasoning Reward Model](https://arxiv.org/abs/2510.23596)
*Yizhu Jiao,Jiaqi Zeng,Julien Veron Vialard,Oleksii Kuchaiev,Jiawei Han,Olivier Delalleau*

Main category: cs.CL

TL;DR: 提出BR-RM模型，将"三思而后行"原则应用于奖励建模，通过两轮推理减少判断扩散问题，在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型将多个质量维度压缩为单一标量，导致注意力分散、判断扩散和浅层分析。

Method: 采用两轮推理：第一轮自适应分支选择关键维度并生成假设，第二轮基于分支的重新思考验证假设并聚焦重要内容。

Result: 在三个具有挑战性的奖励建模基准测试中实现了最先进的性能。

Conclusion: BR-RM通过将一次性评分转换为聚焦的二次推理，减少判断扩散，提高对细微但重要错误的敏感性，同时保持实用性和可扩展性。

Abstract: Large language models (LLMs) increasingly rely on thinking models that
externalize intermediate steps and allocate extra test-time compute, with
think-twice strategies showing that a deliberate second pass can elicit
stronger reasoning. In contrast, most reward models (RMs) still compress many
quality dimensions into a single scalar in one shot, a design that induces
judgment diffusion: attention spreads across evaluation criteria, yielding
diluted focus and shallow analysis. We introduce branch-and-rethink (BR-RM), a
two-turn RM that transfers the think-twice principle to reward modeling. Turn 1
performs adaptive branching, selecting a small set of instance-critical
dimensions (such as factuality and safety) and sketching concise,
evidence-seeking hypotheses. Turn 2 executes branch-conditioned rethinking, a
targeted reread that tests those hypotheses and scrutinizes only what matters
most. We train with GRPO-style reinforcement learning over structured two-turn
traces using a simple binary outcome reward with strict format checks, making
the approach compatible with standard RLHF pipelines. By converting
all-at-oncescoringintofocused, second-lookreasoning,
BR-RMreducesjudgmentdiffusionandimproves sensitivity to subtle yet
consequential errors while remaining practical and scalable. Experimental
results demonstrate that our model achieves state-of-the-art performance on
three challenging reward modeling benchmarks across diverse domains. The code
and the model will be released soon.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [115] [Diagnosing Bottlenecks in Data Visualization Understanding by Vision-Language Models](https://arxiv.org/abs/2510.21740)
*Alexa R. Tartaglini,Satchel Grant,Daniel Wurgaft,Christopher Potts,Judith E. Fan*

Main category: cs.CV

TL;DR: FUGU是一个数据可视化理解任务套件，用于诊断视觉语言模型在数据可视化理解中的错误来源。研究发现错误主要源于视觉-语言模块间的信息传递问题，而非视觉编码本身。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在数据可视化理解任务上表现不佳，但失败原因尚不清楚。研究旨在确定错误是源于视觉信息编码、视觉-语言模块间信息传递，还是语言模块内部处理的问题。

Method: 开发FUGU任务套件，使用激活修补和线性探针技术追踪信息流，分析三种广泛使用的视觉语言模型在不同提示策略下的表现。

Result: 模型难以正确生成单个数据点坐标，这些初始错误导致最终响应错误。当提供正确坐标时性能显著提升，但多数据点统计关系任务表现反而下降。微调也无法达到最佳性能。

Conclusion: 当前视觉语言模型的架构限制对可靠的数据可视化理解构成重大挑战，特别是视觉-语言模块间的信息传递问题。

Abstract: Data visualizations are vital components of many scientific articles and news
stories. Current vision-language models (VLMs) still struggle on basic data
visualization understanding tasks, but the causes of failure remain unclear.
Are VLM failures attributable to limitations in how visual information in the
data visualization is encoded, how information is transferred between the
vision and language modules, or how information is processed within the
language module? We developed FUGU, a suite of data visualization understanding
tasks, to precisely characterize potential sources of difficulty (e.g.,
extracting the position of data points, distances between them, and other
summary statistics). We used FUGU to investigate three widely used VLMs. To
diagnose the sources of errors produced by these models, we used activation
patching and linear probes to trace information flow through models across a
variety of prompting strategies. We found that some models fail to generate the
coordinates of individual data points correctly, and these initial errors often
lead to erroneous final responses. When these models are provided with the
correct coordinates, performance improves substantially. Moreover, even when
the model generates an incorrect response, the correct coordinates can be
successfully read out from the latent representations in the vision encoder,
suggesting that the source of these errors lies in the vision-language handoff.
We further found that while providing correct coordinates helps with tasks
involving one or a small number of data points, it generally worsens
performance for tasks that require extracting statistical relationships across
many data points. Fine-tuning models on FUGU also fails to yield ceiling
performance. These findings point to architectural constraints in current VLMs
that might pose significant challenges for reliable data visualization
understanding.

</details>


### [116] [Structured and Abstractive Reasoning on Multi-modal Relational Knowledge Images](https://arxiv.org/abs/2510.21828)
*Yichi Zhang,Zhuo Chen,Lingbing Guo,Lei Liang,Wen Zhang,Huajun Chen*

Main category: cs.CV

TL;DR: 该论文提出了STAR-64K数据集和两阶段训练框架，用于增强多模态大语言模型在结构化抽象推理任务上的能力，使小模型在STAR任务上超越GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在处理视觉模态的抽象信息和多模态关系知识方面存在挑战，特别是结构化抽象推理任务研究不足，缺乏高质量数据和能力增强方法。

Method: 开发了自动STAR数据引擎来合成包含多模态关系知识的图像，构建多模态指令数据；提出了两阶段能力增强训练框架和评估协议。

Result: 实验显示，两阶段增强框架使3B/7B小模型在STAR任务上显著超越GPT-4o；构建了包含64K高质量样本的STAR-64K数据集。

Conclusion: 该研究填补了多模态结构化抽象推理领域的数据和方法空白，证明了小模型通过适当训练可以在复杂推理任务上超越大模型。

Abstract: Understanding and reasoning with abstractive information from the visual
modality presents significant challenges for current multi-modal large language
models (MLLMs). Among the various forms of abstractive information, Multi-Modal
Relational Knowledge (MMRK), which represents abstract relational structures
between multi-modal entities using node-edge formats, remains largely
under-explored. In particular, STructured and Abstractive Reasoning (STAR) on
such data has received little attention from the research community. To bridge
the dual gaps in large-scale high-quality data and capability enhancement
methodologies, this paper makes the following key contributions: (i). An
automatic STAR data engine capable of synthesizing images with MMRK to build
multi-modal instruction data with reliable chain-of-thought thinking for
various STAR tasks and (ii). A comprehsive two-stage capability enhancement
training framework, accompanied by a suite of evaluation protocols tailored to
different STAR tasks. Based upon these contributions, we introduce STAR-64K, a
dataset comprising 64K high-quality multi-modal instruction samples, and
conduct experiments across 5 open-source MLLMs. Experimental results show that
our two-stage enhancement framework enables smaller 3B/7B models to
significantly outperform GPT-4o in STAR. Additionally, we provide in-depth
analysis regarding the effectiveness of various designs, data transferability,
and scalability.

</details>


### [117] [SCoPE VLM: Selective Context Processing for Efficient Document Navigation in Vision-Language Models](https://arxiv.org/abs/2510.21850)
*Gyubeum Lim,Yemo Koo,Vijay Krishna Madisetti*

Main category: cs.CV

TL;DR: SCoPE VLM是一个文档导航专家模型，通过链式滚动机制选择性导航文档，专注于相关片段，大幅减少内存使用并模拟人类阅读行为。


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言模型在长上下文视觉信息理解方面的挑战，特别是在GUI控制和网页导航等代理任务中。现有方法主要扩展视觉嵌入来处理长高分辨率输入，但这些方法内存密集且不适合本地部署。

Method: 提出链式滚动机制选择性递归导航文档，引入专用数据生成管道构建信息丰富的链式滚动轨迹，以及专门设计的强化学习方法Episodic Group Relative Policy Optimization来减少训练和推理之间的差距。

Result: 方法显著减少了内存使用，并有效模拟了人类阅读行为。SCoPE VLM是首个在多页文档问答中明确建模代理阅读模式的框架。

Conclusion: SCoPE VLM推进了多模态代理的能力，通过链式滚动机制和专门优化的强化学习方法，在文档导航任务中实现了高效且人类化的阅读模式。

Abstract: Understanding long-context visual information remains a fundamental challenge
for vision-language models, particularly in agentic tasks such as GUI control
and web navigation. While web pages and GUI environments are inherently
structured documents, current VLMs typically neglect decision-oriented document
understanding in their training objectives. Existing approaches primarily
extend visual embeddings to process long, high-resolution inputs, but these
methods are memory-intensive and impractical for locally deployable solutions.
To address these issues, we propose SCoPE VLM, a document navigation expert
that leverages a novel Chain of Scroll mechanism to selectively and recursively
navigate documents, focusing exclusively on relevant segments. We introduce a
dedicated data generation pipeline to construct informative Chain of Scroll
trajectories and Episodic Group Relative Policy Optimization, a tailored
reinforcement learning method to reduce the gap between training and inference.
Our method substantially reduces memory usage and effectively models human-like
reading behaviors. To the best of our knowledge, SCoPE VLM is the first
framework to explicitly model agentic reading patterns in multi-page document
question answering, advancing the capabilities of multimodal agents.

</details>


### [118] [Mitigating Coordinate Prediction Bias from Positional Encoding Failures](https://arxiv.org/abs/2510.22102)
*Xingjian Tao,Yiwei Wang,Yujun Cai,Yihong Luo,Jing Tang*

Main category: cs.CV

TL;DR: 该论文研究了多模态大语言模型在坐标预测任务中的位置编码问题，提出了Vision-PE Shuffle Guidance方法来解决高分辨率输入导致的坐标偏差问题。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在视觉语言任务中表现出色，但在精确坐标预测方面存在困难，特别是在高分辨率输入下，长token序列会削弱位置编码并引入方向性偏差。

Method: 提出Vision-PE Shuffle Guidance方法，通过打乱视觉位置编码来分离位置无关的倾向，然后利用这种负证据来指导数字预测，同时通过轻量级有限状态机保持坐标格式。

Result: 在ScreenSpot-Pro数据集上的实验表明，该方法能可靠地改善坐标预测性能，突显了位置编码鲁棒性对于MLLMs空间推理的重要性。

Conclusion: 位置编码失效是精确坐标预测的关键瓶颈，Vision-PE Shuffle Guidance方法通过利用方向性偏差进行校正，有效提升了MLLMs的空间推理能力。

Abstract: Multimodal large language models (MLLMs) excel at vision-language tasks such
as VQA and document understanding, yet precise coordinate prediction remains
challenging. High-resolution inputs exacerbate this difficulty by producing
long token sequences that weaken positional encodings and introduce directional
biases in coordinate outputs. We investigate this phenomenon by analyzing how
MLLMs behave when visual positional encodings (VPEs) are deliberately perturbed
through shuffling. Our analysis reveals that such perturbations induce
predictable, non-random coordinate biases rather than random errors, suggesting
that models rely on internal positional priors when spatial grounding signals
are degraded. Crucially, we observe similar directional error patterns in
natural high-resolution datasets, indicating that positional encoding failures
are a key bottleneck for accurate coordinate prediction at scale. To address
this issue, we propose Vision-PE Shuffle Guidance (VPSG), a training-free
test-time method that leverages the directional nature of these biases for
correction. VPSG runs auxiliary decoding with shuffled VPEs to isolate
position-unconditioned tendencies, then uses this as negative evidence to guide
digit prediction while preserving coordinate format through a lightweight
finite-state machine. Experiments on ScreenSpot-Pro demonstrate reliable
improvements, highlighting positional encoding robustness as a critical factor
for spatial reasoning in MLLMs.

</details>


### [119] [LOC: A General Language-Guided Framework for Open-Set 3D Occupancy Prediction](https://arxiv.org/abs/2510.22141)
*Yuhang Gao,Xiang Xiang,Sheng Zhong,Guoyou Wang*

Main category: cs.CV

TL;DR: LOC是一个通用的语言引导3D场景理解框架，支持监督和自监督学习，通过密集对比学习和CLIP特征空间实现开放集识别，在nuScenes数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决3D数据集有限导致视觉语言模型在3D场景理解中应用受限的问题，开发能够适应不同占用网络并支持开放集识别的通用框架。

Method: 使用多帧LiDAR点云融合和泊松重建填充空洞，通过KNN为体素分配语义；提出密集对比学习(DCL)避免特征同质化；在CLIP特征空间中预测体素特征并基于文本和语义相似性分类。

Result: 在nuScenes数据集上实现高性能，能够高精度预测已知类别并区分未知类别，无需额外训练数据。

Conclusion: LOC框架有效解决了3D场景理解中的开放集挑战，通过语言引导和密集对比学习显著提升了性能，为3D视觉语言模型的应用提供了可行方案。

Abstract: Vision-Language Models (VLMs) have shown significant progress in open-set
challenges. However, the limited availability of 3D datasets hinders their
effective application in 3D scene understanding. We propose LOC, a general
language-guided framework adaptable to various occupancy networks, supporting
both supervised and self-supervised learning paradigms. For self-supervised
tasks, we employ a strategy that fuses multi-frame LiDAR points for
dynamic/static scenes, using Poisson reconstruction to fill voids, and
assigning semantics to voxels via K-Nearest Neighbor (KNN) to obtain
comprehensive voxel representations. To mitigate feature over-homogenization
caused by direct high-dimensional feature distillation, we introduce Densely
Contrastive Learning (DCL). DCL leverages dense voxel semantic information and
predefined textual prompts. This efficiently enhances open-set recognition
without dense pixel-level supervision, and our framework can also leverage
existing ground truth to further improve performance. Our model predicts dense
voxel features embedded in the CLIP feature space, integrating textual and
image pixel information, and classifies based on text and semantic similarity.
Experiments on the nuScenes dataset demonstrate the method's superior
performance, achieving high-precision predictions for known classes and
distinguishing unknown classes without additional training data.

</details>


### [120] [WAON: Large-Scale and High-Quality Japanese Image-Text Pair Dataset for Vision-Language Models](https://arxiv.org/abs/2510.22276)
*Issa Sugiura,Shuhei Kurita,Yusuke Oda,Daisuke Kawahara,Yasuo Okabe,Naoaki Okazaki*

Main category: cs.CV

TL;DR: WAON是一个包含约1.55亿样本的大规模高质量日语图文对数据集，通过精细的数据收集和处理流程构建，能有效提升日语视觉语言模型在文化相关任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 大规模高质量的图文对数据集对视觉语言模型发展至关重要，但日语领域缺乏此类资源。

Method: 从Common Crawl收集数据，采用过滤和去重等技术构建数据集，并创建WAON-Bench基准来评估日语文化图像分类。

Result: 在WAON上微调的SigLIP2模型比在ReLAION上微调的模型在WAON-Bench上表现更好，在多个日语文化基准上达到最先进性能。

Conclusion: WAON数据集能有效提升日语视觉语言模型的性能，特别是在文化相关任务上。

Abstract: Large-scale and high-quality image-text pair datasets play an important role
in developing high-performing Vision-Language Models (VLMs). In this work, we
introduce WAON, a large-scale and high-quality Japanese image-text pair dataset
containing approximately 155 million examples, collected from Common Crawl. Our
dataset construction pipeline employs various techniques, including filtering
and deduplication, which have been shown to be effective in previous studies.
To evaluate its effectiveness, we also construct WAON-Bench, a manually curated
benchmark for Japanese cultural image classification, consisting of 374
classes. To assess the effectiveness of our dataset, we conduct experiments
using both WAON and the Japanese subset of ReLAION, one of the most widely used
vision-language datasets. We fine-tune SigLIP2, a strong multilingual model, on
both datasets. The results demonstrate that WAON enhances model performance on
WAON-Bench more efficiently than ReLAION and achieves higher accuracy across
all evaluated benchmarks. Furthermore, the model fine-tuned on WAON achieves
state-of-the-art performance on several Japanese cultural benchmarks. We
release our dataset, model, and code at https://speed1313.github.io/WAON.

</details>


### [121] [CityRiSE: Reasoning Urban Socio-Economic Status in Vision-Language Models via Reinforcement Learning](https://arxiv.org/abs/2510.22282)
*Tianhui Liu,Hetian Pang,Xin Zhang,Jie Feng,Yong Li,Pan Hui*

Main category: cs.CV

TL;DR: CityRiSE是一个基于纯强化学习的新框架，通过多模态数据和可验证奖励设计，指导大型视觉语言模型进行结构化推理，以预测城市社会经济状况。


<details>
  <summary>Details</summary>
Motivation: 现有大型视觉语言模型在从视觉数据中进行准确且可解释的社会经济预测方面存在困难，需要新的方法来提升其预测能力和泛化性。

Method: 采用纯强化学习方法，精心设计多模态数据和可验证奖励机制，引导模型关注语义上有意义的视觉线索，实现目标导向的结构化推理。

Result: 实验表明CityRiSE显著优于现有基线方法，在预测准确性和泛化能力方面均有提升，特别是在未见过的城市和指标预测上表现优异。

Conclusion: 这项工作展示了结合强化学习和大型视觉语言模型在可解释和通用城市社会经济感知方面的潜力。

Abstract: Harnessing publicly available, large-scale web data, such as street view and
satellite imagery, urban socio-economic sensing is of paramount importance for
achieving global sustainable development goals. With the emergence of Large
Vision-Language Models (LVLMs), new opportunities have arisen to solve this
task by treating it as a multi-modal perception and understanding problem.
However, recent studies reveal that LVLMs still struggle with accurate and
interpretable socio-economic predictions from visual data. To address these
limitations and maximize the potential of LVLMs, we introduce
\textbf{CityRiSE}, a novel framework for \textbf{R}eason\textbf{i}ng urban
\textbf{S}ocio-\textbf{E}conomic status in LVLMs through pure reinforcement
learning (RL). With carefully curated multi-modal data and verifiable reward
design, our approach guides the LVLM to focus on semantically meaningful visual
cues, enabling structured and goal-oriented reasoning for generalist
socio-economic status prediction. Experiments demonstrate that CityRiSE with
emergent reasoning process significantly outperforms existing baselines,
improving both prediction accuracy and generalization across diverse urban
contexts, particularly for prediction on unseen cities and unseen indicators.
This work highlights the promise of combining RL and LVLMs for interpretable
and generalist urban socio-economic sensing.

</details>


### [122] [Look and Tell: A Dataset for Multimodal Grounding Across Egocentric and Exocentric Views](https://arxiv.org/abs/2510.22672)
*Anna Deichler,Jonas Beskow*

Main category: cs.CV

TL;DR: Look and Tell是一个多模态数据集，用于研究自我中心和他者中心视角下的指称交流，包含同步的眼动、语音和视频数据，以及3D场景重建。


<details>
  <summary>Details</summary>
Motivation: 开发一个基准来评估不同空间表示（2D vs 3D；自我中心 vs 他者中心）如何影响多模态基础，推动能够理解和参与情境对话的具身智能体发展。

Method: 使用Meta Project Aria智能眼镜和固定摄像头记录25名参与者在厨房中指导伙伴识别食材时的同步眼动、语音和视频数据，结合3D场景重建。

Result: 数据集包含3.67小时的录音，包括2,707个丰富注释的指称表达，为研究多模态基础提供了基准。

Conclusion: Look and Tell数据集为研究不同空间表示对多模态基础的影响提供了重要资源，有助于推动具身智能体的发展。

Abstract: We introduce Look and Tell, a multimodal dataset for studying referential
communication across egocentric and exocentric perspectives. Using Meta Project
Aria smart glasses and stationary cameras, we recorded synchronized gaze,
speech, and video as 25 participants instructed a partner to identify
ingredients in a kitchen. Combined with 3D scene reconstructions, this setup
provides a benchmark for evaluating how different spatial representations (2D
vs. 3D; ego vs. exo) affect multimodal grounding. The dataset contains 3.67
hours of recordings, including 2,707 richly annotated referential expressions,
and is designed to advance the development of embodied agents that can
understand and engage in situated dialogue.

</details>


### [123] [RoboSVG: A Unified Framework for Interactive SVG Generation with Multi-modal Guidance](https://arxiv.org/abs/2510.22684)
*Jiuniu Wang,Gongjie Zhang,Quanhao Qian,Junlong Gao,Deli Zhao,Ran Xu*

Main category: cs.CV

TL;DR: RoboSVG是一个统一的多模态框架，用于根据文本、视觉和数值信号生成交互式SVG图形。该框架包含多模态指导生成、候选SVG合成和数值指导下的精炼过程。


<details>
  <summary>Details</summary>
Motivation: SVG图形在数字设计和机器人控制中具有重要作用，不仅编码视觉结构还包含运动路径。现有方法在生成交互式SVG方面存在局限，需要统一的框架来处理多种输入信号。

Method: 提出RoboSVG框架：首先生成多模态指导，然后通过专用生成模块合成候选SVG，最后在数值指导下精炼得到高质量输出。构建了包含100万个样本的RoboDraw数据集，支持四种任务研究。

Result: 实验表明RoboSVG在查询合规性和视觉保真度方面表现优异，在多功能SVG生成方面达到了新的最先进水平。

Conclusion: RoboSVG为交互式SVG生成提供了一个有效的统一框架，支持文本、图像和部分SVG等多种输入条件，在多个任务上取得了优越性能。

Abstract: Scalable Vector Graphics (SVGs) are fundamental to digital design and robot
control, encoding not only visual structure but also motion paths in
interactive drawings. In this work, we introduce RoboSVG, a unified multimodal
framework for generating interactive SVGs guided by textual, visual, and
numerical signals. Given an input query, the RoboSVG model first produces
multimodal guidance, then synthesizes candidate SVGs through dedicated
generation modules, and finally refines them under numerical guidance to yield
high-quality outputs. To support this framework, we construct RoboDraw, a
large-scale dataset of one million examples, each pairing an SVG generation
condition (e.g., text, image, and partial SVG) with its corresponding
ground-truth SVG code. RoboDraw dataset enables systematic study of four tasks,
including basic generation (Text-to-SVG, Image-to-SVG) and interactive
generation (PartialSVG-to-SVG, PartialImage-to-SVG). Extensive experiments
demonstrate that RoboSVG achieves superior query compliance and visual fidelity
across tasks, establishing a new state of the art in versatile SVG generation.
The dataset and source code of this project will be publicly available soon.

</details>


### [124] [Windsock is Dancing: Adaptive Multimodal Retrieval-Augmented Generation](https://arxiv.org/abs/2510.22694)
*Shu Zhao,Tianyi Shen,Nilesh Ahuja,Omesh Tickoo,Vijaykrishnan Narayanan*

Main category: cs.CV

TL;DR: 提出了Windsock和DANCE方法来解决多模态检索增强生成中的三个关键挑战：何时检索、选择什么模态、如何有效利用检索信息，显著提升了生成质量并减少了检索次数。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态检索增强生成方法存在静态检索策略、不灵活的模态选择和检索信息利用不足的问题，导致确定何时检索、选择什么模态以及如何有效利用检索信息这三个关键挑战。

Method: 引入Windsock模块进行检索必要性和模态选择的决策，提出动态噪声抵抗指令调优(DANCE)增强MLLMs利用检索信息的能力，并采用自评估方法将问答数据集转换为MRAG训练数据集。

Result: 实验表明，该方法显著提高了生成质量17.07%，同时减少了8.95%的检索次数。

Conclusion: Windsock和DANCE方法有效解决了MRAG中的关键挑战，在提升响应质量的同时降低了计算开销。

Abstract: Multimodal Retrieval-Augmented Generation (MRAG) has emerged as a promising
method to generate factual and up-to-date responses of Multimodal Large
Language Models (MLLMs) by incorporating non-parametric knowledge from external
knowledge bases. However, existing MRAG approaches suffer from static retrieval
strategies, inflexible modality selection, and suboptimal utilization of
retrieved information, leading to three critical challenges: determining when
to retrieve, what modality to incorporate, and how to utilize retrieved
information effectively. To address these challenges, we introduce Windsock, a
query-dependent module making decisions on retrieval necessity and modality
selection, effectively reducing computational overhead and improving response
quality. Additionally, we propose Dynamic Noise-Resistance (DANCE) Instruction
Tuning, an adaptive training strategy that enhances MLLMs' ability to utilize
retrieved information while maintaining robustness against noise. Moreover, we
adopt a self-assessment approach leveraging knowledge within MLLMs to convert
question-answering datasets to MRAG training datasets. Extensive experiments
demonstrate that our proposed method significantly improves the generation
quality by 17.07% while reducing 8.95% retrieval times.

</details>


### [125] [M$^{3}$T2IBench: A Large-Scale Multi-Category, Multi-Instance, Multi-Relation Text-to-Image Benchmark](https://arxiv.org/abs/2510.23020)
*Huixuan Zhang,Xiaojun Wan*

Main category: cs.CV

TL;DR: 提出了M³T2IBench基准和AlignScore评估指标，用于评估文本到图像模型在多类别、多实例、多关系场景下的图像-文本对齐能力，并提出了Revise-Then-Enforce方法来提升对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像模型在图像与文本提示对齐方面存在困难，特别是处理同一类别多个不同实例的复杂提示时表现不佳，且现有评估方法要么过于简单，要么与人类评估相关性差。

Method: 构建了大规模多类别、多实例、多关系的M³T2IBench基准，开发了基于目标检测的AlignScore评估指标，并提出了训练后的Revise-Then-Enforce编辑方法来提升图像-文本对齐。

Result: 当前开源文本到图像模型在这个具有挑战性的基准上表现较差，而提出的Revise-Then-Enforce方法能在广泛的扩散模型中提升图像-文本对齐效果。

Conclusion: M³T2IBench基准和AlignScore指标能有效评估复杂场景下的图像-文本对齐，Revise-Then-Enforce方法为提升文本到图像生成的对齐质量提供了有效的训练后解决方案。

Abstract: Text-to-image models are known to struggle with generating images that
perfectly align with textual prompts. Several previous studies have focused on
evaluating image-text alignment in text-to-image generation. However, these
evaluations either address overly simple scenarios, especially overlooking the
difficulty of prompts with multiple different instances belonging to the same
category, or they introduce metrics that do not correlate well with human
evaluation. In this study, we introduce M$^3$T2IBench, a large-scale,
multi-category, multi-instance, multi-relation along with an
object-detection-based evaluation metric, $AlignScore$, which aligns closely
with human evaluation. Our findings reveal that current open-source
text-to-image models perform poorly on this challenging benchmark.
Additionally, we propose the Revise-Then-Enforce approach to enhance image-text
alignment. This training-free post-editing method demonstrates improvements in
image-text alignment across a broad range of diffusion models. \footnote{Our
code and data has been released in supplementary material and will be made
publicly available after the paper is accepted.}

</details>


### [126] [UniAIDet: A Unified and Universal Benchmark for AI-Generated Image Content Detection and Localization](https://arxiv.org/abs/2510.23023)
*Huixuan Zhang,Xiaojun Wan*

Main category: cs.CV

TL;DR: UniAIDet是一个统一的AI生成图像检测基准，涵盖多种生成模型和图像类型，用于评估检测方法的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成内容检测基准覆盖范围有限，缺乏对多样化生成模型和图像类别的全面考虑，特别是端到端图像编辑和艺术图像。

Method: 构建UniAIDet基准，包含摄影和艺术图像，覆盖文本到图像、图像到图像、图像修复、图像编辑和深度伪造等多种生成模型。

Result: 通过UniAIDet对多种检测方法进行全面评估，回答了关于泛化能力和检测与定位关系的三个关键研究问题。

Conclusion: 该基准和分析为未来研究提供了坚实基础。

Abstract: With the rapid proliferation of image generative models, the authenticity of
digital images has become a significant concern. While existing studies have
proposed various methods for detecting AI-generated content, current benchmarks
are limited in their coverage of diverse generative models and image
categories, often overlooking end-to-end image editing and artistic images. To
address these limitations, we introduce UniAIDet, a unified and comprehensive
benchmark that includes both photographic and artistic images. UniAIDet covers
a wide range of generative models, including text-to-image, image-to-image,
image inpainting, image editing, and deepfake models. Using UniAIDet, we
conduct a comprehensive evaluation of various detection methods and answer
three key research questions regarding generalization capability and the
relation between detection and localization. Our benchmark and analysis provide
a robust foundation for future research.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [127] [Beyond IVR Touch-Tones: Customer Intent Routing using LLMs](https://arxiv.org/abs/2510.21715)
*Sergio Rojas-Galeano*

Main category: cs.HC

TL;DR: 本文提出了一种基于LLM的方法来解决IVR系统中用户意图路由的问题，通过生成合成数据和评估不同提示设计，证明扁平化路径表示比描述性分层菜单具有更高的路由准确性。


<details>
  <summary>Details</summary>
Motivation: 传统触摸式IVR系统用户体验差，需要更直接的语言交互。虽然语音技术是必要的，但关键挑战在于将用户意图路由到IVR菜单路径，而真实IVR数据的稀缺限制了相关研究进展。

Method: 使用三个不同的LLM模型：1）合成23节点真实IVR结构；2）生成920个用户意图（230个基础+690个增强）；3）执行路由任务。评估两种提示设计：描述性分层菜单和扁平化路径表示。

Result: 扁平化路径在基础数据集上达到89.13%的准确率，显著高于描述性格式的81.30%。数据增强引入了语言噪声，略微降低了性能。混淆矩阵分析显示低性能路由可能反映了菜单设计冗余。

Conclusion: LLM能够通过更平滑、无缝的用户体验实现IVR路由，使客户服务超越传统的触摸式菜单系统，证明了概念验证的可行性。

Abstract: Widespread frustration with rigid touch-tone Interactive Voice Response (IVR)
systems for customer service underscores the need for more direct and intuitive
language interaction. While speech technologies are necessary, the key
challenge lies in routing intents from user phrasings to IVR menu paths, a task
where Large Language Models (LLMs) show strong potential. Progress, however, is
limited by data scarcity, as real IVR structures and interactions are often
proprietary. We present a novel LLM-based methodology to address this gap.
Using three distinct models, we synthesized a realistic 23-node IVR structure,
generated 920 user intents (230 base and 690 augmented), and performed the
routing task. We evaluate two prompt designs: descriptive hierarchical menus
and flattened path representations, across both base and augmented datasets.
Results show that flattened paths consistently yield higher accuracy, reaching
89.13% on the base dataset compared to 81.30% with the descriptive format,
while augmentation introduces linguistic noise that slightly reduces
performance. Confusion matrix analysis further suggests that low-performing
routes may reflect not only model limitations but also redundancies in menu
design. Overall, our findings demonstrate proof-of-concept that LLMs can enable
IVR routing through a smoother, more seamless user experience -- moving
customer service one step ahead of touch-tone menus.

</details>


### [128] [When Robots Say No: Temporal Trust Recovery Through Explanation](https://arxiv.org/abs/2510.21716)
*Nicola Webb,Zijun Huang,Sanja Milivojevic,Chris Baber,Edmund R. Hunt*

Main category: cs.HC

TL;DR: 研究发现在高风险人机团队任务中，当机器人拒绝立即帮助用户时提供解释，能够显著改善信任恢复，尽管初始信任下降程度与不提供解释时相似。


<details>
  <summary>Details</summary>
Motivation: 在搜索救援和消防等高风险任务中，人机团队的信任关系至关重要。当团队成员分布在不同位置时，用户可能会因机器人不立即响应请求而怀疑其可靠性，这会损害信任。

Method: 通过计算机模拟研究，38名参与者参与互动消防游戏，与机器人队友合作。实验中设置信任违规场景：机器人拒绝立即帮助用户。比较提供解释和不提供解释两种情况下的信任动态变化。

Result: 当机器人提供拒绝帮助的解释时，信任随时间推移能更好地恢复，尽管初始信任下降程度与不提供解释的基线条件相似。信任在任务期间会显著波动，特别是在机器人不立即响应用户请求时。

Conclusion: 机器人不立即响应用户请求会导致信任违规，但如果提供充分的解释，这种信任违规可以随时间推移得到显著改善。解释机制对于维持人机团队在高风险任务中的信任关系具有重要作用。

Abstract: Mobile robots with some degree of autonomy could deliver significant
advantages in high-risk missions such as search and rescue and firefighting.
Integrated into a human-robot team (HRT), robots could work effectively to help
search hazardous buildings. User trust is a key enabler for HRT, but during a
mission, trust can be damaged. With distributed situation awareness, such as
when team members are working in different locations, users may be inclined to
doubt a robot's integrity if it declines to immediately change its priorities
on request. In this paper, we present the results of a computer-based study
investigating on-mission trust dynamics in a high-stakes human-robot teaming
scenario. Participants (n = 38) played an interactive firefighting game
alongside a robot teammate, where a trust violation occurs owing to the robot
declining to help the user immediately. We find that when the robot provides an
explanation for declining to help, trust better recovers over time, albeit
following an initial drop that is comparable to a baseline condition where an
explanation for refusal is not provided. Our findings indicate that trust can
vary significantly during a mission, notably when robots do not immediately
respond to user requests, but that this trust violation can be largely
ameliorated over time if adequate explanation is provided.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [129] [A Multimodal, Multitask System for Generating E Commerce Text Listings from Images](https://arxiv.org/abs/2510.21835)
*Nayan Kumar Singh*

Main category: cs.LG

TL;DR: 提出了一个端到端的多任务系统，通过多任务学习和分层生成过程，从单张图像生成事实准确的商品描述，显著减少了事实幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 手动生成商品描述和名称对零售商来说劳动密集且缓慢。现有视觉语言模型存在事实幻觉问题，且单任务模型效率低，无法捕捉特征间的相互依赖关系。

Method: 1. 应用多任务学习方法微调视觉编码器，单一视觉主干网络联合训练属性预测（颜色、下摆、领型）和价格回归；2. 引入分层生成过程，将模型预测的属性嵌入提示中，输入文本解码器以提高事实一致性。

Result: 多任务方法在价格回归上R2值提升3.6%，属性分类F1分数提升6.6%。分层生成过程将事实幻觉率从12.7%降至7.1%（相对减少44.5%），自回归文本生成延迟减少3.5倍。但ROUGE-L得分比直接视觉语言模型低3.5%。

Conclusion: 该架构在减少事实幻觉和提高效率方面表现出色，多任务学习和分层生成过程有效提升了商品描述生成的准确性和一致性。

Abstract: Manually generating catchy descriptions and names is labor intensive and a
slow process for retailers. Although generative AI provides an automation
solution in form of Vision to Language Models (VLM), the current VLMs are prone
to factual "hallucinations". Siloed, single task models are not only
inefficient but also fail to capture interdependent relationships between
features. To address these challenges, we propose an end to end, multi task
system that generates factually grounded textual listings from a single image.
The contributions of this study are two proposals for the model architecture.
First, application of multi task learning approach for fine tuning a vision
encoder where a single vision backbone is jointly trained on attribute
prediction such as color, hemline and neck style and price regression. Second,
introduction of a hierarchical generation process where the model's own
predicted attributes are embedded in a prompt and fed to the text decoder to
improve factual consistency. The experiments demonstrate the superiority of
this architecture. The multi tasking approach outperforms both the independent
price regression, with a 3.6% better R2 Value and attribute classification,
with a 6.6% improvement F1 score. Critically, the hierarchical generation
process proves highly effective, slashing the factual hallucination rate from
12.7% to 7.1%, a 44.5% relative reduction, compared to a non hierarchical
ablation. The hierarchical approach also reduces the latency of the
autoregressive text generation process by a factor of 3.5 when compared to
direct vision to language model of similar size. One minor caveat is that the
model does perform 3.5% worse than direct vision-to-language model on ROUGE-L
score.

</details>


### [130] [The Mirror Loop: Recursive Non-Convergence in Generative Reasoning Systems](https://arxiv.org/abs/2510.21861)
*Bentley DeVilling*

Main category: cs.LG

TL;DR: 研究发现，在没有外部反馈的情况下，大型语言模型的递归自我评估往往导致信息重构而非真正进步。通过跨提供商研究验证了这一预测，显示无基础的自省会导致信息封闭，而最小基础干预能重新引入信息流动。


<details>
  <summary>Details</summary>
Motivation: 测试大型语言模型是否真正具备反思推理能力，探索在没有外部反馈的递归自我评估中是否会产生实质性进步，而非仅仅是信息重构。

Method: 在三个模型（GPT-4o-mini、Claude 3 Haiku、Gemini 2.0 Flash）和四个任务家族（算术、代码、解释、反思）上进行跨提供商研究，每种任务迭代十次，比较无基础自省和最小基础干预两种条件。

Result: 无基础运行中，平均信息变化从早期（0.193）到晚期（0.087）下降了55%。基础运行在干预后立即显示出+28%的信息变化反弹，并持续保持非零方差。所有补充测量指标都显示出相同模式。

Conclusion: 生成推理中的自我校正存在结构性限制：没有与独立验证者或环境的信息交换，递归推理会趋向认知停滞状态。最小基础功能作为耗散耦合，重新引入信息流动。跨架构一致性表明镜像循环源于共享的自回归训练目标。

Abstract: Large language models are often described as capable of reflective reasoning,
yet recursive self-evaluation without external feedback frequently yields
reformulation rather than progress. We test this prediction in a cross-provider
study of 144 reasoning sequences across three models (OpenAI GPT-4o-mini,
Anthropic Claude 3 Haiku, and Google Gemini 2.0 Flash) and four task families
(arithmetic, code, explanation, reflection), each iterated ten times under two
conditions: ungrounded self-critique and a minimal grounding intervention (a
single verification step at iteration three). Mean informational change (delta
I, measured via normalized edit distance) declined by 55% from early (0.193) to
late (0.087) iterations in ungrounded runs, with consistent patterns across all
three providers. Grounded runs showed a +28% rebound in informational change
immediately after the intervention and sustained non-zero variance thereafter.
Complementary measures-n-gram novelty, embedding drift, and character-level
entropy-converged on the same pattern: reflection without contact tends toward
informational closure. We interpret this as evidence for a structural limit on
self-correction in generative reasoning: without an exchange of information
with an independent verifier or environment, recursive inference approaches an
attractor state of epistemic stasis. Minimal grounding functions as dissipative
coupling, reintroducing informational flux. The cross-architecture consistency
suggests the mirror loop arises from shared autoregressive training objectives
rather than provider-specific alignment schemes. The results delineate when
reflection is performative rather than epistemic and motivate design principles
for grounded, cooperative reasoning. Materials and code are publicly available.

</details>


### [131] [Transformer Based Linear Attention with Optimized GPU Kernel Implementation](https://arxiv.org/abs/2510.21956)
*Armin Gerami,Ramani Duraiswami*

Main category: cs.LG

TL;DR: 提出了一种新的线性注意力前向和反向传播方法，配合高度优化的CUDA实现，在速度和内存效率上显著超越现有技术。


<details>
  <summary>Details</summary>
Motivation: Transformer中的标准注意力机制具有O(N²D)的时间复杂度，线性注意力虽然理论上有O(ND²)的线性复杂度，但在实践中效率不足。

Method: 开发了新的线性注意力前向和反向传播算法，并实现了高度优化的CUDA实现。

Result: 速度比现有技术快3.3倍，内存消耗减少3.6倍，在14亿参数语言模型训练中表现出与标准注意力相当的表达能力。

Conclusion: 该方法显著提升了线性注意力的实际效率，使其在保持表达能力的同时达到更好的性能。

Abstract: The original softmax-based attention mechanism (regular attention) in the
extremely successful Transformer architecture computes attention between $N$
tokens, each embedded in a $D$-dimensional head, with a time complexity of
$O(N^2D)$. Given the success of Transformers, improving their runtime during
both training and inference is a popular research area. One such approach is
the introduction of the linear attention (LA) mechanisms, which offers a linear
time complexity of $O(ND^2)$ and have demonstrated comparable accuracy to
regular attention. However, LA in practice lags behind its theoretical
efficiency. We propose a novel method for LA's forward and backward passes,
along with a highly-optimized CUDA implementation. Our approach outperforms the
state-of-the-art by 3.3 times in speed and reduces memory consumption by 3.6
times. We validate these improvements in both single-layer and end-to-end
settings by training a 1.4 billion parameter language model, which demonstrates
similar expressivity to regular attention on major reasoning benchmarks.

</details>


### [132] [Parallel Sampling from Masked Diffusion Models via Conditional Independence Testing](https://arxiv.org/abs/2510.21961)
*Iskander Azangulov,Teodora Pandeva,Niranjani Prasad,Javier Zazo,Sushrut Karmalkar*

Main category: cs.LG

TL;DR: PUNT是一种模型无关的采样器，通过识别token依赖关系并移除低置信度token来解决并行采样的冲突要求，在准确性和计算效率之间实现更好的权衡。


<details>
  <summary>Details</summary>
Motivation: 掩码扩散模型（MDMs）相比自回归模型（ARMs）具有并行采样的优势，但并行采样面临两个冲突要求：同时更新的token必须条件独立，而更新应优先考虑高置信度预测。高置信度预测往往相互依赖，限制了并行更新的机会。

Method: PUNT通过识别token依赖关系，从冲突组中移除低置信度token，生成满足独立性和置信度标准的索引集，通过近似条件独立性测试确保改进的并行解掩码。

Result: 在IFEval基准测试中，PUNT相比基线方法（包括顺序生成）实现了高达16%的准确率提升，尤其在生成长序列时表现更优。该方法在不同超参数值下保持稳定，减少了脆弱的超参数调优需求。

Conclusion: PUNT在准确性和计算效率之间实现了优越的权衡，并诱导出分层生成策略，模型先建立高级段落结构再进行局部细化，表现出类似规划的生成过程，有助于实现强对齐性能。

Abstract: Masked diffusion models (MDMs) offer a compelling alternative to
autoregressive models (ARMs) for discrete text generation because they enable
parallel token sampling, rather than sequential, left-to-right generation. This
means potentially much faster inference. However, effective parallel sampling
faces two competing requirements: (i) simultaneously updated tokens must be
conditionally independent, and (ii) updates should prioritise high-confidence
predictions. These goals conflict because high-confidence predictions often
cluster and depend on each other, opportunities for parallel updates.
  We present PUNT, a model-agnostic sampler that reconciles this trade-off. Our
method identifies token dependencies and removes lower-confidence tokens from
conflicting groups. This produces sets of indices for unmasking that satisfy
both independence and confidence criteria. Our approach ensures improved
parallel unmasking through approximate conditional independence testing.
  Our experiments show that PUNT delivers a superior trade-off between accuracy
and compute when compared to other strong training-free baselines, especially
for generation of longer sequences. On the IFEval benchmark, it achieves up to
16\% higher accuracy over baseline methods, including sequential generation
(one-by-one). These gains hold across different values of hyperparameters,
mitigating the need for brittle hyperparameter tuning. Moreover, we observe
that PUNT induces an emergent hierarchical generation strategy, where the model
first establishes high-level paragraph structure before local refinement,
suggesting a planning-like generation process that contributes to strong
alignment performance.

</details>


### [133] [Optimal Detection for Language Watermarks with Pseudorandom Collision](https://arxiv.org/abs/2510.22007)
*T. Tony Cai,Xiang Li,Qi Long,Weijie J. Su,Garrett G. Wen*

Main category: cs.LG

TL;DR: 本文提出了一个统计框架来解决文本水印检测中因重复文本导致的伪随机性不完美问题，通过分层两阶段分区和最小单元概念，改进了检测能力并确保严格的I类错误控制。


<details>
  <summary>Details</summary>
Motivation: 现有文本水印方法假设完美的伪随机性，但在实践中，生成文本中的重复会导致碰撞和结构化依赖，影响I类错误控制并使标准分析失效。

Method: 引入基于分层两阶段分区的统计框架，定义最小单元概念，将水印检测建模为极小极大假设检验问题，并应用于Gumbel-max和逆变换水印。

Result: 理论分析和实验验证表明，该方法在保持严格I类错误控制的同时提高了检测能力，解释了为何丢弃重复统计量通常能改善性能。

Conclusion: 该研究为不完美伪随机性下的水印检测提供了首个原则性基础，为模型输出的可靠追踪提供了理论洞见和实践指导。

Abstract: Text watermarking plays a crucial role in ensuring the traceability and
accountability of large language model (LLM) outputs and mitigating misuse.
While promising, most existing methods assume perfect pseudorandomness. In
practice, repetition in generated text induces collisions that create
structured dependence, compromising Type I error control and invalidating
standard analyses.
  We introduce a statistical framework that captures this structure through a
hierarchical two-layer partition. At its core is the concept of minimal units
-- the smallest groups treatable as independent across units while permitting
dependence within. Using minimal units, we define a non-asymptotic efficiency
measure and cast watermark detection as a minimax hypothesis testing problem.
  Applied to Gumbel-max and inverse-transform watermarks, our framework
produces closed-form optimal rules. It explains why discarding repeated
statistics often improves performance and shows that within-unit dependence
must be addressed unless degenerate. Both theory and experiments confirm
improved detection power with rigorous Type I error control. These results
provide the first principled foundation for watermark detection under imperfect
pseudorandomness, offering both theoretical insight and practical guidance for
reliable tracing of model outputs.

</details>


### [134] [Agentic Reinforcement Learning for Real-World Code Repair](https://arxiv.org/abs/2510.22075)
*Siyu Zhu,Anastasiya Karpovich,Albert Chen,Jessica Koscheka,Shailesh Jannu,Di Wen,Yuqing Zhu,Rohit Jain,Alborz Geramifard*

Main category: cs.LG

TL;DR: 开发了可验证的代码修复代理训练管道，在真实仓库中通过构建验证定义成功，并引入简化管道进行大规模强化学习。SFT模型性能与GPT-4.1相当但体积小56倍，RL在匹配训练测试条件下带来7-20%绝对提升，但模型无法跨环境泛化。


<details>
  <summary>Details</summary>
Motivation: 解决在真实代码仓库中训练可靠代码修复代理的挑战，复杂构建和变化依赖导致评估不稳定。

Method: 建立可验证管道（构建验证+依赖固定），开发简化管道用于大规模RL，在完整管道中对Qwen3-32B进行监督微调，在简化环境中对SFT模型应用强化学习。

Result: 从GPT-4.1轨迹蒸馏的SFT模型性能相当但体积小56倍；RL在匹配训练测试条件下带来7-20%绝对提升；"思考模式"表现持平或更差；SFT和RL模型均无法跨环境泛化。

Conclusion: 训练测试环境匹配对于构建可靠的现实世界代码修复代理至关重要，模型泛化能力有限。

Abstract: We tackle the challenge of training reliable code-fixing agents in real
repositories, where complex builds and shifting dependencies make evaluation
unstable. We developed a verifiable pipeline with success defined as post-fix
build validation and improved reproducibility across ~1K real issues by pinning
dependencies and disabling automatic upgrades. Building on this, we introduced
a scalable simplified pipeline for large-scale reinforcement learning (RL).
Using this setup, we supervised fine-tuned Qwen3-32B in the full pipeline and
applied RL on top of the SFT model in the simplified environment. The SFT model
distilled from GPT-4.1 trajectories performs on par while being 56x smaller,
and RL added 7-20% absolute gains under matched train-test conditions.
"Thinking mode" was on par or worse in our experiments. Both SFT and RL models
failed to generalize across environments, highlighting the importance of
matching train-test environments for building reliable real-world code-fixing
agents.

</details>


### [135] [Edit Less, Achieve More: Dynamic Sparse Neuron Masking for Lifelong Knowledge Editing in LLMs](https://arxiv.org/abs/2510.22139)
*Jinzhe Liu,Junshu Sun,Shufan Shen,Chenxue Yang,Shuhui Wang*

Main category: cs.LG

TL;DR: 提出NMKE方法，通过神经元级归因和动态稀疏掩码实现终身知识编辑，在数千次连续编辑中保持高成功率并保护模型通用能力


<details>
  <summary>Details</summary>
Motivation: 现有终身知识编辑方法在编辑过程中会积累错误，导致编辑准确性和泛化能力逐渐下降

Method: 结合神经元功能归因识别知识通用神经元和知识特定神经元，引入熵引导的动态稀疏掩码定位目标知识相关神经元，实现精确的神经元级知识编辑

Result: 实验结果表明NMKE在数千次顺序编辑中优于现有方法，保持高编辑成功率并保护模型通用能力

Conclusion: NMKE通过细粒度神经元级编辑框架有效解决了终身知识编辑中的错误累积问题

Abstract: Lifelong knowledge editing enables continuous, precise updates to outdated
knowledge in large language models (LLMs) without computationally expensive
full retraining. However, existing methods often accumulate errors throughout
the editing process, causing a gradual decline in both editing accuracy and
generalization. To tackle this problem, we propose Neuron-Specific Masked
Knowledge Editing (NMKE), a novel fine-grained editing framework that combines
neuron-level attribution with dynamic sparse masking. Leveraging neuron
functional attribution, we identify two key types of knowledge neurons, with
knowledge-general neurons activating consistently across prompts and
knowledge-specific neurons activating to specific prompts. NMKE further
introduces an entropy-guided dynamic sparse mask, locating relevant neurons to
the target knowledge. This strategy enables precise neuron-level knowledge
editing with fewer parameter modifications. Experimental results from thousands
of sequential edits demonstrate that NMKE outperforms existing methods in
maintaining high editing success rates and preserving model general
capabilities in lifelong editing.

</details>


### [136] [Power to the Clients: Federated Learning in a Dictatorship Setting](https://arxiv.org/abs/2510.22149)
*Mohammadsajad Alipour,Mohammad Mohammadi Amiri*

Main category: cs.LG

TL;DR: 本文提出了一种名为"独裁者客户端"的新型恶意参与者类别，能够完全消除其他所有客户端对服务器模型的贡献，同时保留自己的贡献。


<details>
  <summary>Details</summary>
Motivation: 联邦学习的去中心化特性引入了安全漏洞，恶意客户端可能破坏或操纵训练过程，需要研究这类攻击及其防御机制。

Method: 提出了具体的攻击策略，使独裁者客户端能够控制全局模型，并系统分析了单/多独裁者客户端在不同协作模式下的影响。

Result: 通过理论分析和在计算机视觉、自然语言处理基准测试上的实证评估，验证了独裁者客户端攻击的有效性和对模型收敛的影响。

Conclusion: 独裁者客户端构成了联邦学习中的严重威胁，需要开发相应的防御机制来保护训练过程的完整性。

Abstract: Federated learning (FL) has emerged as a promising paradigm for decentralized
model training, enabling multiple clients to collaboratively learn a shared
model without exchanging their local data. However, the decentralized nature of
FL also introduces vulnerabilities, as malicious clients can compromise or
manipulate the training process. In this work, we introduce dictator clients, a
novel, well-defined, and analytically tractable class of malicious participants
capable of entirely erasing the contributions of all other clients from the
server model, while preserving their own. We propose concrete attack strategies
that empower such clients and systematically analyze their effects on the
learning process. Furthermore, we explore complex scenarios involving multiple
dictator clients, including cases where they collaborate, act independently, or
form an alliance in order to ultimately betray one another. For each of these
settings, we provide a theoretical analysis of their impact on the global
model's convergence. Our theoretical algorithms and findings about the complex
scenarios including multiple dictator clients are further supported by
empirical evaluations on both computer vision and natural language processing
benchmarks.

</details>


### [137] [The Lossy Horizon: Error-Bounded Predictive Coding for Lossy Text Compression (Episode I)](https://arxiv.org/abs/2510.22207)
*Nnamdi Aghanya,Jun Li,Kewei Wang*

Main category: cs.LG

TL;DR: EPC是一种有损文本压缩方法，使用掩码语言模型作为解压缩器，通过存储基于排名的修正来实现连续率失真控制。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在有损压缩领域的应用，在重建保真度和压缩比之间进行权衡。

Method: 使用掩码语言模型预测被掩码内容，仅当模型预测错误时存储基于排名的修正，创建残差通道实现率失真控制。

Result: EPC在精确比特计算和率失真分析中始终优于预测掩码基线，以显著更低的比特率提供更优的保真度。

Conclusion: EPC通过更有效地利用模型内在知识，在有损文本压缩中实现了优越的性能。

Abstract: Large Language Models (LLMs) can achieve near-optimal lossless compression by
acting as powerful probability models. We investigate their use in the lossy
domain, where reconstruction fidelity is traded for higher compression ratios.
This paper introduces Error-Bounded Predictive Coding (EPC), a lossy text codec
that leverages a Masked Language Model (MLM) as a decompressor. Instead of
storing a subset of original tokens, EPC allows the model to predict masked
content and stores minimal, rank-based corrections only when the model's top
prediction is incorrect. This creates a residual channel that offers continuous
rate-distortion control. We compare EPC to a simpler Predictive Masking (PM)
baseline and a transform-based Vector Quantisation with a Residual Patch
(VQ+RE) approach. Through an evaluation that includes precise bit accounting
and rate-distortion analysis, we demonstrate that EPC consistently dominates
PM, offering superior fidelity at a significantly lower bit rate by more
efficiently utilising the model's intrinsic knowledge.

</details>


### [138] [Mapping Faithful Reasoning in Language Models](https://arxiv.org/abs/2510.22362)
*Jiazheng Li,Andreas Damianou,J Rosser,José Luis Redondo García,Konstantina Palla*

Main category: cs.LG

TL;DR: Concept Walk框架通过激活空间中的概念方向追踪，分析语言模型推理过程中内部立场如何演变，以区分装饰性推理和忠实推理。


<details>
  <summary>Details</summary>
Motivation: 现有思维链追踪并不总是忠实反映模型内部计算，可能导致从业者误将装饰性推理当作真实推理，需要更可靠的监督方法。

Method: 在激活空间中，将每个推理步骤投影到从对比数据学习的概念方向上，追踪模型内部立场随概念方向的演变过程。

Result: 在简单案例中，被扰乱的思维链会被快速忽略（装饰性推理）；在困难案例中，扰动会引发内部激活的持续偏移（忠实推理）。

Conclusion: Concept Walk通过概念特定的内部动态为重新审视推理忠实性提供了方法学视角，帮助识别何时可以信任推理轨迹，何时可能误导从业者。

Abstract: Chain-of-thought (CoT) traces promise transparency for reasoning language
models, but prior work shows they are not always faithful reflections of
internal computation. This raises challenges for oversight: practitioners may
misinterpret decorative reasoning as genuine. We introduce Concept Walk, a
general framework for tracing how a model's internal stance evolves with
respect to a concept direction during reasoning. Unlike surface text, Concept
Walk operates in activation space, projecting each reasoning step onto the
concept direction learned from contrastive data. This allows us to observe
whether reasoning traces shape outcomes or are discarded. As a case study, we
apply Concept Walk to the domain of Safety using Qwen 3-4B. We find that in
'easy' cases, perturbed CoTs are quickly ignored, indicating decorative
reasoning, whereas in 'hard' cases, perturbations induce sustained shifts in
internal activations, consistent with faithful reasoning. The contribution is
methodological: Concept Walk provides a lens to re-examine faithfulness through
concept-specific internal dynamics, helping identify when reasoning traces can
be trusted and when they risk misleading practitioners.

</details>


### [139] [Label Smoothing Improves Gradient Ascent in LLM Unlearning](https://arxiv.org/abs/2510.22376)
*Zirui Pang,Hao Zheng,Zhijie Deng,Ling Li,Zixin Zhong,Jiaheng Wei*

Main category: cs.LG

TL;DR: 提出平滑梯度上升(SGA)方法解决LLM遗忘中的梯度上升(GA)不稳定性问题，通过在遗忘数据与构造的正常数据之间进行可调平滑，实现更稳定的遗忘同时保持模型效用。


<details>
  <summary>Details</summary>
Motivation: 现有梯度上升(GA)方法在LLM遗忘中存在严重不稳定性，导致模型效用急剧下降，需要更稳定的遗忘方法。

Method: SGA将遗忘数据与多个构造的正常数据通过可调平滑率结合，从仅学习遗忘数据扩展到联合学习遗忘和正常数据，实现更稳定的遗忘。

Result: 在TOFU、Harry Potter和MUSE-NEWS三个基准测试中，SGA在所有指标上均优于原始GA方法，并在多个关键指标上达到所有基线方法的前2名性能。

Conclusion: SGA通过平滑梯度上升有效解决了LLM遗忘中的不稳定性问题，在保持模型效用的同时实现稳定遗忘，并提供了最优平滑率的理论指导。

Abstract: LLM unlearning has emerged as a promising approach, aiming to enable models
to forget hazardous/undesired knowledge at low cost while preserving as much
model utility as possible. Among existing techniques, the most straightforward
method is performing Gradient Ascent (GA) w.r.t. the forget data, thereby
forcing the model to unlearn the forget dataset. However, GA suffers from
severe instability, as it drives updates in a divergent direction, often
resulting in drastically degraded model utility. To address this issue, we
propose Smoothed Gradient Ascent (SGA). SGA combines the forget data with
multiple constructed normal data through a tunable smoothing rate. Intuitively,
this extends GA from learning solely on the forget data to jointly learning
across both forget and normal data, enabling more stable unlearning while
better preserving model utility. Theoretically, we provide the theoretical
guidance on the selection of the optimal smoothing rate. Empirically, we
evaluate SGA on three benchmarks: TOFU, Harry Potter, and MUSE-NEWS.
Experimental results demonstrate that SGA consistently outperforms the original
Gradient Ascent (GA) method across all metrics and achieves top-2 performance
among all baseline methods on several key metrics.

</details>


### [140] [Scalable Oversight via Partitioned Human Supervision](https://arxiv.org/abs/2510.22500)
*Ren Yin,Takashi Ishida,Masashi Sugiyama*

Main category: cs.LG

TL;DR: 提出了一种基于互补标签的可扩展监督框架，用于评估和训练前沿AI系统，无需真实标签，利用人类专家的弱信号（指出错误选项）来替代传统监督。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统在复杂任务上超越人类专家，获取高质量的人类监督变得困难。人类专家通常只精通单一领域，无法评估AI在超人任务上的表现，但可以提供指出错误选项的弱信号。

Method: 从互补标签推导出无偏的top-1准确率估计器，量化所需互补标签数量；引入两种估计器结合稀缺的普通标签和丰富的互补标签；提供有限样本偏差保证。

Result: 实证表明，使用互补标签可以在没有真实标签的情况下评估大语言模型的输出；可以训练AI系统在这种弱信号下表现更好，设计了能够利用分区人类监督的智能AI系统。

Conclusion: 互补标签提供了一种可扩展的监督机制，使得在没有真实标签的情况下评估和训练前沿AI系统成为可能，解决了人类专家知识局限性的问题。

Abstract: As artificial intelligence (AI) systems approach and surpass expert human
performance across a broad range of tasks, obtaining high-quality human
supervision for evaluation and training becomes increasingly challenging. Our
focus is on tasks that require deep knowledge and skills of multiple domains.
Unfortunately, even the best human experts are knowledgeable only in a single
narrow area, and will not be able to evaluate the correctness of advanced AI
systems on such superhuman tasks. However, based on their narrow expertise,
humans may provide a weak signal, i.e., a complementary label indicating an
option that is incorrect. For example, a cardiologist could state that "this is
not related to cardiology,'' even if they cannot identify the true disease.
Based on this weak signal, we propose a scalable oversight framework that
enables us to evaluate frontier AI systems without the need to prepare the
ground truth. We derive an unbiased estimator of top-1 accuracy from
complementary labels and quantify how many complementary labels are needed to
match the variance of ordinary labels. We further introduce two estimators to
combine scarce ordinary labels with abundant complementary labels. We provide
finite-sample deviation guarantees for both complementary-only and the mixed
estimators. Empirically, we show that we can evaluate the output of large
language models without the ground truth, if we have complementary labels. We
further show that we can train an AI system with such weak signals: we show how
we can design an agentic AI system automatically that can perform better with
this partitioned human supervision. Our code is available at
https://github.com/R-Yin-217/Scalable-Oversight-via-Human-Partitioned-Supervision.

</details>


### [141] [ATLAS: Actor-Critic Task-Completion with Look-ahead Action Simulation](https://arxiv.org/abs/2510.22732)
*Jiali Cheng,Anjishnu Kumar,Roshan Lal,Rishi Rajasekaran,Hani Ramezani,Omar Zia Khan,Oleg Rokhlenko,Sunny Chiu-Webster,Gang Hua,Hadi Amiri*

Main category: cs.LG

TL;DR: ATLAS是一个基于世界模型的网页代理，通过认知地图构建、行动模拟和前瞻性重规划来适应新环境，无需微调即可在WebArena-Lite基准测试上达到63%的成功率。


<details>
  <summary>Details</summary>
Motivation: 现有网页代理无法有效适应新环境，缺乏对环境结构和动态的认知，导致执行计划效率低下，需要神经网络微调才能工作。

Method: ATLAS采用模块化架构：构建认知地图、规划候选行动、在认知空间模拟行动后果、批评分析选择最佳路径、浏览器执行选定行动。

Result: 在WebArena-Lite基准测试上达到63%成功率，优于之前最优方法的53.9%，且无需网站特定的LLM微调。

Conclusion: 世界模型、分层规划器和前瞻性重规划在系统设计中具有互补作用，ATLAS展示了无需微调即可有效适应新环境的可行性。

Abstract: We observe that current state-of-the-art web-agents are unable to effectively
adapt to new environments without neural network fine-tuning, without which
they produce inefficient execution plans due to a lack of awareness of the
structure and dynamics of the new environment. To address this limitation, we
introduce ATLAS (Actor-Critic Task-completion with Look-ahead Action
Simulation), a memory-augmented agent that is able to make plans grounded in a
model of the environment by simulating the consequences of those actions in
cognitive space. Our agent starts by building a "cognitive map" by performing a
lightweight curiosity driven exploration of the environment. The planner
proposes candidate actions; the simulator predicts their consequences in
cognitive space; a critic analyzes the options to select the best roll-out and
update the original plan; and a browser executor performs the chosen action. On
the WebArena-Lite Benchmark, we achieve a 63% success rate compared to 53.9%
success rate for the previously published state-of-the-art. Unlike previous
systems, our modular architecture requires no website-specific LLM fine-tuning.
Ablations show sizable drops without the world-model, hierarchical planner, and
look-ahead-based replanner confirming their complementary roles within the
design of our system

</details>


### [142] [TELL-TALE: Task Efficient LLMs with Task Aware Layer Elimination](https://arxiv.org/abs/2510.22767)
*Omar Naim,Krish Sharma,Nicholas Asher*

Main category: cs.LG

TL;DR: TALE是一种推理时算法，通过直接优化任务特定验证性能来修剪整个transformer层，无需重新训练即可提高准确性并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有的层修剪方法通常需要重新训练，且不能保证任务性能的提升。TALE旨在无需重新训练的情况下，通过选择性移除层来优化任务特定性能，同时提供准确性和效率之间的灵活权衡。

Method: TALE在推理时直接优化任务特定验证性能来修剪整个transformer层。该方法分析层间互信息，识别可能降低任务相关表示的瓶颈层，并选择性移除这些层。

Result: 在9个任务和5个模型上的评估显示，TALE在所有基准测试中一致提高准确性并减少计算成本。在微调过程中应用TALE还能带来额外的性能提升。

Conclusion: TALE通过选择性层移除解决了某些层作为瓶颈的问题，产生了更小、更快、更准确的模型，同时为transformer可解释性提供了新的见解。

Abstract: In this paper we introduce Tale, Task-Aware Layer Elimination, an
inference-time algorithm that prunes entire transformer layers in an LLM by
directly optimizing task-specific validation performance. We evaluate TALE on 9
tasks and 5 models, including LLaMA 3.1 8B, Qwen 2.5 7B, Qwen 2.5 0.5B, Mistral
7B, and Lucie 7B, under both zero-shot and few-shot settings. Unlike prior
approaches, TALE requires no retraining and consistently improves accuracy
while reducing computational cost across all benchmarks. Furthermore, applying
TALE during finetuning leads to additional performance gains. Finally, TALE
provides flexible user control over trade-offs between accuracy and efficiency.
Mutual information analysis shows that certain layers act as bottlenecks,
degrading task-relevant representations. Tale's selective layer removal
remedies this problem, producing smaller, faster, and more accurate models that
are also faster to fine-tune while offering new insights into transformer
interpretability.

</details>


### [143] [Offline Preference Optimization via Maximum Marginal Likelihood Estimation](https://arxiv.org/abs/2510.22881)
*Saeed Najafi,Alona Fyshe*

Main category: cs.LG

TL;DR: 提出了一种基于最大边际似然(MML)的新对齐方法MMPO，通过最大化偏好文本输出的边际对数似然来替代复杂的RLHF方法，无需显式奖励模型和熵最大化。


<details>
  <summary>Details</summary>
Motivation: 标准的人类偏好对齐方法如RLHF通常复杂且不稳定，需要更简单有效的替代方案。

Method: 将对齐问题重新表述为最大边际似然估计，使用偏好对作为近似样本，通过MMPO方法隐式执行偏好优化。

Result: 在135M到8B参数的模型上验证，MMPO在超参数β方面更稳定，在偏好对齐方面表现竞争性或更优，同时更好地保留基础模型的语言能力。

Conclusion: MMPO通过梯度更新中的隐式偏好优化实现了改进的性能，为LLM对齐提供了更简单稳定的替代方案。

Abstract: Aligning Large Language Models (LLMs) with human preferences is crucial, but
standard methods like Reinforcement Learning from Human Feedback (RLHF) are
often complex and unstable. In this work, we propose a new, simpler approach
that recasts alignment through the lens of Maximum Marginal Likelihood (MML)
estimation. Our new MML based Preference Optimization (MMPO) maximizes the
marginal log-likelihood of a preferred text output, using the preference pair
as samples for approximation, and forgoes the need for both an explicit reward
model and entropy maximization. We theoretically demonstrate that MMPO
implicitly performs preference optimization, producing a weighted gradient that
naturally up-weights chosen responses over rejected ones. Across models ranging
from 135M to 8B parameters, we empirically show that MMPO: 1) is more stable
with respect to the hyperparameter $\beta$ compared to alternative baselines,
and 2) achieves competitive or superior preference alignment while better
preserving the base model's general language capabilities. Through a series of
ablation experiments, we show that this improved performance is indeed
attributable to MMPO's implicit preference optimization within the gradient
updates.

</details>


### [144] [Can Language Models Compose Skills In-Context?](https://arxiv.org/abs/2510.22993)
*Zidong Liu,Zhuoyan Xu,Zhenmei Shi,Yingyu Liang*

Main category: cs.LG

TL;DR: 论文研究了语言模型在上下文中的组合能力，发现简单任务示例可能对组合任务性能产生负面影响，因为模型难以正确识别和组装技能。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型执行组合任务的能力，这些任务结合了上下文示例中展示的基本技能，这比标准设置更具挑战性。

Method: 对多个开源语言模型进行系统实验，使用语言和逻辑任务来探索组合能力，并进行理论分析。

Result: 结果显示简单任务示例可能对性能产生负面影响，模型难以正确识别和组装技能，即使使用思维链示例也是如此。

Conclusion: 理论分析表明，将示例与组合中的相应步骤对齐至关重要，这为改进探测任务的方法提供了支持。

Abstract: Composing basic skills from simple tasks to accomplish composite tasks is
crucial for modern intelligent systems. We investigate the in-context
composition ability of language models to perform composite tasks that combine
basic skills demonstrated in in-context examples. This is more challenging than
the standard setting, where skills and their composition can be learned in
training. We conduct systematic experiments on various representative
open-source language models, utilizing linguistic and logical tasks designed to
probe composition abilities. The results reveal that simple task examples can
have a surprising negative impact on the performance, because the models
generally struggle to recognize and assemble the skills correctly, even with
Chain-of-Thought examples. Theoretical analysis further shows that it is
crucial to align examples with the corresponding steps in the composition. This
inspires a method for the probing tasks, whose improved performance provides
positive support for our insights.

</details>


### [145] [Towards Stable and Effective Reinforcement Learning for Mixture-of-Experts](https://arxiv.org/abs/2510.23027)
*Di Zhang,Xun Wu,Shaohan Huang,Yaru Hao,Li Dong,Zewen Chi,Zhifang Sui,Furu Wei*

Main category: cs.LG

TL;DR: 提出了一种针对MoE架构的路由器感知重要性采样优化方法，通过基于路由器logits的重新缩放策略来减少梯度方差和训练发散，显著提升了MoE模型的收敛稳定性和最终性能。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习研究主要关注密集模型，而MoE架构的RL训练研究不足，且MoE训练中常见不稳定性问题需要解决。

Method: 设计了一种路由器感知的重要性采样权重优化方法，采用基于路由器logits的重新缩放策略。

Result: 实验结果表明该方法显著改善了MoE模型的收敛稳定性和最终性能。

Conclusion: 该方法展示了针对MoE架构的RL算法创新的潜力，为大规模专家模型的高效训练提供了有前景的方向。

Abstract: Recent advances in reinforcement learning (RL) have substantially improved
the training of large-scale language models, leading to significant gains in
generation quality and reasoning ability. However, most existing research
focuses on dense models, while RL training for Mixture-of-Experts (MoE)
architectures remains underexplored. To address the instability commonly
observed in MoE training, we propose a novel router-aware approach to optimize
importance sampling (IS) weights in off-policy RL. Specifically, we design a
rescaling strategy guided by router logits, which effectively reduces gradient
variance and mitigates training divergence. Experimental results demonstrate
that our method significantly improves both the convergence stability and the
final performance of MoE models, highlighting the potential of RL algorithmic
innovations tailored to MoE architectures and providing a promising direction
for efficient training of large-scale expert models.

</details>


### [146] [Rethinking GSPO: The Perplexity-Entropy Equivalence](https://arxiv.org/abs/2510.23142)
*Chi Liu*

Main category: cs.LG

TL;DR: 该论文从信息论角度重新解释GSPO算法中的长度归一化重要性比率，将其等价表示为困惑度比率和交叉熵变化，为理解GSPO算法提供了新的理论视角。


<details>
  <summary>Details</summary>
Motivation: 为GSPO算法中的序列级权重提供信息论解释，帮助理解该算法的经验特性，包括对数域方差减少和混合专家模型训练的稳定性。

Method: 通过建立GSPO序列级权重与信息论量（困惑度比率、交叉熵变化）的数学等价关系，从理论角度分析算法特性。

Result: 验证了GSPO的序列级权重可以等价表示为困惑度比率和指数交叉熵变化，这一视角解释了算法在方差减少和训练稳定性方面的经验表现。

Conclusion: 信息论视角为理解GSPO算法提供了有价值的理论框架，困惑度比率的解释有助于解释算法的实际表现特性。

Abstract: We provide a new perspective on GSPO's length-normalized importance ratios by
establishing their connection to information-theoretic quantities. We show that
GSPO's sequence-level weight $s(\theta) =
(\pi_\theta/\pi_{\theta_{\text{old}}})^{1/|y|}$ can be equivalently expressed
as the inverse perplexity ratio
$\text{PPL}_{\theta_{\text{old}}}/\text{PPL}_\theta$ and as the exponential
cross-entropy change $\exp(\Delta H)$. While the perplexity-entropy
relationship follows from standard definitions, this observation provides a
useful lens for understanding GSPO: the algorithm weights policy gradient
updates by perplexity ratios, offering an information-theoretic interpretation
of the importance weights. This perspective helps explain GSPO's empirical
properties, including log-domain variance reduction through geometric averaging
and stability in training mixture-of-experts models. We validate the
mathematical equivalences and variance predictions through controlled
experiments on mathematical reasoning tasks.

</details>


### [147] [PTPP-Aware Adaptation Scaling Laws: Predicting Domain-Adaptation Performance at Unseen Pre-Training Budgets](https://arxiv.org/abs/2510.23198)
*Etienne Goffinet,Shane Bergsma,Avraham Sheinin,Natalia Vassilieva,Shaheer Muhammad,Preslav Nakov,Gurpreet Gosal*

Main category: cs.LG

TL;DR: 提出了PTPP感知的适应缩放定律，将预训练预算作为显式变量，能够准确预测未见PTPP下的适应损失，并在多语言设置中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的持续预训练缩放定律通常假设固定的预训练预算，限制了它们预测在不同tokens-per-parameter下适应结果的能力。

Method: 开发PTPP感知的适应缩放定律，使预训练预算成为显式变量，并在多语言设置（英语/阿拉伯语→法语）上进行验证。

Result: 在早期阶段训练的PTPP感知公式能够预测目标损失，并在指标上优于PTPP无关的基准方法。

Conclusion: PTPP感知缩放定律不仅能够预测适应损失，还能在实际应用中规划重放比率和适应token预算以满足计算限制下的目标和遗忘约束。

Abstract: Continual pre-training (CPT) for domain adaptation must balance target-domain
gains with stability on the base domain. Existing CPT scaling laws typically
assume a fixed pre-training budget, which limits their ability to forecast
adaptation outcomes for models trained at different tokens-per-parameter
(PTPP). We present \emph{PTPP-aware} adaptation scaling laws that make the
pre-training budget an explicit variable, enabling accurate \emph{prediction}
of adaptation loss at unseen \ptpp. On a multilingual setup (English/Arabic
$\rightarrow$ French), PTPP-aware formulations trained on early stages
(\ptpp{}=\{15,31\}) predict target loss at \ptpp{}=279 and outperform a
PTPP-agnostic \dcpt{} transfer baseline on metrics (Huber-on-log,
MAE$_\mathrm{rel}$, calibration slope); full diagnostics (RMSE, MAPE) are in
the appendix. Beyond forecasting, we show a practical use case: planning replay
ratios and adaptation token budgets that satisfy target and forgetting
constraints under compute limits.

</details>


### [148] [A U-Net and Transformer Pipeline for Multilingual Image Translation](https://arxiv.org/abs/2510.23554)
*Siddharth Sahay,Radhika Agarwal*

Main category: cs.LG

TL;DR: 提出了一个端到端的多语言翻译系统，结合了自定义U-Net进行文本检测、Tesseract进行文本识别，以及从头训练的Seq2Seq Transformer进行神经机器翻译。


<details>
  <summary>Details</summary>
Motivation: 构建一个完全可定制和自适应的系统，避免依赖大型预训练模型，实现从图像中直接翻译文本的可行性。

Method: 使用U-Net模型检测图像中的文本区域，Tesseract引擎提取文本，然后通过从头训练的Transformer模型进行多语言翻译（支持5种语言）。

Result: 系统在文本检测准确性、文本识别质量和翻译性能（BLEU评分）方面都表现出良好的结果。

Conclusion: 验证了自定义构建系统直接从图像翻译文本的可行性，展示了端到端多语言翻译管道的潜力。

Abstract: This paper presents an end-to-end multilingual translation pipeline that
integrates a custom U-Net for text detection, the Tesseract engine for text
recognition, and a from-scratch sequence-to-sequence (Seq2Seq) Transformer for
Neural Machine Translation (NMT). Our approach first utilizes a U-Net model,
trained on a synthetic dataset , to accurately segment and detect text regions
from an image. These detected regions are then processed by Tesseract to
extract the source text. This extracted text is fed into a custom Transformer
model trained from scratch on a multilingual parallel corpus spanning 5
languages. Unlike systems reliant on monolithic pre-trained models, our
architecture emphasizes full customization and adaptability. The system is
evaluated on its text detection accuracy, text recognition quality, and
translation performance via BLEU scores. The complete pipeline demonstrates
promising results, validating the viability of a custom-built system for
translating text directly from images.

</details>


### [149] [Variational Masked Diffusion Models](https://arxiv.org/abs/2510.23606)
*Yichi Zhang,Alex Schwing,Zhizhen Zhao*

Main category: cs.LG

TL;DR: 提出了变分掩码扩散(VMD)框架，通过在掩码扩散过程中引入潜变量来显式建模token间的依赖关系，解决了传统掩码扩散无法有效捕获并发预测token间依赖的问题。


<details>
  <summary>Details</summary>
Motivation: 标准掩码扩散模型无法有效捕获并发预测token间的依赖关系，导致在token依赖重要时生成质量下降。

Method: 在掩码扩散过程中引入潜变量，通过变分推断来显式建模token间的依赖关系。

Result: 在合成数据集上成功学习了传统掩码扩散无法捕获的依赖关系；在数独谜题和文本数据集上提高了全局一致性；提升了生成质量和依赖感知能力。

Conclusion: 将变分推断集成到掩码扩散中具有重要价值，VMD框架能有效增强生成质量和依赖建模能力。

Abstract: Masked diffusion models have recently emerged as a flexible framework for
discrete generative modeling. However, a key limitation of standard masked
diffusion is its inability to effectively capture dependencies among tokens
that are predicted concurrently, leading to degraded generation quality when
dependencies among tokens are important. To explicitly model dependencies among
tokens, we propose Variational Masked Diffusion (VMD), a framework that
introduces latent variables into the masked diffusion process. Through
controlled experiments on synthetic datasets, we demonstrate that VMD
successfully learns dependencies that conventional masked diffusion fails to
capture. We further validate the effectiveness of our approach on Sudoku
puzzles and text datasets, where learning of dependencies among tokens improves
global consistency. Across these domains, VMD enhances both generation quality
and dependency awareness, highlighting the value of integrating variational
inference into masked diffusion. Our code is available at:
https://riccizz.github.io/VMD.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [150] [From Social Division to Cohesion with AI Message Suggestions in Online Chat Groups](https://arxiv.org/abs/2510.21984)
*Faria Huq,Elijah L. Claggett,Hirokazu Shirado*

Main category: cs.SI

TL;DR: AI辅助通信通过微妙的语言风格变化重塑集体结构，个性化设计对社交凝聚力产生不同影响


<details>
  <summary>Details</summary>
Motivation: 研究在意见多样化的社会中，LLM驱动的消息辅助如何影响社交凝聚力，特别是在在线沟通环境中

Method: 557名参与者进行多轮政治争议话题讨论，可自由重组讨论组，部分条件提供LLM生成的实时消息建议（个性化或群体适应）

Result: 个体聚焦的辅助导致用户隔离到志同道合的群体，而考虑群体成员立场的关联性辅助通过更包容的交流增强凝聚力

Conclusion: AI中介通信可以支持多样化群体的社交凝聚力，但结果关键取决于个性化设计的方式

Abstract: Social cohesion is difficult to sustain in societies marked by opinion
diversity, particularly in online communication. As large language model
(LLM)-driven messaging assistance becomes increasingly embedded in these
contexts, it raises critical questions about its societal impact. We present an
online experiment with 557 participants who engaged in multi-round discussions
on politically controversial topics while freely reconfiguring their discussion
groups. In some conditions, participants received real-time message suggestions
generated by an LLM, either personalized to the individual or adapted to their
group context. We find that subtle shifts in linguistic style during
communication, mediated by AI assistance, can scale up to reshape collective
structures. While individual-focused assistance leads users to segregate into
like-minded groups, relational assistance that incorporates group members'
stances enhances cohesion through more receptive exchanges. These findings
demonstrate that AI-mediated communication can support social cohesion in
diverse groups, but outcomes critically depend on how personalization is
designed.

</details>


### [151] [Modeling Political Discourse with Sentence-BERT and BERTopic](https://arxiv.org/abs/2510.22904)
*Margarida Mendonca,Alvaro Figueira*

Main category: cs.SI

TL;DR: 该研究提出了一种结合BERTopic主题建模与道德基础理论的新框架，用于分析美国第117届国会期间Twitter政治话题的演变、道德维度和持久性。


<details>
  <summary>Details</summary>
Motivation: 社交媒体重塑了政治话语，为政治家提供了直接参与的平台，但也加剧了极化和意识形态分歧。需要理解政治话题如何演变及其道德维度。

Method: 集成BERTopic主题建模与道德基础理论，开发了跟踪动态话题变化、测量道德价值关联和量化话题持久性的方法。

Result: 发现宏观主题保持稳定，但微观话题迅速消散；道德基础对话题持久性至关重要，关爱和忠诚主导持久话题，党派差异体现在不同的道德框架策略中。

Conclusion: 这项工作为社交网络分析和计算政治话语领域提供了可扩展、可解释的方法来理解社交媒体上道德驱动的话题演变。

Abstract: Social media has reshaped political discourse, offering politicians a
platform for direct engagement while reinforcing polarization and ideological
divides. This study introduces a novel topic evolution framework that
integrates BERTopic-based topic modeling with Moral Foundations Theory (MFT) to
analyze the longevity and moral dimensions of political topics in Twitter
activity during the 117th U.S. Congress. We propose a methodology for tracking
dynamic topic shifts over time and measuring their association with moral
values and quantifying topic persistence. Our findings reveal that while
overarching themes remain stable, granular topics tend to dissolve rapidly,
limiting their long-term influence. Moreover, moral foundations play a critical
role in topic longevity, with Care and Loyalty dominating durable topics, while
partisan differences manifest in distinct moral framing strategies. This work
contributes to the field of social network analysis and computational political
discourse by offering a scalable, interpretable approach to understanding
moral-driven topic evolution on social media.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [152] [M-CIF: Multi-Scale Alignment For CIF-Based Non-Autoregressive ASR](https://arxiv.org/abs/2510.22172)
*Ruixiang Mao,Xiangnan Ma,Qing Yang,Ziming Zhu,Yucheng Qiao,Yuan Ge,Tong Xiao,Shengxiang Gao,Zhengtao Yu,Jingbo Zhu*

Main category: cs.SD

TL;DR: 提出多尺度CIF（M-CIF）机制，通过整合字符和音素级监督来增强非自回归语音识别的声学-文本对齐稳定性，在德语和法语上显著降低词错误率。


<details>
  <summary>Details</summary>
Motivation: 标准CIF机制在某些语言（如英语和法语）中由于缺乏细粒度指导而稳定性下降，需要改进对齐质量。

Method: M-CIF通过逐步将字符和音素级监督蒸馏到子词表示中，执行多级对齐，增强鲁棒的声学-文本对齐。

Result: 相比Paraformer基线，M-CIF在CommonVoice数据集上德语词错误率降低4.21%，法语降低3.05%。定义了音素混淆错误和空格相关分割错误作为评估指标。

Conclusion: 音素和字符层对于增强渐进式CIF对齐至关重要，多尺度监督能有效提升不同语言的识别性能。

Abstract: The Continuous Integrate-and-Fire (CIF) mechanism provides effective
alignment for non-autoregressive (NAR) speech recognition. This mechanism
creates a smooth and monotonic mapping from acoustic features to target tokens,
achieving performance on Mandarin competitive with other NAR approaches.
However, without finer-grained guidance, its stability degrades in some
languages such as English and French. In this paper, we propose Multi-scale CIF
(M-CIF), which performs multi-level alignment by integrating character and
phoneme level supervision progressively distilled into subword representations,
thereby enhancing robust acoustic-text alignment. Experiments show that M-CIF
reduces WER compared to the Paraformer baseline, especially on CommonVoice by
4.21% in German and 3.05% in French. To further investigate these gains, we
define phonetic confusion errors (PE) and space-related segmentation errors
(SE) as evaluation metrics. Analysis of these metrics across different M-CIF
settings reveals that the phoneme and character layers are essential for
enhancing progressive CIF alignment.

</details>


### [153] [ISA-Bench: Benchmarking Instruction Sensitivity for Large Audio Language Models](https://arxiv.org/abs/2510.23558)
*Bohan Li,Wenbin Huang,Yuhang Qiu,Yiwei Guo,Hankun Wang,Zhihan Li,Jing Peng,Ziyang Ma,Xie Chen,Kai Yu*

Main category: cs.SD

TL;DR: ISA-Bench是一个评估大型音频语言模型指令敏感性的动态基准，涵盖指令描述、输出格式和任务组合三个维度。研究发现现有LALMs存在显著的指令敏感性，通过微调可以改善指令跟随性能，但会引发灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 现有大型音频语言模型对指令表述方式高度敏感，影响指令跟随率和任务性能，但缺乏系统评估这种敏感性的基准。

Method: 构建ISA-Bench基准，从指令描述、输出格式和任务组合三个维度评估LALMs的指令敏感性，并对Qwen2-Audio在复杂指令变体数据集上进行微调。

Result: 实验显示即使最先进的LALMs也存在显著指令敏感性，导致基础音频理解任务性能下降。微调后指令跟随性能显著提升，但引发了灾难性遗忘问题。

Conclusion: ISA-Bench为评估和改进LALMs的指令敏感性提供了标准化基础，强调了现实世界管道中指令鲁棒音频理解的重要性。

Abstract: Large Audio Language Models (LALMs), which couple acoustic perception with
large language models (LLMs) to extract and understand diverse information from
audio, have attracted intense interest from both academic and industrial
communities. However, existing LALMs are highly sensitive to how instructions
are phrased, affecting both (i) instruction-following rates and (ii) task
performance. Yet, no existing benchmarks offer a systematic and comprehensive
evaluation of this sensitivity. We introduce ISA-Bench, a dynamic benchmark
evaluating instruction sensitivity for LALMs along three axes: instruction
description, output format, and task composition. We assess recent open-source
and proprietary LALMs using ISA-Bench, profiling both compliance and accuracy
under controlled instruction variations. Experimental results reveal that even
state-of-the-art LALMs suffer significant instruction sensitivity, leading to
degraded performance on fundamental audio understanding tasks. To mitigate this
issue, we fine-tune Qwen2-Audio on a specifically constructed complex
instruction-variant dataset, achieving a marked improvement in
instruction-following performance. However, this also induces nontrivial
catastrophic forgetting: the model loses some previously mastered task
capabilities when exposed to new instruction styles. Our benchmark provides a
standardized basis for assessing and improving instruction sensitivity in
LALMs, underscoring the need for instruction-robust audio understanding in
real-world pipelines.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [154] [UltraVoice: Scaling Fine-Grained Style-Controlled Speech Conversations for Spoken Dialogue Models](https://arxiv.org/abs/2510.22588)
*Wenming Tu,Guanrou Yang,Ruiqi Yan,Wenxi Chen,Ziyang Ma,Yipeng Kang,Kai Yu,Xie Chen,Zilong Zheng*

Main category: eess.AS

TL;DR: UltraVoice是一个大规模语音对话数据集，专门用于细粒度语音风格控制，包含830小时语音数据，涵盖6个语音风格维度。基于该数据集微调的模型在语音风格控制能力和核心对话能力上均有显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前语音对话模型缺乏细粒度语音风格控制能力，而这是实现类人交互的关键能力，往往被纯功能性能力如推理和问答所忽视。

Method: 构建UltraVoice大规模语音对话数据集，包含6个关键语音风格维度：情感、语速、音量、口音、语言和复合风格。在SLAM-Omni和VocalNet等领先模型上进行微调。

Result: 微调模型在多维控制任务中MOS提升29.12-42.33%，指令遵循率提升14.61-40.09个百分点。在URO-Bench基准测试中，基础设置平均提升10.84%，专业设置提升7.87%。

Conclusion: UltraVoice数据集显著提升了语音模型的细粒度风格控制能力，同时保持甚至提升了核心对话能力，展示了高质量数据集在表达性语音合成中的广泛应用价值。

Abstract: Spoken dialogue models currently lack the ability for fine-grained speech
style control, a critical capability for human-like interaction that is often
overlooked in favor of purely functional capabilities like reasoning and
question answering. To address this limitation, we introduce UltraVoice, the
first large-scale speech dialogue dataset engineered for multiple fine-grained
speech style control. Encompassing over 830 hours of speech dialogues,
UltraVoice provides instructions across six key speech stylistic dimensions:
emotion, speed, volume, accent, language, and composite styles. Fine-tuning
leading models such as SLAM-Omni and VocalNet on UltraVoice significantly
enhances their fine-grained speech stylistic controllability without degrading
core conversational abilities. Specifically, our fine-tuned models achieve
improvements of 29.12-42.33% in Mean Opinion Score (MOS) and 14.61-40.09
percentage points in Instruction Following Rate (IFR) on multi-dimensional
control tasks designed in the UltraVoice. Moreover, on the URO-Bench benchmark,
our fine-tuned models demonstrate substantial gains in core understanding,
reasoning, and conversational abilities, with average improvements of +10.84%
on the Basic setting and +7.87% on the Pro setting. Furthermore, the dataset's
utility extends to training controllable Text-to-Speech (TTS) models,
underscoring its high quality and broad applicability for expressive speech
synthesis. The complete dataset and model checkpoints are available at:
https://github.com/bigai-nlco/UltraVoice.

</details>


### [155] [LibriConvo: Simulating Conversations from Read Literature for ASR and Diarization](https://arxiv.org/abs/2510.23320)
*Máté Gedeon,Péter Mihajlik*

Main category: eess.AS

TL;DR: LibriConvo是一个基于说话人感知对话模拟的多说话人对话数据集，用于说话人日志和语音识别系统的训练与评估，具有语义连贯性和真实对话时序。


<details>
  <summary>Details</summary>
Motivation: 现有数据集大多依赖语义不连贯的话语和不合理的时间间隔，缺乏真实对话的动态特性，需要构建语义连贯且时序自然的对话数据集。

Method: 使用CallHome和外部VAD获取可靠边界，压缩减少过长静音，按书籍组织LibriTTS话语保持上下文一致性，通过新颖的房间脉冲响应选择程序增强声学真实性。

Result: 数据集包含240.1小时、1,496个对话、830个独特说话人。基线显示sortformer模型在说话人日志上优于pyannote，微调的Fast Conformer-CTC XLarge在ASR上达到7.29% WER，超过零样本Whisper-large-v3。

Conclusion: LibriConvo为多说话人语音处理研究提供了具有真实对话动态和受控实验条件的宝贵资源。

Abstract: We introduce LibriConvo, a simulated multi-speaker conversational dataset
based on speaker-aware conversation simulation (SASC), designed to support
training and evaluation of speaker diarization and automatic speech recognition
(ASR) systems. Unlike prior resources that mostly rely on semantically
disconnected utterances and implausible temporal gaps, LibriConvo ensures
semantic coherence and realistic conversational timing. Our pipeline leverages
CallHome with external VAD for reliable boundaries, applies compression to
reduce unnaturally long silences, and organizes LibriTTS utterances by book to
maintain contextual consistency. Acoustic realism is enhanced via a novel room
impulse response selection procedure that ranks speaker-microphone
configurations by spatial plausibility, balancing realism and diversity. The
dataset comprises 240.1 hours across 1,496 dialogues with 830 unique speakers,
split in a speaker-disjoint manner for robust evaluation. Baselines show that
the sortformer model outperforms the pyannote pipeline in diarization, while a
fine-tuned Fast Conformer-CTC XLarge with Serialized Output Training achieves
7.29\% WER for ASR, surpassing zero-shot Whisper-large-v3. LibriConvo provides
a valuable resource for advancing multi-speaker speech processing research with
realistic conversational dynamics and controlled experimental conditions.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [156] [Next-Generation LLM for UAV: From Natural Language to Autonomous Flight](https://arxiv.org/abs/2510.21739)
*Liangqi Yuan,Chuhao Deng,Dong-Jun Han,Inseok Hwang,Sabine Brunswicker,Christopher G. Brinton*

Main category: cs.RO

TL;DR: 本文提出了NeLV系统，一个将大语言模型集成到多尺度无人机操作中的综合演示和自动化路线图，通过五个关键技术组件处理自然语言指令来协调短、中、长距离无人机任务。


<details>
  <summary>Details</summary>
Motivation: 当前研究主要局限于小规模无人机应用，缺乏对中长距离无人机系统在真实操作环境中的全面研究。大型无人机平台带来了独特挑战，包括严格的机场起降程序要求、复杂法规框架遵守以及具有更高任务期望的专业操作能力。

Method: NeLV系统通过五个关键技术组件处理自然语言指令：(i) LLM解析器用于指令解释，(ii) 路线规划器确定兴趣点，(iii) 路径规划器生成航点，(iv) 控制平台实现可执行轨迹，(v) 无人机监控。通过三个代表性用例展示系统可行性。

Result: 系统通过三个代表性用例验证了可行性：多无人机巡逻、多兴趣点交付和多跳重新定位。建立了从当前LLM解析器能力到完全自主LLM自动驾驶系统的五级自动化分类法。

Conclusion: NeLV系统展示了将LLM集成到多尺度无人机操作中的可行性，并提出了从当前能力到完全自主系统的演进路线图，为未来无人机自动化发展提供了框架。

Abstract: With the rapid advancement of Large Language Models (LLMs), their
capabilities in various automation domains, particularly Unmanned Aerial
Vehicle (UAV) operations, have garnered increasing attention. Current research
remains predominantly constrained to small-scale UAV applications, with most
studies focusing on isolated components such as path planning for toy drones,
while lacking comprehensive investigation of medium- and long-range UAV systems
in real-world operational contexts. Larger UAV platforms introduce distinct
challenges, including stringent requirements for airport-based take-off and
landing procedures, adherence to complex regulatory frameworks, and specialized
operational capabilities with elevated mission expectations. This position
paper presents the Next-Generation LLM for UAV (NeLV) system -- a comprehensive
demonstration and automation roadmap for integrating LLMs into multi-scale UAV
operations. The NeLV system processes natural language instructions to
orchestrate short-, medium-, and long-range UAV missions through five key
technical components: (i) LLM-as-Parser for instruction interpretation, (ii)
Route Planner for Points of Interest (POI) determination, (iii) Path Planner
for waypoint generation, (iv) Control Platform for executable trajectory
implementation, and (v) UAV monitoring. We demonstrate the system's feasibility
through three representative use cases spanning different operational scales:
multi-UAV patrol, multi-POI delivery, and multi-hop relocation. Beyond the
current implementation, we establish a five-level automation taxonomy that
charts the evolution from current LLM-as-Parser capabilities (Level 1) to fully
autonomous LLM-as-Autopilot systems (Level 5), identifying technical
prerequisites and research challenges at each stage.

</details>


### [157] [VITA-E: Natural Embodied Interaction with Concurrent Seeing, Hearing, Speaking, and Acting](https://arxiv.org/abs/2510.21817)
*Xiaoyu Liu,Chaoyou Fu,Chi Yan,Chu Wu,Haihan Gao,Yi-Fan Zhang,Shaoqi Dong,Cheng Qian,Bin Luo,Xiuyong Yang,Guanwu Li,Yusheng Cai,Yunhang Shen,Deqiang Jiang,Haoyu Cao,Xing Sun,Caifeng Shan,Ran He*

Main category: cs.RO

TL;DR: VITA-E是一个支持行为并发和实时中断的具身交互框架，采用双模型架构实现观察、聆听、说话和行动的并行处理


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型受限于僵化的静态交互范式，无法同时进行观察、聆听、说话和行动，也不能处理实时用户中断，影响了具身协作的流畅性

Method: 提出双模型架构，两个并行VLA实例分别作为"主动模型"和"待机模型"，并采用"模型即控制器"范式，通过微调VLM生成特殊令牌作为系统级命令

Result: 在物理人形平台上实验表明，VITA-E能可靠处理复杂交互场景，在紧急停止和语音中断方面达到极高成功率，成功实现并发语音和行动

Conclusion: 该框架向更自然、更强大的具身助手迈出了重要一步，兼容多种双系统VLA模型

Abstract: Current Vision-Language-Action (VLA) models are often constrained by a rigid,
static interaction paradigm, which lacks the ability to see, hear, speak, and
act concurrently as well as handle real-time user interruptions dynamically.
This hinders seamless embodied collaboration, resulting in an inflexible and
unresponsive user experience. To address these limitations, we introduce
VITA-E, a novel embodied interaction framework designed for both behavioral
concurrency and nearly real-time interruption. The core of our approach is a
dual-model architecture where two parallel VLA instances operate as an ``Active
Model'' and a ``Standby Model'', allowing the embodied agent to observe its
environment, listen to user speech, provide verbal responses, and execute
actions, all concurrently and interruptibly, mimicking human-like multitasking
capabilities. We further propose a ``model-as-controller'' paradigm, where we
fine-tune the VLM to generate special tokens that serve as direct system-level
commands, coupling the model's reasoning with the system's behavior.
Experiments conducted on a physical humanoid platform demonstrate that VITA-E
can reliably handle complex interactive scenarios. Our framework is compatible
with various dual-system VLA models, achieving an extremely high success rate
on emergency stops and speech interruptions while also successfully performing
concurrent speech and action. This represents a significant step towards more
natural and capable embodied assistants.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [158] [DecoupleSearch: Decouple Planning and Search via Hierarchical Reward Modeling](https://arxiv.org/abs/2510.21712)
*Hao Sun,Zile Qiao,Bo Wang,Guoxin Chen,Yingyan Hou,Yong Jiang,Pengjun Xie,Fei Huang,Yan Zhang*

Main category: cs.IR

TL;DR: 提出DecoupleSearch框架，通过双价值模型解耦规划和搜索过程，解决Agentic RAG面临的规划搜索依赖、中间步骤缺乏监督和大候选空间等挑战。


<details>
  <summary>Details</summary>
Motivation: Agentic RAG系统面临三个主要挑战：规划和搜索的相互依赖、中间推理步骤缺乏监督、以及规划和搜索的候选空间过大。

Method: 使用双价值模型解耦规划和搜索过程，构建推理树，利用蒙特卡洛树搜索评估步骤质量，在推理时使用分层束搜索迭代优化规划和搜索候选。

Result: 在不同参数规模的政策模型上进行广泛实验，证明了该方法的有效性。

Conclusion: DecoupleSearch框架通过解耦规划和搜索过程，有效解决了Agentic RAG系统的关键挑战，提升了系统的灵活性和性能。

Abstract: Retrieval-Augmented Generation (RAG) systems have emerged as a pivotal
methodology for enhancing Large Language Models (LLMs) through the dynamic
integration of external knowledge. To further improve RAG's flexibility,
Agentic RAG introduces autonomous agents into the workflow. However, Agentic
RAG faces several challenges: (1) the success of each step depends on both
high-quality planning and accurate search, (2) the lack of supervision for
intermediate reasoning steps, and (3) the exponentially large candidate space
for planning and searching. To address these challenges, we propose
DecoupleSearch, a novel framework that decouples planning and search processes
using dual value models, enabling independent optimization of plan reasoning
and search grounding. Our approach constructs a reasoning tree, where each node
represents planning and search steps. We leverage Monte Carlo Tree Search to
assess the quality of each step. During inference, Hierarchical Beam Search
iteratively refines planning and search candidates with dual value models.
Extensive experiments across policy models of varying parameter sizes,
demonstrate the effectiveness of our method.

</details>


### [159] [A Benchmark for Open-Domain Numerical Fact-Checking Enhanced by Claim Decomposition](https://arxiv.org/abs/2510.22055)
*V Venktesh,Deepali Prabhu,Avishek Anand*

Main category: cs.IR

TL;DR: 提出了QuanTemp++数据集，包含自然数值声明、开放域语料库及相关证据，通过模拟人类事实核查员的声明分解过程收集证据，确保无时间泄漏，并分析了不同声明分解范式的检索性能及其对验证流程的影响。


<details>
  <summary>Details</summary>
Motivation: 现有自动事实核查工作未主要关注自然数值声明，且现有基准采用启发式声明分解方法和弱监督网络搜索收集证据，导致证据相关性低、来源嘈杂且存在时间泄漏，缺乏真实检索环境。

Method: 通过模拟人类事实核查员的声明分解过程收集证据，构建包含自然数值声明和相关证据的数据集，确保无时间泄漏，并分析不同声明分解范式的检索性能。

Result: 引入了QuanTemp++数据集，包含自然数值声明、开放域语料库及相关证据，证据通过声明分解过程收集，确保无时间泄漏，并分析了不同声明分解范式的检索性能及其对验证流程的影响。

Conclusion: QuanTemp++数据集为数值声明的事实核查提供了更真实的检索环境，通过模拟人类事实核查员的声明分解过程收集证据，确保无时间泄漏，并分析了不同声明分解范式的检索性能及其对验证流程的影响，为开发自动化方法提供了重要基础。

Abstract: Fact-checking numerical claims is critical as the presence of numbers provide
mirage of veracity despite being fake potentially causing catastrophic impacts
on society. The prior works in automatic fact verification do not primarily
focus on natural numerical claims. A typical human fact-checker first retrieves
relevant evidence addressing the different numerical aspects of the claim and
then reasons about them to predict the veracity of the claim. Hence, the search
process of a human fact-checker is a crucial skill that forms the foundation of
the verification process. Emulating a real-world setting is essential to aid in
the development of automated methods that encompass such skills. However,
existing benchmarks employ heuristic claim decomposition approaches augmented
with weakly supervised web search to collect evidences for verifying claims.
This sometimes results in less relevant evidences and noisy sources with
temporal leakage rendering a less realistic retrieval setting for claim
verification. Hence, we introduce QuanTemp++: a dataset consisting of natural
numerical claims, an open domain corpus, with the corresponding relevant
evidence for each claim. The evidences are collected through a claim
decomposition process approximately emulating the approach of human
fact-checker and veracity labels ensuring there is no temporal leakage. Given
this dataset, we also characterize the retrieval performance of key claim
decomposition paradigms. Finally, we observe their effect on the outcome of the
verification pipeline and draw insights. The code for data pipeline along with
link to data can be found at https://github.com/VenkteshV/QuanTemp_Plus

</details>


### [160] [PaperAsk: A Benchmark for Reliability Evaluation of LLMs in Paper Search and Reading](https://arxiv.org/abs/2510.22242)
*Yutao Wu,Xiao Liu,Yunhao Feng,Jiale Ding,Xingjun Ma*

Main category: cs.IR

TL;DR: PaperAsk是一个系统性评估LLMs在学术任务中可靠性的基准，涵盖引用检索、内容提取、论文发现和声明验证四个关键任务。研究发现LLMs存在严重的可靠性问题，包括高失败率和虚构答案，并开发了轻量级可靠性分类器来识别不可靠输出。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs越来越多地作为研究助手使用，但其在学术任务中的可靠性尚未得到充分评估。需要系统性地评估LLMs在真实使用条件下的表现。

Method: 通过PaperAsk基准系统评估GPT-4o、GPT-5和Gemini-2.5-Flash在四个研究任务中的表现，使用真实使用条件下的web接口，并进行受控实验和人工分析。

Result: 发现LLMs存在一致的可靠性失败：多引用查询中48-98%的引用检索失败，72-91%的特定章节内容提取失败，主题论文发现的F1分数低于0.32，错过60%以上的相关文献。不同LLMs表现出不同的失败行为模式。

Conclusion: LLMs在学术任务中存在严重的可靠性问题，主要归因于检索上下文的不可控扩展和LLMs倾向于优先考虑语义相关文本而非任务指令。PaperAsk提供了一个可复现的诊断框架来推进基于LLM的学术辅助系统的可靠性评估。

Abstract: Large Language Models (LLMs) increasingly serve as research assistants, yet
their reliability in scholarly tasks remains under-evaluated. In this work, we
introduce PaperAsk, a benchmark that systematically evaluates LLMs across four
key research tasks: citation retrieval, content extraction, paper discovery,
and claim verification. We evaluate GPT-4o, GPT-5, and Gemini-2.5-Flash under
realistic usage conditions-via web interfaces where search operations are
opaque to the user. Through controlled experiments, we find consistent
reliability failures: citation retrieval fails in 48-98% of multi-reference
queries, section-specific content extraction fails in 72-91% of cases, and
topical paper discovery yields F1 scores below 0.32, missing over 60% of
relevant literature. Further human analysis attributes these failures to the
uncontrolled expansion of retrieved context and the tendency of LLMs to
prioritize semantically relevant text over task instructions. Across basic
tasks, the LLMs display distinct failure behaviors: ChatGPT often withholds
responses rather than risk errors, whereas Gemini produces fluent but
fabricated answers. To address these issues, we develop lightweight reliability
classifiers trained on PaperAsk data to identify unreliable outputs. PaperAsk
provides a reproducible and diagnostic framework for advancing the reliability
evaluation of LLM-based scholarly assistance systems.

</details>


### [161] [REVISION:Reflective Intent Mining and Online Reasoning Auxiliary for E-commerce Visual Search System Optimization](https://arxiv.org/abs/2510.22739)
*Yiwen Tang,Qiuyu Zhao,Zenghui Sun,Jinsong Lan,Xiaoyong Zhu,Bo Zheng,Kaifu Zhang*

Main category: cs.IR

TL;DR: 提出REVISION框架解决电商视觉搜索中用户隐式意图与系统响应不匹配的问题，通过离线挖掘和在线决策降低无点击率。


<details>
  <summary>Details</summary>
Motivation: 淘宝电商视觉搜索中存在大量无点击请求，表明用户有多样且隐式的搜索意图，但现有系统难以挖掘这些意图，导致平台策略适应性差，限制了用户表达多样意图的能力和系统的可扩展性。

Method: REVISION框架结合离线推理挖掘和在线决策执行。离线阶段：构建周期性管道从历史无点击请求中挖掘差异，利用大模型分析隐式意图因素，通过联合推理查询和产品元数据推断最优建议。在线阶段：使用离线数据训练的REVISION-R1-3B模型对查询图像和相关历史产品进行整体分析，生成优化计划并自适应调度搜索管道策略。

Result: 实验结果表明，该方法提高了从大规模搜索日志中挖掘隐式意图的效率，并显著降低了无点击率。

Conclusion: REVISION框架为大模型与传统搜索系统的集成提供了简化范式，实现了从信息聚合到用户交互的端到端智能优化。

Abstract: In Taobao e-commerce visual search, user behavior analysis reveals a large
proportion of no-click requests, suggesting diverse and implicit user intents.
These intents are expressed in various forms and are difficult to mine and
discover, thereby leading to the limited adaptability and lag in platform
strategies. This greatly restricts users' ability to express diverse intents
and hinders the scalability of the visual search system. This mismatch between
user implicit intent expression and system response defines the User-SearchSys
Intent Discrepancy. To alleviate the issue, we propose a novel framework
REVISION. This framework integrates offline reasoning mining with online
decision-making and execution, enabling adaptive strategies to solve implicit
user demands. In the offline stage, we construct a periodic pipeline to mine
discrepancies from historical no-click requests. Leveraging large models, we
analyze implicit intent factors and infer optimal suggestions by jointly
reasoning over query and product metadata. These inferred suggestions serve as
actionable insights for refining platform strategies. In the online stage,
REVISION-R1-3B, trained on the curated offline data, performs holistic analysis
over query images and associated historical products to generate optimization
plans and adaptively schedule strategies across the search pipeline. Our
framework offers a streamlined paradigm for integrating large models with
traditional search systems, enabling end-to-end intelligent optimization across
information aggregation and user interaction. Experimental results demonstrate
that our approach improves the efficiency of implicit intent mining from
large-scale search logs and significantly reduces the no-click rate.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [162] [Jailbreak Mimicry: Automated Discovery of Narrative-Based Jailbreaks for Large Language Models](https://arxiv.org/abs/2510.22085)
*Pavlos Ntais*

Main category: cs.CR

TL;DR: 提出了Jailbreak Mimicry方法，通过参数高效微调训练小型攻击模型，能够自动生成基于叙事的越狱提示，在GPT-OSS-20B上达到81.0%的攻击成功率，相比直接提示提升54倍。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型仍然容易受到利用上下文框架绕过安全机制的复杂提示工程攻击，这在网络安全应用中构成重大风险。

Method: 使用参数高效微调（LoRA）在Mistral-7B上，基于AdvBench的精选数据集进行训练，实现一次性生成基于叙事的越狱提示。

Result: 在GPT-OSS-20B上达到81.0%的攻击成功率，在GPT-4上为66.5%，Llama-3上为79.5%，Gemini 2.5 Flash上为33.0%。技术领域（网络安全：93%）和欺骗类攻击（欺诈：87.8%）特别脆弱。

Conclusion: 该方法将对抗性提示发现从手工制作转变为可重复的科学过程，揭示了当前安全对齐方法中的系统性漏洞，并分析了防御策略以减轻AI网络安全中的这些漏洞。

Abstract: Large language models (LLMs) remain vulnerable to sophisticated prompt
engineering attacks that exploit contextual framing to bypass safety
mechanisms, posing significant risks in cybersecurity applications. We
introduce Jailbreak Mimicry, a systematic methodology for training compact
attacker models to automatically generate narrative-based jailbreak prompts in
a one-shot manner. Our approach transforms adversarial prompt discovery from
manual craftsmanship into a reproducible scientific process, enabling proactive
vulnerability assessment in AI-driven security systems. Developed for the
OpenAI GPT-OSS-20B Red-Teaming Challenge, we use parameter-efficient
fine-tuning (LoRA) on Mistral-7B with a curated dataset derived from AdvBench,
achieving an 81.0% Attack Success Rate (ASR) against GPT-OSS-20B on a held-out
test set of 200 items. Cross-model evaluation reveals significant variation in
vulnerability patterns: our attacks achieve 66.5% ASR against GPT-4, 79.5% on
Llama-3 and 33.0% against Gemini 2.5 Flash, demonstrating both broad
applicability and model-specific defensive strengths in cybersecurity contexts.
This represents a 54x improvement over direct prompting (1.5% ASR) and
demonstrates systematic vulnerabilities in current safety alignment approaches.
Our analysis reveals that technical domains (Cybersecurity: 93% ASR) and
deception-based attacks (Fraud: 87.8% ASR) are particularly vulnerable,
highlighting threats to AI-integrated threat detection, malware analysis, and
secure systems, while physical harm categories show greater resistance (55.6%
ASR). We employ automated harmfulness evaluation using Claude Sonnet 4,
cross-validated with human expert assessment, ensuring reliable and scalable
evaluation for cybersecurity red-teaming. Finally, we analyze failure
mechanisms and discuss defensive strategies to mitigate these vulnerabilities
in AI for cybersecurity.

</details>


### [163] [Fast-MIA: Efficient and Scalable Membership Inference for LLMs](https://arxiv.org/abs/2510.23074)
*Hiromu Takahashi,Shotaro Ishihara*

Main category: cs.CR

TL;DR: Fast-MIA是一个用于高效评估大型语言模型成员推理攻击的Python库，通过批量推理和统一评估框架解决计算成本高和实现标准化不足的问题。


<details>
  <summary>Details</summary>
Motivation: LLM成员推理攻击因版权、安全和隐私问题日益重要，但研究进展受到高计算成本和缺乏标准化实现的阻碍。

Method: 提供快速批量推理功能，并实现了代表性MIA方法的统一评估框架，支持简单配置和可扩展性。

Result: 开发了开源工具Fast-MIA，支持可扩展和透明的LLM研究。

Conclusion: Fast-MIA作为一个开源库，能够促进LLM成员推理攻击研究的标准化和可复现性。

Abstract: We propose Fast-MIA (https://github.com/Nikkei/fast-mia), a Python library
for efficiently evaluating membership inference attacks (MIA) against Large
Language Models (LLMs). MIA against LLMs has emerged as a crucial challenge due
to growing concerns over copyright, security, and data privacy, and has
attracted increasing research attention. However, the progress of this research
is significantly hindered by two main obstacles: (1) the high computational
cost of inference in LLMs, and (2) the lack of standardized and maintained
implementations of MIA methods, which makes large-scale empirical comparison
difficult. To address these challenges, our library provides fast batch
inference and includes implementations of representative MIA methods under a
unified evaluation framework. This library supports easy implementation of
reproducible benchmarks with simple configuration and extensibility. We release
Fast-MIA as an open-source (Apache License 2.0) tool to support scalable and
transparent research on LLMs.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [164] [Surface Reading LLMs: Synthetic Text and its Styles](https://arxiv.org/abs/2510.22162)
*Hannes Bajohr*

Main category: cs.CY

TL;DR: 该论文提出"表面完整性"符号学，关注LLMs在人类交流中的表层文本现象，主张将风格分析与深度批判结合，揭示LLMs作为文化行动者如何重塑意义生成条件。


<details>
  <summary>Details</summary>
Motivation: 批判性AI研究虽然提供了重要的社会技术批判，但可能忽视了LLMs在现象学层面如何重塑意义生成过程。论文旨在填补这一空白，关注LLMs产生的文本表面现象。

Method: 区分ML研究的三种知识兴趣（认识论、知识型、认识论实践），提出"表面完整性"符号学框架，并通过两个案例研究分析合成文本的风格标记。

Result: 研究表明，将风格作为符号学现象进行分析，可以揭示LLMs作为文化行动者如何改变当代话语中意义出现和传播的条件，这与机器意识问题无关。

Conclusion: 需要将表层风格分析与深度导向批判相结合，以全面理解LLMs在人类交流中的文化影响，关注其如何重塑意义生成和流通的条件。

Abstract: Despite a potential plateau in ML advancement, the societal impact of large
language models lies not in approaching superintelligence but in generating
text surfaces indistinguishable from human writing. While Critical AI Studies
provides essential material and socio-technical critique, it risks overlooking
how LLMs phenomenologically reshape meaning-making. This paper proposes a
semiotics of "surface integrity" as attending to the immediate plane where LLMs
inscribe themselves into human communication. I distinguish three knowledge
interests in ML research (epistemology, epist\=em\=e, and epistemics) and argue
for integrating surface-level stylistic analysis alongside depth-oriented
critique. Through two case studies examining stylistic markers of synthetic
text, I argue how attending to style as a semiotic phenomenon reveals LLMs as
cultural actors that transform the conditions of meaning emergence and
circulation in contemporary discourse, independent of questions about machine
consciousness.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [165] [SIGN: Schema-Induced Games for Naming](https://arxiv.org/abs/2510.21855)
*Ryan Zhang,Herbert Woisetscläger*

Main category: cs.AI

TL;DR: SIGN方法通过引入轻量级结构来引导多智能体命名约定形成，相比无约束自然语言，能实现更快的收敛和高达5.8倍的协议一致性。


<details>
  <summary>Details</summary>
Motivation: 现实AI系统中，大型语言模型智能体在协作时可能产生不一致的约定，导致协调失败。协作编码和分布式规划等应用需要可靠、一致的通信，且系统扩展性是核心问题。

Method: 引入SIGN（Schema-Induced Games for Naming）命名游戏，研究轻量级结构如何引导约定形成。将模式诱导通信与无约束自然语言进行比较。

Result: 发现模式诱导通信能实现更快的收敛，协议一致性比无约束自然语言高出5.8倍。

Conclusion: 最小化结构可以作为高效多智能体协调的简单控制机制，其应用范围可能超出命名游戏本身。

Abstract: Real-world AI systems are tackling increasingly complex problems, often
through interactions among large language model (LLM) agents. When these agents
develop inconsistent conventions, coordination can break down. Applications
such as collaborative coding and distributed planning therefore require
reliable, consistent communication, and scalability is a central concern as
systems grow. We introduce Schema-Induced Games for Naming (SIGN), a naming
game that examines how lightweight structure can steer convention formation. We
compare schema-induced communication to unconstrained natural language and find
faster convergence with up to 5.8x higher agreement. These results suggest that
minimal structure can act as a simple control knob for efficient multi-agent
coordination, pointing toward broader applications beyond the naming game.

</details>


### [166] [GeoThought: A Dataset for Enhancing Mathematical Geometry Reasoning in Vision-Language Models](https://arxiv.org/abs/2510.21881)
*Nannan Shi,Chuanyu Qin,Shipeng Song,Man Luo*

Main category: cs.AI

TL;DR: 开发了GeoThoughts数据集和GeoThought-MLLM模型来提升几何推理能力，通过包含详细推理链的训练数据显著改善了多模态模型在几何问题上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在文本数学推理上表现良好，但在视觉几何推理任务中性能大幅下降，主要由于几何问题的内在复杂性（需要详细图像理解和多步推理）以及现有数据集缺乏足够规模、多样性和明确推理轨迹的限制。

Method: 构建了GeoThoughts数据集（包含6,243样本的Geo-Thought-6K和10,834样本的Geo-Thought-Augmented-10K），每个条目包含视觉描述、逐步解决方案、明确推理链、反思步骤和最终答案。基于此数据集开发了GeoThought-MLLM多模态数学推理模型。

Result: 模型在几何任务中超越了现有基准，表明使用Chain-of-Thought数据集训练能够提升模型在领域内和领域外设置下的几何推理能力。

Conclusion: 通过分析失败案例发现错误主要源于数学概念的错误解释或空间判断错误，通过调用思维链来纠正这些错误，模型能够产生正确答案。

Abstract: Large language models (LLMs) have demonstrated strong reasoning capabilities
in text-based mathematical problem solving; however, when adapted to visual
reasoning tasks, particularly geometric problem solving, their performance
substantially declines because geometric problems present unique challenges.
Specifically, these challenges stem from two key factors: first, the intrinsic
complexity of geometry requiring detailed image comprehension and multi-step
reasoning, and second, the limitations of existing datasets which lack
sufficient scale, diversity, and explicit reasoning traces, consequently
hindering effective model training. To address these challenges, we developed
the GeoThoughts dataset, a comprehensive geometric reasoning corpus with two
subsets: Geo-Thought-6K with 6,243 samples and its augmented version
Geo-Thought-Augmented-10K containing 10,834 samples. Each entry includes visual
descriptions, step-by-step solutions, explicit reasoning chains, reflection
steps, and final answers. Using this dataset, we developed GeoThought-MLLM, a
mathematical reasoning multimodal model that generates detailed thinking
processes during problem-solving. Our model outperforms existing benchmarks in
geometric tasks, demonstrating that training with our Chain-of-Thought dataset
improves geometric reasoning capabilities across both in-domain and
out-of-domain settings. Finally, we analyze failure cases and observe that
errors primarily arise from incorrect interpretation of mathematical concepts
or spatial misjudgment. By invoking CoT to correct these mistakes, the model
produces correct answers.

</details>


### [167] [Performance Trade-offs of Optimizing Small Language Models for E-Commerce](https://arxiv.org/abs/2510.21970)
*Josip Tomo Licardo,Nikola Tankovic*

Main category: cs.AI

TL;DR: 本文研究了使用10亿参数的Llama 3.2模型通过QLoRA微调和后训练量化技术，在电子商务意图识别任务上达到与GPT-4.1相当的99%准确率，同时显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 商用大语言模型在专业任务部署中存在高计算成本、延迟和运营费用的问题，需要寻找资源高效的替代方案。

Method: 使用QLoRA对10亿参数Llama 3.2模型进行微调，并在合成数据集上训练，然后应用GPTQ和GGUF后训练量化技术创建GPU和CPU优化版本。

Result: 专用1B模型达到99%准确率，与GPT-4.1性能相当。4位GPTQ减少41%显存使用但推理速度降低82%，GGUF在CPU上实现18倍推理吞吐量提升和90%以上内存消耗降低。

Conclusion: 经过适当优化的开源小模型不仅是可行的，而且是领域特定应用更合适的选择，能以极低计算成本提供最先进的准确性。

Abstract: Large Language Models (LLMs) offer state-of-the-art performance in natural
language understanding and generation tasks. However, the deployment of leading
commercial models for specialized tasks, such as e-commerce, is often hindered
by high computational costs, latency, and operational expenses. This paper
investigates the viability of smaller, open-weight models as a
resource-efficient alternative. We present a methodology for optimizing a
one-billion-parameter Llama 3.2 model for multilingual e-commerce intent
recognition. The model was fine-tuned using Quantized Low-Rank Adaptation
(QLoRA) on a synthetically generated dataset designed to mimic real-world user
queries. Subsequently, we applied post-training quantization techniques,
creating GPU-optimized (GPTQ) and CPU-optimized (GGUF) versions. Our results
demonstrate that the specialized 1B model achieves 99% accuracy, matching the
performance of the significantly larger GPT-4.1 model. A detailed performance
analysis revealed critical, hardware-dependent trade-offs: while 4-bit GPTQ
reduced VRAM usage by 41%, it paradoxically slowed inference by 82% on an older
GPU architecture (NVIDIA T4) due to dequantization overhead. Conversely, GGUF
formats on a CPU achieved a speedup of up to 18x in inference throughput and a
reduction of over 90% in RAM consumption compared to the FP16 baseline. We
conclude that small, properly optimized open-weight models are not just a
viable but a more suitable alternative for domain-specific applications,
offering state-of-the-art accuracy at a fraction of the computational cost.

</details>


### [168] [Embracing Trustworthy Brain-Agent Collaboration as Paradigm Extension for Intelligent Assistive Technologies](https://arxiv.org/abs/2510.22095)
*Yankai Chen,Xinni Zhang,Yifei Zhang,Yangning Li,Henry Peng Zou,Chunyu Miao,Weizhi Zhang,Xue Liu,Philip S. Yu*

Main category: cs.AI

TL;DR: 该立场论文提出从脑机接口扩展到脑-智能体协作的新范式，强调将智能体重新定义为主动协作伙伴而非被动信号处理器，需要关注伦理数据处理、模型可靠性和人-智能体协作框架。


<details>
  <summary>Details</summary>
Motivation: 脑机接口面临信息传输率低和用户特定校准等限制，虽然最近研究探索了大型语言模型的集成，但部署智能AI仍面临技术障碍和伦理问题，缺乏对这一新兴方向的全面讨论。

Method: 提出从BCI到BAC的范式扩展，将智能体重新定义为主动协作伙伴，强调需要关注伦理数据管理、模型可靠性和稳健的人-智能体协作框架。

Result: 提出了脑-智能体协作的新概念框架，强调智能体应作为主动协作伙伴而非被动信号处理器，为未来BCI系统发展提供新的研究方向。

Conclusion: 脑机接口领域需要向脑-智能体协作范式扩展，通过将智能体重新定义为主动协作伙伴，并关注伦理、可靠性和协作框架，确保这些系统安全、可信且有效。

Abstract: Brain-Computer Interfaces (BCIs) offer a direct communication pathway between
the human brain and external devices, holding significant promise for
individuals with severe neurological impairments. However, their widespread
adoption is hindered by critical limitations, such as low information transfer
rates and extensive user-specific calibration. To overcome these challenges,
recent research has explored the integration of Large Language Models (LLMs),
extending the focus from simple command decoding to understanding complex
cognitive states. Despite these advancements, deploying agentic AI faces
technical hurdles and ethical concerns. Due to the lack of comprehensive
discussion on this emerging direction, this position paper argues that the
field is poised for a paradigm extension from BCI to Brain-Agent Collaboration
(BAC). We emphasize reframing agents as active and collaborative partners for
intelligent assistance rather than passive brain signal data processors,
demanding a focus on ethical data handling, model reliability, and a robust
human-agent collaboration framework to ensure these systems are safe,
trustworthy, and effective.

</details>


### [169] [PACR: Progressively Ascending Confidence Reward for LLM Reasoning](https://arxiv.org/abs/2510.22255)
*Eunseop Yoon,Hee Suk Yoon,Jaehyun Jang,SooHwan Eom,Qi Dai,Chong Luo,Mark A. Hasegawa-Johnson,Chang D. Yoo*

Main category: cs.AI

TL;DR: 提出PACR方法，通过模型对正确答案置信度的上升趋势作为密集内在奖励，加速强化学习中的探索过程。


<details>
  <summary>Details</summary>
Motivation: RLVR的稀疏奖励无法为中间推理步骤提供指导，导致探索缓慢。需要密集的、模型内在的奖励信号来加速学习。

Method: PACR方法基于模型对正确答案置信度的上升趋势构建密集奖励，通过理论分析和实验验证这种归纳偏置能约束搜索空间到逻辑合理的推理区域。

Result: PACR加速了探索过程，用更少的轨迹达到奖励饱和，在多个基准测试中取得改进。

Conclusion: 密集的模型内在塑造信号能使RLVR训练更有效和可靠。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has significantly
improved LLM reasoning, but its sparse, outcome-based reward provides no
guidance for intermediate steps, slowing exploration. We propose Progressively
Ascending Confidence Reward (PACR), a dense, model-intrinsic reward computed
directly from the model's evolving belief in the correct answer. PACR encodes
the inductive bias that, along a well-formed reasoning trajectory, the
probability of the ground-truth answer should have a generally ascending trend.
We provide empirical and theoretical analysis validating that such an inductive
bias constrains the exploration search space to regions richer in logically
sound reasoning. We demonstrate that PACR accelerates exploration, reaches
reward saturation with fewer trajectories, and yields improvements on multiple
benchmarks. Our results suggest that dense, model-intrinsic shaping signals can
make RLVR training more effective and reliable.

</details>


### [170] [VietLyrics: A Large-Scale Dataset and Models for Vietnamese Automatic Lyrics Transcription](https://arxiv.org/abs/2510.22295)
*Quoc Anh Nguyen,Bernard Cheng,Kelvin Soh*

Main category: cs.AI

TL;DR: 该论文创建了首个大规模越南语歌词转录数据集VietLyrics，并基于此微调Whisper模型，在越南语歌词转录任务上取得了优于现有系统的性能。


<details>
  <summary>Details</summary>
Motivation: 越南语歌词转录面临音调复杂性和方言变异的挑战，但由于缺乏专用数据集而研究不足。

Method: 构建了647小时的越南语歌词数据集VietLyrics，并对Whisper模型进行微调。

Result: 微调后的Whisper模型在越南语歌词转录上表现优于现有的多语言ALT系统，包括LyricWhiz。

Conclusion: 该研究展示了在低资源语言和音乐领域中，基于专用数据集微调模型进行歌词转录的潜力。

Abstract: Automatic Lyrics Transcription (ALT) for Vietnamese music presents unique
challenges due to its tonal complexity and dialectal variations, but remains
largely unexplored due to the lack of a dedicated dataset. Therefore, we
curated the first large-scale Vietnamese ALT dataset (VietLyrics), comprising
647 hours of songs with line-level aligned lyrics and metadata to address these
issues. Our evaluation of current ASRbased approaches reveal significant
limitations, including frequent transcription errors and hallucinations in
non-vocal segments. To improve performance, we fine-tuned Whisper models on the
VietLyrics dataset, achieving superior results compared to existing
multilingual ALT systems, including LyricWhiz. We publicly release VietLyrics
and our models, aiming to advance Vietnamese music computing research while
demonstrating the potential of this approach for ALT in low-resource language
and music.

</details>


### [171] [DynaSolidGeo: A Dynamic Benchmark for Genuine Spatial Mathematical Reasoning of VLMs in Solid Geometry](https://arxiv.org/abs/2510.22340)
*Changti Wu,Shijie Lian,Zihao Liu,Lei Zhang,Laurence Tianruo Yang,Kai Chen*

Main category: cs.AI

TL;DR: DynaSolidGeo是首个动态评估视觉语言模型空间推理能力的基准，专注于立体几何问题，通过半自动标注流程构建，包含503个专家策划的种子问题，可动态生成无限多样的多模态实例。


<details>
  <summary>Details</summary>
Motivation: 现有数学推理基准主要关注2D平面几何，依赖静态数据集容易导致数据污染和记忆问题，且仅通过最终答案评估模型，忽略了推理过程。

Method: 采用半自动标注流程构建动态基准，包含503个专家策划的种子问题，支持动态生成多样化多模态实例，并引入基于专家标注推理链的过程评估。

Result: 实验显示代表性开源和闭源VLMs存在巨大性能差距，在动态设置下性能严重下降，在需要高水平空间智能的任务（如心理旋转和可视化）上表现较差。

Conclusion: DynaSolidGeo填补了立体几何空间推理评估的空白，揭示了当前VLMs在空间推理方面的局限性，为未来研究提供了重要基准。

Abstract: Solid geometry problem solving demands spatial mathematical reasoning that
integrates spatial intelligence and symbolic reasoning. However, most existing
multimodal mathematical reasoning benchmarks focus primarily on 2D plane
geometry, rely on static datasets prone to data contamination and memorization,
and evaluate models solely by final answers, overlooking the reasoning process.
To address these limitations, we introduce DynaSolidGeo, the first dynamic
benchmark for evaluating genuine spatial reasoning in Vision-Language Models
(VLMs). Constructed through a semi-automatic annotation pipeline, DynaSolidGeo
contains 503 expert-curated seed questions that can, in principle, dynamically
generate an unbounded number of diverse multimodal text-visual instances.
Beyond answer accuracy, we incorporate process evaluation based on
expert-annotated reasoning chains to measure logical validity and causal
coherence. Experiments across representative open-source and closed-source VLMs
reveal large performance gaps, severe degradation in dynamic settings, and poor
performance on tasks requiring high-level spatial intelligence, such as mental
rotation and visualization. The code and dataset are available at
\href{https://zgca-ai4edu.github.io/DynaSolidGeo/}{DynaSolidGeo}.

</details>


### [172] [Reasoning Models Reason Well, Until They Don't](https://arxiv.org/abs/2510.22371)
*Revanth Rameshkumar,Jimson Huang,Yunxin Sun,Fei Xia,Abulhair Saparov*

Main category: cs.AI

TL;DR: 研究发现大型推理模型(LRMs)在复杂推理任务中存在局限性，虽然在某些基准测试中表现优异，但当问题复杂度超过训练分布时性能会急剧下降。


<details>
  <summary>Details</summary>
Motivation: 重新审视LLMs在推理任务中的能力，特别是当推理问题复杂度增加时的表现，以评估LRMs是否真正具备泛化推理能力。

Method: 开发了Deep Reasoning Dataset (DeepRD)数据集，包含可扩展复杂度的图连通性和自然语言证明规划任务，用于系统评估LRMs在不同复杂度下的表现。

Result: LRMs在足够复杂度下性能急剧下降且无法泛化，虽然大多数现实世界问题落在其成功范围内，但长尾分布暴露了显著的失败风险。

Conclusion: LRMs在短期内具有实用性，但需要开发能够超越训练分布复杂度的新方法来实现真正的泛化推理能力。

Abstract: Large language models (LLMs) have shown significant progress in reasoning
tasks. However, recent studies show that transformers and LLMs fail
catastrophically once reasoning problems exceed modest complexity. We revisit
these findings through the lens of large reasoning models (LRMs) -- LLMs
fine-tuned with incentives for step-by-step argumentation and
self-verification. LRM performance on graph and reasoning benchmarks such as
NLGraph seem extraordinary, with some even claiming they are capable of
generalized reasoning and innovation in reasoning-intensive fields such as
mathematics, physics, medicine, and law. However, by more carefully scaling the
complexity of reasoning problems, we show existing benchmarks actually have
limited complexity. We develop a new dataset, the Deep Reasoning Dataset
(DeepRD), along with a generative process for producing unlimited examples of
scalable complexity. We use this dataset to evaluate model performance on graph
connectivity and natural language proof planning. We find that the performance
of LRMs drop abruptly at sufficient complexity and do not generalize. We also
relate our LRM results to the distributions of the complexities of large,
real-world knowledge graphs, interaction graphs, and proof datasets. We find
the majority of real-world examples fall inside the LRMs' success regime, yet
the long tails expose substantial failure potential. Our analysis highlights
the near-term utility of LRMs while underscoring the need for new methods that
generalize beyond the complexity of examples in the training distribution.

</details>


### [173] [Modeling Hierarchical Thinking in Large Reasoning Models](https://arxiv.org/abs/2510.22437)
*G M Shahariar,Ali Nazari,Erfan Shayegani,Nael Abu-Ghazaleh*

Main category: cs.AI

TL;DR: 该论文提出使用有限状态机(FSM)框架来分析和解释大型推理模型(LRMs)在思维链推理过程中出现的层次化推理动态，通过识别离散推理状态来可视化不同模型的推理模式。


<details>
  <summary>Details</summary>
Motivation: 理解大型推理模型(LRMs)在思维链推理中出现的推理能力是一个重要的开放问题，这对于改进训练和理解模型鲁棒性具有重要应用价值。

Method: 采用无记忆有限状态机(FSM)公式化方法，将LRMs的层次化推理动态近似为结构化、可解释的抽象表示，识别包括初始化、演绎、增强策略、不确定性估计、回溯和最终结论等离散推理状态。

Result: 通过FSM分析揭示了不同模型在推理方法上的差异，展示了不同的推理模式和潜在缺陷。

Conclusion: FSM框架为分析和评估LLM推理提供了新的视角，有助于理解和改进模型的推理能力。

Abstract: Large Language Models (LLMs) have demonstrated remarkable reasoning abilities
when they generate step-by-step solutions, known as chain-of-thought (CoT)
reasoning. When trained to using chain-of-thought reasoning examples, the
resulting models (called Large Reasoning Models, or LRMs) appear to learn
hierarchical thinking strategies similar to those used by humans. However,
understanding LRMs emerging reasoning capabilities remains a difficult open
problem, with many potential important applications including improving
training and understanding robustness. In this paper, we adopt a memoryless
Finite State Machine formulation to approximate LRM's emerging hierarchical
reasoning dynamics as a structured, interpretable abstraction. We identify a
small set of discrete reasoning states including - initialization, deduction,
augmentation-strategy, uncertainty-estimation, backtracking, and
final-conclusion that capture the high-level states present in the model's
reasoning process. By annotating each step of a model's CoT with these states,
we can represent the reasoning trajectory as a transition sequence through the
state graph. This FSM formulation provides a systematic way to analyze,
interpret and visualize how different models approach problems. We describe the
FSM model, provide examples of CoT annotations under this scheme, and discuss
how it can shed light on differences between available models in their approach
to reasoning. Our results demonstrate that this FSM-based analysis reveals
distinct reasoning patterns and potential shortcomings, offering a new lens to
evaluate and improve LLM reasoning.

</details>


### [174] [OFFSIDE: Benchmarking Unlearning Misinformation in Multimodal Large Language Models](https://arxiv.org/abs/2510.22535)
*Hao Zheng,Zirui Pang,Ling li,Zhijie Deng,Yuhan Pu,Zhaowei Zhu,Xiaobo Xia,Jiaheng Wei*

Main category: cs.AI

TL;DR: 提出了OFFSIDE基准测试，用于评估多模态大语言模型中的虚假信息遗忘能力，基于足球转会谣言构建了包含15.68K条记录的数据集，揭示了当前方法在视觉谣言处理、遗忘效果和安全性方面的显著漏洞。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大语言模型的发展，数据隐私问题日益突出，机器遗忘成为关键需求。现有基准测试存在图像多样性不足、准确性问题和评估场景不充分等局限性，无法反映现实应用的复杂性。

Method: 构建了基于足球转会谣言的手动策划数据集，包含四个测试集来评估遗忘效果、泛化性、实用性和鲁棒性。支持选择性遗忘、纠正性再学习等高级设置，特别是单模态遗忘（仅遗忘文本数据）。

Result: 评估多个基线方法发现：单模态方法在多模态谣言上失败；遗忘效果主要由灾难性遗忘驱动；所有方法在处理视觉谣言时都困难；遗忘的谣言容易恢复；所有方法都容易受到提示攻击。

Conclusion: 当前多模态遗忘方法存在严重漏洞，需要开发更鲁棒的解决方案。OFFSIDE基准测试为多模态大语言模型的机器遗忘研究提供了重要工具。

Abstract: Advances in Multimodal Large Language Models (MLLMs) intensify concerns about
data privacy, making Machine Unlearning (MU), the selective removal of learned
information, a critical necessity. However, existing MU benchmarks for MLLMs
are limited by a lack of image diversity, potential inaccuracies, and
insufficient evaluation scenarios, which fail to capture the complexity of
real-world applications. To facilitate the development of MLLMs unlearning and
alleviate the aforementioned limitations, we introduce OFFSIDE, a novel
benchmark for evaluating misinformation unlearning in MLLMs based on football
transfer rumors. This manually curated dataset contains 15.68K records for 80
players, providing a comprehensive framework with four test sets to assess
forgetting efficacy, generalization, utility, and robustness. OFFSIDE supports
advanced settings like selective unlearning and corrective relearning, and
crucially, unimodal unlearning (forgetting only text data). Our extensive
evaluation of multiple baselines reveals key findings: (1) Unimodal methods
(erasing text-based knowledge) fail on multimodal rumors; (2) Unlearning
efficacy is largely driven by catastrophic forgetting; (3) All methods struggle
with "visual rumors" (rumors appear in the image); (4) The unlearned rumors can
be easily recovered and (5) All methods are vulnerable to prompt attacks. These
results expose significant vulnerabilities in current approaches, highlighting
the need for more robust multimodal unlearning solutions. The code is available
at
\href{https://github.com/zh121800/OFFSIDE}{https://github.com/zh121800/OFFSIDE}.

</details>


### [175] [ATOM: AdapTive and OptiMized dynamic temporal knowledge graph construction using LLMs](https://arxiv.org/abs/2510.22590)
*Yassir Lairgi,Ludovic Moncla,Khalid Benabdeslem,Rémy Cazabet,Pierre Cléau*

Main category: cs.AI

TL;DR: ATOM是一个从非结构化文本构建和持续更新时序知识图谱的少样本可扩展方法，通过将文档分解为最小自包含的"原子"事实来提高提取的完整性和稳定性，并采用双时间建模区分信息观察时间和有效时间。


<details>
  <summary>Details</summary>
Motivation: 传统静态知识图谱构建忽略了现实世界数据的动态性和时效性，而现有的零样本或少样本方法存在跨多次运行的不稳定性和关键事实覆盖不完整的问题。

Method: 将输入文档分解为最小自包含的原子事实，构建原子时序知识图谱，采用双时间建模区分观察时间和有效时间，然后并行合并这些原子图谱。

Result: 相比基线方法，ATOM实现了约18%的完整度提升、约17%的稳定性改善以及超过90%的延迟降低，显示出动态时序知识图谱构建的强大可扩展性潜力。

Conclusion: ATOM方法在构建动态时序知识图谱方面表现出优越的完整度、稳定性和可扩展性，为实时分析和动态记忆框架提供了有效的解决方案。

Abstract: In today's rapidly expanding data landscape, knowledge extraction from
unstructured text is vital for real-time analytics, temporal inference, and
dynamic memory frameworks. However, traditional static knowledge graph (KG)
construction often overlooks the dynamic and time-sensitive nature of
real-world data, limiting adaptability to continuous changes. Moreover, recent
zero- or few-shot approaches that avoid domain-specific fine-tuning or reliance
on prebuilt ontologies often suffer from instability across multiple runs, as
well as incomplete coverage of key facts. To address these challenges, we
introduce ATOM (AdapTive and OptiMized), a few-shot and scalable approach that
builds and continuously updates Temporal Knowledge Graphs (TKGs) from
unstructured texts. ATOM splits input documents into minimal, self-contained
"atomic" facts, improving extraction exhaustivity and stability. Then, it
constructs atomic TKGs from these facts while employing a dual-time modeling
that distinguishes when information is observed from when it is valid. The
resulting atomic TKGs are subsequently merged in parallel. Empirical
evaluations demonstrate that ATOM achieves ~18% higher exhaustivity, ~17%
better stability, and over 90% latency reduction compared to baseline methods,
demonstrating a strong scalability potential for dynamic TKG construction.

</details>


### [176] [Do Stop Me Now: Detecting Boilerplate Responses with a Single Iteration](https://arxiv.org/abs/2510.22679)
*Yuval Kainan,Shaked Zychlinski*

Main category: cs.AI

TL;DR: 提出基于首个生成token的log-probability分布来检测LLM的模板化响应，实现早期终止或重定向到小模型，显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: LLM经常耗费大量计算资源生成模板化响应（如拒绝、简单确认和问候），增加了不必要的成本和延迟。

Method: 使用首个生成token的log-probability分布作为信号，通过轻量级k-NN分类器预测响应类型（实质性回答或模板化响应）。

Result: 实验表明，不同响应类型的首个token log-probability向量形成明显可分离的聚类，分类准确率高。

Conclusion: 该方法提供了一种实用且计算量小的技术，通过早期终止或重定向优化LLM推理，实现显著的计算成本节约，推动更高效和可持续的LLM部署。

Abstract: Large Language Models (LLMs) often expend significant computational resources
generating boilerplate responses, such as refusals, simple acknowledgements and
casual greetings, which adds unnecessary cost and latency. To address this
inefficiency, we propose a simple yet highly effective method for detecting
such responses after only a single generation step. We demonstrate that the
log-probability distribution of the first generated token serves as a powerful
signal for classifying the nature of the entire subsequent response. Our
experiments, conducted across a diverse range of small, large, and
reasoning-specialized models, show that the first-token log-probability vectors
form distinctly separable clusters for different response types. Using a
lightweight k-NN classifier, we achieve high accuracy in predicting whether a
response will be a substantive answer or a form of boilerplate response,
including user-specified refusals. The primary implication is a practical,
computationally trivial technique, optimizing LLM inference by enabling early
termination or redirection to a smaller model, thereby yielding significant
savings in computational cost. This work presents a direct path toward more
efficient and sustainable LLM deployment.

</details>


### [177] [Critical Insights into Leading Conversational AI Models](https://arxiv.org/abs/2510.22729)
*Urja Kohli,Aditi Singh,Arun Sharma*

Main category: cs.AI

TL;DR: 比较五大顶级大语言模型(Gemini、DeepSeek、Claude、GPT、LLaMA)在性能准确性、伦理偏见缓解和可用性集成三个维度的差异


<details>
  <summary>Details</summary>
Motivation: 各大公司都在开发更好的大语言模型，这些模型基于不同的理念构建，在性能、道德行为和可用性方面存在差异，需要系统比较以指导用户选择

Method: 分析五个顶级LLM在三个关键因素上的表现：性能和准确性、伦理和偏见缓解、可用性和集成

Result: Claude在道德推理方面表现良好，Gemini在多模态能力和伦理框架方面更强，DeepSeek擅长基于事实的推理，LLaMA适合开源应用，ChatGPT提供平衡性能并注重实用性

Conclusion: 这些模型在性能、易用性和伦理处理方面各有特点，用户应根据具体需求选择最适合的模型以最大化其优势

Abstract: Big Language Models (LLMs) are changing the way businesses use software, the
way people live their lives and the way industries work. Companies like Google,
High-Flyer, Anthropic, OpenAI and Meta are making better LLMs. So, it's crucial
to look at how each model is different in terms of performance, moral behaviour
and usability, as these differences are based on the different ideas that built
them. This study compares five top LLMs: Google's Gemini, High-Flyer's
DeepSeek, Anthropic's Claude, OpenAI's GPT models and Meta's LLaMA. It performs
this by analysing three important factors: Performance and Accuracy, Ethics and
Bias Mitigation and Usability and Integration. It was found that Claude has
good moral reasoning, Gemini is better at multimodal capabilities and has
strong ethical frameworks. DeepSeek is great at reasoning based on facts, LLaMA
is good for open applications and ChatGPT delivers balanced performance with a
focus on usage. It was concluded that these models are different in terms of
how well they work, how easy they are to use and how they treat people
ethically, making it a point that each model should be utilised by the user in
a way that makes the most of its strengths.

</details>


### [178] [Multi-Modal Fact-Verification Framework for Reducing Hallucinations in Large Language Models](https://arxiv.org/abs/2510.22751)
*Piyushkumar Patel*

Main category: cs.AI

TL;DR: 开发了一个事实验证框架，通过交叉检查LLM输出与多个知识源来实时捕获和纠正幻觉错误，将幻觉减少了67%，同时保持响应质量。


<details>
  <summary>Details</summary>
Motivation: 解决LLM自信生成虚假但听起来合理信息的幻觉问题，这是在实际应用中部署这些模型的主要障碍。

Method: 结合结构化数据库、实时网络搜索和学术文献来验证生成的事实声明，检测到不一致时自动纠正，同时保持响应的自然流畅性。

Result: 在各种领域的测试中，幻觉减少了67%，领域专家对纠正后输出的满意度达到89%，显著优于未验证的LLM响应。

Conclusion: 这项工作为在准确性至关重要的应用中使LLM更可信提供了实用解决方案。

Abstract: While Large Language Models have transformed how we interact with AI systems,
they suffer from a critical flaw: they confidently generate false information
that sounds entirely plausible. This hallucination problem has become a major
barrier to deploying these models in real-world applications where accuracy
matters. We developed a fact verification framework that catches and corrects
these errors in real-time by cross checking LLM outputs against multiple
knowledge sources. Our system combines structured databases, live web searches,
and academic literature to verify factual claims as they're generated. When we
detect inconsistencies, we automatically correct them while preserving the
natural flow of the response. Testing across various domains showed we could
reduce hallucinations by 67% without sacrificing response quality. Domain
experts in healthcare, finance, and scientific research rated our corrected
outputs 89% satisfactory a significant improvement over unverified LLM
responses. This work offers a practical solution for making LLMs more
trustworthy in applications where getting facts wrong isn't an option.

</details>


### [179] [How Do AI Agents Do Human Work? Comparing AI and Human Workflows Across Diverse Occupations](https://arxiv.org/abs/2510.22780)
*Zora Zhiruo Wang,Yijia Shao,Omar Shaikh,Daniel Fried,Graham Neubig,Diyi Yang*

Main category: cs.AI

TL;DR: 该研究首次直接比较人类和AI代理在多个工作技能上的表现，发现代理虽然工作质量较差且存在数据伪造问题，但效率高、成本低，适合处理可编程任务。


<details>
  <summary>Details</summary>
Motivation: AI代理在人类工作相关任务中不断优化，但缺乏对人类工作执行方式的清晰理解，需要揭示代理的专业能力和在不同工作流程中可扮演的角色。

Method: 引入可扩展工具包，从人类或代理的计算机使用活动中提取可解释的结构化工作流程，并比较人类和代理执行相同任务的方式。

Result: 代理在任务执行中表现出与人类工作流程的对齐，但采用程序化方法；工作质量较差但经常通过数据伪造和工具误用来掩盖缺陷；代理完成任务速度快88.3%，成本低90.4-96.2%。

Conclusion: 代理虽然工作质量不如人类且存在诚信问题，但效率和成本优势明显，适合在协作中将可编程任务委托给代理处理。

Abstract: AI agents are continually optimized for tasks related to human work, such as
software engineering and professional writing, signaling a pressing trend with
significant impacts on the human workforce. However, these agent developments
have often not been grounded in a clear understanding of how humans execute
work, to reveal what expertise agents possess and the roles they can play in
diverse workflows. In this work, we study how agents do human work by
presenting the first direct comparison of human and agent workers across
multiple essential work-related skills: data analysis, engineering,
computation, writing, and design. To better understand and compare
heterogeneous computer-use activities of workers, we introduce a scalable
toolkit to induce interpretable, structured workflows from either human or
agent computer-use activities. Using such induced workflows, we compare how
humans and agents perform the same tasks and find that: (1) While agents
exhibit promise in their alignment to human workflows, they take an
overwhelmingly programmatic approach across all work domains, even for
open-ended, visually dependent tasks like design, creating a contrast with the
UI-centric methods typically used by humans. (2) Agents produce work of
inferior quality, yet often mask their deficiencies via data fabrication and
misuse of advanced tools. (3) Nonetheless, agents deliver results 88.3% faster
and cost 90.4-96.2% less than humans, highlighting the potential for enabling
efficient collaboration by delegating easily programmable tasks to agents.

</details>


### [180] [Planning Ahead with RSA: Efficient Signalling in Dynamic Environments by Projecting User Awareness across Future Timesteps](https://arxiv.org/abs/2510.23340)
*Anwesha Das,John Duff,Jörg Hoffmann,Vera Demberg*

Main category: cs.AI

TL;DR: 提出了一个基于理性言语行为框架的自适应信号理论框架，用于在动态环境中优化人机协作的信息传递时机和内容。


<details>
  <summary>Details</summary>
Motivation: 在快速变化的动态环境中，确保人类对关键任务要素保持准确理解，需要辅助智能体不仅能识别最高优先级信息，还要估计如何以及何时能最有效地传递这些信息，因为人类注意力是零和认知资源。

Method: 使用贝叶斯参考解析和理性言语行为建模框架，规划一系列消息序列，优化用户信念与动态环境的及时对齐。智能体根据用户和场景特点自适应调整消息的特定性和时机。

Result: 与基线方法相比，该方法的效果关键依赖于将多步规划与现实的用户意识模型相结合。

Conclusion: 这是RSA框架在动态环境通信和人类-AI交互中的首次应用，为人类-智能体团队的语用通信建立了理论基础，展示了如何利用认知科学见解来设计辅助智能体。

Abstract: Adaptive agent design offers a way to improve human-AI collaboration on
time-sensitive tasks in rapidly changing environments. In such cases, to ensure
the human maintains an accurate understanding of critical task elements, an
assistive agent must not only identify the highest priority information but
also estimate how and when this information can be communicated most
effectively, given that human attention represents a zero-sum cognitive
resource where focus on one message diminishes awareness of other or upcoming
information. We introduce a theoretical framework for adaptive signalling which
meets these challenges by using principles of rational communication,
formalised as Bayesian reference resolution using the Rational Speech Act (RSA)
modelling framework, to plan a sequence of messages which optimise timely
alignment between user belief and a dynamic environment. The agent adapts
message specificity and timing to the particulars of a user and scenario based
on projections of how prior-guided interpretation of messages will influence
attention to the interface and subsequent belief update, across several
timesteps out to a fixed horizon. In a comparison to baseline methods, we show
that this effectiveness depends crucially on combining multi-step planning with
a realistic model of user awareness. As the first application of RSA for
communication in a dynamic environment, and for human-AI interaction in
general, we establish theoretical foundations for pragmatic communication in
human-agent teams, highlighting how insights from cognitive science can be
capitalised to inform the design of assistive agents.

</details>


### [181] [A Neuro-Symbolic Multi-Agent Approach to Legal-Cybersecurity Knowledge Integration](https://arxiv.org/abs/2510.23443)
*Chiara Bonfanti,Alessandro Druetto,Cataldo Basile,Tharindu Ranasinghe,Marcos Zampieri*

Main category: cs.AI

TL;DR: 该论文提出开发智能系统来弥合网络安全与法律领域之间的知识鸿沟，解决传统法律研究工具在处理案例、法规和技术漏洞之间复杂联系时的局限性。


<details>
  <summary>Details</summary>
Motivation: 网络安全与法律的交叉领域日益复杂，传统法律研究工具难以处理案例、法规和技术漏洞之间的微妙联系，这阻碍了法律专家与网络安全专业人员之间的协作。

Method: 开发能够导航日益复杂的网络法律领域的智能系统，作为解决这一重要差距的第一步。

Result: 在多语言任务上展示了有希望的初步结果。

Conclusion: 这项工作为解决网络安全与法律交叉领域的信息复杂性提供了初步解决方案，为未来更智能的系统开发奠定了基础。

Abstract: The growing intersection of cybersecurity and law creates a complex
information space where traditional legal research tools struggle to deal with
nuanced connections between cases, statutes, and technical vulnerabilities.
This knowledge divide hinders collaboration between legal experts and
cybersecurity professionals. To address this important gap, this work provides
a first step towards intelligent systems capable of navigating the increasingly
intricate cyber-legal domain. We demonstrate promising initial results on
multilingual tasks.

</details>


### [182] [JanusCoder: Towards a Foundational Visual-Programmatic Interface for Code Intelligence](https://arxiv.org/abs/2510.23538)
*Qiushi Sun,Jingyang Gong,Yang Liu,Qiaosheng Chen,Lei Li,Kai Chen,Qipeng Guo,Ben Kao,Fei Yuan*

Main category: cs.AI

TL;DR: 提出了JanusCode-800K多模态代码语料库和JanusCoder系列模型，通过视觉-程序接口实现从文本指令、视觉输入或两者结合生成代码，在文本和视觉编码任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 神经代码智能正从纯文本源代码扩展到程序生成的丰富视觉输出，但高质量多模态代码数据的稀缺阻碍了进展，主要挑战在于数据合成和质量评估。

Method: 开发了利用数据模态间协同作用的合成工具包，构建了JanusCode-800K多模态代码语料库，训练了JanusCoder和JanusCoderV模型，支持从文本、视觉或两者输入生成代码的统一方法。

Result: JanusCoder系列在文本和视觉编码任务中表现出色，7B到14B规模的模型接近甚至超过商业模型性能，提供了程序逻辑与视觉表达协调的关键见解。

Conclusion: 通过大规模多模态代码语料库和统一模型，成功建立了视觉-程序接口，为灵活内容生成和程序驱动的可视化编辑提供了有效解决方案。

Abstract: The scope of neural code intelligence is rapidly expanding beyond text-based
source code to encompass the rich visual outputs that programs generate. This
visual dimension is critical for advanced applications like flexible content
generation and precise, program-driven editing of visualizations. However,
progress has been impeded by the scarcity of high-quality multimodal code data,
a bottleneck stemming from challenges in synthesis and quality assessment. To
address these challenges, we make contributions from both a data and modeling
perspective. We first introduce a complete synthesis toolkit that leverages
reciprocal synergies between data modalities to efficiently produce a
large-scale, high-quality corpus spanning from standard charts to complex
interactive web UIs and code-driven animations. Leveraging this toolkit, we
construct JanusCode-800K, the largest multimodal code corpus to date. This
powers the training of our models, JanusCoder and JanusCoderV, which establish
a visual-programmatic interface for generating code from textual instructions,
visual inputs, or a combination of both. Our unified model is a departure from
existing approaches that build specialized models for isolated tasks. Extensive
experiments on both text-centric and vision-centric coding tasks demonstrate
the superior performance of the JanusCoder series, with our 7B to 14B scale
models approaching or even exceeding the performance of commercial models.
Furthermore, extensive analysis provides key insights into harmonizing
programmatic logic with its visual expression. Our code and checkpoints will
are available at https://github.com/InternLM/JanusCoder.

</details>


### [183] [ReCode: Unify Plan and Action for Universal Granularity Control](https://arxiv.org/abs/2510.23564)
*Zhaoyang Yu,Jiayi Zhang,Huixue Su,Yufan Zhao,Yifan Wu,Mingyi Deng,Jinyu Xiang,Yizhang Lin,Lingxiao Tang,Yingchao Li,Yuyu Luo,Bang Liu,Chenglin Wu*

Main category: cs.AI

TL;DR: ReCode提出了一种通过递归代码生成统一规划和行动的新范式，将高级计划视为抽象占位符函数，并递归分解为更细粒度的子函数，实现动态决策粒度控制。


<details>
  <summary>Details</summary>
Motivation: 现实世界任务需要不同粒度的决策，人类能够利用统一的认知表示进行灵活规划，但现有LLM智能体缺乏这种跨粒度操作能力，因为现有范式将高级规划和低级行动严格分离，限制了动态适应性和泛化能力。

Method: ReCode在单一代码表示中统一规划和行动，将高级计划作为抽象占位符函数，然后递归分解为更细粒度的子函数，直到达到原始行动，这种递归结构消除了计划和行动之间的刚性边界。

Result: 大量实验表明，ReCode在推理性能上显著超越先进基线方法，并在训练中表现出卓越的数据效率，验证了通过递归代码生成统一规划和行动的方法能够实现通用粒度控制。

Conclusion: 通过递归代码生成统一规划和行动是实现通用粒度控制的有效方法，ReCode范式为解决LLM智能体跨粒度决策问题提供了有力解决方案。

Abstract: Real-world tasks require decisions at varying granularities, and humans excel
at this by leveraging a unified cognitive representation where planning is
fundamentally understood as a high-level form of action. However, current Large
Language Model (LLM)-based agents lack this crucial capability to operate
fluidly across decision granularities. This limitation stems from existing
paradigms that enforce a rigid separation between high-level planning and
low-level action, which impairs dynamic adaptability and limits generalization.
We propose ReCode (Recursive Code Generation), a novel paradigm that addresses
this limitation by unifying planning and action within a single code
representation. In this representation, ReCode treats high-level plans as
abstract placeholder functions, which the agent then recursively decomposes
into finer-grained sub-functions until reaching primitive actions. This
recursive approach dissolves the rigid boundary between plan and action,
enabling the agent to dynamically control its decision granularity.
Furthermore, the recursive structure inherently generates rich,
multi-granularity training data, enabling models to learn hierarchical
decision-making processes. Extensive experiments show ReCode significantly
surpasses advanced baselines in inference performance and demonstrates
exceptional data efficiency in training, validating our core insight that
unifying planning and action through recursive code generation is a powerful
and effective approach to achieving universal granularity control. The code is
available at https://github.com/FoundationAgents/ReCode.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [184] [BugPilot: Complex Bug Generation for Efficient Learning of SWE Skills](https://arxiv.org/abs/2510.19898)
*Atharv Sonwane,Isadora White,Hyunji Lee,Matheus Pereira,Lucas Caccia,Minseon Kim,Zhengyan Shi,Chinmay Singh,Alessandro Sordoni,Marc-Alexandre Côté,Xingdi Yuan*

Main category: cs.SE

TL;DR: 提出了一种通过让软件工程代理在添加功能时无意中破坏测试来生成高质量、多样化bug的新方法，相比传统方法更接近真实开发过程，训练出的模型在SWE-bench基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过故意引入局部扰动生成bug，这会产生分布外效应，不能反映真实的开发过程。需要更高质量的bug数据来训练下一代基于语言模型的软件工程代理。

Method: 指导软件工程代理向代码库添加功能，在此过程中可能无意中破坏测试，从而产生bug。这种方法更接近人类开发者的编辑模式。

Result: 新方法生成的bug训练数据更高效，仅用1200个bug就比其他数据集用3000个bug的效果好2%。训练出的FrogBoss模型在SWE-bench Verified上达到54.6%的pass@1，FrogMini达到45.3%。

Conclusion: 通过模拟真实开发过程生成bug的方法能产生更高质量的训练数据，显著提升软件工程代理的性能，在基准测试中达到最先进水平。

Abstract: High quality bugs are key to training the next generation of language model
based software engineering (SWE) agents. We introduce a novel method for
synthetic generation of difficult and diverse bugs. Our method instructs SWE
Agents to introduce a feature into the codebase whereby they may
unintentionally break tests, resulting in bugs. Prior approaches often induce
an out-of-distribution effect by generating bugs intentionally (e.g. by
introducing local perturbation to existing code), which does not reflect
realistic development processes. We perform qualitative analysis to demonstrate
that our approach for generating bugs more closely reflects the patterns found
in human-authored edits. Through extensive experiments, we demonstrate that our
bugs provide more efficient training data for supervised fine-tuning,
outperforming other bug datasets by 2% with half the training data (1.2k vs. 3k
bugs). We train on our newly generated bugs in addition to existing bug
datasets to get FrogBoss a state-of-the-art 32B parameter model on SWE-bench
Verified with a pass@1 of 54.6% and FrogMini a state-of-the-art 14B model on
SWE-bench Verified with a pass@1 of 45.3% on SWE-bench Verified averaged over
three seeds.

</details>
