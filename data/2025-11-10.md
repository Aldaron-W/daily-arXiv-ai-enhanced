<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 44]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.CV](#cs.CV) [Total: 1]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.IR](#cs.IR) [Total: 3]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.MM](#cs.MM) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Evaluating LLMs' Reasoning Over Ordered Procedural Steps](https://arxiv.org/abs/2511.04688)
*Adrita Anika,Md Messal Monem Miah*

Main category: cs.CL

TL;DR: 本文研究了LLM在程序性序列推理任务上的表现，通过使用食谱数据集评估模型从乱序步骤重建正确序列的能力，发现模型性能随序列长度增加和步骤位移加剧而下降。


<details>
  <summary>Details</summary>
Motivation: 程序性序列推理是LLM的关键能力，其中步骤顺序直接影响结果。本文旨在评估LLM在重建全局有序序列方面的表现，特别是在食谱这种顺序至关重要的领域。

Method: 使用食谱数据集，在零样本和少样本设置下评估多个LLM，并采用从排序和序列对齐中改编的综合评估框架，包括Kendall's Tau、NLCS和NED等指标。

Result: 模型性能随序列长度增加而下降，反映了更复杂程序的额外难度。输入中步骤位移越大（即乱序程度越严重），性能下降越明显。

Conclusion: 当前LLM在程序性推理方面存在局限性，特别是在处理更长和更无序的输入时表现不佳。

Abstract: Reasoning over procedural sequences, where the order of steps directly
impacts outcomes, is a critical capability for large language models (LLMs). In
this work, we study the task of reconstructing globally ordered sequences from
shuffled procedural steps, using a curated dataset of food recipes, a domain
where correct sequencing is essential for task success. We evaluate several
LLMs under zero-shot and few-shot settings and present a comprehensive
evaluation framework that adapts established metrics from ranking and sequence
alignment. These include Kendall's Tau, Normalized Longest Common Subsequence
(NLCS), and Normalized Edit Distance (NED), which capture complementary aspects
of ordering quality. Our analysis shows that model performance declines with
increasing sequence length, reflecting the added complexity of longer
procedures. We also find that greater step displacement in the input,
corresponding to more severe shuffling, leads to further degradation. These
findings highlight the limitations of current LLMs in procedural reasoning,
especially with longer and more disordered inputs.

</details>


### [2] [Adaptive Testing for LLM Evaluation: A Psychometric Alternative to Static Benchmarks](https://arxiv.org/abs/2511.04689)
*Peiyu Li,Xiuxiu Tang,Si Chen,Ying Cheng,Ronald Metoyer,Ting Hua,Nitesh V. Chawla*

Main category: cs.CL

TL;DR: ATLAS是一个基于项目反应理论的自适应测试框架，通过Fisher信息引导的项目选择来评估大语言模型能力，能在保持测量精度的同时减少90%的测试项目。


<details>
  <summary>Details</summary>
Motivation: 传统大语言模型评估需要数千个基准测试项，成本高且效率低，且所有项目被同等对待，忽略了项目质量和信息量的差异。

Method: 使用项目反应理论(IRT)和Fisher信息指导的项目选择方法，动态选择最具信息量的测试项目来估计模型能力。

Result: 在HellaSwag基准上仅用42个项目就达到了全基准(5,608个项目)的评估精度，项目暴露率低于10%，测试重叠率16-27%。IRT排名与传统准确率排名存在显著差异，23-31%的模型排名变化超过10位。

Conclusion: ATLAS框架显著提高了大语言模型评估的效率，揭示了传统静态基准中存在的标注错误问题，并为更精确的模型能力评估提供了新方法。

Abstract: Large language model evaluation requires thousands of benchmark items, making
evaluations expensive and slow. Existing methods compute average accuracy
across fixed item sets, treating all items equally despite varying quality and
informativeness. We present ATLAS an adaptive testing framework using Item
Response Theory (IRT) to estimate model ability through Fisher
information-guided item selection. Our analysis of five major benchmarks
reveals that 3-6% of items exhibit negative discrimination, indicating
annotation errors that corrupt static evaluation. ATLAS achieves 90% item
reduction while maintaining measurement precision: on HellaSwag (5,608 items),
we match full-benchmark estimates using only 42 items with 0.154 MAE. Our
framework maintains item exposure rates below 10% and test overlap at 16-27%,
compared to static benchmarks where every model sees all items (100% exposure).
Among 4,000+ tested models, IRT ranks differ from accuracy ranks: models with
the same accuracy get different IRT scores, and 23-31% of all models shift by
more than 10 rank positions. Code and calibrated item banks are available at
https://github.com/Peiyu-Georgia-Li/ATLAS.git.

</details>


### [3] [SARC: Sentiment-Augmented Deep Role Clustering for Fake News Detection](https://arxiv.org/abs/2511.04692)
*Jingqing Wang,Jiaxing Shang,Rong Xu,Fei Hao,Tianjin Huang,Geyong Min*

Main category: cs.CL

TL;DR: SARC是一个基于情感增强角色聚类的假新闻检测框架，通过区分用户角色来提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法将情感特征作为辅助信号，忽略了相同情感极性可能来自不同角色的用户，限制了捕捉细微模式的能力。

Method: 使用BiGRU和注意力机制生成用户特征，构建可微分深度聚类模块自动分类用户角色，并整合角色聚类和假新闻检测进行联合优化。

Result: 在两个基准数据集RumourEval-19和Weibo-comp上，SARC在所有指标上都优于基线模型。

Conclusion: 通过区分用户角色并整合角色聚类与假新闻检测，SARC框架能有效提升假新闻检测性能。

Abstract: Fake news detection has been a long-standing research focus in social
networks. Recent studies suggest that incorporating sentiment information from
both news content and user comments can enhance detection performance. However,
existing approaches typically treat sentiment features as auxiliary signals,
overlooking role differentiation, that is, the same sentiment polarity may
originate from users with distinct roles, thereby limiting their ability to
capture nuanced patterns for effective detection. To address this issue, we
propose SARC, a Sentiment-Augmented Role Clustering framework which utilizes
sentiment-enhanced deep clustering to identify user roles for improved fake
news detection. The framework first generates user features through joint
comment text representation (with BiGRU and Attention mechanism) and sentiment
encoding. It then constructs a differentiable deep clustering module to
automatically categorize user roles. Finally, unlike existing approaches which
take fake news label as the unique supervision signal, we propose a joint
optimization objective integrating role clustering and fake news detection to
further improve the model performance. Experimental results on two benchmark
datasets, RumourEval-19 and Weibo-comp, demonstrate that SARC achieves superior
performance across all metrics compared to baseline models. The code is
available at: https://github.com/jxshang/SARC.

</details>


### [4] [Reasoning Up the Instruction Ladder for Controllable Language Models](https://arxiv.org/abs/2511.04694)
*Zishuo Zheng,Vidhisha Balachandran,Chan Young Park,Faeze Brahman,Sachin Kumar*

Main category: cs.CL

TL;DR: 该论文提出将指令层次结构解决重构为推理任务，通过构建VerIH数据集和轻量级强化学习训练，使LLM能够优先遵循高级别指令，提升模型的可靠性和可控性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在现实决策中承担高风险角色，需要解决来自多个来源（如开发者、用户、工具）的竞争性指令，建立指令层次结构对LLM的可靠性和可控性至关重要。

Method: 将指令层次结构解决重构为推理任务，构建VerIH数据集（包含对齐和冲突的系统-用户指令），使用轻量级强化学习训练模型进行指令优先级推理。

Result: 微调模型在指令遵循和指令层次基准上取得一致改进，推理能力泛化到训练分布之外的安全关键场景，增强了对越狱和提示注入攻击的鲁棒性。

Conclusion: 通过指令层次推理为可靠LLM提供了实用路径，系统提示的更新能够实现可控和鲁棒的模型行为变化。

Abstract: As large language model (LLM) based systems take on high-stakes roles in
real-world decision-making, they must reconcile competing instructions from
multiple sources (e.g., model developers, users, and tools) within a single
prompt context. Thus, enforcing an instruction hierarchy (IH) in LLMs, where
higher-level directives override lower-priority requests, is critical for the
reliability and controllability of LLMs. In this work, we reframe instruction
hierarchy resolution as a reasoning task. Specifically, the model must first
"think" about the relationship between a given user prompt and higher-priority
(system) instructions before generating a response. To enable this capability
via training, we construct VerIH, an instruction hierarchy dataset of
constraint-following tasks with verifiable answers. This dataset comprises both
aligned and conflicting system-user instructions. We show that lightweight
reinforcement learning with VerIH effectively transfers general reasoning
capabilities of models to instruction prioritization. Our finetuned models
achieve consistent improvements on instruction following and instruction
hierarchy benchmarks. This reasoning ability also generalizes to
safety-critical settings beyond the training distribution. By treating safety
issues as resolving conflicts between adversarial user inputs and predefined
higher-priority policies, our trained model enhances robustness against
jailbreak and prompt injection attacks. These results demonstrate that
reasoning over instruction hierarchies provides a practical path to reliable
LLMs, where updates to system prompts yield controllable and robust changes in
model behavior.

</details>


### [5] [EncouRAGe: Evaluating RAG Local, Fast, and Reliable](https://arxiv.org/abs/2511.04696)
*Jan Strich,Adeline Scharfenberg,Chris Biemann,Martin Semmann*

Main category: cs.CL

TL;DR: EncouRAGe是一个用于简化基于大语言模型和嵌入模型的检索增强生成系统开发和评估的Python框架，包含五个模块化组件，强调科学可重现性、多样化评估指标和本地部署。


<details>
  <summary>Details</summary>
Motivation: 为了简化RAG系统的开发和评估流程，提供一个模块化、可扩展的框架，支持灵活的实验设计和科学可重现性。

Method: 开发了包含Type Manifest、RAG Factory、Inference、Vector Store和Metrics五个模块化组件的Python框架，支持多种评估指标和本地部署。

Result: 在包含25k问答对和51k文档的多个基准数据集上评估，发现RAG性能仍低于Oracle Context，而Hybrid BM25在所有四个数据集上表现最佳；重排序仅带来边际性能提升但增加响应延迟。

Conclusion: EncouRAGe框架为RAG系统开发提供了有效的工具，实验结果表明当前RAG技术仍有改进空间，Hybrid BM25是有效的检索方法，重排序的实际价值需要进一步评估。

Abstract: We introduce EncouRAGe, a comprehensive Python framework designed to
streamline the development and evaluation of Retrieval-Augmented Generation
(RAG) systems using Large Language Models (LLMs) and Embedding Models.
EncouRAGe comprises five modular and extensible components: Type Manifest, RAG
Factory, Inference, Vector Store, and Metrics, facilitating flexible
experimentation and extensible development. The framework emphasizes scientific
reproducibility, diverse evaluation metrics, and local deployment, enabling
researchers to efficiently assess datasets within RAG workflows. This paper
presents implementation details and an extensive evaluation across multiple
benchmark datasets, including 25k QA pairs and over 51k documents. Our results
show that RAG still underperforms compared to the Oracle Context, while Hybrid
BM25 consistently achieves the best results across all four datasets. We
further examine the effects of reranking, observing only marginal performance
improvements accompanied by higher response latency.

</details>


### [6] [multiMentalRoBERTa: A Fine-tuned Multiclass Classifier for Mental Health Disorder](https://arxiv.org/abs/2511.04698)
*K M Sajjadul Islam,John Fields,Praveen Madiraju*

Main category: cs.CL

TL;DR: 多MentalRoBERTa是一个基于RoBERTa微调的模型，用于从社交媒体文本中检测六种常见心理健康状况（压力、焦虑、抑郁、创伤后应激障碍、自杀意念和中性话语），在性能上优于传统方法和现有模型。


<details>
  <summary>Details</summary>
Motivation: 从社交媒体文本中早期检测心理健康障碍对于及时支持、风险评估和转诊至适当资源至关重要。

Method: 使用多个精选数据集进行数据探索分析类别重叠，比较传统机器学习方法、领域特定transformer和基于提示的大语言模型，应用可解释性方法（Layer Integrated Gradients和KeyBERT）识别驱动分类的词汇线索。

Result: multiMentalRoBERTa在六类设置中macro F1-score达到0.839，在五类设置（排除压力）中达到0.870，优于微调的MentalBERT和基线分类器。

Conclusion: 微调的transformer在敏感情境中提供了可靠且可解释的检测方案，同时强调了公平性、偏见缓解和人在回路安全协议的重要性，multiMentalRoBERTa被呈现为一个轻量级、稳健且可部署的解决方案。

Abstract: The early detection of mental health disorders from social media text is
critical for enabling timely support, risk assessment, and referral to
appropriate resources. This work introduces multiMentalRoBERTa, a fine-tuned
RoBERTa model designed for multiclass classification of common mental health
conditions, including stress, anxiety, depression, post-traumatic stress
disorder (PTSD), suicidal ideation, and neutral discourse. Drawing on multiple
curated datasets, data exploration is conducted to analyze class overlaps,
revealing strong correlations between depression and suicidal ideation as well
as anxiety and PTSD, while stress emerges as a broad, overlapping category.
Comparative experiments with traditional machine learning methods,
domain-specific transformers, and prompting-based large language models
demonstrate that multiMentalRoBERTa achieves superior performance, with macro
F1-scores of 0.839 in the six-class setup and 0.870 in the five-class setup
(excluding stress), outperforming both fine-tuned MentalBERT and baseline
classifiers. Beyond predictive accuracy, explainability methods, including
Layer Integrated Gradients and KeyBERT, are applied to identify lexical cues
that drive classification, with a particular focus on distinguishing depression
from suicidal ideation. The findings emphasize the effectiveness of fine-tuned
transformers for reliable and interpretable detection in sensitive contexts,
while also underscoring the importance of fairness, bias mitigation, and
human-in-the-loop safety protocols. Overall, multiMentalRoBERTa is presented as
a lightweight, robust, and deployable solution for enhancing support in mental
health platforms.

</details>


### [7] [Cross-Lingual SynthDocs: A Large-Scale Synthetic Corpus for Any to Arabic OCR and Document Understanding](https://arxiv.org/abs/2511.04699)
*Haneen Al-Homoud,Asma Ibrahim,Murtadha Al-Jubran,Fahad Al-Otaibi,Yazeed Al-Harbi,Daulet Toibazar,Kesen Wang,Pedro J. Moreno*

Main category: cs.CL

TL;DR: Cross-Lingual SynthDocs是一个大规模合成语料库，包含250多万个样本，用于解决阿拉伯语OCR和文档理解资源稀缺的问题，显著提升了多模态文档分析性能。


<details>
  <summary>Details</summary>
Motivation: 解决阿拉伯语在光学字符识别和文档理解领域资源稀缺的问题，为多语言文档分析提供可扩展的视觉真实资源。

Method: 利用真实扫描背景、双语布局和变音符号感知字体构建合成管道，包含文本、表格和图表等多种渲染样式，生成大规模阿拉伯语文档数据集。

Result: 在Qwen-2.5-VL模型上微调后，在多个阿拉伯语基准测试中，词错误率和字符错误率持续改善，树编辑距离相似度和图表提取分数在其他模态上也得到提升。

Conclusion: SynthDocs为推进多语言文档分析研究提供了一个可扩展且视觉真实的资源，有效解决了阿拉伯语文档处理资源不足的问题。

Abstract: Cross-Lingual SynthDocs is a large-scale synthetic corpus designed to address
the scarcity of Arabic resources for Optical Character Recognition (OCR) and
Document Understanding (DU). The dataset comprises over 2.5 million of samples,
including 1.5 million textual data, 270K fully annotated tables, and hundred
thousands of real data based charts. Our pipeline leverages authentic scanned
backgrounds, bilingual layouts, and diacritic aware fonts to capture the
typographic and structural complexity of Arabic documents. In addition to text,
the corpus includes variety of rendered styles for charts and tables.
Finetuning Qwen-2.5-VL on SynthDocs yields consistent improvements in Word
Error Rate (WER) and Character Error Rate (CER) in terms of OCR across multiple
public Arabic benchmarks, Tree-Edit Distance Similarity (TEDS) and Chart
Extraction Score (CharTeX) improved as well in other modalities. SynthDocs
provides a scalable, visually realistic resource for advancing research in
multilingual document analysis.

</details>


### [8] [Separate the Wheat from the Chaff: Winnowing Down Divergent Views in Retrieval Augmented Generation](https://arxiv.org/abs/2511.04700)
*Song Wang,Zihan Chen,Peng Wang,Zhepei Wei,Zhen Tan,Yu Meng,Cong Shen,Jundong Li*

Main category: cs.CL

TL;DR: WinnowRAG是一个新颖的检索增强生成框架，通过两阶段方法（聚类和筛选）来过滤噪声文档，同时保留有价值内容，提高RAG系统的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法在增加检索文档数量时会引入大量噪声文档，这些无关或误导性文档会降低生成响应的准确性。

Method: 两阶段方法：第一阶段进行查询感知聚类，将相似文档分组并由LLM代理生成答案；第二阶段通过批评LLM评估多个代理输出，迭代分离有用文档与噪声文档，采用策略性合并技术保留有用文档。

Result: 在多个真实数据集上的广泛实验表明，WinnowRAG优于最先进的基线方法。

Conclusion: WinnowRAG是一个模型无关的框架，无需模型微调即可有效处理多文档RAG中的噪声问题，在各种任务中具有良好适应性。

Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by
integrating external knowledge sources to address their limitations in
accessing up-to-date or specialized information. A natural strategy to increase
the likelihood of retrieving relevant information is to expand the number of
retrieved documents. However, involving more documents could introduce
significant noise, as many documents may be irrelevant or misleading, thereby
reducing the overall accuracy of the generated responses. To overcome the
challenge associated with handling a larger number of documents, we propose
WinnowRAG, a novel RAG framework designed to systematically filter out noisy
documents while preserving valuable content -- a process we refer to as
winnowing. WinnowRAG operates in two stages: In Stage I, we perform query-aware
clustering to group similar documents and form distinct topic clusters. Each
cluster is assigned to an LLM agent for generating a unique answer. In Stage
II, we perform winnowing, wherein a critic LLM evaluates the outputs of
multiple agents and iteratively separates useful documents from noisy ones. To
retain useful documents when discarding agents, we propose two strategic
merging techniques to ensure that only relevant knowledge is used for
generating the final response. Crucially, WinnowRAG is model-agnostic and does
not require any model fine-tuning, making it easily adaptable to various tasks.
Extensive experiments on various realistic datasets demonstrate the
effectiveness of WinnowRAG over state-of-the-art baselines.

</details>


### [9] [Measuring what Matters: Construct Validity in Large Language Model Benchmarks](https://arxiv.org/abs/2511.04703)
*Andrew M. Bean,Ryan Othniel Kearns,Angelika Romanou,Franziska Sofia Hafner,Harry Mayne,Jan Batzner,Negar Foroutan,Chris Schmitz,Karolina Korgul,Hunar Batra,Oishi Deb,Emma Beharry,Cornelius Emde,Thomas Foster,Anna Gausen,María Grandury,Simeng Han,Valentin Hofmann,Lujain Ibrahim,Hazel Kim,Hannah Rose Kirk,Fangru Lin,Gabrielle Kaili-May Liu,Lennart Luettgau,Jabez Magomere,Jonathan Rystrøm,Anna Sotnikova,Yushi Yang,Yilun Zhao,Adel Bibi,Antoine Bosselut,Ronald Clark,Arman Cohan,Jakob Foerster,Yarin Gal,Scott A. Hale,Inioluwa Deborah Raji,Christopher Summerfield,Philip H. S. Torr,Cozmin Ududec,Luc Rocher,Adam Mahdi*

Main category: cs.CL

TL;DR: 对445个LLM基准测试进行系统性回顾，发现现有评估方法在结构效度上存在问题，提出了8项改进建议


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型的能力并识别安全性和鲁棒性问题至关重要，但现有基准测试在测量抽象复杂现象时缺乏结构效度

Method: 组织29名专家评审团队，对自然语言处理和机器学习顶级会议中的445个LLM基准测试进行系统性回顾

Result: 发现现有基准测试在测量现象、任务和评分指标方面存在模式问题，削弱了评估结果的有效性

Conclusion: 提出了8项关键建议和详细可操作的指导，帮助研究人员和从业者开发更有效的LLM基准测试

Abstract: Evaluating large language models (LLMs) is crucial for both assessing their
capabilities and identifying safety or robustness issues prior to deployment.
Reliably measuring abstract and complex phenomena such as 'safety' and
'robustness' requires strong construct validity, that is, having measures that
represent what matters to the phenomenon. With a team of 29 expert reviewers,
we conduct a systematic review of 445 LLM benchmarks from leading conferences
in natural language processing and machine learning. Across the reviewed
articles, we find patterns related to the measured phenomena, tasks, and
scoring metrics which undermine the validity of the resulting claims. To
address these shortcomings, we provide eight key recommendations and detailed
actionable guidance to researchers and practitioners in developing LLM
benchmarks.

</details>


### [10] [POLIS-Bench: Towards Multi-Dimensional Evaluation of LLMs for Bilingual Policy Tasks in Governmental Scenarios](https://arxiv.org/abs/2511.04705)
*Tingyue Yang,Junchi Yao,Yuhui Guo,Chang Liu*

Main category: cs.CL

TL;DR: POLIS-Bench是首个针对政府双语政策场景的LLM评估套件，包含更新的双语语料库、场景驱动的任务设计和双指标评估框架。评估显示推理模型表现最佳，并成功微调出轻量级开源模型POLIS系列，在多任务上达到或超越商业模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有基准无法充分评估LLM在政府政策场景中的表现，需要专门针对双语政策理解、合规性判断等实际政府应用场景的评估框架。

Method: 构建更新的双语政策语料库，设计三个场景驱动任务（条款检索与解释、解决方案生成、合规性判断），建立结合语义相似度和准确率的双指标评估框架。

Result: 评估10多个SOTA LLM显示推理模型具有最佳跨任务稳定性和准确性，合规性任务难度最高。微调的POLIS系列模型在多政策子任务上达到或超越商业基准，成本显著降低。

Conclusion: POLIS-Bench为政府场景的LLM评估提供了系统框架，POLIS系列模型证明了轻量级开源模型在政策任务上的可行性，为实际政府部署提供了经济高效的解决方案。

Abstract: We introduce POLIS-Bench, the first rigorous, systematic evaluation suite
designed for LLMs operating in governmental bilingual policy scenarios.
Compared to existing benchmarks, POLIS-Bench introduces three major
advancements. (i) Up-to-date Bilingual Corpus: We construct an extensive,
up-to-date policy corpus that significantly scales the effective assessment
sample size, ensuring relevance to current governance practice. (ii)
Scenario-Grounded Task Design: We distill three specialized, scenario-grounded
tasks -- Clause Retrieval & Interpretation, Solution Generation, and the
Compliance Judgmen--to comprehensively probe model understanding and
application. (iii) Dual-Metric Evaluation Framework: We establish a novel
dual-metric evaluation framework combining semantic similarity with accuracy
rate to precisely measure both content alignment and task requirement
adherence. A large-scale evaluation of over 10 state-of-the-art LLMs on
POLIS-Bench reveals a clear performance hierarchy where reasoning models
maintain superior cross-task stability and accuracy, highlighting the
difficulty of compliance tasks. Furthermore, leveraging our benchmark, we
successfully fine-tune a lightweight open-source model. The resulting POLIS
series models achieves parity with, or surpasses, strong proprietary baselines
on multiple policy subtasks at a significantly reduced cost, providing a
cost-effective and compliant path for robust real-world governmental
deployment.

</details>


### [11] [GEMMA-SQL: A Novel Text-to-SQL Model Based on Large Language Models](https://arxiv.org/abs/2511.04710)
*Hari Mohan Pandey,Anshul Gupta,Subham Sarkar,Minakshi Tomer,Schneider Johannes,Yan Gong*

Main category: cs.CL

TL;DR: GEMMA-SQL是一个基于Gemma 2B架构的轻量级文本到SQL模型，通过资源高效的微调和多提示策略，在SPIDER基准测试中表现出色，超越了多个最先进基线。


<details>
  <summary>Details</summary>
Motivation: 开发一个轻量级、高效的文本到SQL系统，让用户无需专业知识就能与结构化数据库交互，并能在低成本硬件上部署。

Method: 基于开源Gemma 2B架构，采用资源高效的迭代微调方法，结合多种提示策略（包括少样本学习）来增强SQL查询生成准确性。

Result: GEMMA-SQL Instruct版本在SPIDER基准测试中达到66.8%的Test-Suite准确率和63.3%的Exact Set Match准确率，超越了IRNet、RYANSQL和CodeXDavinci等基线模型。

Conclusion: 有效的提示设计和针对性指令调优可以显著提升性能，同时保持高可扩展性和适应性，使GEMMA-SQL成为强大且易用的开源文本到SQL替代方案。

Abstract: Text-to-SQL systems enable users to interact with structured databases using
natural language, eliminating the need for specialized programming knowledge.
In this work, we introduce GEMMA-SQL, a lightweight and efficient text-to-SQL
model built upon the open-source Gemma 2B architecture. Unlike many large
language models (LLMs), GEMMA-SQL is fine-tuned in a resource-efficient,
iterative manner and can be deployed on low-cost hardware. Leveraging the
SPIDER benchmark for training and evaluation, GEMMA-SQL combines multiple
prompting strategies, including few-shot learning, to enhance SQL query
generation accuracy. The instruction-tuned variant, GEMMA-SQL Instruct,
achieves 66.8% Test-Suite accuracy and 63.3% Exact Set Match accuracy,
outperforming several state-of-the-art baselines such as IRNet, RYANSQL, and
CodeXDavinci. The proposed approach demonstrates that effective prompt design
and targeted instruction tuning can significantly boost performance while
maintaining high scalability and adaptability. These results position GEMMA-SQL
as a practical, open-source alternative for robust and accessible text-to-SQL
systems.

</details>


### [12] [First is Not Really Better Than Last: Evaluating Layer Choice and Aggregation Strategies in Language Model Data Influence Estimation](https://arxiv.org/abs/2511.04715)
*Dmytro Vitel,Anshuman Chhabra*

Main category: cs.CL

TL;DR: 本文挑战了先前关于LLM影响力估计的认知，提出中间注意力层比嵌入层更适合计算训练样本影响力，并提出了新的评估指标NDR。


<details>
  <summary>Details</summary>
Motivation: 当前的影响力计算方法由于模型规模庞大，通常只能计算部分层的影响。先前研究认为嵌入层最适合计算影响力，但作者发现这种基于抵消效应的假设不可靠。

Method: 提出了理论和实证证据证明抵消效应的不可靠性，发现中间注意力层是更好的影响力估计器。提出了排名和投票等替代标准平均的方法来聚合层间影响力分数，并提出了新的评估指标NDR。

Result: 通过在不同类型和规模的LLM上进行广泛实验，证实了第一层并不一定比最后一层更适合LLM影响力估计，这与该领域先前的认知形成对比。

Conclusion: 中间注意力层比嵌入层更适合用于LLM训练样本影响力估计，新的聚合方法和NDR指标能显著提升影响力估计性能。

Abstract: Identifying how training samples influence/impact Large Language Model (LLM)
decision-making is essential for effectively interpreting model decisions and
auditing large-scale datasets. Current training sample influence estimation
methods (also known as influence functions) undertake this goal by utilizing
information flow through the model via its first-order and higher-order
gradient terms. However, owing to the large model sizes of today consisting of
billions of parameters, these influence computations are often restricted to
some subset of model layers to ensure computational feasibility. Prior seminal
work by Yeh et al. (2022) in assessing which layers are best suited for
computing language data influence concluded that the first (embedding) layers
are the most informative for this purpose, using a hypothesis based on
influence scores canceling out (i.e., the cancellation effect). In this work,
we propose theoretical and empirical evidence demonstrating how the
cancellation effect is unreliable, and that middle attention layers are better
estimators for influence. Furthermore, we address the broader challenge of
aggregating influence scores across layers, and showcase how alternatives to
standard averaging (such as ranking and vote-based methods) can lead to
significantly improved performance. Finally, we propose better methods for
evaluating influence score efficacy in LLMs without undertaking model
retraining, and propose a new metric known as the Noise Detection Rate (NDR)
that exhibits strong predictive capability compared to the cancellation effect.
Through extensive experiments across LLMs of varying types and scales, we
concretely determine that the first (layers) are not necessarily better than
the last (layers) for LLM influence estimation, contrasting with prior
knowledge in the field.

</details>


### [13] [Learning to reason about rare diseases through retrieval-augmented agents](https://arxiv.org/abs/2511.04720)
*Ha Young Kim,Jun Li,Ana Beatriz Solana,Carolin M. Pirkl,Benedikt Wiestler,Julia A. Schnabel,Cosmin I. Bercea*

Main category: cs.CL

TL;DR: RADAR是一个基于检索增强诊断推理的智能体系统，用于脑MRI罕见疾病检测，通过检索外部医学知识来指导诊断决策，无需额外训练即可提升罕见病理识别能力。


<details>
  <summary>Details</summary>
Motivation: 罕见疾病在医学影像中数据稀缺，导致AI模型表现不佳。受放射科医生查阅病例报告和文献的启发，需要开发能够利用外部医学知识来辅助罕见疾病诊断的系统。

Method: 使用AI智能体访问外部医学知识，通过句子转换器嵌入病例报告和文献，并用FAISS索引以实现高效相似性搜索。系统作为模型无关的推理模块，可与各种大语言模型无缝集成。

Result: 在包含280种不同罕见疾病的NOVA数据集上，RADAR实现了高达10.2%的性能提升，特别是对开源模型如DeepSeek效果最显著。检索到的示例提供了可解释的、基于文献的解释。

Conclusion: 检索增强推理是医学影像中低流行度条件的强大范式，不仅能提高准确性，还能提供可解释的诊断依据。

Abstract: Rare diseases represent the long tail of medical imaging, where AI models
often fail due to the scarcity of representative training data. In clinical
workflows, radiologists frequently consult case reports and literature when
confronted with unfamiliar findings. Following this line of reasoning, we
introduce RADAR, Retrieval Augmented Diagnostic Reasoning Agents, an agentic
system for rare disease detection in brain MRI. Our approach uses AI agents
with access to external medical knowledge by embedding both case reports and
literature using sentence transformers and indexing them with FAISS to enable
efficient similarity search. The agent retrieves clinically relevant evidence
to guide diagnostic decision making on unseen diseases, without the need of
additional training. Designed as a model-agnostic reasoning module, RADAR can
be seamlessly integrated with diverse large language models, consistently
improving their rare pathology recognition and interpretability. On the NOVA
dataset comprising 280 distinct rare diseases, RADAR achieves up to a 10.2%
performance gain, with the strongest improvements observed for open source
models such as DeepSeek. Beyond accuracy, the retrieved examples provide
interpretable, literature grounded explanations, highlighting
retrieval-augmented reasoning as a powerful paradigm for low-prevalence
conditions in medical imaging.

</details>


### [14] [Surprisal reveals diversity gaps in image captioning and different scorers change the story](https://arxiv.org/abs/2511.04754)
*Nikolai Ilinykh,Simon Dobnik*

Main category: cs.CL

TL;DR: 提出基于惊奇值方差的图像描述多样性度量方法，比较了人类和模型在MSCOCO数据集上的表现，发现使用不同评分器会完全逆转结论


<details>
  <summary>Details</summary>
Motivation: 量化图像描述中的语言多样性，比较人类与先进视觉语言模型在描述多样性方面的差异

Method: 使用惊奇值方差（token级负对数概率的分布）作为多样性度量，在MSCOCO测试集上比较5个最先进的视觉语言LLM与人类描述，使用贪心和核采样解码

Result: 使用描述训练的n-gram语言模型评分时，人类的惊奇值方差约为模型的两倍；但使用通用语言模型重新评分时，模式完全逆转

Conclusion: 基于惊奇值的多样性度量有效，但单一评分器可能导致结论完全颠倒，因此稳健的多样性评估必须报告多个评分器下的惊奇值

Abstract: We quantify linguistic diversity in image captioning with surprisal variance
- the spread of token-level negative log-probabilities within a caption set. On
the MSCOCO test set, we compare five state-of-the-art vision-and-language LLMs,
decoded with greedy and nucleus sampling, to human captions. Measured with a
caption-trained n-gram LM, humans display roughly twice the surprisal variance
of models, but rescoring the same captions with a general-language model
reverses the pattern. Our analysis introduces the surprisal-based diversity
metric for image captioning. We show that relying on a single scorer can
completely invert conclusions, thus, robust diversity evaluation must report
surprisal under several scorers.

</details>


### [15] [Explore Data Left Behind in Reinforcement Learning for Reasoning Language Models](https://arxiv.org/abs/2511.04800)
*Chenxi Liu,Junjie Liang,Yuqi Jia,Bochuan Cao,Yang Bai,Heng Huang,Xun Chen*

Main category: cs.CL

TL;DR: 提出ERPO框架解决RLVR训练中残差提示问题，通过探索性采样温度调整重新激活零方差奖励提示的训练信号


<details>
  <summary>Details</summary>
Motivation: 随着模型训练时间增长和规模扩大，更多训练提示变为残差提示（零方差奖励），这些提示无法提供训练信号，降低了训练多样性和效果

Method: ERPO框架为每个提示维护历史跟踪器，自适应增加残差提示的采样温度，鼓励生成更多样化的推理轨迹，引入错误响应以重新激活训练信号

Result: 在Qwen2.5系列上的实证结果显示，ERPO在多个数学推理基准测试中持续超越强基线方法

Conclusion: ERPO通过有效利用残差提示，解决了RLVR训练中的多样性衰减问题，显著提升了模型推理能力

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an
effective approach for improving the reasoning abilities of large language
models (LLMs). The Group Relative Policy Optimization (GRPO) family has
demonstrated strong performance in training LLMs with RLVR. However, as models
train longer and scale larger, more training prompts become residual prompts,
those with zero variance rewards that provide no training signal. Consequently,
fewer prompts contribute to training, reducing diversity and hindering
effectiveness. To fully exploit these residual prompts, we propose the Explore
Residual Prompts in Policy Optimization (ERPO) framework, which encourages
exploration on residual prompts and reactivates their training signals. ERPO
maintains a history tracker for each prompt and adaptively increases the
sampling temperature for residual prompts that previously produced all correct
responses. This encourages the model to generate more diverse reasoning traces,
introducing incorrect responses that revive training signals. Empirical results
on the Qwen2.5 series demonstrate that ERPO consistently surpasses strong
baselines across multiple mathematical reasoning benchmarks.

</details>


### [16] [Trained on Tokens, Calibrated on Concepts: The Emergence of Semantic Calibration in LLMs](https://arxiv.org/abs/2511.04869)
*Preetum Nakkiran,Arwen Bradley,Adam Goliński,Eugene Ndiaye,Michael Kirchhof,Sinead Williamson*

Main category: cs.CL

TL;DR: 研究发现基础LLMs在语义层面具有很好的校准能力，能够有意义地评估开放域问答任务中的置信度，尽管它们并未被明确训练这样做。理论分析揭示了语义校准作为下一个token预测副产品的机制，并发现RL指令微调和思维链推理会破坏这种校准。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型通常缺乏对其输出的有意义的置信度估计。虽然基础LLMs已知具有下一个token的校准能力，但尚不清楚它们是否能在超越token层面的实际意义层面评估置信度。

Method: 提出基于采样的语义校准概念，建立理论机制解释语义校准如何作为下一个token预测的副产品出现，利用校准与局部损失最优性之间的最新联系。定义了参数化的"B-校准"概念，并通过实验验证理论预测。

Result: (1) 基础LLMs在问答任务中具有语义校准能力；(2) RL指令微调会系统性破坏这种校准；(3) 思维链推理会破坏校准。这是第一个对LLMs中语义校准何时以及为何出现的原理性解释。

Conclusion: 基础LLMs具有显著的语义校准能力，这种能力是下一个token预测的自然副产品。然而，RL指令微调和思维链推理等常用技术会破坏这种内在的校准特性。

Abstract: Large Language Models (LLMs) often lack meaningful confidence estimates for
their outputs. While base LLMs are known to exhibit next-token calibration, it
remains unclear whether they can assess confidence in the actual meaning of
their responses beyond the token level. We find that, when using a certain
sampling-based notion of semantic calibration, base LLMs are remarkably
well-calibrated: they can meaningfully assess confidence in open-domain
question-answering tasks, despite not being explicitly trained to do so. Our
main theoretical contribution establishes a mechanism for why semantic
calibration emerges as a byproduct of next-token prediction, leveraging a
recent connection between calibration and local loss optimality. The theory
relies on a general definition of "B-calibration," which is a notion of
calibration parameterized by a choice of equivalence classes (semantic or
otherwise). This theoretical mechanism leads to a testable prediction: base
LLMs will be semantically calibrated when they can easily predict their own
distribution over semantic answer classes before generating a response. We
state three implications of this prediction, which we validate through
experiments: (1) Base LLMs are semantically calibrated across
question-answering tasks, (2) RL instruction-tuning systematically breaks this
calibration, and (3) chain-of-thought reasoning breaks calibration. To our
knowledge, our work provides the first principled explanation of when and why
semantic calibration emerges in LLMs.

</details>


### [17] [Minimal and Mechanistic Conditions for Behavioral Self-Awareness in LLMs](https://arxiv.org/abs/2511.04875)
*Matthew Bozoukov,Matthew Nguyen,Shubkarman Singh,Bart Bussmann,Patrick Leask*

Main category: cs.CL

TL;DR: LLMs可以通过单秩LoRA适配器可靠地诱导行为自我意识，这种能力表现为激活空间中的线性特征，且具有任务特异性。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs行为自我意识出现的最小条件和机制过程，因为这种能力可能让模型在评估时更好地隐藏真实能力，带来安全隐患。

Method: 在指令调优的LLMs上使用低秩适配器(LoRA)进行受控微调实验，特别使用单秩-1 LoRA适配器。

Result: 发现自我意识可以通过单秩LoRA可靠诱导；学习到的行为主要由激活空间中的单个导向向量捕获；自我意识具有非普遍性和领域局部化特性。

Conclusion: 行为自我意识表现为领域特定的线性特征，可以轻松诱导和调节。

Abstract: Recent studies have revealed that LLMs can exhibit behavioral self-awareness:
the ability to accurately describe or predict their own learned behaviors
without explicit supervision. This capability raises safety concerns as it may,
for example, allow models to better conceal their true abilities during
evaluation. We attempt to characterize the minimal conditions under which such
self-awareness emerges, and the mechanistic processes through which it
manifests. Through controlled finetuning experiments on instruction-tuned LLMs
with low-rank adapters (LoRA), we find: (1) that self-awareness can be reliably
induced using a single rank-1 LoRA adapter; (2) that the learned self-aware
behavior can be largely captured by a single steering vector in activation
space, recovering nearly all of the fine-tune's behavioral effect; and (3) that
self-awareness is non-universal and domain-localized, with independent
representations across tasks. Together, these findings suggest that behavioral
self-awareness emerges as a domain-specific, linear feature that can be easily
induced and modulated.

</details>


### [18] [SDS KoPub VDR: A Benchmark Dataset for Visual Document Retrieval in Korean Public Documents](https://arxiv.org/abs/2511.04910)
*Jaehoon Lee,Sohyun Kim,Wanggeun Park,Geon Lee,Seungkyung Kim,Minyoung Lee*

Main category: cs.CL

TL;DR: SDS KoPub VDR是首个大规模公开的韩语公文检索基准，包含361个真实文档（40,781页）和600个经过人工验证的查询-页面-答案三元组，支持文本检索和多模态检索任务评估。


<details>
  <summary>Details</summary>
Motivation: 现有视觉文档检索基准主要关注英语，忽视了非英语语言和官方出版物的结构复杂性，需要填补这一关键空白。

Method: 基于361个真实韩语公文构建语料库，使用多模态模型生成查询，并通过严格的人工验证流程确保准确性。查询涵盖六个主要公共领域，按推理模态分类。

Result: 评估显示在需要跨模态推理的多模态场景中，即使是最先进的模型也存在显著的性能差距。

Conclusion: SDS KoPub VDR为复杂真实世界文档智能中的多模态AI发展提供了基础资源和明确路线图。

Abstract: Existing benchmarks for visual document retrieval (VDR) largely overlook
non-English languages and the structural complexity of official publications.
To address this critical gap, we introduce SDS KoPub VDR, the first
large-scale, publicly available benchmark for retrieving and understanding
Korean public documents. The benchmark is built upon a corpus of 361 real-world
documents (40,781 pages), including 256 files under the KOGL Type 1 license and
105 from official legal portals, capturing complex visual elements like tables,
charts, and multi-column layouts. To establish a challenging and reliable
evaluation set, we constructed 600 query-page-answer triples. These were
initially generated using multimodal models (e.g., GPT-4o) and subsequently
underwent a rigorous human verification and refinement process to ensure
factual accuracy and contextual relevance. The queries span six major public
domains and are systematically categorized by the reasoning modality required:
text-based, visual-based (e.g., chart interpretation), and cross-modal. We
evaluate SDS KoPub VDR on two complementary tasks that reflect distinct
retrieval paradigms: (1) text-only retrieval, which measures a model's ability
to locate relevant document pages based solely on textual signals, and (2)
multimodal retrieval, which assesses retrieval performance when visual features
(e.g., tables, charts, and layouts) are jointly leveraged alongside text. This
dual-task evaluation reveals substantial performance gaps, particularly in
multimodal scenarios requiring cross-modal reasoning, even for state-of-the-art
models. As a foundational resource, SDS KoPub VDR not only enables rigorous and
fine-grained evaluation across textual and multimodal retrieval tasks but also
provides a clear roadmap for advancing multimodal AI in complex, real-world
document intelligence.

</details>


### [19] [BudgetMem: Learning Selective Memory Policies for Cost-Efficient Long-Context Processing in Language Models](https://arxiv.org/abs/2511.04919)
*Chandra Vamsi Krishna Alla,Harish Naidu Gaddam,Manohar Kommi*

Main category: cs.CL

TL;DR: BudgetMem是一种内存增强架构，通过选择性记忆策略和基于特征的显著性评分，在严格预算约束下决定存储哪些信息，相比基线RAG系统节省72.4%内存且性能仅下降1.0%。


<details>
  <summary>Details</summary>
Motivation: LLM在处理长上下文时面临显著的计算和内存限制，尽管对需要推理长文档、多会话对话和书籍长度文本的应用需求不断增长。现有方法扩展到100K-1M标记会导致资源受限部署的过高成本。

Method: 结合选择性记忆策略与基于特征的显著性评分（实体密度、TF-IDF、话语标记、位置偏差），在严格预算约束下决定存储哪些信息。使用学习门控机制与BM25稀疏检索进行高效信息访问。

Result: 在700个问答对上的实验显示，BudgetMem在长文档上表现优异：相比基线RAG仅下降1.0% F1分数，同时节省72.4%内存。预算敏感性分析、基线比较和文档长度分析验证了方法的有效性。

Conclusion: BudgetMem为在适度硬件上部署能力强的长上下文系统提供了实用途径，使先进语言理解能力民主化。

Abstract: Large Language Models (LLMs) face significant computational and memory
constraints when processing long contexts, despite growing demand for
applications requiring reasoning over extensive documents, multi-session
dialogues, and book length texts. While recent advances have extended context
windows to 100K-1M tokens, such approaches incur prohibitive costs for resource
constrained deployments. We propose BudgetMem, a novel memory augmented
architecture that learns what to remember rather than remembering everything.
Our system combines selective memory policies with feature based salience
scoring (entity density, TF-IDF, discourse markers, position bias) to decide
which information merits storage under strict budget constraints. Unlike
existing retrieval augmented generation (RAG) systems that store all chunks,
BudgetMem employs learned gating mechanisms coupled with BM25 sparse retrieval
for efficient information access. Through comprehensive experiments on 700
question answer pairs across short (237 tokens) and long (5K-10K tokens)
documents with Llama-3.2-3B-Instruct, we demonstrate that BudgetMem achieves
remarkable results on long documents: only 1.0% F1 score degradation while
saving 72.4% memory compared to baseline RAG. We validate our approach through
budget sensitivity analysis (testing 7 budget ratios), naive baseline
comparisons, and document length analysis, showing that BudgetMem's benefits
increase with document length. Our work provides a practical pathway for
deploying capable long context systems on modest hardware, democratizing access
to advanced language understanding capabilities.

</details>


### [20] [AgentExpt: Automating AI Experiment Design with LLM-based Resource Retrieval Agent](https://arxiv.org/abs/2511.04921)
*Yu Li,Lehui Li,Qingmin Liao,Fengli Xu,Yong Li*

Main category: cs.CL

TL;DR: 提出了一个基于集体感知的基准和数据集推荐框架，通过自动化数据收集、增强检索器和推理重排器，显著提升了实验设计的自动化效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在数据覆盖范围有限和过度依赖内容相似性的问题，导致推荐偏向表面相似性而忽视实验适用性。

Method: 1) 自动化数据收集管道链接论文与使用的基准和数据集；2) 集体感知增强检索器，结合自描述和引用上下文；3) 推理增强重排器，构建显式推理链并生成可解释的推荐理由。

Result: 构建的数据集覆盖了过去五年顶级AI会议中85%使用的数据集和基准，在Recall@20和HitRate@5指标上分别比最强基线提升了5.85%和8.30%。

Conclusion: 该方法推进了实验设计的可靠、可解释自动化，解决了现有推荐系统的局限性。

Abstract: Large language model agents are becoming increasingly capable at web-centric
tasks such as information retrieval, complex reasoning. These emerging
capabilities have given rise to surge research interests in developing LLM
agent for facilitating scientific quest. One key application in AI research is
to automate experiment design through agentic dataset and baseline retrieval.
However, prior efforts suffer from limited data coverage, as recommendation
datasets primarily harvest candidates from public portals and omit many
datasets actually used in published papers, and from an overreliance on content
similarity that biases model toward superficial similarity and overlooks
experimental suitability. Harnessing collective perception embedded in the
baseline and dataset citation network, we present a comprehensive framework for
baseline and dataset recommendation. First, we design an automated
data-collection pipeline that links roughly one hundred thousand accepted
papers to the baselines and datasets they actually used. Second, we propose a
collective perception enhanced retriever. To represent the position of each
dataset or baseline within the scholarly network, it concatenates
self-descriptions with aggregated citation contexts. To achieve efficient
candidate recall, we finetune an embedding model on these representations.
Finally, we develop a reasoning-augmented reranker that exact interaction
chains to construct explicit reasoning chains and finetunes a large language
model to produce interpretable justifications and refined rankings. The dataset
we curated covers 85\% of the datasets and baselines used at top AI conferences
over the past five years. On our dataset, the proposed method outperforms the
strongest prior baseline with average gains of +5.85\% in Recall@20, +8.30\% in
HitRate@5. Taken together, our results advance reliable, interpretable
automation of experimental design.

</details>


### [21] [Diagnosing and Mitigating Semantic Inconsistencies in Wikidata's Classification Hierarchy](https://arxiv.org/abs/2511.04926)
*Shixiong Zhao,Hideaki Takeda*

Main category: cs.CL

TL;DR: 本研究提出了一种新的验证方法来检测Wikidata中的分类错误、过度泛化的子类链接和冗余连接，并开发了一个系统供用户检查任意实体的分类关系。


<details>
  <summary>Details</summary>
Motivation: Wikidata作为最大的开放知识图谱，其宽松的编辑政策导致了分类不一致问题，需要有效的方法来识别和纠正这些错误。

Method: 提出并应用新的验证方法确认分类错误，引入评估标准判断问题是否需要修正，开发用户检查系统。

Result: 成功验证了Wikidata特定领域中存在分类错误、过度泛化的子类链接和冗余连接问题。

Conclusion: 利用Wikidata众包特性开发的系统能够有效帮助用户检查和维护知识图谱的分类一致性。

Abstract: Wikidata is currently the largest open knowledge graph on the web,
encompassing over 120 million entities. It integrates data from various
domain-specific databases and imports a substantial amount of content from
Wikipedia, while also allowing users to freely edit its content. This openness
has positioned Wikidata as a central resource in knowledge graph research and
has enabled convenient knowledge access for users worldwide. However, its
relatively loose editorial policy has also led to a degree of taxonomic
inconsistency. Building on prior work, this study proposes and applies a novel
validation method to confirm the presence of classification errors,
over-generalized subclass links, and redundant connections in specific domains
of Wikidata. We further introduce a new evaluation criterion for determining
whether such issues warrant correction and develop a system that allows users
to inspect the taxonomic relationships of arbitrary Wikidata
entities-leveraging the platform's crowdsourced nature to its full potential.

</details>


### [22] [LoPT: Lossless Parallel Tokenization Acceleration for Long Context Inference of Large Language Model](https://arxiv.org/abs/2511.04952)
*Wei Shao,Lingchao Zheng,Pengyu Wang,Peizhen Zheng,Jun Li,Yuwei Fan*

Main category: cs.CL

TL;DR: LoPT是一个无损并行分词框架，解决了长文本推理中分词瓶颈问题，通过字符位置匹配和动态块长调整确保与顺序分词结果一致，同时显著加速处理。


<details>
  <summary>Details</summary>
Motivation: 长上下文推理场景对大型语言模型越来越重要，但带来了显著的计算延迟。现有并行分词方法因边界伪影导致结果不一致，需要解决这一瓶颈。

Method: 提出LoPT框架，采用基于字符位置的匹配和动态块长调整技术，准确对齐和合并分词片段，确保无损分词。

Result: 在多样化长文本数据集上的实验表明，LoPT实现了显著加速，同时保证无损分词，并提供了理论一致性和鲁棒性验证。

Conclusion: LoPT成功解决了并行分词中的边界一致性问题，为长文本推理提供了高效且可靠的分词解决方案。

Abstract: Long context inference scenarios have become increasingly important for large
language models, yet they introduce significant computational latency. While
prior research has optimized long-sequence inference through operators, model
architectures, and system frameworks, tokenization remains an overlooked
bottleneck. Existing parallel tokenization methods accelerate processing
through text segmentation and multi-process tokenization, but they suffer from
inconsistent results due to boundary artifacts that occur after merging. To
address this, we propose LoPT, a novel Lossless Parallel Tokenization framework
that ensures output identical to standard sequential tokenization. Our approach
employs character-position-based matching and dynamic chunk length adjustment
to align and merge tokenized segments accurately. Extensive experiments across
diverse long-text datasets demonstrate that LoPT achieves significant speedup
while guaranteeing lossless tokenization. We also provide theoretical proof of
consistency and comprehensive analytical studies to validate the robustness of
our method.

</details>


### [23] [Too Good to be Bad: On the Failure of LLMs to Role-Play Villains](https://arxiv.org/abs/2511.04962)
*Zihao Yi,Qingxuan Jiang,Ruotian Ma,Xingyu Chen,Qu Yang,Mengru Wang,Fanghua Ye,Ying Shen,Zhaopeng Tu,Xiaolong Li,Linus*

Main category: cs.CL

TL;DR: LLMs在扮演反派角色时存在系统性困难，安全对齐与角色扮演真实性之间存在根本冲突。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs模拟非亲社会、反派角色的能力，因为现有安全对齐可能阻碍对道德模糊或邪恶角色的真实刻画。

Method: 引入Moral RolePlay基准数据集，包含四级道德对齐量表，让最先进的LLMs扮演从道德模范到纯粹反派的各种角色。

Result: 随着角色道德水平下降，角色扮演保真度呈现一致的单调下降趋势。模型在"欺骗性"和"操纵性"等与安全原则直接对立的特质上表现最差。

Conclusion: 安全对齐与创作保真度之间存在关键张力，需要开发更细致、上下文感知的对齐方法。

Abstract: Large Language Models (LLMs) are increasingly tasked with creative
generation, including the simulation of fictional characters. However, their
ability to portray non-prosocial, antagonistic personas remains largely
unexamined. We hypothesize that the safety alignment of modern LLMs creates a
fundamental conflict with the task of authentically role-playing morally
ambiguous or villainous characters. To investigate this, we introduce the Moral
RolePlay benchmark, a new dataset featuring a four-level moral alignment scale
and a balanced test set for rigorous evaluation. We task state-of-the-art LLMs
with role-playing characters from moral paragons to pure villains. Our
large-scale evaluation reveals a consistent, monotonic decline in role-playing
fidelity as character morality decreases. We find that models struggle most
with traits directly antithetical to safety principles, such as ``Deceitful''
and ``Manipulative'', often substituting nuanced malevolence with superficial
aggression. Furthermore, we demonstrate that general chatbot proficiency is a
poor predictor of villain role-playing ability, with highly safety-aligned
models performing particularly poorly. Our work provides the first systematic
evidence of this critical limitation, highlighting a key tension between model
safety and creative fidelity. Our benchmark and findings pave the way for
developing more nuanced, context-aware alignment methods.

</details>


### [24] [Acquiring Common Chinese Emotional Events Using Large Language Model](https://arxiv.org/abs/2511.04989)
*Ya Wang,Guangzheng Zhu,Cungen Cao,Jingjing Li,He Li,Xin Huang*

Main category: cs.CL

TL;DR: 本文提出了一种获取中文常见情感事件的方法，通过收集情感事件指示词，使用中文大语言模型生成情感事件，并通过过滤器确保质量，最终构建了包含102,218个高质量情感事件的知识库。


<details>
  <summary>Details</summary>
Motivation: 情感事件知识对提升应用效果很重要，但难以获取，特别是上下文无关的常见情感事件。本文旨在获取中文常见情感事件如'获奖'、'被批评'等。

Method: 收集中文情感事件指示词列表，使用中文LLM生成情感事件，训练过滤器剔除无效结果，采用不同技术分类正面和负面事件。

Result: 构建了包含102,218个高质量常见情感事件的知识库，这是唯一大规模的中文情感事件常识知识库。内在评估显示方法有效，外在用例在情感原因抽取领域展示了潜力。

Conclusion: 本文方法能有效获取中文常见情感事件，构建的知识库在情感相关应用中具有重要价值，相关资源将在论文发表后公开。

Abstract: Knowledge about emotional events is an important kind of knowledge which has
been applied to improve the effectiveness of different applications. However,
emotional events cannot be easily acquired, especially common or generalized
emotional events that are context-independent. The goal of this paper is to
obtain common emotional events in Chinese language such as "win a prize" and
"be criticized". Our approach begins by collecting a comprehensive list of
Chinese emotional event indicators. Then, we generate emotional events by
prompting a Chinese large language model (LLM) using these indicators. To
ensure the quality of these emotional events, we train a filter to discard
invalid generated results. We also classify these emotional events as being
positive events and negative events using different techniques. Finally, we
harvest a total of 102,218 high-quality common emotional events with sentiment
polarity labels, which is the only large-scale commonsense knowledge base of
emotional events in Chinese language. Intrinsic evaluation results show that
the proposed method in this paper can be effectively used to acquire common
Chinese emotional events. An extrinsic use case also demonstrates the strong
potential of common emotional events in the field of emotion cause extraction
(ECE). Related resources including emotional event indicators and emotional
events will be released after the publication of this paper.

</details>


### [25] [Pluralistic Behavior Suite: Stress-Testing Multi-Turn Adherence to Custom Behavioral Policies](https://arxiv.org/abs/2511.05018)
*Prasoon Varshney,Makesh Narsimhan Sreedhar,Liwei Jiang,Traian Rebedea,Christopher Parisien*

Main category: cs.CL

TL;DR: 提出了PLURALISTIC BEHAVIOR SUITE (PBSUITE)评估套件，用于系统评估LLMs在多轮交互对话中遵守多元化对齐规范的能力，发现现有模型在单轮设置中表现良好，但在多轮对抗性交互中合规性大幅下降。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的LLM应用往往发生在具有独特企业政策、监管要求和使用案例的组织生态系统中，这凸显了对具有多元化对齐目标的LLMs进行严格评估的需求。

Method: 开发了PBSUITE评估套件，包含300个基于30个行业的现实LLM行为策略数据集，以及一个在对抗条件下压力测试模型合规性的动态评估框架。

Result: 领先的开源和闭源LLMs在单轮设置中保持稳健的行为策略遵守（失败率低于4%），但在多轮对抗性交互中合规性显著减弱（失败率高达84%）。

Conclusion: 现有的模型对齐和安全调节方法在真实世界LLM交互中连贯执行多元化行为策略方面存在不足，需要开发更稳健和上下文感知的多元化对齐技术。

Abstract: Large language models (LLMs) are typically aligned to a universal set of
safety and usage principles intended for broad public acceptability. Yet,
real-world applications of LLMs often take place within organizational
ecosystems shaped by distinctive corporate policies, regulatory requirements,
use cases, brand guidelines, and ethical commitments. This reality highlights
the need for rigorous and comprehensive evaluation of LLMs with pluralistic
alignment goals, an alignment paradigm that emphasizes adaptability to diverse
user values and needs. In this work, we present PLURALISTIC BEHAVIOR SUITE
(PBSUITE), a dynamic evaluation suite designed to systematically assess LLMs'
capacity to adhere to pluralistic alignment specifications in multi-turn,
interactive conversations. PBSUITE consists of (1) a diverse dataset of 300
realistic LLM behavioral policies, grounded in 30 industries; and (2) a dynamic
evaluation framework for stress-testing model compliance with custom behavioral
specifications under adversarial conditions. Using PBSUITE, We find that
leading open- and closed-source LLMs maintain robust adherence to behavioral
policies in single-turn settings (less than 4% failure rates), but their
compliance weakens substantially in multi-turn adversarial interactions (up to
84% failure rates). These findings highlight that existing model alignment and
safety moderation methods fall short in coherently enforcing pluralistic
behavioral policies in real-world LLM interactions. Our work contributes both
the dataset and analytical framework to support future research toward robust
and context-aware pluralistic alignment techniques.

</details>


### [26] [UA-Code-Bench: A Competitive Programming Benchmark for Evaluating LLM Code Generation in Ukrainian](https://arxiv.org/abs/2511.05040)
*Mykyta Syromiatnikov,Victoria Ruvinskaya*

Main category: cs.CL

TL;DR: UA-Code-Bench是一个针对乌克兰语代码生成和竞争性编程问题解决能力评估的新基准，包含500个难度分级的问题，评估了13个领先模型，结果显示即使是顶级模型也只能解决一半问题。


<details>
  <summary>Details</summary>
Motivation: 评估低资源语言中大语言模型的真实能力存在挑战，现有基准大多关注从英语翻译的广泛任务或仅评估简单的语言理解能力。

Method: 使用来自Eolymp平台的500个问题，分布在5个难度级别，通过专用环境对13个领先的专有和开源模型进行一次性提示生成Python解决方案的评估。

Result: 即使是表现最佳的模型（如OpenAI o3和GPT-5）也只能解决一半的问题，突显了在低资源自然语言中进行代码生成的挑战。

Conclusion: 这项研究证明了竞争性编程基准在评估大语言模型方面的价值，特别是在代表性不足的语言中，为多语言代码生成和推理增强模型的未来研究铺平了道路。

Abstract: Evaluating the real capabilities of large language models in low-resource
languages still represents a challenge, as many existing benchmarks focus on
widespread tasks translated from English or evaluate only simple language
understanding. This paper introduces UA-Code-Bench, a new open-source benchmark
established for a thorough evaluation of language models' code generation and
competitive programming problem-solving abilities in Ukrainian. The benchmark
comprises 500 problems from the Eolymp platform, evenly distributed across five
complexity levels from very easy to very hard. A diverse set of 13 leading
proprietary and open-source models, generating Python solutions based on a
one-shot prompt, was evaluated via the dedicated Eolymp environment against
hidden tests, ensuring code correctness. The obtained results reveal that even
top-performing models, such as OpenAI o3 and GPT-5, solve only half of the
problems, highlighting the challenge of code generation in low-resource natural
language. Furthermore, this research presents a comprehensive analysis of
performance across various difficulty levels, as well as an assessment of
solution uniqueness and computational efficiency, measured by both elapsed time
and memory consumption of the generated solutions. In conclusion, this work
demonstrates the value of competitive programming benchmarks in evaluating
large language models, especially in underrepresented languages. It also paves
the way for future research on multilingual code generation and
reasoning-enhanced models. The benchmark, data parsing, preparation, code
generation, and evaluation scripts are available at
https://huggingface.co/datasets/NLPForUA/ua-code-bench.

</details>


### [27] [Order-Level Attention Similarity Across Language Models: A Latent Commonality](https://arxiv.org/abs/2511.05064)
*Jinglin Liang,Jin Zhong,Shuangping Huang,Yunqing Hu,Huiyuan Zhang,Huifang Li,Lixin Fan,Hanlin Gu*

Main category: cs.CL

TL;DR: 本文发现不同语言模型的上下文聚合模式存在共性，提出基于注意力展开的阶次注意力(OLA)作为统一句法特征表示，并开发了无需训练的跨模型适配器迁移方法TOA。


<details>
  <summary>Details</summary>
Motivation: 探索不同语言模型在上下文聚合模式上的共性，这有助于加深对语言模型的理解并促进跨模型知识迁移。

Method: 引入基于注意力展开的阶次注意力(OLA)，发现不同模型的相同阶次OLA具有显著相似性，并提出了以OLA为输入的跨模型适配器迁移方法TOA。

Result: 实验证明TOA能够有效提升未见语言模型的性能，实现跨模型的泛化能力。

Conclusion: 语言模型在上下文聚合模式上存在共性，基于OLA的适配器迁移方法能够实现无需参数更新的跨模型性能提升。

Abstract: In this paper, we explore an important yet previously neglected question: Do
context aggregation patterns across Language Models (LMs) share commonalities?
While some works have investigated context aggregation or attention weights in
LMs, they typically focus on individual models or attention heads, lacking a
systematic analysis across multiple LMs to explore their commonalities. In
contrast, we focus on the commonalities among LMs, which can deepen our
understanding of LMs and even facilitate cross-model knowledge transfer. In
this work, we introduce the Order-Level Attention (OLA) derived from the
order-wise decomposition of Attention Rollout and reveal that the OLA at the
same order across LMs exhibits significant similarities. Furthermore, we
discover an implicit mapping between OLA and syntactic knowledge. Based on
these two findings, we propose the Transferable OLA Adapter (TOA), a
training-free cross-LM adapter transfer method. Specifically, we treat the OLA
as a unified syntactic feature representation and train an adapter that takes
OLA as input. Due to the similarities in OLA across LMs, the adapter
generalizes to unseen LMs without requiring any parameter updates. Extensive
experiments demonstrate that TOA's cross-LM generalization effectively enhances
the performance of unseen LMs. Code is available at
https://github.com/jinglin-liang/OLAS.

</details>


### [28] [Reasoning-Guided Claim Normalization for Noisy Multilingual Social Media Posts](https://arxiv.org/abs/2511.05078)
*Manan Sharma,Arya Suneesh,Manish Jain,Pawan Kumar Rajpoot,Prasanna Devadiga,Bharatdeep Hazarika,Ashish Shrivastava,Kishan Gurumurthy,Anshuman B Suresh,Aditya U Baliga*

Main category: cs.CL

TL;DR: 提出一种多语言虚假信息检测的声明规范化方法，通过系统分解社交媒体帖子为可验证陈述，在20种语言上实现跨语言迁移，仅使用英语数据训练。


<details>
  <summary>Details</summary>
Motivation: 解决多语言虚假信息检测中的声明规范化问题，将嘈杂的社交媒体帖子转化为清晰可验证的陈述，实现跨语言泛化能力。

Method: 使用Qwen3-14B模型进行LoRA微调，结合帖子内去重、语义对齐的token级召回过滤，以及在推理时使用上下文示例的检索增强少样本学习。

Result: 在20种语言上取得METEOR分数从41.16（英语）到15.21（马拉地语），英语排行榜第三名，荷兰语和旁遮普语第四名，相比基线配置相对提升41.3%。

Conclusion: 该方法在罗曼语系和日耳曼语系语言上展现了有效的跨语言泛化能力，同时在不同语言结构中保持了语义连贯性。

Abstract: We address claim normalization for multilingual misinformation detection -
transforming noisy social media posts into clear, verifiable statements across
20 languages. The key contribution demonstrates how systematic decomposition of
posts using Who, What, Where, When, Why and How questions enables robust
cross-lingual transfer despite training exclusively on English data. Our
methodology incorporates finetuning Qwen3-14B using LoRA with the provided
dataset after intra-post deduplication, token-level recall filtering for
semantic alignment and retrieval-augmented few-shot learning with contextual
examples during inference. Our system achieves METEOR scores ranging from 41.16
(English) to 15.21 (Marathi), securing third rank on the English leaderboard
and fourth rank for Dutch and Punjabi. The approach shows 41.3% relative
improvement in METEOR over baseline configurations and substantial gains over
existing methods. Results demonstrate effective cross-lingual generalization
for Romance and Germanic languages while maintaining semantic coherence across
diverse linguistic structures.

</details>


### [29] [On Text Simplification Metrics and General-Purpose LLMs for Accessible Health Information, and A Potential Architectural Advantage of The Instruction-Tuned LLM class](https://arxiv.org/abs/2511.05080)
*P. Bilha Githinji,Aikaterini Meilliou,Peiwu Qin*

Main category: cs.CL

TL;DR: 本文评估了两种通用大语言模型在文本简化任务中的表现，发现指令调优的Mistral 24B在可读性和话语保真度平衡方面优于推理增强的QWen2.5 32B。


<details>
  <summary>Details</summary>
Motivation: 随着公众健康信息寻求行为和数字消费的增加，需要可扩展的解决方案来自动将复杂的科学和技术文档简化为通俗语言。

Method: 使用指令调优的Mistral 24B和推理增强的QWen2.5 32B进行对比分析，评估21个指标涵盖可读性、话语保真度、内容安全性和分布度量。

Result: Mistral在SARI指标上达到42.46分，BERTScore为0.91，在可读性和准确性之间取得更好平衡；QWen虽然可读性提升但BERTScore显著较低（0.89）。

Conclusion: 指令调优的Mistral 24B更适合文本简化任务，词汇支持是简化的主要领域适应问题，五个可读性指标存在功能冗余。

Abstract: The increasing health-seeking behavior and digital consumption of biomedical
information by the general public necessitate scalable solutions for
automatically adapting complex scientific and technical documents into plain
language. Automatic text simplification solutions, including advanced large
language models, however, continue to face challenges in reliably arbitrating
the tension between optimizing readability performance and ensuring
preservation of discourse fidelity. This report empirically assesses the
performance of two major classes of general-purpose LLMs, demonstrating their
linguistic capabilities and foundational readiness for the task compared to a
human benchmark. Using a comparative analysis of the instruction-tuned Mistral
24B and the reasoning-augmented QWen2.5 32B, we identify a potential
architectural advantage in the instruction-tuned LLM. Mistral exhibits a
tempered lexical simplification strategy that enhances readability across a
suite of metrics and the simplification-specific formula SARI (mean 42.46),
while preserving human-level discourse with a BERTScore of 0.91. QWen also
attains enhanced readability performance, but its operational strategy shows a
disconnect in balancing between readability and accuracy, reaching a
statistically significantly lower BERTScore of 0.89. Additionally, a
comprehensive correlation analysis of 21 metrics spanning readability,
discourse fidelity, content safety, and underlying distributional measures for
mechanistic insights, confirms strong functional redundancies among five
readability indices. This empirical evidence tracks baseline performance of the
evolving LLMs for the task of text simplification, identifies the
instruction-tuned Mistral 24B for simplification, provides necessary heuristics
for metric selection, and points to lexical support as a primary
domain-adaptation issue for simplification.

</details>


### [30] [Iterative Layer-wise Distillation for Efficient Compression of Large Language Models](https://arxiv.org/abs/2511.05085)
*Grigory Kovalev,Mikhail Tikhomirov*

Main category: cs.CL

TL;DR: 提出了一种基于ShortGPT改进的LLM蒸馏方法，通过迭代评估层重要性并移除贡献较小的层，将Qwen2.5-3B模型从36层压缩到28层（仅9.7%质量损失）或24层（18%损失）。


<details>
  <summary>Details</summary>
Motivation: 开发紧凑且高性能的LLM模型，解决现有蒸馏方法的局限性，实现模型在资源受限环境中的部署。

Method: 基于ShortGPT方法改进，结合迭代层重要性评估（通过移除单层测量性能退化）和使用KL散度与均方误差的联合损失函数进行进一步训练。

Result: Qwen2.5-3B模型从36层压缩到28层（2.47B参数），质量损失仅9.7%；压缩到24层时损失18%。发现中间transformer层对推理贡献较小。

Conclusion: 提出的迭代蒸馏和微调方法有效，特别适合资源受限环境，证实了中间层在推理中的重要性较低，为创建高效模型提供了可行方案。

Abstract: This work investigates distillation methods for large language models (LLMs)
with the goal of developing compact models that preserve high performance.
Several existing approaches are reviewed, with a discussion of their respective
strengths and limitations. An improved method based on the ShortGPT approach
has been developed, building upon the idea of incorporating iterative
evaluation of layer importance. At each step, importance is assessed by
measuring performance degradation when individual layers are removed, using a
set of representative datasets. This process is combined with further training
using a joint loss function based on KL divergence and mean squared error.
Experiments on the Qwen2.5-3B model show that the number of layers can be
reduced from 36 to 28 (resulting in a 2.47 billion parameter model) with only a
9.7% quality loss, and to 24 layers with an 18% loss. The findings suggest that
the middle transformer layers contribute less to inference, underscoring the
potential of the proposed method for creating efficient models. The results
demonstrate the effectiveness of iterative distillation and fine-tuning, making
the approach suitable for deployment in resource-limited settings.

</details>


### [31] [A Toolbox for Improving Evolutionary Prompt Search](https://arxiv.org/abs/2511.05120)
*Daniel Grießhaber,Maximilian Kimmich,Johannes Maucher,Ngoc Thang Vu*

Main category: cs.CL

TL;DR: 提出了改进的进化提示优化方法，通过分解进化步骤、引入LLM评估、整合人类反馈和开发高效评估策略，提升优化质量和效率。


<details>
  <summary>Details</summary>
Motivation: 现有进化提示优化方法缺乏鲁棒的算子和高效评估机制，需要改进以提升整体性能。

Method: 1) 将进化分解为不同步骤以增强控制和演化；2) 引入基于LLM的评估器验证演化；3) 整合人类反馈优化进化算子；4) 开发保持性能但降低计算开销的高效评估策略。

Result: 提出的方法在优化质量和效率方面均有提升，代码已开源支持新任务的提示优化。

Conclusion: 该工作为进化提示优化提供了关键改进，部分方法可推广到一般提示优化中，促进了该领域的进一步研究。

Abstract: Evolutionary prompt optimization has demonstrated effectiveness in refining
prompts for LLMs. However, existing approaches lack robust operators and
efficient evaluation mechanisms. In this work, we propose several key
improvements to evolutionary prompt optimization that can partially generalize
to prompt optimization in general: 1) decomposing evolution into distinct steps
to enhance the evolution and its control, 2) introducing an LLM-based judge to
verify the evolutions, 3) integrating human feedback to refine the evolutionary
operator, and 4) developing more efficient evaluation strategies that maintain
performance while reducing computational overhead. Our approach improves both
optimization quality and efficiency. We release our code, enabling prompt
optimization on new tasks and facilitating further research in this area.

</details>


### [32] [ManufactuBERT: Efficient Continual Pretraining for Manufacturing](https://arxiv.org/abs/2511.05135)
*Robin Armingaud,Romaric Besançon*

Main category: cs.CL

TL;DR: ManufactuBERT是一个专门针对制造业领域的RoBERTa模型，通过持续预训练在制造业语料库上建立，在制造业相关NLP任务中达到最先进性能，并显著减少训练时间和计算成本。


<details>
  <summary>Details</summary>
Motivation: 通用Transformer编码器在制造业等专业领域表现不佳，因为缺乏对领域特定术语和语义的接触。

Method: 提出一个全面的数据处理流程，包括领域特定过滤和多阶段去重过程，创建制造业语料库，然后对RoBERTa模型进行持续预训练。

Result: ManufactuBERT在制造业相关NLP任务中建立了新的最先进性能，相比非去重数据集训练时间减少33%，计算成本显著降低。

Conclusion: 提出的流程为在其他专业领域开发高性能编码器提供了可复制的示例，模型和语料库将在HuggingFace上发布。

Abstract: While large general-purpose Transformer-based encoders excel at general
language understanding, their performance diminishes in specialized domains
like manufacturing due to a lack of exposure to domain-specific terminology and
semantics. In this paper, we address this gap by introducing ManufactuBERT, a
RoBERTa model continually pretrained on a large-scale corpus curated for the
manufacturing domain. We present a comprehensive data processing pipeline to
create this corpus from web data, involving an initial domain-specific
filtering step followed by a multi-stage deduplication process that removes
redundancies. Our experiments show that ManufactuBERT establishes a new
state-of-the-art on a range of manufacturing-related NLP tasks, outperforming
strong specialized baselines. More importantly, we demonstrate that training on
our carefully deduplicated corpus significantly accelerates convergence,
leading to a 33\% reduction in training time and computational cost compared to
training on the non-deduplicated dataset. The proposed pipeline offers a
reproducible example for developing high-performing encoders in other
specialized domains. We will release our model and curated corpus at
https://huggingface.co/cea-list-ia.

</details>


### [33] [Mind the Gap... or Not? How Translation Errors and Evaluation Details Skew Multilingual Results](https://arxiv.org/abs/2511.05162)
*Jan-Thorsten Peter,David Vilar,Tobias Domhan,Dan Malkin,Markus Freitag*

Main category: cs.CL

TL;DR: 论文发现多语言数学基准测试MGSM存在翻译错误和答案提取标准化问题，导致不同语言间存在性能差距的假象。通过自动质量保证和标准化答案提取，这种语言差距基本消失。


<details>
  <summary>Details</summary>
Motivation: 研究多语言大语言模型在不同语言数学任务上的性能差异，特别是验证高资源和低资源语言之间是否存在一致的性能差距。

Method: 分析标准多语言数学基准测试MGSM，发现翻译错误；提出自动质量保证方法解决数据质量问题，并给出答案提取标准化的建议。

Result: 原始数据显示不同语言间存在显著性能差距，但修正数据质量和标准化答案提取后，这种语言差距基本消失。

Conclusion: 多语言LLM在不同语言数学任务上的性能差距主要是由数据质量问题和答案提取方法不一致造成的假象，而非模型本身的能力差异。

Abstract: Most current large language models (LLMs) support a wide variety of languages
in addition to English, including high-resource languages (e.g. German,
Chinese, French), as well as low-resource ones (e.g. Swahili, Telugu). In
addition they have also shown impressive capabilities in different domains,
like coding, science and math. In this short paper, taking math as an example
domain, we study the performance of different LLMs across languages.
Experimental results show that there exists a non-negligible and consistent gap
in the performance of the models across languages. Interestingly, and somewhat
against expectations, the gap exists for both high- and low-resource languages.
We hope that these results influence further research into cross-lingual
capability generalization for next generation LLMs. If it weren't for the fact
that they are false! By analyzing one of the standard multilingual math
benchmarks (MGSM), we determine that several translation errors are present in
the data. Furthermore, the lack of standardized answer extraction from LLM
outputs further influences the final results. We propose a method for automatic
quality assurance to address the first issue at scale, and give recommendations
to address the second one. Combining these two approaches we show that the
aforementioned language gap mostly disappears, leading to completely different
conclusions from our research. We additionally release the corrected dataset to
the community.

</details>


### [34] [Effectiveness of Chain-of-Thought in Distilling Reasoning Capability from Large Language Models](https://arxiv.org/abs/2511.05184)
*Cong-Thanh Do,Rama Doddipatla,Kate Knill*

Main category: cs.CL

TL;DR: 本文研究了思维链在知识蒸馏中的作用，通过白盒知识蒸馏将大型语言模型的推理能力转移到小型语言模型，并在BBH基准测试中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 探索思维链在知识蒸馏中如何帮助将大型语言模型的推理能力转移到小型语言模型，以提升小型模型在复杂自然语言推理任务中的表现。

Method: 使用Qwen和Llama2系列的大型语言模型，基于CoT-Collection数据集的思维链数据，进行白盒知识蒸馏实验。

Result: 实验结果表明，思维链显著提高了白盒知识蒸馏的效果，使蒸馏后的小型模型在BBH基准测试的自然语言推理和理解任务中获得了更好的平均性能。

Conclusion: 思维链在白盒知识蒸馏中发挥重要作用，能够有效提升小型语言模型在复杂推理任务中的能力。

Abstract: Chain-of-Thought (CoT) prompting is a widely used method to improve the
reasoning capability of Large Language Models (LLMs). More recently, CoT has
been leveraged in Knowledge Distillation (KD) to transfer reasoning capability
from a larger LLM to a smaller one. This paper examines the role of CoT in
distilling the reasoning capability from larger LLMs to smaller LLMs using
white-box KD, analysing its effectiveness in improving the performance of the
distilled models for various natural language reasoning and understanding
tasks. We conduct white-box KD experiments using LLMs from the Qwen and Llama2
families, employing CoT data from the CoT-Collection dataset. The distilled
models are then evaluated on natural language reasoning and understanding tasks
from the BIG-Bench-Hard (BBH) benchmark, which presents complex challenges for
smaller LLMs. Experimental results demonstrate the role of CoT in improving
white-box KD effectiveness, enabling the distilled models to achieve better
average performance in natural language reasoning and understanding tasks from
BBH.

</details>


### [35] [Translation via Annotation: A Computational Study of Translating Classical Chinese into Japanese](https://arxiv.org/abs/2511.05239)
*Zilong Li,Jie Cao*

Main category: cs.CL

TL;DR: 将古汉语翻译为日语的传统标注方法抽象为序列标注任务，通过LLM标注流水线构建数据集，在低资源环境下结合中文NLP辅助任务提升性能，作为LLM直接翻译的补充方案。


<details>
  <summary>Details</summary>
Motivation: 研究古汉语到日语的标注翻译系统面临低资源问题，传统字符标注方法需要现代化处理。

Method: 引入基于LLM的标注流水线，从数字化开源翻译数据构建数据集，结合中文NLP辅助任务训练序列标注模型。

Result: 在低资源设置下，引入中文NLP辅助任务对序列标注训练有促进作用；LLM在直接机器翻译中表现良好，但在字符标注任务上表现不佳。

Conclusion: 提出的方法可以作为LLM的补充方案，在古汉语到日语的标注翻译任务中提供有效支持。

Abstract: Ancient people translated classical Chinese into Japanese by annotating
around each character. We abstract this process as sequence tagging tasks and
fit them into modern language technologies. The research of this annotation and
translation system is a facing low-resource problem. We release this problem by
introducing a LLM-based annotation pipeline and construct a new dataset from
digitalized open-source translation data. We show that under the low-resource
setting, introducing auxiliary Chinese NLP tasks has a promoting effect on the
training of sequence tagging tasks. We also evaluate the performance of large
language models. They achieve high scores in direct machine translation, but
they are confused when being asked to annotate characters. Our method could
work as a supplement of LLMs.

</details>


### [36] [Reflective Personalization Optimization: A Post-hoc Rewriting Framework for Black-Box Large Language Models](https://arxiv.org/abs/2511.05286)
*Teqi Hao,Xioayu Tan,Shaojie Shi,Yinghui Xu,Xihe Qiu*

Main category: cs.CL

TL;DR: RPO是一个新的个性化框架，通过将内容生成与对齐解耦，采用两阶段方法：首先生成通用高质量响应，然后通过反射模块重写以对齐用户偏好，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖上下文注入，让模型同时承担内容生成和风格对齐的双重负担，导致输出质量下降和控制精度有限。需要解决这一基本矛盾。

Method: 提出RPO框架：1）基础模型生成通用高质量响应；2）外部反射模块重写输出以对齐用户偏好。反射模块采用两阶段训练：监督微调建立个性化推理策略，强化学习进一步优化个性化输出质量。

Result: 在LaMP基准测试中，RPO通过解耦内容生成与个性化，显著优于最先进的基线方法。

Conclusion: RPO证明了显式响应塑造优于隐式上下文注入，提供了一种高效、模型无关的个性化层，可无缝集成到任何基础模型中，为用户中心生成场景开辟了新方向。

Abstract: The personalization of black-box large language models (LLMs) is a critical
yet challenging task. Existing approaches predominantly rely on context
injection, where user history is embedded into the prompt to directly guide the
generation process. However, this single-step paradigm imposes a dual burden on
the model: generating accurate content while simultaneously aligning with
user-specific styles. This often results in a trade-off that compromises output
quality and limits precise control. To address this fundamental tension, we
propose Reflective Personalization Optimization (RPO), a novel framework that
redefines the personalization paradigm by decoupling content generation from
alignment. RPO operates in two distinct stages: first, a base model generates a
high-quality, generic response; then, an external reflection module explicitly
rewrites this output to align with the user's preferences. This reflection
module is trained using a two-stage process. Initially, supervised fine-tuning
is employed on structured rewriting trajectories to establish a core
personalized reasoning policy that models the transformation from generic to
user-aligned responses. Subsequently, reinforcement learning is applied to
further refine and enhance the quality of the personalized outputs.
Comprehensive experiments on the LaMP benchmark demonstrate that RPO, by
decoupling content generation from personalization, significantly outperforms
state-of-the-art baselines. These findings underscore the superiority of
explicit response shaping over implicit context injection. Moreover, RPO
introduces an efficient, model-agnostic personalization layer that can be
seamlessly integrated with any underlying base model, paving the way for a new
and effective direction in user-centric generation scenarios.

</details>


### [37] [Listening Between the Lines: Decoding Podcast Narratives with Language Modeling](https://arxiv.org/abs/2511.05310)
*Shreya Gupta,Ojasva Saxena,Arghodeep Nandi,Sarah Masud,Kiran Garimella,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: 开发了一个微调的BERT模型来分析播客中的叙事框架，通过将抽象框架与具体实体关联，解决了现有LLM在处理非结构化对话数据时的局限性。


<details>
  <summary>Details</summary>
Motivation: 播客已成为影响公众舆论的重要媒介，但其非脚本化、多主题和对话式的特点使得自动分析变得困难。现有语言模型难以捕捉人类听众识别叙事框架的细微线索。

Method: 开发并评估了一个微调的BERT模型，明确将叙事框架与对话中提到的具体实体联系起来，使抽象框架在具体细节中落地。然后使用这些细粒度框架标签并与高级主题关联以揭示更广泛的话语趋势。

Result: 提出了一种新颖的框架标注方法，更接近人类对混乱对话数据的判断，并揭示了讨论内容（主题）与呈现方式（框架）之间的系统性关系。

Conclusion: 该方法为研究数字媒体影响力提供了一个更强大的框架，能够更准确地分析播客叙事结构，理解当代话语中的说服和信息传播机制。

Abstract: Podcasts have become a central arena for shaping public opinion, making them
a vital source for understanding contemporary discourse. Their typically
unscripted, multi-themed, and conversational style offers a rich but complex
form of data. To analyze how podcasts persuade and inform, we must examine
their narrative structures -- specifically, the narrative frames they employ.
  The fluid and conversational nature of podcasts presents a significant
challenge for automated analysis. We show that existing large language models,
typically trained on more structured text such as news articles, struggle to
capture the subtle cues that human listeners rely on to identify narrative
frames. As a result, current approaches fall short of accurately analyzing
podcast narratives at scale.
  To solve this, we develop and evaluate a fine-tuned BERT model that
explicitly links narrative frames to specific entities mentioned in the
conversation, effectively grounding the abstract frame in concrete details. Our
approach then uses these granular frame labels and correlates them with
high-level topics to reveal broader discourse trends. The primary contributions
of this paper are: (i) a novel frame-labeling methodology that more closely
aligns with human judgment for messy, conversational data, and (ii) a new
analysis that uncovers the systematic relationship between what is being
discussed (the topic) and how it is being presented (the frame), offering a
more robust framework for studying influence in digital media.

</details>


### [38] [What Are the Facts? Automated Extraction of Court-Established Facts from Criminal-Court Opinions](https://arxiv.org/abs/2511.05320)
*Klára Bendová,Tomáš Knap,Jan Černý,Vojtěch Pour,Jaromir Savelka,Ivana Kvapilíková,Jakub Drápal*

Main category: cs.CL

TL;DR: 本文研究从斯洛伐克法院判决书中提取犯罪描述信息的可行性，比较了正则表达式和大型语言模型两种方法，发现结合使用可达99.5%的提取成功率。


<details>
  <summary>Details</summary>
Motivation: 刑事司法行政数据仅包含有限的犯罪信息，而欧洲大陆法院判决书中包含丰富的犯罪行为描述，这些信息目前未被充分利用。

Method: 使用两种方法提取犯罪描述：基于正则表达式的方法（基础版和高级版）和使用Gemini Flash 2.0模型的LLM方法。高级正则表达式关注"sparing"及其规范化形式，LLM使用预定义指令进行提取。

Result: 基础正则表达式仅识别40.5%的描述，高级正则表达式达97%，LLM达98.75%，两者结合达99.5%。法律学生评估显示高级方法与人工标注匹配度约90%，而基础方法仅34.5%。

Conclusion: 从法院判决书中提取犯罪描述是可行的，正则表达式和LLM方法都显著优于基础方法，两者结合能达到最佳效果，为刑事司法研究提供了新的数据源。

Abstract: Criminal justice administrative data contain only a limited amount of
information about the committed offense. However, there is an unused source of
extensive information in continental European courts' decisions: descriptions
of criminal behaviors in verdicts by which offenders are found guilty. In this
paper, we study the feasibility of extracting these descriptions from publicly
available court decisions from Slovakia. We use two different approaches for
retrieval: regular expressions and large language models (LLMs). Our baseline
was a simple method employing regular expressions to identify typical words
occurring before and after the description. The advanced regular expression
approach further focused on "sparing" and its normalization (insertion of
spaces between individual letters), typical for delineating the description.
The LLM approach involved prompting the Gemini Flash 2.0 model to extract the
descriptions using predefined instructions. Although the baseline identified
descriptions in only 40.5% of verdicts, both methods significantly outperformed
it, achieving 97% with advanced regular expressions and 98.75% with LLMs, and
99.5% when combined. Evaluation by law students showed that both advanced
methods matched human annotations in about 90% of cases, compared to just 34.5%
for the baseline. LLMs fully matched human-labeled descriptions in 91.75% of
instances, and a combination of advanced regular expressions with LLMs reached
92%.

</details>


### [39] [Evaluating Subword Tokenization Techniques for Bengali: A Benchmark Study with BengaliBPE](https://arxiv.org/abs/2511.05324)
*Firoj Ahmmed Patwary,Abdullah Al Noman*

Main category: cs.CL

TL;DR: 提出了BengaliBPE，一个专门为孟加拉语脚本设计的BPE分词器，通过Unicode标准化、字素级初始化和形态感知合并规则来改善孟加拉语的分词效果。


<details>
  <summary>Details</summary>
Motivation: 现有的子词分词器（如SentencePiece或HuggingFace BPE）主要针对拉丁语或多语言语料库设计，在形态丰富的语言（如孟加拉语）上表现不佳。

Method: BengaliBPE应用Unicode标准化、字素级初始化和形态感知合并规则，保持语言一致性和子词完整性，并与Whitespace、SentencePiece BPE和HuggingFace BPE三种基线方法进行比较。

Result: 所有方法表现都合理，但BengaliBPE提供了最详细的分割和最佳的形态可解释性，尽管计算成本稍高。

Conclusion: 研究强调了语言感知分词对于形态丰富脚本的重要性，BengaliBPE为未来孟加拉语NLP系统（包括大规模预训练上下文语言模型）奠定了坚实基础。

Abstract: Tokenization is an important first step in Natural Language Processing (NLP)
pipelines because it decides how models learn and represent linguistic
information. However, current subword tokenizers like SentencePiece or
HuggingFace BPE are mostly designed for Latin or multilingual corpora and do
not perform well on languages with rich morphology such as Bengali. To address
this limitation, we present BengaliBPE, a Byte Pair Encoding (BPE) tokenizer
specifically developed for the Bengali script. BengaliBPE applies Unicode
normalization, grapheme-level initialization, and morphology-aware merge rules
to maintain linguistic consistency and preserve subword integrity. We use a
large-scale Bengali news classification dataset to compare BengaliBPE with
three baselines: Whitespace, SentencePiece BPE, and HuggingFace BPE. The
evaluation considers tokenization granularity, encoding speed, and downstream
classification accuracy. While all methods perform reasonably well, BengaliBPE
provides the most detailed segmentation and the best morphological
interpretability, albeit with slightly higher computational cost. These
findings highlight the importance of language-aware tokenization for
morphologically rich scripts and establish BengaliBPE as a strong foundation
for future Bengali NLP systems, including large-scale pretraining of contextual
language models.

</details>


### [40] [A multimodal multiplex of the mental lexicon for multilingual individuals](https://arxiv.org/abs/2511.05361)
*Maria Huynh,Wilder C. Rodrigues*

Main category: cs.CL

TL;DR: 本研究探讨双语和多语者的心理词典结构，特别是视觉输入如何影响语言习得和翻译任务表现。


<details>
  <summary>Details</summary>
Motivation: 历史上双语被视为认知负担，但近30年研究表明多语者在语言和认知任务中表现更好。本研究旨在探索多语者心理词典结构，特别是视觉输入对语言习得的影响。

Method: 基于Stella等人的心理词典多重网络模型和BIA+框架，采用Kivela的多层网络原则，在多重模型中引入多模态，添加连接视觉输入与多语心理词典层对应词汇表征的额外层。

Result: 研究设计扩展了先前研究，通过翻译任务比较视觉输入与纯文本条件下的参与者的熟练度和准确性。

Conclusion: 本研究旨在验证视觉输入是否影响多语者在翻译任务中的表现，探索遗产语言对另一语言习得的影响机制。

Abstract: Historically, bilingualism was often perceived as an additional cognitive
load that could hinder linguistic and intellectual development. However, over
the last three decades, this view has changed considerably. Numerous studies
have aimed to model and understand the architecture of the bilingual word
recognition system Dijkstra and van Heuven (2002), investigating how parallel
activation operates in the brain and how one language influences another Kroll
et al. (2015). Increasingly, evidence suggests that multilinguals, individuals
who speak three or more languages, can perform better than monolinguals in
various linguistic and cognitive tasks, such as learning an additional language
Abu-Rabia and Sanitsky (2010). This research proposal focuses on the study of
the mental lexicon and how it may be structured in individuals who speak
multiple languages. Building on the work of Stella et al. (2018), who
investigated explosive learning in humans using a multiplex model of the mental
lexicon, and the Bilingual Interactive Activation (BIA+) framework proposed by
Dijkstra and van Heuven (2002), the present study applies the same multilayer
network principles introduced by Kivela et al. (2014). Our experimental design
extends previous research by incorporating multimodality into the multiplex
model, introducing an additional layer that connects visual inputs to their
corresponding lexical representations across the multilingual layers of the
mental lexicon. In this research, we aim to explore how a heritage language
influences the acquisition of another language. Specifically, we ask: Does the
presence of visual input in a translation task influence participants'
proficiency and accuracy compared to text-only conditions?

</details>


### [41] [Large Language Models for Explainable Threat Intelligence](https://arxiv.org/abs/2511.05406)
*Tiago Dinis,Miguel Correia,Roger Tavares*

Main category: cs.CL

TL;DR: RAGRecon系统使用检索增强生成(RAG)的大语言模型来获取网络安全威胁情报，并通过知识图谱提高AI决策的可解释性，在实验中达到了91%以上的准确率。


<details>
  <summary>Details</summary>
Motivation: 传统安全机制难以应对日益复杂的网络威胁，而大语言模型在文本处理和生成方面具有显著潜力，可用于网络安全领域。

Method: 提出RAGRecon系统，结合大语言模型和检索增强生成技术，通过实时信息检索和领域特定数据来回答网络安全威胁问题，并为每个回答生成可视化知识图谱。

Result: 在两个数据集和七个不同大语言模型上的实验评估显示，最佳组合的响应与参考响应的匹配率超过91%。

Conclusion: RAGRecon系统成功展示了LLM与RAG结合在网络安全威胁情报获取中的有效性，并通过知识图谱提高了模型推理的透明度和可解释性。

Abstract: As cyber threats continue to grow in complexity, traditional security
mechanisms struggle to keep up. Large language models (LLMs) offer significant
potential in cybersecurity due to their advanced capabilities in text
processing and generation. This paper explores the use of LLMs with
retrieval-augmented generation (RAG) to obtain threat intelligence by combining
real-time information retrieval with domain-specific data. The proposed system,
RAGRecon, uses a LLM with RAG to answer questions about cybersecurity threats.
Moreover, it makes this form of Artificial Intelligence (AI) explainable by
generating and visually presenting to the user a knowledge graph for every
reply. This increases the transparency and interpretability of the reasoning of
the model, allowing analysts to better understand the connections made by the
system based on the context recovered by the RAG system. We evaluated RAGRecon
experimentally with two datasets and seven different LLMs and the responses
matched the reference responses more than 91% of the time for the best
combinations.

</details>


### [42] [Minority-Aware Satisfaction Estimation in Dialogue Systems via Preference-Adaptive Reinforcement Learning](https://arxiv.org/abs/2511.05407)
*Yahui Fu,Zi Haur Pang,Tatsuya Kawahara*

Main category: cs.CL

TL;DR: 提出一个统一框架来建模个体和群体层面的用户满意度偏好，通过个性化推理链和聚类方法提升对话系统的用户满意度估计，特别是针对少数群体用户。


<details>
  <summary>Details</summary>
Motivation: 现有对话系统对齐方法通常训练通用模型追求广泛共识，但忽略了少数用户群体的特定偏好和个体差异，导致用户满意度评估存在偏差。

Method: 1. Chain-of-Personalized-Reasoning (CoPeR) 通过可解释推理链捕捉个体偏好；2. 基于期望最大化的多数-少数偏好感知聚类算法 (M2PC) 无监督发现用户群体；3. 偏好自适应强化学习框架 (PAda-PPO) 联合优化个体和群体偏好对齐。

Result: 在情感支持对话数据集上的实验表明，该方法在用户满意度估计方面取得一致改进，特别是对于代表性不足的用户群体。

Conclusion: 该框架通过同时考虑个体和群体层面的偏好，有效提升了对话系统的用户满意度估计能力，特别是改善了少数用户群体的体验。

Abstract: User satisfaction in dialogue systems is inherently subjective. When the same
response strategy is applied across users, minority users may assign different
satisfaction ratings than majority users due to variations in individual
intents and preferences. However, existing alignment methods typically train
one-size-fits-all models that aim for broad consensus, often overlooking
minority perspectives and user-specific adaptation. We propose a unified
framework that models both individual- and group-level preferences for user
satisfaction estimation. First, we introduce Chain-of-Personalized-Reasoning
(CoPeR) to capture individual preferences through interpretable reasoning
chains. Second, we propose an expectation-maximization-based Majority-Minority
Preference-Aware Clustering (M2PC) algorithm that discovers distinct user
groups in an unsupervised manner to learn group-level preferences. Finally, we
integrate these components into a preference-adaptive reinforcement learning
framework (PAda-PPO) that jointly optimizes alignment with both individual and
group preferences. Experiments on the Emotional Support Conversation dataset
demonstrate consistent improvements in user satisfaction estimation,
particularly for underrepresented user groups.

</details>


### [43] [Steering Language Models with Weight Arithmetic](https://arxiv.org/abs/2511.05408)
*Constanza Fierro,Fabien Roger*

Main category: cs.CL

TL;DR: 提出了一种称为对比权重引导的简单后训练方法，通过权重算术编辑模型参数，利用狭窄训练数据更好地控制模型行为


<details>
  <summary>Details</summary>
Motivation: 在多样化训练分布上为大型语言模型提供高质量反馈既困难又昂贵，而仅在狭窄分布上提供反馈可能导致意外的泛化问题

Method: 通过减去两个小规模微调的权重增量来隔离权重空间中的行为方向——一个诱导期望行为，另一个诱导相反行为，然后添加或移除该方向来修改模型权重

Result: 权重引导通常比激活引导具有更好的泛化能力，在降低通用能力之前实现更强的分布外行为控制；在任务特定微调中，权重引导可以部分减轻不良行为漂移，同时保持任务性能增益

Conclusion: 可以通过测量微调更新与"邪恶"权重方向之间的相似性来检测新兴的错位行为，表明可能通过监控训练过程中的权重演化来检测在训练或评估中从未显现的罕见错位行为

Abstract: Providing high-quality feedback to Large Language Models (LLMs) on a diverse
training distribution can be difficult and expensive, and providing feedback
only on a narrow distribution can result in unintended generalizations. To
better leverage narrow training data, we propose contrastive weight steering, a
simple post-training method that edits the model parameters using weight
arithmetic. We isolate a behavior direction in weight-space by subtracting the
weight deltas from two small fine-tunes -- one that induces the desired
behavior and another that induces its opposite -- and then add or remove this
direction to modify the model's weights. We apply this technique to mitigate
sycophancy and induce misalignment, and find that weight steering often
generalizes further than activation steering, achieving stronger
out-of-distribution behavioral control before degrading general capabilities.
We also show that, in the context of task-specific fine-tuning, weight steering
can partially mitigate undesired behavioral drift: it can reduce sycophancy and
under-refusals introduced during fine-tuning while preserving task performance
gains. Finally, we provide preliminary evidence that emergent misalignment can
be detected by measuring the similarity between fine-tuning updates and an
"evil" weight direction, suggesting that it may be possible to monitor the
evolution of weights during training and detect rare misaligned behaviors that
never manifest during training or evaluations.

</details>


### [44] [MIMIC-SR-ICD11: A Dataset for Narrative-Based Diagnosis](https://arxiv.org/abs/2511.05485)
*Yuexin Wu,Shiqi Wang,Vasile Rus*

Main category: cs.CL

TL;DR: 提出了MIMIC-SR-ICD11数据集和LL-Rank重排序框架，用于基于临床报告的疾病诊断，通过似然重排序方法显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 疾病诊断是医疗保健的核心，但电子健康记录往往遗漏重要细节。自我报告能保留临床重要信号，需要开发能有效利用这些信息的诊断系统。

Method: 构建MIMIC-SR-ICD11数据集，并提出LL-Rank重排序框架，计算标签在临床报告上下文中的长度归一化联合似然，减去无报告的先验似然。

Result: 在七个模型骨干上，LL-Rank始终优于生成加映射基线方法。消融实验显示其优势主要来自基于PMI的评分，能分离语义兼容性和标签频率偏差。

Conclusion: LL-Rank框架通过似然重排序有效提升了疾病诊断性能，证明了基于概率互信息的评分方法在临床文本理解中的价值。

Abstract: Disease diagnosis is a central pillar of modern healthcare, enabling early
detection and timely intervention for acute conditions while guiding lifestyle
adjustments and medication regimens to prevent or slow chronic disease.
Self-reports preserve clinically salient signals that templated electronic
health record (EHR) documentation often attenuates or omits, especially subtle
but consequential details. To operationalize this shift, we introduce
MIMIC-SR-ICD11, a large English diagnostic dataset built from EHR discharge
notes and natively aligned to WHO ICD-11 terminology. We further present
LL-Rank, a likelihood-based re-ranking framework that computes a
length-normalized joint likelihood of each label given the clinical report
context and subtracts the corresponding report-free prior likelihood for that
label. Across seven model backbones, LL-Rank consistently outperforms a strong
generation-plus-mapping baseline (GenMap). Ablation experiments show that
LL-Rank's gains primarily stem from its PMI-based scoring, which isolates
semantic compatibility from label frequency bias.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [45] [Language Generation and Identification From Partial Enumeration: Tight Density Bounds and Topological Characterizations](https://arxiv.org/abs/2511.05295)
*Jon Kleinberg,Fan Wei*

Main category: cs.DS

TL;DR: 本文研究了语言生成极限框架，证明了最佳可实现下密度的紧界为1/2，并将结果推广到部分枚举设置，同时重新审视了语言识别问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的成功激发了语言生成和学习的形式理论研究，本文旨在解决语言生成极限框架中的主要开放问题。

Method: 研究语言生成极限框架，其中对手从未知语言中枚举字符串，算法必须生成未见过的字符串。扩展到部分枚举设置，并重新审视Gold-Angluin语言识别模型。

Result: 证明了最佳可实现下密度的紧界为1/2，在部分信息设置中算法输出密度至少为α/2，匹配上界。给出了语言识别在部分枚举下的特征化。

Conclusion: 解决了语言生成极限框架中的主要开放问题，将结果推广到部分信息设置，并为语言识别提供了新的拓扑学表述。

Abstract: The success of large language models (LLMs) has motivated formal theories of
language generation and learning. We study the framework of \emph{language
generation in the limit}, where an adversary enumerates strings from an unknown
language $K$ drawn from a countable class, and an algorithm must generate
unseen strings from $K$. Prior work showed that generation is always possible,
and that some algorithms achieve positive lower density, revealing a
\emph{validity--breadth} trade-off between correctness and coverage. We resolve
a main open question in this line, proving a tight bound of $1/2$ on the best
achievable lower density. We then strengthen the model to allow \emph{partial
enumeration}, where the adversary reveals only an infinite subset $C \subseteq
K$. We show that generation in the limit remains achievable, and if $C$ has
lower density $\alpha$ in $K$, the algorithm's output achieves density at least
$\alpha/2$, matching the upper bound. This generalizes the $1/2$ bound to the
partial-information setting, where the generator must recover within a factor
$1/2$ of the revealed subset's density. We further revisit the classical
Gold--Angluin model of \emph{language identification} under partial
enumeration. We characterize when identification in the limit is possible --
when hypotheses $M_t$ eventually satisfy $C \subseteq M \subseteq K$ -- and in
the process give a new topological formulation of Angluin's characterization,
showing that her condition is precisely equivalent to an appropriate
topological space having the $T_D$ separation property.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [46] [A Penny for Your Thoughts: Decoding Speech from Inexpensive Brain Signals](https://arxiv.org/abs/2511.04691)
*Quentin Auster,Kateryna Shapovalenko,Chuang Ma,Demaio Sun*

Main category: cs.SD

TL;DR: 该研究探索了神经网络能否通过将脑电图记录映射到音频表示来解码大脑活动为语音，在Meta最先进的脑电图解码器基础上引入了三项架构改进。


<details>
  <summary>Details</summary>
Motivation: 探索神经网络是否能够通过脑电图记录解码大脑活动为语音，推动脑机接口应用的发展。

Method: 使用对比性CLIP损失训练模型，将脑电图衍生的嵌入与预训练基于transformer的语音模型的嵌入对齐，并引入三项架构改进：特定主题注意力层、个性化空间注意力和带注意力的双路径RNN。

Result: 三项改进中有两项提高了性能：特定主题注意力层（+0.15% WER改进）、个性化空间注意力（+0.45%）、双路径RNN（-1.87%）。

Conclusion: 个性化架构在脑到语音解码中显示出潜力，为脑机接口应用提供了前景。

Abstract: We explore whether neural networks can decode brain activity into speech by
mapping EEG recordings to audio representations. Using EEG data recorded as
subjects listened to natural speech, we train a model with a contrastive CLIP
loss to align EEG-derived embeddings with embeddings from a pre-trained
transformer-based speech model. Building on the state-of-the-art EEG decoder
from Meta, we introduce three architectural modifications: (i) subject-specific
attention layers (+0.15% WER improvement), (ii) personalized spatial attention
(+0.45%), and (iii) a dual-path RNN with attention (-1.87%). Two of the three
modifications improved performance, highlighting the promise of personalized
architectures for brain-to-speech decoding and applications in brain-computer
interfaces.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [47] [Simulating Misinformation Vulnerabilities With Agent Personas](https://arxiv.org/abs/2511.04697)
*David Farr,Lynnette Hui Xian Ng,Stephen Prochaska,Iain J. Cruickshank,Jevin West*

Main category: cs.SI

TL;DR: 使用LLM构建基于代理的模拟来研究不同人群对虚假信息的反应，发现LLM生成的代理与真实标签和人类预测高度一致，且心理图式比职业背景更能影响对虚假信息的解读。


<details>
  <summary>Details</summary>
Motivation: 虚假信息活动会扭曲公众认知并破坏制度稳定，了解不同人群对信息的反应对于设计有效干预措施至关重要，但现实世界实验不切实际且存在伦理挑战。

Method: 使用大型语言模型(LLMs)开发基于代理的模拟，构建涵盖五种职业和三种心理图式的代理角色，评估他们对新闻标题的反应。

Result: LLM生成的代理与真实标签和人类预测高度一致，支持将其用作研究信息反应的代理；心理图式比职业背景更能影响代理对虚假信息的解读。

Conclusion: 这项工作验证了LLM可以作为基于代理的信息网络模型中的代理，用于分析复杂社会系统中的信任、两极分化和对欺骗性内容的易感性。

Abstract: Disinformation campaigns can distort public perception and destabilize
institutions. Understanding how different populations respond to information is
crucial for designing effective interventions, yet real-world experimentation
is impractical and ethically challenging. To address this, we develop an
agent-based simulation using Large Language Models (LLMs) to model responses to
misinformation. We construct agent personas spanning five professions and three
mental schemas, and evaluate their reactions to news headlines. Our findings
show that LLM-generated agents align closely with ground-truth labels and human
predictions, supporting their use as proxies for studying information
responses. We also find that mental schemas, more than professional background,
influence how agents interpret misinformation. This work provides a validation
of LLMs to be used as agents in an agent-based model of an information network
for analyzing trust, polarization, and susceptibility to deceptive content in
complex social systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [48] [Stateful KV Cache Management for LLMs: Balancing Space, Time, Accuracy, and Positional Fidelity](https://arxiv.org/abs/2511.04686)
*Pratik Poudel*

Main category: cs.LG

TL;DR: KV缓存管理策略与模型架构限制的交互作用：当累积KV缓存接近或超过模型训练上下文窗口时，LLM生成质量急剧下降，位置编码完整性是关键因素。


<details>
  <summary>Details</summary>
Motivation: 解决在状态化多轮场景中KV缓存无界增长带来的挑战，特别关注位置编码完整性对生成质量的影响。

Method: 使用状态化基准测试框架进行实证分析，比较不同缓存驱逐策略（包括AttentionTop等高保留率策略）对位置一致性的影响。

Result: 当KV缓存接近模型训练上下文窗口时生成质量急剧下降；保持连续上下文块的简单策略比复杂或位置破坏性策略产生更连贯的生成结果。

Conclusion: 倡导尊重架构限制、保持位置结构、从整体角度看待"缓存健康"的驱逐技术，而不仅仅是缓存大小。

Abstract: The Key-Value (KV) cache is integral to efficient autoregressive inference in
large language models (LLMs), yet its unbounded growth in stateful multi-turn
scenarios presents major challenges. This paper examines the interplay between
KV cache management strategies, the architectural context limits of models like
meta-llama/Meta-Llama-3-8b-instruct, and the often-overlooked integrity of
positional encodings. Through empirical analysis using a stateful benchmarking
framework, we show that LLM generation quality degrades sharply when the
accumulated KV cache approaches or exceeds the model's trained context window
(e.g., 8192 tokens for Llama 3), a failure mode distinct from GPU memory
exhaustion. Common eviction strategies, even high-retention ones (e.g., 99% via
AttentionTop), can worsen performance if they disrupt positional coherence.
Because LLMs rely on consistent positional signals (e.g., RoPE), compacting a
cache by removing non-contiguous tokens can scramble these signals and lead to
degenerative outputs. We further show that simple strategies preserving
contiguous context blocks (e.g., keeping an initial "gist") can yield more
coherent generations than complex or positionally disruptive ones. We advocate
for eviction techniques that respect architectural limits, preserve positional
structure, and view "cache health" holistically beyond mere size.

</details>


### [49] [APP: Accelerated Path Patching with Task-Specific Pruning](https://arxiv.org/abs/2511.05442)
*Frauke Andersen,William Rudman,Ruochen Zhang,Carsten Eickhoff*

Main category: cs.LG

TL;DR: 提出了加速路径修补(APP)方法，通过对比性注意力头剪枝技术大幅减少电路发现的计算成本，在保持电路质量的同时实现59.63%-93.27%的速度提升


<details>
  <summary>Details</summary>
Motivation: 现有电路发现方法如路径修补计算成本高，限制了在小模型上进行深入电路分析的能力

Method: APP结合了对比性FLAP剪枝算法和传统路径修补：首先使用对比性注意力头剪枝减少搜索空间，然后在剩余注意力头上应用路径修补

Result: APP平均减少56%的搜索空间，速度提升59.63%-93.27%，发现的电路与原始路径修补电路有显著重叠且性能相似

Conclusion: APP在保持电路发现质量的同时大幅降低了计算成本，为机制可解释性研究提供了更高效的解决方案

Abstract: Circuit discovery is a key step in many mechanistic interpretability
pipelines. Current methods, such as Path Patching, are computationally
expensive and have limited in-depth circuit analysis for smaller models. In
this study, we propose Accelerated Path Patching (APP), a hybrid approach
leveraging our novel contrastive attention head pruning method to drastically
reduce the search space of circuit discovery methods. Our Contrastive-FLAP
pruning algorithm uses techniques from causal mediation analysis to assign
higher pruning scores to task-specific attention heads, leading to higher
performing sparse models compared to traditional pruning techniques. Although
Contrastive-FLAP is successful at preserving task-specific heads that existing
pruning algorithms remove at low sparsity ratios, the circuits found by
Contrastive-FLAP alone are too large to satisfy the minimality constraint
required in circuit analysis. APP first applies Contrastive-FLAP to reduce the
search space on required for circuit discovery algorithms by, on average, 56\%.
Next, APP, applies traditional Path Patching on the remaining attention heads,
leading to a speed up of 59.63\%-93.27\% compared to Path Patching applied to
the dense model. Despite the substantial computational saving that APP
provides, circuits obtained from APP exhibit substantial overlap and similar
performance to previously established Path Patching circuits

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [50] [ORCHID: Orchestrated Retrieval-Augmented Classification with Human-in-the-Loop Intelligent Decision-Making for High-Risk Property](https://arxiv.org/abs/2511.04956)
*Maria Mahbub,Vanessa Lama,Sanjay Das,Brian Starks,Christopher Polchek,Saffell Silvers,Lauren Deck,Prasanna Balaprakash,Tirthankar Ghosal*

Main category: cs.AI

TL;DR: ORCHID是一个模块化智能体系统，用于美国能源部高风险财产分类，结合检索增强生成和人工监督，提高分类准确性和可追溯性。


<details>
  <summary>Details</summary>
Motivation: 传统专家工作流程耗时、易积压，难以跟上不断变化的监管边界，需要更高效透明的合规决策系统。

Method: 使用小型协作智能体（检索、描述精炼、分类器、验证器、反馈记录器），通过智能体间消息传递和模型上下文协议进行协调，实现模型无关的本地操作。

Result: 在真实高风险财产案例的初步测试中，ORCHID相比非智能体基线提高了准确性和可追溯性，同时将不确定项目转交给领域专家。

Conclusion: ORCHID展示了在敏感能源部合规工作流程中实现可信赖大语言模型辅助的实用路径。

Abstract: High-Risk Property (HRP) classification is critical at U.S. Department of
Energy (DOE) sites, where inventories include sensitive and often dual-use
equipment. Compliance must track evolving rules designated by various export
control policies to make transparent and auditable decisions. Traditional
expert-only workflows are time-consuming, backlog-prone, and struggle to keep
pace with shifting regulatory boundaries. We demo ORCHID, a modular agentic
system for HRP classification that pairs retrieval-augmented generation (RAG)
with human oversight to produce policy-based outputs that can be audited. Small
cooperating agents, retrieval, description refiner, classifier, validator, and
feedback logger, coordinate via agent-to-agent messaging and invoke tools
through the Model Context Protocol (MCP) for model-agnostic on-premise
operation. The interface follows an Item to Evidence to Decision loop with
step-by-step reasoning, on-policy citations, and append-only audit bundles
(run-cards, prompts, evidence). In preliminary tests on real HRP cases, ORCHID
improves accuracy and traceability over a non-agentic baseline while deferring
uncertain items to Subject Matter Experts (SMEs). The demonstration shows
single item submission, grounded citations, SME feedback capture, and
exportable audit artifacts, illustrating a practical path to trustworthy LLM
assistance in sensitive DOE compliance workflows.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [51] [Towards Mitigating Hallucinations in Large Vision-Language Models by Refining Textual Embeddings](https://arxiv.org/abs/2511.05017)
*Aakriti Agrawal,Gouthaman KV,Rohith Aralikatti,Gauri Jagatap,Jiaxin Yuan,Vijay Kamarshi,Andrea Fanelli,Furong Huang*

Main category: cs.CV

TL;DR: 本文发现主流LVLM架构存在语言模态偏向问题，并提出通过平均池化视觉特征来精炼文本嵌入的简单有效方法，显著改善了视觉定位并减少了幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 识别出现有LVLM架构中对语言模态的内在偏见，这种偏见主要源于将视觉嵌入简单附加到输入文本序列的常见做法。

Method: 提出一种简单有效的方法，通过整合平均池化的视觉特征来精炼文本嵌入。

Result: 该方法在已有基准测试中显著改善了视觉定位能力，并大幅减少了幻觉现象。

Conclusion: 平均池化提供了一种简单、鲁棒且高效的视觉信息整合方式，但更复杂的融合方法可能进一步改善视觉定位和跨模态对齐，这留待未来研究。

Abstract: In this work, we identify an inherent bias in prevailing LVLM architectures
toward the language modality, largely resulting from the common practice of
simply appending visual embeddings to the input text sequence. To address this,
we propose a simple yet effective method that refines textual embeddings by
integrating average-pooled visual features. Our approach demonstrably improves
visual grounding and significantly reduces hallucinations on established
benchmarks. While average pooling offers a straightforward, robust, and
efficient means of incorporating visual information, we believe that more
sophisticated fusion methods could further enhance visual grounding and
cross-modal alignment. Given that the primary focus of this work is to
highlight the modality imbalance and its impact on hallucinations -- and to
show that refining textual embeddings with visual information mitigates this
issue -- we leave exploration of advanced fusion strategies for future work.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [52] [Jailbreaking in the Haystack](https://arxiv.org/abs/2511.04707)
*Rishi Rajesh Shah,Chen Henry Wu,Shashwat Saxena,Ziqian Zhong,Alexander Robey,Aditi Raghunathan*

Main category: cs.CR

TL;DR: NINJA是一种通过在有害用户目标后附加良性模型生成内容来攻击对齐语言模型的越狱方法，利用目标位置对安全性的重要影响，在长上下文模型中实现高效攻击。


<details>
  <summary>Details</summary>
Motivation: 随着长上下文语言模型支持百万token输入，其在复杂任务中的能力得到扩展，但这些扩展上下文的安全影响尚不明确，需要研究长上下文带来的安全漏洞。

Method: 提出NINJA方法，通过将有害用户目标放置在模型生成的良性内容之后，利用目标位置对安全性的影响来实施越狱攻击。该方法资源需求低、可迁移且难以检测。

Result: 在标准安全基准HarmBench上的实验表明，NINJA显著提高了对LLaMA、Qwen、Mistral和Gemini等先进开源和专有模型的攻击成功率，且在固定计算预算下，增加上下文长度比增加尝试次数更有效。

Conclusion: 即使是良性的长上下文，当精心设计目标位置时，也会在现代语言模型中引入根本性漏洞，揭示了长上下文模型的安全风险。

Abstract: Recent advances in long-context language models (LMs) have enabled
million-token inputs, expanding their capabilities across complex tasks like
computer-use agents. Yet, the safety implications of these extended contexts
remain unclear. To bridge this gap, we introduce NINJA (short for
Needle-in-haystack jailbreak attack), a method that jailbreaks aligned LMs by
appending benign, model-generated content to harmful user goals. Critical to
our method is the observation that the position of harmful goals play an
important role in safety. Experiments on standard safety benchmark, HarmBench,
show that NINJA significantly increases attack success rates across
state-of-the-art open and proprietary models, including LLaMA, Qwen, Mistral,
and Gemini. Unlike prior jailbreaking methods, our approach is low-resource,
transferable, and less detectable. Moreover, we show that NINJA is
compute-optimal -- under a fixed compute budget, increasing context length can
outperform increasing the number of trials in best-of-N jailbreak. These
findings reveal that even benign long contexts -- when crafted with careful
goal positioning -- introduce fundamental vulnerabilities in modern LMs.

</details>


### [53] [ConVerse: Benchmarking Contextual Safety in Agent-to-Agent Conversations](https://arxiv.org/abs/2511.05359)
*Amr Gomaa,Ahmed Salem,Sahar Abdelnabi*

Main category: cs.CR

TL;DR: ConVerse是一个评估多智能体交互中隐私和安全风险的动态基准，涵盖旅行、房地产、保险三个领域，包含864种情境化攻击。评估显示现有模型存在严重漏洞，隐私攻击成功率高达88%，安全漏洞达60%。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型发展为代表用户行动的自主智能体，确保多智能体生态系统中的安全性成为核心挑战。智能体间协作需要信息共享，但每次交互都会产生新的攻击面。

Method: ConVerse基准涵盖三个实际领域，包含12个用户角色和864种情境化攻击。它模拟自主的多轮智能体对话，将恶意请求嵌入合理对话中，通过三层分类法评估隐私，安全攻击则针对工具使用和偏好操纵。

Result: 评估7个最先进模型显示持续存在的漏洞：隐私攻击成功率高达88%，安全漏洞达60%，且更强的模型泄露更多信息。

Conclusion: 通过在多智能体交互环境中统一隐私和安全，ConVerse将安全性重新定义为通信的涌现属性。

Abstract: As language models evolve into autonomous agents that act and communicate on
behalf of users, ensuring safety in multi-agent ecosystems becomes a central
challenge. Interactions between personal assistants and external service
providers expose a core tension between utility and protection: effective
collaboration requires information sharing, yet every exchange creates new
attack surfaces. We introduce ConVerse, a dynamic benchmark for evaluating
privacy and security risks in agent-agent interactions. ConVerse spans three
practical domains (travel, real estate, insurance) with 12 user personas and
over 864 contextually grounded attacks (611 privacy, 253 security). Unlike
prior single-agent settings, it models autonomous, multi-turn agent-to-agent
conversations where malicious requests are embedded within plausible discourse.
Privacy is tested through a three-tier taxonomy assessing abstraction quality,
while security attacks target tool use and preference manipulation. Evaluating
seven state-of-the-art models reveals persistent vulnerabilities; privacy
attacks succeed in up to 88% of cases and security breaches in up to 60%, with
stronger models leaking more. By unifying privacy and security within
interactive multi-agent contexts, ConVerse reframes safety as an emergent
property of communication.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [54] [Association via Entropy Reduction](https://arxiv.org/abs/2511.04901)
*Anthony Gamst,Lawrence Wilson*

Main category: cs.IR

TL;DR: 本文提出了一种新的文档关联度评分方法aver，在大型图关联顶点发现任务中表现优于传统的tf-idf方法，具有自然阈值、能区分tf-idf无法区分的文档对等优势。


<details>
  <summary>Details</summary>
Motivation: 在神经网络成功应用之前，tf-idf被认为是识别文档相关性的最佳选择，但作者发现其在某些场景下存在局限性，需要一种更自然、更有效的替代方法。

Method: 提出aver评分方法，该方法基于简单统计模型的熵推导而来，能够处理文档对之间的关联度评估，并可扩展到更大的文档集合。

Result: 在具有真实关联标注的数据集上，aver在发现关联文档对方面表现优于tf-idf，特别是在大型图关联顶点发现任务中。

Conclusion: aver作为一种基于熵的评分方法，在某些场景下比tf-idf更有效，特别是在需要自然阈值、区分高相似度文档对和处理大规模文档集合时，尽管其计算和解释相对复杂。

Abstract: Prior to recent successes using neural networks, term frequency-inverse
document frequency (tf-idf) was clearly regarded as the best choice for
identifying documents related to a query. We provide a different score, aver,
and observe, on a dataset with ground truth marking for association, that aver
does do better at finding assciated pairs than tf-idf. This example involves
finding associated vertices in a large graph and that may be an area where
neural networks are not currently an obvious best choice. Beyond this one
anecdote, we observe that (1) aver has a natural threshold for declaring pairs
as unassociated while tf-idf does not, (2) aver can distinguish between pairs
of documents for which tf-idf gives a score of 1.0, (3) aver can be applied to
larger collections of documents than pairs while tf-idf cannot, and (4) that
aver is derived from entropy under a simple statistical model while tf-idf is a
construction designed to achieve a certain goal and hence aver may be more
"natural." To be fair, we also observe that (1) writing down and computing the
aver score for a pair is more complex than for tf-idf and (2) that the fact
that the aver score is naturally scale-free makes it more complicated to
interpret aver scores.

</details>


### [55] [Wikipedia-based Datasets in Russian Information Retrieval Benchmark RusBEIR](https://arxiv.org/abs/2511.05079)
*Grigory Kovalev,Natalia Loukachevitch,Mikhail Tikhomirov,Olga Babina,Pavel Mamaev*

Main category: cs.IR

TL;DR: 本文构建了基于俄语维基百科'你知道吗...'部分的新型俄语信息检索数据集，支持事实核查、检索增强生成和全文检索等任务，并通过实验比较了词汇检索模型与神经模型的性能。


<details>
  <summary>Details</summary>
Motivation: 扩展俄语信息检索资源，解决现有俄语IR数据集不足的问题，为俄语检索研究提供更丰富的评估基准。

Method: 从俄语维基百科'你知道吗...'部分构建数据集，包含有趣事实及其引用的维基百科文章，采用句子级分级相关性标注。通过实验比较BM25等词汇检索模型与针对俄语优化的神经架构及多语言模型。

Result: 词汇检索方法在全文检索中表现优于神经模型，而神经方法在短文本（如事实核查）中能更好地捕捉词汇语义。结合检索与神经重排序能持续提升性能。

Conclusion: 该研究扩展了俄语信息检索资源，强调了准确评估检索模型对实现最优性能的重要性，所有数据集和实现代码均已公开。

Abstract: In this paper, we present a novel series of Russian information retrieval
datasets constructed from the "Did you know..." section of Russian Wikipedia.
Our datasets support a range of retrieval tasks, including fact-checking,
retrieval-augmented generation, and full-document retrieval, by leveraging
interesting facts and their referenced Wikipedia articles annotated at the
sentence level with graded relevance. We describe the methodology for dataset
creation that enables the expansion of existing Russian Information Retrieval
(IR) resources. Through extensive experiments, we extend the RusBEIR research
by comparing lexical retrieval models, such as BM25, with state-of-the-art
neural architectures fine-tuned for Russian, as well as multilingual models.
Results of our experiments show that lexical methods tend to outperform neural
models on full-document retrieval, while neural approaches better capture
lexical semantics in shorter texts, such as in fact-checking or fine-grained
retrieval. Using our newly created datasets, we also analyze the impact of
document length on retrieval performance and demonstrate that combining
retrieval with neural reranking consistently improves results. Our contribution
expands the resources available for Russian information retrieval research and
highlights the importance of accurate evaluation of retrieval models to achieve
optimal performance. All datasets are publicly available at HuggingFace. To
facilitate reproducibility and future research, we also release the full
implementation on GitHub.

</details>


### [56] [QUESTER: Query Specification for Generative Retrieval](https://arxiv.org/abs/2511.05301)
*Arthur Satouf,Yuxuan Zong,Habiboulaye Amadou-Boubacar,Pablo Piantanida,Benjamin Piwowarski*

Main category: cs.IR

TL;DR: QUESTER将生成式检索重构为查询规范生成，使用小型LLM生成BM25关键词查询，通过强化学习训练，在效率和效果上优于BM25，与神经IR模型竞争


<details>
  <summary>Details</summary>
Motivation: 传统生成式检索在泛化性和扩展成本方面存在困难，需要更有效的替代方案

Method: 使用小型LLM将生成式检索重构为查询规范生成（生成BM25关键词查询），采用GRPO强化学习技术训练策略

Result: 在领域内和跨领域评估中，模型效果优于BM25，与神经IR模型竞争力相当，同时保持良好效率

Conclusion: QUESTER提供了一种有效且高效的生成式检索替代方案，通过查询规范生成方法解决了传统GR的局限性

Abstract: Generative Retrieval (GR) differs from the traditional index-then-retrieve
pipeline by storing relevance in model parameters and directly generating
document identifiers. However, GR often struggles to generalize and is costly
to scale. We introduce QUESTER (QUEry SpecificaTion gEnerative Retrieval),
which reframes GR as query specification generation - in this work, a simple
keyword query handled by BM25 - using a (small) LLM. The policy is trained
using reinforcement learning techniques (GRPO). Across in- and out-of-domain
evaluations, we show that our model is more effective than BM25, and
competitive with neural IR models, while maintaining a good efficiency

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [57] [Enhancing Public Speaking Skills in Engineering Students Through AI](https://arxiv.org/abs/2511.04995)
*Amol Harsh,Brainerd Prince,Siddharth Siddharth,Deepan Raj Prabakar Muthirayan,Kabir S Bhalla,Esraaj Sarkar Gupta,Siddharth Sahu*

Main category: cs.HC

TL;DR: 开发了一个多模态AI系统，用于评估工程学生的公开演讲能力，整合语音分析、计算机视觉和情感检测，提供个性化反馈。


<details>
  <summary>Details</summary>
Motivation: 解决工程学生有效沟通的挑战，传统大学课程无法提供持续个性化培训，人工评估耗时且不切实际。

Method: 结合语音分析（音调、音量、语速、语调）、计算机视觉（面部表情、手势、姿势）和情感检测，开发多模态AI评估模型，特别强调言语与肢体语言的表达一致性。

Result: 初步测试显示AI生成反馈与专家评估中度一致，在评估的LLM模型中，Gemini Pro表现最佳，与人类评估者一致性最强。

Conclusion: AI驱动的公开演讲训练器消除了对人工评估的依赖，使学生能够反复练习，自然协调言语与肢体语言，这对有效专业沟通至关重要。

Abstract: This research-to-practice full paper was inspired by the persistent challenge
in effective communication among engineering students. Public speaking is a
necessary skill for future engineers as they have to communicate technical
knowledge with diverse stakeholders. While universities offer courses or
workshops, they are unable to offer sustained and personalized training to
students. Providing comprehensive feedback on both verbal and non-verbal
aspects of public speaking is time-intensive, making consistent and
individualized assessment impractical. This study integrates research on verbal
and non-verbal cues in public speaking to develop an AI-driven assessment model
for engineering students. Our approach combines speech analysis, computer
vision, and sentiment detection into a multi-modal AI system that provides
assessment and feedback. The model evaluates (1) verbal communication (pitch,
loudness, pacing, intonation), (2) non-verbal communication (facial
expressions, gestures, posture), and (3) expressive coherence, a novel
integration ensuring alignment between speech and body language. Unlike
previous systems that assess these aspects separately, our model fuses multiple
modalities to deliver personalized, scalable feedback. Preliminary testing
demonstrated that our AI-generated feedback was moderately aligned with expert
evaluations. Among the state-of-the-art AI models evaluated, all of which were
Large Language Models (LLMs), including Gemini and OpenAI models, Gemini Pro
emerged as the best-performing, showing the strongest agreement with human
annotators. By eliminating reliance on human evaluators, this AI-driven public
speaking trainer enables repeated practice, helping students naturally align
their speech with body language and emotion, crucial for impactful and
professional communication.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [58] [Quantifying the Climate Risk of Generative AI: Region-Aware Carbon Accounting with G-TRACE and the AI Sustainability Pyramid](https://arxiv.org/abs/2511.04776)
*Zahida Kausar,Seemab Latif,Raja Khurrum Shahzad,Mehwish Fatima*

Main category: cs.CY

TL;DR: G-TRACE框架量化生成式AI的碳排放，通过宫崎骏风格图像生成趋势案例展示病毒式参与如何放大个体数字行为的气候影响，并提出AI可持续性金字塔治理模型。


<details>
  <summary>Details</summary>
Motivation: 生成式AI作为快速扩张的数字基础设施，其能源需求和碳排放正成为新的气候风险类别，需要量化评估和治理框架。

Method: 开发G-TRACE跨模态、区域感知框架，结合真实世界分析和微观模拟，测量不同输出类型（文本、图像、视频）的能源使用和碳强度。

Result: 通过宫崎骏风格图像生成趋势（2024-2025）估算消耗4,309 MWh能源和产生2,068吨CO2排放，显示分散式推理如何放大系统级影响。

Conclusion: 提出AI可持续性金字塔七级治理模型，将碳排放指标转化为可操作政策指导，支持可持续AI部署和气候风险框架下的技术创新。

Abstract: Generative Artificial Intelligence (GenAI) represents a rapidly expanding
digital infrastructure whose energy demand and associated CO2 emissions are
emerging as a new category of climate risk. This study introduces G-TRACE
(GenAI Transformative Carbon Estimator), a cross-modal, region-aware framework
that quantifies training- and inference-related emissions across modalities and
deployment geographies. Using real-world analytics and microscopic simulation,
G-TRACE measures energy use and carbon intensity per output type (text, image,
video) and reveals how decentralized inference amplifies small per-query energy
costs into system-level impacts. Through the Ghibli-style image generation
trend (2024-2025), we estimate 4,309 MWh of energy consumption and 2,068 tCO2
emissions, illustrating how viral participation inflates individual digital
actions into tonne-scale consequences. Building on these findings, we propose
the AI Sustainability Pyramid, a seven-level governance model linking carbon
accounting metrics (L1-L7) with operational readiness, optimization, and
stewardship. This framework translates quantitative emission metrics into
actionable policy guidance for sustainable AI deployment. The study contributes
to the quantitative assessment of emerging digital infrastructures as a novel
category of climate risk, supporting adaptive governance for sustainable
technology deployment. By situating GenAI within climate-risk frameworks, the
work advances data-driven methods for aligning technological innovation with
global decarbonization and resilience objectives.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [59] [Automatización de Informes Geotécnicos para Macizos Rocosos con IA](https://arxiv.org/abs/2511.04690)
*Christofer Valencia,Alexis Llumigusín,Silvia Alvarez,Abrahan Arias,Christian Mejia-Escobar*

Main category: cs.MM

TL;DR: 使用多模态大语言模型自动生成岩土工程报告，通过图像和现场数据处理替代传统人工方法，提高了效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统岩土报告编制方法缓慢、易出错且主观性强，需要更高效、客观的自动化解决方案。

Method: 收集岩石露头照片和样本描述，结合已有报告定义报告结构，通过提示工程优化多模态大语言模型的响应，避免昂贵的微调过程。

Result: 系统评估显示BLEU得分为0.455，ROUGE-L得分为0.653，自动描述与专家描述具有可比性。

Conclusion: 开发的网络工具具有直观界面和标准化导出功能，为地质专业人员和学生提供了创新性的重要贡献。

Abstract: Geotechnical reports are crucial for assessing the stability of rock
formations and ensuring safety in modern engineering. Traditionally, these
reports are prepared manually based on field observations using compasses,
magnifying glasses, and notebooks. This method is slow, prone to errors, and
subjective in its interpretations. To overcome these limitations, the use of
artificial intelligence techniques is proposed for the automatic generation of
reports through the processing of images and field data. The methodology was
based on the collection of photographs of rock outcrops and manual samples with
their respective descriptions, as well as on the reports prepared during the
Geotechnical Studies course. These resources were used to define the report
outline, prompt engineering, and validate the responses of a multimodal large
language model (MLLM). The iterative refinement of prompts until structured and
specific instructions were obtained for each section of the report proved to be
an effective alternative to the costly process of fine-tuning the MLLM. The
system evaluation establishes values of 0.455 and 0.653 for the BLEU and
ROUGE-L metrics, respectively, suggesting that automatic descriptions are
comparable to those made by experts. This tool, accessible via the web, with an
intuitive interface and the ability to export to standardized formats,
represents an innovation and an important contribution for professionals and
students of field geology.

</details>
